<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-6" href="../jmlr2008/jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">jmlr2008-6</a> <a title="jmlr-2008-6-reference" href="#">jmlr2008-6-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</h1>
<br/><p>Source: <a title="jmlr-2008-6-pdf" href="http://jmlr.org/papers/volume9/xie08a/xie08a.pdf">pdf</a></p><p>Author: Xianchao Xie, Zhi Geng</p><p>Abstract: In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is ﬁrst decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efﬁciency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method. Keywords: Bayesian network, conditional independence, decomposition, directed acyclic graph, structural learning</p><br/>
<h2>reference text</h2><p>B. Abramson, J. Brown, A. Murphy, and R. L. Winkler. Hailﬁnder: A Bayesian system for forecasting severe weather. International Journal of Forecasting, 12:57-71, 1996. C.F. Aliferis, I. Tsamardinos, and A. Statnikov. Causal Explorer: A probabilistic network learning toolkit for discovery. The 2003 International Conference on Mathematics and Engineering Techniques in Medicine and Biological Sciences, 2003. S. Arnborg, D.G. Corneil, and A. Proskurowski. Complexity of ﬁnding embeddings in k-trees. SIAM Journal of Algebraic and Discrete Methods, 8(2):277-284, 1987. A. Becker and D. Geiger, A sufﬁciently fast algorithm for ﬁnding close to optimal clique trees. Artiﬁcial Intelligence, 125:3-17, 2001. 480  R ECURSIVE M ETHOD FOR L EARNING DAG S  I. Beinlich, H. Suermondt, R. Chavez, and G. Cooper. The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks. In Proceedings of the 2nd European Conference on Artiﬁcial Intelligence in Medicine, pages 247-256, Springer-Verlag, Berlin, 1989. J. Binder, D. Koller, S.J. Russell, and K. Kanazawa. Adaptive probabilistic networks with hidden variables. Machine Learning, 29:213-244, 1997. R. Castelo and A. Roverato. A robust procedure for Gaussian graphical model search from Microarray data with p larger than n. Journal of Machine Learning Research, 7:2621-2650, 2006. J. Cheng, R. Greiner, J. Kelly, D. Bell, and W. Liu. Learning Bayesian networks from data: An information-theory based approach. Artiﬁcial Intelligence, 137(1):43-90, 2002. D.M. Chickering. Learning equivalence classes of Bayesian-network structures. Journal of Machine Learning Research, 2:445-498, 2002. D.M. Chickering, D. Heckerman, and C. Meek. Large-sample learning of Bayesian networks is NP-hard. Journal of Machine Learning Research, 5:1287-1330, 2004. G.F. Cooper and E. Herskovits. A Bayesian method for the induction of probabilistic networks from data. Machine Learning, 9:309-348, 1992. R.G. Cowell, A. P. David, S.L. Lauritzen, and D.J. Spiegelhalter. Probabilistic Networks and Expert Systems, Springer Publications, New York, 1999. A.P. Dempster. Covariance selection. Biometrics, 28:157-175, 1972. B. Engelhardt, M.I. Jordan, and S. Brenner. A statistical graphical model for predicting protein molecular function. In Proceedings of the 23rd International Conference on Machine Learning, pages 297-304, Pittsburgh, Pennsylvania, 2006. N. Friedman, I. Nachmana, and D. Pe’er. Learning Bayesian network structure from massive datasets: The “Sparse Candidate” algorithm. In Proceedings of the Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 206-215, Stockholm, Sweden, 1999. N. Friedman and D. Koller. Being Bayesian about Bayesian Network structure: A Bayesian approach to structure discovery in Bayesian Networks. Machine Learning, 50:95-125, 2003. Z. Geng, C. Wang, and Q. Zhao. Decomposition of search for v-structures in DAGs. Journal of Multivariate Analysis, 96(2):282-294, 2005. D. Heckerman, D. Geiger, and D. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20:197-243, 1995. D. Heckerman. A tutorial on learning with Bayesian networks. Learning in graphical models, pages 301-354, M. Jordan (Ed.), Kluwer Academic Pub., Netherlands, 1998. F.V. Jensen and F. Jensen. Optimal junction trees. In Proccedings of the 10th Conference on Uncertainty in Artiﬁcial Intelligence, pages 360-366, San Fransisco, CA, 1994. 481  X IE AND G ENG  M. I. Jordan. Graphical models. Statistical Science, (Special Issue on Bayesian Statistics), 19:140155, 2004. M. Kalisch and P. B¨ hlmann. Estimating high-dimensional directed acyclic graphs with the PCu algorithm. Journal of Machine Learning Research, 8:613-636, 2007. D. Koller and M. Sahami. Toward optimal feature selection. In Proceedings of the Thirteenth International Conference on Machine Learning, pages 284-292, Bari, Italy, 1996. S. Kullback and R.A. Leibler. On information and sufﬁciency, Annals of Mathematical Statistics, 22:79-86, 1951. S.L. Lauritzen. Graphical Models, Clarendon Press, Oxford, 1996. D. Margaritis and S. Thrun. Bayesian network induction via local neighborhoods. In Proceedings of the Twelfth Advances in Neural Information Processing Systems, Denver, Colorado, 505-511, 1999. C. Meek. Causal inference and causal explanation with background knowledge. In Proceedings of the Eleventh Conference on Uncertainty in Artiﬁcial Intelligence, pages 403-410, Montreal, Quebec, 1995. N. Meinshausen and P. B¨ hlmann. High-dimensional graphs and variable selection with the Lasso. u Annals of Statistics, 34: 1436-1462, 2006. K. Murphy. The Bayes net toolbox for Matlab. Computing Science and Statistics, 33:331-350, 2001. M. Narasimhan and J. Bilmes. Optimal Sub-graphical Models. In Advances in Neural Information Processing Systems, vol. 17, pages 961-968, L. Saul and Y. Weiss and L´ on Bottou (Ed.), MIT e Press, Cambridge, 2005. J. Pearl. Causality, Cambridge University Press, Cambridge, 2000. T. Richardson and P. Spirtes. Ancestral graph Markov models. Annals of Statistics, 30:962-1030, 2002. D. Rose, R. Tarjan, and G. Lueker. Algorithmic aspects of vertex elimination on graphs. SIAM Journal on Computing, 5:266-283, 1976. M. Schmidt, A. Niculescu-Mizil, and K. Murphy. Learning graphical model structure using L1Regularization paths. In Proceedings of the 22nd Conference on Artiﬁcial Intelligence, pages 1278-1283, Vancouver, British Columbia, 2007. P. Spirtes and C. Glymour. An algorithm for fast recovery of sparse causal graphs. Social Science Computer Review, 9:62-72, 1991. P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction and Search, MIT Press, Cambridge, 2000. R. E. Tarjan and M. Yannakakis. Simple linear-time algorithm to test chordality of graphs, test acyclicity of hypergraphs, and selective reduce acyclic hypergraphs. SIAM Journal on Computing, 13:566-579, 1984. 482  R ECURSIVE M ETHOD FOR L EARNING DAG S  R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B. 58(1):267-288, 1996. I. Tsamardinos, C.F. Aliferis, and A. Statnikov. Algorithms for large scale Markov blanket discovery. In Proceedings of the 16th International FLAIRS Conference, pages 592-597, 2003. I. Tsamardinos, L. Brown, and C. Aliferis. The max-min hill-climbing Bayesian network structure learning algorithm. Machine Learning, 65(1):31-78, 2006. T. Verma and J. Pearl. Equivalence and synthesis of causal models. In Proceedings of the Sixth Annual Conference on Uncertainty in Artiﬁcial Intelligence, pages 255-268, Cambridge, MA, 1990. M. J. Wainwright, P. Ravikumar, and J.D. Lafferty. High-dimensional graphical model selection using L1-regularized logistic regression. In Proceedings of Twentieth Advances in Neural Information Processing Systems, pages 1465-1472, Vancouver, 2006. J. Whittaker. Graphical Models in Applied Multivariate Statistics, John Wiley & Sons, New York, 1990. S.S. Wilks. The large-sample distribution of the likelihood ratio for testing composite hypotheses. Annals of Mathematical Statistics, 20:595-601, 1938. A. Wille and P. B¨ hlmann. Low-order conditional independence graphs for inferring genetic netu works. Statistical Applications in Genetics and Molecular Biology, 5(1):1-32, 2006. X. Xie, Z. Geng, and Q. Zhao. Decomposition of structural learning about directed acyclic graphs. Artiﬁcial Intelligence, 170:422-439, 2006. P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning Research, 7:2541-2563, 2006.  483</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
