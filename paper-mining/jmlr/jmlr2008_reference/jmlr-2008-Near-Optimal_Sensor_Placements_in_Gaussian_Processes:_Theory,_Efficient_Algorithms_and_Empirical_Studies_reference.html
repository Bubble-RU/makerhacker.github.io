<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>67 jmlr-2008-Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-67" href="../jmlr2008/jmlr-2008-Near-Optimal_Sensor_Placements_in_Gaussian_Processes%3A_Theory%2C_Efficient_Algorithms_and_Empirical_Studies.html">jmlr2008-67</a> <a title="jmlr-2008-67-reference" href="#">jmlr2008-67-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>67 jmlr-2008-Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies</h1>
<br/><p>Source: <a title="jmlr-2008-67-pdf" href="http://jmlr.org/papers/volume9/krause08a/krause08a.pdf">pdf</a></p><p>Author: Andreas Krause, Ajit Singh, Carlos Guestrin</p><p>Abstract: When monitoring spatial phenomena, which can often be modeled as Gaussian processes (GPs), choosing sensor locations is a fundamental task. There are several common strategies to address this task, for example, geometry or disk models, placing sensors at the points of highest entropy (variance) in the GP model, and A-, D-, or E-optimal design. In this paper, we tackle the combinatorial optimization problem of maximizing the mutual information between the chosen locations and the locations which are not selected. We prove that the problem of ﬁnding the conﬁguration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1 − 1/e) of the optimum by exploiting the submodularity of mutual information. We also show how submodularity can be used to obtain online bounds, and design branch and bound search procedures. We then extend our algorithm to exploit lazy evaluations and local structure in the GP, yielding signiﬁcant speedups. We also extend our approach to ﬁnd placements which are robust against node failures and uncertainties in the model. These extensions are again associated with rigorous theoretical approximation guarantees, exploiting the submodularity of the objective function. We demonstrate the advantages of our approach towards optimizing mutual information in a very extensive empirical study on two real-world data sets. Keywords: networks Gaussian processes, experimental design, active learning, spatial learning; sensor</p><br/>
<h2>reference text</h2><p>A. C. Atkinson. Recent developments in the methods of optimum and related experimental designs. International Statistical Review / Revue Internationale de Statistique, 56(2):99–115, Aug. 1988. A. C. Atkinson. The usefulness of optimum experimental designs. Journal of the Royal Statistical Society. Series B (Methodological), 58(1):59–76, 1996. S. Axelrod, S. Fine, R. Gilad-Bachrach, R. Mendelson, and N. Tishby. The information of observations and application for active learning with uncertainty. Technical report, Jerusalem: Leibniz Center, Hebrew University, 2001. X. Bai, S. Kumar, Z. Yun, D. Xuan, and T. H. Lai. Deploying wireless sensors to achieve both coverage and connectivity. In ACM International Symposium on Mobile Ad Hoc Networking and Computing, Florence, Italy, 2006. J. M. Bernardo. Expected information as expected utility. Annals of Statistics, 7(3):686–690, May 1979. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge UP, March 2004. W. F. Caselton and T. Hussain. Hydrologic networks: Information transmission. Journal of Water Resources Planning and Management, WR2:503–520, 1980. W. F. Caselton and J. V. Zidek. Optimal monitoring network designs. Statistics and Probability Letters, 2(4):223–227, 1984. W. F. Caselton, L. Kan, and J. V. Zidek. Statistics in the Environmental and Earth Sciences, chapter Quality data networks that minimize entropy, pages 10–38. Halsted Press, 1992. K. Chaloner and I. Verdinelli. Bayesian experimental design: A review. Statistical Science, 10(3): 273–304, Aug. 1995. ISSN 08834237. D. A. Cohn. Neural network exploration using optimal experiment design. In Jack D. Cowan, Gerald Tesauro, and Joshua Alspector, editors, Advances in Neural Information Processing Systems, volume 6, pages 679–686. Morgan Kaufmann Publishers, Inc., 1994. R. D. Cook and C. J. Nachtsheim. A comparison of algorithms for constructing exact D-optimal designs. Technometrics, 22(3):315–324, Aug. 1980. ISSN 00401706. T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley Interscience, 1991. N. A. C. Cressie. Statistics for Spatial Data. Wiley, 1991. C. Currin, T. Mitchell, M. Morris, and D. Ylvisaker. Bayesian prediction of deterministic functions, with applications to the design and analysis of computer experiments. Journal of the American Statistical Association, 86(416):953–963, 1991. A. Das and D. Kempe. Algorithms for subset selection in linear regression. In Symposium on the Theory of Computing, 2008. 280  N EAR -O PTIMAL S ENSOR P LACEMENTS IN G AUSSIAN P ROCESSES  A. Deshpande, C. Guestrin, S. Madden, J. Hellerstein, and W. Hong. Model-driven data acquisition in sensor networks. In Proceedings of the Thirtieth International Conference on Very Large Data Bases (VLDB), 2004. V. Federov and W. Mueller. Comparison of two approaches in the optimal design of an observation network. Statistics, 20:339–351, 1989. V. V. Fedorov. Theory of Optimal Experiments. Academic Press, 1972. Trans. W. J. Studden and E. M. Klimko. P. Flaherty, M. Jordan, and A. Arkin. Robust design of biological experiments. In Advances in Neural Information Processing Systems (NIPS) 19, 2006. Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by committee algorithm. Machine Learning, 28(2-3):133–168, 1997. G. Golub and C. Van Loan. Matrix Computations. Johns Hopkins, 1989. H. H. Gonzalez-Banos and J. Latombe. A randomized art-gallery algorithm for sensor placement. In Proc. 17th ACM Symposium on Computational Geometry, pages 232–240, 2001. C. Guestrin, A. Krause, and A. Singh. Near-optimal sensor placements in Gaussian processes. In Machine Learning, Proceedings of the Twenty-Second International Conference (ICML), 2005. P. Guttorp, N. D. Le, P. D. Sampson, and J. V. Zidek. Using entropy in the redesign of an environmental monitoring network. Technical report, Department of Statistics. University of British Columbia., 1992. Tech. Rep. 116. T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of Statistical Learning. Springer, 2003. N. Heo and P. K. Varshney. Energy-efﬁcient deployment of intelligent mobile sensor networks. IEEE Transactions on Systems, Man and Cybernetics, Part A, 35(1):78–92, 2005. D. S. Hochbaum and W. Maas. Approximation schemes for covering and packing problems in image processing and VLSI. Journal of the ACM, 32:130–136, 1985. H. Hotelling. Relations between two sets of variates. Biometrika, 28(3/4):321–377, 1936. A. Howard, M. Mataric, and G. Sukhatme. Mobile sensor network deployment using potential ﬁelds: A distributed, scalable solution to the area coverage problem, 2002. R. Kershner. The number of circles covering a set. American Journal of Mathematics, 61:665–671, 1939. C. Ko, J. Lee, and M. Queyranne. An exact algorithm for maximum entropy sampling. Operations Research, 43(4):684–691, 1995. A. Krause and C. Guestrin. A note on the budgeted maximization of submodular functions. Technical report, CMU-CALD-05-103, 2005. A. Krause and C. Guestrin. Nonmyopic active learning of gaussian processes: An exploration– exploitation approach. In International Conference on Machine Learning, 2007. 281  K RAUSE , S INGH AND G UESTRIN  A. Krause, C. Guestrin, A. Gupta, and J. Kleinberg. Near-optimal sensor placements: Maximizing information while minimizing communication cost. In Proceedings of the Fifth International Symposium on Information Processing in Sensor Networks (IPSN), 2006. N. Lawrence, M. Seeger, and R. Herbrich. Fast sparse gaussian process methods: The informative vector machine. In Advances in Neural Information Processing Systems (NIPS) 16, 2003. U. Lerner and R. Parr. Inference in hybrid networks: Theoretical limits and practical algorithms. In Proceedings of the 17th Conference in Uncertainty in Artiﬁcial Intelligence (UAI), 2001. D. V. Lindley. On a measure of the information provided by an experiment. Annals of Mathematical Statistics, 27:986–1005, 1956. D. V. Lindley and A. F. M. Smith. Bayes’ estimates for the linear model. Journal of the Royal Statistical Society, Ser. B, 34:1–18, 1972. S. P. Luttrell. The use of transinformation in the design of data sampling schemes for inverse problems. Inverse Problems, 1:199–218, 1985. D. MacKay. Information-based objective functions for active data selection. Neural Computation, 4(4):590–604, 1992. D. J. C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge UP, 2003. G. P. McCabe. Principal variables. Technometrics, 26(2):137–144, 1984. M. D. McKay, R. J. Beckman, and W. J. Conover. A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. Technometrics, 21(2): 239–245, May 1979. R. K. Meyer and C. J. Nachtsheim. Constructing exact D-optimal experimental designs by simulated annealing. American Journal of Mathematical and Management Sciences, 8(3-4):329–359, 1988. T. J. Mitchell. An algorithm for the construction of ”D-optimal” experimental designs. Technometrics, 16(2):203–210, May 1974a. ISSN 00401706. T.J. Mitchell. Computer construction of ”D-optimal” ﬁrst-order designs. Technometrics, 16(2): 211–220, May 1974b. ISSN 00401706. Avidan Moghaddam, Weiss. Fast pixel/part selection with sparse eigenvectors. In International Conference on Computer Vision, 2007. B. Moghaddam, Y. Weiss, and S. Avidan. Spectral bounds for sparse PCA: Exact and greedy algorithms. In Advances in Neural Information Processing Systems (NIPS) 18, 2005. B. Moghaddam, Y. Weiss, and S. Avidan. Generalized spectral bounds for sparse LDA. In Machine Learning, Proceedings of the Twenty-Third International Conference (ICML), 2006. M. Narasimhan and J. Bilmes. A submodular-supermodular procedure with applications to discriminative structure learning. In Advances in Neural Information Processing Systems (NIPS) 19, 2006. 282  N EAR -O PTIMAL S ENSOR P LACEMENTS IN G AUSSIAN P ROCESSES  G. Nemhauser, L. Wolsey, and M. Fisher. An analysis of the approximations for maximizing submodular set functions. Mathematical Programming, 14:265–294, 1978. G. L. Nemhauser and L. A. Wolsey. Studies on Graphs and Discrete Programming, chapter Maximizing submodular set functions: Formulations and analysis of algorithms, pages 279–301. North-Holland, 1981. N-.K Nguyen and A. J. Miller. A review of some exchange algorithms for constructing discrete D-optimal designs. Computational Statistics and Data Analysis, 14:489–498, 1992. D. J. Nott and W. T. M. Dunsmuir. Estimation of nonstationary spatial covariance structure. Biometrika, 89:819–829, 2002. A. O’Hagan. Curve ﬁtting and optimal design for prediction (with discussion). Journal of the Royal Statistical Society, Ser. B, 40:1–42, 1978. C. J. Paciorek. Nonstationary Gaussian Processes for Regression and Spatial Modelling. PhD thesis, Carnegie Mellon University, Pittsburgh, PA, May 2003. L. Paninski. Asymptotic theory of information-theoretic experimental design. Neural Computation, 17(7):1480–1507, 2005. F. Pukelsheim. Information increasing orderings in experimental design theory. International Statistical Review / Revue Internationale de Statistique, 55(2):203–219, Aug. 1987. ISSN 03067734. N. Ramakrishnan, C. Bailey-Kellogg, S. Tadepalli, and V. N. Pandey. Gaussian processes for active data mining of spatial aggregates. In SIAM Data Mining, 2005. C. E. Rasmussen and C. K. I. Williams. Gaussian Process for Machine Learning. Adaptive Computation and Machine Learning. MIT Press, 2006. P. J. Ribeiro Jr. and P. J. Diggle. geoR: A package for geostatistical analysis. R-NEWS Vol 1, No 2. ISSN 1609-3631, 2001. T. G. Robertazzi and S. C. Schwartz. An accelerated sequential algorithm for producing D-optimal designs. SIAM Journal of Scientiﬁc and Statistical Computing, 10(2):341–358, March 1989. J. Sacks, S. B. Schiller, and W. J. Welch. Designs for computer experiments. Technometrics, 31(1): 41–47, Feb. 1989. ISSN 00401706. P. Sebastiani and H. P. Wynn. Maximum entropy sampling and optimal bayesian experimental design. Journal of the Royal Statistical Society, Series B, 62(1):145–157, 2000. M. Seeger. Gaussian processes for machine learning. International Journal of Neural Systems, 14 (2):69–106, 2004. M. Seeger, C. K. I. Williams, and N. D. Lawrence. Fast forward selection to speed up sparse gaussian process regression. In Proceedings of the Ninth International Workshop on Artiﬁcial Intelligence and Statistics (AISTATS), 2003. 283  K RAUSE , S INGH AND G UESTRIN  S. Seo, M. Wallat, T. Graepel, and K. Obermayer. Gaussian process regression: Active data selection and test point rejection. In Proceedings of the International Joint Conference on Neural Networks (IJCNN), volume 3, pages 241–246, 2000. M. C. Shewry and H. P. Wynn. Maximum entropy sampling. Journal of Applied Statistics, 14: 165–170, 1987. A. Singh, A. Krause, C. Guestrin, W. Kaiser, and M. Batalin. Efﬁcient planning of informative paths for multiple robots. In IJCAI, 2007. E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems (NIPS) 18, 2005. P. Sollich. Learning from minimum entropy queries in a large committee machine. Physical Review E, 53:R2060–R2063, 1996. A. J. Storkey. Truncated covariance matrices and Toeplitz methods in Gaussian processes. In Artiﬁcial Neural Networks - ICANN 1999, 1999. J. F. Sturm. Using sedumi 1.02, a matlab toolbox for optimization over symmetric cones. Optimization Methods and Software, special issue on interior-point methods, 11(12):625–653, 1999. M. Sviridenko. A note on maximizing a submodular set function subject to knapsack constraint. Operations Research Letters, 32:41–43, 2004. S. Toumpis and G. A. Gupta. Optimal placement of nodes in large sensor networks under a general physical layer model. In Proc. IEEE Communications Society Conference on Sensor and Ad Hoc Communications (SECON), 2005. W. J. Welch. Branch-and-bound search for experimental design based on D-optimality and other criteria. Technometrics, 24(1):41–48, 1982. M. Widmann and C. S. Bretherton. 50 km resolution daily precipitation for the paciﬁc northwest. http://www.jisao.washington.edu/data sets/widmann/, May 1999. S. Wu and J. V. Zidek. An entropy based review of selected NADP/NTN network sites for 1983–86. Atmospheric Environment, 26A:2089–2103, 1992. D. Ylvisaker. A Survey of Statistical Design and Linear Models, chapter Design on random ﬁelds. North-Holland, 1975. K. Yu, J. Bi, and V. Tresp. Active learning via transductive experimental design. In Machine Learning, Proceedings of the Twenty-Third International Conference (ICML), 2006. Z. Zhu and M. L. Stein. Spatial sampling design for prediction with estimated parameters. Journal of Agricultural, Biological and Environmental Statistics, 11:24–49, 2006. J. V. Zidek, W. Sun, and N. D. Le. Designing and integrating composite networks for monitoring multivariate gaussian pollution ﬁelds. Applied Statistics, 49:63–79, 2000. D. L. Zimmerman. Optimal network design for spatial prediction, covariance parameter estimation, and empirical prediction. Environmetrics, 17(6):635–652, 2006. 284</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
