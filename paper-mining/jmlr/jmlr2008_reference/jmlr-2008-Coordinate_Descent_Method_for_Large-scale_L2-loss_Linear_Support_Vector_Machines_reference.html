<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-28" href="../jmlr2008/jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">jmlr2008-28</a> <a title="jmlr-2008-28-reference" href="#">jmlr2008-28-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</h1>
<br/><p>Source: <a title="jmlr-2008-28-pdf" href="http://jmlr.org/papers/volume9/chang08a/chang08a.pdf">pdf</a></p><p>Author: Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classiﬁcation and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while ﬁxing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efﬁcient and stable than state of the art methods such as Pegasos and TRON. Keywords: linear support vector machines, document classiﬁcation, coordinate descent</p><br/>
<h2>reference text</h2><p>Dimitri P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, Belmont, MA 02178-9998, second edition, 1999. Bernhard E. Boser, Isabelle Guyon, and Vladimir Vapnik. A training algorithm for optimal margin classiﬁers. In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, pages 144–152. ACM Press, 1992. 1396  C OORDINATE D ESCENT M ETHOD FOR L ARGE - SCALE L2- LOSS L INEAR SVM  Leon Bottou. Stochastic gradient descent examples, 2007. http://leon.bottou.org/projects/ sgd. Leon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20. MIT Press, Cambridge, MA, 2008. Iain S. Duff, Roger G. Grimes, and John G. Lewis. Sparse matrix test problems. ACM Transactions on Mathematical Software, 15:1–14, 1989. Luigi Grippo and Marco Sciandrone. Globally convergent block-coordinate techniques for unconstrained optimization. Optimization Methods and Software, 10:587–637, 1999. Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and Sellamanickam Sundararajan. A dual coordinate descent method for large-scale linear SVM. In Proceedings of the Twenty Fifth International Conference on Machine Learning (ICML), 2008. URL http://www.csie.ntu. edu.tw/˜cjlin/papers/cddual.pdf. Software available at http://www.csie.ntu.edu.tw/ ˜cjlin/liblinear. Thorsten Joachims. Training linear SVMs in linear time. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD). ACM, 2006. S. Sathiya Keerthi and Dennis DeCoste. A modiﬁed ﬁnite Newton method for fast solution of large scale linear SVMs. Journal of Machine Learning Research, 6:341–361, 2005. David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. RCV1: A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361–397, 2004. Chih-Jen Lin, Ruby C. Weng, and S. Sathiya Keerthi. Trust region Newton method for largescale logistic regression. Journal of Machine Learning Research, 9:627–650. URL http:// www.csie.ntu.edu.tw/˜cjlin/papers/logistic.pdf. Software available at http://www. csie.ntu.edu.tw/˜cjlin/liblinear. Zhi-Quan Luo and Paul Tseng. On the convergence of coordinate descent method for convex differentiable minimization. Journal of Optimization Theory and Applications, 72(1):7–35, 1992. Olvi L. Mangasarian. A ﬁnite Newton method for classiﬁcation. Optimization Methods and Software, 17(5):913–929, 2002. Duk´k Miroslav, Steven J. Phillips, and Robert E. Schapire. Performance guarantees for reguları ized maximum entropy density estimation. In Proceedings of the 17th Annual Conference on Computational Learning Theory, pages 655–662, New York, 2004. ACM press. Gunnar R¨ tsch, Sebastian Mika, and Manfred K. Warmuth. On the convergence of leveraging. a In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 487–494. MIT Press, Cambridge, MA, 2002. Nicol N. Schraudolph. A fast, compact approximation of the exponential function. Neural Computation, 11:853–862, 1999. 1397  C HANG , H SIEH AND L IN  Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: primal estimated sub-gradient solver for SVM. In Proceedings of the 24th International Conference on Machine Learning (ICML), 2007. Alex J. Smola, S V N Vishwanathan, and Quoc Le. Bundle methods for machine learning. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20. MIT Press, Cambridge, MA, 2008. Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In Proceedings of the 21th International Conference on Machine Learning (ICML), 2004. Tong Zhang and Frank J. Oles. Text categorization based on regularized linear classiﬁcation methods. Information Retrieval, 4(1):5–31, 2001.  1398</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
