<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-89" href="../jmlr2008/jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">jmlr2008-89</a> <a title="jmlr-2008-89-reference" href="#">jmlr2008-89-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</h1>
<br/><p>Source: <a title="jmlr-2008-89-pdf" href="http://jmlr.org/papers/volume9/lin08a/lin08a.pdf">pdf</a></p><p>Author: Hsuan-Tien Lin, Ling Li</p><p>Abstract: Ensemble learning algorithms such as boosting can achieve better performance by averaging over the predictions of some base hypotheses. Nevertheless, most existing algorithms are limited to combining only a ﬁnite number of hypotheses, and the generated ensemble is usually sparse. Thus, it is not clear whether we should construct an ensemble classiﬁer with a larger or even an inﬁnite number of hypotheses. In addition, constructing an inﬁnite ensemble itself is a challenging task. In this paper, we formulate an inﬁnite ensemble learning framework based on the support vector machine (SVM). The framework can output an inﬁnite and nonsparse ensemble through embedding inﬁnitely many hypotheses into an SVM kernel. We use the framework to derive two novel kernels, the stump kernel and the perceptron kernel. The stump kernel embodies inﬁnitely many decision stumps, and the perceptron kernel embodies inﬁnitely many perceptrons. We also show that the Laplacian radial basis function kernel embodies inﬁnitely many decision trees, and can thus be explained through inﬁnite ensemble learning. Experimental results show that SVM with these kernels is superior to boosting with the same base hypothesis set. In addition, SVM with the stump kernel or the perceptron kernel performs similarly to SVM with the Gaussian radial basis function kernel, but enjoys the beneﬁt of faster parameter selection. These properties make the novel kernels favorable choices in practice. Keywords: ensemble learning, boosting, support vector machine, kernel</p><br/>
<h2>reference text</h2><p>Annalisa Barla, Francesca Odone, and Alessandro Verri. Histogram intersection kernel for image classiﬁcation. In Proceedings of the 2003 Conference on Image Processing, volume 3, pages 513–516, 2003. Brad J. C. Baxter. Conditionally positive functions and p-norm distance matrices. Constructive Approximation, 7(1):427–440, 1991. Christian Berg, Jens P. R. Christensen, and Paul Ressel. Harmonic Analysis on Semigroups: Theory of Positive Deﬁnite and Related Functions. Springer-Verlag, 1984. Sabri Boughorbel, Jean-Philippe Tarel, and Nozha Boujemaa. Generalized histogram intersection kernel for image recognition. In Proceedings of the 2005 Conference on Image Processing, volume 3, pages 161–164, 2005. Leo Breiman. Prediction games and arcing algorithms. Neural Computation, 11(7):1493–1517, 1999. 309  L IN AND L I  Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines, 2001a. Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm. Chih-Chung Chang and Chih-Jen Lin. Training ν-support vector classiﬁers: Theory and algorithms. Neural Computation, 13(9):2119–2147, 2001b. Olivier Chapelle, Patrick Haffner, and Vladimir N. Vapnik. Support vector machines for histogrambased image classiﬁcation. IEEE Transactions on Neural Networks, 10(5):1055–1064, 1999. Ayhan Demiriz, Kristin P. Bennett, and John Shawe-Taylor. Linear programming boosting via column generation. Machine Learning, 46(1–3):225–254, 2002. Thomas G. Dietterich. An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization. Machine Learning, 40(2):139–157, 2000. Francois Fleuret and Hichem Sahbi. Scale-invariance of support vector machines based on the ¸ triangular kernel. In Proceedings of the Third International Workshop on Statistical and Computational Theories of Vision, 2003. Yoav Freund and Robert E. Schapire. Experiments with a new boosting algorithm. In Machine Learning: Proceedings of the Thirteenth International Conference, pages 148–156, 1996. Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997. Yoav Freund and Robert E. Schapire. A short introduction to boosting. Journal of Japanese Society for Artiﬁcial Intelligence, 14(5):771–780, 1999. Thore Graepel, Ralf Herbrich, and John Shawe-Taylor. PAC-Bayesian compression bounds on the prediction error of learning algorithms for classiﬁcation. Machine Learning, 59(1–2):55–76, 2005. Kristen Grauman and Trevor Darrell. The pyramid match kernel: Discriminative classiﬁcation with sets of image features. In Proceedings of the Tenth IEEE International Conference on Computer Vision, pages 1458–1465, 2005. Trevor Hastie and Robert Tibshirani. Generalized Additive Models. Chapman and Hall, 1990. Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer-Verlag, 2001. Simon Haykin. Neural Networks: A Comprehensive Foundation. Prentice Hall, second edition, 1999. Seth Hettich, Catherine L. Blake, and Christopher J. Merz. UCI repository of machine learning databases, 1998. Downloadable at http://www.ics.uci.edu/ ˜mlearn/MLRepository.html. Robert C. Holte. Very simple classiﬁcation rules perform well on most commonly used datasets. Machine Learning, 11(1):63–91, 1993. 310  S UPPORT V ECTOR M ACHINERY FOR I NFINITE E NSEMBLE L EARNING  Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin. A practical guide to support vector classiﬁcation. Technical report, National Taiwan University, 2003. S. Sathiya Keerthi and Chih-Jen Lin. Asymptotic behaviors of support vector machines with Gaussian kernel. Neural Computation, 15(7):1667–1689, 2003. Ling Li and Hsuan-Tien Lin. Optimizing 0/1 loss for perceptrons by random coordinate descent. In Proceedings of the 2007 International Joint Conference on Neural Networks, pages 749–754, 2007. Hsuan-Tien Lin. Inﬁnite ensemble learning with support vector machines. Master’s thesis, California Institute of Technology, 2005. Hsuan-Tien Lin and Ling Li. Novel distance-based SVM kernels for inﬁnite ensemble learning. In Proceedings of the 12th International Conference on Neural Information Processing, pages 761–766, 2005a. Hsuan-Tien Lin and Ling Li. Analysis of SAGE results with combined learning techniques. In P. Berka and B. Cr´ milleux, editors, Proceedings of the ECML/PKDD 2005 Discovery Challenge, e pages 102–113, 2005b. Ron Meir and Gunnar R¨ tsch. An introduction to boosting and leveraging. In S. Mendelson and a A. J. Smola, editors, Advanced Lectures on Machine Learning, pages 118–183. Springer-Verlag, 2003. Charles A. Micchelli. Interpolation of scattered data: Distance matrices and conditionally positive deﬁnite functions. Constructive Approximation, 2(1):11–22, 1986. John C. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Sch¨ lkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods: Support Vector o Learning, pages 185–208. MIT Press, 1999. J. Ross Quinlan. Induction of decision trees. Machine Learning, 1(1):81–106, 1986. Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006. ¨ Gunnar R¨ tsch, Takashi Onoda, and Klaus-Robert Muller. Soft margins for AdaBoost. Machine a Learning, 42(3):287–320, 2001. Gunnar R¨ tsch, Ayhan Demiriz, and Kristin P. Bennett. Sparse regression ensembles in inﬁnite and a ﬁnite hypothesis spaces. Machine Learning, 48(1–3):189–218, 2002. ¨ Gunnar R¨ tsch, Sebastian Mika, Bernhard Scholkopf, and Klaus-Robert M¨ ller. Constructing a u boosting algorithms from SVMs: an application to one-class classiﬁcation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(9):1184–1199, 2002. Michael Reed and Barry Simon. Functional Analysis. Academic Press, revised and enlarged edition, 1980. 311  L IN AND L I  Saharon Rosset, Ji Zhu, and Trevor Hastie. Boosting as a regularized path to a maximum margin classiﬁer. Journal of Machine Learning Research, 5:941–973, 2004. Saharon Rosset, Grzegorz Swirszcz, Nathan Srebro, and Ji Zhu. 1 regularization in inﬁnite dimensional feature spaces. In N. H. Bshouty and C. Gentile, editors, Learning Theory: 20th Annual Conference on Learning Theory, volume 4539 of Lecture Notes in Computer Science, pages 544– 558. Springer-Verlag, 2007. Bernhard Sch¨ lkopf and Alex J. Smola. Learning with Kernels. MIT Press, 2002. o Vladimir N. Vapnik. Statistical Learning Theory. John Wiley & Sons, 1998. Christopher K. I. Williams. Computation with inﬁnite neural networks. Neural Computation, 10 (5):1203–1216, 1998.  312</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
