<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>33 jmlr-2008-Evidence Contrary to the Statistical View of Boosting</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-33" href="../jmlr2008/jmlr-2008-Evidence_Contrary_to_the_Statistical_View_of_Boosting.html">jmlr2008-33</a> <a title="jmlr-2008-33-reference" href="#">jmlr2008-33-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>33 jmlr-2008-Evidence Contrary to the Statistical View of Boosting</h1>
<br/><p>Source: <a title="jmlr-2008-33-pdf" href="http://jmlr.org/papers/volume9/mease08a/mease08a.pdf">pdf</a></p><p>Author: David Mease, Abraham Wyner</p><p>Abstract: The statistical perspective on boosting algorithms focuses on optimization, drawing parallels with maximum likelihood estimation for logistic regression. In this paper we present empirical evidence that raises questions about this view. Although the statistical perspective provides a theoretical framework within which it is possible to derive theorems and create new algorithms in general contexts, we show that there remain many unanswered important questions. Furthermore, we provide examples that reveal crucial ﬂaws in the many practical suggestions and new methods that are derived from the statistical view. We perform carefully designed experiments using simple simulation models to illustrate some of these ﬂaws and their practical consequences. Keywords: boosting algorithms, LogitBoost, AdaBoost</p><br/>
<h2>reference text</h2><p>L. Breiman. Discussion of additive logistic regression: A statistical view of boosting. Annals of Statistics, 28:374–377, 2000. L. Breiman. Bagging predictors. Machine Learning, 24:123–140, 1996. L. Breiman. Random forests. Machine Learning, 45:5–32, 2001. P. Buhlmann and B. Yu. Boosting with the L2 loss: Regression and classiﬁcation. Journal of the American Statistical Association, 98:324–339, 2003. A. Buja. Discussion of additive logistic regression: A statistical view of boosting. Annals of Statistics, 28:387–391, 2000. A. Buja, W. Stuetzle, and Y. Shen. Loss functions for binary class probability estimation and classiﬁcation: Structure and applications. 2006. M. Collins, R. E. Schapire, and Y. Singer. Logistic regression, adaboost and bregman distances. In Computational Learing Theory, pages 158–169, 2000. A. Cutler and G. Zhao. Pert: Perfect random tree ensembles. Computing Science and Statistics, 33: 490–497, 2001. M. Dettling and P. Buhlmann. Boosting for tumor classiﬁcation with gene expression data. Bioinformatics, 19:1061–1069, 2003. Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm. In Machine Learning: Proceedings of the Thirteenth International Conference, pages 148–156, 1996. Y. Freund and R. E. Schapire. Discussion of additive logistic regression: A statistical view of boosting. Annals of Statistics, 28:391–393, 2000. J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: A statistical view of boosting. Annals of Statistics, 28:337–374, 2000a. J. Friedman, T. Hastie, and R. Tibshirani. Rejoiner for additive logistic regression: A statistical view of boosting. Annals of Statistics, 28:400–407, 2000b. 155  M EASE AND W YNER  A. J. Grove and D. Schuurmans. Boosting in the limit: Maximizing the margin of learned ensembles. In Proceedings of the Fifteenth National Conference on Artiﬁcial Intelligence (AAAI-98), pages 692–699, 1998. T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer, 2001. W. Jiang. Does boosting overﬁt: Views from an exact solution. Technical Report 00-03, Department of Statistics, Northwestern University, 2000. W. Jiang. Is regularization unnecessary for boosting? In Proceedings of the Eighth International Workshop on Artiﬁcial Intelligence and Statistics, pages 57–64, 2001. W. Jiang. On weak base hypotheses and their implications for boosting regression and classiﬁcation. Annals of Statistics, 30:51–73, 2002. A. Krieger, C. Long, and A. J. Wyner. Boosting noisy data. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 274–281, 2001. G. Lugosi and N. Vayatis. On the bayes-risk consistency of regularized boosting methods. Annals of Statistics, 32:30–55, 2004. D. Mease, A. Wyner, and A. Buja. Boosted classiﬁcation trees and class probability/quantile estimation. Journal of Machine Learning Research, 8:409–439, 2007. G. Ridgeway. Discussion of additive logistic regression: A statistical view of boosting. Annals of Statistics, 28:393–400, 2000. G. Ridgeway. Generalized boosted models: A guide to the gbm package. 2005. R. E. Schapire, Y. Freund, P. L. Bartlett, and W. S. Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. Annals of Statistics, 26:1651–1686, 1998. T. Zhang and B. Yu. Boosting with early stopping: Convergence and consistency. Annals of Statistics, 33:1538–1579, 2005.  156</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
