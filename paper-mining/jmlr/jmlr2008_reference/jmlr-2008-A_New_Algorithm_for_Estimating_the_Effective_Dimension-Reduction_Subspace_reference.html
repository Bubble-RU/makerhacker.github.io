<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-5" href="../jmlr2008/jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">jmlr2008-5</a> <a title="jmlr-2008-5-reference" href="#">jmlr2008-5-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</h1>
<br/><p>Source: <a title="jmlr-2008-5-pdf" href="http://jmlr.org/papers/volume9/dalalyan08a/dalalyan08a.pdf">pdf</a></p><p>Author: Arnak S. Dalalyan, Anatoly Juditsky, Vladimir Spokoiny</p><p>Abstract: The statistical problem of estimating the effective dimension-reduction (EDR) subspace in the multi-index regression model with deterministic design and additive noise is considered. A new procedure for recovering the directions of the EDR subspace is proposed. Many methods for estimating the EDR subspace perform principal component analysis on a family of vectors, say ˆ ˆ β1 , . . . , βL , nearly lying in the EDR subspace. This is in particular the case for the structure-adaptive approach proposed by Hristache et al. (2001a). In the present work, we propose to estimate the projector onto the EDR subspace by the solution to the optimization problem minimize ˆ ˆ max β (I − A)β =1,...,L subject to A ∈ Am∗ , where Am∗ is the set of all symmetric matrices with eigenvalues in [0, 1] and trace less than or equal √ to m∗ , with m∗ being the true structural dimension. Under mild assumptions, n-consistency of the proposed procedure is proved (up to a logarithmic factor) in the case when the structural dimension is not larger than 4. Moreover, the stochastic error of the estimator of the projector onto the EDR subspace is shown to depend on L logarithmically. This enables us to use a large number of vectors ˆ β for estimating the EDR subspace. The empirical behavior of the algorithm is studied through numerical simulations. Keywords: dimension-reduction, multi-index regression model, structure-adaptive approach, central subspace</p><br/>
<h2>reference text</h2><p>P. Bickel, C. Klaassen, Y. Ritov, and J. Wellner. Efﬁcient and Adaptive Estimation for Semiparametric Models. Princeton University Press, Springer, New York, 1998. E. Bura. Using linear smoothers to assess the structural dimension of regressions. Statistica Sinica, 13(1):143–162, 2003. E. Bura and R. D. Cook. Estimating the structural dimension of regressions via parametric inverse regression. J. R. Stat. Soc. Ser. B Stat. Methodol., 63(2):393–410, 2001a. E. Bura and R. D. Cook. Extending sliced inverse regression: The weighted chi-squared test. J. Amer. Statist. Assoc., 96(455):996–1003, 2001b. K. S. Chan, M. C. Li, and H. Tong. Partially linear reduced-rank regression. Technical report, available at www.stat.uiowa.edu/techrep/tr328.pdf, 2004. 1676  E STIMATION OF THE D IMENSION -R EDUCTION S UBSPACE  R. D. Cook. Regression graphics. Ideas for studying regressions through graphics. Wiley Series in Probability and Statistics: Probability and Statistics, John Wiley & Sons, Inc., New York, 1998. R. D. Cook and B. Li. Dimension reduction for conditional mean in regression. Ann. Statist., 30(2): 455–474, 2002. R. D. Cook and B. Li. Determining the dimension of iterative hessian transformation. Ann. Statist., 32(6):2501–2531, 2004. R. D. Cook and L. Ni. Sufﬁcient dimension reduction via inverse regression: a minimum discrepancy approach. J. Amer. Statist. Assoc., 100(470):410–428, 2005. R. D. Cook and L. Ni. Using intraslice covariances for improved estimation of the central subspace in regression. Biometrika, 93(1):65–74, 2006. R. D. Cook and S. Weisberg. Applied Regression Including Computing and Graphics. Hoboken NJ: John Wiley, 1999. R. D. Cook and S. Weisberg. Discussion of “sliced inverse regression for dimension reduction” by K. C. Li. J. Amer. Statist. Assoc., 86(414):328–332, 1991. M. Delecroix, M. Hristache, and V. Patilea. On semiparametric m-estimation in single-index regression. J. Statist. Plann. Inference, 136(3):730–769, 2006. J. Fan and I. Gijbels. Local polynomial modelling and its applications. Monographs on Statistics and Applied Probability, 66, Chapman & Hall, London, 1996. M. Hristache, A. Juditsky, J. Polzehl, and V. Spokoiny. Structure adaptive approach for dimension reduction. Ann. Statist., 29(6):1537–1566, 2001a. M. Hristache, A. Juditsky, and V Spokoiny. Direct estimation of the index coefﬁcient in a singleindex model. Ann. Statist., 29(3):595–623, 2001b. K.C. Li. On principal hessian directions for data visualization and dimension reduction: another application of stein’s lemma. J. Amer. Statist. Assoc., 87(420):1025–1039, 1992. K.C. Li. Sliced inverse regression for dimension reduction. with discussion and a rejoinder by the author. J. Amer. Statist. Assoc., 86(414):316–342, 1991. K.C. Li and N. Duan. Regression analysis under link violation. Ann. Statist., 17(3):1009–1052, 1989. L. Li. Sparse sufﬁcient dimension reduction. Biometrika, 94(3):603–613, 2007. L. Ni, R. D. Cook, and C.-L. Tsai. A note on shrinkage sliced inverse regression. Biometrika, 92 (1):242–247, 2005. A. Samarov, V. Spokoiny, and C. Vial. Component identiﬁcation and estimation in nonlinear highdimensional regression models by structural adaptation. J. Amer. Statist. Assoc., 100(470):429– 445, 2005. 1677  DALALYAN , J UDITSKY AND S POKOINY  H. Wang, L. Ni, and C.-L. Tsai. Improving dimension reduction via contour- projection. Statistica Sinica, 18:299–311, 2008. Y. Xia. A constructive approach to the estimation of dimension reduction directions. Ann. Statist., 35(6):2654–2690, 2007. Y. Xia, H. Tong, W. K. Li, and L. X. Zhu. An adaptive estimation of dimension reduction space. J. R. Stat. Soc. Ser. B Stat. Methodol., 64(3):363–410, 2002. X. Yin and R. D. Cook. Direction estimation in single-index regressions. Biometrika, 92(2):371– 384, 2005. X. Yin and R. D. Cook. Dimension reduction via marginal high moments in regression. Statist. Probab. Lett., 76(4):393–400, 2006.  1678</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
