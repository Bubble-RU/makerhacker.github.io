<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-63" href="../jmlr2008/jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">jmlr2008-63</a> <a title="jmlr-2008-63-reference" href="#">jmlr2008-63-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</h1>
<br/><p>Source: <a title="jmlr-2008-63-pdf" href="http://jmlr.org/papers/volume9/koo08b/koo08b.pdf">pdf</a></p><p>Author: Imhoi Koo, Rhee Man Kil</p><p>Abstract: This paper presents a new method of model selection for regression problems using the modulus of continuity. For this purpose, we suggest the prediction risk bounds of regression models using the modulus of continuity which can be interpreted as the complexity of functions. We also present the model selection criterion referred to as the modulus of continuity information criterion (MCIC) which is derived from the suggested prediction risk bounds. The suggested MCIC provides a risk estimate using the modulus of continuity for a trained regression model (or an estimation function) while other model selection criteria such as the AIC and BIC use structural information such as the number of training parameters. As a result, the suggested MCIC is able to discriminate the performances of trained regression models, even with the same structure of training models. To show the effectiveness of the proposed method, the simulation for function approximation using the multilayer perceptrons (MLPs) was conducted. Through the simulation for function approximation, it was demonstrated that the suggested MCIC provides a good selection tool for nonlinear regression models, even with the limited size of data. Keywords: regression models, multilayer perceptrons, model selection, information criteria, modulus of continuity</p><br/>
<h2>reference text</h2><p>H. Akaike. Information theory and an extension of the maximum likelihood principle. In Proceedings of the Second International Symposium on Information Theory, pages 267–281, 1973. G. Anastassiou and S. Gal. Approximation Theory: Moduli of Continuity and Global Smoothness Preservation. Birkh¨ user, Boston, 2000. a A. Barron, J. Rissanen, and B. Yu. The minimum description length principle in coding and modeling. IEEE Transactions on Information Theory, 44:2743–2760, 1998. C. Chang and C. Lin. LIBSVM: a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm. O. Chapelle, V. Vapnik, and Y. Bengio. Model selection for small sample regression. Machine Learning, 48:315-333, 2002. V. Cherkassky and Y. Ma. Comparison of model selection for regression. Neural Computation, 15:1691–1714, 2003. V. Cherkassky, X. Shao, F. Mulier, and V. Vapnik. Model complexity control for regression using VC generalization bounds. IEEE Transactions on Neural Networks, 10:1075–1089, 1999. S. Cohen and N. Intrator. On different model selection criteria in a forward and backward regression hybrid network. International Journal of Pattern Recognition and Artiﬁcial Intelligence, 18:847– 865, 2004. D. Donoho and I. Johnstone. Adapting to unknown smoothness via wavelet shrinkage. Journal of the American Statistical Association, 90:1200–1224, 1995. D. Foster and E. George. The risk inﬂation criterion for multiple regression. Annals of Statistics, 22:1947–1975, 1994. T. Hastie, R. Tibshirani, and J. Friedman. Note on “comparison of model selection for regression” by V. Cherkassky and Y. Ma. Neural Computation, 15:1477–1480, 2003. 2632  M ODEL S ELECTION FOR R EGRESSION  G. Hinton and D. van Camp. Keeping neural networks simple by minimizing the description length of the weights. In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, pages 5–13, 1993. W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:13-30, 1963. M. Karpinski and A. Macintyre. Polynomial bounds for VC dimension of sigmoidal neural networks. In Proceedings of the Twenty-Seventh Annual ACM Symposium on Theory of Computing, pages 200–208, 1995. I. Koo and R. Kil. Nonlinear model selection based on the modulus of continuity. In Proceedings of World Congress on Computational Intelligence, pages 3552–3559, 2006. G. Lorentz. Approximation of Functions. Chelsea Publishing Company, New York, 1986. J. Rissanen. Stochastic complexity and modeling. Annals of Statistics, 14:1080–1100, 1986. A. Sakurai. Polynomial bounds for the VC dimension of sigmoidal, radial basis function, and sigmapi networks. In Proceedings of the World Congress on Neural Networks, pages 58–63, 1995. G. Schwartz. Estimating the dimension of a model. Annals of Statistics, 6:461–464, 1978. A. Timan. Theory of Approximation of Functions of a Real Variable. English translation 1963, Pergaman Press, Russian original published in Moscow by Fizmatgiz in 1960. V. Vapnik. Statistical Learning Theory. J. Wiley, 1998. G. Wahba, G.Golub, and M. Heath. Generalized cross-validation as a method for choosing a good ridge parameter. Technometrics, 21:215–223, 1979.  2633</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
