<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-66" href="../jmlr2008/jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">jmlr2008-66</a> <a title="jmlr-2008-66-reference" href="#">jmlr2008-66-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</h1>
<br/><p>Source: <a title="jmlr-2008-66-pdf" href="http://jmlr.org/papers/volume9/ye08b/ye08b.pdf">pdf</a></p><p>Author: Jieping Ye, Shuiwang Ji, Jianhui Chen</p><p>Abstract: Regularized kernel discriminant analysis (RKDA) performs linear discriminant analysis in the feature space via the kernel trick. Its performance depends on the selection of kernels. In this paper, we consider the problem of multiple kernel learning (MKL) for RKDA, in which the optimal kernel matrix is obtained as a linear combination of pre-speciﬁed kernel matrices. We show that the kernel learning problem in RKDA can be formulated as convex programs. First, we show that this problem can be formulated as a semideﬁnite program (SDP). Based on the equivalence relationship between RKDA and least square problems in the binary-class case, we propose a convex quadratically constrained quadratic programming (QCQP) formulation for kernel learning in RKDA. A semi-inﬁnite linear programming (SILP) formulation is derived to further improve the efﬁciency. We extend these formulations to the multi-class case based on a key result established in this paper. That is, the multi-class RKDA kernel learning problem can be decomposed into a set of binary-class kernel learning problems which are constrained to share a common kernel. Based on this decomposition property, SDP formulations are proposed for the multi-class case. Furthermore, it leads naturally to QCQP and SILP formulations. As the performance of RKDA depends on the regularization parameter, we show that this parameter can also be optimized in a joint framework with the kernel. Extensive experiments have been conducted and analyzed, and connections to other algorithms are discussed. Keywords: model selection, kernel discriminant analysis, semideﬁnite programming, quadratically constrained quadratic programming, semi-inﬁnite linear programming</p><br/>
<h2>reference text</h2><p>F. Alizadeh and D. Goldfarb. Second-order cone programming. Mathematical Programming, 95: 3–51, 2003. E. D. Andersen and K. D. Andersen. The MOSEK interior point optimizer for linear programming: an implementation of the homogeneous algorithm. In T. Terlaky H. Frenk, K. Roos and S. Zhang, editors, High Performance Optimization, pages 197–232. Kluwer Academic Publishers, 2000. A. Argyriou, R. Hauser, C. Micchelli, and M. Pontil. A DC-programming algorithm for kernel selection. In Proceedings of the Twenty-Third International Conference on Machine Learning, pages 41–48, 2006. F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In Proceedings of the Twenty-First International Conference on Machine Learning, pages 41–48, 2004. G. Baudat and F. Anouar. Generalized discriminant analysis using a kernel approach. Neural Compututation, 12(10):2385–2404, 2000. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004. L. Breiman, J. Friedman, C. J. Stone, and R.A. Olshen. Classiﬁcation and Regression Trees. Chapman and Hall/CRC, 1984. C.-C. Chang and C.-J. Lin. LIBSVM: A Library for Support Vector Machines, 2001. Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm. N. Cristianini and J.S. Taylor. An Introduction to Support Vector Machines and other Kernel-based Learning Methods. Cambridge University Press, 2000. A. d’Aspremont, L. E. Ghaoui, M. I. Jordan, and G. R. G. Lanckriet. A direct formulation for sparse PCA using semideﬁnite programming. SIAM Review, 49(3):434–448, 2007. J.G. Daugman. Complete discrete 2-D Gabor transform by neural networks for image analysis and compression. IEEE Trans. on Acoustics, Speech, and Signal Processing, 36(7):1169–1179, 1988. T. De Bie, G.R.G. Lanckriet, and N. Cristianini. Convex tuning of the soft margin parameter. Technical Report UCB/CSD-03-1289, EECS Department, University of California, Berkeley, 2003. G. Fung, M. Dundar, J. Bi, and B. Rao. A fast iterative algorithm for Fisher discriminant using heterogeneous kernels. In Proceedings of the Twenty-First International Conference on Machine Learning, 2004. M. Gargesha, J. Yang, B. Van Emden, S. Panchanathan, and S. Kumar. Automatic annotation techniques for gene expression images of the fruit ﬂy embryo. In S. Li, F. Pereira, H.-Y. Shum, and A. G. Tescher, editors, Visual Communications and Image Processing 2005., pages 576–583, July 2005. 755  Y E , J I AND C HEN  G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, third edition, 1996. R. Hettich and K. O. Kortanek. Semi-inﬁnite programming: Theory, methods, and applications. SIAM Review, 35(3):380–429, 1993. S. C. H. Hoi, R. Jin, and M. R. Lyu. Learning nonparametric kernel matrices from pairwise constraints. In Proceedings of the Twenty-Fourth International Conference on Machine Learning, pages 361–368, 2007. J. J. Hull. A database for handwritten text recognition research. IEEE Trans. Pattern Analysis Machine Intelligence, 16(5):550–554, 1994. T. Jebara. Multi-task feature and kernel selection for SVMs. In Proceedings of the Twenty-First International Conference on Machine Learning, 2004. S.-J. Kim, A. Magnani, and S. Boyd. Optimal kernel selection in kernel Fisher discriminant analysis. In Proceedings of the Twenty-Third International Conference on Machine Learning, pages 465– 472, 2006. S. Kumar, K. Jayaraman, S. Panchanathan, R. Gurunathan, A. Marti-Subirana, and S. J. Newfeld. BEST: a novel computational approach for comparing gene expression patterns from early stages of Drosophila melanogaster develeopment. Genetics, 169:2037–2047, 2002. G.R.G. Lanckriet, L. El Ghaoui, C. Bhattacharyya, and M. I. Jordan. A robust minimax approach to classiﬁcation. Journal of Machine Learning Research, 3:555–582, 2003. G.R.G. Lanckriet, T. De Bie, N. Cristianini, M.I. Jordan, and W.S. Noble. A statistical framework for genomic data fusion. Bioinformatics, 20(16):2626–2635, 2004a. G.R.G. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and M. I. Jordan. Learning the kernel matrix with semideﬁnite programming. Journal of Machine Learning Research, 5:27–72, 2004b. V. Lavrenko, R. Manmatha, and J. Jeon. A model for learning the semantics of pictures. In Advances in Neural Information Processing Systems 16. 2004. D. Lewis, T. Jebara, and W. S. Noble. Nonstationary kernel combination. In Proceedings of the Twenty-Third International Conference on Machine Learning, pages 553–560, 2006. M. S. Lobo, L. Vandenberghe, S. Boyd, and H. Lebret. Applications of second-order cone programming. Linear Algebra and its Applications, 284:193–228, 1998. C. A. Micchelli and M. Pontil. Feature space perspectives for learning the kernel. Machine Learning, 66(2-3):297–319, 2007. C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine Learning Research, 6:1099–1125, 2005. S. Mika. Kernel Fisher Discriminants. PhD thesis, University of Technology, Berlin, Oct. 2002. 756  D ISCRIMINANT K ERNEL L EARNING  S. Mika, G. R¨ tsch, J. Weston, B. Sch¨ lkopf, and K.-R. M¨ ller. Fisher discriminant analysis with a o u kernels. In Y.-H. Hu, J. Larsen, E. Wilson, and S. Douglas, editors, Neural Networks for Signal Processing IX, pages 41–48. IEEE, 1999. S. Mika, G. R¨ tsch, and K.-R. M¨ ller. A mathematical programming approach to the kernel Fisher a u algorithm. In Advances in Neural Information Processing Systems 13, pages 591–597, 2001. S. Mika, G. R¨ tsch, J. Weston, B. Sch¨ lkopf, A. Smola, and K. M¨ ller. Constructing descriptive a o u and discriminative nonlinear features: Rayleigh coefﬁcients in kernel feature spaces. IEEE Trans. Pattern Analysis Machine Intelligence, 25(5):623–633, 2003. Y. Nesterov and A. Nemirovskii. Interior-point Polynomial Algorithms in Convex Programming. SIAM Studies in Applied Mathematics. SIAM, 1994. D.J. Newman, S. Hettich, C.L. Blake, and C.J. Merz. UCI repository of machine learning databases, 1998. URL http://www.ics.uci.edu/˜mlearn/MLRepository.html. C. S. Ong, A. J. Smola, and R. C. Williamson. Learning the kernel with hyperkernels. Journal of Machine Learning Research, 6:1043–1071, 2005. H. Peng and E. W. Myers. Comparing in situ mRNA expression patterns of Drosophila embryos. In Proceedings of the Eighth Annual International Conference on Research in Computational Molecular Biology, pages 157–166, 2004. J. C. Platt. Fast training of support vector machines using sequential minimal optimization. In Advances in Kernel Methods: Support Vector Learning, pages 185–208. MIT Press, Cambridge, MA, USA, 1999. A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. More efﬁciency in multiple kernel learning. In Proceedings of the Twenty-Fourth International Conference on Machine Learning, pages 775–782, 2007. S. Sch¨ lkopf and A. Smola. Learning with Kernels: Support Vector Machines,Regularization, Opo timization and Beyond. MIT Press, 2002. A. Shashua. On the relationship between the support vector machine for classiﬁcation and sparsiﬁed Fisher’s linear discriminant. Neural Processing Letter, 9(2):129–139, 1999. J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004. S. Sonnenburg, G. R¨ tsch, C. Sch¨ fer, and B. Sch¨ lkopf. Large scale multiple kernel learning. a a o Journal of Machine Learning Research, 7:1531–1565, July 2006. J. F. Sturm. Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones. Optimization Methods and Software, 11-12:625–653, 1999. P. Tomancak, A. Beaton, R. Weiszmann, E. Kwan, S. Shu, S. E Lewis, S. Richards, M. Ashburner, V. Hartenstein, S. E Celniker, and G. M Rubin. Systematic determination of patterns of gene expression during Drosophila embryogenesis. Genome Biology, 3(12), 2002. 757  Y E , J I AND C HEN  I. W. Tsang and J. T. Kwok. Efﬁcient hyperkernel learning using second-order cone programming. IEEE Trans. on Neural Networks, 17(1):48–58, 2006. L. Vandenberghe and S. Boyd. Semideﬁnite programming. SIAM Review, 38:49–95, 1996. V.N. Vapnik. Statistical Learning Theory. Wiley, New York, 1998. J. Ye. Least squares linear discriminant analysis. In Proceedings of the Twenty-Fourth International Conference on Machine Learning, pages 1087–1093, 2007. J. Ye. Characterization of a family of algorithms for generalized discriminant analysis on undersampled problems. Journal of Machine Learning Research, 6:483–502, 2005. J. Ye and T. Xiong. Computational and theoretical analysis of null space and orthogonal linear discriminant analysis. Journal of Machine Learning Research, 7:1183–1204, 2006. J. Ye, J. Chen, Q. Li, and S. Kumar. Classiﬁcation of Drosophila embryonic developmental stage range based on gene expression pattern images. In Proceedings of the Computational Systems Bioinformatics Conference, pages 293–298, 2006. A. Zien and C. S. Ong. Multiclass multiple kernel learning. In Proceedings of the Twenty-Fourth International Conference on Machine Learning, pages 1191–1198, 2007.  758</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
