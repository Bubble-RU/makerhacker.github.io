<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-19" href="../jmlr2008/jmlr-2008-Bouligand_Derivatives_and_Robustness_of_Support_Vector_Machines_for_Regression.html">jmlr2008-19</a> <a title="jmlr-2008-19-reference" href="#">jmlr2008-19-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</h1>
<br/><p>Source: <a title="jmlr-2008-19-pdf" href="http://jmlr.org/papers/volume9/christmann08a/christmann08a.pdf">pdf</a></p><p>Author: Andreas Christmann, Arnout Van Messem</p><p>Abstract: We investigate robustness properties for a broad class of support vector machines with non-smooth loss functions. These kernel methods are inspired by convex risk minimization in inﬁnite dimensional Hilbert spaces. Leading examples are the support vector machine based on the ε-insensitive loss function, and kernel based quantile regression based on the pinball loss function. Firstly, we propose with the Bouligand inﬂuence function (BIF) a modiﬁcation of F.R. Hampel’s inﬂuence function. The BIF has the advantage of being positive homogeneous which is in general not true for Hampel’s inﬂuence function. Secondly, we show that many support vector machines based on a Lipschitz continuous loss function and a bounded kernel have a bounded BIF and are thus robust in the sense of robust statistics based on inﬂuence functions. Keywords: Bouligand derivatives, empirical risk minimization, inﬂuence function, robustness, support vector machines</p><br/>
<h2>reference text</h2><p>V.I. Averbukh and O.G. Smolyanov. The theory of differentiation in linear topological spaces. Russian Mathematical Surveys, 22:201–258, 1967. V.I. Averbukh and O.G. Smolyanov. The various deﬁnitions of the derivative in linear topological spaces. Russian Mathematical Surveys, 23:67–113, 1968. A. Christmann and I. Steinwart. On robust properties of convex risk minimization methods for pattern recognition. Journal of Machine Learning Research, 5:1007–1034, 2004. A. Christmann and I. Steinwart. Consistency and robustness of kernel based regression in convex minimization. Bernoulli, 13:799–819, 2007. F.H. Clarke. Optimization and Nonsmooth Analysis. Wiley & Sons, New York, 1983. L.T. Fernholz. Von Mises Calculus for Statistical Functionals, volume 19 of Lecture Notes in Statistics. Springer, New York, 1983. F.R. Hampel. Contributions to the theory of robust estimation. Unpublished Ph.D. thesis, Dept. of Statistics, University of California, Berkeley, 1968. F.R. Hampel. The inﬂuence curve and its role in robust estimation. Journal of the American Statistical Association, 69:383–393, 1974. F.R. Hampel, E.M. Ronchetti, P.J. Rousseeuw, and W.A. Stahel. Robust statistics: The Approach Based on Inﬂuence Functions. Wiley & Sons, New York, 1986. P.J. Huber. Robust estimation of a location parameter. Ann. Math. Statist., 35:73–101, 1964. C. Ip and J. Kyparisis. Local convergence of quasi-Newton methods for B-differentiable equations. Mathematical Programming, 56:71–89, 1992. 935  C HRISTMANN AND VAN M ESSEM  R. Koenker. Quantile Regression. Cambridge University Press, New York, 2005. R.W. Koenker and G.W. Bassett. Regression quantiles. Econometrica, 46:33–50, 1978. P.D. Lax. Functional Analysis. Wiley & Sons, New York, 2002. R.A. Maronna, R.D. Martin, and V.J. Yohai. Robust Statistics. Theory and Methods. Wiley & Sons, New York, 2006. ¨ H. Rademacher. Uber partielle und totale Differenzierbarkeit. Math. Ann., 79:254–269, 1919. H. Rieder. Robust Asymptotic Statistics. Springer, New York, 1994. S.M. Robinson. Local structure of feasible sets in nonlinear programming, Part III: Stability and sensitivity. Mathematical Programming Study, 30:45–66, 1987. S.M. Robinson. An implicit-function theorem for a class of nonsmooth functions. Mathematics of Operations Research, 16:292–309, 1991. B. Sch¨ lkopf and A.J. Smola. Learning with Kernels. Support Vector Machines, Regularization, o Optimization, and Beyond. MIT Press, Cambridge, Massachusetts, 2002. I. Steinwart. How to compare different loss functions. Constrained Approximation, 26:225–287, 2007. I. Steinwart. On the inﬂuence of the kernel on the consistency of support vector machines. Journal of Machine Learning Research, 2:67–93, 2001. I. Steinwart and A. Christmann. How SVMs can estimate quantiles and the median. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20. MIT Press, Cambridge, Massachusetts, 2008a. I. Steinwart and A. Christmann. Support Vector Machines. Springer, New York, 2008b. I. Takeuchi, Q.V. Le, T.D. Sears, and A.J. Smola. Nonparametric quantile estimation. Journal of Machine Learning Research, 7:1231–1264, 2006. V.N. Vapnik. Statistical Learning Theory. Wiley & Sons, New York, 1998.  936</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
