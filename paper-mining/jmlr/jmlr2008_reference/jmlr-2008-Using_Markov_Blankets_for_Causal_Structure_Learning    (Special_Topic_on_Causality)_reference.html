<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-93" href="../jmlr2008/jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">jmlr2008-93</a> <a title="jmlr-2008-93-reference" href="#">jmlr2008-93-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</h1>
<br/><p>Source: <a title="jmlr-2008-93-pdf" href="http://jmlr.org/papers/volume9/pellet08a/pellet08a.pdf">pdf</a></p><p>Author: Jean-Philippe Pellet, André Elisseeff</p><p>Abstract: We show how a generic feature-selection algorithm returning strongly relevant variables can be turned into a causal structure-learning algorithm. We prove this under the Faithfulness assumption for the data distribution. In a causal graph, the strongly relevant variables for a node X are its parents, children, and children’s parents (or spouses), also known as the Markov blanket of X. Identifying the spouses leads to the detection of the V-structure patterns and thus to causal orientations. Repeating the task for all variables yields a valid partially oriented causal graph. We ﬁrst show an efﬁcient way to identify the spouse links. We then perform several experiments in the continuous domain using the Recursive Feature Elimination feature-selection algorithm with Support Vector Regression and empirically verify the intuition of this direct (but computationally expensive) approach. Within the same framework, we then devise a fast and consistent algorithm, Total Conditioning (TC), and a variant, TCbw , with an explicit backward feature-selection heuristics, for Gaussian data. After running a series of comparative experiments on ﬁve artiﬁcial networks, we argue that Markov blanket algorithms such as TC/TCbw or Grow-Shrink scale better than the reference PC algorithm and provides higher structural accuracy. Keywords: causal structure learning, feature selection, Markov blanket, partial correlation, statistical test of conditional independence</p><br/>
<h2>reference text</h2><p>B. Abramson, J. Brown, A. Murphy, and R. L. Winkler. Hailﬁnder: A Bayesian system for forecasting severe weather. International Journal of Forecasting, 12:57–71, 1996. C. F. Aliferis, I. Tsamardinos, and A. Statnikov. HITON, a novel Markov blanket algorithm for optimal variable selection. In Proceedings of the 2003 American Medical Informatics Association (AMIA) Annual Symposium, pages 21–25, 2003. 1339  P ELLET AND E LISSEEFF  S. Andreassen, R. Hovorka, J. Benn, Kristian G. Olesen, and E. R. Carson. A model-based approach to insulin adjustment. In Proc. of the Third Conf. on AI in Medicine, pages 239–248. SpringerVerlag, 1991. K. Baba, R. Shibata, and M. Sibuya. Partial correlation and conditional correlation as measures of conditional independence. Australian & New Zealand Journal of Statistics, 46(4), 2004. F. R. Bach and M. I. Jordan. Learning graphical models with Mercer kernels. In Advances in Neural Information Processing Systems 15, 2003. I. Beinlich, H. J. Suermondt, R. M. Chavez, and G. F. Cooper. The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks. In Proc. of the Second European Conf. on AI in Medicine, pages 247–256, 1989. J. Binder, D. Koller, S. Russell, and K. Kanazawa. Adaptive probabilistic networks with hidden variables. Machine Learning, 29, 1997. D. M. Chickering. Optimal structure identiﬁcation with greedy search. The Journal of Machine Learning Research, 3:507–554, 2002. G.  Elidan. Bayes net repository. http://compbio.cs.huji.ac.il/Repository/.  Website,  2001.  URL  N. Friedman, M. Linial, I. Nachman, and D. Pe’er. Using Bayesian networks to analyze expression data. In RECOMB, pages 127–135, 2000. L. D. Fu. A comparison of state-of-the-art algorithms for learning Bayesian network structures from continuous data. Master’s thesis, Vanderbilt University, 2005. I. Guyon and A. Elisseeff. An introduction to variable and feature selection. Journal of Machine Learning Research, 3:1157–1182, 2003. I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for cancer classiﬁcation using support vector machines. Machine Learning, 46(1-3):389–422, 2002. I. Guyon, C. Aliferis, and A. Elisseeff. Causal feature selection. In H. Liu and H. Motoda, editors, Computational Methods of Feature Selection. Chapman and Hall/CRC Press, 2007. D. Hardin, I. Tsamardinos, and C. F. Aliferis. A theoretical characterization of linear SVM-based feature selection. In Proceedings of the Twenty First International Conference on Machine Learning, 2004. D. M. Hausman and J. Woodward. Independence, invariance and the causal Markov condition. British Journal for the Philosophy of Science, 50:521–583, 1999. G. H. John, R. Kohavi, and K. Pﬂeger. Irrelevant feature and the subset selection problem. In Proceedings of the Eleventh International Conference on Machine Learning, pages 121–129, 1994. ¨ G. G. Judge, R. Carter Hill, W. E. Grifﬁths, H. Lutkepohl, and T.-C. Lee. Introduction to the Theory and Practice of Econometrics, 2nd Edition. Wiley, 1988. 1340  U SING M ARKOV B LANKETS FOR C AUSAL S TRUCTURE L EARNING  R. Kohavi and G. H. John. Wrappers for feature subset selection. Artiﬁcial Intelligence, 97:273– 324, 1997. S. L. Lauritzen and D. J. Spiegelhalter. Local computations with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society, Series B, 50:157– 224, 1988. P.  Leray and O. Francois. ¸ BNT structure learning banquiseasi.insa-rouen.fr/projects/bnt-slp/.  package,  2004.  URL  D. Margaritis. Distribution-free learning of Bayesian network structure in continuous domains. In Proc. of the 20th National Conf. on AI, 2005. D. Margaritis and S. Thrun. Bayesian network induction via local neighborhoods. In Advances in Neural Information Processing Systems 12, 1999. C. Meek. Causal inference and causal explanation with background knowledge. In Proceedings of the 11th Conference on Uncertainty in Artiﬁcial Intelligence, 1995. R. Nilsson, J. M. Pe˜ a, J. Bj¨ rkegren, and J. Tegn´ r. Consistent feature selection for pattern recogn o e nition in polynomial time. The Journal of Machine Learning Research, 8:589–612, 2007. J. M. Pe˜ a, J. Bj¨ rkegren, and J. Tegn´ r. Scalable, efﬁcient and correct learning of Markov boundn o e aries under the faithfulness assumption. In Proceedings of the Eighth European Conference on Symbolic and Quantitative Approaches to Reasoning under Uncertainty, pages 136–147, 2005. J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2000. J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, Los Altos, 1988. J. Pearl. Causal diagrams for empirical research. Biometrika, 82(4):669–709, 1995. J. Pearl and T. Verma. A theory of inferred causation. In Proc. of the Second Int. Conf. on Principles of Knowledge Representation and Reasoning. Morgan Kaufmann, 1991. J.-P. Pellet and A. Elisseeff. A partial correlation-based algorithm for causal structure discovery with continuous variables. In 7th International Symposium on Intelligent Data Analysis, 2007. A. Raveh. On the use of the inverse of the correlation matrix in multivariate data analysis. The American Statistician, 39:39–42, 1985. R. Scheines, P. Spirtes, C. Glymour, C. Meek, and T. Richardson. The TETRAD project: Constraint based aids to causal model speciﬁcation. Technical report, Carnegie Mellon University, Dpt. of Philosophy, 1995. A. J. Smola and B. Sch¨ lkopf. A tutorial on support vector regression. Technical report, Neuroo COLT2, 1998. P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search, Second Edition. The MIT Press, 2001. ISBN 0262194406. 1341  P ELLET AND E LISSEEFF  A. Statnikov, D. Hardin, and C. F. Aliferis. Using SVM weight-based methods to identify causally relevant and non-causally relevant variables. Technical report, Vanderbilt University, USA, 2006. D. Steel. Homogeneity, selection, and the faithfulness condition. Technical report, Michigan State University, Department of Philosophy, 2005. M. Talih. Markov Random Fields on Time-Varying Graphs, with an Application to Portfolio Selection. PhD thesis, Hunter College, 2003. R. Tibshirani. Regression shrinkage and selection via the lasso. Technical report, University of Toronto, 1994. I. Tsamardinos and C. Aliferis. Towards principled feature selection: Relevancy. Artiﬁcial Intelligence and Statistics, 2003. I. Tsamardinos, C. F. Aliferis, and A. Statnikov. Time and sample efﬁcient discovery of Markov blankets and direct causal relations. In ACM Press, editor, Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 673–678, 2003. I. Tsamardinos, L. E. Brown, and C. F. Aliferis. The max-min hill-climbing Bayesian network structure learning algorithm. Machine Learning, 2006.  1342</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
