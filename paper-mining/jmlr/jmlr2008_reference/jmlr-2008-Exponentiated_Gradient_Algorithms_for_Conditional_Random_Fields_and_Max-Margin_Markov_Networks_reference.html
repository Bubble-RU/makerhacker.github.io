<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-34" href="../jmlr2008/jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">jmlr2008-34</a> <a title="jmlr-2008-34-reference" href="#">jmlr2008-34-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</h1>
<br/><p>Source: <a title="jmlr-2008-34-pdf" href="http://jmlr.org/papers/volume9/collins08a/collins08a.pdf">pdf</a></p><p>Author: Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, Peter L. Bartlett</p><p>Abstract: Log-linear and maximum-margin models are two commonly-used methods in supervised machine learning, and are frequently used in structured prediction problems. Efﬁcient learning of parameters in these models is therefore an important problem, and becomes a key factor when learning from very large data sets. This paper describes exponentiated gradient (EG) algorithms for training such models, where EG updates are applied to the convex dual of either the log-linear or maxmargin objective function; the dual in both the log-linear and max-margin cases corresponds to minimizing a convex function with simplex constraints. We study both batch and online variants of the algorithm, and provide rates of convergence for both cases. In the max-margin case, O( 1 ) EG ε updates are required to reach a given accuracy ε in the dual; in contrast, for log-linear models only O(log( 1 )) updates are required. For both the max-margin and log-linear cases, our bounds suggest ε that the online EG algorithm requires a factor of n less computation to reach a desired accuracy than the batch EG algorithm, where n is the number of training examples. Our experiments conﬁrm that the online algorithms are much faster than the batch algorithms in practice. We describe how the EG updates factor in a convenient way for structured prediction problems, allowing the algorithms to be efﬁciently applied to problems such as sequence learning or natural language parsing. We perform extensive evaluation of the algorithms, comparing them to L-BFGS and stochastic gradient descent for log-linear models, and to SVM-Struct for max-margin models. The algorithms are applied to a multi-class problem as well as to a more complex large-scale parsing task. In all these settings, the EG algorithms presented here outperform the other methods. Keywords: exponentiated gradient, log-linear models, maximum-margin models, structured prediction, conditional random ﬁelds ∗. These authors contributed equally. c 2008 Michael Col</p><br/>
<h2>reference text</h2><p>J. Baker. Trainable grammars for speech recognition. In J.J. Wolf and D.H. Klatt, editors, Proceedings of the 97th meeting of the Acoustical Society of America, pages 547–550. Acoustical Society of America, New York, NY, 1979. P. L. Bartlett, M. Collins, B. Taskar, and D. McAllester. Exponentiated gradient algorithms for large–margin structured classiﬁcation. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 113–120, Cambridge, MA, 2005. MIT Press. A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31:167–175, 2003. L.M. Bregman. The relaxation method of ﬁnding the common point of convex sets and its application to the solution of problems in convex programming. U.S.S.R. Computational Mathematics and Mathematical Physics, 7:200–217, 1967. S. Buchholz and E. Marsi. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the 10th Conference on Computational Natural Language Learning, pages 149–164, New York City, 2006. Association for Computational Linguistics. R.H. Byrd, P. Lu, and J. Nocedal. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientiﬁc and Statistical Computing, 16(5):1190–1208, 1995. Y. Censor and S.A. Zenios. Parallel Optimization. Oxford University Press, 1997. M. Civit and M. Ant` nia Mart´. Design principles for a Spanish treebank. In Proceedings of the 1st o ı Workshop on Treebanks and Linguistic Theories, pages 61–77, 2002. C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995. T.M. Cover and J.A Thomas. Elements of Information Theory. Wiley, 1991. K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning Research, 2:265–292, 2002. N. Cristianini, C. Campbell, and J. Shawe-Taylor. Multiplicative updatings for support-vector learning. Technical report, NC-TR-98-016, Neuro COLT, Royal Holloway College, 1998. A. Globerson, T. Koo, X. Carreras, and M. Collins. Exponentiated gradient algorithms for log-linear structured prediction. In Z. Ghahramani, editor, Proceedings of the 24th International Conference on Machine Learning, pages 305–312. ACM Press, New York, NY, 2007. D. Hush, P. Kelly, C. Scovel, and I. Steinwart. QP algorithms with guaranteed accuracy and run time for support vector machines. Journal of Machine Learning Research, 7:733–769, 2006. T. Jaakkola and D. Haussler. Probabilistic kernel regression models. In D. Heckerman and J. Whittaker, editors, Proceedings of 7th Workshop on Artiﬁcial Intelligence and Statistics. Morgan Kaufmann, San Francisco, CA, 1999. 1820  E XPONENTIATED G RADIENT A LGORITHMS FOR CRF S AND M AX -M ARGIN M ARKOV N ETWORKS  T. Joachims. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 217–226. ACM Press, New York, NY, 2006. S.S. Keerthi, K.B. Duan, S.K. Shevade, and A. N. Poo. A fast dual algorithm for kernel logistic regression. Machine Learning, 61:151–165, 2005. J. Kivinen and M. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1–63, 1997. J. Kivinen and M. Warmuth. Relative loss bounds for multidimensional regression problems. Machine Learning, 45(3):301–329, 2001. K. Koh, S.J. Kim, and S. Boyd. An interior point method for large scale l 1 -regularized logistic regression. Journal of Machine Learning Research, 8:1519–1555, 2007. T. Koo, A. Globerson, X. Carreras, and M. Collins. Structured prediction models via the matrixtree theorem. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 141–150. Association for Computational Linguistics, 2007. J. Lafferty, A. McCallum, and F. Pereira. Conditonal random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In C.E. Brodley and A.P. Danyluk, editors, Proceedings of the 18th International Conference on Machine Learning, pages 282–289, San Francisco, CA, 2001. Morgan Kaufmann. G. Lebanon and J. Lafferty. Boosting and maximum likelihood for exponential models. In T.G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 447–454. MIT Press, Cambridge, MA, 2002. Y. LeCun, L. Bottou, Y. Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. N. List, D. Hush, C. Scovel, and I. Steinwart. Gaps in support vector optimization. In Proceedings of the 20th Conference on Learning Theory, pages 336–348, 2007. R. McDonald, K. Crammer, and F. Pereira. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the ACL, pages 91–98. Association for Computational Linguistics, 2005. R. Memisevic. Dual optimization of conditional probability models. Technical report, University of Toronto, 2006. T. Minka. A comparison of numerical optimizers for logistic regression. Technical report, Carnegie Mellon University, 2003. M. Mitzenmacher and E. Upfal. Probability and Computing: Randomized Algorithms and Probabilistic Analysis. Cambridge University Press, 2005. A. Nedic and D. P. Bertsekas. Incremental subgradient methods for nondifferentiable optimization. SIAM Journal on Optimization, 12(1):109–138, 2001. 1821  C OLLINS , G LOBERSON , KOO , C ARRERAS AND BARTLETT  J. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Sch¨ lkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Veco tor Learning, pages 41–64. MIT Press, 1998. F. Sha and F. Pereira. Shallow parsing with conditional random ﬁelds. In Proceedings of the conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 134–141. Association for Computational Linguistics, 2003. F. Sha, Y. Lin, L.K. Saul, and D.D. Lee. Multiplicative updates for nonnegative quadratic programming. Neural Computation, 19(8):2004–2031, 2007. S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In Z. Ghahramani, editor, Proceedings of the 24th International Conference on Machine Learning, pages 807–814. ACM Press, New York, NY, 2007. B. Taskar, C. Guestrin, and D. Koller. Max margin Markov networks. In S. Thrun, L. Saul, and B. Sch¨ lkopf, editors, Advances in Neural Information Processing Systems 16, pages 25–32. MIT o Press, Cambridge, MA, 2004a. B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning. Max-margin parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1–8. Association for Computational Linguistics, 2004b. B. Taskar, S. Lacoste-Julien, and M. Jordan. Structured prediction, dual extragradient and Bregman projections. Journal of Machine Learning Research, pages 1627–1653, 2006. C.H. Teo, Q. Le, A. Smola, and S.V.N. Vishwanathan. A scalable modular convex solver for regularized risk minimization. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 727–736. ACM Press, New York, NY, USA, 2007. I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdependent and structured output spaces. In C.E. Brodley, editor, Proceedings of the 21st International Conference on Machine Learning, pages 823–830. ACM, New York, NY, 2004. S.V. N. Vishwanathan, N. N. Schraudolph, M. W. Schmidt, and K. P. Murphy. Accelerated training of conditional random ﬁelds with stochastic gradient methods. In W.W. Cohen and A. Moore, editors, Proceedings of the 23rd International Conference on Machine Learning, pages 969–976. ACM Press, New York, NY, 2006. T. Zhang. On the dual formulation of regularized linear systems with convex risks. Machine Learning, 46:91–129, 2002. J. Zhu and T. Hastie. Kernel logistic regression and the import vector machine. In T.G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 1081–1088. MIT Press, Cambridge, MA, 2001.  1822</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
