<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-64" href="../jmlr2008/jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">jmlr2008-64</a> <a title="jmlr-2008-64-reference" href="#">jmlr2008-64-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</h1>
<br/><p>Source: <a title="jmlr-2008-64-pdf" href="http://jmlr.org/papers/volume9/debruyne08a/debruyne08a.pdf">pdf</a></p><p>Author: Michiel Debruyne, Mia Hubert, Johan A.K. Suykens</p><p>Abstract: Recent results about the robustness of kernel methods involve the analysis of inﬂuence functions. By deﬁnition the inﬂuence function is closely related to leave-one-out criteria. In statistical learning, the latter is often used to assess the generalization of a method. In statistics, the inﬂuence function is used in a similar way to analyze the statistical efﬁciency of a method. Links between both worlds are explored. The inﬂuence function is related to the ﬁrst term of a Taylor expansion. Higher order inﬂuence functions are calculated. A recursive relation between these terms is found characterizing the full Taylor expansion. It is shown how to evaluate inﬂuence functions at a speciﬁc sample distribution to obtain an approximation of the leave-one-out error. A speciﬁc implementation is proposed using a L1 loss in the selection of the hyperparameters and a Huber loss in the estimation procedure. The parameter in the Huber loss controlling the degree of robustness is optimized as well. The resulting procedure gives good results, even when outliers are present in the data. Keywords: kernel based regression, robustness, stability, inﬂuence function, model selection</p><br/>
<h2>reference text</h2><p>D.D. Boos and R.J. Serﬂing. A note on differentials and the CLT and LIL for statistical functions. Annals of Statistics, 8:618–624, 1980. O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research, 2:499–526, 2001. A. Christmann and I. Steinwart. Consistency and robustness of kernel based regression. Bernoulli, 13:799–819, 2007. A. Christmann and I. Steinwart. On robust properties of convex risk minimization methods for pattern recognition. Journal of Machine Learning Research, 5:1007–1034, 2004. 2399  D EBRUYNE , H UBERT AND S UYKENS  M. Debruyne, A. Christmann, M. Hubert, and J.A.K. Suykens. Robustness and stability of reweighted kernel based regression. Technical report TR 06-09, K.U. Leuven, available at http://wis.kuleuven.be/stat/robust, 2006. E. DeVito, L. Rosasco, A. Caponnetto, M. Piana, and A. Verri. Some properties of regularized kernel methods. Journal of Machine Learning Research, 5:1363–1390, 2004. T. Evgeniou, M. Pontil, and T. Poggio. Regularization networks and support vector machines. Advances in Computational Mathematics, 13:1–50, 2000. L.T. Fernholz. Von Mises Calculus for Statistical Functionals. Lecture Notes in statistics 19, Springer, New York, 1983. F.R. Hampel, E.M. Ronchetti, P.J. Rousseeuw, and W.A. Stahel. Robust Statistics: The Approach Based on Inﬂuence Functions. Wiley, New York, 1986. P.J. Huber. Robust Statistics. Wiley, New York, 1981. S. Kutin and P. Niyogi. Almost everywhere algorithmic stability and generalization error. In A. Daruich and N. Friedman, editors, Proceedings of Uncertainty in AI. Morgan Kaufmann, Edmonton, 2002. T. Poggio, R. Rifkin, S. Mukherjee, and P. Niyogi. General conditions for predictivity in learning theory. Nature, 428:419–422, 2004. J.A.K. Suykens, J. De Brabanter, L. Lukas, and J. Vandewalle. Weighted least squares support vector machines : Robustness and sparse approximation. Neurocomputing, 48:85–105, 2002a. J.A.K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, and J. Vandewalle. Least Squares Support Vector Machines. World Scientiﬁc, Singapore, 2002b. A.N. Tikhonov and V.Y. Arsenin. Solutions of Ill Posed Problems. W.H. Winston, Washington D.C., 1977. G. Wahba. Spline Models for Observational Data. CBMS-NSF Regional Conference Series in Applied Mathematics, SIAM, 1990.  2400</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
