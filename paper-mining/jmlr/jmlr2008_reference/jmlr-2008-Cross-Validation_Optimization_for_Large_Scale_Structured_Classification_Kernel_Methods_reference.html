<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-29" href="../jmlr2008/jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">jmlr2008-29</a> <a title="jmlr-2008-29-reference" href="#">jmlr2008-29-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</h1>
<br/><p>Source: <a title="jmlr-2008-29-pdf" href="http://jmlr.org/papers/volume9/seeger08b/seeger08b.pdf">pdf</a></p><p>Author: Matthias W. Seeger</p><p>Abstract: We propose a highly efﬁcient framework for penalized likelihood kernel methods applied to multiclass models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the ﬁtting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work. Parts of this work appeared in the conference paper Seeger (2007). Keywords: multi-way classiﬁcation, kernel logistic regression, hierarchical classiﬁcation, cross validation optimization, Newton-Raphson optimization</p><br/>
<h2>reference text</h2><p>F. Bach and M. Jordan. Kernel independent component analysis. Journal of Machine Learning Research, 3:1–48, 2002. P. Bartlett and A. Tewari. Sparseness vs estimating conditional probabilities: Some asymptotic results. In Conference on Computational Learning Theory 17, pages 564–578. Springer, 2004. D. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 2nd edition, 1999. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2002. L. Cai and T. Hofmann. Hierarchical document categorization with support vector machines. In CIKM 13, pages 78–87, 2004. K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning Research, 2:265–292, 2001. 24. Matrices in BLAS are stored column-wise, each column has to be contiguous in memory, but the striding value (offset in memory required to jump to the next column) can be larger than the number of rows.  1176  L ARGE S CALE S TRUCTURED C LASSIFICATION K ERNEL M ETHODS  P. Craven and G. Wahba. Smoothing noisy data with spline functions: Estimating the correct degree of smoothing by the method of generalized cross-validation. Numerische Mathematik, 31:377– 403, 1979. L. Csat´ . Gaussian Processes — Iterative Sparse Approximations. PhD thesis, Aston University, o Birmingham, UK, March 2002. S. Della Pietra, V. Della Pietra, and J. Lafferty. Inducing features of random ﬁelds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393, 1997. A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of Roy. Stat. Soc. B, 39:1–38, 1977. M. Gibbs. Bayesian Gaussian Processes for Regression and Classiﬁcation. PhD thesis, University of Cambridge, 1997. P.J. Green and B. Silverman. Nonparametric Regression and Generalized Linear Models. Monographs on Statistics and Probability. Chapman & Hall, 1994. C.-W. Hsu and C.-J. Lin. A comparison of methods for multi-class support vector machines. IEEE Transactions on Neural Networks, 13:415–425, 2002. S. Keerthi and D. DeCoste. A modiﬁed ﬁnite Newton method for fast solution of large scale linear SVMs. Journal of Machine Learning Research, 6:341–361, 2005. S. Keerthi, V. Sindhwani, and O. Chapelle. An efﬁcient method for gradient-based adaptation of ¨ hyperparameters in SVM models. In B. Scholkopf, J. Platt, and T. Hofmann, editors, Advances in Neural Information Processing Systems 19. MIT Press, 2007. J. Lafferty, Y. Liu, and X. Zhu. Kernel conditional random ﬁelds: Representation, clique selection, and semi-supervised learning. Technical Report CMU-CS-04-115, Carnegie Mellon University, 2004. Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1:541–551, 1989. P. McCullach and J.A. Nelder. Generalized Linear Models. Number 37 in Monographs on Statistics and Applied Probability. Chapman & Hall, 1st edition, 1983. M. Opper and O. Winther. Gaussian processes for classiﬁcation: Mean ﬁeld algorithms. Neural Computation, 12(11):2655–2684, 2000. B. Pearlmutter. Fast exact multiplication by the Hessian. Neural Computation, 6(1):147–160, 1994. J. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Sch¨ lkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods: Support Vector o Learning, pages 185–208. MIT Press, 1998. Y. Qi, T. Minka, R. Picard, and Z. Ghahramani. Predictive automatic relevance determination by expectation propagation. In C. Brodley, editor, International Conference on Machine Learning 21. Morgan Kaufmann, 2004. 1177  S EEGER  C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006. Y. Saad. Iterative Methods for Sparse Linear Systems. International Thomson Publishing, 1st edition, 1996. B. Sch¨ lkopf and A. Smola. Learning with Kernels. MIT Press, 1st edition, 2002. o M. Seeger. Bayesian Gaussian Process Models: PAC-Bayesian Generalisation Error Bounds and Sparse Approximations. PhD thesis, University of Edinburgh, July 2003. See www.kyb.tuebingen.mpg.de/bs/people/seeger. M. Seeger. Gaussian processes for machine learning. International Journal of Neural Systems, 14 (2):69–106, 2004. M. Seeger. Cross-validation optimization for large scale hierarchical classiﬁcation kernel methods. In B. Sch¨ lkopf, J. Platt, and T. Hofmann, editors, Advances in Neural Information Processing o Systems 19, pages 1233–1240. MIT Press, 2007. B. Shahbaba and R. Neal. Improving classiﬁcation when a class hierarchy is available using a hierarchy-based prior. Bayesian Analysis, 2(1):221–238, 2007. Y. Shen, A. Ng, and M. Seeger. Fast Gaussian process regression using KD-trees. In Y. Weiss, B. Sch¨ lkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18. MIT o Press, 2006. E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Y. Weiss, B. Sch¨ lkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18. MIT o Press, 2006. Y.-W. Teh, M. Seeger, and M. I. Jordan. Semiparametric latent factor models. In Z. Ghahramani and R. Cowell, editors, Workshop on Artiﬁcial Intelligence and Statistics 10, 2005. G. Wahba. Spline Models for Observational Data. CBMS-NSF Regional Conference Series. Society for Industrial and Applied Mathematics, 1990. C. Williams and C. Rasmussen. Gaussian processes for regression. In D. Touretzky, M. Mozer, and M. Hasselmo, editors, Advances in Neural Information Processing Systems 8. MIT Press, 1996. C. K. I. Williams and D. Barber. Bayesian classiﬁcation with Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342–1351, 1998. C. Yang, R. Duraiswami, and L. Davis. Efﬁcient kernel machines using the improved fast Gauss transform. In L. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 1561–1568. MIT Press, 2005.  1178</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
