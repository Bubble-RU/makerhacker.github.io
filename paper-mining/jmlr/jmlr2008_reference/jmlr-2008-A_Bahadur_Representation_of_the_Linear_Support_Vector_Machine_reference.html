<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-1" href="../jmlr2008/jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">jmlr2008-1</a> <a title="jmlr-2008-1-reference" href="#">jmlr2008-1-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</h1>
<br/><p>Source: <a title="jmlr-2008-1-pdf" href="http://jmlr.org/papers/volume9/koo08a/koo08a.pdf">pdf</a></p><p>Author: Ja-Yong Koo, Yoonkyung Lee, Yuwon Kim, Changyi Park</p><p>Abstract: The support vector machine has been successful in a variety of applications. Also on the theoretical front, statistical properties of the support vector machine have been studied quite extensively with a particular attention to its Bayes risk consistency under some conditions. In this paper, we study somewhat basic statistical properties of the support vector machine yet to be investigated, namely the asymptotic behavior of the coefﬁcients of the linear support vector machine. A Bahadur type representation of the coefﬁcients is established under appropriate conditions, and their asymptotic normality and statistical variability are derived on the basis of the representation. These asymptotic results do not only help further our understanding of the support vector machine, but also they can be useful for related statistical inferences. Keywords: asymptotic normality, Bahadur representation, classiﬁcation, convexity lemma, Radon transform</p><br/>
<h2>reference text</h2><p>R. R. Bahadur. A note on quantiles in large samples. Annals of Mathematical Statistics, 37:577–580, 1966. P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classiﬁcation, and risk bounds. Journal of the American Statististical Association, 101:138–156, 2006. G. Blanchard, O. Bousquet, and P. Massart. Statistical performance of support vector machines. The Annals of Statistics, 36:489–531, 2008. P. Chaudhuri. Nonparametric estimates of regression quantiles and their local Bahadur representation. The Annals of Statistics, 19:760–777, 1991. C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20:273–297, 1995. N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines and Other Kernelbased Learning Methods. Cambridge University Press, Cambridge, 2000. S. R. Deans. The Radon Transform and Some of Its Applications. Krieger Publishing Company, Florida, 1993. I. Guyon and A. Elisseeff. Introduction to variable and feature selection. Journal of Machine Learning Research, 3:1157–1182, 2003. I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for cancer classiﬁcation using support vector machines. Machine Learning, 46:389–422, 2002. A. B. Ishak and B. Ghattas. An efﬁcient method for variable selection using svm-based criteria. Preprint, Institut de Math´ matiques de Luminy, 2005. e R. Koenker. Quantile Regression. Econometric Society Monographs. Cambridge University Press, 2005. Y. Lin. Some asymptotic properties of the support vector machine. Technical report 1029, Department of Statistics, University of Wisconsin-Madison, 2000. Y. Lin. A note on margin-based loss functions in classiﬁcation. Statististics and Probability Letters, 68:73–82, 2002. F. Natterer. The Mathematics of Computerized Tomography. Wiley & Sons, New York, 1986. D. Pollard. Asymptotics for least absolute deviation regression estimators. Econometric Theory, 7: 186–199, 1991. A. G. Ramm and A. I. Katsevich. The Radon Transform and Local Tomography. CRC Press, Boca Raton, 1996. B. Sch¨ lkopf and A. Smola. Learning with Kernels - Support Vector Machines, Regularization, o Optimization and Beyond. MIT Press, 2002. 1367  KOO , L EE , K IM AND PARK  J. C. Scovel and I. Steinwart. Fast rates for support vector machines using gaussian kernels. The Annals of Statistics, 35:575–607, 2007. X. Shen. On methods of sieves and penalization. The Annals of Statistics, 25:2555–2591, 1997. I. Steinwart. Consistency of support vector machines and other regularized kernel machines. IEEE Transactions on Information Theory, 51:128–142, 2005. V. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, 1996. T. Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. The Annals of Statistics, 32:56–84, 2004.  1368</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
