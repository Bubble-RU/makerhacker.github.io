<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-95" href="../jmlr2008/jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">jmlr2008-95</a> <a title="jmlr-2008-95-reference" href="#">jmlr2008-95-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</h1>
<br/><p>Source: <a title="jmlr-2008-95-pdf" href="http://jmlr.org/papers/volume9/csaji08a/csaji08a.pdf">pdf</a></p><p>Author: Balázs Csanád Csáji, László Monostori</p><p>Abstract: The paper investigates the possibility of applying value function based reinforcement learning (RL) methods in cases when the environment may change over time. First, theorems are presented which show that the optimal value function of a discounted Markov decision process (MDP) Lipschitz continuously depends on the immediate-cost function and the transition-probability function. Dependence on the discount factor is also analyzed and shown to be non-Lipschitz. Afterwards, the concept of (ε, δ)-MDPs is introduced, which is a generalization of MDPs and ε-MDPs. In this model the environment may change over time, more precisely, the transition function and the cost function may vary from time to time, but the changes must be bounded in the limit. Then, learning algorithms in changing environments are analyzed. A general relaxed convergence theorem for stochastic iterative algorithms is presented. We also demonstrate the results through three classical RL methods: asynchronous value iteration, Q-learning and temporal difference learning. Finally, some numerical experiments concerning changing environments are presented. Keywords: Markov decision processes, reinforcement learning, changing environments, (ε, δ)MDPs, value function bounds, stochastic iterative algorithms</p><br/>
<h2>reference text</h2><p>D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 2. Athena Scientiﬁc, Belmont, Massachusetts, 3rd edition, 2007. D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, 1996. B. Cs. Cs´ ji. Adaptive Resource Control: Machine Learning Approaches to Resource Allocation a ¨ o in Uncertain and Changing Environments. PhD thesis, Faculty of Informatics, E otv¨ s Lor´ nd a University, Budapest, 2008. B. Cs. Cs´ ji and L. Monostori. Adaptive sampling based large-scale stochastic resource control. a In Proceedings of the 21st National Conference on Artiﬁcial Intelligence (AAAI-06), July 16-20, Boston, Massachusetts, pages 815–820, 2006. 1708  R EINFORCEMENT L EARNING IN C HANGING E NVIRONMENTS  R. Montes de Oca, A. Sakhanenko, and F. Salem. Estimates for perturbations of general discounted Markov control chains. Applied Mathematics, 30:287–304, 2003. E. Even-Dar and Y. Mansour. Learning rates for Q-learning. Journal of Machine Learning Research (JMLR), 5:1–25, Dec. 2003. G. Favero and W. J. Runggaldier. A robustness result for stochastic control. Systems and Control Letters, 46:91–66, 2002. E. A. Feinberg and A. Shwartz, editors. Handbook of Markov Decision Processes: Methods and Applications. Kluwer Academic Publishers, 2002. E. Gordienko and F. S. Salem. Estimates of stability of Markov control processes with unbounded cost. Kybernetika, 36:195–210, 2000. Zs. Kalm´ r, Cs. Szepesv´ ri, and A. L˝ rincz. Module-based reinforcement learning: Experiments a a o with a real robot. Machine Learning, 31:55–85, 1998. M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning, 49:209–232, 2002. A. M¨ ller. How does the solution of a Markov decision process depend on the transition probabilu ities? Technical report, Institute for Economic Theory and Operations Research, University of Karlsruhe, 1996. R. Munos and A. W. Moore. Rates of convergence for variable resolution schemes in optimal control. In Proceedings of the 17th International Conference on Machine Learning (ICML), pages 647–654. Morgan Kaufmann, San Francisco, CA, 2000. M. Pinedo. Scheduling: Theory, Algorithms, and Systems. Prentice-Hall, 2002. S. Singh and D. Bertsekas. Reinforcement learning for dynamic channel allocation in cellular telephone systems. In Advances in Neural Information Processing Systems, volume 9, pages 974–980. The MIT Press, 1997. R. S. Sutton and A. G. Barto. Reinforcement Learning. The MIT Press, 1998. R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in Neural Information Processing Systems, 12: 1057–1063, 2000. Cs. Szepesv´ ri and M. L. Littman. A uniﬁed analysis of value-function-based reinforcement learna ing algorithms. Neural Computation, 11(8):2017–2060, 1999. I. Szita, B. Tak´ cs, and A. L˝ rincz. ε-MDPs: Learning in varying environments. Journal of Machine a o Learning Research (JMLR), 3:145–174, 2002. B. Van Roy, D. Bertsekas, Y. Lee, and J. Tsitsiklis. A neuro-dynamic programming approach to retailer inventory management. Technical report, Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA., 1996. 1709</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
