<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-12" href="../jmlr2008/jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">jmlr2008-12</a> <a title="jmlr-2008-12-reference" href="#">jmlr2008-12-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</h1>
<br/><p>Source: <a title="jmlr-2008-12-pdf" href="http://jmlr.org/papers/volume9/balakrishnan08a/balakrishnan08a.pdf">pdf</a></p><p>Author: Suhrid Balakrishnan, David Madigan</p><p>Abstract: Classiﬁers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classiﬁers, etc., provide competitive methods for classiﬁcation problems in high dimensions. However, current algorithms for training sparse classiﬁers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function. Keywords: Laplace approximation, expectation propagation, LASSO</p><br/>
<h2>reference text</h2><p>J. M. Bernardo and A. F. M. Smith. Bayesian Theory. John Wiley and Sons, Inc., 1994. S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by Basis Pursuit. SIAM Journal on Scientiﬁc Computing, 20(1):33–61, January 1999. B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. The Annals of Statistics, 32(2):407–499, 2004. E. Eskin, A. J. Smola, and S.V.N. Vishwanathan. Laplace Propagation. In Neural Information Processing Systems, 16. MIT Press, 2003. M. A. T. Figueiredo and A. K. Jain. Bayesian learning of sparse classiﬁers. In Proceedings of the Computer Vision and Pattern Recognition Conference, volume 1, pages 35–41, 2001. W. J. Fu. Penalized regressions: The Bridge versus the Lasso. Journal of Computational and Graphical Statistics, 7(3):397–416, 1998. A. Genkin, D. D. Lewis, and D. Madigan. Large-scale Bayesian logisitic regression for text categorization. Technometrics, 49:291–304, 2007. T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. The entire regularization path for the Support Vector Machine. Journal of Machine Learning Research, 5:1391–1415, 2004. T. Jaakkola and M. Jordan. Bayesian parameter estimation via variational methods. Statistics and Computing, 10:25–37, 2000. R. E. Kass and A. E. Raftery. Bayes factors. Journal of the American Statistical Association, 90: 773–795, 1995. K. Koh, S.-J. Kim, and S. Boyd. An interior-point method for large-scale l 1 -regularized logistic regression. Journal of Machine Learning Research, 8:1519–1555, 2007. P. Komarek and A. Moore. Making logistic regression a core data mining tool: A practical investigation of accuracy, speed, and simplicity. Technical Report TR-05-27, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, May 2005. B. Krishnapuram, L. Carin, M. A. T. Figueiredo, and A. J. Hartemink. Sparse multinomial logistic regression: Fast algorithms and generalization bounds. IEEE Transactions on Pattern Analalysis and Machine Intelligence, 2005. D.  D.  Lewis. Reuters-21578 text categorization test collection: Distribution 1.0 readme ﬁle (v 1.3)., 2004. URL http://www.daviddlewis.com/resources/testcollections/reuters21578/readme.txt. 336  S PARSE C LASSIFIERS FOR M ASSIVE DATA  D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361–397, 2004. D. J. C. MacKay. Probable networks and plausible predictions: a review of practical Bayesian methods for supervised neural networks. Network: Computation in Neural Systems, 6:469–505, 1995. T. P. Minka. Expectation Propagation for approximate Bayesian inference. In Jack Breese and Daphne Koller, editors, Proceedings of the Seventeenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI-01), pages 362–369, San Francisco, CA, August 2–5 2001a. Morgan Kaufmann Publishers. T. P. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2001b. M. Opper. A Bayesian approach to on-line learning. In D. Saad, editor, Online Learning in Neural Networks, pages 363–378. Cambridge University Press, 1998. M. R. Osborne, B. Presnell, and B. A. Turlach. A new approach to variable selection in least squares problems. IMA Journal of Numerical Analysis, 20(3):389–403, July 2000. Y. Qi, T. P. Minka, R. W. Picard, and Z. Ghahramani. Predictive automatic relevance determination by Expectation Propagation. In Proceedings of Twenty-ﬁrst International Conference on Machine Learning, Banff, Alberta, Canada, July 4-8 2004. R. T. Rockafellar. Convex Analysis. Princeton University Press, Princeton, N.J, 1970. S. K. Shevade and S. S. Keerthi. A simple and efﬁcient algorithm for gene selection using sparse logistic regression. Bioinformatics., 19(17):2246–2253, 2003. R. J. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58(1):267–288, 1996. T. Zhang. On the dual formulation of regularized linear systems. Machine Learning, 46:91–129, 2002. T. Zhang and F. J. Oles. Text categorization based on regularized linear classiﬁcation methods. Information Retrieval, 4(1):5–31, 2001. H. Zou. The adaptive Lasso and its oracle properties. Journal of the American Statistical Association, 101:1418–1429, 2006.  337</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
