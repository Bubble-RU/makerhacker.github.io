<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-18" href="../jmlr2008/jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">jmlr2008-18</a> <a title="jmlr-2008-18-reference" href="#">jmlr2008-18-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</h1>
<br/><p>Source: <a title="jmlr-2008-18-pdf" href="http://jmlr.org/papers/volume9/seeger08a/seeger08a.pdf">pdf</a></p><p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><br/>
<h2>reference text</h2><p>H. Attias. A variational Bayesian framework for graphical models. In S. Solla, T. Leen, and K.-R. M¨ ller, editors, Advances in Neural Information Processing Systems 12, pages 209–215. MIT u Press, 2000. 43. We used their extraction code in order to create the data set in the ﬁrst place.  808  BAYESIAN I NFERENCE FOR S PARSE L INEAR M ODEL  P. Berkes, R. Turner, and M. Sahani. On sparsity and overcompleteness in image models. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20. MIT Press, 2008. C. Bishop and J. Winn. Structured variational distributions in VIBES. In C. Bishop and B. Frey, editors, Workshop on Artiﬁcial Intelligence and Statistics 9, pages 244–251, 2003. Electronic Proceedings (ISBN 0-9727358-0-1). V. Bogachev. Gaussian Measures. Mathematical Surveys and Monographs. American Mathematical Society, 1998. L. Bottou. Online learning and stochastic approximations. In D. Saad, editor, On-Line Learning in Neural Networks. Cambridge University Press, 1998. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2002. E. Cand` s, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction e from highly incomplete frequency information. IEEE Transactions on Information Theory, 52 (2):489–509, 2006. K. Chaloner and I. Verdinelli. Bayesian experimental design: A review. Statistical Science, 10(3): 273–304, 1995. S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientiﬁc Computing, 20(1):33–61, 1999. R. Chhikara and L. Folks. The Inverse Gaussian Distribution: Theory, Methodology, and Applications. Marcel Dekker Inc., 1989. S. Chib. Marginal likelihood from the Gibbs output. Journal of the American Statistical Association, 90(432):1313–1321, 1995. J. DeRisi, V. Iyer, and P. Brown. Exploring the matebolic and genetic control of gene expression on a genomic scale. Science, 282:699–705, 1997. J. Dongarra, C. Moler, J. Bunch, and G. Stewart. LINPACK User’s Guide. Society for Industrial and Applied Mathematics, 1979. D. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289–1306, 2006. D. Donoho and M. Elad. Optimally sparse representation in general (nonorthogonal) dictionaries via l1 minimization. Proc. Natl. Acad. Sci. USA, 100:2197–2202, 2003. A. Faul and M. Tipping. Analysis of sparse Bayesian learning. In T. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 383–389. MIT Press, 2002. V. Fedorov. Theory of Optimal Experiments. Academic Press, 1972. 809  S EEGER  M. Figueiredo. Adaptive sparseness for supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(9):1050–1059, 2003. T. Gardner, C. Cantor, and J. Collins. Construction of a genetic toggle switch in Escherichia coli. Nature, 403(6767):339–342, 2000. S. Gerwinn, J. Macke, M. Seeger, and M. Bethge. Bayesian inference for spiking neuron models with a sparsity prior. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20. MIT Press, 2008. Z. Ghahramani and M. Beal. Propagation algorithms for variational Bayesian learning. In T. Leen, T. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 507–513. MIT Press, 2001. W. Gilks, S. Richardson, and D. Spiegelhalter, editors. Markov Chain Monte Carlo in Practice. Chapman & Hall, 1st edition, 1996. W. R. Gilks and P. Wild. Adaptive rejection sampling for Gibbs sampling. Applied Statistics, 41(2): 337–348, 1992. M. Girolami. A variational method for learning sparse and overcomplete representations. Neural Computation, 13:2517–2532, 2001. T. Gneiting. Normal scale mixtures and dual probability densities. J. Statist. Comput. Simul., 59: 375–384, 1997. T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. The entire regularization path for the support vector machine. Journal of Machine Learning Research, 5:1391–1415, 2004. H. Henderson and S. Searle. On deriving the inverse of a sum of matrices. SIAM Review, 23:53–60, 1981. P. Hojen-Sorensen, O. Winther, and L. Hansen. Mean ﬁeld approaches to independent component analysis. Neural Computation, 14:889–918, 2002. R. Horn and C. Johnson. Matrix Analysis. Cambridge University Press, 1st edition, 1985. H. Ishwaran and J. Rao. Spike and slab gene selection for multigroup microarray data. Journal of the American Statistical Association, 100(471):764–780, 2005. T. Jaakkola. Variational Methods for Inference and Estimation in Graphical Models. PhD thesis, Massachusetts Institute of Technology, 1997. S. Ji and L. Carin. Bayesian compressive sensing and projection optimization. In Z. Ghahramani, editor, International Conference on Machine Learning 24. Omni Press, 2007. M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. An introduction to variational methods in graphical models. In M. I. Jordan, editor, Learning in Graphical Models. Kluwer, 1997. B. Kholodenko, A. Kiyatkin, F. Bruggeman, E. Sontag, H. Westerhoff, and J. Hoek. Untangling the wires: A strategy to trace functional interactions in signaling and gene networks. PNAS, 99(20): 12841–12846, 2002. 810  BAYESIAN I NFERENCE FOR S PARSE L INEAR M ODEL  H. Kushner and A. Budhiraja. A nonlinear ﬁltering algorithm based on an approximation of the conditional distribution. IEEE Transactions on Automatic Control, 45:580–585, 2000. M. Kuss and C. Rasmussen. Assessing approximate inference for binary Gaussian process classiﬁcation. Journal of Machine Learning Research, 6:1679–1704, 2005. N. D. Lawrence, M. Seeger, and R. Herbrich. Fast sparse Gaussian process methods: The informative vector machine. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 609–616. MIT Press, 2003. J. Lewi, R. Butera, and L. Paninski. Real-time adaptive information-theoretic optimization of neu¨ rophysiological experiments. In B. Scholkopf, J. Platt, and T. Hofmann, editors, Advances in Neural Information Processing Systems 19. MIT Press, 2007. M. Lewicki and B. Olshausen. Probabilistic framework for the adaption and comparison of image codes. J. Opt. Soc. Amer. A, 16(7):1587–1601, 1999. L. Lov´ sz and S. Vempala. Hit and run is fast and fun. Technical Report MSR-TR-2003-05, a Microsoft Research, Redmond, January 2003. D. MacKay. Information-based objective functions for active data selection. Neural Computation, 4(4):589–603, 1991. D. MacKay. Bayesian interpolation. Neural Computation, 4(3):415–447, 1992. T. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, Massachusetts Institute of Technology, January 2001a. T. Minka. Expectation propagation for approximate Bayesian inference. In J. Breese and D. Koller, editors, Uncertainty in Artiﬁcial Intelligence 17. Morgan Kaufmann, 2001b. T. Minka. Power EP. Technical report, Microsoft Research, Cambridge, 2004. R. M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical Report CRG-TR-93-1, University of Toronto, 1993. See www.cs.toronto.edu/˜radford. R. M. Neal. Bayesian Learning for Neural Networks. Number 118 in Lecture Notes in Statistics. Springer, 1996. A. O’Hagan. Bayesian Inference, volume 2B of Kendall’s Advanced Theory of Statistics. Arnold, London, 1994. B. Olshausen and D. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research, 37:3311–3325, 1997. M. Opper and O. Winther. Expectation consistent approximate inference. Journal of Machine Learning Research, 6:2177–2204, 2005. M. Opper and O. Winther. Gaussian processes for classiﬁcation: Mean ﬁeld algorithms. Neural Computation, 12(11):2655–2684, 2000. 811  S EEGER  A. Palmer, D. Wipf, K. Kreutz-Delgado, and B. Rao. Variational EM algorithms for non-Gaussian ¨ latent variable models. In Y. Weiss, B. Scholkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18. MIT Press, 2006. L. Paninski. Log-concavity results on Gaussian process methods for supervised and unsupervised learning. In L. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17. MIT Press, 2005. T. Park and G. Casella. The Bayesian Lasso. Technical report, University of Florida, 2005. R. Peeters and R. Westra. On the identiﬁcation of sparse gene regulatory networks. In Proc. 16th Int. Symp. on Math. Theory of Networks, 2004. J. Pratt. Concavity of the log likelihood. Journal of the American Statistical Association, 76(373): 103–106, 1981. Y. Qi, T. Minka, R. Picard, and Z. Ghahramani. Predictive automatic relevance determination by expectation propagation. In C. Brodley, editor, International Conference on Machine Learning 21. Morgan Kaufmann, 2004. L. Rabiner and B. Juang. Fundamentals of Speech Recognition. Prentice Hall, 1st edition, 2003. S. Rogers and M. Girolami. A Bayesian regression approach to the inference of regulatory networks from gene expression data. Bioinformatics, 21(14):3131–3137, 2005. Y. Saad. Iterative Methods for Sparse Linear Systems. International Thomson Publishing, 1st edition, 1996. M. Seeger. Bayesian Gaussian Process Models: PAC-Bayesian Generalisation Error Bounds and Sparse Approximations. PhD thesis, University of Edinburgh, July 2003. See www.kyb.tuebingen.mpg.de/bs/people/seeger. M. Seeger. Low rank updates for the Cholesky decomposition. Technical report, University of California at Berkeley, 2004. See www.kyb.tuebingen.mpg.de/bs/people/seeger. M. Seeger. Expectation propagation for exponential families. Technical report, University of California at Berkeley, 2005. See www.kyb.tuebingen.mpg.de/bs/people/seeger. M. Seeger and H. Nickisch. Compressed sensing and Bayesian experimental design. To appear at ICML, 2008. M. Seeger, S. Gerwinn, and M. Bethge. Bayesian inference for sparse generalized linear models. In J. Kok, J. Koronacki, R. Lopez, S. Matwin, D. Mladenic, and A. Skowron, editors, European Conference on Machine Learning 18. Springer, 2007a. M. Seeger, F. Steinke, and K. Tsuda. Bayesian inference and optimal design in the sparse linear model. In M. Meila and X. Shen, editors, Workshop on Artiﬁcial Intelligence and Statistics 11, 2007b. H. S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Conference on Computational Learning Theory 5, pages 287–294. Morgan Kaufmann, 1992. 812  BAYESIAN I NFERENCE FOR S PARSE L INEAR M ODEL  D. Spiegelhalter, A. Thomas, N. Best, and W. Gilks. BUGS: Bayesian inference using Gibbs sampling. Technical report, MRC Biostatistics Unit, Cambridge University, 1995. F. Steinke, M. Seeger, and K. Tsuda. Experimental design for efﬁcient identiﬁcation of gene regulatory networks using sparse Bayesian models. BMC Systems Biology, 1(51), 2007. J. Tegn´ r, M. Yeung, J. Hasty, and J. Collins. Reverse engineering gene networks: Integrating e genetic perturbations with dynamical modeling. PNAS, 100(10):5944–5949, 2003. R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of Roy. Stat. Soc. B, 58: 267–288, 1996. M. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1:211–244, 2001. M. J. Wainwright. Sharp thresholds for noisy and high-dimensional recovery of sparsity using l 1 constrained quadratic programming. Technical Report 709, UC Berkeley, Dept. of Statistics, 2006. M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Technical Report 649, UC Berkeley, Dept. of Statistics, 2003. D. Wipf, J. Palmer, and B. Rao. Perspectives on sparse Bayesian learning. In S. Thrun, L. Saul, and B. Sch¨ lkopf, editors, Advances in Neural Information Processing Systems 16. MIT Press, 2004. o D. Wipf, J. Palmer, B. Rao, and K. Kreutz-Delgado. Performance analysis of latent variable models with sparse priors. In Proceedings of ICASSP 2007, 2007. O. Zoeter and T. Heskes. Gaussian quadrature based expectation propagation. In Z. Ghahramani and R. Cowell, editors, Workshop on Artiﬁcial Intelligence and Statistics 10, 2005.  813</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
