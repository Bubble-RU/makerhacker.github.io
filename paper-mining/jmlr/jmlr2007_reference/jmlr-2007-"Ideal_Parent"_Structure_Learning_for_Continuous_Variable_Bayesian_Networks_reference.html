<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-1" href="../jmlr2007/jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">jmlr2007-1</a> <a title="jmlr-2007-1-reference" href="#">jmlr2007-1-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</h1>
<br/><p>Source: <a title="jmlr-2007-1-pdf" href="http://jmlr.org/papers/volume8/elidan07a/elidan07a.pdf">pdf</a></p><p>Author: Gal Elidan, Iftach Nachman, Nir Friedman</p><p>Abstract: Bayesian networks in general, and continuous variable networks in particular, have become increasingly popular in recent years, largely due to advances in methods that facilitate automatic learning from data. Yet, despite these advances, the key task of learning the structure of such models remains a computationally intensive procedure, which limits most applications to parameter learning. This problem is even more acute when learning networks in the presence of missing values or hidden variables, a scenario that is part of many real-life problems. In this work we present a general method for speeding structure search for continuous variable networks with common parametric distributions. We efﬁciently evaluate the approximate merit of candidate structure modiﬁcations and apply time consuming (exact) computations only to the most promising ones, thereby achieving signiﬁcant improvement in the running time of the search algorithm. Our method also naturally and efﬁciently facilitates the addition of useful new hidden variables into the network structure, a task that is typically considered both conceptually difﬁcult and computationally prohibitive. We demonstrate our method on synthetic and real-life data sets, both for learning structure on fully and partially observable data, and for introducing new hidden variables during structure search. Keywords: Bayesian networks, structure learning, continuous variables, hidden variables</p><br/>
<h2>reference text</h2><p>D. M. Chickering. Learning Bayesian networks is NP-complete. In D. Fisher and H. J. Lenz, editors, Learning from Data: Artiﬁcial Intelligence and Statistics V, pages 121–130. SpringerVerlag, New York, 1996a. D. M. Chickering. Learning equivalence classes of Bayesian network structures. In E. Horvitz and F. Jensen, editors, Proc. Twelfth Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’96), pages 150–157, San Francisco, 1996b. Morgan Kaufmann. S. Della Pietra, V. Della Pietra, and J. Lafferty. Inducing features of random ﬁelds. IEEE Trans. on Pattern Analysis and Machine Intelligence, 19(4):380–393, 1997. A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B 39:1–39, 1977. R. O. Duda and P. E. Hart. Pattern Classiﬁcation and Scene Analysis. John Wiley & Sons, New York, 1973. G. Elidan and N. Friedman. The information bottleneck EM algorithm. In C. Meek and U. Kjærulff, editors, Proc. Nineteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’03), pages 200–208, San Francisco, 2003. Morgan Kaufmann. G. Elidan, N. Lotner, N. Friedman, and D. Koller. Discovering hidden variables: A structure-based approach. In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 479–485, Cambridge, Mass., 2001. MIT Press. 1831  E LIDAN , NACHMAN AND F RIEDMAN  N. Friedman. Learning belief networks in the presence of missing values and hidden variables. In D. Fisher, editor, Proc. Fourteenth International Conference on Machine Learning, pages 125– 133. Morgan Kaufmann, San Francisco, 1997. N. Friedman, M. Linial, I. Nachman, and D. Pe’er. Using Bayesian networks to analyze expression data. Computational Biology, 7:601–620, 2000. N. Friedman, I. Nachman, and D. Pe’er. Learning Bayesian network structure from massive data sets: The ‘sparse candidate” algorithm. In K. Laskey and H. Prade, editors, Proc. Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’99), page 206–215, San Francisco, 1999. A. P. Gasch, P. T. Spellman, C. M. Kao, O. Carmel-Harel, M. B. Eisen, G. Storz, D. Botstein, and P. O. Brown. Genomic expression program in the response of yeast cells to environmental changes. Molecular Biology of the Cell, 11:4241–4257, 2000. ´ D. Geiger and D. Heckerman. Learning Gaussian networks. In R. L opez de Mantar´ s and D. Poole, a editors, Proc. Tenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’94), pages 235– 243, San Francisco, 1994. Morgan Kaufmann. F. Glover and M. Laguna. Tabu search. In C. Reeves, editor, Modern Heuristic Techniques for Combinatorial Problems, Oxford, England, 1993. Blackwell Scientiﬁc Publishing. M. I. Jordan, Z. Ghahramani, T. Jaakkola, and L. K. Saul. An introduction to variational approximations methods for graphical models. In M. I. Jordan, editor, Learning in Graphical Models. Kluwer, Dordrecht, Netherlands, 1998. M. Koivisto and K. Sood. Exact Bayesian structure discovery in Bayesian networks. Journal of Machine Learning Research, 5:549–573, 2004. S. L. Lauritzen and N. Wermuth. Graphical models for associations between variables, some of which are qualitative and some quantitative. Annals of Statistics, 17:31–57, 1989. J. Martin and K. VanLehn. Discrete factor analysis: Learning hidden variables in Bayesian networks. Technical report, Department of Computer Science, University of Pittsburgh, 1995. P. McCullagh and J.A. Nelder. Generalized Linear Models. Chapman & Hall, London, 1989. A. Moore and W. Wong. Optimal reinsertion: A new search operator for accelerated and more accurate Bayesian network structure learning. In T. Fawcett and N. Mishra, editors, Proceedings of the 20th International Conference on Machine Learning (ICML ’03), pages 552–559, Menlo Park, California, 2003. K. Murphy and Y. Weiss. Loopy belief propagation for approximate inference: An empirical study. In K. Laskey and H. Prade, editors, Proc. Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’99), page 467–475, San Francisco, 1999. Morgan Kaufmann. I. Nachman, A. Regev, and N. Friedman. Inferring quantitative models of regulatory networks from expression data. Bioinformatics, 20(Suppl 1):S1248–1256, 2004. 1832  T HE “I DEAL PARENT ” A LGORITHM  G. Schwarz. Estimating the dimension of a model. Annals of Statistics, 6:461–464, 1978. M.A. Shwe, B. Middleton, D.E. Heckerman, M. Henrion, E.J. Horvitz, H.P. Lehmann, and G.F. Cooper. Probabilistic diagnosis using a reformulation of the INTERNIST-1/QMR knowledge base. I. The probabilistic model and inference algorithms. Methods of Information in Medicine, 30:241–55, 1991. T. Silander and P. Myllym. A simple approach for ﬁnding the globally optimal Bayesian network structure. In Dechter and Richardson, editors, Proc. Twenty Second Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’06), San Francisco, 2006. Morgan Kaufmann. A. Singh and A. Moore. Finding optimal Bayesian networks by dynamic programming. Technical report, Carnegie Mellon University, 2005. P. T. Spellman, G. Sherlock, M. Q. Zhang, V. R. Iyer, K. Anders, M. B. Eisen, P. O. Brown, D. Botstein, and B. Futcher. Comprehensive identiﬁcation of cell cycle-regulated genes of the yeast saccharomyces cerevisiae by microarray hybridization. Molecular Biology of the Cell, 9(12): 3273–97, 1998. M. Teyssier and D. Koller. Ordering-based search: A simple and effective algorithm for learning Bayesian networks. In F. Bacchus and T. Jaakkola, editors, Proc. Twenty First Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’05), pages 584–590, San Francisco, 2005. Morgan Kaufmann. R. Parr U. Lerner and D. Koller. Bayesian fault detection and diagnosis in dynamic systems. In Proc. of the Seventeenth National Conference on Artiﬁcial Intelligence (AAAI), pages 531–537, 2000. J. Wilkinson. The Algebric Eigenvalue Problem. Claderon Press, Oxford, 1965. N.L. Zhang. Hierarchical latent class models for cluster analysis. Journal of Machine Learning Research, 5:697–723, 2004.  1833</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
