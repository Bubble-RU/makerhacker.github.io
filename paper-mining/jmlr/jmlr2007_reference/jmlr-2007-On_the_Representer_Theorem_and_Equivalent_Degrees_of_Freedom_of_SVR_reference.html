<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-63" href="../jmlr2007/jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">jmlr2007-63</a> <a title="jmlr-2007-63-reference" href="#">jmlr2007-63-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</h1>
<br/><p>Source: <a title="jmlr-2007-63-pdf" href="http://jmlr.org/papers/volume8/dinuzzo07a/dinuzzo07a.pdf">pdf</a></p><p>Author: Francesco Dinuzzo, Marta Neve, Giuseppe De Nicolao, Ugo Pietro Gianazza</p><p>Abstract: Support Vector Regression (SVR) for discrete data is considered. An alternative formulation of the representer theorem is derived. This result is based on the newly introduced notion of pseudoresidual and the use of subdifferential calculus. The representer theorem is exploited to analyze the sensitivity properties of ε-insensitive SVR and introduce the notion of approximate degrees of freedom. The degrees of freedom are shown to play a key role in the evaluation of the optimism, that is the difference between the expected in-sample error and the expected empirical risk. In this way, it is possible to deﬁne a C p -like statistic that can be used for tuning the parameters of SVR. The proposed tuning procedure is tested on a simulated benchmark problem and on a real world problem (Boston Housing data set). Keywords: statistical learning, reproducing kernel Hilbert spaces, support vector machines, representer theorem, regularization theory</p><br/>
<h2>reference text</h2><p>J. M. Borwein and A. J. Lewis. Convex Analisys and Nonlinear Optimization. Springer, 2000. M. W. Chang and C. J. Lin. Leave-one-out bounds for support vector regression model selection. Neural Computation, 17:1188–1222, 2005. V. Cherkassky and Y. Ma. Comparison of model selection for regression. Neural Computation, 15: 1691–1714, 2003. D. Cox and F. O’ Sullivan. Asymptotic analysis of penalized likelihood and related estimators. Ann.Stat., 18:1676–1695, 1990. F. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin of AMS, 39:1–49, 2001. 2493  D INUZZO , N EVE , G IANAZZA AND D E N ICOLAO  G. De Nicolao, G. Sparacino, and C. Cobelli. Nonparametric input estimation in physiological systems: Problems, methods, and case studies. Automatica, 33:851–870, 1997. G. De Nicolao, G. Ferrari Trecate, and G. Sparacino. Fast spline smoothing via spectral factorization concepts. Automatica, 36:1733–1739, 2000. E. De Vito, L. Rosasco, A. Caponnetto, M. Piana, and A. Verri. Some properties of regularized kernel methods. Journal of Machine Learning Research, 5:1363–1390, 2004. B. Efron. How biased is the apparent error rate of a prediction rule? Journal of the American Statistical Association, 81(394):461–470, 1986. I. Ekeland and R. Temam. Analyse Convexe et Probl´ mes Variationnels. Gauthier-Villards, Paris, e 1974. T. Evgeniou, M. Pontil, and T. Poggio. Regularization networks and support vector machines. Advances in Computational Mathematics, 13:1–150, 2000. J. B. Gao, S. R. Gunn, C. J. Harris, and M. Brown. A probabilistic framework for SVM regression and error bar estimation. Machine Learning, 46:71–89, 2002. L. Gunter and J. Zhu. Efﬁcient computation and model selection for the support vector regression. Neural Computation, 19:1633–1655, 2007. T. J. Hastie and R. J. Tibshirani. Generalized additive models. In Monographs on Statistics and Applied Probability, volume 43. Chapman and Hall, London, UK, 1990. T. J. Hastie, R. J. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Data Mining, Inference and Prediction. Springer, Canada, 2001. T. J. Hastie, R. J. Tibshirani, and J. Friedman. Note on “Comparison of Model Selection for Regression” by Vladimir Cherkassky and Yunqian Ma. Neural Computation, 15:1477–1480, 2003. T. J. Hastie, S. Rosset, R. J. Tibshirani, and J. Zhu. The entire regularization path for the support vector machine. Journal of Machine Learning Research, 5:1391–1415, 2004. G. Kimeldorf and G. Wahba. A correspondence between Bayesian estimation of stochastic processes and smoothing by splines. Ann. Math. Stat., 41:495–502, 1979. J. T. Kwok and I. W. Tsang. Linear dependency between ε and the input noise in ε-support vector regression. IEEE Transactions On Neural Networks, XX, 2003. C. Loader. Local Regression and Likehood. Statistics and Computing. Springer, 1999. T. Poggio and F. Girosi. A theory of networks for approximation and learning. Foundation of Neural Networks, page 91–106, 1992. M. Pontil and A. Verri. Properties of support vector machines. Neural Computation, 10:955–974, 1998. B. Sch¨ lkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett. New support vector algorithms. o Neural Computation, 12:1207–1245, 2000. 2494  R EPRESENTER T HEOREM AND D EGREES OF F REEDOM OF SVR  B. Sch¨ lkopf, R. Herbrich, and A. J. Smola. A generalized representer theorem. Neural Networks o and Computational Learning Theory, 81:416–426, 2001. A. J. Smola, N. Murata, B. Sch¨ lkopf, and K. Muller. Asymptotically optimal choice of ε-loss for o support vector machines. In L. Niklasson, M. Boden, and T. Ziemke, editors, Proceedings of the 8th International Conference on Artiﬁcial Neural Networks, Perspectives in Neural Computing, pages 105–110, Berlin, 1998. Springer. C. Stein. Estimation of the mean of a multivariate normal distribution. Annals of Statistics, 9: 1135–1151, 1981. I. Steinwart. Sparseness of support vector machines. Journal of Machine Learning Research, 4: 1071–1105, 2003. A. N. Tikhonov and V. Y. Arsenin. Solutions of Ill Posed Problems. W. H. Winston, Washington, D. C., 1977. V. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, NY, USA, 1995. G. Wahba. Spline Models for Observational Data. SIAM, Philadelphia, USA, 1990. G. Wahba. Support vector machines, reproducing kernel Hilbert spaces and randomized GACV. Technical Report 984, Department of Statistics, University of Wisconsin, 1998.  2495</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
