<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-68" href="../jmlr2007/jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">jmlr2007-68</a> <a title="jmlr-2007-68-reference" href="#">jmlr2007-68-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</h1>
<br/><p>Source: <a title="jmlr-2007-68-pdf" href="http://jmlr.org/papers/volume8/cawley07a/cawley07a.pdf">pdf</a></p><p>Author: Gavin C. Cawley, Nicola L. C. Talbot</p><p>Abstract: While the model parameters of a kernel machine are typically given by the solution of a convex optimisation problem, with a single global optimum, the selection of good values for the regularisation and kernel parameters is much less straightforward. Fortunately the leave-one-out cross-validation procedure can be performed or a least approximated very efﬁciently in closed form for a wide variety of kernel learning methods, providing a convenient means for model selection. Leave-one-out cross-validation based estimates of performance, however, generally exhibit a relatively high variance and are therefore prone to over-ﬁtting. In this paper, we investigate the novel use of Bayesian regularisation at the second level of inference, adding a regularisation term to the model selection criterion corresponding to a prior over the hyper-parameter values, where the additional regularisation parameters are integrated out analytically. Results obtained on a suite of thirteen real-world and synthetic benchmark data sets clearly demonstrate the beneﬁt of this approach. Keywords: model selection, kernel methods, Bayesian regularisation</p><br/>
<h2>reference text</h2><p>D. M. Allen. The relationship between variable selection and prediction. Technometrics, 16:125– 127, 1974. E. Anderson, Z. Bai, C. Bischof, S. Blackford, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, and D. Sorenson. LAPACK Users’ Guide. SIAM Press, third edition, 1999. C. M. Bishop. Curvature-driven smoothing: a learning algorithm for feedforward networks. IEEE Transactions on Neural Networks, 4(5):882–884, September 1993. C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995. L. Bo, L. Wang, and L. Jiao. Feature scaling for kernel Fisher discriminant analysis using leaveone-out cross-validation. Neural Computation, 18:961–978, April 2006. W. L. Buntine and A. S. Weigend. Bayesian back-propagation. Complex Systems, 5:603–643, 1991. G. C. Cawley. Leave-one-out cross-validation based model selection criteria for weighted LSSVMs. In Proceedings of the International Joint Conference on Neural Networks (IJCNN-2006), pages 2970–2977, Vancouver, BC, Canada, July 16–21 2006. G. C. Cawley and N. L. C. Talbot. Efﬁcient leave-one-out cross-validation of kernel Fisher discriminant classiﬁers. Pattern Recognition, 36(11):2585–2592, November 2003. G. C. Cawley and N. L. C. Talbot. Fast leave-one-out cross-validation of sparse least-squares support vector machines. Neural Networks, 17(10):1467–1475, December 2004. G. C. Cawley and N. L. C. Talbot. Approximate leave-one-out cross-validation for kernel logistic regression. Machine Learning (submitted), 2007. 858  BAYESIAN R EGULARISATION IN M ODEL S ELECTION  C. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector machines. Machine Learning, 46(1):131–159, 2002. C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:273–297, 1995. J. Demˇar. Statistical comparisons of classiﬁers over multiple data sets. Journal of Machine Learns ing Research, 7:1–30, 2006. H. Drucker and Y. Le Cun. Improving generalization performance using double back-propagation. IEEE Transactions on Neural Networks, 3(6):991–997, 1992. S. Geman, E. Bienenstock, and R. Doursat. Neural networks and the bias/variance dilema. Neural Computation, 4(1):1–58, 1992. G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore, third edition edition, 1996. I. S. Gradshteyn and I. M. Ryzhic. Table of Integrals, Series and Products. Academic Press, ﬁfth edition, 1994. I. Guyon, A. R. Saffari Azar Alamdari, G. Dror, and J. Buhmann. Performance prediction challenge. In Proceedings of the International Joint Conference on Neural Networks (IJCNN-2006), pages 1649–1656, Vancouver, BC, Canada, July 16–21 2006. T. Joachims. Learning to Classify Text using Support Vector Machines - Methods, Theory and Algorithms. Kluwer Academic Publishers, 2002. S. S. Keerthi, K. B. Duan, S. K. Shevade, and A. N. Poo. A fast dual algorithm for kernel logistic regression. Machine Learning, 61(1–3):151–165, November 2005. G. S. Kimeldorf and G. Wahba. Some results on Tchebychefﬁan spline functions. J. Math. Anal. Applic., 33:82–95, 1971. R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence (IJCAI), pages 1137–1143, San Mateo, CA, 1995. Morgan Kaufmann. P. A. Lachenbruch and M. R. Mickey. Estimation of error rates in discriminant analysis. Technometrics, 10(1):1–11, February 1968. A. Luntz and V. Brailovsky. On estimation of characters obtained in statistical procedure of recognition (in Russian). Techicheskaya Kibernetica, 3, 1969. D. J. C. MacKay. Bayesian interpolation. Neural Computation, 4(3):415–447, 1992a. D. J. C. MacKay. A practical Bayesian framework for backprop networks. Neural Computation, 4 (3):448–472, 1992b. D. J. C. MacKay. The evidence framework applied to classiﬁcation networks. Neural Computation, 4(5):720–736, 1992c. 859  C AWLEY AND TALBOT  D. J. C. MacKay. Hyperparameters: Optimise or integrate out? In G. Heidbreder, editor, Maximum Entropy and Bayesian Methods. Kluwer, 1994. J. Mercer. Functions of positive and negative type and their connection with the theory of integral equations. Philosophical Transactions of the Royal Society of London, A, 209:415–446, 1909. C. A. Micchelli. Interpolation of scattered data: Distance matrices and conditionally positive deﬁnite functions. Constructive Approximation, 2:11–22, 1986. S. Mika, G. R¨ tsch, J. Weston, B. Sch¨ lkopf, and K.-R. M¨ ller. Fisher discriminant analysis with a o u kernels. In Neural Networks for Signal Processing, volume IX, pages 41–48. IEEE Press, New York, 1999. ¨ S. Mika, G. R¨ tsch, J. Weston, B. Sch¨ lkopf, A. J. Smola, and K.-R. Muller. Invariant feature a o ¨ extraction and classiﬁcation in feature spaces. In S. A. Solla, T. K. Leen, and K.-R. M uller, editors, Advances in Neural Information Processing Systems, volume 12, pages 526–532. MIT Press, 2000. T. P. Minka. Expectation propagation for approximate Bayesian inference. In Proceedings of the 17th Annual Conference on Uncertainty in Artiﬁcial Intelligence, pages 362–369. Morgan Kauffmann, 2001. J. A. Nelder and R. Mead. A simplex method for function minimization. Computer Journal, 7: 308–313, 1965. Y. Qi, T. P. Minka, R. W. Picard, and Z. Ghahramani. Predictive automatic relevance determination by expectation propagation. In Proceedings of the 21 st International Conference on Machine Learning, pages 671–678, Banff, Alberta, Canada, July 4–8 2004. C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. Adaptive Computation and Machine Learning. MIT Press, 2006. ¨ G. R¨ tsch, T. Onoda, and K.-R. Muller. Soft margins for AdaBoost. Machine Learning, 42(3): a 287–320, 2001. K. Saadi, N. L. C. Talbot, and G. C. Cawley. Optimally regularised kernel Fisher discriminant analysis. In Proceedings of the 17th International Conference on Pattern Recognition (ICPR2004), volume 2, pages 427–430, Cambridge, United Kingdom, August 23–26 2004. W. S. Sarle. Stopped training and other remidies for overﬁtting. In Proceedings of the 27th Symposium on the Interface of Computer Science and Statistics, pages 352–360, Pittsburgh, PA, USA, June 21–24 1995. B. Sch¨ lkopf, K. Tsuda, and J.-P. Vert. Kernel Methods in Computational Biology. MIT Press, o 2004. T. Seaks. SYMINV : An algorithm for the inversion of a positive deﬁnite matrix by the Cholesky decomposition. Econometrica, 40(5):961–962, September 1972. J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004. 860  BAYESIAN R EGULARISATION IN M ODEL S ELECTION  M. Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the Royal Statistical Society, B 36(1):111–147, 1974. S. Sundararajan and S. S. Keerthi. Predictive approaches for choosing hyperparameters in Gaussian processes. Neural Computation, 13(5):1103–1118, May 2001. J. A. K. Suykens and J. Vandewalle. Least squares support vector machine classiﬁers. Neural Processing Letters, 9(3):293–300, June 1999. J. A. K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, and J. Vandewalle. Least Squares Support Vector Machines. World Scientiﬁc, 2002. A. N. Tikhonov and V. Y. Arsenin. Solutions of Ill-posed Problems. John Wiley, New York, 1977. M. E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1:211–244, June 2000. G. Wahba. Spline Models for Observational Data. SIAM Press, Philadelphia, PA, 1990. C. K. I. Williams and D. Barber. Bayesian classiﬁcation with Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342–1351, December 1998. P. M. Williams. A Marquardt algorithm for choosing the step size in backpropagation learning with conjugate gradients. Technical Report CSRP-229, University of Sussex, February 1991. P. M. Williams. Bayesian regularization and pruning using a Laplace prior. Neural Computation, 7 (1):117–143, 1995.  861</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
