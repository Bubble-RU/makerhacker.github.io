<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-75" href="../jmlr2007/jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">jmlr2007-75</a> <a title="jmlr-2007-75-reference" href="#">jmlr2007-75-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</h1>
<br/><p>Source: <a title="jmlr-2007-75-pdf" href="http://jmlr.org/papers/volume8/bartlett07a/bartlett07a.pdf">pdf</a></p><p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: One of the nice properties of kernel classiﬁers such as SVMs is that they often produce sparse solutions. However, the decision functions of these classiﬁers cannot always be used to estimate the conditional probability of the class label. We investigate the relationship between these two properties and show that these are intimately related: sparseness does not occur when the conditional probabilities can be unambiguously estimated. We consider a family of convex loss functions and derive sharp asymptotic results for the fraction of data that becomes support vectors. This enables us to characterize the exact trade-off between sparseness and the ability to estimate conditional probabilities for these loss functions. Keywords: kernel methods, support vector machines, sparseness, estimating conditional probabilities</p><br/>
<h2>reference text</h2><p>Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, Cambridge, 1999. Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Large margin classiﬁers: Convex loss, low noise and convergence rates. In Advances in Neural Information Processing Systems 16. MIT Press, 2004. Anthony V. Fiacco. Introduction to Sensitivity and Stability Analysis in Nonlinear Programming. Academic Press, New York, 1983. G´ bor Lugosi and Nicolas Vayatis. On the Bayes-risk consistency of regularized boosting methods. a Annals of Statistics, 32(1):30–55, 2004. David Pollard. Convergence of Stochastic Processes. Springer-Verlag, New York, 1984. R Tyrrell Rockafellar. Convex Analysis. Princeton University Press, Princeton, 1970. Ingo Steinwart. Sparseness of support vector machines. Journal of Machine Learning Research, 4: 1071–1105, 2003. Ingo Steinwart. Sparseness of support vector machines – some asymptotically sharp bounds. In Advances in Neural Information Processing Systems 16. MIT Press, 2004. Ingo Steinwart. Consistency of support vector machines and other regularized kernel classiﬁers. IEEE Transactions on Information Theory, 51(1):128–142, 2005. 789  BARTLETT AND T EWARI  Grace Wahba. Soft and hard classiﬁcation by reproducing kernel Hilbert space methods. Proceedings of the National Academy of Sciences USA, 99(26):16524–16530, 2002. Tong Zhang. Covering number bounds of certain regularized linear function classes. Journal of Machine Learning Research, 2:527–550, 2002. Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. Annals of Statistics, 32(1):56–85, 2004.  790</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
