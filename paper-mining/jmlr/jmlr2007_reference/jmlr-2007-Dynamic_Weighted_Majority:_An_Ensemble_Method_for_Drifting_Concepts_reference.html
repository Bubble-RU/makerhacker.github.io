<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-29" href="../jmlr2007/jmlr-2007-Dynamic_Weighted_Majority%3A_An_Ensemble_Method_for_Drifting_Concepts.html">jmlr2007-29</a> <a title="jmlr-2007-29-reference" href="#">jmlr2007-29-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</h1>
<br/><p>Source: <a title="jmlr-2007-29-pdf" href="http://jmlr.org/papers/volume8/kolter07a/kolter07a.pdf">pdf</a></p><p>Author: J. Zico Kolter, Marcus A. Maloof</p><p>Abstract: We present an ensemble method for concept drift that dynamically creates and removes weighted experts in response to changes in performance. The method, dynamic weighted majority (DWM), uses four mechanisms to cope with concept drift: It trains online learners of the ensemble, it weights those learners based on their performance, it removes them, also based on their performance, and it adds new experts based on the global performance of the ensemble. After an extensive evaluation— consisting of ﬁve experiments, eight learners, and thirty data sets that varied in type of target concept, size, presence of noise, and the like—we concluded that DWM outperformed other learners that only incrementally learn concept descriptions, that maintain and use previously encountered examples, and that employ an unweighted, ﬁxed-size ensemble of experts. Keywords: concept learning, online learning, ensemble methods, concept drift</p><br/>
<h2>reference text</h2><p>D. W. Aha, D. Kibler, and M. K. Albert. Instance-based learning algorithms. Machine Learning, 6: 37–66, 1991. A. Asuncion and D. J. Newman. UCI Machine Learning Repository. Web site, Department of Information and Computer Sciences, University of California, Irvine, http://www.ics.uci. edu/˜mlearn/MLRepository.html, 2007. P. Auer and M. K. Warmuth. Tracking the best disjunction. Machine Learning, 32(2):127–150, 1998. B. Bauer and R. Kohavi. An empirical comparison of voting classiﬁcation algorithms: Bagging, boosting, and variants. Machine Learning, 36(1–2):105–139, 1999. M. M. Black and R. J. Hickey. Maintaining the performance of a learned classiﬁer under concept drift. Intelligent Data Analysis, 3(6):453–474, 1999. M. M. Black and R. J. Hickey. Classiﬁcation of customer call data in the presence of concept drift and noise. In Soft-Ware 2002: Computing in an Imperfect World, volume 2311 of Lecture Notes in Computer Science, pages 74–87. Springer, Berlin, 2002. First International Conference, Soft-Ware 2002, Belfast, Northern Ireland, April 8–10, 2002. Proceedings. A. Blum. Empirical support for Winnow and Weighted-Majority algorithms: Results on a calendar scheduling domain. Machine Learning, 26:5–23, 1997. B. E. Boser, I. Guyon, and V. Vapnik. A training algorithm for optimal margin classiﬁers. In Proceedings of the Fourth Workshop on Computational Learning Theory, pages 144–152. ACM Press, New York, NY, 1992. O. Bousquet and M. K. Warmuth. Tracking a small set of experts by mixing past posteriors. Journal of Machine Learning Research, 3:363–396, 2002. L. Breiman. Arcing classiﬁers. The Annals of Statistics, 26(3):801–849, 1998. L. Breiman. Bagging predictors. Machine Learning, 24:123–140, 1996. N. V. Chawla, L. O. Hall, K. W. Bowyer, and W. P. Kegelmeyer. Learning ensembles from bites: A scalable and accurate approach. Journal of Machine Learning Research, 5:421–451, 2004. S. J. Delany, P. Cunningham, A. Tsymbal, and L. Coyle. A case-based technique for tracking concept drift in spam ﬁltering. Knowledge-Based Systems, 18(4–5):187–195, 2005. T. G. Dietterich. An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization. Machine Learning, 40(2):139–158, 2000. P. Domingos and G. Hulten. Mining high-speed data streams. In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 71–80. ACM Press, New York, NY, 2000. 2785  KOLTER AND M ALOOF  W. Fan. StreamMiner: A classiﬁer ensemble-based engine to mine concept-drifting data streams. In Proceedings of the Thirtieth International Conference on Very Large Data Bases, pages 1257– 1260. Morgan Kaufmann, San Francisco, CA, 2004. W. Fan, S. J. Stolfo, and J. Zhang. The application of AdaBoost for distributed, scalable and on-line learning. In Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 362–366. ACM Press, New York, NY, 1999. A. Fern and R. Givan. Online ensemble learning: An empirical study. Machine Learning, 53: 71–109, 2003. Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm. In Proceedings of the Thirteenth International Conference on Machine Learning, pages 148–156. Morgan Kaufmann, San Francisco, CA, 1996. J. Gama, P. Medas, G. Castillo, and P. Rodrigues. Learning with drift detection. In Advances in Artiﬁcial Intelligence, volume 3171 of Lecture Notes in Computer Science, pages 286–295. Springer, Berlin, 2004. Seventeenth Brazilian Symposium on Artiﬁcial Intelligence, SBIA-2004, S˜ o Luis, Maranh˜ , Brazil, September 29–October 1, 2004, Proceedings. a o J. Gama, P. Medas, and P. Rodrigues. Learning decision trees from dynamic data streams. In Proceedings of the 2005 ACM Symposium on Applied Computing (SAC-2005), pages 573–577. ACM Press, New York, NY, 2005. R. B. Gramacy, M. K. Warmuth, S. A. Brandt, and I. Ari. Adaptive caching by refetching. In Advances in Neural Information Processing Systems 15, pages 1465–1472. MIT Press, Cambridge, MA, 2003. L. K. Hansen and P. Salamon. Neural network ensembles. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(10):993–1001, 1990. M. Harries. Splice-2 Comparative Evaluation: Electricity Pricing. Technical Report UNSW-CSETR-9905, Artiﬁcial Intelligence Group, School of Computer Science and Engineering, The University of New South Wales, Sidney, Australia, July 1999. M. Harries and K. Horn. Detecting concept drift in ﬁnancial time series prediction using symbolic machine learning. In Proceedings of the Eighth Australian Joint Conference on Artiﬁcial Intelligence, pages 91–98. World Scientiﬁc, Singapore, 1995. D. P. Helmbold and P. M. Long. Tracking drifting concepts using random examples. In L. G. Valiant and M. K. Warmuth, editors, Proceedings of the Fourth Annual Workshop on Computational Learning Theory (COLT’91), pages 13–23. Morgan Kaufmann, San Francisco, CA, 1991. D. P. Helmbold and P. M. Long. Tracking drifting concepts by minimising disagreements. Machine Learning, 14:27–45, 1994. M. Herbster and M. K. Warmuth. Tracking the best expert. Machine Learning, 32:151–178, 1998. 2786  DYNAMIC W EIGHTED M AJORITY  R. J. Hickey and M. M. Black. Reﬁned time stamps for concept drift detection during mining for classiﬁcation rules. In Temporal, Spatial, and Spatio-Temporal Data Mining, volume 2007 of Lecture Notes in Artiﬁcial Intelligence, pages 20–30. Springer, Berlin, 2000. First International Workshop, TSDM 2000, Lyon, France, September 2000, Revised papers. D. Hinkley. Jackknife methods. In S. Kotz, N. L. Johnson, and C. B. Read, editors, Encyclopedia of Statistical Sciences, volume 4, pages 280–287. John Wiley & Sons, New York, NY, 1983. W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58(301):13–30, 1963. G. Hulten, L. Spencer, and P. Domingos. Mining time-changing data streams. In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 97–106. ACM Press, New York, NY, 2001. M. G. Kelly, D. J. Hand, and N. M. Adams. The impact of changing populations on classiﬁer performance. In Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 367–371. ACM Press, New York, NY, 1999. M. Klenner and U. Hahn. Concept versioning: A methodology for tracking evolutionary concept drift in dynamic concept systems. In Proceedings of the Eleventh European Conference on Artiﬁcial Intelligence, pages 473–477. John Wiley & Sons, London, 1994. R. Klinkenberg and T. Joachims. Detecting concept drift with support vector machines. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 487–494. Morgan Kaufmann, San Francisco, CA, 2000. Ralf Klinkenberg. Learning drifting concepts: Example selection vs. example weighting. Intelligent Data Analysis, 8(3):281–300, 2004. Special Issue on Incremental Learning Systems Capable of Dealing with Concept Drift. J. Z. Kolter and M. A. Maloof. Dynamic weighted majority: A new ensemble method for tracking concept drift. In Proceedings of the Third IEEE International Conference on Data Mining, pages 123–130. IEEE Press, Los Alamitos, CA, 2003. J. Z. Kolter and M. A. Maloof. Using additive expert ensembles to cope with concept drift. In Proceedings of the Twenty-second International Conference on Machine Learning, pages 449– 456. ACM Press, New York, NY, 2005. A. Kuh, T. Petsche, and R. L. Rivest. Learning time-varying concepts. In Advances in Neural Information Processing Systems 3, pages 183–189. Morgan Kaufmann, San Francisco, CA, 1991. T. Lane and C. E. Brodley. Approaches to online learning and concept drift for user identiﬁcation in computer security. In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining, pages 259–263. AAAI Press, Menlo Park, CA, 1998. N. Littlestone. Redundant noisy attributes, attribute errors, and linear-threshold learning using Winnow. In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, pages 147–156. Morgan Kaufmann, San Francisco, CA, 1991. 2787  KOLTER AND M ALOOF  N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2:285–318, 1988. N. Littlestone and M. K. Warmuth. The Weighted Majority algorithm. Information and Computation, 108:212–261, 1994. R. Maclin and D. Opitz. An empirical evaluation of bagging and boosting. In Proceedings of the Fourteenth National Conference on Artiﬁcial Intelligence, pages 546–551. AAAI Press, Menlo Park, CA, 1997. M. A. Maloof. Concept drift. In J. Wang, editor, Encyclopedia of Data Warehousing and Mining, pages 202–206. Information Science Publishing, Hershey, PA, 2005. M. A. Maloof. Incremental rule learning with partial instance memory for changing concepts. In Proceedings of the International Joint Conference on Neural Networks, pages 2764–2769. IEEE Press, Los Alamitos, CA, 2003. M. A. Maloof and R. S. Michalski. Incremental learning with partial instance memory. Artiﬁcial Intelligence, 154:95–126, 2004. M. A. Maloof and R. S. Michalski. Selecting examples for partial memory learning. Machine Learning, 41:27–52, 2000. C. Mesterharm. Tracking linear-threshold concepts with Winnow. Journal of Machine Learning Research, 4:819–838, 2003. R. S. Michalski. On the quasi-minimal solution of the general covering problem. In Proceedings of the Fifth International Symposium on Information Processing, volume A3, pages 125–128. 1969. R. S. Michalski and J. B. Larson. Incremental Generation of VL 1 Hypotheses: The Underlying Methodology and the Description of Program AQ11. Technical Report UIUCDCS-F-83-905, Department of Computer Science, University of Illinois, Urbana, 1983. T. M. Mitchell, R. Caruana, D. Freitag, J. McDermott, and D. Zabowski. Experience with a learning personal assistant. Communications of the ACM, 37(7):80–91, July 1994. C. Monteleoni and T. S. Jaakkola. Online learning of non-stationary sequences. In Advances in Neural Information Processing Systems 16. MIT Press, Cambridge, MA, 2004. D. Opitz and R. Maclin. Popular ensemble methods: An empirical study. Journal of Artiﬁcial Intelligence Research, 11:169–198, 1999. J. R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Francisco, CA, 1993. J. R. Quinlan. Bagging, boosting, and C4.5. In Proceedings of the Thirteenth National Conference on Artiﬁcial Intelligence, pages 725–730. AAAI Press, Menlo Park, CA, 1996. J. R. Quinlan. Induction of decision trees. Machine Learning, 1:81–106, 1986. 2788  DYNAMIC W EIGHTED M AJORITY  R. E. Reinke and R. S. Michalski. Incremental learning of concept descriptions: A method and experimental results. In J. E. Hayes, D. Michie, and J. Richards, editors, Machine Intelligence 11, pages 263–288. Clarendon Press, Oxford, 1988. R. A. Rescorla. Probability of shock in the presence and absence of CS in fear conditioning. Journal of Comparative and Physiological Psychology, 66:1–5, 1968. J. C. Schlimmer. Concept Acquisition through Representational Adjustment. PhD thesis, Department of Information and Computer Science, University of California, Irvine, 1987. J. C. Schlimmer and D. Fisher. A case study of incremental concept induction. In Proceedings of the Fifth National Conference on Artiﬁcial Intelligence, pages 496–501. AAAI Press, Menlo Park, CA, 1986. J. C. Schlimmer and R. H. Granger. Beyond incremental processing: Tracking concept drift. In Proceedings of the Fifth National Conference on Artiﬁcial Intelligence, pages 502–507. AAAI Press, Menlo Park, CA, 1986. M. Scholz and R. Klinkenberg. Boosting Classiﬁers for Drifting Concepts. Technical report, Collaborative Research Center on the Reduction of Complexity for Multivariate Data Structures (SFB 475), University of Dortmund, Dortmund, Germany, January 2006. M. Scholz and R. Klinkenberg. An ensemble classiﬁer for drifting concepts. In J. Aguilar and J. Gama, editors, Proceedings of the Second International Workshop on Knowledge Discovery in Data Streams, pages 53–64. http://www.liacc.up.pt/ ˜jgama/IWKDDS/, 2005. Held at the 16th European Conference on Machine Learning (ECML) and European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD), ECML/PKDD-2005. W. N. Street and Y. Kim. A streaming ensemble algorithm (SEA) for large-scale classiﬁcation. In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 377–382. ACM Press, New York, NY, 2001. N. A. Syed, H. Liu, and K. K. Sung. Handling concept drifts in incremental learning with support vector machines. In Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 272–276. ACM Press, New York, NY, 1999. A. Tsymbal, M. Pechenizkiy, P. Cunningham, and S. Puuronen. Dynamic Integration of Classiﬁers for Tracking Concept Drift in Antibiotic Resistance Data. Technical Report TCD-CS-2005-26, Trinity College Dublin, Dublin, Ireland, February 2005. P. E. Utgoff, N. C. Berkman, and J. A. Clouse. Decision tree induction based on efﬁcient tree restructuring. Machine Learning, 29:5–44, 1997. H. Wang, W. Fan, P. S. Yu, and J. Han. Mining concept-drifting data streams using ensemble classiﬁers. In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 226–235. ACM Press, New York, NY, 2003. G. Widmer. Tracking context changes through meta-learning. Machine Learning, 27:259–286, 1997. 2789  KOLTER AND M ALOOF  G. Widmer and M. Kubat. Learning in the presence of concept drift and hidden contexts. Machine Learning, 23:69–101, 1996. I. H. Witten and E. Frank. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann, San Francisco, CA, 2nd edition, 2005. D. H. Wolpert. Stacked generalization. Neural Networks, 5(2):241–259, 1992. K. Woods, W. P. Kegelmeyer, and K. Bowyer. Combination of multiple classiﬁers using local accuracy estimates. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4): 405–410, 1997. Z. Zheng. Naive Bayesian classiﬁer committees. In Proceedings of the Tenth European Conference on Machine Learning, pages 196–207. Springer, Berlin, 1998. Z.-H. Zhou, J. Wu, and W. Tang. Ensembling neural networks: Many could be better than all. Artiﬁcial Intelligence, 137(1–2):239–263, 2002.  2790</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
