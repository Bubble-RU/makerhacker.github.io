<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-6" href="../jmlr2007/jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">jmlr2007-6</a> <a title="jmlr-2007-6-reference" href="#">jmlr2007-6-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</h1>
<br/><p>Source: <a title="jmlr-2007-6-pdf" href="http://jmlr.org/papers/volume8/dasgupta07a/dasgupta07a.pdf">pdf</a></p><p>Author: Sanjoy Dasgupta, Leonard Schulman</p><p>Abstract: We show that, given data from a mixture of k well-separated spherical Gaussians in R d , a simple two-round variant of EM will, with high probability, learn the parameters of the Gaussians to nearoptimal precision, if the dimension is high (d ln k). We relate this to previous theoretical and empirical work on the EM algorithm. Keywords: expectation maximization, mixtures of Gaussians, clustering, unsupervised learning, probabilistic analysis</p><br/>
<h2>reference text</h2><p>D. Achlioptas and F. McSherry. On spectral learning of mixtures of distributions. In Proceedings of the 18th Conference on Learning Theory, 458–469, 2005. S. Arora and R. Kannan. Learning mixtures of separated nonspherical Gaussians. Annals of Applied Probability, 15(1A):69–92, 2005. C. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, New York, 1995. 225  DASGUPTA AND S CHULMAN  S. Dasgupta. Learning mixtures of Gaussians. In Proceedings of the 40th Annual IEEE Symposium on Foundations of Computer Science, 634–644, 1999. A. Dempster, N. Laird, and D. Rubin. Maximum-likelihood from incomplete data via the EM algorithm. J. Royal Statist. Soc. Ser. B, 39:1–38, 1977. P. Diaconis and D. Freedman. Asymptotics of graphical projection pursuit. Annals of Statistics, 12:793–815, 1984. R. Duda and P. Hart. Pattern Classiﬁcation and Scene Analysis. John Wiley, New York, 1973. R. Dudley. Balls in Rk do not cut all subsets of k + 2 points. Advances in Mathematics, 31:306–308, 1979. R. Durrett. Probability: Theory and Examples. Duxbury, Belmont, California, 1996. W. Feller. An Introduction to Probability Theory and its Applications, vol. II. John Wiley, New York, 1966. T. Gonzalez. Clustering to minimize the maximum intercluster distance. Theoretical Computer Science, 38:293–306, 1985. D. Haussler. Decision-theoretic generalizations of the PAC model for neural net and other learning applications. Information and Computation, 100:78–150, 1992. M. Kearns, Y. Mansour, and A. Ng. An information-theoretic analysis of hard and soft assignment methods for clustering. In Proceedings of the 13th Conference on Uncertainty in Artiﬁcial Intelligence, 1997. M. Kearns and U.V. Vazirani. An Introduction to Computational Learning Theory. MIT Press, Cambridge, Massachusetts, 1994. R. Redner and H. Walker. Mixture densities, maximum likelihood and the EM algorithm. SIAM Review, 26(2):195–239, 1984. D. Titterington, A. Smith, and U. Makov. Statistical Analysis of Finite Mixture Distributions. John Wiley, London, 1985. S. Vempala and G. Wang. A spectral algorithm for learning mixtures of distributions. Journal of Computer and Systems Sciences, 68(4):841–860, 2004. S. Vempala, R. Kannan, and H. Salmasian. The spectral method for general mixture models. In Proceedings of the 18th Conference on Learning Theory, 2005. C.F.J. Wu. On the convergence properties of the EM algorithm. Annals of Statistics, 11:95–103, 1983. L. Xu and M.I. Jordan. On convergence properties of the EM algorithm for Gaussian mixtures. Neural Computation, 8:129–151, 1996. 226</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
