<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>35 jmlr-2007-General Polynomial Time Decomposition Algorithms     (Special Topic on the Conference on Learning Theory 2005)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-35" href="../jmlr2007/jmlr-2007-General_Polynomial_Time_Decomposition_Algorithms_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">jmlr2007-35</a> <a title="jmlr-2007-35-reference" href="#">jmlr2007-35-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>35 jmlr-2007-General Polynomial Time Decomposition Algorithms     (Special Topic on the Conference on Learning Theory 2005)</h1>
<br/><p>Source: <a title="jmlr-2007-35-pdf" href="http://jmlr.org/papers/volume8/list07a/list07a.pdf">pdf</a></p><p>Author: Nikolas List, Hans Ulrich Simon</p><p>Abstract: We present a general decomposition algorithm that is uniformly applicable to every (suitably normalized) instance of Convex Quadratic Optimization and efﬁciently approaches an optimal solution. The number of iterations required to be within ε of optimality grows linearly with 1/ε and quadratically with the number m of variables. The working set selection can be performed in polynomial time. If we restrict our considerations to instances of Convex Quadratic Optimization with at most k0 equality constraints for some ﬁxed constant k0 plus some so-called box-constraints (conditions that hold for most variants of SVM-optimization), the working set is found in linear time. Our analysis builds on a generalization of the concept of rate certifying pairs that was introduced by Hush and Scovel. In order to extend their results to arbitrary instances of Convex Quadratic Optimization, we introduce the general notion of a rate certifying q-set. We improve on the results by Hush and Scovel (2003) in several ways. First our result holds for Convex Quadratic Optimization whereas the results by Hush and Scovel are specialized to SVM-optimization. Second, we achieve a higher rate of convergence even for the special case of SVM-optimization (despite the generality of our approach). Third, our analysis is technically simpler. We prove furthermore that the strategy for working set selection which is based on rate certifying sets coincides with a strategy which is based on a so-called “sparse witness of sub-optimality”. Viewed from this perspective, our main result improves on convergence results by List and Simon (2004) and Simon (2004) by providing convergence rates (and by holding under more general conditions). Keywords: convex quadratic optimization, decomposition algorithms, support vector machines</p><br/>
<h2>reference text</h2><p>Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. A training algorithm for optimal margin classiﬁers. In Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory, pages 144–152, 1992. Steven Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004. Chih-Chung Chang, Chih-Wei Hsu, and Chih-Jen Lin. The analysis of decomposition methods for support vector machines. IEEE Transactions on Neural Networks, 11(4):248–250, 2000. Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines, 2001. Available from http://www.csie.ntu.edu.tw/∼cjlin/libsvm. Pai-Hsuen Chen, Rong-En Fan, and Chih-Jen Lin. Training support vector machines via SMO-type decomposition methods. In Proceedings of the 16th International Conference on Algorithmic Learning Theory, pages 45–62, 2005. Pai-Hsuen Chen, Rong-En Fan, and Chih-Jen Lin. A study on SMO-type decomposition methods for support vector machines. IEEE Transactions on Neural Networks, 17(4):893–908, 2006. J. Dunn. Rates of convergence for conditional gradient algorithms near singular and non-singular extremals. SIAM J. Control and Optimization, 17(2):187–211, 1979. Chih-Wei Hsu and Chih-Jen Lin. A simple decomposition method for support vector machines. Machine Learning, 46(1–3):291–314, 2002. 319  L IST AND S IMON  Don Hush, Patrick Kelly, Clint Scovel, and Ingo Steinwart. QP Algorithms with Guaranteed Aaccuracy and Run Time for Support Vector Machines. Journal of Machine Learning Research, 7: 733–769, May 2006. Don Hush and Clint Scovel. Polynomial-time decomposition algorithms for support vector machines. Machine Learning, 51(1):51–71, 2003. ¨ Thorsten Joachims. Making large scale SVM learning practical. In Bernhard Sch olkopf, Christopher J. C. Burges, and Alexander J. Smola, editors, Advances in Kernel Methods—Support Vector Learning, pages 169–184. MIT Press, 1998. S. Sathiya Keerthi and E. G. Gilbert. Convergence of a generalized SMO algorithm for SVM classiﬁer design. Machine Learning, 46(1–3):351–360, 2002. S. Sathiya Keerthi, Shirish Krishnaj Shevade, Chiranjib Bhattacharyya, and K. R. K. Murthy. Improvements to SMO algorithm for SVM regression. IEEE Transactions on Neural Networks, 11 (5):1188–1193, 2000. S. Sathiya Keerthi, Shirish Krishnaj Shevade, Chiranjib Bhattacharyya, and K. R. K. Murthy. Improvements to Platt’s SMO algorithm for SVM classiﬁer design. Neural Computation, 13(3): 637–649, 2001. Pavel Laskov. Feasible direction decomposition algorithms for training support vector machines. Machine Learning, 46(1–3):315–349, 2002. Shuo-Peng Liao, Hsuan-Tien Lin, and Chih-Jen Lin. A note on the decomposition methods for support vector regression. Neural Computation, 14(6):1267–1281, 2002. Chih-Jen Lin. Linear convergence of a decomposition method for support vector machines, 2001a. Available from http://www.csie.ntu.edu.tw/∼cjlin/papers/linearconv.pdf. Chih-Jen Lin. On the convergence of the decomposition method for support vector machines. IEEE Transactions on Neural Networks, 12(6):1288–1298, 2001b. Chih-Jen Lin. Asymptotic convergence of an SMO algorithm without any assumptions. IEEE Transactions on Neural Networks, 13(1):248–250, 2002a. Chih-Jen Lin. A formal analysis of stopping criteria of decomposition methods for support vector machines. IEEE Transactions on Neural Networks, 13(5):1045–1052, 2002b. Nikolas List. Convergence of a generalized gradient selection approach for the decomposition method. In Proceedings of the 15th International Conference on Algorithmic Learning Theory, pages 338–349, 2004. Nikolas List and Hans Ulrich Simon. A general convergence theorem for the decomposition method. In Proceedings of the 17th Annual Conference on Computational Learning Theory, pages 363– 377, 2004. Olvi L. Mangasarian and David R. Musicant. Successive overrelaxation for support vector machines. IEEE Transactions on Neural Networks, 10(5):1032–1037, 1999. 320  G ENERAL P OLYNOMIAL T IME D ECOMPOSITION A LGORITHMS  Olvi L. Mangasarian and David R. Musicant. Active support vector machine classiﬁcation. In Advances in Neural Information Processing Systems 12, pages 577–583. MIT Press, 2000. Olvi L. Mangasarian and David R. Musicant. Lagrangian support vector machines. Journal of Machine Learning Research, 1:161–177, 2001. Nimrod Megiddo. Linear programming in linear time when the dimension is ﬁxed. Journal of the Association on Computing Machinery, 31(1):114–127, 1984. Edgar E. Osuna, Robert Freund, and Federico Girosi. Training support vector machines: an application to face detection. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 130–136, 1997. Christos H. Papadimitriou and Kenneth Steiglitz Combinatorial Optimization. Prentice Hall, 1982. John C. Platt. Fast training of support vector machines using sequential minimal optimization. In Bernhard Sch¨ lkopf, Christopher J. C. Burges, and Alexander J. Smola, editors, Advances in o Kernel Methods—Support Vector Learning, pages 185–208. MIT Press, 1998. ¨ Craig Saunders, Mark O. Stitson, Jason Weston, Leon Bottou, Bernhard Sch olkopf, and Alexander J. Smola. Support vector machine reference manual. Technical Report CSD-TR-98-03, Royal Holloway, University of London, Egham, UK, 1998. Hans Ulrich Simon. On the complexity of working set selection. In Proceedings of the 15th International Conference on Algorithmic Learning Theory, pages 324–337, 2004. A full version of the paper will appear in TCS. Vladimir Vapnik. Statistical Learning Theory. Wiley Series on Adaptive and Learning Systems for Signal Processing, Communications, and Control. John Wiley & Sons, 1998. S. V. N. Vishwanthan, Alexander J. Smola, and M. Narasimha Murty. SimpleSVM. In Proceedings of the 20th International Conference on Machine Learning, pages 760–767, 2003.  321</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
