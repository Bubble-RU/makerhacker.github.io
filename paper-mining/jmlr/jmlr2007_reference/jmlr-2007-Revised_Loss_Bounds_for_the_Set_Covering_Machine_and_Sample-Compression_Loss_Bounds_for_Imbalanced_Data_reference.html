<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-73" href="../jmlr2007/jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">jmlr2007-73</a> <a title="jmlr-2007-73-reference" href="#">jmlr2007-73-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</h1>
<br/><p>Source: <a title="jmlr-2007-73-pdf" href="http://jmlr.org/papers/volume8/hussain07a/hussain07a.pdf">pdf</a></p><p>Author: Zakria Hussain, François Laviolette, Mario Marchand, John Shawe-Taylor, Spencer Charles Brubaker, Matthew D. Mullin</p><p>Abstract: Marchand and Shawe-Taylor (2002) have proposed a loss bound for the set covering machine that has the property to depend on the observed fraction of positive examples and on what the classiﬁer achieves on the positive training examples. We show that this loss bound is incorrect. We then propose a loss bound, valid for any sample-compression learning algorithm (including the set covering machine), that depends on the observed fraction of positive examples and on what the classiﬁer achieves on them. We also compare numerically the loss bound proposed in this paper with the incorrect bound, the original SCM bound and a recently proposed loss bound of Marchand and Sokolova (2005) (which does not depend on the observed fraction of positive examples) and show that the latter loss bounds can be substantially larger than the new bound in the presence of imbalanced misclassiﬁcations. Keywords: set covering machines, sample-compression, loss bounds</p><br/>
<h2>reference text</h2><p>Sally Floyd and Manfred Warmuth. Sample compression, learnability, and the VapnikChervonenkis dimension. Machine Learning, 21(3):269–304, 1995. John Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine Learning Research, 6:273–306, 2005. Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Technical report, University of California Santa Cruz, Santa Cruz, CA, 1986. Mario Marchand and John Shawe-Taylor. Learning with the set covering machine. Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001), pages 345–352, 2001. Mario Marchand and John Shawe-Taylor. The set covering machine. Journal of Machine Learning Reasearch, 3:723–746, 2002. Mario Marchand and Marina Sokolova. Learning with decision lists of data-dependent features. Journal of Machine Learning Reasearch, 6:427–451, 2005.  2549</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
