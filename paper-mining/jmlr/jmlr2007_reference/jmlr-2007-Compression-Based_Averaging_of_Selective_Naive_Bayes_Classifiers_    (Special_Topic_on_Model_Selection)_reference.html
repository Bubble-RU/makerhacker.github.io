<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 jmlr-2007-Compression-Based Averaging of Selective Naive Bayes Classifiers     (Special Topic on Model Selection)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-22" href="../jmlr2007/jmlr-2007-Compression-Based_Averaging_of_Selective_Naive_Bayes_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">jmlr2007-22</a> <a title="jmlr-2007-22-reference" href="#">jmlr2007-22-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>22 jmlr-2007-Compression-Based Averaging of Selective Naive Bayes Classifiers     (Special Topic on Model Selection)</h1>
<br/><p>Source: <a title="jmlr-2007-22-pdf" href="http://jmlr.org/papers/volume8/boulle07a/boulle07a.pdf">pdf</a></p><p>Author: Marc Boullé</p><p>Abstract: The naive Bayes classiﬁer has proved to be very effective on many real data applications. Its performance usually beneﬁts from an accurate estimation of univariate conditional probabilities and from variable selection. However, although variable selection is a desirable feature, it is prone to overﬁtting. In this paper, we introduce a Bayesian regularization technique to select the most probable subset of variables compliant with the naive Bayes assumption. We also study the limits of Bayesian model averaging in the case of the naive Bayes assumption and introduce a new weighting scheme based on the ability of the models to conditionally compress the class labels. The weighting scheme on the models reduces to a weighting scheme on the variables, and ﬁnally results in a naive Bayes classiﬁer with “soft variable selection”. Extensive experiments show that the compressionbased averaged classiﬁer outperforms the Bayesian model averaging scheme. Keywords: naive Bayes, Bayesian, model selection, model averaging</p><br/>
<h2>reference text</h2><p>C.L. Blake and C.J. Merz. UCI repository of machine learning databases, 1996. http://www.ics.uci.edu/mlearn/MLRepository.html. M. Boull´ . Feature Extraction: Foundations And Applications, chapter 25, pages 499–507. e Springer, 2006a. M. Boull´ . Regularization and averaging of the selective naive Bayes classiﬁer. In International e Joint Conference on Neural Networks, pages 2989–2997, 2006b. M. Boull´ . A Bayes optimal approach for partitioning the values of categorical attributes. Journal e of Machine Learning Research, 6:1431–1452, 2005a. M. Boull´ . MODL: a Bayes optimal discretization method for continuous attributes. Machine e Learning, 65(1):131–165, 2006c. M. Boull´ . A grouping method for categorical attributes having very large number of values. In e P. Perner and A. Imiya, editors, Proceedings of the Fourth International Conference on Machine Learning and Data Mining in Pattern Recognition, volume 3587 of LNAI, pages 228–242. Springer verlag, 2005b. L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. Classiﬁcation and Regression Trees. California: Wadsworth International, 1984. D. Dash and G.F. Cooper. Exact model averaging with naive Bayesian classiﬁers. In Proceedings of the Nineteenth International Conference on Machine Learning, pages 91–98, 2002. 1683  ´ B OULL E  K. Deng, C. Bourke, S. Scott, and N.V. Vinodchandran. New algorithms for optimizing multi-class classiﬁers via ROC surfaces. In Proceedings of the ICML 2006 Workshop on ROC Analysis in Machine Learning, pages 17–24, 2006. P. Domingos and M.J. Pazzani. On the optimality of the simple bayesian classiﬁer under zero-one loss. Machine Learning, 29(2-3):103–130, 1997. J. Dougherty, R. Kohavi, and M. Sahami. Supervised and unsupervised discretization of continuous features. In Proceedings of the 12th International Conference on Machine Learning, pages 194– 202. Morgan Kaufmann, San Francisco, CA, 1995. T. Fawcett. ROC graphs: Notes and practical considerations for researchers. Technical Report HPL-2003-4, HP Laboratories, 2003. I. Guyon, S. Gunn, A. Ben Hur, and G. Dror. Feature Extraction: Foundations And Applications, chapter 9, pages 237–263. Springer, 2006a. Design and Analysis of the NIPS2003 Challenge. I. Guyon, S. Gunn, M. Nikravesh, and L. Zadeh, editors. Feature Extraction: Foundations And Applications. Springer, 2006b. I. Guyon, A.R. Saffari, G. Dror, and J.M. Bumann. Performance prediction challenge. In International Joint Conference on Neural Networks, pages 2958–2965, 2006c. D.J. Hand and K. Yu. Idiot bayes ? not so stupid after all? International Statistical Review, 69(3): 385–399, 2001. J.A. Hoeting, D. Madigan, A.E. Raftery, and C.T. Volinsky. Bayesian model averaging: A tutorial. Statistical Science, 14(4):382–417, 1999. R. Kohavi and G. John. Wrappers for feature selection. Artiﬁcial Intelligence, 97(1-2):273–324, 1997. P. Langley and S. Sage. Induction of selective Bayesian classiﬁers. In Proceedings of the 10th Conference on Uncertainty in Artiﬁcial Intelligence, pages 399–406. Morgan Kaufmann, 1994. P. Langley, W. Iba, and K. Thompson. An analysis of Bayesian classiﬁers. In 10th national conference on Artiﬁcial Intelligence, pages 223–228. AAAI Press, 1992. H. Liu, F. Hussain, C.L. Tan, and M. Dash. Discretization: An enabling technique. Data Mining and Knowledge Discovery, 4(6):393–423, 2002. T.M. Mitchell. Machine Learning. McGraw-Hill, New York, 1997. V. Pareto. Manuale di Economia Politica. Piccola Biblioteca Scientiﬁca, Milan, 1906. Translated into English by Ann S. Schwier (1971), Manual of Political Economy, MacMillan, London. F. Provost and P. Domingos. Well-trained pets: Improving probability estimation trees. Technical Report CeDER #IS-00-04, New York University, 2001. F. Provost, T. Fawcett, and R. Kohavi. The case against accuracy estimation for comparing induction algorithms. In Proceedings of the Fifteenth International Conference on Machine Learning, pages 445–553, 1998. 1684  C OMPRESSION -BASED AVERAGING OF S ELECTIVE NAIVE BAYES C LASSIFIERS  A.E. Raftery and Y. Zheng. Long-run performance of Bayesian model averaging. Technical Report 433, Department of Statistics, University of Washington, 2003. J. Rissanen. Modeling by shortest data description. Automatica, 14:465–471, 1978. C.P. Robert. The Bayesian Choice: A Decision-Theoretic Motivation. Springer-Verlag, New York, 1997. C.E. Shannon. A mathematical theory of communication. Technical report, Bell systems technical journal, 1948. I.H. Witten and E. Frank. Data Mining. Morgan Kaufmann, 2000. Y. Yang and G. Webb. A comparative study of discretization methods for naive-Bayes classiﬁers. In Proceedings of the Paciﬁc Rim Knowledge Acquisition Workshop, pages 159–173, 2002.  1685</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
