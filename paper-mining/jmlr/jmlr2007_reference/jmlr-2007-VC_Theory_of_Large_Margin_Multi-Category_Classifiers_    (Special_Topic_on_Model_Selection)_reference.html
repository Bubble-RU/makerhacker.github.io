<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-89" href="../jmlr2007/jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">jmlr2007-89</a> <a title="jmlr-2007-89-reference" href="#">jmlr2007-89-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</h1>
<br/><p>Source: <a title="jmlr-2007-89-pdf" href="http://jmlr.org/papers/volume8/guermeur07a/guermeur07a.pdf">pdf</a></p><p>Author: Yann Guermeur</p><p>Abstract: In the context of discriminant analysis, Vapnik’s statistical learning theory has mainly been developed in three directions: the computation of dichotomies with binary-valued functions, the computation of dichotomies with real-valued functions, and the computation of polytomies with functions taking their values in ﬁnite sets, typically the set of categories itself. The case of classes of vectorvalued functions used to compute polytomies has seldom been considered independently, which is unsatisfactory, for three main reasons. First, this case encompasses the other ones. Second, it cannot be treated appropriately through a na¨ve extension of the results devoted to the computation of ı dichotomies. Third, most of the classiﬁcation problems met in practice involve multiple categories. In this paper, a VC theory of large margin multi-category classiﬁers is introduced. Central in this theory are generalized VC dimensions called the γ-Ψ-dimensions. First, a uniform convergence bound on the risk of the classiﬁers of interest is derived. The capacity measure involved in this bound is a covering number. This covering number can be upper bounded in terms of the γ-Ψdimensions thanks to generalizations of Sauer’s lemma, as is illustrated in the speciﬁc case of the scale-sensitive Natarajan dimension. A bound on this latter dimension is then computed for the class of functions on which multi-class SVMs are based. This makes it possible to apply the structural risk minimization inductive principle to those machines. Keywords: multi-class discriminant analysis, large margin classiﬁers, uniform strong laws of large numbers, generalized VC dimensions, multi-class SVMs, structural risk minimization inductive principle, model selection</p><br/>
<h2>reference text</h2><p>E.L. Allwein, R.E. Schapire, and Y. Singer. Reducing multiclass to binary: A unifying approach for margin classiﬁers. Journal of Machine Learning Research, 1:113–141, 2000. N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haussler. Scale-sensitive dimensions, uniform convergence, and learnability. Journal of the ACM, 44(4):615–631, 1997. A. Ambroladze, E. Parrado-Hernandez, and J. Shawe-Taylor. Tighter PAC-Bayes bounds. In Advances in Neural Information Processing Systems 19, 2007. (to appear). M. Anthony and P.L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, Cambridge, 1999. N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society, 68(3):337–404, 1950. P.L. Bartlett. The sample complexity of pattern classiﬁcation with neural networks: The size of the weights is more important than the size of the network. IEEE Transactions on Information Theory, 44(2):525–536, 1998. P.L. Bartlett and J. Shawe-Taylor. Generalization performance of support vector machines and other pattern classiﬁers. In B. Sch¨ lkopf, C.J.C. Burges, and A. Smola, editors, Advances in Kernel o Methods - Support Vector Learning, chapter 4, pages 43–54. The MIT Press, Cambridge, MA, 1999. P.L. Bartlett, P.M. Long, and R.C. Williamson. Fat-shattering and the learnability of real-valued functions. Journal of Computer and System Sciences, 52(3):434–452, 1996. S. Ben-David, N. Cesa-Bianchi, D. Haussler, and P.M. Long. Characterizations of learnability for classes of {0, . . . , n}-valued functions. Journal of Computer and System Sciences, 50(1):74–86, 1995. A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics. Kluwer Academic Publishers, Boston, 2004. G. Blanchard, O. Bousquet, and P. Massart. Statistical performance of support vector machines. The Annals of Statistics, 2007. (to appear). B. Boser, I. Guyon, and V.N. Vapnik. A training algorithm for optimal margin classiﬁers. In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, pages 144–152, 1992. S. Boucheron, O. Bousquet, and G. Lugosi. Theory of classiﬁcation: A survey of some recent advances. ESAIM: Probability and Statistics, 9:323–375, 2005. O. Bousquet. Concentration Inequalities and Empirical Processes Theory Applied to the Analysis of Learning Algorithms. PhD thesis, Ecole Polytechnique, 2002. B. Carl and I. Stephani. Entropy, Compactness and the Approximation of Operators. Cambridge University Press, Cambridge, 1990. 2591  G UERMEUR  O. Chapelle, V.N. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector machines. Machine Learning, 46(1):131–159, 2002. C. Cortes and V.N. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995. K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning Research, 2:265–292, 2001. K. Crammer and Y. Singer. On the learnability and design of output codes for multiclass problems. Machine Learning, 47(2):201–233, 2002. Y. Darcy and Y. Guermeur. Radius-margin bound on the leave-one-out error of multi-class SVMs. Technical Report RR-5780, INRIA, 2005. L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springero Verlag, New York, 1996. R.M. Dudley. Central limit theorems for empirical measures. The Annals of Probability, 6(6): 899–929, 1978. R.M. Dudley. A course on empirical processes. In P.L. Hennequin, editor, Ecole d’Et e de Proba´ bilit´ s de Saint-Flour XII - 1982, volume 1097 of Lecture Notes in Mathematics, pages 1–142. e Springer-Verlag, 1984. R.M. Dudley. Universal Donsker classes and metric entropy. The Annals of Probability, 15(4): 1306–1326, 1987. A. Elisseeff, Y. Guermeur, and H. Paugam-Moisy. Margin error and generalization capabilities of multi-class discriminant models. Technical Report NC-TR-99-051-R, NeuroCOLT2, 1999. (revised in 2001). J. F¨ rnkranz. Round robin classiﬁcation. Journal of Machine Learning Research, 2:721–747, 2002. u Y. Guermeur. Combining discriminant models with new multi-class SVMs. Pattern Analysis and Applications, 5(2):168–179, 2002. Y. Guermeur, A. Elisseeff, and H. Paugam-Moisy. Estimating the sample complexity of a multi-class discriminant model. In International Conference on Artiﬁcial Neural Networks, pages 310–315. IEE, 1999. Y. Guermeur, M. Maumy, and F. Sur. Model selection for multi-class SVMs. In International Symposium on Applied Stochastic Models and Data Analysis, pages 507–517, 2005. L. Gurvits. A note on a scale-sensitive dimension of linear bounded functionals in Banach spaces. Theoretical Computer Science, 261(1):81–90, 2001. T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. The entire regularization path for the support vector machine. Journal of Machine Learning Research, 5:1391–1415, 2004. D. Haussler and P.M. Long. A generalization of Sauer’s lemma. Journal of Combinatorial Theory, Series A, 71(2):219–240, 1995. 2592  VC T HEORY OF L ARGE M ARGIN M ULTI -C ATEGORY C LASSIFIERS  W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:13–30, 1963. K. Jogdeo and S.M. Samuels. Monotone convergence of binomial probabilities and a generalization of Ramanujan’s equation. The Annals of Mathematical Statistics, 39(4):1191–1195, 1968. M.J. Kearns and R.E. Schapire. Efﬁcient distribution-free learning of probabilistic concepts. Journal of Computer and System Sciences, 48(3):464–497, 1994. A.N. Kolmogorov and V.M. Tihomirov. ε-entropy and ε-capacity of sets in functional spaces. American Mathematical Society Translations, series 2, 17:277–364, 1961. R.S. Kroon. Support vector machines, generalization bounds, and transduction. Master’s thesis, University of Stellenbosch, South Africa, December 2003. http://www.cs.sun.ac.za/˜skroon/personal/pubs/kroon2003support.ps. M. Ledoux. On Talagrand’s deviation inequalities for product measures. ESAIM: Probability and Statistics, 1:63–87, 1996. Y. Lee and Z. Cui. Characterizing the solution path of multicategory support vector machines. Statistica Sinica, 16:391–409, 2006. Y. Lee, Y. Lin, and G. Wahba. Multicategory support vector machines: Theory and application to the classiﬁcation of microarray data and satellite radiance data. Journal of the American Statistical Association, 99(465):67–81, 2004. G. Lugosi. Concentration-of-measure inequalities. Lecture notes, Summer School on Machine Learning at the Australian National University, Canberra, 2004. P. Massart. Some applications of concentration inequalities to statistics. Annales de la Facult´ des e Sciences de Toulouse, 9(2):245–303, 2000. E. Monfrini and Y. Guermeur. A quadratic loss multi-class SVM. Technical report, LORIA, 2007. (to appear). B.K. Natarajan. On learning sets and functions. Machine Learning, 4(1):67–97, 1989. J.C. Platt, N. Cristianini, and J. Shawe-Taylor. Large margin DAGs for multiclass classiﬁcation. In Advances in Neural Information Processing Systems 12, pages 547–553, 2000. D. Pollard. Convergence of Stochastic Processes. Springer-Verlag, New York, 1984. R. Rifkin and A. Klautau. In defense of one-vs-all classiﬁcation. Journal of Machine Learning Research, 5:101–141, 2004. W. Rudin. Real and Complex Analysis. McGraw-Hill, New York, third edition, 1987. N. Sauer. On the density of families of sets. Journal of Combinatorial Theory (A), 13:145–147, 1972. B. Sch¨ lkopf and A.J. Smola. Learning with Kernels - Support Vector Machines, Regularization, o Optimization, and Beyond. The MIT Press, Cambridge, MA, 2002. 2593  G UERMEUR  J. Shawe-Taylor, P.L. Bartlett, R.C. Williamson, and M. Anthony. Structural risk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory, 44(5):1926–1940, 1998. S. Shelah. A combinatorial problem: Stability and order for models and theories in inﬁnitary languages. Paciﬁc Journal of Mathematics, 41(1):247–261, 1972. I. Steinwart and C. Scovel. Fast rates for support vector machines. In Proceedings of the eighteenth annual Conference on Learning Theory, pages 279–294, 2005. M. Stone. Asymptotics for and against cross-validation. Biometrika, 64(1):29–35, 1977. M. Talagrand. Concentration of measure and isoperimetric inequalities in product spaces. Publications math´ matiques de l’I.H.E.S., 81:73–205, 1995. e M. Talagrand. A new look at independence. The Annals of Probability, 24(1):1–34, 1996. A. Tewari and P.L. Bartlett. On the consistency of multiclass classiﬁcation methods. Journal of Machine Learning Research, 8:1007–1025, 2007. L.G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984. A.W. van der Vaart and J.A. Wellner. Weak Convergence and Empirical Processes - With Applications to Statistics. Springer Series in Statistics. Springer-Verlag, New York, 1996. V.N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-Verlag, New York, 1982. V.N. Vapnik. Statistical Learning Theory. John Wiley & Sons, Inc., New York, 1998. V.N. Vapnik and A.Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, XVI(2):264–280, 1971. G. Wahba. Support vector machines, reproducing kernel Hilbert spaces, and randomized GACV. In B. Sch¨ lkopf, C.J.C. Burges, and A.J. Smola, editors, Advances in Kernel Methods - Support o Vector Learning, chapter 6, pages 69–88. The MIT Press, Cambridge, MA, 1999. L. Wang, P. Xue, and K.L. Chan. Generalized radius-margin bounds for model selection in multiclass SVMs. Technical report, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, 639798, 2005. J. Weston and C. Watkins. Multi-class support vector machines. Technical Report CSD-TR-98-04, Royal Holloway, University of London, Department of Computer Science, 1998. ¨ R.C. Williamson, A.J. Smola, and B. Scholkopf. Entropy numbers of linear function classes. In Proceedings of the Thirteenth Annual Workshop on Computational Learning Theory, pages 309– 319, 2000. T. Zhang. Statistical analysis of some multi-category large margin classiﬁcation methods. Journal of Machine Learning Research, 5:1225–1251, 2004.  2594</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
