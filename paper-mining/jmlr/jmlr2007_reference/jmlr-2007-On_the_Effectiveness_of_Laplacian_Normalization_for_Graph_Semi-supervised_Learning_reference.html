<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-62" href="../jmlr2007/jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">jmlr2007-62</a> <a title="jmlr-2007-62-reference" href="#">jmlr2007-62-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</h1>
<br/><p>Source: <a title="jmlr-2007-62-pdf" href="http://jmlr.org/papers/volume8/johnson07a/johnson07a.pdf">pdf</a></p><p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><br/>
<h2>reference text</h2><p>Rie K. Ando and Tong Zhang. Learning on graph with Laplacian regularization. In NIPS 19, 2007. Mikhail Belkin and Partha Niyogi. Semi-supervised learning on Riemannian manifolds. Machine Learning, Special Issue on Clustering:209–239, 2004. Fan R.K. Chung. Spectral Graph Theory. Regional Conference Series in Mathematics. American Mathematical Society, Rhode Island, 1998. Chris Ding, Xiaofeng He, Hongyuan Zha, Ming Gu, and Horst Simon. A min-max cut algorithm for graph partitioning and data clustering. In IEEE Int’l Conf. Data Mining, pages 107–114, 2001. Daniel B. Graham and Nigel M. Allinson. Characterizing virtual eigensignatures for general purpose face recognition. Face Recognition: From Theory to Applications, NATO ASI Series F, Computer and Systems Sciences, 163:446–456, 1998. Lars Hagen and Andrew B. Kahng. New spectral methods for ratio cut partitioning and clustering. IEEE Transactions on Computer-Aided Design, 11(9):1074–1085, 1992. Gert R.G. Lanckriet, Nello Cristianini, Peter Bartlett, Laurent El Ghaoui, and Michael I. Jordan. Learning the kernel matrix with semideﬁnite programming. Journal of Machine Learning Research, 5:27–72, 2004. 1516  O N THE E FFECTIVENESS OF L APLACIAN N ORMALIZATION FOR G RAPH S EMI - SUPERVISED L EARNING  Marina Meil˘ , Susan Shortreed, and Liang Xu. Regularized spectral learning. In AISTATS, 2005. a Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. In NIPS 14, pages 849–856, 2002. Ralph Tyrrell Rockafellar. Convex analysis. Princeton University Press, Princeton, NJ, 1970. Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Trans. Pattern Anal. Mach. Intell, 22:888–905, 2000. Martin Szummer and Tommi Jaakkola. Partially labeled classiﬁcation with Markov random walks. In Advances in Neural Information Processing Systems 14, 2002. Ulrike von Luxburg, Olivier Bousquet, and Mikhail Belkin. Limits of spectral clustering. In Lawrence K. Saul, Yair Weiss, and L´ on Bottou, editors, Advances in Neural Information Proe cessing Systems 17, pages 857–864. MIT Press, Cambridge, MA, 2005. Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stuart Russell. Distance metric learning, with application to clustering with side-information. In NIPS 15, 2003. Tong Zhang. Leave-one-out bounds for kernel methods. Neural Computation, 15:1397–1437, 2003. Tong Zhang and Rie K. Ando. Analysis of spectral kernel design based semi-supervised learning. In NIPS 18, 2006. Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Schlkopf. Learning with local and global consistency. In Advances in Neural Information Processing Systems 16, pages 321–328, 2004. Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using Gaussian ﬁelds and harmonic functions. In ICML 2003, 2003.  1517</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
