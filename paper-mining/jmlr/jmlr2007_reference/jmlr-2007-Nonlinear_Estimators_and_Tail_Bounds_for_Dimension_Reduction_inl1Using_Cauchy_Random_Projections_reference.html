<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-60" href="../jmlr2007/jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">jmlr2007-60</a> <a title="jmlr-2007-60-reference" href="#">jmlr2007-60-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</h1>
<br/><p>Source: <a title="jmlr-2007-60-pdf" href="http://jmlr.org/papers/volume8/li07b/li07b.pdf">pdf</a></p><p>Author: Ping Li, Trevor J. Hastie, Kenneth W. Church</p><p>Abstract: For1 dimension reduction in the l1 norm, the method of Cauchy random projections multiplies the original data matrix A ∈ Rn×D with a random matrix R ∈ RD×k (k D) whose entries are i.i.d. samples of the standard Cauchy C(0, 1). Because of the impossibility result, one can not hope to recover the pairwise l1 distances in A from B = A × R ∈ Rn×k , using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computations, information retrieval, learning, and data mining. We study three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators (MLE). We derive tail bounds for the geometric mean estimators and establish that k = O log n sufﬁces with the constants explicitly ε2 given. Asymptotically (as k → ∞), both the sample median and the geometric mean estimators are about 80% efﬁcient compared to the MLE. We analyze the moments of the MLE and propose approximating its distribution of by an inverse Gaussian. Keywords: dimension reduction, l1 norm, Johnson-Lindenstrauss (JL) lemma, Cauchy random projections</p><br/>
<h2>reference text</h2><p>Dimitris Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Journal of Computer and System Sciences, 66(4):671–687, 2003. Dimitris Achlioptas. Database-friendly random projections. In PODS, pages 274–281, Santa Barbara, CA, 2001. Charu C. Aggarwal and Joel L. Wolf. A new method for similarity indexing of market basket data. In SIGMOD, pages 407–418, Philadelphia, PA, 1999. Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform. In STOC, pages 557–563, Seattle, WA, 2006. Charles Antle and Lee Bain. A property of maximum likelihood estimators of location and scale parameters. SIAM Review, 11(2):251–253, 1969. 2528  C AUCHY R ANDOM P ROJECTIONS  Rosa Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and random projection. Machine Learning, 63(2):161–182, 2006. Rosa Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and random projection. In FOCS, pages 616–623, New York, 1999. Brian Babcock, Shivnath Babu, Mayur Datar, Rajeev Motwani, and Jennifer Widom. Models and issues in data stream systems. In PODS, pages 1–16, Madison, WI, 2002. Maurice S. Bartlett. Approximate conﬁdence intervals, II. Biometrika, 40(3/4):306–317, 1953. Rabi N. Bhattacharya and Jayanta K. Ghosh. On the validity of the formal edgeworth expansion. The Annals of Statistics, 6(2):434–451, 1978. Bo Brinkman and Mose Charikar. On the impossibility of dimension reduction in l 1 . Journal of ACM, 52(2):766–788, 2005. Bo Brinkman and Mose Charikar. On the impossibility of dimension reduction in l 1 . In FOCS, pages 514–523, Cambridge, MA, 2003. Olivier Chapelle, Patrick Haffner, and Vladimir N. Vapnik. Support vector machines for histogrambased image classiﬁcation. IEEE Trans. Neural Networks, 10(5):1055–1064, 1999. Herman Chernoff. A measure of asymptotic efﬁciency for tests of a hypothesis based on the sum of observations. The Annals of Mathematical Statistics, 23(4):493–507, 1952. Raj S. Chhikara and J. Leroy Folks. The Inverse Gaussian Distribution: Theory, Methodology, and Applications. Marcel Dekker, Inc, New York, 1989. Graham Cormode and S. Muthukrishnan. Estimating dominance norms of multiple data streams. In ESA, pages 148–160, 2003. Graham Cormode, Mayur Datar, Piotr Indyk, and S. Muthukrishnan. Comparing data streams using hamming norms (how to zero in). In VLDB, pages 335–345, Hong Kong, China, 2002. Graham Cormode, Mayur Datar, Piotr Indyk, and S. Muthukrishnan. Comparing data streams using hamming norms (how to zero in). IEEE Transactions on Knowledge and Data Engineering, 15 (3):529–540, 2003. Francisco Jose De. A. Cysneiros, Sylvio Jose P. dos Santos, and Gass M. Cordeiro. Skewness and kurtosis for maximum likelihood estimator in one-parameter exponential family models. Brazilian Journal of Probability and Statistics, 15(1):85–105, 2001. Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of Johnson and Lindenstrauss. Random Structures and Algorithms, 22(1):60 – 65, 2003. Herbert A. David. Order Statistics. John Wiley & Sons, Inc., New York, NY, second edition, 1981. Inderjit S. Dhillon and Dharmendra S. Modha. Concept decompositions for large sparse text data using clustering. Machine Learning, 42(1-2):143–175, 2001. 2529  L I , H ASTIE AND C HURCH  Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. The Annals of Statistics, 32(2):407–499, 2004. Eugene F. Fama and Richard Roll. Some properties of symmetric stable distributions. Journal of the American Statistical Association, 63(323):817–836, 1968. Eugene F. Fama and Richard Roll. Parameter estimates for symmetric stable distributions. Journal of the American Statistical Association, 66(334):331–338, 1971. Joan Feigenbaum, Sampath Kannan, Martin Strauss, and Mahesh Viswanathan. An approximate l1 -difference algorithm for massive data streams. In FOCS, pages 501–511, New York, 1999. William Feller. An Introduction to Probability Theory and Its Applications (Volume II). John Wiley & Sons, New York, NY, second edition, 1971. Silvia L. P. Ferrari, Denise A. Botter, Gauss M. Cordeiro, and Francisco Cribari-Neto. Second and third order bias reduction for one-parameter family models. Stat. and Prob. Letters, 30:339–345, 1996. Ronald A. Fisher. Two new properties of mathematical likelihood. Proceedings of the Royal Society of London, 144(852):285–307, 1934. Peter Frankl and Hiroshi Maehara. The Johnson-Lindenstrauss lemma and the sphericity of some graphs. Journal of Combinatorial Theory A, 44(3):355–362, 1987. Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. The Annals of Statistics, 29(5):1189–1232, 2001. Hans U. Gerber. From the generalized gamma to the generalized negative binomial distribution. Insurance:Mathematics and Economics, 10(4):303–309, 1991. Izrail S. Gradshteyn and Iosif M. Ryzhik. Table of Integrals, Series, and Products. Academic Press, New York, ﬁfth edition, 1994. Gerald Haas, Lee Bain, and Charles Antle. Inferences for the Cauchy distribution based on maximum likelihood estimation. Biometrika, 57(2):403–408, 1970. Monika R. Henzinger, Prabhakar Raghavan, and Sridhar Rajagopalan. Computing on Data Streams. American Mathematical Society, Boston, MA, USA, 1999. David V. Hinkley. Likelihood inference about location and scale parameters. Biometrika, 65(2): 253–261, 1978. Philip Hougaard. Survival models for heterogeneous populations derived from stable distributions. Biometrika, 73(2):387–396, 1986. Peter J. Huber. Robust Statistics. Wiley, New York, NY, 1981. Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream computation. Journal of ACM, 53(3):307–323, 2006. 2530  C AUCHY R ANDOM P ROJECTIONS  Piotr Indyk. Stable distributions, pseudorandom generators, embeddings and data stream computation. In FOCS, pages 189–197, Redondo Beach, CA, 2000. Piotr Indyk. Algorithmic applications of low-distortion geometric embeddings. In FOCS, pages 10–33, Las Vegas, NV, 2001. Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse of dimensionality. In STOC, pages 604–613, Dallas, TX, 1998. Jens Ledet Jensen. Saddlepoint Approximations. Oxford University Press, New York, 1995. William B. Johnson and Joram Lindenstrauss. Extensions of Lipschitz mapping into Hilbert space. Contemporary Mathematics, 26:189–206, 1984. William B. Johnson and Gideon Schechtman. Embedding l p into l1 . Acta. Math., 149:71–85, 1982. Jerry F. Lawless. Conditional conﬁdence interval procedures for the location and scale parameters of the Cauchy and logistic distributions. Biometrika, 59(2):377–386, 1972. James R. Lee and Assaf Naor. Embedding the diamond graph in l p and dimension reduction in l1 . Geometric And Functional Analysis, 14(4):745–747, 2004. Ping Li. Very sparse stable random projections for dimension reduction in l α (0 < α ≤ 2) norm. In KDD, San Jose, CA, 2007. Ping Li. Estimators and tail bounds for dimension reduction in l α (0 < α ≤ 2) using stable random projections. In SODA, 2008. Ping Li and Kenneth W. Church. A sketch algorithm for estimating two-way and multi-way associations. Computational Linguistics, 33(3):305–354, 2007. Ping Li and Kenneth W. Church. Using sketches to estimate associations. In HLT/EMNLP, pages 708–715, Vancouver, BC, Canada, 2005. Ping Li, Trevor J. Hastie, and Kenneth W. Church. Improving random projections using marginal information. In COLT, pages 635–649, Pittsburgh, PA, 2006a. Ping Li, Trevor J. Hastie, and Kenneth W. Church. Very sparse random projections. In KDD, pages 287–296, Philadelphia, PA, 2006b. Ping Li, Debashis Paul, Ravi Narasimhan, and John Ciofﬁ. On the distribution of SINR for the MMSE MIMO receiver and performance analysis. IEEE Trans. Inform. Theory, 52(1):271–286, 2006c. Ping Li, Kenneth W. Church, and Trevor J. Hastie. Conditional random sampling: A sketch-based sampling technique for sparse data. In NIPS, pages 873–880, Vancouver, BC, Canada, 2007a. Ping Li, Trevor J. Hastie, and Kenneth W. Church. Nonlinear estimators and tail bounds for dimensional reduction in l1 using Cauchy random projections. In COLT, 2007b. Gabor Lugosi. Concentration-of-measure inequalities. Lecture Notes, 2004. 2531  L I , H ASTIE AND C HURCH  J. Huston McCulloch. Simple consistent estimators of stable distribution parameters. Communications on Statistics-Simulation, 15(4):1109–1136, 1986. Thomas K. Philips and Randolph Nelson. The moment bound is tighter than Chernoff’s bound for positive tail probabilities. The American Statistician, 49(2):175–178, 1995. V. Seshadri. The Inverse Gaussian Distribution: A Case Study in Exponential Families. Oxford University Press Inc., New York, 1993. Thomas A. Severini. Likelihood Methods in Statistics. Oxford University Press, New York, 2000. Gregory Shakhnarovich, Trevor Darrell, and Piotr Indyk, editors. Nearest-Neighbor Methods in Learning and Vision, Theory and Practice. The MIT Press, Cambridge, MA, 2005. Leonard. R. Shenton and Kimiko O. Bowman. Higher moments of a maximum-likelihood estimate. Journal of Royal Statistical Society B, 25(2):305–317, 1963. Alexander Strehl and Joydeep Ghosh. A scalable approach to balanced, high-dimensional clustering of market-baskets. In HiPC, pages 525–536, Bangalore, India, 2000. Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of Royal Statistical Society B, 58(1):267–288, 1996. Maurice C. K. Tweedie. Statistical properties of inverse Gaussian distributions. I. The Annals of Mathematical Statistics, 28(2):362–377, 1957a. Maurice C. K. Tweedie. Statistical properties of inverse Gaussian distributions. II. The Annals of Mathematical Statistics, 28(3):696–705, 1957b. Santosh Vempala. The Random Projection Method. American Mathematical Society, Providence, RI, 2004. Ji Zhu, Saharon Rosset, Trevor Hastie, and Robert Tibshirani. 1-norm support vector machines. In NIPS, Vancouver, BC, Canada, 2003. Vladimir M. Zolotarev. One-dimensional Stable Distributions. American Mathematical Society, Providence, RI, 1986.  2532</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
