<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-36" href="../jmlr2007/jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">jmlr2007-36</a> <a title="jmlr-2007-36-reference" href="#">jmlr2007-36-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</h1>
<br/><p>Source: <a title="jmlr-2007-36-pdf" href="http://jmlr.org/papers/volume8/rigollet07a/rigollet07a.pdf">pdf</a></p><p>Author: Philippe Rigollet</p><p>Abstract: We consider semi-supervised classiﬁcation when part of the available data is unlabeled. These unlabeled data can be useful for the classiﬁcation problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution. Seeger (2000) proposed the well-known cluster assumption as a reasonable one. We propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples. Keywords: semi-supervised learning, statistical learning theory, classiﬁcation, cluster assumption, generalization bounds</p><br/>
<h2>reference text</h2><p>M. Ankerst, M. M. Breunig, H.-P. Kriegel, and J. Sander. Optics: Ordering points to identify the clustering structure. In Proceedings ACM SIGMOD International Conference on Management of Data, pages 49–60, 1999. J.-Y. Audibert and A. Tsybakov. Fast learning rates for plug-in classiﬁers. Ann. Statist., 34(2), 2007. M.-F. Balcan and A. Blum. A pac-style model for learning from labeled and unlabeled data. In Proceedings of the Eighteenth Annual Conference on Learning Theory, pages 111–126, 2005. M. Belkin and P. Niyogi. Semi-supervised learning on Riemannian manifolds. Mach. Learn., 56 (1-3):209–239, 2004. 1390  G ENERALIZATION E RROR B OUNDS IN S EMI - SUPERVISED C LASSIFICATION  A. Blum and T. M. Mitchell. Combining labeled and unlabeled sata with co-training. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pages 92–100, 1998. S. Boucheron, O. Bousquet, and G. Lugosi. Theory of classiﬁcation: a survey of some recent advances. ESAIM Probab. Stat., 9:323–375 (electronic), 2005. O. Bousquet, O. Chapelle, and M. Hein. Measure based regularization. In Advances in Neural Information Processing Systems, volume 16, 2004. V. Castelli and T. M. Cover. On the exponential value of labeled samples. Pattern Recogn. Lett., 16 (1):105–111, 1995. V. Castelli and T. M. Cover. The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter. IEEE Trans. Inform. Theory, 42(6, part 2):2102–2117, 1996. O. Chapelle, B. Schlkopf, and A. Zien. Semi-Supervised Learning. MIT Press, Cambridge, Massachusetts, 2006. O. Chapelle and A. Zien. Semi-supervised classiﬁcation by low density separation. In Proceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics, pages 57–64, 2005. A. Cuevas, M. Febrero, and R. Fraiman. Cluster analysis: a further approach based on density estimation. Comput. Statist. Data Anal., 36(4):441–459, 2001. A. Cuevas and R. Fraiman. A plug-in approach to support estimation. Ann. Statist., 25(6):2300– 2312, 1997. F. d’Alch´ Buc, Y. Grandvalet, and C. Ambroise. Semi-supervised marginboost. In Advances in e Neural Information Processing Systems, volume 14, pages 553–560, 2001. L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springero Verlag, New York, 1996. D. S. Dummit and R. M. Foote. Abstract Algebra. Prentice Hall Inc., Englewood Cliffs, New Jersey, 1991. M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, pages 226–231, 1996. J. H. Hartigan. Clustering Algorithms. John Wiley & Sons, Inc., New York, 1975. R. Herbei and M. Wegkamp. Classiﬁcation with rejection option. Canad. J. Statist., 34(4):709–721, 2006. T. Hertz, A. Bar-Hillel, and D. Weinshall. Boosting margin based distance functions for clustering. In Proceedings of the Twenty-First International Conference on Machine learning, 2004. E. Mammen and A. B. Tsybakov. Smooth discrimination analysis. Ann. Statist., 27(6):1808–1829, 1999. 1391  R IGOLLET  W. Polonik. Measuring mass concentrations and estimating density contour clusters—an excess mass approach. Ann. Statist., 23(3):855–881, 1995. M. Rattray. A model-based distance for clustering. In Proc. of the IEEE-INNS-ENNS Int. Joint Conf. on Neural Networks, pages 13–16, 2000. P. Rigollet and R. Vert. Fast rates for plug-in estimators of density level sets. Technical Report 1102, Laboratoire de Probabilit´ s et Mod` les Al´ atoires de Paris 6, 2006. URL e e e http://hal.ccsd.cnrs.fr/ccsd-00114180. M. Seeger. Learning with labeled and unlabeled data. Technical report, Institute for ANC, Edinburgh, UK, 2000. URL http://www.dai.ed.ac.uk/ ˜seeger/papers.html. I. Steinwart, D. Hush, and C. Scovel. A classiﬁcation framework for anomaly detection. J. Mach. Learn. Res., 6:211–232, 2005. W. Stuetzle. Estimating the cluster type of a density by analyzing the minimal spanning tree of a sample. J. Classiﬁcation, 20(1):25–47, 2003. M. Tipping. Deriving cluster analytic distance functions from Gaussian mixture models. In Proceedings of the Ninth International Conference on Neural Networks, pages 815–820, 1999. A. B. Tsybakov. On nonparametric estimation of density level sets. Ann. Statist., 25(3):948–969, 1997. A. B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Ann. Statist., 32(1): 135–166, 2004. S. A. van de Geer. Applications of Empirical Process Theory. Cambridge University Press, Cambridge, 2000. V. N. Vapnik. Statistical Learning Theory. Wiley, New-York, 1998. T. Zhang and F. J. Oles. A probability analysis on the value of unlabeled data for classiﬁcation problems. In Proceedings of the Seventeenth International Conference on Machine Learning, 2000. X.  Zhu. Semi-supervised learning literature survey. 1530, Computer Sciences, University of Wisconsin-Madison, http://www.cs.wisc.edu/˜jerryzhu/pub/ssl survey.pdf.  1392  Technical 2005.  Report URL</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
