<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-26" href="../jmlr2007/jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">jmlr2007-26</a> <a title="jmlr-2007-26-reference" href="#">jmlr2007-26-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</h1>
<br/><p>Source: <a title="jmlr-2007-26-pdf" href="http://jmlr.org/papers/volume8/sugiyama07b/sugiyama07b.pdf">pdf</a></p><p>Author: Masashi Sugiyama</p><p>Abstract: Reducing the dimensionality of data without losing intrinsic information is an important preprocessing step in high-dimensional data analysis. Fisher discriminant analysis (FDA) is a traditional technique for supervised dimensionality reduction, but it tends to give undesired results if samples in a class are multimodal. An unsupervised dimensionality reduction method called localitypreserving projection (LPP) can work well with multimodal data due to its locality preserving property. However, since LPP does not take the label information into account, it is not necessarily useful in supervised learning scenarios. In this paper, we propose a new linear supervised dimensionality reduction method called local Fisher discriminant analysis (LFDA), which effectively combines the ideas of FDA and LPP. LFDA has an analytic form of the embedding transformation and the solution can be easily computed just by solving a generalized eigenvalue problem. We demonstrate the practical usefulness and high scalability of the LFDA method in data visualization and classiﬁcation tasks through extensive simulation studies. We also show that LFDA can be extended to non-linear dimensionality reduction scenarios by applying the kernel trick. Keywords: dimensionality reduction, supervised learning, Fisher discriminant analysis, locality preserving projection, afﬁnity matrix</p><br/>
<h2>reference text</h2><p>A. Albert. Regression and the Moore-Penrose Pseudoinverse. Academic Press, New York and London, 1972. N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society, 68:337–404, 1950. G. Baudat and F. Anouar. Generalized discriminant analysis using a kernel approach. Neural Computation, 12(10):2385–2404, 2000. M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373–1396, 2003. D. P. Bertsekas. On the Goldstein-Levitin-Polyak gradient projection method. IEEE Transactions on Automatic Control, 21(2):174–184, 1976. C. L. Blake and C. J. Merz. UCI repository of machine learning databases, 1998. http://www.ics.uci.edu/˜mlearn/MLRepository.html.  URL  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, 2004. F. R. K. Chung. Spectral Graph Theory. American Mathematical Society, Providence, R.I., 1997. D. Coomans, M. Broeckaert, M. Jonckheer, and D. L. Massart. Comparison of multivariate discriminant techniques for clinical data—Application to the thyroid functional state. Methods of Information in Medicine, 22:93–101, 1983. 1058  L OCAL F ISHER D ISCRIMINANT A NALYSIS  A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, series B, 39(1):1–38, 1977. R. O. Duda, P. E. Hart, and D. G. Stor. Pattern Classiﬁcation. Wiley, New York, 2001. N. Duffy and M. Collins. Convolution kernels for natural language. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, Cambridge, MA, 2002. MIT Press. B. S. Everitt, S. Landau, and M. Leese. Cluster Analysis. Arnold, London, 2001. R. A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2): 179–188, 1936. J. H. Friedman. Regularized discriminant analysis. Journal of the American Statistical Association, 84:165–175, 1989. K. Fukunaga. Introduction to Statistical Pattern Recognition. Academic Press, Inc., Boston, second edition, 1990. T. G¨ rtner. A survey of kernels for structured data. SIGKDD Explorations, 5(1):S268–S275, 2003. a T. G¨ rtner, P. Flach, and S. Wrobel. On graph kernels: Hardness results and efﬁcient alternatives. a In Proceedings of the Sixteenth Annual Conference on Computational Learning Theory, 2003. A. Globerson, G. Chechik, F. Pereira, and N. Tishby. Euclidean embedding of co-occurrence data. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 497–504. MIT Press, Cambridge, MA, 2005. ¨ A. Globerson and S. Roweis. Metric learning by collapsing classes. In Y. Weiss, B. Sch olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 451–458, Cambridge, MA, 2006. MIT Press. J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 513–520. MIT Press, Cambridge, MA, 2005. ¨ J. Ham, D. D. Lee, S. Mika, and B. Scholkopf. A kernel view of the dimensionality reduction of manifolds. In Proceedings of the Twenty-First International Conference on Machine Learning, New York, NY, 2004. ACM Press. T. Hastie and R. Tibshirani. Discriminant adaptive nearest neighbor classiﬁcation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(6):607–615, 1996a. T. Hastie and R. Tibshirani. Discriminant analysis by Gaussian mixtures. Journal of the Royal Statistical Society, Series B, 58(1):155–176, 1996b. ¨ X. He and P. Niyogi. Locality preserving projections. In S. Thrun, L. Saul, and B. Sch olkopf, editors, Advances in Neural Information Processing Systems 16. MIT Press, Cambridge, MA, 2004. 1059  S UGIYAMA  R. E. Henkel. Tests of Signiﬁcance. SAGE Publication, Beverly Hills, 1979. G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006. H. Kashima and T. Koyanagi. Kernels for semi-structured date. In Proceedings of the Nineteenth International Conference on Machine Learning, pages 291–298, San Francisco, CA, 2002. Morgan Kaufmann. H. Kashima, K. Tsuda, and A. Inokuchi. Marginalized kernels between labeled graphs. In Proceedings of the Twentieth International Conference on Machine Learning, San Francisco, CA, 2003. Morgan Kaufmann. T. Kohonen. Self-Organization and Associative Memory. Springer-Verlag, Berlin, 1989. R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete input spaces. In Proceedings of the Nineteenth International Conference on Machine Learning, pages 315–322, 2002. S. Kullback and R. A. Leibler. On information and sufﬁciency. Annals of Mathematical Statistics, 22:79–86, 1951. H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. Text classiﬁcation using string kernels. Journal of Machine Learning Research, 2:419–444, 2002. J. B. MacQueen. Some methods for classiﬁcation and analysis of multivariate observations. In Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281–297, Berkeley, CA., USA, 1967. University of California Press. ¨ S. Mika, G. R¨ tsch, J. Weston, B. Sch¨ lkopf, A. Smola, and K.-R. Muller. Constructing descripa o tive and discriminative nonlinear features: Rayleigh coefﬁcients in kernel feature spaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(5):623–628, 2003. ¨ G. R¨ tsch, T. Onoda, and K.-R. Muller. Soft margins for adaboost. Machine Learning, 42(3): a 287–320, 2001. S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000. L. K. Saul and S. T. Roweis. Think globally, ﬁt locally: Unsupervised learning of low dimensional manifolds. Journal of Machine Learning Research, 4(Jun):119–155, 2003. ¨ B. Sch¨ lkopf, A. Smola, and K.-R. Muller. Nonlinear component analysis as a kernel eigenvalue o problem. Neural Computation, 10(5):1299–1319, 1998. B. Sch¨ lkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002. o M. Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the Royal Statistical Society, Series B, 36:111–147, 1974. 1060  L OCAL F ISHER D ISCRIMINANT A NALYSIS  J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. V. N. Vapnik. Statistical Learning Theory. Wiley, New York, 1998. G. Wahba. Spline Model for Observational Data. Society for Industrial and Applied Mathematics, Philadelphia and Pennsylvania, 1990. K. Weinberger, J. Blitzer, and L. Saul. Distance metric learning for large margin nearest neighbor ¨ classiﬁcation. In Y. Weiss, B. Scholkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 1473–1480, Cambridge, MA, 2006. MIT Press. L. Zelnik-Manor and P. Perona. Self-tuning spectral clustering. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 1601–1608. MIT Press, Cambridge, MA, 2005.  1061</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
