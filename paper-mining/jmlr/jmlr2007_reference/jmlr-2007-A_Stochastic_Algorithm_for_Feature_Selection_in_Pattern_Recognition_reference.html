<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-7" href="../jmlr2007/jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">jmlr2007-7</a> <a title="jmlr-2007-7-reference" href="#">jmlr2007-7-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</h1>
<br/><p>Source: <a title="jmlr-2007-7-pdf" href="http://jmlr.org/papers/volume8/gadat07a/gadat07a.pdf">pdf</a></p><p>Author: Sébastien Gadat, Laurent Younes</p><p>Abstract: We introduce a new model addressing feature selection from a large dictionary of variables that can be computed from a signal or an image. Features are extracted according to an efﬁciency criterion, on the basis of speciﬁed classiﬁcation or recognition tasks. This is done by estimating a probability distribution P on the complete dictionary, which distributes its mass over the more efﬁcient, or informative, components. We implement a stochastic gradient descent algorithm, using the probability as a state variable and optimizing a multi-task goodness of ﬁt criterion for classiﬁers based on variable randomly chosen according to P. We then generate classiﬁers from the optimal distribution of weights learned on the training set. The method is ﬁrst tested on several pattern recognition problems including face detection, handwritten digit recognition, spam classiﬁcation and micro-array analysis. We then compare our approach with other step-wise algorithms like random forests or recursive feature elimination. Keywords: stochastic learning algorithms, Robbins-Monro application, pattern recognition, classiﬁcation algorithm, feature selection</p><br/>
<h2>reference text</h2><p>Y. Amit and D. Geman. A computational model for visual selection. Neural Computation, 11(7): 1691 – 1715, 1999. Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation, 9(7):1545–1588, 1997. M. Bena¨m. Convergence with probability one of stochastic approximation algorithms whose averı age is cooperative. Nonlinearity, 13(3):601–616, 2000. M. Benaim. A dynamical system approach to stochastic approximations. SIAM Journal on Control and Optimization, 34(2):437–472, 1996. A. Benveniste, M. M´ tivier, and P. Priouret. Adaptive Algorithms and Stochastic Approximations, e volume 22 of Applications of Mathematics (New York). Springer-Verlag, Berlin, 1990. J. Bins and B. A. Draper. Feature selection from huge feature sets. In Proceedings of the Eighth International Conference On Computer Vision, volume 2, pages 159–165, 2001. A. Blum and R. L. Rivest. Training a 3-node neural network is NP-complete. Neural Networks, 5 (1):117– 127, 1992. L. Breiman. Arcing classiﬁers. The Annals of Statistics, 26(3):801–849, 1998. L. Breiman. Random forests. Machine Learning, 45(1):5–32, 2001. R. Buche and H. J. Kushner. Rate of convergence for constrained stochastic approximation algorithms. SIAM Journal on Control Optimization, 40(4):1011–1041, 2001. O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector machines. Machine Learning, 46(1):131–159, 2002. Y. W. Chen and C. J. Lin. Combining SVMs with various feature selection strategies. In Feature extraction, foundations and applications. Springer-Verlag, Berlin, 2006. 544  A S TOCHASTIC A LGORITHM FOR F EATURE S ELECTION IN PATTERN R ECOGNITION  S. Cohen, E. Ruppin, and G. Dror. Feature selection based on the Shapley value. In International Joint Conference on Artiﬁcial Intelligence, pages 665–670, 2005. T. Cover and J. Thomas. Information Theory. Wiley, New York, 1991. K. Deb and R. Reddy. Reliable classiﬁcation of two-class cancer data using evolutionary algorithms. BioSystems, 72(1):111 – 129, 2003. T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting output codes. Journal of Artiﬁcial Intelligence Research, 2:263–286, 1995. C.L. Blake D.J. Newman, S. Hettich and C.J. Merz. UCI repository of machine learning databases, 1998. URL http://www.ics.uci.edu/∼mlearn/MLRepository.html. M. Duﬂo. Algorithmes Stochastiques, volume 23 of Math´ matiques & Applications. Springere Verlag, Berlin, 1996. K. Dune, P. Cunningham, and F. Azuaje. Solutions to instability problems with sequential wrapperbased approaches to feature selection. In Technical Report-2002-28, Department of Computer Science Courses, Trinity College, Dublin, 2002. P. Dupuis and H. Ishii. On Lipschitz continuity of the solution mapping to the Skorokhod problem, with applications. Stochastics and Stochastics Reports, 35(1):31–62, 1991. P. Dupuis and K. Ramanan. Convex duality and the Skorokhod problem. I, II. Probability Theory and Related Fields, 115(2):153–195, 197–236, 1999. F. Fleuret. Fast binary feature selection with conditional mutual information. Journal of Machine Learning Research, 5:1531–1555, 2004. F. Fleuret and D. Geman. Coarse-to-ﬁne face detection. International Journal of Computer Vision, 41(1-2):85–107, 2001. J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. The Annals of Statistics, 28(2):337–407, 2000. S. Gadat. Apprentissage d’un vocabulaire symbolique pour la d´ tection d’objets dans une image. e ´ PhD thesis, Ecole Normale Sup´ rieure de Cachan, France, 2004. e S. Geman, E. Bienenstock, and R. Doursat. Neural networks and the bias/variance dilemma. Neural Computation, 4(1):1–58, 1992. R. Gilad-Bachrach, A. Navot, and N. Tishby. Margin based feature selection - theory and algorithms. In Proceedings of the 21rd International Conference on Machine Learning, 2004. T.R. Golub, D.K. Slonim, P. Tamazyo, C. Huard, M. Gaasenbeek, J.P. Mesirov, H. Coller, M.L. Loh, J.R. Downing, M.A. Caligiuri, C.D. Bloomﬁeld, and E.S. Lander. Molecular classiﬁcation of cancer: Class discovery and class prediction by gene expression monitoring. Science, 286: 531– 537, 1999. 545  G ADAT AND YOUNES  I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for cancer classiﬁcation using support vector machines. Machine Learning, 46(1-3):389–422, 2002. I. Guyon, S. Gunn, A. B. Hur, and G. Dror. Result analysis of the nips 2003 feature selection challenge. In Proceedings of the Neural Information Processing Systems, pages 545 – 552, 2004. I. Guyon, J. Li, T. Mader, P. Pletscher, G. Schneider, and M. Uhr. Teaching machine learning from examples. Unpublished, 2006. T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning : Data Mining, Inference, and Prediction. Springer-Verlag, Berlin, 2001. T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining, 2002. T. Joachims and R. Klinkenberg. Detecting concept drift with support vector machines. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 487–494, 2000. C. Jutten and J. H´ rault. Blind separation of sources, part 1: An adaptative algorithm bases on e neuromimetic architecture. Signal Processing, 24(1):1–10, 1991. H. J. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms and Applications, volume 35 of Applications of Mathematics. Springer-Verlag, New York, second edition, 2003. ¨ T. N. Lal, O. Chapelle, and B. Scholkopf. Combining a ﬁlter method with svms. In Feature extraction, foundations and applications. Springer-Verlag, 2006. S. K. Lee, S. J. Yi, and B. T. Zhang. Combining information-based supervised and unsupervised feature selection. In Feature extraction, foundations and Applications. Springer-Verlag, 2006. Y. Liu and J. R. Render. Video retrieval under sparse training data. In Proceedings of the Second International Conference on Image and Video Retrieval, pages 406–413, 2003. D.J.C. MacKay. A practical Bayesian framework for back propagation networks. Neural Computation, 3:448–472, 1992. B. Sch¨ lkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regularization, o Optimization, and Beyond. MIT Press, Cambridge, 2002. B. Sch¨ lkopf, C. Burges, and V. Vapnik. Extracting support data for a given task. In Proceedings of o the First International Conference on Knowledge Discovery and Data Mining, pages 252–257, 1995. P. Simard and Y. LeCun. Memory based character recognition using a transformation invariant metric. In Proceedings of the 12th International Conference on Pattern Recognition, pages 262– 267, 1998. S. Singhi and H. Liu. Feature subset selection bias for classiﬁcation learning. In Proceedings of the 23rd International Conference on Machine Learning, pages 849 – 856, 2006. 546  A S TOCHASTIC A LGORITHM FOR F EATURE S ELECTION IN PATTERN R ECOGNITION  Y. Sun and J. Li. Iterative relief for feature weighting. In Proceedings of the 23rd International Conference on Machine Learning, pages 913 – 920, 2006. V. Vapnik. The Nature of Statistical Learning Theory. Statistics for Engineering and Information Science. Springer-Verlag, New York, second edition, 2000. V. Vapnik. Statistical Learning Theory. Adaptive and Learning Systems for Signal Processing, Communications, and Control. John Wiley & Sons Inc., New York, 1998. J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Poggio, and V. Vapnik. Feature selection for SVMs. In Neural Information Processing Systems, pages 668–674, 2000. J. Weston, A. Elisseeff, B. Schlkopf, and M. E. Tipping. Use of the zero-norm with linear models and kernel methods. Journal of Machine Learning Research, 3:1439–1461, 2003. Z. Wu and C. Li. Feature selection for classiﬁcation using transductive support vector machines. In Feature Extraction, Foundations and Applications. Springer-Verlag, Berlin, 2006. E. Xing, M. I. Jordan, and S. Russel. Feature selection for high-dimensional genomic microarray data. In Proceedings of the Eighteenth Internation Conference on Machine Learning, pages 601– 608, 2001. L. Yu and H. Liu. Efﬁcient feature selection via analysis of relevance and redundancy. Journal of Machine Learning Research, 5:1205–1224, 2004.  547</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
