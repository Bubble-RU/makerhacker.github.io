<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-3" href="../jmlr2007/jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">jmlr2007-3</a> <a title="jmlr-2007-3-reference" href="#">jmlr2007-3-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</h1>
<br/><p>Source: <a title="jmlr-2007-3-pdf" href="http://jmlr.org/papers/volume8/banerjee07a/banerjee07a.pdf">pdf</a></p><p>Author: Arindam Banerjee, Inderjit Dhillon, Joydeep Ghosh, Srujana Merugu, Dharmendra S. Modha</p><p>Abstract: Co-clustering, or simultaneous clustering of rows and columns of a two-dimensional data matrix, is rapidly becoming a powerful data analysis technique. Co-clustering has enjoyed wide success in varied application domains such as text clustering, gene-microarray analysis, natural language processing and image, speech and video analysis. In this paper, we introduce a partitional co-clustering formulation that is driven by the search for a good matrix approximation—every co-clustering is associated with an approximation of the original data matrix and the quality of co-clustering is determined by the approximation error. We allow the approximation error to be measured using a large class of loss functions called Bregman divergences that include squared Euclidean distance and KL-divergence as special cases. In addition, we permit multiple structurally different co-clustering schemes that preserve various linear statistics of the original data matrix. To accomplish the above tasks, we introduce a new minimum Bregman information (MBI) principle that simultaneously generalizes the maximum entropy and standard least squares principles, and leads to a matrix approximation that is optimal among all generalized additive models in a certain natural parameter space. Analysis based on this principle yields an elegant meta algorithm, special cases of which include most previously known alternate minimization based clustering algorithms such as kmeans and co-clustering algorithms such as information theoretic (Dhillon et al., 2003b) and minimum sum-squared residue co-clustering (Cho et al., 2004). To demonstrate the generality and ﬂexibility of our co-clustering framework, we provide examples and empirical evidence on a varic 2007 Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, Srujana Merugu and Dharmendra Modha. BANERJEE , D HILLON , G HOSH , M ERUGU AND M ODHA ety of problem domains and also describe novel co-clustering applications such as missing value prediction and</p><br/>
<h2>reference text</h2><p>K. S. Azoury and M. K. Warmuth. Relative loss bounds for on-line density estimation with the exponential family of distributions. Machine Learning, 43(3):211–246, 2001. A. Banerjee, X. Guo, and H. Wang. On the optimality of conditional expectation as a Bregman predictor. IEEE Transactions on Information Theory, 51(7):2664–2669, July 2005a. A. Banerjee, S. Merugu, I. Dhillon, and J. Ghosh. Clustering with Bregman divergences. Journal of Machine Learning Research, 6:1705–1749, 2005b. H. H. Bauschke and J. M. Borowein. Legendre functions and the method of random Bregman projections. Journal of Convex Analysis, 4(1):27–67, 1997. 1983  BANERJEE , D HILLON , G HOSH , M ERUGU AND M ODHA  R. Bekkerman, R. El-Yaniv, and A. McCallum. Multi-way distributional clustering via pairwise interactions. In Proceedings of the International Conference on Machine Learning (ICML), pages 41–48, 2005. L. M. Bregman. The relaxation method of ﬁnding the common point of convex sets and its application to the solution of problems in convex programming. USSR Computational Mathematics and Physics, 7:200–217, 1967. R. Cai, L. Lu, and L. Cai. Unsupervised auditory scene categorization via key audio effects and information-theoretic co-clustering. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP05), pages 1073–1076, 2005. J. J. M. Carrasco, D. Fain, K. Lang, and L. Zhukov. Clustering of bipartite advertiser-keyword graph. In Proceedings of the Workshop on Large Scale Clustering, ICDM, 2003. Y. Censor and S. Zenios. Parallel Optimization: Theory, Algorithms, and Applications. Oxford University Press, 1998. D. Chakrabarti, S. Papadimitriou, D. S. Modha, and C. Faloutsos. Fully automatic crossassociations. In Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD), pages 79–88, 2004. Y. Cheng and G. M. Church. Biclustering of expression data. In Proceedings of the 8th International Conference on Intelligent Systems for Molecular Biology (ISMB), pages 93–103, 2000. H. Cho, I. S. Dhillon, Y. Guan, and S. Sra. Minimum sum-squared residue co-clustering of gene expression data. In Proceedings of the 4th SIAM International Conference on Data Mining (SDM), pages 114–125, 2004. M. Collins, R. E. Schapire, and Y. Singer. Logistic regression, adaboost and bregman distances. In Proceedings of the 13th Annual Conference on Computational Learing Theory (COLT), pages 158–169, 2000. T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley-Interscience, 1991. I. Csisz´ r. Why least squares and maximum entropy? An axiomatic approach to inference for linear a inverse problems. The Annals of Statistics, 19:2032–2066, 1991. S. Della Pietra, V. Della Pietra, and J. Lafferty. Duality and auxiliary functions for Bregman distances. Technical Report CMU-CS-01-109, School of Computer Science, Carnegie Mellon University, 2001. I. Dhillon, S. Mallela, and R. Kumar. A divisive information-theoretic feature clustering algorithm for text classiﬁcation. Journal of Machine Learning Research, 3(4):1265–1287, 2003a. I. Dhillon, S. Mallela, and D. Modha. Information-theoretic co-clustering. In Proceedings of the 9th International Conference on Knowledge Discovery and Data Mining (KDD), pages 89–98, 2003b. 1984  B REGMAN C O - CLUSTERING AND M ATRIX A PPROXIMATION  I. S. Dhillon. Co-clustering documents and words using bipartite spectral graph partitioning. In Proceedings of the 7th International Conference on Knowledge Discovery and Data Mining, pages 269–274, 2001. I. S. Dhillon and D. S. Modha. Concept decompositions for large sparse text data using clustering. Machine Learning, 42(1):143–175, January 2001. D. Freitag. Trained named entity recognition using distributional clusters. In EMNLP, pages 262– 269, 2004. B. Gao, T. Liu, X. Zheng, Q. Cheng, and W. Ma. Consistent bipartite graph co-partitioning for starstructured high-order heterogeneous data co-clustering. In Proceedings of the 11th International Conference on Knowledge Discovery and Data Mining (KDD), pages 41–50, 2005. T. George and S. Merugu. A scalable collaborative ﬁltering framework based on co-clustering. In Proceedings of the IEEE Conference on Data Mining, pages 625–628, 2005. J. Ghosh. Scalable clustering. In Nong Ye, editor, The Handbook of Data Mining, pages 247–277. Lawrence Erlbaum Assoc., 2003. GroupLens. Movielens data set. http://www.cs.umn.edu/Research/GroupLens/data/ml-data.tar.gz. P. D. Gr¨ nwald and A. Dawid. Game theory, maximum entropy, minimum discrepancy, and robust u Bayesian decision theory. Annals of Statistics, 32(4), 2004. J. Guan, G. Qiu, and X. Y. Xue. Spectral images and features co-clustering with application to content-based image retrieval. In IEEE Workshop on Multimedia Signal Processing, 2005. J. A. Hartigan. Direct clustering of a data matrix. Journal of the American Statistical Association, 67(337):123–129, 1972. T. Hofmann. Latent semantic models for collaborative ﬁltering. ACM Transactions on Information Systems, 22(1):89–115, 2004. T. Hofmann and J. Puzicha. Unsupervised learning from dyadic data. Technical Report ICSI TR98-042, International Computer Science Institute (ICSI), Berkeley, 1998. A. K. Jain and R. C. Dubes. Algorithms for Clustering Data. Prentice Hall, New Jersey, 1988. E. T. Jaynes. Information theory and statistical mechanics. Physical Reviews, 106:620–630, 1957. Y. Kluger, R. Basri, J. T. Chang, and M. Gerstein. Spectral biclustering of microarray data: Coclustering genes and conditions. Genome Research, 13(4):703–716, 2003. J. Lafferty. Additive models, boosting, and inference for generalized divergences. In Proceedings of the 13th Annual Conference on Computational Learing Theory (COLT), 1999. D. L. Lee and S. Seung. Algorithms for non-negative matrix factorization. In Proceedings of the 14th Annual Conference on Neural Information Processing Systems (NIPS), pages 556–562, 2001. 1985  BANERJEE , D HILLON , G HOSH , M ERUGU AND M ODHA  H. Li and N. Abe. Word clustering and disambiguation based on co-occurence data. In COLINGACL, pages 749–755, 1998. T. Li. A general model for clustering binary data. In Proceedings of the 11th International Conference on Knowledge Discovery and Data Mining (KDD), pages 188–197, 2005. Z. Lin and R.B. Altman. Finding haplotype tagging snps by use of principal components analysis. The American Journal of Human Genetics, 75:850–861, 2004. S. C. Madeira and A. L. Oliveira. Biclustering algorithms for biological data analysis: A survey. IEEE Trans. Computational Biology and Bioinformatics, 1(1):24–45, 2004. C. H. Papadimitriou, P. Raghavan, H. Tamaki, and S. Vempala. Latent semantic indexing: A probabilistic analysis. In Proceedings of the 16th Annual ACM Symposium on Principles of Distributed Computing (PODC), pages 159–168, 1998. L. Parsons, E. Haque, and H. Liu. Subspace clustering for high dimensinal data: A review. ACM SIGKDD Explorations, 6(1):90–105, 2004. G. Qiu. Image and feature co-clustering. In Proceedings of the International Conference on Pattern Recognition, pages 991–994, 2004. P. Resnick, N. Iacovou, M. Suchak, P. Bergstorm, and J. Riedl. GroupLens: An Open Architecture for Collaborative Filtering of Netnews. In Proceedings of the ACM Conference on CSCW, pages 175–186, 1994. R. T. Rockafellar. Convex Analysis. Princeton Landmarks in Mathematics. Princeton University Press, 1970. R. Rohwer and D. Freitag. Towards full automation of lexicon construction. In Dan Moldovan and Roxana Girju, editors, HLT-NAACL 2004: Workshop on Computational Lexical Semantics, pages 9–16, 2004. B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Application of dimensionality reduction in recommender systems–a case study. In WebKDD Workshop., 2000. J. Shore and R. Johnson. Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross entropy. IEEE Transactions on Information Theory, 26(1):26–37, 1980. A. Strehl and J. Ghosh. Cluster ensembles – a knowledge reuse framework for combining partitionings. Journal of Machine Learning Research, 3(3):583–617, 2002. H. Takamura and Y. Matsumoto. Co-clustering for text categorization. Information Processing Society of Japan Journal, 2003. H. Zhong, J. Shi, and M. Visontai. Detecting unusual activity in video. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, pages 819–826, 2004.  1986</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
