<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-65" href="../jmlr2007/jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">jmlr2007-65</a> <a title="jmlr-2007-65-reference" href="#">jmlr2007-65-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</h1>
<br/><p>Source: <a title="jmlr-2007-65-pdf" href="http://jmlr.org/papers/volume8/laviolette07a/laviolette07a.pdf">pdf</a></p><p>Author: François Laviolette, Mario Marchand</p><p>Abstract: We propose a PAC-Bayes theorem for the sample-compression setting where each classiﬁer is described by a compression subset of the training data and a message string of additional information. This setting, which is the appropriate one to describe many learning algorithms, strictly generalizes the usual data-independent setting where classiﬁers are represented only by data-independent message strings (or parameters taken from a continuous set). The proposed PAC-Bayes theorem for the sample-compression setting reduces to the PAC-Bayes theorem of Seeger (2002) and Langford (2005) when the compression subset of each classiﬁer vanishes. For posteriors having all their weights on a single sample-compressed classiﬁer, the general risk bound reduces to a bound similar to the tight sample-compression bound proposed in Laviolette et al. (2005). Finally, we extend our results to the case where each sample-compressed classiﬁer of a data-dependent ensemble may abstain of predicting a class label. Keywords: PAC-Bayes, risk bounds, sample-compression, set covering machines, decision list machines</p><br/>
<h2>reference text</h2><p>Leo Breiman. Bagging predictors. Machine Learning, 24:123–140, 1996. ´ Olivier Catoni. A PAC-Bayesian approach to adaptive classiﬁcation. Thecnical report, Universit e Paris 6, 2004. Thomas M. Cover and Joy A. Thomas. Elements of Information Theory, chapter 12. Wiley, 1991. Richard O. Duda, Peter E. Hart, and David G. Stork. Pattern Classiﬁcation. Wiley, 2000. Sally Floyd and Manfred Warmuth. Sample compression, learnability, and the VapnikChervonenkis dimension. Machine Learning, 21(3):269–304, 1995. Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55:119–139, 1997. 1485  L AVIOLETTE AND M ARCHAND  Pascal Germain, Alexandre Lacasse, Francois Laviolette, and Mario Marchand. A pac-bayes risk ¨ bound for general loss functions. In B. Scholkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19. MIT Press, Cambridge, MA, 2007. Thore Graepel, Ralf Herbrich, and John Shawe-Taylor. PAC-Bayesian compression bounds on the prediction error of learning algorithms for classiﬁcation. Machine Learning, 59:55–76, 2005. Ralf Herbrich, Thore Graepel, and Colin Campbell. Bayes point machines. Journal of Machine Learning Research, 1:245–279, 2001. Alexandre Lacasse, Francois Laviolette, Mario Marchand, Pascal Germain, and Nicolas Usunier. PAC-Bayes bounds for the risk of the majority vote and the variance of the Gibbs classiﬁer. In B. Sch¨ lkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing o Systems 19. MIT Press, Cambridge, MA, 2007. John Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine Learning Research, 6:273–306, 2005. John Langford and John Shawe-Taylor. PAC-Bayes & margins. In S. Thrun S. Becker and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 423–430. MIT Press, Cambridge, MA, 2003. Francois Laviolette and Mario Marchand. PAC-Bayes risk bounds for sample-compressed Gibbs ¸ classiﬁers. Proceedings of the 22nth International Conference on Machine Learning (ICML 2005), pages 481–488, 2005. Francois Laviolette, Mario Marchand, and Mohak Shah. Margin-sparsity trade-off for the set cover¸ ing machine. Proceedings of the 16th European Conference on Machine Learning (ECML 2005); Lecture Notes in Artiﬁcial Intelligence, 3720:206–217, 2005. Francois Laviolette, Mario Marchand, and Mohak Shah. A PAC-Bayes approach to the set covering ¸ machine. Proceedings of the 2005 conference on Neural Information Processing Systems (NIPS 2005), 2006. Nicholas Littlestone and Manfred K. Warmuth. Relating data compression and learnability. Technical report, University of California Santa Cruz, Santa Cruz, CA, 1986. Mario Marchand and John Shawe-Taylor. The set covering machine. Journal of Machine Learning Reasearch, 3:723–746, 2002. Mario Marchand and Marina Sokolova. Learning with decision lists of data-dependent features. Journal of Machine Learning Reasearch, 6:427–451, 2005. David McAllester. Some PAC-Bayesian theorems. Machine Learning, 37:355–363, 1999. David McAllester. PAC-Bayesian stochastic model selection. Machine Learning, 51:5–21, 2003a. David McAllester. Simpliﬁed PAC-Bayesian margin bounds. Proceedings of the 16th Annual Conference on Learning Theory, Lecture Notes in Artiﬁcial Intelligence, 2777:203–215, 2003b. 1486  S AMPLE -C OMPRESSED PAC-BAYES B OUNDS  Douglas L. Reilly, Leon N. Cooper, and Charles Elbaum. A neural model for category learning. Biological Cybernetics, 45:35–41, 1982. Ronald L. Rivest. Learning decision lists. Machine Learning, 2:229–246, 1987. Matthias Seeger. PAC-Bayesian generalization bounds for gaussian processes. Journal of Machine Learning Research, 3:233–269, 2002. Matthias Seeger. Bayesian gaussian process models: PAC-Bayesian generalisation error bounds and sparse approximations. PhD Thesis, University of Edinburgh, 2003. Leslie G. Valiant. A theory of the learnable. Communications of the Association of Computing Machinery, 27(11):1134–1142, November 1984.  1487</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
