<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-24" href="../jmlr2007/jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">jmlr2007-24</a> <a title="jmlr-2007-24-reference" href="#">jmlr2007-24-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</h1>
<br/><p>Source: <a title="jmlr-2007-24-pdf" href="http://jmlr.org/papers/volume8/nilsson07a/nilsson07a.pdf">pdf</a></p><p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><br/>
<h2>reference text</h2><p>Hussein Almuallim and Thomas G. Dietterich. Learning with many irrelevant features. In Proceedings of the Ninth National Conference on Artiﬁcial Intelligence, volume 2, pages 547–552, 1991. AAAI Press. David A. Bell and Hui Wang. A formalism for relevance and its application in feature subset selection. Machine Learning, 41:175–195, 2000. Avril L. Blum and Pat Langley. Selection of relevant features and examples in machine learning. Artiﬁcial Intelligence, 97:245–271, 1997. George Casella and Roger L. Berger. Statistical Inference. Duxbury, 2nd edition, 2002. David Chickering and Christopher Meek. Finding optimal Bayesian networks. In Proceedings of the 18th Annual Conference on Uncertainty in Artiﬁcial Intelligence, pages 94–102, San Francisco, CA, 2002. Morgan Kaufmann Publishers. David Maxwell Chickering, David Heckerman, and Christopher Meek. Large-sample learning of Bayesian networks is NP-hard. Journal of Machine Learning Research, 5:1287–1330, 2004. 610  C ONSISTENT F EATURE S ELECTION IN P OLYNOMIAL T IME  Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995. Thomas M. Cover and Jan M. van Campenhout. On the possible orderings of the measurement selection problem. IEEE Transactions on Systems, Man, and Cybernetics, 7(9):657–661, 1977. James E. Darnell. Transcription factors as targets for cancer therapy. Nature Reviews Cancer, 2: 740–749, 2002. Luc Devroye, L´ zl´ Gy¨ rﬁ, and G´ bor Lugosi. A Probabilistic Theory of Pattern Recognition. a o o a Springer-Verlag, New York, 1996. Thomas G. Dietterich. Approximate statistical tests for comparing supervised classiﬁcation learning algorithms. Neural Computation, 10:1895–1923, 1998. Nir Friedman. Inferring Cellular Networks Using Probabilistic Graphical Models. Science, 303: 799–805, 2004. Todd R. Golub, Donna K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J.P. Mesirov, H. Coller, M.L. Loh, J.R. Downing, M.A. Caligiuri, C.D. Bloomﬁeld, and Eric S. Lander. Molecular classiﬁation of cancer: Class discovery and class prediction by gene expression monitoring. Science, 286:531–537, 1999. ¨ Arthur Gretton, Ralf Herbrich, Alexander Smola, Olivier Bousquet, and Bernhard Sch olkopf. Kernel methods for measuring independence. Journal of Machine Learning Research, 6:2075–2129, 2005. Isabelle Guyon and Andr´ Elisseeff. An introduction to variable and feature selection. Journal of e Machine Learning Research, 3:1157–1182, 2003. Isabelle Guyon, Jason Weston, Stephen Barnhill, and Vladimir Vapnik. Gene selection for cancer classiﬁcation using support vector machines. Machine Learning, 46:389–422, 2002. Douglas P. Hardin, Constantin Aliferis, and Ioannis Tsamardinos. A theoretical characterization of SVM-based feature selection. In Proceedings of the 21st International Conference on Machine Learning, 2004. Michael J. Holland. Transcript abundance in yeast varies over six orders of magnitude. Journal of Biological Chemistry, 277:14363–66, 2002. Anil K. Jain and William G. Waller. On the optimal number of features in the classiﬁcation of multivariate gaussian data. Pattern Recognition, 10:365–374, 1978. George H. John, Ron Kohavi, and Karl Pﬂeger. Irrelevant features and the subset selection problem. In Proceedings of the 11th International Conference on Machine Learning, pages 121–129, 1994. Markus Kalisch and Peter B¨ hlmann. Estimating high-dimensional directed acyclic graphs with the u ¨ PC-algorithm. Technical report, Seminar fur Statistik, ETH Z¨ rich, Switzerland, 2005. u S. Sathiya Keerthi. Efﬁcient tuning of SVM hyperparameters using radius/margin bound and iterative algorithms. IEEE Transactions on Neural Networks, 13(5):1225–1229, 2002. 611  ˜ ¨ ´ N ILSSON , P E NA , B J ORKEGREN AND T EGN E R  Kenji Kira. and Larry A. Rendell. The feature selection problem: Traditional methods and a new algorithm. In Proceedings of the Ninth National Conference on Artiﬁcial Intelligence, pages 129–134, 1992. Ron Kohavi and George H. John. Wrappers for feature subset selection. Artiﬁcial Intelligence, 97: 273–324, 1997. Daphne Koller and Mehran Sahami. Towards optimal feature selection. In Proceedings of the 13th International Conference of Machine Learning, pages 248–292, 1996. David J. Newman, S. Hettich, C.L. Blake, and C.J. Merz. UCI repository of machine learning databases, 1998. URL http://www.ics.uci.edu/∼mlearn/MLRepository.html. Judea Pearl. Probabilistic reasoning in intelligent systems. Morgan Kauffman Publishers, Inc., San Fransisco, California, 1988. Jos´ M. Pe˜ a, Johan Bj¨ rkegren, and Jesper Tegn´ r. Scalable, efﬁcient and correct learning of e n o e markov boundaries under the faithfulness assumption. In Proceedings of the Eighth European Conference on Symbolic and Quantitative Approaches to Reasoning under Uncertainty, pages 136–147, 2005. Jose M. Pe˜ a, Roland Nilsson, Johan Bj¨ rkegren, and Jesper Tegn´ r. Identifying the relevant nodes n o e before learning the structure. In Proceedings of the 22nd Conference on Uncertainty in Artiﬁcial Intelligence, pages 367–374, 2006. Donna K. Slonim. From patterns to pathways: Gene expression comes of age. Nature Genetics, 32: 502–508, 2002. Supplement. Ingo Steinwart. On the inﬂuence of the kernel on the consistency of support vector machines. Journal of Machine Learning Research, 2:67–93, 2002. Milan Studen´ . Probabilistic Conditional Independence Structures. Springer, 1st edition, 2004. y Ioannis Tsamardinos and Constantin Aliferis. Towards principled feature selection: Relevancy, ﬁlters and wrappers. In Proceedings of the Ninth International Workshop on Artiﬁcial Intelligence and Statistics, 2003. Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984. Vladimir N. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, 2nd edition, 2000. Lei Yu and Huan Liu. Efﬁcient feature selection via analysis of relevance and redundancy. Journal of Machine Learning Research, 5:1205–1224, 2004.  612</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
