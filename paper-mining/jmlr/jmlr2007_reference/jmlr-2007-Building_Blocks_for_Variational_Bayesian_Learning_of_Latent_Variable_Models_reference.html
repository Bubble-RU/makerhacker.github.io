<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-17" href="../jmlr2007/jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">jmlr2007-17</a> <a title="jmlr-2007-17-reference" href="#">jmlr2007-17-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</h1>
<br/><p>Source: <a title="jmlr-2007-17-pdf" href="http://jmlr.org/papers/volume8/raiko07a/raiko07a.pdf">pdf</a></p><p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><br/>
<h2>reference text</h2><p>B. Anderson and J. Moore. Optimal Filtering. Prentice-Hall, Englewood Cliffs, NJ, 1979. H. Attias. ICA, graphical models and variational methods. In S. Roberts and R. Everson, editors, Independent Component Analysis: Principles and Practice, pages 95–112. Cambridge University Press, 2001. H. Attias. Independent factor analysis. Neural Computation, 11(4):803–851, 1999. H. Attias. A variational Bayesian framework for graphical models. In T. Lee et al., editor, Advances in Neural Information Processing Systems 12, pages 209–215, Cambridge, 2000. MIT Press. D. Barber and C. M. Bishop. Ensemble learning in Bayesian neural networks. In C. M. Bishop, editor, Neural Networks and Machine Learning, pages 215–237. Springer, Berlin, 1998. M. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, University of London, UK, 2003. M. Beal and Z. Ghahramani. The variational Bayesian EM algorithm for incomplete data: with application to scoring graphical model structures. Bayesian Statistics 7, pages 453–464, 2003. C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. C. M. Bishop. Neural Networks for Pattern Recognition. Clarendon Press, 1995. C. M. Bishop. Latent variable models. In M. I. Jordan, editor, Learning in Graphical Models, pages 371–403. The MIT Press, Cambridge, MA, USA, 1999. J.-F. Cardoso. Multidimensional independent component analysis. In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP’98), pages 1941–1944, Seattle, Washington, USA, May 12–15, 1998. K. Chan, T.-W. Lee, and T. Sejnowski. Variational learning of clusters of undercomplete nonsymmetric independent components. In Proc. Int. Conf. on Independent Component Analysis and Signal Separation (ICA2001), pages 492–497, San Diego, USA, 2001. R. Choudrey, W. Penny, and S. Roberts. An ensemble learning approach to independent component analysis. In Proc. of the IEEE Workshop on Neural Networks for Signal Processing, Sydney, Australia, December 2000, pages 435–444. IEEE Press, 2000. 195  R AIKO , VALPOLA , H ARVA AND K ARHUNEN  P. Dayan and R. Zemel. Competition and multiple cause models. Neural Computation, 7(3):565– 579, 1995. A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. J. of the Royal Statistical Society, Series B (Methodological), 39(1):1–38, 1977. A. Doucet, N. de Freitas, and N. J. Gordon. Sequential Monte Carlo Methods in Practice. Springer Verlag, 2001. B. J. Frey and G. E. Hinton. Variational learning in nonlinear Gaussian belief networks. Neural Computation, 11(1):193–214, 1999. A. Gelfand and A. Smith. Sampling-based approaches to calculating marginal densities. Journal of the American Statistical Association, 85(410):398–409, 1990. A. Gelman, J. Carlin, H. Stern, and D. Rubin. Bayesian Data Analysis. Chapman & Hall/CRC Press, Boca Raton, Florida, 1995. S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741, 1984. Z. Ghahramani and M. Beal. Propagation algorithms for variational Bayesian learning. In T. Leen, T. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 507–513. The MIT Press, Cambridge, MA, USA, 2001. Z. Ghahramani and G. E. Hinton. Hierarchical non-linear factor analysis and topographic maps. In M. I. Jordan, M. J. Kearns, and S. A. Solla, editors, Advances in Neural Information Processing Systems 10, pages 486–492. The MIT Press, Cambridge, MA, USA, 1998. Z. Ghahramani and S. Roweis. Learning nonlinear dynamical systems using an EM algorithm. In M. Kearns, S. Solla, and D. Cohn, editors, Advances in Neural Information Processing Systems 11, pages 431–437. The MIT Press, Cambridge, MA, USA, 1999. A. Gray, B. Fischer, J. Schumann, and W. Buntine. Automatic derivation of statistical algorithms: The EM family and beyond. In Advances in Neural Information Processing Systems 15, 2002. URL http://www.hiit.fi/u/buntine/nips2002.htm. M. Harva. Hierarchical Variance Models of Image Sequences. Helsinki Univ. of Technology, Dept. of Computer Science and Eng., Espoo, Finland, March 2004. Master of Science (Dipl.Eng.) thesis. Available at http://www.cis.hut.fi/mha. M. Harva and A. Kab´ n. Variational learning for rectiﬁed factor analysis. Signal Processing, 87(3): a 509–527, 2007. M. Harva, T. Raiko, A. Honkela, H. Valpola, and J. Karhunen. Bayes Blocks: An implementation of the variational Bayesian building blocks framework. In Proceedings of the 21st Conference on Uncertainty in Artiﬁcial Intelligence, UAI 2005, pages 259–266, Edinburgh, Scotland, July 2005. S. Haykin, editor. Kalman Filtering and Neural Networks. Wiley, New York, 2001. S. Haykin. Neural Networks – A Comprehensive Foundation, 2nd ed. Prentice-Hall, 1998. 196  B UILDING B LOCKS FOR VARIATIONAL BAYESIAN L EARNING  G. E. Hinton and D. van Camp. Keeping neural networks simple by minimizing the description length of the weights. In Proc. of the 6th Ann. ACM Conf. on Computational Learning Theory, pages 5–13, Santa Cruz, CA, USA, 1993. P. Højen-Sørensen, O. Winther, and L.K. Hansen. Mean-ﬁeld approaches to independent component analysis. Neural Computation, 14(4):889–918, 2002. A. Honkela. Speeding up cyclic update schemes by pattern searches. In Proc. of the 9th Int. Conf. on Neural Information Processing (ICONIP’02), pages 512–516, Singapore, 2002. A. Honkela and H. Valpola. Variational learning and bits-back coding: an information-theoretic view to Bayesian learning. IEEE Transactions on Neural Networks, 15(4):800–810, 2004. A. Honkela and H. Valpola. Unsupervised variational Bayesian learning of nonlinear models. In L. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 593–600. MIT Press, Cambridge, MA, USA, 2005. A. Honkela, H. Valpola, and J. Karhunen. Accelerating cyclic update algorithms for parameter estimation by pattern searches. Neural Processing Letters, 17(2):191–203, 2003. A. Honkela, S. Harmeling, L. Lundqvist, and H. Valpola. Using kernel PCA for initialisation of variational Bayesian nonlinear blind source separation method. In C. Puntonet and A. Prieto, editors, Proc. of the Fifth Int. Conf. on Independent Component Analysis and Blind Signal Separation (ICA 2004), volume 3195 of Lecture Notes in Computer Science, pages 790–797, Granada, Spain, 2004. Springer-Verlag, Berlin. ¨ A. Honkela, T. Ostman, and R. Vig´ rio. Empirical evidence of the linear nature of magnetoena cephalograms. In Proc. 13th European Symposium on Artiﬁcial Neural Networks (ESANN 2005), pages 285–290, Bruges, Belgium, 2005. A. Hyv¨ rinen and P. Hoyer. Emergence of phase and shift invariant features by decomposition a of natural images into independent feature subspaces. Neural Computation, 12(7):1705–1720, 2000a. A. Hyv¨ rinen and P. Hoyer. Emergence of topography and complex cell properties from natural a ¨ images using extensions of ICA. In S. A. Solla, T. K. Leen, and K.-R. M uller, editors, Advances in Neural Information Processing Systems 12, pages 827–833. The MIT Press, Cambridge, MA, USA, 2000b. A. Hyv¨ rinen, J. Karhunen, and E. Oja. Independent Component Analysis. J. Wiley, 2001. a A. Ilin and H. Valpola. On the effect of the form of the posterior approximation in variational learning of ICA models. In Proc. of the 4th Int. Symp. on Independent Component Analysis and Blind Signal Separation (ICA2003), pages 915–920, Nara, Japan, 2003. A. Ilin, H. Valpola, and E. Oja. Nonlinear dynamical factor analysis for state change detection. IEEE Trans. on Neural Networks, 15(3):559–575, May 2003. T. Jaakkola and M. I. Jordan. A variational approach to Bayesian logistic regression models and their extensions. In Proceedings of the Sixth International Workshop on Artiﬁcial Intelligence and Statistics, Fort Lauderdale, Florida, January 1997. 197  R AIKO , VALPOLA , H ARVA AND K ARHUNEN  M. I. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. An introduction to variational methods for graphical models. In M. Jordan, editor, Learning in Graphical Models, pages 105–161. The MIT Press, Cambridge, MA, USA, 1999. T. Kohonen. Self-Organizing Maps. Springer, 3rd, extended edition, 2001. T. Kohonen, S. Kaski, and H. Lappalainen. Self-organized formation of various invariant-feature ﬁlters in the Adaptive-Subspace SOM. Neural Computation, 9(6):1321–1344, 1997. H. Lappalainen and A. Honkela. Bayesian nonlinear independent component analysis by multilayer perceptrons. In M. Girolami, editor, Advances in Independent Component Analysis, pages 93–121. Springer-Verlag, Berlin, 2000. H. Lappalainen and J. Miskin. Ensemble learning. In M. Girolami, editor, Advances in Independent Component Analysis, pages 75–92. Springer-Verlag, Berlin, 2000. D. J. C. MacKay. Local minima, symmetry-breaking, and model pruning in variational free energy minimization. Available at http://www.inference.phy.cam.ac.uk/mackay/, 2001. D. J. C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University Press, 2003. D. J. C. MacKay. Developments in probabilistic modelling with neural networks – ensemble learning. In Neural Networks: Artiﬁcial Intelligence and Industrial Applications. Proc. of the 3rd Annual Symposium on Neural Networks, pages 191–198, 1995. D. J. C. MacKay. Ensemble learning for hidden Markov models. http://wol.ra.phy.cam.ac.uk/mackay/, 1997.  Available at  T. Minka. Expectation propagation for approximate Bayesian inference. In Proceedings of the 17th Conference in Uncertainty in Artiﬁcial Intelligence, UAI 2001, pages 362–369, Seattle, Washington, USA, 2001. J. Miskin and D. J. C. MacKay. Ensemble learning for blind source separation. In S. Roberts and R. Everson, editors, Independent Component Analysis: Principles and Practice, pages 209–233. Cambridge University Press, 2001. K. Murphy. The Bayes net toolbox for Matlab. Computing Science and Statistics, 33:331–350, 2001. K. Murphy. A variational approximation for Bayesian networks with discrete and continuous latent variables. In Proc. of the 15th Annual Conf. on Uncertainty in Artiﬁcial Intelligence (UAI–99), pages 457–466, Stockholm, Sweden, 1999. R. Neal. Annealed importance sampling. Statistics and Computing, 11(2):125–139, April 2001. R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models, pages 355–368. The MIT Press, Cambridge, MA, USA, 1999. 198  B UILDING B LOCKS FOR VARIATIONAL BAYESIAN L EARNING  L. Nolan, M. Harva, A. Kab´ n, and S. Raychaudhury. A data-driven Bayesian approach for ﬁnding a young stellar populations in early-type galaxies from their UV-optical spectra. Monthly Notices of the Royal Astronomical Society, 366(1):321–338, 2006. H.-J. Park and T-W. Lee. A hierarchical ICA method for unsupervised learning of nonlinear dependencies in natural images. In C. Puntonet and A. Prieto, editors, Proc. of the 5th Int. Conf. on Independent Component Analysis and Blind Signal Separation (ICA2004), pages 1253–1261, Granada, Spain, 2004. L. Parra, C. Spence, and P. Sajda. Higher-order statistical properties arising from the nonstationarity of natural signals. In T. Leen, T. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 786–792. The MIT Press, Cambridge, MA, USA, 2001. J. Pearl, editor. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers, San Francisco, California, 1988. D.-T. Pham and J.-F. Cardoso. Blind separation of instantaneous mixtures of nonstationary sources. IEEE Trans. on Signal Processing, 49(9):1837–1848, 2001. T. Raiko. Partially observed values. In Proc. Int. Joint Conf. on Neural Networks (IJCNN’04), pages 2825–2830, Budapest, Hungary, 2004. T. Raiko. Nonlinear relational Markov networks with an application to the game of Go. In Proceedings of the International Conference on Artiﬁcial Neural Networks (ICANN 2005), pages 989–996, Warsaw, Poland, September 2005. T. Raiko and M. Tornio. Learning nonlinear state-space models for control. In Proc. Int. Joint Conf. on Neural Networks (IJCNN’05), pages 815–820, Montreal, Canada, 2005. ¨ T. Raiko, H. Valpola, T. Ostman, and J. Karhunen. Missing values in hierarchical nonlinear factor analysis. In Proc. of the Int. Conf. on Artiﬁcial Neural Networks and Neural Information Processing (ICANN/ICONIP 2003), pages 185–189, Istanbul, Turkey, 2003. T. Raiko, M. Tornio, A. Honkela, and J. Karhunen. State inference in variational Bayesian nonlinear state-space models. In Proceedings of the 6th International Conference on Independent Component Analysis and Blind Source Separation (ICA 2006), pages 222–229, Charleston, South Carolina, USA, March 2006. S. Roberts and R. Everson, editors. Independent Component Analysis: Principles and Practice. Cambridge Univ. Press, 2001. S. Roberts, E. Roussos, and R. Choudrey. Hierarchy, priors and wavelets: structure and signal modelling using ICA. Signal Processing, 84(2):283–297, February 2004. D. Rowe. Multivariate Bayesian Statistics: Models for Source Separation and Signal Unmixing. Chapman & Hall/CRC, Medical College of Wisconsin, 2003. G. Schwarz. Estimating the dimension of a model. Annals of Statistics, 6:461–464, 1978. 199  R AIKO , VALPOLA , H ARVA AND K ARHUNEN  D. J. Spiegelhalter, A. Thomas, N. G. Best, and W. R. Gilks. BUGS: Bayesian inference using Gibbs sampling, version 0.50. Available at http://www.mrc-bsu.cam.ac.uk/bugs/, 1995. T. Swartz and M. Evans. Approximating Integrals Via Monte Carlo and Deterministic Methods. Oxford University Press, 2000. L. Tierney and J.B. Kadane. Accurate approximations for posterior moments and marginal densities. Journal of the American Statistical Association, 81(393):82–86, 1986. H. Valpola and J. Karhunen. An unsupervised ensemble learning method for nonlinear dynamic state-space models. Neural Computation, 14(11):2647–2692, 2002. H. Valpola, T. Raiko, and J. Karhunen. Building blocks for hierarchical latent variable models. In Proc. 3rd Int. Conf. on Independent Component Analysis and Signal Separation (ICA2001), pages 710–715, San Diego, USA, 2001. H. Valpola, A. Honkela, and J. Karhunen. An ensemble learning approach to nonlinear dynamic blind source separation using state-space models. In Proc. Int. Joint Conf. on Neural Networks (IJCNN’02), pages 460–465, Honolulu, Hawaii, USA, 2002. ¨ H. Valpola, A. Honkela, M. Harva, A. Ilin, T. Raiko, and T. Ostman. Bayes Blocks software library, 2003a. Available at http://www.cis.hut.fi/projects/bayes/software/. H. Valpola, E. Oja, A. Ilin, A. Honkela, and J. Karhunen. Nonlinear blind source separation by variational Bayesian learning. IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences, E86-A(3):532–541, 2003b. ¨ H. Valpola, T. Ostman, and J. Karhunen. Nonlinear independent factor analysis by hierarchical models. In Proc. 4th Int. Symp. on Independent Component Analysis and Blind Signal Separation (ICA2003), pages 257–262, Nara, Japan, 2003c. H. Valpola, M. Harva, and J. Karhunen. Hierarchical models of variance sources. Signal Processing, 84(2):267–282, 2004. J. H. van Hateren and D. L. Ruderman. Independent component analysis of natural image sequences yields spatio-temporal ﬁlters similar to simple cells in primary visual cortex. Proceedings of the Royal Society of London B, 265(1412):2315–2320, 1998. J. Vesanto, J. Himberg, E. Alhoniemi, and J. Parhankangas. Self-organizing map in Matlab: the SOM toolbox. In Proceedings of the Matlab DSP Conference, pages 35–40, Espoo, Finland, November 1999. Available at http://www.cis.hut.fi/projects/somtoolbox/. C. Wallace. Classiﬁcation by minimum-message-length inference. In S. G. Aki, F. Fiala, and W. W. Koczkodaj, editors, Advances in Computing and Information – ICCI ’90, volume 468 of Lecture Notes in Computer Science, pages 72–81. Springer, Berlin, 1990. S. Waterhouse, D. MacKay, and T. Robinson. Bayesian methods for mixtures of experts. In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, editors, Advances in Neural Information Processing Systems, volume 8, pages 351–357. The MIT Press, 1996. 200  B UILDING B LOCKS FOR VARIATIONAL BAYESIAN L EARNING  J. Winn and C. M. Bishop. Variational message passing. Journal of Machine Learning Research, 6: 661–694, April 2005.  201</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
