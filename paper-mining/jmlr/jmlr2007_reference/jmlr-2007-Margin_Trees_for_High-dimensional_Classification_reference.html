<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 jmlr-2007-Margin Trees for High-dimensional Classification</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-52" href="../jmlr2007/jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">jmlr2007-52</a> <a title="jmlr-2007-52-reference" href="#">jmlr2007-52-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>52 jmlr-2007-Margin Trees for High-dimensional Classification</h1>
<br/><p>Source: <a title="jmlr-2007-52-pdf" href="http://jmlr.org/papers/volume8/tibshirani07a/tibshirani07a.pdf">pdf</a></p><p>Author: Robert Tibshirani, Trevor Hastie</p><p>Abstract: We propose a method for the classiﬁcation of more than two classes, from high-dimensional features. Our approach is to build a binary decision tree in a top-down manner, using the optimal margin classiﬁer at each split. We implement an exact greedy algorithm for this task, and compare its performance to less greedy procedures based on clustering of the matrix of pairwise margins. We compare the performance of the “margin tree” to the closely related “all-pairs” (one versus one) support vector machine, and nearest centroids on a number of cancer microarray data sets. We also develop a simple method for feature selection. We ﬁnd that the margin tree has accuracy that is competitive with other methods and offers additional interpretability in its putative grouping of the classes. Keywords: maximum margin classiﬁer, support vector machine, decision tree, CART</p><br/>
<h2>reference text</h2><p>A. Alizadeh, M. Eisen, R. E. Davis, C. Ma, I. Lossos, A. Rosenwal, J. Boldrick, H. Sabet, T. Tran, X. Yu, Pwellm J., G. Marti, T. Moore, J. Hudsom, L. Lu, D. Lewis, R. Tibshirani, G. Sherlock, W. Chan, T. Greiner, D. Weisenburger, K. Armitage, R. Levy, W. Wilson, M. Greve, J. Byrd, D. Botstein, P. Brown, and L. Staudt. Identiﬁcation of molecularly and clinically distinct substypes of diffuse large b cell lymphoma by gene expression proﬁling. Nature, 403:503–511, 2000. K. Bennett and J. Blue. A support vector machine approach to decision trees. Technical report, Rensselaer Polytechnic Institute, Troy, NY, 1997. R.P.I Math Report No. 97-100. L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classiﬁcation and Regression Trees. Wadsworth, 1984. I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for cancer classiﬁcation using support vector machines. Machine Learning, pages 389–422, 2002. J. Khan, J. S. Wei, M. Ringn´ r, L. H. Saal, M. Ladanyi, F. Westermann, F. Berthold, M. Schwab, e C. R. Antonescu, C. Peterson, and P. S. Meltzer. Classiﬁcation and diagnostic prediction of cancers using gene expression proﬁling and artiﬁcial neural networks. Nature Medicine, 7:673– 679, 2001. H. Kim and W.Y. Loh. Classiﬁcation trees with unbiased multiway splits. Journal of the American Statistical Association, 96:589–604, 2001. Y. Lee, Y. Lin, and G. Wahba. Multicategory support vector machines, theory, and application to the classiﬁcation of microarray data and satellite radiance data. Journal of the Amer. Statist. Assoc., 99:67–81, 2004. W.Y. Loh and N. Vanichsetakul. Tree structured classiﬁcation via generalized discriminant analysis. Journal of the American Statistical Association, 83:715–728, 1988. 651  T IBSHIRANI AND H ASTIE  K. Munagala, R. Tibshirani, and P. Brown. Cancer characterization and feature set extraction by discriminative margin clustering. BMC Bioinformatics, 5:5–21, 2004. M. Y. Park and T. Hastie. Hierarchical classiﬁcation using shrunken centroids. Technical report, Stanford University, 2005. S. L. Pomeroy, P Tamayo, M. Gaasenbeek, L. M. Sturla, M. Angelo, M. E. McLaughlin, J. Y. Kim, L. C. Goumnerova, P. M. Black, C. Lau, J. C. Allen, D. Zagzag, J. M. Olson, T. Curran, C. Wetmore, J. A. Biegel, T. Poggio, S. Mukherjee, R. Rifkin, A. Califano, G. Stolovitzky, D. N. Louis, J. P. Mesirov, E. S. Lander, and T. R. Golub. Prediction of central nervous system embryonal tumour outcome based on gene expression. Nature, 5:436–42, 2002. R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, 1993. S. Ramaswamy, P. Tamayo, R. Rifkin, S. Mukherjee, C. Yeang, M. Angelo, C. Ladd, M. Reich, E. Latulippe, J. Mesirov, T. Poggio, W. Gerald, M. Loda, E. Lander, and T. Golub. Multiclass cancer diagnosis using tumor gene expression signature. PNAS, 98:15149–15154, 2001. S. Rosset, J. Zhu, and T. Hastie. Margin maximizing loss functions. In Advances in Neural Information Processing Systems, (NIPS*2005), 2005. A. Statnikov, C.F. Aliferis, I. Tsamardinos, D. Hardin, and S. Levy. A comprehensive evaluation of multicategory classiﬁcation methods for microarray gene expression cancer diagnosis. Bioinformatics, pages 631–43, 2004. J.E. Staunton, D.K. Slonim, H.A. Coller, P. Tamayo, M.J. Angelo, U. Park, J. Scherf, J.K. Lee, W.O. Reinhold, and J.N. Weinstein. Chemosensitivity prediction by transcriptional proﬁling. Proc. Natl Acad. Sci. USA, 98:10787–10792, 2001. A.I. Su, J.B. Welsh, L.M. Sapinoso, S.G. Kern, P. Dimitrov, H. Lapp, P.G. Schultz, S.M. Powell, C.A. Moskaluk, H.F. Frierson, Jr, and G.M. Hampton. Molecular classiﬁcation of human carcinomas by use of gene expression signatures. Cancer Research, 61:7388–7393, 2001. R. Tibshirani. The lasso method for variable selection in the cox model. Statistics in Medicine, 16: 385–395, 1997. R. Tibshirani, T. Hastie, B. Narasimhan, and G Chu. Diagnosis of multiple cancer types by shrunken centroids of gene expression. Proc. Natl. Acad. Sci., 99:6567–6572, 2001. V. Vural and J. G. Dy. A hierarchical method for multi-class support vector machines. 2004. International Conference on Machine Learning; Proceeding Series; Vol. 69. J. Weston and C. Watkins. Multi-class support vector machines. In M. Verleysen, editor, Proceedings of ESANN99. D. Facto Press, Brussels, 1999. J. Zhu, S. Rosset, T. Hastie, and R. Tibshirani. L1 norm support vector machines. Technical report, Stanford University, 2003.  652</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
