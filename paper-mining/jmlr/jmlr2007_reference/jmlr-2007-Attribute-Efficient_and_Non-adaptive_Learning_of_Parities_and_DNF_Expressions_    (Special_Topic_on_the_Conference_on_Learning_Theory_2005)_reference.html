<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-12" href="../jmlr2007/jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">jmlr2007-12</a> <a title="jmlr-2007-12-reference" href="#">jmlr2007-12-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</h1>
<br/><p>Source: <a title="jmlr-2007-12-pdf" href="http://jmlr.org/papers/volume8/feldman07a/feldman07a.pdf">pdf</a></p><p>Author: Vitaly Feldman</p><p>Abstract: We consider the problems of attribute-efﬁcient PAC learning of two well-studied concept classes: parity functions and DNF expressions over {0, 1}n . We show that attribute-efﬁcient learning of parities with respect to the uniform distribution is equivalent to decoding high-rate random linear codes from low number of errors, a long-standing open problem in coding theory. This is the ﬁrst evidence that attribute-efﬁcient learning of a natural PAC learnable concept class can be computationally hard. An algorithm is said to use membership queries (MQs) non-adaptively if the points at which the algorithm asks MQs do not depend on the target concept. Using a simple non-adaptive parity learning algorithm and a modiﬁcation of Levin’s algorithm for locating a weakly-correlated parity due to Bshouty et al. (1999), we give the ﬁrst non-adaptive and attribute-efﬁcient algorithm for ˜ learning DNF with respect to the uniform distribution. Our algorithm runs in time O(ns4 /ε) and ˜ uses O(s4 · log2 n/ε) non-adaptive MQs, where s is the number of terms in the shortest DNF representation of the target concept. The algorithm improves on the best previous algorithm for learning DNF (of Bshouty et al., 1999) and can also be easily modiﬁed to tolerate random persistent classiﬁcation noise in MQs. Keywords: attribute-efﬁcient, non-adaptive, membership query, DNF, parity function, random linear code</p><br/>
<h2>reference text</h2><p>A. Aho, J. Hopcroft, and J. Ullman. The Design and Analysis of Computer Algorithms. AddisonWesley Series in Computer Science and Information Processing. Addison-Wesley, 1974. D. Angluin and P. Laird. Learning from noisy examples. Machine Learning, 2:343–370, 1988. A. Barg. Complexity issues in coding theory. Electronic Colloquium on Computational Complexity (ECCC), 4(046), 1997. A. Blum, L. Hellerstein, and N. Littlestone. Learning in the presence of ﬁnitely or inﬁnitely many irrelevant attributes. Journal of Computer and System Sciences, 50:32–40, 1995. 1457  F ELDMAN  A. Blum, A. Kalai, and H. Wasserman. Noise-tolerant learning, the parity problem, and the statistical query model. In Proceedings of STOC, pages 435–440, 2000. N. Bshouty and V. Feldman. On using extended statistical queries to avoid membership queries. Journal of Machince Learning Research, 2:359–395, 2002. N. Bshouty and L. Hellerstein. Attribute efﬁcient learning with queries. Journal of Computer and System Sciences, 56:310–319, 1998. N. Bshouty, J. Jackson, and C. Tamon. More efﬁcient PAC learning of DNF with membership queries under the uniform distribution. In Proceedings of COLT, pages 286–295, 1999. N. Bshouty, E. Mossel, R. O’Donnell, and R. Servedio. Learning DNF from random walks. In Proceedings of FOCS, pages 189–199, 2003. H. Chernoff. A measure of asymptotic efﬁciency for tests of a hypothesis based on the sum of observations. Ann. Math. Statist., 23:493–507, 1952. J. Cooley and J. Tukey. An Algorithm for the Machine Calculation of Complex Fourier Series. Math. Computat., 19:297–301, 1965. P. Damaschke. Adaptive versus nonadaptive attribute-efﬁcient learning. In Proceedings of STOC, pages 590–596, 1998. S. Decatur, O. Goldreich, and D. Ron. Computational sample complexity. SIAM Journal on Computing, 29(3):854–879, 1999. M. Farach, S. Kannan, E. Knill, and S. Muthukrishnan. Group testing problems in experimental molecular biology. In Proceedings of Sequences ’97, 1997. V. Feldman, P. Gopalan, S. Khot, and A. Ponuswami. New Results for Learning Noisy Parities and Halfspaces. In Proceedings of FOCS, pages 563–574, 2006. Y. Freund. Boosting a weak learning algorithm by majority. In Proceedings of the Third Annual Workshop on Computational Learning Theory, pages 202–216, 1990. Y. Freund. An improved boosting algorithm and its implications on learning complexity. In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, pages 391–398, 1992. D. Gavinsky. Optimally-smooth adaptive boosting and application to agnostic learning. Journal of Machine Learning Research, 4:101–117, 2003. S. Goldman, M. Kearns, and R. Schapire. Exact identiﬁcation of read-once formulas using ﬁxed points of ampliﬁcation functions. SIAM Journal on Computing, 22(4):705–726, 1993. O. Goldreich and L. Levin. A hard-core predicate for all one-way functions. In Proceedings of STOC, pages 25–32, 1989. O. Goldreich, S. Goldwasser, and S. Micali. How to construct random functions. Journal of the ACM, 33(4):792–807, 1986. 1458  ATTRIBUTE -E FFICIENT L EARNING BY N ON - ADAPTIVE M EMBERSHIP Q UERIES  D. Guijarro, V. Lavin, and V. Raghavan. Exact learning when irrelevant variables abound. In Proceedings of EuroCOLT ’99, pages 91–100, 1999a. D. Guijarro, J. Tarui, and T. Tsukiji. Finding relevant variables in PAC model with membership queries. Lecture Notes in Artiﬁcial Intelligence, 1720:313 – 322, 1999b. D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning applications. Information and Computation, 100(1):78–150, 1992. T. Hofmeister. An application of codes to attribute-efﬁcient learning. In Proceedings of EuroCOLT, pages 101–110, 1999. J. Jackson. Personal communication, 2004. J. Jackson. An efﬁcient membership-query algorithm for learning DNF with respect to the uniform distribution. Journal of Computer and System Sciences, 55:414–440, 1997. J. Jackson, E. Shamir, and C. Shwartzman. Learning with queries corrupted by classiﬁcation noise. In Proceedings of the Fifth Israel Symposium on the Theory of Computing Systems, pages 45–53, 1997. M. Kearns. Efﬁcient noise-tolerant learning from statistical queries. Journal of the ACM, 45(6): 983–1006, 1998. M. Kearns and L. Valiant. Cryptographic limitations on learning boolean formulae and ﬁnite automata. Journal of the ACM, 41(1):67–95, 1994. M. Kearns and U. Vazirani. An introduction to computational learning theory. MIT Press, Cambridge, MA, 1994. M. Kearns, R. Schapire, and L. Sellie. Toward efﬁcient agnostic learning. Machine Learning, 17 (2-3):115–141, 1994. A. Klivans and R. Servedio. Boosting and hard-core set construction. Machine Learning, 51(3): 217–238, 2003. A. Klivans and R. Servedio. Toward attribute efﬁcient learning of decision lists and parities. In Proceedings of COLT, pages 234–248, 2004. E. Kushilevitz and Y. Mansour. Learning decision trees using the Fourier spectrum. In Proceedings of STOC, pages 455–464, 1991. L. Levin. Randomness and non-determinism. Journal of Symbolic Logic, 58(3):1102–1103, 1993. Y. Mansour. Learning boolean functions via the fourier transform. In V. P. Roychodhury, K. Y. Siu, and A. Orlitsky, editors, Theoretical Advances in Neural Computation and Learning, pages 391–424. Kluwer, 1994. J. Massey. Shift-register synthesis and BCH decoding. IEEE Trans. Inform. Theory, 15:122–127, 1969. 1459  F ELDMAN  R. J. McEliece. A public-key cryptosystem based on algebraic coding theory. DSN progress report, 42-44, 1978. R. Motwani and P. Raghavan. Randomized Algorithms. Cambridge University Press, 1995. R. Schapire. The strength of weak learnability. Machine Learning, 5(2):197–227, 1990. R. Servedio. Computational sample complexity and attribute-efﬁcient learning. Journal of Computer and System Sciences, 60(1):161–178, 2000. R. Servedio. Smooth boosting and learning with malicious noise. Journal of Machine Learning Research, 4:633–648, 2003. M. Sudan. Essential coding theory (lecture http://theory.lcs.mit.edu/˜madhu/FT02/, 2002.  notes).  Available  at  R. Uehara, K. Tsuchida, and I. Wegener. Optimal attribute-efﬁcient learning of disjunction, parity, and threshold functions. In Proceedings of EuroCOLT ’97, pages 171–184, 1997. L. Valiant. A neuroidal architecture for cognitive computation. Journal of the ACM, 47(5):854–882, 2000. L. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984. L. Valiant. Circuits of the Mind. Oxford University Press, 1994. L. G. Valiant. Knowledge infusion. In Proceedings of AAAI, 2006. J.H. van Lint. Introduction to Coding Theory. Springer, Berlin, 1998. A. Vardy. Algorithmic complexity in coding theory and the minimum distance problem. In Proceedings of STOC, pages 92–109, 1997.  1460</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
