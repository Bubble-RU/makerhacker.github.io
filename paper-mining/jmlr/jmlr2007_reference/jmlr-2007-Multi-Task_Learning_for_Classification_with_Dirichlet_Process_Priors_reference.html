<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 jmlr-2007-Multi-Task Learning for Classification with Dirichlet Process Priors</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-56" href="../jmlr2007/jmlr-2007-Multi-Task_Learning_for_Classification_with_Dirichlet_Process_Priors.html">jmlr2007-56</a> <a title="jmlr-2007-56-reference" href="#">jmlr2007-56-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>56 jmlr-2007-Multi-Task Learning for Classification with Dirichlet Process Priors</h1>
<br/><p>Source: <a title="jmlr-2007-56-pdf" href="http://jmlr.org/papers/volume8/xue07a/xue07a.pdf">pdf</a></p><p>Author: Ya Xue, Xuejun Liao, Lawrence Carin, Balaji Krishnapuram</p><p>Abstract: Consider the problem of learning logistic-regression models for multiple classiﬁcation tasks, where the training data set for each task is not drawn from the same statistical distribution. In such a multi-task learning (MTL) scenario, it is necessary to identify groups of similar tasks that should be learned jointly. Relying on a Dirichlet process (DP) based statistical model to learn the extent of similarity between classiﬁcation tasks, we develop computationally efﬁcient algorithms for two different forms of the MTL problem. First, we consider a symmetric multi-task learning (SMTL) situation in which classiﬁers for multiple tasks are learned jointly using a variational Bayesian (VB) algorithm. Second, we consider an asymmetric multi-task learning (AMTL) formulation in which the posterior density function from the SMTL model parameters (from previous tasks) is used as a prior for a new task: this approach has the signiﬁcant advantage of not requiring storage and use of all previous data from prior tasks. The AMTL formulation is solved with a simple Markov Chain Monte Carlo (MCMC) construction. Experimental results on two real life MTL problems indicate that the proposed algorithms: (a) automatically identify subgroups of related tasks whose training data appear to be drawn from similar distributions; and (b) are more accurate than simpler approaches such as single-task learning, pooling of data across all tasks, and simpliﬁed approximations to DP. Keywords: classiﬁcation, hierarchical Bayesian models, Dirichlet process</p><br/>
<h2>reference text</h2><p>R.K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6:1817C1853, 2005. C.E. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. Annals of Statistics, 2:1152–1174, 1974. B. Bakker and T. Heskes. Task clustering and gating for Bayesian multitask learning. Journal of Machine Learning Research, 4:83–99, 2003. J. Baxter. Learning internal representations. In COLT: Proceedings of the Workshop on Computational Learning Theory, 1995. J. Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Research, 2000. D. Blei and M.I. Jordan. Variational inference for Dirichlet process mixtures. Journal of Bayesian Analysis, 1(1):121–144, 2005. 61  X UE , L IAO , C ARIN AND K RISHNAPURAM  D. Burr and H. Doss. A bayesian semiparametric model for random-effects meta-analysis. Journal of the American Statistical Association, 100(469):242–251, Mar. 2005. R. Caruana. Multitask learning. Machine Learning, 28:41–75, 1997. F. Dominici, G. Parmigiani, R. Wolpert, and K. Reckhow. Combining information from related regressions. Journal of Agricultural, Biological, and Environmental Statistics, 2(3):294–312, 1997. Good literature review on the application of hierarchical models to meta analysis, Page 4. M.D. Escobar and M. West. Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association, 90:577–588, 1995. T. Evgeniou, C.A. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. Journal of Machine Learning Research, 6:615–637, 2005. T. Ferguson. A Bayesian analysis of some nonparametric problems. The Annals of Statistics, 1: 209–230, 1973. Z. Ghahramani and M. Beal. Propagation algorithms for variational Bayesian learning. In T. Leen, T. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13. MIT Press, Cambridge, MA, 2001. G. V. Glass. Primary, secondary and meta-analysis of research. Educatinal Researcher, 5, 1976. G.E. Hinton and T.J. Sejnowski. Learning and relearning in Boltzmann machines. In McClelland J.L. Rumelhart D.E. and the PDP Research Group, editors, Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1, chapter 7, pages 282–317. MIT Press, Cambridge, MA, 1986. P.D. Hoff. Nonparametric modeling of hierarchically exchangeable data. Technical Report 421, University of Washington Statistics Department, 2003. H. Ishwaran. Inference for the random effects in Bayesian generalized linear mixed models. In ASA Proceedings of the Bayesian Statistical Science Section, pages 1–10, 2000. H. Ishwaran and L.F. James. Gibbs sampling methods for stick-breaking priors. Journal of the American Statistical Association, 96:161–173, 2001. T.S. Jaakkola and M.I. Jordan. A variational approach to Bayesian logistic regression models and their extensions. In Proceedings of the Sixth International Workshop on Artiﬁcial Intelligence and Statistics, 1997. M.I. Jordan, Z. Ghahramani, T.S. Jaakkola, and L.K. Saul. An introduction to variational methods for graphical models. In M.I. Jordan, editor, Learning in Graphical Models. MIT Press, Cambridge, 1999. N.D. Lawrence and J.C. Platt. Learning to learn with the informative vector machine. In Proceedings of the 21st International Conference on Machine Learning, 2004. D.J.C. MacKay. The evidence framework applied to classiﬁcation networks. Neural Computation, 4(5):720–736, 1992. 62  M ULTI -TASK L EARNING FOR C LASSIFICATION WITH D IRICHLET P ROCESS P RIORS  B.K. Mallick and S.G. Walker. Combining information from several experiments with nonparametric priors. Biometrika, 84(3):697–706, 1997. A. McCallum and K. Nigam. Employing EM in pool-based active learning for text classiﬁcation. In Proceedings of the 15th International Conference on Machine Learning, 1998. S. Mukhopadhyay and A.E. Gelfand. Dirichlet process mixed generalized linear models. Journal of the American Statistical Association, 92(438):633–639, 1997. P. M¨ ller, F. Quintana, and G. Rosner. A method for combining inference across related nonparau metric Bayesian models. Journal of the Royal Statistical Society Series B, 66(3):735–749, 2004. R.M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Technical Report 9815, Dept. of Statistics, University of Toronto, 1998. C.P. Robert and G. Casella. Monte Carlo Statistical Methods. New York: Springer-Verlag, second edition, 2004. J. Sethuraman. A constructive deﬁnition of Dirichlet priors. Statistica Sinica, 4, 1994. S. Thrun and J. O’Sullivan. Discovering structure in multiple learning tasks: The TC algorithm. In Proceedings of the 13th International Conference on Machine Learning, 1996. S. Thrun and L.Y. Pratt, editors. Learning To Learn. Kluwer Academic Publishers, Boston, MA, 1998. M. West, P. M¨ ller, and M.D. Escobar. Hierarchical priors and mixture models, with application u in regression and density estimation. Aspects of Uncertainty: A Tribute to D.V. Lindley, pages 363–386, 1994. K. Yu, A. Schwaighofer, V. Tresp, W.-Y. Ma, and H. Zhang. Collaborative ensemble learning: Combining collaborative and content-based information ﬁltering via hierarchical bayes. In Proceedings of the 19th Conference on Uncertainty in Artiﬁcial Intelligence, 2003. K. Yu, V. Tresp, and S. Yu. A nonparametric hierarchical Bayesian framework for information ﬁltering. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004. K. Yu, A. Schwaighofer, and V. Tresp. Learning Gaussian processes from multiple tasks. In Proceedings of the 22nd International Conference on Machine Learning, 2005. J. Zhang, Z. Ghahramani, and Y. Yang. Learning multiple related tasks using latent independent ¨ component analysis. In Y. Weiss, B. Scholkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18. MIT Press, Cambridge, MA, 2006.  63</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
