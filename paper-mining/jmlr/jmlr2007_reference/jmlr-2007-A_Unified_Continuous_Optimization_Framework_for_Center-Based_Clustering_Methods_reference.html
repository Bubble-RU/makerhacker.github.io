<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-8" href="../jmlr2007/jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">jmlr2007-8</a> <a title="jmlr-2007-8-reference" href="#">jmlr2007-8-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</h1>
<br/><p>Source: <a title="jmlr-2007-8-pdf" href="http://jmlr.org/papers/volume8/teboulle07a/teboulle07a.pdf">pdf</a></p><p>Author: Marc Teboulle</p><p>Abstract: Center-based partitioning clustering algorithms rely on minimizing an appropriately formulated objective function, and different formulations suggest different possible algorithms. In this paper, we start with the standard nonconvex and nonsmooth formulation of the partitioning clustering problem. We demonstrate that within this elementary formulation, convex analysis tools and optimization theory provide a unifying language and framework to design, analyze and extend hard and soft center-based clustering algorithms, through a generic algorithm which retains the computational simplicity of the popular k-means scheme. We show that several well known and more recent center-based clustering algorithms, which have been derived either heuristically, or/and have emerged from intuitive analogies in physics, statistical techniques and information theoretic perspectives can be recovered as special cases of the proposed analysis and we streamline their relationships. Keywords: clustering, k-means algorithm, convex analysis, support and asymptotic functions, distance-like functions, Bregman and Csiszar divergences, nonlinear means, nonsmooth optimization, smoothing algorithms, ﬁxed point methods, deterministic annealing, expectation maximization, information theory and entropy methods</p><br/>
<h2>reference text</h2><p>A. Auslender. Optimisation: Methodes Num´ riques. Masson, Paris, 1976. e A. Auslender. Penalty and barrier methods: a uniﬁed framework. SIAM J. Optimization 10(1), 211230, 1999. A. Auslender and M. Teboulle. Asymptotic Cones and Functions in Optimization and Variational Inequalities. Springer–Verlag, New York, 2003. A. Auslender and M. Teboulle. Interior gradient and proximal methods for convex and conic optimization. SIAM J. Optimization, 16(3):697-725, 2006. A. M. Bagirov, A. M. Rubinov, N.V. Soukhoroukova, and J. Yearwood. Unsupervised and supervised data classiﬁcation via nonsmooth and global optimization. TOP, (Formerly Trabajos Investigaci´ n Operativa) 11(1):1–93, 2003. o A. M. Bagirov and J. Ugon. An algorithm for minimizing clustering functions. Optimization, 54(45): 351–368, 2005. A. Banerjee, S. Merugu, I. S. Dhillon, and J. Ghosh. Clustering with Bregman Divergences. Journal of Machine Learning Research, 6, 1705–1749, 2005. A. Ben-Tal and M. Teboulle. Expected utility, penalty functions, and duality in stochastic nonlinear programming. Management Sciences, 32(11):1445–1466, 1986. A. Ben-Tal and M. Teboulle. A smoothing technique for nondifferentiable optimization problems. In Springer Verlag Lecture Notes in Mathematics, volume 1405, pages 1–11, Berlin, 1989. A. Ben-Tal, M. Teboulle, and W. H. Yang, A least-squares based method for a class of nonsmooth minimization problems with applications in plasticity. Applied Mathematics and Optimization, 24(3):273–288, 1991. T. Berger. Rate Distortion Theory: A Mathematical Basis for Data Compression. Prentice Hall, Englewood Cliffs, New Jersey, 1971. D. P. Bertsekas. Constrained Optimization and Lagrange Multiplier Methods. Athena Scientiﬁc, Belmont, Massachusetts, second edition, 1996. D.P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, Belmont, Massachusetts, second edition, 1999. 99  T EBOULLE  D. P. Bertsekas and J. N. Tsitsiklis, Parallel and Distributed Computation: Numerical Methods. Prentice-Hall, New Jersey, 1989. J. Bezdek. Pattern Recognition with Fuzzy Objective Function Algorithms. Plenum Press, New York, 1981. L. M. Bregman. A relaxation method of ﬁnding a common point of convex sets and its application to the solution of problems in convex programming. USSR Comp. Math. and Math Phys.,7:200-217, 1967. J. Brimberg and Love R.F. Global convergence of a generalized iterative procedure for the minisum location problem with l p distances. Operations Research, 41:1153–1163, 1993. Y. Censor and A. Lent. An interval row action method for interval convex programming. J. of Optimization Theory and Applications, 34:321–353, 1981. Y. Censor and S. A. Zenios, Parallel Optimization, Oxford University Press, Oxford, 1997. G. Chen and M. Teboulle. Convergence analysis of a proximal-like algorithm using Bregman functions. SIAM J. on Optimization 3:538–543, 1993. T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley-Interscience, 1991. I. Csiszar. Information-type measures of difference of probability distributions and indirect observations. Studia Sci. Mathematica Hungarica, 2:299–318, 1967. R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁcation. John Wiley & Sons, Inc., second edition, 2001. C. Elkan. Clustering documents with an exponential-family approximation of the Dirichlet compound multinomial distribution. In Proceedings of the 23rd International Conference on Machine Learning ICML 06, 289-296, 2006. E. Forgy. Cluster analysis of multivariate data: Efﬁciency vs. interpretability of classiﬁcations. Biometrics, Abstract, 21(3):768, 1965. M.R. Garey and D.S. Johnson, Computers and Intractability: A Guide to the Theory of NPCompleteness, W.H. Freeman and Company, San Francisco, CA, 1979. A.D. Gordon and J.T. Henderson. An algorithm for Euclidean sum of squares classiﬁcation. Biometrics, 33:355-362, 1977. R.M. Gray and D.L. Neuhoff. Quantization. IEEE Transaction on Information Theory, 44(6):2325– 2382, 1998. G. Hamerly and C. Elkan. Alternatives to the k-means algorithm that ﬁnd better clusterings. In Proceedings of the Eleventh International Conference on Information and Knowledge Management (CIKM’02), pages 600-607, 2002. J P. Hansen and N. Mladenovic. J-Means: A New Heuristic for Minimum Sum-of-Squares Clustering. Pattern Recognition 34:405–413, 2001. 100  A U NIFIED C ONTINUOUS O PTIMIZATION F RAMEWORK TO C LUSTERING  G. Hardy, J.E. Littlewood, and G. Polya. Inequalities. Cambridge University Press, Cambridge, 1934. A.K. Jain, M.N. Murty, and P.J. Flynn. Data clustering: A review, ACM Computing Surveys, 31(3):264-323, 1999. R. Kannan, H. Salmasian, and S. Vempala. The spectral method for general mixture models. In Proceedings of the 18th Annual Conference on Learning Theory, 444-457, 2005. N.B. Karayiannis. An axiomatic approach to soft learning vector quantization and clustering. IEEE Transactions on Neural Networks, 10(5):1153-1165, 1999. S.P. Lloyd. Least squares quantization in PCM. Bell Telephone Laboratories Paper, Murray Hill, NJ, 1957. Also in, IEEE Transactions on Information Theory, 28:127-135, 1982. Y. Linde, A. Buzo, and R.M. Gray. An algorithm for vector quantizer design. IEEE Transactions on Communications, 28(1):84-95, 1980. J. MacQueen. Some methods for classiﬁcation and analysis of multivariate observations. In Proceedings of the ﬁfth Berkeley Symposium on Math., Stat. and Probability, pages 281–296, 1967. D. Modha and S. Spangler. Feature weighting in k-means clustering. Machine Learning, 52(3):217237, 2003. M.R. Rao. Cluster analysis and mathematical programming. J. American Statistical Association, 66:622–626, 1971. R. T. Rockafellar. Convex Analysis, Princeton University Press, Princeton, NJ, 1970. K. Rose, E. Gurewitz, and C.G. Fox. A deterministic annealing approach to clustering. Pattern Recognition Letters, 11(9):589–594, 1990. K. Rose. Deterministic annealing for clustering, compression, classiﬁcation, regression, and related optimization problems. Proceedings of the IEEE, 86(11):2210–2239, 1998. H. Steinhaus. Sur la division des corps materiels en parties. Bull. Acad. Polon. Sci., C1. III, vol. IV, 801–804, 1956. M. Teboulle. “On ϕ-divergence and its applications”. In Systems and Management Science by Extremal Methods (F.Y. Phillips, J. Rousseau, eds.), Kluwer Academic Press, chap. 17, pages 255– 273, 1992. M. Teboulle. Entropic proximal mappings with application to nonlinear programming. Mathematics of Operations Research, 17:670–690, 1992. M. Teboulle. Convergence of proximal-like algorithms. SIAM J. of Optimization, 7:1069-1083, 1997. M. Teboulle and J. Kogan. Deterministic annealing and a k-means type smoothing optimization algorithm. In Proceedings of the Workshop on Clustering High Dimensional Data and its Applications (held in conjunction with the Fifth SIAM International Conference on Data Mining). I. Dhillon, J. Ghosh and J. Kogan (eds.), pages 13-22, 2005. 101  T EBOULLE  M. Teboulle, P. Berkhin, I. Dhillon, Y. Guan, and J. Kogan. Clustering with entropy-like k-means algorithms. In Grouping Multidimensional Data: Recent Advances in Clustering. J. Kogan, C. Nicholas, and M. Teboulle, (Eds.), Springer Verlag, NY, pages 127–160, 2006. N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. In Proc. of the 37th Annual Allerton Conference on Communication, Control and Computing, pages 368–377, 1999. N. Ueda and R. Nakano. Deterministic annealing EM algorithm. Neural Networks, 11(2): 271–282, 1998. E. Weiszfeld. Sur le point pour lequel la somme des distances de n points donn´ s est minimum. e Tohoku Mathematical Journal, 43:355–386, 1937. B. Zhang, M. Hsu, and U. Dayal. K-harmonic means - a data clustering algorithm. Technical Report HPL-1999-124 991029, HP Labs, Palo Alto, CA, 1999.  102</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
