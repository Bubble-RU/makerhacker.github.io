<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-28" href="../jmlr2007/jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">jmlr2007-28</a> <a title="jmlr-2007-28-reference" href="#">jmlr2007-28-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</h1>
<br/><p>Source: <a title="jmlr-2007-28-pdf" href="http://jmlr.org/papers/volume8/sutton07a/sutton07a.pdf">pdf</a></p><p>Author: Charles Sutton, Andrew McCallum, Khashayar Rohanimanesh</p><p>Abstract: In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random ﬁelds (DCRFs), a generalization of linear-chain conditional random ﬁelds (CRFs) in which each time slice contains a set of state variables and edges—a distributed state representation as in dynamic Bayesian networks (DBNs)—and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data. In addition to maximum conditional likelihood, we present two alternative approaches for training DCRFs: marginal likelihood training, for when we are primarily interested in predicting only a subset of the variables, and cascaded training, for when we have a distinct data set for each state variable, as in transfer learning. We evaluate marginal training and cascaded training on both synthetic data and real-world text data, ﬁnding that marginal training can improve accuracy when uncertainty exists over the latent variables, and that for transfer learning, a DCRF trained in a cascaded fashion performs better than a linear-chain CRF that predicts the ﬁnal task directly. Keywords: conditional random ﬁelds, graphical models, sequence labeling</p><br/>
<h2>reference text</h2><p>Srinivas M. Aji, Gavin B. Horn, and Robert J. McEliece. On the convergence of iterative decoding on graphs with a single cycle. In Proc. IEEE Int’l Symposium on Information Theory, 1998. Yasemin Altun, Ioannis Tsochantaridis, and Thomas Hofmann. Hidden Markov support vector machines. In International Conference on Machine Learning (ICML), 2003. Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71, 1996. Jeff Bilmes. Graphical models and automatic speech recognition. In M. Johnson, S.P. Khudanpur, M. Ostendorf, and R. Rosenfeld, editors, Mathematical Foundations of Speech and Language Processing. Springer-Verlag, 2003. Eric Brill. Some advances in rule-based part of speech tagging. In National Conference on Artiﬁcial Intelligence (AAAI), 1994. Hung H. Bui, Svetha Venkatesh, and Geoff West. Policy recognition in the Abstract Hidden Markov Model. Journal of Artiﬁcial Intelligence Research, 17, 2002. Mary Elaine Califf and Raymond J. Mooney. Relational learning of pattern-match rules for information extraction. In National Conference on Artiﬁcial Intelligence (AAAI), pages 328–334, 1999. Fabio Ciravegna. Adaptive information extraction from text by rule induction and generalisation. In International Joint Conference on Artiﬁcial Intelligence (ICML), 2001. Michael Collins. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2002. Thomas Dean and Keiji Kanazawa. A model for reasoning about persistence and causation. Computational Intelligence, 5(3):142–150, 1989. Gal Elidan, Ian McGraw, and Daphne Koller. Residual belief propagation: Informed scheduling for asynchronous message passing. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2006. Shai Fine, Yoram Singer, and Naftali Tishby. The hierarchical hidden Markov model: Analysis and applications. Machine Learning, 32(1):41–62, 1998. Dayne Freitag. Machine Learning for Information Extraction in Informal Domains. PhD thesis, Carnegie Mellon University, 1998. 719  S UTTON , M C C ALLUM AND ROHANIMANESH  Dayne Frietag and Andrew McCallum. Information extraction with HMMs and shrinkage. In AAAI Workshop on Machine Learning for Information Extraction, 1999. Zoubin Ghahramani and Michael I. Jordan. Factorial hidden Markov models. Machine Learning, (29):245–273, 1997. ´ Xuming He, Richard S. Zemel, and Miguel A. Carreira-Perpi˜ i´ n. Multiscale conditional random na ﬁelds for image labelling. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14:1771–1800, 2002. Junhwan Kim and Ramin Zabih. Factorial Markov random ﬁelds. In European Conference on Computer Vision (ECCV), pages 321–334, 2002. Taku Kudo and Yuji Matsumoto. Chunking with support vector machines. In Conference of the North American Chapter of the Association for Computation Linguistics (NAACL), 2001. Sanjiv Kumar and Martial Hebert. Discriminative ﬁelds for modeling spatial dependencies in nat¨ ural images. In Sebastian Thrun, Lawrence Saul, and Bernhard Sch olkopf, editors, Advances in Neural Information Processing Systems (NIPS) 16. MIT Press, Cambridge, MA, 2003. John Lafferty, Andrew McCallum, and Fernando Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. International Conference on Machine Learning (ICML), 2001. Robert Malouf. A comparison of algorithms for maximum entropy parameter estimation. In Dan Roth and Antal van den Bosch, editors, Conference on Natural Language Learning (CoNLL), pages 49–55, 2002. ¨ Christopher D. Manning and Hinrich Schutze. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, MA, 1999. Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, 1993. Andrew McCallum and David Jensen. A note on the uniﬁcation of information extraction and data mining using conditional-probability, relational models. In IJCAI’03 Workshop on Learning Statistical Models from Relational Data, 2003. Andrew McCallum, Dayne Freitag, and Fernando Pereira. Maximum entropy Markov models for information extraction and segmentation. In International Conference on Machine Learning (ICML), pages 591–598. Morgan Kaufmann, San Francisco, CA, 2000. Andrew McCallum, Kedar Bellare, and Fernando Pereira. A conditional random ﬁeld for discriminatively-trained ﬁnite-state string edit distance. In Conference on Uncertainty in AI (UAI), 2005. Ryan McDonald, Koby Crammer, and Fernando Pereira. Online large-margin training of dependency parsers. In Proceedings of the Annual Meeting of the ACL, pages 91–98, 2005. 720  DYNAMIC C ONDITIONAL R ANDOM F IELDS  Thomas P. Minka. The EP energy function and minimization schemes. http://research. microsoft.com/˜minka/papers/ep/minka-ep-energy.pdf, 2001a. Tom Minka. Divergence measures and message passing. Technical Report MSR-TR-2005-173, Microsoft Research, 2005. Tom Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, MIT, 2001b. Mehryar Mohri, Fernando Pereira, and Michael Riley. Weighted ﬁnite-state transducers in speech recognition. Computer Speech and Language, 16(1):69–88, 2002. Kevin Murphy and Mark A. Paskin. Linear time inference in hierarchical HMMs. In Advances in Neural Information Processing Systems (NIPS), 2001. Kevin P. Murphy. Dynamic Bayesian Networks: Representation, Inference and Learning. PhD thesis, U.C. Berkeley, July 2002. Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. Loopy belief propagation for approximate inference: An empirical study. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 467–475, 1999. Ara V. Neﬁan, Luhong Liang, Xiaobo Pi, Liu Xiaoxiang, Crusoe Mao, and Kevin Murphy. A coupled HMM for audio-visual speech recognition. In IEEE Int’l Conference on Acoustics, Speech and Signal Processing, pages 2013–2016, 2002. Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer-Verlag, New York, 1999. ISBN 0-387-98793-2. Leonid Peshkin and Avi Pfeffer. Bayesian information extraction network. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2003. David Pinto, Andrew McCallum, Xing Wei, and W. Bruce Croft. Table extraction using conditional random ﬁelds. In Proceedings of the ACM SIGIR, 2003. Ariadna Quattoni, Michael Collins, and Trevor Darrell. Conditional random ﬁelds for object recognition. In Lawrence K. Saul, Yair Weiss, and L´ on Bottou, editors, Advances in Neural Informae tion Processing Systems 17, pages 1097–1104. MIT Press, Cambridge, MA, 2005. Lawrence R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257 – 286, 1989. Lance A. Ramshaw and Mitchell P. Marcus. Text chunking using transformation-based learning. In Proceedings of the Third ACL Workshop on Very Large Corpora, 1995. Adwait Ratnaparkhi. A maximum entropy model for part-of-speech tagging. In Conference on Empirical Methods in Natural Language Proceeding (EMNLP), 1996. Dan Roth and Wen-tau Yih. Relational learning via propositional algorithms: An information extraction case study. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), pages 1257–1263, 2001. 721  S UTTON , M C C ALLUM AND ROHANIMANESH  Erik F. Tjong Kim Sang and Sabine Buchholz. Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of CoNLL-2000 and LLL-2000, 2000. See http://lcg-www.uia.ac.be/ ˜erikt/research/np-chunking.html. Fei Sha and Fernando Pereira. Shallow parsing with conditional random ﬁelds. In Conference on Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL), pages 213–220, 2003. Marios Skounakis, Mark Craven, and Soumya Ray. Hierarchical hidden Markov models for information extraction. In Proceedings of the 18th International Joint Conference on Artiﬁcial Intelligence, 2003. Stephen Soderland. Learning information extraction rules for semi-structured and free text. Machine Learning, pages 233–272, 1999. Charles Sutton and Andrew McCallum. Composition of conditional random ﬁelds for transfer learning. In Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP), 2005. Charles Sutton and Andrew McCallum. An introduction to conditional random ﬁelds for relational learning. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning. MIT Press, 2006. To appear. Charles Sutton, Khashayar Rohanimanesh, and Andrew McCallum. Dynamic conditional random ﬁelds: Factorized probabilistic models for labeling and segmenting sequence data. In International Conference on Machine Learning (ICML), 2004. Ben Taskar, Pieter Abbeel, and Daphne Koller. Discriminative probabilistic models for relational data. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2002. Ben Taskar, Carlos Guestrin, and Daphne Koller. Max-margin Markov networks. In Sebastian ¨ Thrun, Lawrence Saul, and Bernhard Scholkopf, editors, Advances in Neural Information Processing Systems 16. MIT Press, Cambridge, MA, 2004a. Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and Chris Manning. Max-margin parsing. In Empirical Methods in Natural Language Processing (EMNLP04), 2004b. Georgios Theocharous, Khashayar Rohanimanesh, and Sridhar Mahadevan. Learning hierarchical partially observable Markov decision processes for robot navigation. In Proceedings of the IEEE Conference on Robotics and Automation, 2001. Paul Viola and Mukund Narasimhan. Learning to extract information from semi-structured text using a discriminative context free grammar. In Proceedings of the ACM SIGIR, 2005. S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W. Schmidt, and Kevin Murphy. Accelerated training of conditional random ﬁelds with stochastic meta-descent. In International Conference on Machine Learning (ICML), pages 969–976, 2006. Martin Wainwright. Stochastic processes on graphs with cycles: geometric and variational approaches. PhD thesis, MIT, 2002. 722  DYNAMIC C ONDITIONAL R ANDOM F IELDS  Martin Wainwright, Tommi Jaakkola, and Alan S. Willsky. Tree-based reparameterization for approximate estimation on graphs with cycles. Advances in Neural Information Processing Systems (NIPS), 2001. Hanna Wallach. Efﬁcient training of conditional random ﬁelds. M.Sc. thesis, University of Edinburgh, 2002. David H. Wolpert. Stacked generalization. Neural Networks, 5(2):241–259, 1992. Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. Constructing free-energy approximations and generalized belief propagation algorithms. IEEE Transactions on Information Theory, 51(7): 2282–2312, July 2005.  723</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
