<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-37" href="../jmlr2007/jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">jmlr2007-37</a> <a title="jmlr-2007-37-reference" href="#">jmlr2007-37-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</h1>
<br/><p>Source: <a title="jmlr-2007-37-pdf" href="http://jmlr.org/papers/volume8/chakrabartty07a/chakrabartty07a.pdf">pdf</a></p><p>Author: Shantanu Chakrabartty, Gert Cauwenberghs</p><p>Abstract: Many classiﬁcation tasks require estimation of output class probabilities for use as conﬁdence scores or for inference integrated with other models. Probability estimates derived from large margin classiﬁers such as support vector machines (SVMs) are often unreliable. We extend SVM large margin classiﬁcation to GiniSVM maximum entropy multi-class probability regression. GiniSVM combines a quadratic (Gini-Simpson) entropy based agnostic model with a kernel based similarity model. A form of Huber loss in the GiniSVM primal formulation elucidates a connection to robust estimation, further corroborated by the impulsive noise ﬁltering property of the reverse water-ﬁlling procedure to arrive at normalized classiﬁcation margins. The GiniSVM normalized classiﬁcation margins directly provide estimates of class conditional probabilities, approximating kernel logistic regression (KLR) but at reduced computational cost. As with other SVMs, GiniSVM produces a sparse kernel expansion and is trained by solving a quadratic program under linear constraints. GiniSVM training is efﬁciently implemented by sequential minimum optimization or by growth transformation on probability functions. Results on synthetic and benchmark data, including speaker veriﬁcation and face detection data, show improved classiﬁcation performance and increased tolerance to imprecision over soft-margin SVM and KLR. Keywords: support vector machines, large margin classiﬁers, kernel regression, probabilistic models, quadratic entropy, Gini index, growth transformation</p><br/>
<h2>reference text</h2><p>E.L. Allwein, R.E. Schapire, and Y. Singer. Reducing multiclass to binary: A unifying approach for margin classiﬁers. Journal of Machine Learning Research, 1:113-141, 2000. M. Alvira and R. Rifkin. An empirical comparison of SNoW and SVMs for face detection. CBCL paper 193 /AI Memo 2001-2004, MIT, 2001. R. Auckenthaler, M. Carey and H. Lloyd-Thomas. Score normalization for text-independent speaker veriﬁcation system. Digital Signal Processing, 10(1):42-54, 2000. L.E. Baum and G. Sell. Growth transformations for functions on manifolds. Paciﬁc J. Math., 27(2):211-227, 1968. D. Bertsekas. Non-linear Programming. Athena Scientiﬁc, MA, 1995. B. Boser, I. Guyon and V. Vapnik. A training algorithm for optimal margin classiﬁer. Proc. 5th Ann. ACM Workshop on Computational Learning Theory (COLT), pages 144-52, 1992. L. Breiman, J.H. Friedman and R. Olshen. Classiﬁcation and Regression Trees. Wadsworth and Brooks, Paciﬁc Grove CA, 1984. C. Burges. A tutorial on support vector machines for pattern recognition. U. Fayyad, Ed., Proc. Data Mining and Knowledge Discovery, pages 1-43, 1998. W.M. Campbell, K.T. Assaleh and C.C. Broun. Speaker with polynomial classiﬁers. IEEE Trans. Speech and Audio Proc., 10(4):205-212, May 2002. G. Cauwenberghs and T. Poggio. Incremental and decremental support vector machine learning. Adv. Neural Information Processing Systems 10, Cambridge MA: MIT Press, 2001. S. Chakrabartty and G. Cauwenberghs. Forward decoding kernel machines: A hybrid HMM/SVM approach to sequence recognition. IEEE Int. Conf. of Pattern Recognition: SVM workshop. (ICPR’2002), 2002. S. Chakrabartty and G. Cauwenberghs. Margin propagation and forward decoding in analog VLSI. Proc. IEEE Int. Symp. Circuits and Systems (ISCAS’2004), 2004. S. Chakrabartty and G. Cauwenberghs. Sub-microwatt analog VLSI support vector machine for pattern classiﬁcation and sequence estimation. Adv in Neural Information Processing Systems 17, Cambridge: MIT Press, 2005. T.M. Cover and J.A. Thomas, Elements of Information Theory. John Wiley and Sons, 1991. K. Crammer and Y. Singer. The learnability and design of output codes for multiclass problems. Proc. 13th Ann. Conf. Computational Learning Theory (COLT), 2000. T.G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting output codes. Journal of Artiﬁcial Intelligence Research, 2:263-286, 1995. F. Girosi, M. Jones and T. Poggio. Regularization theory and neural networks architectures. Neural Computation, 7:219-269, 1995. 837  C HAKRABARTTY AND C AUWENBERGHS  P.S. Gopalakrishnan, D. Kanevsky, A. Nadas and D. Nahamoo. An inequality for rational functions with applications to some statistical estimation problems. IEEE Trans. Information Theory, 37(1):107-113, 1991. Y. Gu and T. Thomas. A text-independent speaker veriﬁcation system using support vector machines classiﬁer. Proc. Eur. Conf. Speech Communication and Technology (Eurospeech’01), pages 17651769, 2001. C. Hsu and C. Lin. A comparison of methods for multi-class support vector machines. IEEE Trans. Neural Networks, 13(2):415-425, 2002. P.J. Huber. Robust Estimation of Location Parameter. Annals of Mathematical Statistics, volume 35, 1964. T. Jaakkola and D. Haussler. Probabilistic kernel regression models. Proc. 7th Int. Workshop on Artiﬁcial Intelligence and Statistics, 1999. E. Jaynes. Information theory and statistical mechanics. Physics Review, 106:620-630, 1957. T. Jebara. Discriminative, generative and imitative learning. PhD Thesis, MIT Media Laboratory, 2001. T. Joachims. Text categorization with support vector machines. Technical Report LS-8 23, Univ. of Dortmund, 1997. ¨ T. Joachims. Making large-scale support vector machine learning practical, In Sch olkopf, Burges and Smola, Eds., Advances in Kernel Methods: Support Vector Machines, Cambridge MA: MIT Press, 1998. M.I. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Proc. Int. Joint Conference on Neural Networks, 2:1339-1344, 1993. S.S. Keerthi, S.K. Shevade, C. Bhattacharyya and K.R.K. Murthy. Improvements to Platt’s SMO algorithm for SVM classiﬁer design. Neural Computation, 13:637-649, 2001. J.T.Y. Kwok. Moderating the outputs of support vector machine classiﬁers. IEEE Transactions on Neural Networks, 10(5):1018-1031, 1999. T. Nayak and C.R. Rao. Cross entropy, dissimilarity measures and characterizations of quadratic entropy. IEEE Trans. Information Theory, IT-31:589-593, 1985. N. Lawrence, M. Seeger and R. Herbrich. Fast sparse gaussian process methods: The informative vector machine. Neural Information Processing Systems 15, pages 609-616, 2003. M. Oren, C. Papageorgiou, P. Sinha, E. Osuna and T. Poggio. Pedestrian detection using wavelet templates. Computer Vision and Pattern Recognition (CVPR),pages 193-199, 1997. E. Osuna, R. Freund and F. Girosi. Training support vector machines: An application to face detection. Computer Vision and Pattern Recognition, pages 130-136, 1997. D.S. Pietra and D.V. Pietra. Statistical Modeling by ME. IBM Internal Report, 1993. 838  G INI S UPPORT V ECTOR M ACHINE  J. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In A. Smola et al., Eds., Adv. Large Margin Classiﬁers, Cambridge MA: MIT Press, 1999. J. Platt. Fast training of support vector machine using sequential minimal optimization. In Scholkopf, Burges and Smola, Eds., Adv. Kernel Methods, Cambridge MA: MIT Press, 1999. M. Pontil and A. Verri. Properties of support vector machines. Neural Computation, 10:977-996, 1998. M. Pontil and A. Verri. Support Vector Machines for 3-D Object Recognition. IEEE Transactions of Pattern Analysis and Machine Intelligence, 20:637-646, 1998. L. Rabiner and B.H. Juang. Fundamentals of Speech Recognition, Englewood Cliffs, NJ: PrenticeHall, 1993. R.T. Rockefeller. Convex Analysis. Princeton Landmarks in Mathematics and Physics, Princeton University Press, 1970. H.A. Rowley, S.A. Baluja and T. Kanade. Neural network based face detection. IEEE Transactions of Pattern Analysis and Machine Intelligence, 20(1):23-38, 1998. M. Schmidt and H. Gish. Speaker identiﬁcation via support vector classiﬁers. Proc. IEEE Int. Conf. Acoustics, Speech, Signal Processing (ICASSP’96), 1:105-108, 1996. B. Sch¨ lkopf, C. Burges and A. Smola Eds., Adv. Kernel Methods-Support Vector Learning, MIT o Press, Cambridge MA, 1998. B. Sch¨ lkopf and A. Smola. Learning with Kernels: Support Vector Machines, Regularization, o Optimization, and Beyond, MIT Press, Cambridge MA, 2001. M.E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1:211-244, 2001. V. Vapnik. The Nature of Statistical Learning Theory. New York: Springer-Verlag, 1995. G. Wahba. Support vector machines, reproducing kernel Hilbert spaces and randomized GACV. ¨ Adv. Kernel Methods-Support Vector Learning, B. Scholkopf, C.J.C. Burges and A.J. Smola, Eds., Cambridge MA: MIT Press, 1998. J. Weston and C. Watkins. Multi-class support vector machines. Technical Report CSD-TR-980004, Department of Computer Science, Royal Holloway, Univ. London, 1998. J. Zhu and T. Hastie. Kernel logistic regression and import vector machine. Adv. Neural Information Processing Systems (NIPS’2001), Cambridge, MA: MIT Press, 2002.  839</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
