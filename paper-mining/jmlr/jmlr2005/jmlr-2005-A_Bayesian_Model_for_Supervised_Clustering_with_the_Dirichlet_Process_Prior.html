<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-2" href="#">jmlr2005-2</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</h1>
<br/><p>Source: <a title="jmlr-2005-2-pdf" href="http://jmlr.org/papers/volume6/daume05a/daume05a.pdf">pdf</a></p><p>Author: Hal Daumé III, Daniel Marcu</p><p>Abstract: We develop a Bayesian framework for tackling the supervised clustering problem, the generic problem encountered in tasks such as reference matching, coreference resolution, identity uncertainty and record linkage. Our clustering model is based on the Dirichlet process prior, which enables us to deﬁne distributions over the countably inﬁnite sets that naturally arise in this problem. We add supervision to our model by positing the existence of a set of unobserved random variables (we call these “reference types”) that are generic across all clusters. Inference in our framework, which requires integrating over inﬁnitely many parameters, is solved using Markov chain Monte Carlo techniques. We present algorithms for both conjugate and non-conjugate priors. We present a simple—but general—parameterization of our model based on a Gaussian assumption. We evaluate this model on one artiﬁcial task and three real-world tasks, comparing it against both unsupervised and state-of-the-art supervised algorithms. Our results show that our model is able to outperform other models across a variety of tasks and performance metrics. Keywords: supervised clustering, record linkage, citation matching, coreference, Dirichlet process, non-parametric Bayesian</p><p>Reference: <a title="jmlr-2005-2-reference" href="../jmlr2005_reference/jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our clustering model is based on the Dirichlet process prior, which enables us to deﬁne distributions over the countably inﬁnite sets that naturally arise in this problem. [sent-4, score-0.284]
</p><p>2 The distinction between supervised clustering and standard clustering is that in the supervised form we are given training examples. [sent-17, score-0.674]
</p><p>3 In the end, the supervised clustering task is a prediction problem: a new X (n+1) ⊆ X is presented and a system must produce a partition of it. [sent-23, score-0.337]
</p><p>4 The supervised clustering problem goes under many names, depending on the goals of the interested community. [sent-24, score-0.337]
</p><p>5 Here, the task is known as reference matching (McCallum et al. [sent-31, score-0.302]
</p><p>6 In natural language processing, the problem arises in the context of coreference resolution, wherein one wishes to identify which entities mentioned in a document are the same person (or organization) in real life (Soon et al. [sent-33, score-0.336]
</p><p>7 In this paper, we propose a generative model for solving the supervised clustering problem. [sent-36, score-0.437]
</p><p>8 In Section 2, we describe prior efforts to tackle the supervised clustering problem. [sent-42, score-0.38]
</p><p>9 We then discuss performance metrics for the supervised clustering problem in Section 6 and present experimental results of our models’ performance on artiﬁcial and real-world problems in Section 7. [sent-46, score-0.407]
</p><p>10 Prior Work The most common technique for solving supervised clustering is by mapping it to binary classiﬁcation. [sent-49, score-0.337]
</p><p>11 In addition to the classiﬁcation plus clustering approach, there have been several attempts to solve the supervised clustering problem directly. [sent-67, score-0.567]
</p><p>12 Using a learned distance metric, one is able to use a standard clustering algorithm for doing the ﬁnal predictions. [sent-73, score-0.324]
</p><p>13 Two other recent techniques have been proposed for directly solving the supervised clustering problem, and are not phrased in terms of learning a Mahalanobis distance. [sent-77, score-0.337]
</p><p>14 Exact inference in this model is intractable (as in most supervised clustering models), and they employ a simple perceptron-style update scheme, which they show to be quite effective on this task. [sent-82, score-0.454]
</p><p>15 The other direct solution to the supervised clustering problem, due to Finley and Joachims (2005), is based on the SVMs for Interdependent and Structured Outputs technique (Tsochantaridis et al. [sent-86, score-0.337]
</p><p>16 In this model, a particular clustering method, correlation clustering, is held ﬁxed, and weights are optimized to minimize the regularized empirical loss of the training data with respect to this clustering function. [sent-88, score-0.46]
</p><p>17 This model formulates the identity uncertainty/citation matching problem in a generative framework, based on a complex generative model under which inference is intractable. [sent-103, score-0.384]
</p><p>18 (2003) model that solves the ﬁrst problem: estimating the number of clusters in the citation matching domain. [sent-110, score-0.289]
</p><p>19 The fact that this model has now been proposed twice, independently, is not surprising: citation matching is a well-known problem that suffers from the need to estimate the number of clusters in a data set, and the Dirichlet process excels at precisely this task. [sent-112, score-0.289]
</p><p>20 Supervised Clustering Model In this section, we describe our model for the supervised clustering problem. [sent-114, score-0.391]
</p><p>21 To facilitate discussion, we take our terminology and notation from the reference matching task. [sent-115, score-0.302]
</p><p>22 The ﬁrst attribute speciﬁes which publication this 1554  A BAYESIAN M ODEL FOR S UPERVISED C LUSTERING  reference belongs to. [sent-132, score-0.347]
</p><p>23 ” A reference type encompasses the notion that under different circumstances, references to the same publication are realized differently. [sent-134, score-0.347]
</p><p>24 In the terminology of reference matching, in the context of a short workshop paper (for instance), author ﬁrst names might be abbreviated as initials, page numbers might be left off and conferences and journals might be referred to by abbreviations. [sent-135, score-0.289]
</p><p>25 On the contrary, in a reference appearing in a journal, page numbers are included, as are full conference/journal names and author names. [sent-136, score-0.289]
</p><p>26 In the context of coreference resolution, one reference type might be for generating proper names (“Bill Clinton”), one for nominal constructions (“the President”) and one for pronouns (“he”). [sent-137, score-0.622]
</p><p>27 For each reference rn appearing in the data set: p (a) Select the corresponding publication pn ∼ G0 . [sent-145, score-0.412]
</p><p>28 (c) Generate rn by a problem-speciﬁc distribution parameterized by the publication and reference type: rn ∼ F(pn ,tn ). [sent-147, score-0.477]
</p><p>29 The Dirichlet process (DP), which is a distribution over distributions, can be most easily understood via a generalized P` lya urn scheme, where one draws colored balls from an urn with o replacement. [sent-150, score-0.338]
</p><p>30 Just as a multiclass classiﬁcation model can be seen as a ﬁnite mixture model where the mixture components correspond to the ﬁnite classes, the supervised clustering model can be seen as an inﬁnite mixture model. [sent-155, score-0.499]
</p><p>31 In the case of our choice of the Dirichlet process as a prior over publications, one such issue is that of the expected number p G0  1555  ´ DAUM E III AND M ARCU  αp  πp k  p  pk  G  cn  rn  π tl  αt  tl  dn  G  t  N Figure 1: Graphical model for our generic supervised clustering model. [sent-160, score-0.898]
</p><p>32 We have performed such experiments and veriﬁed that on a variety of problems (reference matching, identity uncertainty and coreference resolution), the Dirichlet process is appropriate with respect to this measure (see Section 7. [sent-162, score-0.358]
</p><p>33 2 Hierarchical Model The model we propose is structured as follows: πp | αp cn | π p p pk | G0 rn | cn , dn , p,t  ∼ ∼ ∼ ∼  Dir(α p /K, . [sent-165, score-0.419]
</p><p>34 The αs give rise to multinomial random variables π, which in turn determine indicator variables cn (specifying the publication to which rn belongs) and dn (specifying the reference type used by reference rn ). [sent-179, score-0.846]
</p><p>35 The base density G p generates publications pk (according to a problem-speciﬁc distribution), while the base density Gt generates reference types tl (again according to a problem-speciﬁc distribution). [sent-180, score-0.482]
</p><p>36 Finally, the observed reference rn is generated according to publication pcn and reference type tdn with problemspeciﬁc distribution F. [sent-181, score-0.739]
</p><p>37 As indicated by the counts on the plates for the (π p , p) and (πt ,t) variables, we take the limit as K → ∞ and L → ∞ (where K is the number of publications and L is the number of reference types). [sent-183, score-0.344]
</p><p>38 The MCMC-based Bayesian solution to the supervised clustering problem (or, indeed, any problem) is to write down the expression corresponding to the posterior distribution of the cn s for the test data and draw samples from that posterior. [sent-195, score-0.449]
</p><p>39 In each iteration of sampling, we ﬁrst resample each active publication pc and reference type td according to their posterior densities (in the case of conjugate priors, this is possible). [sent-205, score-0.518]
</p><p>40 Then, for each test reference, we resample its publication and for all references, we resample the corresponding reference type. [sent-206, score-0.443]
</p><p>41 The overall structure of the sampling algorithm remains identical in the case of non-conjugate priors; however, the sampling for the indicator variables cn and dn changes slightly, and so does the sampling of the p andR variables. [sent-222, score-0.313]
</p><p>42 For instance, in the conjugate case, dn is sampled according to the t marginal distribution dGt0 (t)F(rn | pcn ,t), which is analytically unavailable when Gt0 is not conjugate to F (with respect to the second variable). [sent-223, score-0.35]
</p><p>43 West (1992) gives a method for drawing samples for the precision parameter given the number of references Nand the number of publications K (or, for αt , the number of reference types); in his analysis, it is natural to place a gamma prior on α. [sent-238, score-0.51]
</p><p>44 In most cases, his analysis can be applied directly; however, in the case of coreference resolution, the problem is a bit more complicated because we have multiple observations pairs (N, K) for each “training document. [sent-239, score-0.279]
</p><p>45 In terms of our generative story, this means that ﬁrst we choose a publication then, based on the publication, choose a reference type. [sent-270, score-0.393]
</p><p>46 We parameterize the distribution on the means p (p j | pk ,ti ) by treating the distance between p j and pk , measured by ti as a random variable with an exponential distribution: 2 p (p j | pk ,ti ) = λ exp[−λ p j − pk ti ]. [sent-281, score-0.627]
</p><p>47 All of these metrics assume that we have a gold standard (correct) clustering G and a hypothesis clustering H and that the total number of data points is N. [sent-295, score-0.53]
</p><p>48 1 Rand Index The rand index (Rand, 1971) is computed by viewing the clustering problem as a binary classiﬁcation problem. [sent-297, score-0.382]
</p><p>49 Letting N11 denote the number of pairs that are in the same cluster in both G and in H, and letting N00 denote the number of pairs that are in different clusters in both G and H, the rand index has value RI(G, H) = 2[N11 + N00 ]/[N(N − 1)]. [sent-298, score-0.349]
</p><p>50 The rand index is the most frequently reported metric in the clustering literature, though we believe that its value is often misleading. [sent-301, score-0.461]
</p><p>51 As we show in our results (Section 7), a very simple baseline system that places each element in its own cluster tends to achieve a very high rand index. [sent-302, score-0.302]
</p><p>52 Moreover, the inﬂuence of large clusters on the rand index quadratically outnumbers the inﬂuence of small clusters on this value, so system performance on small clusters (which are typically the most difﬁcult) becomes insigniﬁcant. [sent-305, score-0.461]
</p><p>53 We recommend other researchers in the supervised clustering ﬁeld to report on other metrics of system performance than the rand index. [sent-307, score-0.559]
</p><p>54 Extending the notation used for the rand index, we write N10 for the number of pairs that are in the same cluster in G, but in different clusters in H. [sent-310, score-0.349]
</p><p>55 3 Cluster Edit Distance and Normalized Edit Score Pantel (2003) proposes a metric called the cluster edit distance, which computes the number of “create,” “move,” and “merge” operations required to transform the hypothesis clustering into the gold standard. [sent-317, score-0.628]
</p><p>56 Since no “split” operation is allowed, the cluster edit distance can be computed easily and efﬁciently. [sent-318, score-0.413]
</p><p>57 However, the lack of a split operation (which is absent precisely so that the computation of the metric is efﬁcient) means that the cluster edit distance favors algorithms that tend to make too many clusters, rather than too few clusters. [sent-319, score-0.492]
</p><p>58 The cluster edit distance has a minimum at 0 for the perfect clustering and a maximum of N. [sent-321, score-0.643]
</p><p>59 Also note that the cluster edit distance is not symmetric: in general, it does not hold that CED(G, H) = CED(H, G) (again, precisely because splits are disallowed). [sent-322, score-0.413]
</p><p>60 We propose a variant of the cluster edit distance that we call the normalized edit score. [sent-323, score-0.638]
</p><p>61 Experimental Results In this section, we present experimental results on both artiﬁcial and real-world data sets, comparing our model against other supervised clustering algorithms as well as other standard clustering algorithms. [sent-338, score-0.621]
</p><p>62 After the SVM has been optimized, we use an agglomerative clustering algorithm to create clusters according to either minimum, maximum or average link, with a threshold to stop merging. [sent-354, score-0.333]
</p><p>63 This model learns a distance metric in the form of a positive semi-deﬁnite matrix A and computes the distance between vectors x and y as [(x − y) A(x − y)]1/2 . [sent-360, score-0.321]
</p><p>64 (2004), proper noun coreference data from NIST and reference matching data from McCallum et al. [sent-378, score-0.737]
</p><p>65 In order to treat it as a supervised clustering problem, we randomly selected ﬁve of the ten digits as the “training data” and use the remaining ﬁve as “test data. [sent-408, score-0.421]
</p><p>66 The idea is that upon seeing only the digits {1, 3, 5, 8, 9}, a supervised clustering model should have learned enough about the structure of digits to be able to separate the digits {0, 2, 4, 6, 7}, even though it has seen none of them (of course, it will not be able to label them). [sent-410, score-0.643]
</p><p>67 According to F-score and NES, our model is universally better; however, according to VI and NVI, the distance metric method outperforms our model 1, but not models 2 and 3. [sent-494, score-0.281]
</p><p>68 However, according to both the edit distance metrics and the information metrics, our models all outperform the binary classiﬁer. [sent-602, score-0.436]
</p><p>69 3 P ROPER N OUN C OREFERENCE DATA The third set of data on which we evaluate is a subtask of the coreference task, namely, coreference of proper nouns (e. [sent-607, score-0.66]
</p><p>70 This subtask is signiﬁcantly simpler than the full task, since one need not identify coreference between pronouns and proper nouns (“he” ↔ “George Bush”), nor proper nouns and deﬁnite descriptions (“George Bush” ↔ “the President”). [sent-610, score-0.435]
</p><p>71 As features we use string edit distance, string edit distance on the heads (ﬁnal word), the length of the longest common substring, the length of the longest common subsequence, and the string edit distance between the abbreviations of both terms. [sent-615, score-1.046]
</p><p>72 , “IBM”) alone; we then compute string edit distance between these pairs. [sent-618, score-0.38]
</p><p>73 The results of the systems on the coreference data are shown in Table 3. [sent-622, score-0.279]
</p><p>74 958  Table 3: Results on the proper noun coreference data. [sent-704, score-0.435]
</p><p>75 Overall, however, based on this data, it seems reasonable to say that one might be better served writing a dozen more rules to capture notions of abbreviation, post-modiﬁcation, and a few other simple phenomena to handle the proper noun coreference task, rather than try to learn a model from data. [sent-722, score-0.489]
</p><p>76 4 R EFERENCE M ATCHING DATA Lastly, we perform evaluation on the Cora reference matching data set McCallum et al. [sent-725, score-0.302]
</p><p>77 We use the same feature set as in the identity uncertainty evaluation, with the exception that the ﬁrst two features become the string edit distance between the publication names and the string edit distance between the primary author names, respectively. [sent-732, score-1.039]
</p><p>78 Of course, the proper-noun coreference task is the easiest subtask of full coreference resolution, where empirical results have shown learned systems are able to outperform rule-based systems. [sent-736, score-0.653]
</p><p>79 847  Table 4: Results on the reference matching data. [sent-811, score-0.302]
</p><p>80 The results of the systems on the reference matching data are shown in Table 4. [sent-833, score-0.302]
</p><p>81 Again, as in the identity uncertainty data, we see that learning a distance metric can hurt performance (at least according to F-score; with respect to edit score and normalized VI, it seems to help, but only marginally so). [sent-835, score-0.477]
</p><p>82 In two of the data sets (digits and proper noun coreference), the learned distance metric (X ING K) achieved superior performance to standard K-means, but in the other two data sets, learning the distance metric hurt. [sent-847, score-0.502]
</p><p>83 It tended to consistently outperform the binary classiﬁer in terms of Fscore, but in terms of NES and NVI, the binary classiﬁer was better on the reference matching data. [sent-853, score-0.349]
</p><p>84 On the proper noun coreference data, our model was unable to match the performance of the binary classiﬁer, but both performed more poorly than the simple head-matching baseline system, suggesting that future work on this subtask is perhaps best handled by rules, rather than learning. [sent-854, score-0.593]
</p><p>85 Discussion In this paper, we have presented a Bayesian model for the supervised clustering problem. [sent-857, score-0.391]
</p><p>86 Full Bayesian inference (similar to transduction) has an advantage over the more standard training/prediction phases: the test data has no inﬂuence on the reference types. [sent-874, score-0.281]
</p><p>87 The most computationally intensive run of our model with the application of Model 3 to the proper noun coreference data, which required roughly one CPU year to perform. [sent-882, score-0.489]
</p><p>88 Under the Gaussian assumption, the reference types become covariance matrices, which—when there is only one reference type—can be interpreted as a transform on the data. [sent-892, score-0.436]
</p><p>89 However, when there is more than one reference type, or in the case of full Bayesian inference, the sorts of data distributions accounted for by our model are more general than in the standard metric learning scenario. [sent-893, score-0.351]
</p><p>90 Finally, to foster further research in the supervised clustering problem, we have contributed our data sets and scoring software to the RIDDLE data repository, http://www. [sent-901, score-0.337]
</p><p>91 In this scheme, we can see that there is a clustering effect in this model: as more balls of one color (say, blue) are drawn, the number of blue balls in the urn increases, so the probability of drawing a blue ball in the next iteration is higher. [sent-942, score-0.419]
</p><p>92 Placing a gamma prior on α with shape parameter a and scale parameter b, we obtain the posterior distribution of α given all the nm , km as  M  α p (α | x, k, n) ∝ e−bα αa−1 ∏ αkm −1 (α + nm )xm (1 − xm )nm −1 m=1  M  ∝ αa−M−1+∑m=1 km e−α(b−log ∏m=1 xm ) ∏ (α + nm ). [sent-971, score-0.585]
</p><p>93 m=1  (5)  ´ DAUM E III AND M ARCU  ˆ Where, writing a to denote the value a − M − 1 + ∑M km and b to denote b − log ∏M xm , the ˆ m=1 m=1 mixing weights ρ are deﬁned by ρi =  M 1 Γ a + ∑ im ˆ Z m=1  M  ∏  ˆ nm b  1−im  . [sent-973, score-0.286]
</p><p>94 There are ∑ im choices of α, corresponding to the ∑ im in the shape parameter for the posterior gamma distribution in Equation (5). [sent-975, score-0.323]
</p><p>95 Finally, when the shape parameter of the gamma dism tribution increases by 1 for each im = 1, the constant of proportionality for the gamma distribution increases by a factor of b − log ∏ xm , which is compensated for by the last term above. [sent-978, score-0.288]
</p><p>96 Similarly, we can obtain a marginal distribution for each xm conditional on α and k as: α xm | α, nm , km ∝ xm (1 − xm )nm −1 ∼ Bet(α + 1, nm )  (7)  In order to sample α, we ﬁrst sample x by a sequence of m beta distributions according to Equation (7), conditioned on the current value of α and n. [sent-979, score-0.407]
</p><p>97 The derivation of this posterior is straightforward:  im = 1 | i−m =  a + ∑m =m im ˆ ˆ a + ∑m =m im + nm b ˆ  . [sent-986, score-0.44]
</p><p>98 Efﬁcient clustering of high-dimensional data sets with application to reference matching. [sent-1067, score-0.448]
</p><p>99 A machine learning approach to coreference resolution of noun phrases. [sent-1104, score-0.422]
</p><p>100 Distance metric learning, with application to clustering with side-information. [sent-1119, score-0.309]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coreference', 0.279), ('dirichlet', 0.232), ('clustering', 0.23), ('edit', 0.225), ('reference', 0.218), ('daum', 0.194), ('dp', 0.189), ('arcu', 0.158), ('rand', 0.152), ('urn', 0.145), ('publication', 0.129), ('publications', 0.126), ('nvi', 0.109), ('pcn', 0.109), ('supervised', 0.107), ('im', 0.107), ('odel', 0.107), ('upervised', 0.107), ('clusters', 0.103), ('noun', 0.102), ('lustering', 0.098), ('cdp', 0.097), ('mccallum', 0.095), ('cluster', 0.094), ('distance', 0.094), ('digits', 0.084), ('matching', 0.084), ('ti', 0.084), ('dn', 0.083), ('pk', 0.081), ('conjugate', 0.079), ('metric', 0.079), ('nm', 0.075), ('bayesian', 0.075), ('wellner', 0.074), ('bush', 0.073), ('eans', 0.073), ('ferguson', 0.073), ('pasula', 0.073), ('ced', 0.072), ('inary', 0.072), ('names', 0.071), ('ing', 0.071), ('metrics', 0.07), ('cn', 0.068), ('rn', 0.065), ('gamma', 0.065), ('inference', 0.063), ('string', 0.061), ('gam', 0.061), ('dir', 0.061), ('oarse', 0.061), ('sampler', 0.061), ('precision', 0.058), ('entities', 0.057), ('tl', 0.057), ('chains', 0.056), ('baseline', 0.056), ('chain', 0.056), ('model', 0.054), ('sampling', 0.054), ('proper', 0.054), ('km', 0.053), ('generic', 0.053), ('iii', 0.053), ('xm', 0.051), ('priors', 0.051), ('story', 0.051), ('citation', 0.048), ('doan', 0.048), ('lya', 0.048), ('president', 0.048), ('resample', 0.048), ('subtask', 0.048), ('outperform', 0.047), ('generative', 0.046), ('posterior', 0.044), ('euclidean', 0.044), ('ball', 0.044), ('prior', 0.043), ('ine', 0.042), ('uncertainty', 0.042), ('neal', 0.041), ('mcmc', 0.041), ('resolution', 0.041), ('blackwell', 0.041), ('crf', 0.041), ('parameterize', 0.041), ('precisions', 0.041), ('bk', 0.04), ('er', 0.039), ('xing', 0.039), ('gibbs', 0.037), ('identity', 0.037), ('vi', 0.037), ('ame', 0.036), ('blei', 0.036), ('finley', 0.036), ('hal', 0.036), ('jasa', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="2-tfidf-1" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>Author: Hal Daumé III, Daniel Marcu</p><p>Abstract: We develop a Bayesian framework for tackling the supervised clustering problem, the generic problem encountered in tasks such as reference matching, coreference resolution, identity uncertainty and record linkage. Our clustering model is based on the Dirichlet process prior, which enables us to deﬁne distributions over the countably inﬁnite sets that naturally arise in this problem. We add supervision to our model by positing the existence of a set of unobserved random variables (we call these “reference types”) that are generic across all clusters. Inference in our framework, which requires integrating over inﬁnitely many parameters, is solved using Markov chain Monte Carlo techniques. We present algorithms for both conjugate and non-conjugate priors. We present a simple—but general—parameterization of our model based on a Gaussian assumption. We evaluate this model on one artiﬁcial task and three real-world tasks, comparing it against both unsupervised and state-of-the-art supervised algorithms. Our results show that our model is able to outperform other models across a variety of tasks and performance metrics. Keywords: supervised clustering, record linkage, citation matching, coreference, Dirichlet process, non-parametric Bayesian</p><p>2 0.1211364 <a title="2-tfidf-2" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>Author: Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, Suvrit Sra</p><p>Abstract: Several large scale data mining applications, such as text categorization and gene expression analysis, involve high-dimensional data that is also inherently directional in nature. Often such data is L2 normalized so that it lies on the surface of a unit hypersphere. Popular models such as (mixtures of) multi-variate Gaussians are inadequate for characterizing such data. This paper proposes a generative mixture-model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. In particular, we derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the mean and concentration parameters of this mixture. Numerical estimation of the concentration parameters is non-trivial in high dimensions since it involves functional inversion of ratios of Bessel functions. We also formulate two clustering algorithms corresponding to the variants of EM that we derive. Our approach provides a theoretical basis for the use of cosine similarity that has been widely employed by the information retrieval community, and obtains the spherical kmeans algorithm (kmeans with cosine similarity) as a special case of both variants. Empirical results on clustering of high-dimensional text and gene-expression data based on a mixture of vMF distributions show that the ability to estimate the concentration parameter for each vMF component, which is not present in existing approaches, yields superior results, especially for difﬁcult clustering tasks in high-dimensional spaces. Keywords: clustering, directional distributions, mixtures, von Mises-Fisher, expectation maximization, maximum likelihood, high dimensional data c 2005 Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh and Suvrit Sra. BANERJEE , D HILLON , G HOSH AND S RA</p><p>3 0.089217305 <a title="2-tfidf-3" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>Author: Lior Wolf, Amnon Shashua</p><p>Abstract: The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classiﬁcation tasks, for example, it is not uncommon to have 104 to 107 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a deﬁnition of “relevancy” based on spectral properties of the Laplacian of the features’ measurement matrix. The feature selection process is then based on a continuous ranking of the features deﬁned by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a “biased non-negativity” of a key matrix in the process. As a result, a simple leastsquares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.</p><p>4 0.073343962 <a title="2-tfidf-4" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>Author: John Winn, Christopher M. Bishop</p><p>Abstract: Bayesian inference is now widely established as one of the principal foundations for machine learning. In practice, exact inference is rarely possible, and so a variety of approximation techniques have been developed, one of the most widely used being a deterministic framework called variational inference. In this paper we introduce Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to Bayesian Networks. Like belief propagation, VMP proceeds by sending messages between nodes in the network and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing additional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational message passing has been implemented in the form of a general purpose inference engine called VIBES (‘Variational Inference for BayEsian networkS’) which allows models to be speciﬁed graphically and then solved variationally without recourse to coding. Keywords: Bayesian networks, variational inference, message passing</p><p>5 0.06805528 <a title="2-tfidf-5" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>Author: Rie Kubota Ando, Tong Zhang</p><p>Abstract: One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don’t have a complete understanding of their eﬀectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Speciﬁcally we consider learning predictive structures on hypothesis spaces (that is, what kind of classiﬁers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the eﬀectiveness of the proposed algorithms in the semi-supervised learning setting.</p><p>6 0.067048654 <a title="2-tfidf-6" href="./jmlr-2005-Clustering_with_Bregman_Divergences.html">20 jmlr-2005-Clustering with Bregman Divergences</a></p>
<p>7 0.066004492 <a title="2-tfidf-7" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>8 0.064712904 <a title="2-tfidf-8" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>9 0.063382044 <a title="2-tfidf-9" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>10 0.061630879 <a title="2-tfidf-10" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>11 0.060592368 <a title="2-tfidf-11" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>12 0.057665315 <a title="2-tfidf-12" href="./jmlr-2005-Local_Propagation_in_Conditional_Gaussian_Bayesian_Networks.html">51 jmlr-2005-Local Propagation in Conditional Gaussian Bayesian Networks</a></p>
<p>13 0.054803681 <a title="2-tfidf-13" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>14 0.053234149 <a title="2-tfidf-14" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>15 0.052627269 <a title="2-tfidf-15" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>16 0.05128305 <a title="2-tfidf-16" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>17 0.051095873 <a title="2-tfidf-17" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>18 0.050983962 <a title="2-tfidf-18" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>19 0.048298109 <a title="2-tfidf-19" href="./jmlr-2005-Prioritization_Methods_for_Accelerating_MDP_Solvers.html">61 jmlr-2005-Prioritization Methods for Accelerating MDP Solvers</a></p>
<p>20 0.047140889 <a title="2-tfidf-20" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.288), (1, -0.003), (2, 0.015), (3, -0.177), (4, 0.104), (5, 0.043), (6, -0.265), (7, -0.055), (8, 0.041), (9, -0.087), (10, 0.099), (11, -0.025), (12, -0.072), (13, 0.048), (14, -0.023), (15, -0.04), (16, 0.009), (17, -0.009), (18, 0.09), (19, 0.204), (20, -0.176), (21, 0.081), (22, -0.003), (23, 0.014), (24, 0.121), (25, -0.147), (26, -0.009), (27, -0.014), (28, -0.085), (29, -0.08), (30, 0.142), (31, 0.026), (32, 0.084), (33, 0.057), (34, -0.146), (35, 0.062), (36, -0.093), (37, -0.058), (38, 0.01), (39, 0.224), (40, 0.124), (41, 0.227), (42, -0.034), (43, 0.045), (44, 0.08), (45, -0.244), (46, 0.119), (47, -0.137), (48, -0.052), (49, -0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96294975 <a title="2-lsi-1" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>Author: Hal Daumé III, Daniel Marcu</p><p>Abstract: We develop a Bayesian framework for tackling the supervised clustering problem, the generic problem encountered in tasks such as reference matching, coreference resolution, identity uncertainty and record linkage. Our clustering model is based on the Dirichlet process prior, which enables us to deﬁne distributions over the countably inﬁnite sets that naturally arise in this problem. We add supervision to our model by positing the existence of a set of unobserved random variables (we call these “reference types”) that are generic across all clusters. Inference in our framework, which requires integrating over inﬁnitely many parameters, is solved using Markov chain Monte Carlo techniques. We present algorithms for both conjugate and non-conjugate priors. We present a simple—but general—parameterization of our model based on a Gaussian assumption. We evaluate this model on one artiﬁcial task and three real-world tasks, comparing it against both unsupervised and state-of-the-art supervised algorithms. Our results show that our model is able to outperform other models across a variety of tasks and performance metrics. Keywords: supervised clustering, record linkage, citation matching, coreference, Dirichlet process, non-parametric Bayesian</p><p>2 0.5570522 <a title="2-lsi-2" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>Author: Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, Suvrit Sra</p><p>Abstract: Several large scale data mining applications, such as text categorization and gene expression analysis, involve high-dimensional data that is also inherently directional in nature. Often such data is L2 normalized so that it lies on the surface of a unit hypersphere. Popular models such as (mixtures of) multi-variate Gaussians are inadequate for characterizing such data. This paper proposes a generative mixture-model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. In particular, we derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the mean and concentration parameters of this mixture. Numerical estimation of the concentration parameters is non-trivial in high dimensions since it involves functional inversion of ratios of Bessel functions. We also formulate two clustering algorithms corresponding to the variants of EM that we derive. Our approach provides a theoretical basis for the use of cosine similarity that has been widely employed by the information retrieval community, and obtains the spherical kmeans algorithm (kmeans with cosine similarity) as a special case of both variants. Empirical results on clustering of high-dimensional text and gene-expression data based on a mixture of vMF distributions show that the ability to estimate the concentration parameter for each vMF component, which is not present in existing approaches, yields superior results, especially for difﬁcult clustering tasks in high-dimensional spaces. Keywords: clustering, directional distributions, mixtures, von Mises-Fisher, expectation maximization, maximum likelihood, high dimensional data c 2005 Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh and Suvrit Sra. BANERJEE , D HILLON , G HOSH AND S RA</p><p>3 0.30826715 <a title="2-lsi-3" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>Author: Theodoros Evgeniou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we deﬁne is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Speciﬁc kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can signiﬁcantly outperform standard single-task learning particularly when there are many related tasks but few data per task. Keywords: multi-task learning, kernels, vector-valued functions, regularization, learning algorithms</p><p>4 0.29052699 <a title="2-lsi-4" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>Author: Lior Wolf, Amnon Shashua</p><p>Abstract: The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classiﬁcation tasks, for example, it is not uncommon to have 104 to 107 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a deﬁnition of “relevancy” based on spectral properties of the Laplacian of the features’ measurement matrix. The feature selection process is then based on a continuous ranking of the features deﬁned by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a “biased non-negativity” of a key matrix in the process. As a result, a simple leastsquares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.</p><p>5 0.29006097 <a title="2-lsi-5" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>Author: Aapo Hyvärinen</p><p>Abstract: One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data. Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence</p><p>6 0.26887694 <a title="2-lsi-6" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>7 0.26885185 <a title="2-lsi-7" href="./jmlr-2005-Prioritization_Methods_for_Accelerating_MDP_Solvers.html">61 jmlr-2005-Prioritization Methods for Accelerating MDP Solvers</a></p>
<p>8 0.25087959 <a title="2-lsi-8" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>9 0.2454018 <a title="2-lsi-9" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>10 0.24352203 <a title="2-lsi-10" href="./jmlr-2005-Clustering_with_Bregman_Divergences.html">20 jmlr-2005-Clustering with Bregman Divergences</a></p>
<p>11 0.2372321 <a title="2-lsi-11" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<p>12 0.22822815 <a title="2-lsi-12" href="./jmlr-2005-A_Unifying_View_of_Sparse_Approximate_Gaussian_Process_Regression.html">7 jmlr-2005-A Unifying View of Sparse Approximate Gaussian Process Regression</a></p>
<p>13 0.2273449 <a title="2-lsi-13" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>14 0.21939903 <a title="2-lsi-14" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>15 0.19662139 <a title="2-lsi-15" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>16 0.19625486 <a title="2-lsi-16" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>17 0.19149742 <a title="2-lsi-17" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>18 0.18728158 <a title="2-lsi-18" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>19 0.18367766 <a title="2-lsi-19" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>20 0.18364263 <a title="2-lsi-20" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.013), (17, 0.022), (19, 0.017), (36, 0.024), (37, 0.043), (42, 0.018), (43, 0.039), (47, 0.017), (52, 0.09), (59, 0.535), (70, 0.032), (88, 0.066), (94, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86417896 <a title="2-lda-1" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>Author: Hal Daumé III, Daniel Marcu</p><p>Abstract: We develop a Bayesian framework for tackling the supervised clustering problem, the generic problem encountered in tasks such as reference matching, coreference resolution, identity uncertainty and record linkage. Our clustering model is based on the Dirichlet process prior, which enables us to deﬁne distributions over the countably inﬁnite sets that naturally arise in this problem. We add supervision to our model by positing the existence of a set of unobserved random variables (we call these “reference types”) that are generic across all clusters. Inference in our framework, which requires integrating over inﬁnitely many parameters, is solved using Markov chain Monte Carlo techniques. We present algorithms for both conjugate and non-conjugate priors. We present a simple—but general—parameterization of our model based on a Gaussian assumption. We evaluate this model on one artiﬁcial task and three real-world tasks, comparing it against both unsupervised and state-of-the-art supervised algorithms. Our results show that our model is able to outperform other models across a variety of tasks and performance metrics. Keywords: supervised clustering, record linkage, citation matching, coreference, Dirichlet process, non-parametric Bayesian</p><p>2 0.34483388 <a title="2-lda-2" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>Author: Aharon Bar-Hillel, Tomer Hertz, Noam Shental, Daphna Weinshall</p><p>Abstract: Many learning algorithms use a metric deﬁned over the input space as a principal tool, and their performance critically depends on the quality of this metric. We address the problem of learning metrics using side-information in the form of equivalence constraints. Unlike labels, we demonstrate that this type of side-information can sometimes be automatically obtained without the need of human intervention. We show how such side-information can be used to modify the representation of the data, leading to improved clustering and classiﬁcation. Speciﬁcally, we present the Relevant Component Analysis (RCA) algorithm, which is a simple and efﬁcient algorithm for learning a Mahalanobis metric. We show that RCA is the solution of an interesting optimization problem, founded on an information theoretic basis. If dimensionality reduction is allowed within RCA, we show that it is optimally accomplished by a version of Fisher’s linear discriminant that uses constraints. Moreover, under certain Gaussian assumptions, RCA can be viewed as a Maximum Likelihood estimation of the within class covariance matrix. We conclude with extensive empirical evaluations of RCA, showing its advantage over alternative methods. Keywords: clustering, metric learning, dimensionality reduction, equivalence constraints, side information.</p><p>3 0.322171 <a title="2-lda-3" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>Author: John Winn, Christopher M. Bishop</p><p>Abstract: Bayesian inference is now widely established as one of the principal foundations for machine learning. In practice, exact inference is rarely possible, and so a variety of approximation techniques have been developed, one of the most widely used being a deterministic framework called variational inference. In this paper we introduce Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to Bayesian Networks. Like belief propagation, VMP proceeds by sending messages between nodes in the network and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing additional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational message passing has been implemented in the form of a general purpose inference engine called VIBES (‘Variational Inference for BayEsian networkS’) which allows models to be speciﬁed graphically and then solved variationally without recourse to coding. Keywords: Bayesian networks, variational inference, message passing</p><p>4 0.31263667 <a title="2-lda-4" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>Author: Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, Suvrit Sra</p><p>Abstract: Several large scale data mining applications, such as text categorization and gene expression analysis, involve high-dimensional data that is also inherently directional in nature. Often such data is L2 normalized so that it lies on the surface of a unit hypersphere. Popular models such as (mixtures of) multi-variate Gaussians are inadequate for characterizing such data. This paper proposes a generative mixture-model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. In particular, we derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the mean and concentration parameters of this mixture. Numerical estimation of the concentration parameters is non-trivial in high dimensions since it involves functional inversion of ratios of Bessel functions. We also formulate two clustering algorithms corresponding to the variants of EM that we derive. Our approach provides a theoretical basis for the use of cosine similarity that has been widely employed by the information retrieval community, and obtains the spherical kmeans algorithm (kmeans with cosine similarity) as a special case of both variants. Empirical results on clustering of high-dimensional text and gene-expression data based on a mixture of vMF distributions show that the ability to estimate the concentration parameter for each vMF component, which is not present in existing approaches, yields superior results, especially for difﬁcult clustering tasks in high-dimensional spaces. Keywords: clustering, directional distributions, mixtures, von Mises-Fisher, expectation maximization, maximum likelihood, high dimensional data c 2005 Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh and Suvrit Sra. BANERJEE , D HILLON , G HOSH AND S RA</p><p>5 0.29637569 <a title="2-lda-5" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>Author: Antoine Bordes, Seyda Ertekin, Jason Weston, Léon Bottou</p><p>Abstract: Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efﬁcient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We ﬁrst present an online SVM algorithm based on this premise. LASVM yields competitive misclassiﬁcation rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.</p><p>6 0.29488194 <a title="2-lda-6" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>7 0.29190889 <a title="2-lda-7" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>8 0.28922385 <a title="2-lda-8" href="./jmlr-2005-Active_Coevolutionary_Learning_of_Deterministic_Finite_Automata.html">8 jmlr-2005-Active Coevolutionary Learning of Deterministic Finite Automata</a></p>
<p>9 0.28921345 <a title="2-lda-9" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>10 0.28774542 <a title="2-lda-10" href="./jmlr-2005-Clustering_with_Bregman_Divergences.html">20 jmlr-2005-Clustering with Bregman Divergences</a></p>
<p>11 0.28574091 <a title="2-lda-11" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>12 0.28459767 <a title="2-lda-12" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>13 0.28380492 <a title="2-lda-13" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>14 0.28175175 <a title="2-lda-14" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<p>15 0.2794742 <a title="2-lda-15" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>16 0.27676696 <a title="2-lda-16" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>17 0.2711468 <a title="2-lda-17" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>18 0.26880074 <a title="2-lda-18" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>19 0.26875162 <a title="2-lda-19" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>20 0.26480362 <a title="2-lda-20" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
