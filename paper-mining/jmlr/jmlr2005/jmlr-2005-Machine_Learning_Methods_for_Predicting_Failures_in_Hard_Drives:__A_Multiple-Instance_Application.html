<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-53" href="#">jmlr2005-53</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</h1>
<br/><p>Source: <a title="jmlr-2005-53-pdf" href="http://jmlr.org/papers/volume6/murray05a/murray05a.pdf">pdf</a></p><p>Author: Joseph F. Murray, Gordon F. Hughes, Kenneth Kreutz-Delgado</p><p>Abstract: We compare machine learning methods applied to a difﬁcult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classiﬁer (mi-NB) which is speciﬁcally designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum signiﬁcance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures. Keywords: hard drive failure prediction, rank-sum test, support vector machines (SVM), exact nonparametric statistics, multiple instance naive-Bayes</p><p>Reference: <a title="jmlr-2005-53-reference" href="../jmlr2005_reference/jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 These normal approximations may be particularly inaccurate for rare event problems like hard drive failures. [sent-14, score-0.467]
</p><p>2 Keywords: hard drive failure prediction, rank-sum test, support vector machines (SVM), exact nonparametric statistics, multiple instance naive-Bayes  1. [sent-15, score-0.711]
</p><p>3 Modern hard drives are reliable devices, yet failures can be costly to users and many would beneﬁt from a warning of potential problems that would give them enough time to backup their data. [sent-17, score-0.699]
</p><p>4 M URRAY, H UGHES AND K REUTZ -D ELGADO  Hard drive manufacturers have been developing self-monitoring technology in their products since 1994, in an effort to predict failures early enough to allow users to backup their data (Hughes et al. [sent-22, score-0.595]
</p><p>5 This Self-Monitoring and Reporting Technology (SMART) system uses attributes collected during normal operation (and during off-line tests) to set a failure prediction ﬂag. [sent-24, score-0.539]
</p><p>6 Some of the attributes used to make the failure prediction include counts of track-seek retries, read errors, write faults, reallocated sectors, head ﬂy height too low or high, and high temperature. [sent-26, score-0.509]
</p><p>7 Most internally-monitored attributes are error count data, implying positive integer data values, and a pattern of increasing attribute values (or their rates of change) over time is indicative of impending failure. [sent-27, score-0.483]
</p><p>8 Each manufacturer develops and uses its own set of attributes and algorithm for failure prediction. [sent-28, score-0.497]
</p><p>9 Every time a failure warning is triggered the drive can be returned to the factory for warranty replacement, so manufacturers are very concerned with reducing the false alarm rates of their algorithms. [sent-29, score-1.05]
</p><p>10 These thresholds are set conservatively to avoid false alarms at the expense of predictive accuracy, with an acceptable false alarm rate on the order of 0. [sent-31, score-0.73]
</p><p>11 Our previous work has shown that by using nonparametric statistical tests, the accuracy of correctly detected failures can be improved to as much as 40-60% while maintaining acceptably low false alarm rates (Hughes et al. [sent-34, score-0.61]
</p><p>12 First, we cast the hard drive failure prediction problem as a multiple-instance (MI) learning problem (Dietterich et al. [sent-37, score-0.687]
</p><p>13 We show that the rank-sum test provides good performance in terms of achieving a high failure detection rate with low false alarms at a low computational cost. [sent-41, score-0.759]
</p><p>14 This observation led us to investigate nonparametric statistical tests for comparing the distribution of a test drive attribute to the known distribution of good drives. [sent-55, score-0.654]
</p><p>15 The single-variate test was combined for multiple attributes using a logical OR operation, that is, if any of the single attribute tests indicated that the drive was not from the good population, then the drive was labeled failed. [sent-58, score-1.215]
</p><p>16 Data Description The data set consists of time series of SMART attributes from a single drive model, and is a different data set than that used in Hughes et al. [sent-74, score-0.594]
</p><p>17 1 Data from 369 drives were collected, and each drive was labeled good or failed, with 178 drives in the good class and 191 drives in the failed class. [sent-76, score-1.932]
</p><p>18 It should be noted that since the good drive data were collected in a controlled uniform environment and the failed data come from drives that were operated by users, it is reasonable to expect that there will be differences between the two populations due to the different 1. [sent-79, score-1.06]
</p><p>19 The ﬁrst sample available in the data set for this drive is from Hours = 1927, as only the most recent 300 samples are stored in drives of this model. [sent-103, score-0.832]
</p><p>20 Algorithms that attempt to learn the difference between the good and failed populations may in fact be learning this difference and not the desired difference between good and nearly-failing drive samples. [sent-105, score-0.639]
</p><p>21 A sample is all the attributes for a single drive for a single time interval. [sent-107, score-0.594]
</p><p>22 The number of available valid samples for each drive i is denoted Ni , and Ni may be less than 300 for those drives that did not survive 600 hours of operation. [sent-109, score-0.863]
</p><p>23 Not all attributes are monitored in every drive, and the unmonitored attributes are set to a constant, non-informative value. [sent-111, score-0.466]
</p><p>24 For drives labeled good, Yi = 0 and for failed drives Yi = 1. [sent-121, score-1.15]
</p><p>25 Also, all good and failed drive data were collected during a single reliability test (whereas in the current set, the failed drives were returns from the ﬁeld). [sent-128, score-1.392]
</p><p>26 A preliminary examination of the current set of SMART data was done by plotting the histograms of attributes from good and failed drives. [sent-129, score-0.511]
</p><p>27 As will be demonstrated below, some attributes are not strongly correlated with future drive failure and including these attributes can have a negative impact on classiﬁer performance. [sent-138, score-1.064]
</p><p>28 If an attribute appeared promising with either method it was considered for use in the failure detection algorithms (see Section 4). [sent-140, score-0.474]
</p><p>29 1 Reverse Arrangements Test The reverse arrangements test is a nonparametric test for trend which is applied to each attribute in the data set (Mann, 1945; Bendat and Piersol, 2000). [sent-142, score-0.513]
</p><p>30 5  2 4  x 10  Servo10 −− Good  Failed  1000  1000  500  500  0  0  1  2  3  4  0  5 4  0  1  2  3  4 6  x 10  x 10  Figure 2: Histograms of representative attributes from good and failed drives, illustrating the nonparametric nature of many of the attributes. [sent-163, score-0.574]
</p><p>31 It is calculated over all samples, m f − mg , z= σ2 f nf  σ2  g + ng  where m f and σ2 are the mean and variance of the attribute in failed drives, mg and σ2 are the mean g f and variance in good drives, n f and ng are the total number of samples of failed and good drives. [sent-187, score-0.73]
</p><p>32 Large positive z-scores indicate the attribute is higher in the population of failed drive samples, and that there is likely a signiﬁcant difference in the means between good and failed samples. [sent-188, score-1.065]
</p><p>33 The percentage of drives for which the null hypothesis of no trend is rejected is calculated for good and failed drives. [sent-195, score-0.77]
</p><p>34 3 lists attributes and the percent of drives that have signiﬁcant trends for the good and failed populations. [sent-197, score-0.968]
</p><p>35 This is in contrast to the way a failure prediction algorithm must work, which must test each of many (usually N) consecutive series of samples, and if any fail, then the drive is predicted to fail (see Section 4. [sent-203, score-0.719]
</p><p>36 Spin-ups is the number of times the drive motors start the platters spinning, which happens every time the drive is turned on, or when it reawakens from a low-power state. [sent-207, score-0.722]
</p><p>37 It is expected that most drives will be turned on and off repeatedly, so it is unsurprising that both good and failed drives show increasing trends in Table 1. [sent-208, score-1.156]
</p><p>38 Attributes near the top are initially more interesting because of more signiﬁcant differences in the means, that is, the mean value of an attribute (over all samples) for failed drives was higher than for good drives. [sent-212, score-0.823]
</p><p>39 From the results of the reverse arrangements and z-score tests, a set of 25 attributes2 was selected by hand from those attributes which appear to be promising due to increasing attribute trends in failed drives and large z-score values. [sent-215, score-1.269]
</p><p>40 Attributes that provided good failure detection with low false alarms in the classiﬁers were then used together (see Section 5). [sent-219, score-0.705]
</p><p>41 We note that the feature selection process is not a black-box automatic method, and required trial-and-error testing of attributes and combinations of attributes in the classiﬁers. [sent-220, score-0.466]
</p><p>42 Also excluded are other attributes that appear to be measured improperly for certain drives are FlyHeight13-16, Temp5, and Temp6. [sent-227, score-0.654]
</p><p>43 3%  Table 1: Percent of drives with signiﬁcant trends by the reverse arrangements test for selected attributes, which indicates potentially useful attributes. [sent-266, score-0.688]
</p><p>44 Note that this test is performed only on the last n = 100 samples of each drive, while a true failure prediction algorithm must test each pattern of n samples taken throughout the drives’ history. [sent-267, score-0.529]
</p><p>45 CSS are cumulative and are reported over the life of the drive, so it is unsurprising that most good and failed drives show increasing trends (which simply indicate that the drive has been turned on and off). [sent-269, score-1.096]
</p><p>46 For the hard-drive failure problem, the number of drives is limited, and the variance of the classiﬁcation error (see Section 5) is already quite high. [sent-277, score-0.658]
</p><p>47 The best solution is to collect more data from drives to validate the false alarm and detection rates, which a drive manufacturer would do in any case to test the method and set the operating curve level before actual implementation of improved SMART algorithms in drives. [sent-281, score-1.351]
</p><p>48 A vector x of n consecutive samples (out of the N total samples from each drive) of each selected attribute is used to make the classiﬁcation, and every vector of n consecutive samples in the history of the 792  M ETHODS FOR P REDICTING FAILURES IN H ARD D RIVES  drive is used (see Figure 1). [sent-309, score-0.717]
</p><p>49 2 The Multiple-Instance Framework The hard drive failure prediction problem can be cast as a multiple-instance learning problem, which is a two-class semi-supervised problem. [sent-337, score-0.687]
</p><p>50 Each pattern x (composed of n samples) is an instance, and the set of all patterns for a drive i is the bag Xi . [sent-345, score-0.583]
</p><p>51 The terms bag label and drive label are interchangeable, with failed drives labeled Yi = 1 and good drives labeled Yi = 0. [sent-346, score-1.658]
</p><p>52 Algorithms that violate the MI-assumption include Citation-k-Nearest-Neighbors (Wang and Zucker, 2000), SVMs with polynomial minimax kernel, and the statistical and wrapper methods of Xu (2003), and these will not be considered further for hard drive failure prediction. [sent-362, score-0.648]
</p><p>53 The mi-NB algorithm begins by assigning a label y j to each pattern: for good drives, all patterns are assigned y j = 0; for failed drives, all patterns except for the last one in the time series are assigned y j = 0, with the last one assigned to the failed class, yNi = 1. [sent-369, score-0.748]
</p><p>54 Bag 3: One instance of the failed drive is classiﬁed as -, but another is classiﬁed as +, so the bag is correctly classiﬁed (failed). [sent-378, score-0.756]
</p><p>55 Algorithm 1 mi-NB Train (for SMART failure prediction) Input: x, Y , FAdesired (desired false alarm rate) Initialize: Good drives: For drives with Yi = 0 initialize y j = 0 for j = 1 . [sent-393, score-1.033]
</p><p>56 Next, we show that the mi-NB algorithm has non-decreasing detection and false alarm rates over the iterations. [sent-414, score-0.511]
</p><p>57 Lemma 1 At each iteration t, the mi-NB algorithm does not decrease the detection and false alarm rates (as measured on the training set) over the previous iteration t − 1, (t−1)  (x j ) ≤ f1 (x j )  (t−1)  (x j ) ≥ f0 (x j )  f1  f0  (t)  (t)  ∀ j = 1. [sent-415, score-0.511]
</p><p>58 The initial conditions of the algorithm ensure a low false alarm rate, and the algorithm proceeds (in a greedy fashion) to pick patterns that are mostly likely representatives of the failed class without re-evaluating previous choices. [sent-427, score-0.737]
</p><p>59 To apply the SVM to the SMART data set, drives are randomly assigned into training and test sets for a single trial. [sent-438, score-0.499]
</p><p>60 For validation, means and standard deviations of detection and false alarm rates are found over 10 trials, each with different training and test sets. [sent-439, score-0.565]
</p><p>61 Each pattern is assigned to the same label as the drive (all patterns in a failed drive Y = 1 are assigned to the failed class, yi = +1, and all patterns in good drives Y = 0 are set to yi = −1). [sent-440, score-1.912]
</p><p>62 A failure prediction warning is triggered for a drive if the probability of any of its samples is below a threshold (which is a parameter of the algorithm). [sent-469, score-0.711]
</p><p>63 The Autoclass threshold parameter was varied to adjust tradeoff between detection and false alarm rates. [sent-471, score-0.488]
</p><p>64 One set T comes from the drive under test and the other R is a reference set composed of samples from good drives. [sent-475, score-0.491]
</p><p>65 The test set T (size n = 15 for most experiments) is chosen from consecutive samples of the drive under test. [sent-491, score-0.493]
</p><p>66 If the test set for any attribute over the history of the drive is found to be signiﬁcantly different from the reference set R then the drive is predicted to fail. [sent-492, score-0.952]
</p><p>67 The use of the OR test is motivated by the fact that very different signiﬁcance level ranges (per-pattern) for each attribute were needed to achieve low false alarm rates (per-drive). [sent-501, score-0.576]
</p><p>68 7 Reverse Arrangements Tests The reverse arrangements test described above for feature selection can also be used for failure prediction. [sent-503, score-0.468]
</p><p>69 For each drive, if any test of any attribute shows a signiﬁcant trend, then the drive is predicted to fail. [sent-506, score-0.539]
</p><p>70 As with the rank-sum test, the signiﬁcance level α controls the tradeoff between detection and false alarm rates. [sent-507, score-0.488]
</p><p>71 We also can clearly see that some methods are signiﬁcantly better than the current industry-used SMART thresholds implemented in hard drives (which provide only an estimated 3-10% detection rate with 0. [sent-511, score-0.584]
</p><p>72 One sample per pattern was used, and all patterns in the history of each test drive were tested. [sent-516, score-0.546]
</p><p>73 ) The detection and false alarm rates were measured per drive: if any pattern in the drive’s history was classiﬁed as failed, the drive was classiﬁed as failed. [sent-518, score-0.943]
</p><p>74 For mi-NB, even at the initial condition (which includes only the last sample from each failed drive in the failed class) there is a relatively high false alarm rate of 1. [sent-539, score-1.292]
</p><p>75 80 SVM, default scaling SVM, EW bins SVM, EF bins  70  Detection (%)  60 50 40 30 20 10 0  0  5  10  15  False alarms (%)  Figure 5: Comparison of preprocessing with the SVM using 25 attributes (one sample per pattern). [sent-558, score-0.55]
</p><p>76 Figure 7 shows a histogram of the time before actual failure that the drives are correctly predicted as failing, plotted for SVM at the point 50. [sent-561, score-0.658]
</p><p>77 A substantial number of failures were detected over 100 hours before failure, which is one of the motivations for initially labeling all patterns from failed drives as being examples of the failed class (remembering that our data only includes the last 600 hours of SMART samples from each drive). [sent-565, score-1.298]
</p><p>78 2 Single-attribute Experiments In an effort to understand which attributes are most useful in predicting imminent hard-drive failure, we tested the attributes individually using the non-parametric statistical methods (rank-sum and reverse arrangements). [sent-567, score-0.57]
</p><p>79 1% of failed drives and 0 good drives showing signiﬁcant increasing trends. [sent-571, score-1.12]
</p><p>80 Figure 8 shows the failure prediction results using only the ReadError18 attribute with the rank-sum, reverse arrangements, and SVM classiﬁers. [sent-572, score-0.475]
</p><p>81 3% detection with false alarms too low to measure, and 33. [sent-575, score-0.468]
</p><p>82 Of these 25, only 8 attributes (Figure 9) were able to detect failures at sufﬁciently low false alarm rates: ReadError1, ReadError2, ReadError3, ReadError18, ReadError19, Servo7, GList3 and Servo10. [sent-581, score-0.757]
</p><p>83 We also note that the number of good drives is relatively small (178) and with up to 40% of these used in the training set, measuring low false alarm rates is imprecise. [sent-599, score-0.819]
</p><p>84 200  # of drives  150  100  50  0  0  100  200 300 400 Hours before failure  500  600  Figure 7: Histogram of time (hours) before failure that a correct failure prediction was made. [sent-606, score-1.171]
</p><p>85 rates of < 1%, this means that some of the trials had no false alarm drives while other trials had very few (1 or 2). [sent-619, score-0.819]
</p><p>86 Because some drives are inherently more likely to be predicted as false alarms, whether these drives are included in the test or training sets can lead to a variance from trial to trial, causing large error bars at some of the points. [sent-620, score-1.119]
</p><p>87 The SMART failure prediction algorithms (as currently implemented in hard-drives) run on the internal CPU’s of the drive and have rather limited memory and processing to devote to SMART. [sent-630, score-0.637]
</p><p>88 At higher false alarm rates, the rank-sum detection rate is 52. [sent-636, score-0.488]
</p><p>89 7% false alarms, which means (due to the small number of good drives) that only 1 drive at most triggered a false alarm in the test set. [sent-638, score-0.989]
</p><p>90 A larger sample of good drives would be desirable for a more accurate measure of the false alarm rate. [sent-639, score-0.796]
</p><p>91 In this case the four attributes were selected because of good performance in the rank-sum test, and so of course it is not an entirely fair comparison but in some situations the only attributes available may be those that favor rank-sum. [sent-653, score-0.466]
</p><p>92 From a drive reliability perspective, the rank-sum test indicates that attributes that measure read errors (in this case, ReadError1, ReadError3, ReadError18 and ReadError19) were the most useful in predicting 807  M URRAY, H UGHES AND K REUTZ -D ELGADO  Figure 4 Point 1 2 3  Detection 50. [sent-654, score-0.677]
</p><p>93 This semi-supervised approach can be contrasted with the unsupervised Autoclass and the fully supervised SVM, where all patterns from failed drives were labeled failed. [sent-790, score-0.812]
</p><p>94 The reverse-arrangements test performed more poorly than expected, as we believed that the assumption of increasing trend made by this test was well suited for hard drive attributes (like read-error counts) that would presumably increase before a failure. [sent-791, score-0.793]
</p><p>95 There are physical reasons in drive technology why impending failure need not be associated with an increasing trend in error counts. [sent-794, score-0.675]
</p><p>96 The simplest example is sudden stress from a failing drive component which causes a sudden increase in errors, followed by drive failure. [sent-795, score-0.722]
</p><p>97 Conclusions We have shown that both nonparametric statistical tests and machine learning methods can signiﬁcantly improve over the performance of the hard drive failure-prediction algorithms which are currently implemented. [sent-805, score-0.526]
</p><p>98 Attributes useful for failure prediction were selected by using z-scores and the reverse arrangements test for increasing trend. [sent-821, score-0.507]
</p><p>99 Improving the performance of hard drive failure prediction will have many practical beneﬁts. [sent-822, score-0.687]
</p><p>100 While we believe the algorithms presented here are of high enough quality (relative to the current commercially-used algorithms) to be implemented in drives, it is still important to test them on larger number of drives (on the order of thousands) to measure accuracy to the desired precision of 0. [sent-826, score-0.475]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('drives', 0.421), ('drive', 0.361), ('failed', 0.278), ('failure', 0.237), ('attributes', 0.233), ('false', 0.199), ('alarm', 0.176), ('smart', 0.17), ('alarms', 0.156), ('failures', 0.149), ('binning', 0.126), ('attribute', 0.124), ('elgado', 0.122), ('reutz', 0.122), ('ughes', 0.122), ('urray', 0.122), ('bag', 0.117), ('autoclass', 0.117), ('redicting', 0.115), ('rives', 0.115), ('detection', 0.113), ('arrangements', 0.102), ('ard', 0.078), ('reverse', 0.075), ('ties', 0.073), ('hughes', 0.072), ('svm', 0.069), ('ethods', 0.067), ('wx', 0.064), ('radial', 0.063), ('nonparametric', 0.063), ('patterns', 0.06), ('ranks', 0.059), ('bins', 0.056), ('test', 0.054), ('tied', 0.054), ('wilcoxon', 0.053), ('tests', 0.052), ('cance', 0.052), ('naive', 0.051), ('hard', 0.05), ('samples', 0.05), ('zk', 0.049), ('hamerly', 0.048), ('statistic', 0.047), ('pattern', 0.045), ('mann', 0.043), ('ranksum', 0.043), ('trend', 0.041), ('nb', 0.04), ('prediction', 0.039), ('elkan', 0.037), ('ws', 0.036), ('abrera', 0.036), ('andrews', 0.036), ('impending', 0.036), ('mysvm', 0.036), ('wy', 0.036), ('trends', 0.036), ('minutes', 0.033), ('guration', 0.033), ('xm', 0.033), ('murray', 0.032), ('lehmann', 0.032), ('hours', 0.031), ('users', 0.031), ('mi', 0.03), ('labeled', 0.03), ('null', 0.03), ('manufacturers', 0.03), ('bayes', 0.03), ('normal', 0.03), ('bin', 0.029), ('predicting', 0.029), ('fc', 0.029), ('molecule', 0.029), ('ucsd', 0.029), ('consecutive', 0.028), ('pt', 0.028), ('recommendations', 0.027), ('manufacturer', 0.027), ('rare', 0.026), ('default', 0.026), ('history', 0.026), ('reference', 0.026), ('roc', 0.025), ('likely', 0.024), ('assigned', 0.024), ('dougherty', 0.024), ('backup', 0.024), ('kenneth', 0.024), ('warning', 0.024), ('ers', 0.023), ('rates', 0.023), ('classi', 0.023), ('preprocessing', 0.023), ('unsupervised', 0.023), ('er', 0.022), ('count', 0.022), ('bendat', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000014 <a title="53-tfidf-1" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>Author: Joseph F. Murray, Gordon F. Hughes, Kenneth Kreutz-Delgado</p><p>Abstract: We compare machine learning methods applied to a difﬁcult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classiﬁer (mi-NB) which is speciﬁcally designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum signiﬁcance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures. Keywords: hard drive failure prediction, rank-sum test, support vector machines (SVM), exact nonparametric statistics, multiple instance naive-Bayes</p><p>2 0.095606886 <a title="53-tfidf-2" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<p>Author: Weng-Keen Wong, Andrew Moore, Gregory Cooper, Michael Wagner</p><p>Abstract: Traditional biosurveillance algorithms detect disease outbreaks by looking for peaks in a univariate time series of health-care data. Current health-care surveillance data, however, are no longer simply univariate data streams. Instead, a wealth of spatial, temporal, demographic and symptomatic information is available. We present an early disease outbreak detection algorithm called What’s Strange About Recent Events (WSARE), which uses a multivariate approach to improve its timeliness of detection. WSARE employs a rule-based technique that compares recent health-care data against data from a baseline distribution and ﬁnds subgroups of the recent data whose proportions have changed the most from the baseline data. In addition, health-care data also pose difﬁculties for surveillance algorithms because of inherent temporal trends such as seasonal effects and day of week variations. WSARE approaches this problem using a Bayesian network to produce a baseline distribution that accounts for these temporal trends. The algorithm itself incorporates a wide range of ideas, including association rules, Bayesian networks, hypothesis testing and permutation tests to produce a detection algorithm that is careful to evaluate the signiﬁcance of the alarms that it raises. Keywords: anomaly detection, syndromic surveillance, biosurveillance, Bayesian networks, applications</p><p>3 0.066645555 <a title="53-tfidf-3" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of ﬁnding level sets for the data generating density. We interpret this learning problem as a binary classiﬁcation problem and compare the corresponding classiﬁcation risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classiﬁcation risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justiﬁcation for the well-known heuristic of artiﬁcially sampling “labeled” samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM. Keywords: unsupervised learning, anomaly detection, density levels, classiﬁcation, SVMs</p><p>4 0.056026924 <a title="53-tfidf-4" href="./jmlr-2005-A_Bayes_Optimal_Approach_for_Partitioning_the_Values_of_Categorical_Attributes.html">1 jmlr-2005-A Bayes Optimal Approach for Partitioning the Values of Categorical Attributes</a></p>
<p>Author: Marc Boullé</p><p>Abstract: In supervised machine learning, the partitioning of the values (also called grouping) of a categorical attribute aims at constructing a new synthetic attribute which keeps the information of the initial attribute and reduces the number of its values. In this paper, we propose a new grouping method MODL1 founded on a Bayesian approach. The method relies on a model space of grouping models and on a prior distribution defined on this model space. This results in an evaluation criterion of grouping, which is minimal for the most probable grouping given the data, i.e. the Bayes optimal grouping. We propose new super-linear optimization heuristics that yields near-optimal groupings. Extensive comparative experiments demonstrate that the MODL grouping method builds high quality groupings in terms of predictive quality, robustness and small number of groups. Keywords: data preparation, grouping, Bayesianism, model selection, classification, naïve Bayes 1</p><p>5 0.051785372 <a title="53-tfidf-5" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>Author: Wei Chu, Zoubin Ghahramani</p><p>Abstract: We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative ﬁltering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach. Keywords: Gaussian processes, ordinal regression, approximate Bayesian inference, collaborative ﬁltering, gene expression analysis, feature selection</p><p>6 0.043441042 <a title="53-tfidf-6" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>7 0.041901611 <a title="53-tfidf-7" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>8 0.040859181 <a title="53-tfidf-8" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>9 0.03929954 <a title="53-tfidf-9" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>10 0.036852695 <a title="53-tfidf-10" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>11 0.03563961 <a title="53-tfidf-11" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>12 0.035248324 <a title="53-tfidf-12" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>13 0.032761239 <a title="53-tfidf-13" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>14 0.03156814 <a title="53-tfidf-14" href="./jmlr-2005-Generalization_Bounds_for_the_Area_Under_the_ROC_Curve.html">38 jmlr-2005-Generalization Bounds for the Area Under the ROC Curve</a></p>
<p>15 0.030229479 <a title="53-tfidf-15" href="./jmlr-2005-An_MDP-Based_Recommender_System.html">12 jmlr-2005-An MDP-Based Recommender System</a></p>
<p>16 0.029588625 <a title="53-tfidf-16" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>17 0.029231265 <a title="53-tfidf-17" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>18 0.029138014 <a title="53-tfidf-18" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>19 0.02900758 <a title="53-tfidf-19" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>20 0.028136328 <a title="53-tfidf-20" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.167), (1, 0.021), (2, 0.096), (3, -0.106), (4, 0.142), (5, -0.071), (6, 0.015), (7, -0.194), (8, -0.018), (9, -0.198), (10, 0.06), (11, -0.045), (12, -0.046), (13, -0.475), (14, -0.105), (15, -0.026), (16, 0.064), (17, 0.024), (18, -0.089), (19, -0.012), (20, 0.033), (21, -0.002), (22, -0.035), (23, -0.047), (24, 0.077), (25, 0.041), (26, 0.015), (27, -0.085), (28, -0.019), (29, 0.041), (30, -0.026), (31, 0.008), (32, 0.02), (33, 0.019), (34, 0.034), (35, 0.07), (36, 0.031), (37, 0.023), (38, 0.073), (39, -0.047), (40, -0.108), (41, 0.104), (42, 0.074), (43, -0.066), (44, 0.031), (45, 0.53), (46, 0.018), (47, -0.165), (48, -0.187), (49, 0.141)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96985954 <a title="53-lsi-1" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>Author: Joseph F. Murray, Gordon F. Hughes, Kenneth Kreutz-Delgado</p><p>Abstract: We compare machine learning methods applied to a difﬁcult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classiﬁer (mi-NB) which is speciﬁcally designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum signiﬁcance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures. Keywords: hard drive failure prediction, rank-sum test, support vector machines (SVM), exact nonparametric statistics, multiple instance naive-Bayes</p><p>2 0.3307848 <a title="53-lsi-2" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<p>Author: Weng-Keen Wong, Andrew Moore, Gregory Cooper, Michael Wagner</p><p>Abstract: Traditional biosurveillance algorithms detect disease outbreaks by looking for peaks in a univariate time series of health-care data. Current health-care surveillance data, however, are no longer simply univariate data streams. Instead, a wealth of spatial, temporal, demographic and symptomatic information is available. We present an early disease outbreak detection algorithm called What’s Strange About Recent Events (WSARE), which uses a multivariate approach to improve its timeliness of detection. WSARE employs a rule-based technique that compares recent health-care data against data from a baseline distribution and ﬁnds subgroups of the recent data whose proportions have changed the most from the baseline data. In addition, health-care data also pose difﬁculties for surveillance algorithms because of inherent temporal trends such as seasonal effects and day of week variations. WSARE approaches this problem using a Bayesian network to produce a baseline distribution that accounts for these temporal trends. The algorithm itself incorporates a wide range of ideas, including association rules, Bayesian networks, hypothesis testing and permutation tests to produce a detection algorithm that is careful to evaluate the signiﬁcance of the alarms that it raises. Keywords: anomaly detection, syndromic surveillance, biosurveillance, Bayesian networks, applications</p><p>3 0.23067121 <a title="53-lsi-3" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>Author: Wei Chu, Zoubin Ghahramani</p><p>Abstract: We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative ﬁltering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach. Keywords: Gaussian processes, ordinal regression, approximate Bayesian inference, collaborative ﬁltering, gene expression analysis, feature selection</p><p>4 0.21178056 <a title="53-lsi-4" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of ﬁnding level sets for the data generating density. We interpret this learning problem as a binary classiﬁcation problem and compare the corresponding classiﬁcation risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classiﬁcation risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justiﬁcation for the well-known heuristic of artiﬁcially sampling “labeled” samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM. Keywords: unsupervised learning, anomaly detection, density levels, classiﬁcation, SVMs</p><p>5 0.20008048 <a title="53-lsi-5" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>Author: Theodoros Evgeniou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we deﬁne is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Speciﬁc kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can signiﬁcantly outperform standard single-task learning particularly when there are many related tasks but few data per task. Keywords: multi-task learning, kernels, vector-valued functions, regularization, learning algorithms</p><p>6 0.19150738 <a title="53-lsi-6" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>7 0.17612229 <a title="53-lsi-7" href="./jmlr-2005-A_Bayes_Optimal_Approach_for_Partitioning_the_Values_of_Categorical_Attributes.html">1 jmlr-2005-A Bayes Optimal Approach for Partitioning the Values of Categorical Attributes</a></p>
<p>8 0.17193864 <a title="53-lsi-8" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>9 0.15843537 <a title="53-lsi-9" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>10 0.13411126 <a title="53-lsi-10" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>11 0.12922026 <a title="53-lsi-11" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>12 0.11807058 <a title="53-lsi-12" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>13 0.11770313 <a title="53-lsi-13" href="./jmlr-2005-Generalization_Bounds_for_the_Area_Under_the_ROC_Curve.html">38 jmlr-2005-Generalization Bounds for the Area Under the ROC Curve</a></p>
<p>14 0.11735131 <a title="53-lsi-14" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>15 0.1139486 <a title="53-lsi-15" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>16 0.10695549 <a title="53-lsi-16" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>17 0.10504721 <a title="53-lsi-17" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>18 0.10390241 <a title="53-lsi-18" href="./jmlr-2005-Multiclass_Boosting_for_Weak_Classifiers.html">57 jmlr-2005-Multiclass Boosting for Weak Classifiers</a></p>
<p>19 0.10166433 <a title="53-lsi-19" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>20 0.1000488 <a title="53-lsi-20" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.019), (17, 0.544), (19, 0.012), (36, 0.019), (37, 0.042), (42, 0.012), (43, 0.032), (47, 0.019), (52, 0.08), (59, 0.022), (70, 0.03), (88, 0.057), (90, 0.011), (94, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87793416 <a title="53-lda-1" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>Author: Joseph F. Murray, Gordon F. Hughes, Kenneth Kreutz-Delgado</p><p>Abstract: We compare machine learning methods applied to a difﬁcult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classiﬁer (mi-NB) which is speciﬁcally designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum signiﬁcance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures. Keywords: hard drive failure prediction, rank-sum test, support vector machines (SVM), exact nonparametric statistics, multiple instance naive-Bayes</p><p>2 0.84261811 <a title="53-lda-2" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>Author: Dmitry Rusakov, Dan Geiger</p><p>Abstract: We develop a closed form asymptotic formula to compute the marginal likelihood of data given a naive Bayesian network model with two hidden states and binary features. This formula deviates from the standard BIC score. Our work provides a concrete example that the BIC score is generally incorrect for statistical models that belong to stratiﬁed exponential families. This claim stands in contrast to linear and curved exponential families, where the BIC score has been proven to provide a correct asymptotic approximation for the marginal likelihood. Keywords: Bayesian networks, asymptotic model selection, Bayesian information criterion (BIC)</p><p>3 0.38702625 <a title="53-lda-3" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<p>Author: Weng-Keen Wong, Andrew Moore, Gregory Cooper, Michael Wagner</p><p>Abstract: Traditional biosurveillance algorithms detect disease outbreaks by looking for peaks in a univariate time series of health-care data. Current health-care surveillance data, however, are no longer simply univariate data streams. Instead, a wealth of spatial, temporal, demographic and symptomatic information is available. We present an early disease outbreak detection algorithm called What’s Strange About Recent Events (WSARE), which uses a multivariate approach to improve its timeliness of detection. WSARE employs a rule-based technique that compares recent health-care data against data from a baseline distribution and ﬁnds subgroups of the recent data whose proportions have changed the most from the baseline data. In addition, health-care data also pose difﬁculties for surveillance algorithms because of inherent temporal trends such as seasonal effects and day of week variations. WSARE approaches this problem using a Bayesian network to produce a baseline distribution that accounts for these temporal trends. The algorithm itself incorporates a wide range of ideas, including association rules, Bayesian networks, hypothesis testing and permutation tests to produce a detection algorithm that is careful to evaluate the signiﬁcance of the alarms that it raises. Keywords: anomaly detection, syndromic surveillance, biosurveillance, Bayesian networks, applications</p><p>4 0.33372763 <a title="53-lda-4" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>Author: Eran Segal, Dana Pe'er, Aviv Regev, Daphne Koller, Nir Friedman</p><p>Abstract: Methods for learning Bayesian networks can discover dependency structure between observed variables. Although these methods are useful in many applications, they run into computational and statistical problems in domains that involve a large number of variables. In this paper,1 we consider a solution that is applicable when many variables have similar behavior. We introduce a new class of models, module networks, that explicitly partition the variables into modules, so that the variables in each module share the same parents in the network and the same conditional probability distribution. We deﬁne the semantics of module networks, and describe an algorithm that learns the modules’ composition and their dependency structure from data. Evaluation on real data in the domains of gene expression and the stock market shows that module networks generalize better than Bayesian networks, and that the learned module network structure reveals regularities that are obscured in learned Bayesian networks. 1. A preliminary version of this paper appeared in the Proceedings of the Nineteenth Conference on Uncertainty in Artiﬁcial Intelligence, 2003 (UAI ’03). c 2005 Eran Segal, Dana Pe’er, Aviv Regev, Daphne Koller and Nir Friedman. S EGAL , P E ’ ER , R EGEV, KOLLER AND F RIEDMAN</p><p>5 0.33254802 <a title="53-lda-5" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>Author: Gal Chechik, Amir Globerson, Naftali Tishby, Yair Weiss</p><p>Abstract: The problem of extracting the relevant aspects of data was previously addressed through the information bottleneck (IB) method, through (soft) clustering one variable while preserving information about another - relevance - variable. The current work extends these ideas to obtain continuous representations that preserve relevant information, rather than discrete clusters, for the special case of multivariate Gaussian variables. While the general continuous IB problem is difﬁcult to solve, we provide an analytic solution for the optimal representation and tradeoff between compression and relevance for the this important case. The obtained optimal representation is a noisy linear projection to eigenvectors of the normalized regression matrix Σx|y Σ−1 , which is also the basis obtained x in canonical correlation analysis. However, in Gaussian IB, the compression tradeoff parameter uniquely determines the dimension, as well as the scale of each eigenvector, through a cascade of structural phase transitions. This introduces a novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level. Our analysis also provides a complete analytic expression of the preserved information as a function of the compression (the “information-curve”), in terms of the eigenvalue spectrum of the data. As in the discrete case, the information curve is concave and smooth, though it is made of different analytic segments for each optimal dimension. Finally, we show how the algorithmic theory developed in the IB framework provides an iterative algorithm for obtaining the optimal Gaussian projections. Keywords: information bottleneck, Gaussian processes, dimensionality reduction, canonical correlation analysis</p><p>6 0.32775313 <a title="53-lda-6" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>7 0.32306564 <a title="53-lda-7" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>8 0.30868819 <a title="53-lda-8" href="./jmlr-2005-Clustering_with_Bregman_Divergences.html">20 jmlr-2005-Clustering with Bregman Divergences</a></p>
<p>9 0.30633458 <a title="53-lda-9" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>10 0.30396044 <a title="53-lda-10" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>11 0.30142197 <a title="53-lda-11" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>12 0.29762468 <a title="53-lda-12" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>13 0.29656908 <a title="53-lda-13" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>14 0.29377794 <a title="53-lda-14" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>15 0.29103252 <a title="53-lda-15" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>16 0.29099602 <a title="53-lda-16" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>17 0.28468373 <a title="53-lda-17" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>18 0.28372151 <a title="53-lda-18" href="./jmlr-2005-Learning_the_Kernel_Function_via_Regularization.html">48 jmlr-2005-Learning the Kernel Function via Regularization</a></p>
<p>19 0.27857301 <a title="53-lda-19" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<p>20 0.27846298 <a title="53-lda-20" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
