<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-18" href="#">jmlr2005-18</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</h1>
<br/><p>Source: <a title="jmlr-2005-18-pdf" href="http://jmlr.org/papers/volume6/ye05a/ye05a.pdf">pdf</a></p><p>Author: Jieping Ye</p><p>Abstract: A generalized discriminant analysis based on a new optimization criterion is presented. The criterion extends the optimization criteria of the classical Linear Discriminant Analysis (LDA) when the scatter matrices are singular. An efﬁcient algorithm for the new optimization problem is presented. The solutions to the proposed criterion form a family of algorithms for generalized LDA, which can be characterized in a closed form. We study two speciﬁc algorithms, namely Uncorrelated LDA (ULDA) and Orthogonal LDA (OLDA). ULDA was previously proposed for feature extraction and dimension reduction, whereas OLDA is a novel algorithm proposed in this paper. The features in the reduced space of ULDA are uncorrelated, while the discriminant vectors of OLDA are orthogonal to each other. We have conducted a comparative study on a variety of real-world data sets to evaluate ULDA and OLDA in terms of classiﬁcation accuracy. Keywords: dimension reduction, linear discriminant analysis, uncorrelated LDA, orthogonal LDA, singular value decomposition</p><p>Reference: <a title="jmlr-2005-18-reference" href="../jmlr2005_reference/jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Department of Computer Science University of Minnesota Minneapolis, MN 55455, USA  Editor: Bin Yu  Abstract A generalized discriminant analysis based on a new optimization criterion is presented. [sent-3, score-0.257]
</p><p>2 The criterion extends the optimization criteria of the classical Linear Discriminant Analysis (LDA) when the scatter matrices are singular. [sent-4, score-0.334]
</p><p>3 The features in the reduced space of ULDA are uncorrelated, while the discriminant vectors of OLDA are orthogonal to each other. [sent-9, score-0.268]
</p><p>4 Keywords: dimension reduction, linear discriminant analysis, uncorrelated LDA, orthogonal LDA, singular value decomposition  1. [sent-11, score-0.417]
</p><p>5 We consider dimension reduction of high-dimensional, undersampled data, where the data dimension is much larger than the sample size. [sent-13, score-0.15]
</p><p>6 The high-dimensional, undersampled problems frequently occur in many applications including information retrieval (Berry et al. [sent-14, score-0.086]
</p><p>7 Linear Discriminant Analysis (LDA) is a classical statistical approach for feature extraction and dimension reduction (Duda et al. [sent-19, score-0.098]
</p><p>8 The optimal transformation can be readily computed by applying an eigen-decomposition on the scatter matrices of the given training data set. [sent-23, score-0.275]
</p><p>9 However classical LDA requires the total scatter matrix to be nonsingular. [sent-24, score-0.288]
</p><p>10 In many applications such as information retrieval, face recognition, and microarray data analysis, all scatter matrices in question can be singular since the data points are from a very high-dimensional space and in general the sample size does not exceed this dimension. [sent-25, score-0.333]
</p><p>11 This is known as the singularity or undersampled problems (Krzanowski et al. [sent-26, score-0.118]
</p><p>12 YE  In recent years, many approaches have been brought to bear on such high-dimensional, undersampled problems, including PCA+LDA (Belhumeur et al. [sent-29, score-0.086]
</p><p>13 1 Contribution In this paper, we present a new optimization criterion for discriminant analysis, which is applicable to undersampled problems. [sent-37, score-0.325]
</p><p>14 ULDA was recently proposed for extracting feature vectors with uncorrelated attributes (Jin et al. [sent-42, score-0.099]
</p><p>15 , 2004a) showed that classical LDA is equivalent to ULDA, in the sense that both classical LDA and ULDA produce the same transformation matrix when the total scatter matrix is nonsingular. [sent-45, score-0.426]
</p><p>16 , 2004a) for computing the optimal discriminant vectors of ULDA. [sent-47, score-0.195]
</p><p>17 The key property of OLDA is that the discriminant vectors of OLDA are orthogonal to each other, i. [sent-51, score-0.268]
</p><p>18 We have conducted a comparative study on a variety of real-world data sets, including text documents, face images, and gene expression data to evaluate ULDA and OLDA, and compare with Regularized LDA (RLDA). [sent-59, score-0.14]
</p><p>19 A generalization of classical LDA using the new criterion is presented in Section 3. [sent-64, score-0.1]
</p><p>20 Classical Discriminant Analysis Given a data matrix A ∈ IRm×n , classical linear discriminant analysis computes a linear transformation G ∈ IRm× that maps each column ai of A in the m-dimensional space to a vector yi in the -dimensional space: G : ai ∈ IRm → yi = GT ai ∈ IR ( < m). [sent-71, score-0.333]
</p><p>21 The goal of classical LDA is to ﬁnd a transformation G such that the cluster structure of the original high-dimensional space is preserved in the reduced-dimensional space. [sent-73, score-0.097]
</p><p>22 The traces of the two scatter matrices Sw and Sb can be computed as follows: trace(Sw ) =  1 k 1 k (x − c(i) )T (x − c(i) ) = ∑ ∑ ||x − c(i) ||2 ∑∑ n i=1 x∈Ai n i=1 x∈Ai  trace(Sb ) =  1 k 1 k ni (c(i) − c)T (c(i) − c) = ∑ ni ||c(i) − c||2 . [sent-79, score-0.282]
</p><p>23 In the lower-dimensional space resulting from the linear transformation G, the scatter matrices become L L Sw = GT Sw G, Sb = GT Sb G, StL = GT St G. [sent-81, score-0.275]
</p><p>24 A common optimization in classical discriminant analysis (Fukunaga, 1990) is L G = arg max trace((StL )−1 Sb ) . [sent-83, score-0.275]
</p><p>25 There are at most k − 1 eigenvectors corresponding to nonzero eigenvalues, since the rank of the matrix Sb is bounded from above by k − 1. [sent-87, score-0.083]
</p><p>26 Therefore, the reduced dimension by classical LDA is at most k − 1. [sent-88, score-0.078]
</p><p>27 A stable way to solve this eigen-decomposition problem is to apply Singular Value Decomposition (SVD) (Golub and Loan, 1996) on the scatter matrices. [sent-89, score-0.191]
</p><p>28 , 1995) that −1 arg min{(h − c( j) )T Sw (h − c( j) )} = arg min{||GT (h − c( j) )||2 }, j  j  (7)  where G is the optimal transformation solving the optimization problem in Eq. [sent-95, score-0.089]
</p><p>29 Note that classical discriminant analysis requires the total scatter matrix St to be nonsingular, which may not hold for undersampled data. [sent-105, score-0.569]
</p><p>30 A common way to deal with the singularity problems is to apply an intermediate dimension reduction stage such as PCA to reduce the dimension of the original data before classical LDA is applied. [sent-107, score-0.152]
</p><p>31 In this two-stage PCA+LDA algorithm, the discriminant stage is preceded by a dimension reduction stage using PCA. [sent-111, score-0.237]
</p><p>32 The dimension of the subspace transformed by PCA is chosen such as the “reduced” total scatter matrix in the subspace is nonsingular, so that classical LDA can be applied. [sent-112, score-0.31]
</p><p>33 PLDA penalizes the withinclass scatter matrix as Sw + Ω, for some penalty matrix Ω. [sent-124, score-0.273]
</p><p>34 The penalties are designed to produce smoothness in the discriminant functions. [sent-126, score-0.195]
</p><p>35 The use of pseudo-inverse in discriminant analysis has been studied in the past. [sent-130, score-0.195]
</p><p>36 The Pseudo Fisher Linear Discriminant (PFLDA) (Fukunaga, 1990; Raudys and Duin, 1998; Skurichina and Duin, 1996, 1999) is based on the pseudoinverse of the scatter matrices. [sent-131, score-0.191]
</p><p>37 Pseudo-inverses of the scatter matrices were also studied in (Krzanowski et al. [sent-133, score-0.234]
</p><p>38 , 2004b) is: L L F0 (G) = trace (Sb )+ Sw ,  (8)  L where (Sb )+ denotes the pseudo-inverse of the between-class scatter matrix. [sent-142, score-0.246]
</p><p>39 An overview of LDA on undersampled problems can be found in (Krzanowski et al. [sent-148, score-0.086]
</p><p>40 The current paper focuses on linear discriminant analysis, which applies linear decision boundary. [sent-150, score-0.195]
</p><p>41 Discriminant analysis can also be studied in the non-linear fashion, so-called kernel discriminant analysis, by using the kernel trick (Sch¨ kopf and Smola, 2002). [sent-151, score-0.221]
</p><p>42 The interested readers can ﬁnd more details on kernel discriminant analysis in (Baudat and Anouar, 2000; Hand, 1982; Lu et al. [sent-153, score-0.195]
</p><p>43 Generalization of Discriminant Analysis Classical discriminant analysis solves an eigen-decomposition problem when St is nonsingular. [sent-156, score-0.195]
</p><p>44 For undersampled problems, St is singular, since the sample size n may be smaller than its dimension m. [sent-157, score-0.108]
</p><p>45 The new criterion F1 is a natural extension of the classical one in Eq. [sent-159, score-0.1]
</p><p>46 While the inverse of a matrix may not exist, the pseudo-inverse of any matrix is well deﬁned. [sent-161, score-0.082]
</p><p>47 The new criterion F1 is deﬁned as L F1 (G) = trace (StL )+ Sb . [sent-163, score-0.099]
</p><p>48 (9)  The optimal transformation matrix G is computed so that F1 (G) is maximized. [sent-164, score-0.082]
</p><p>49 It is based on the simultaneous diagonalization of the three scatter matrices. [sent-167, score-0.262]
</p><p>50 1 Simultaneous Diagonalization of Scatter Matrices In this section, we take a closer look at the relationship among three scatter matrices Sb , Sw , and St , and show how to diagonalize them simultaneously. [sent-170, score-0.234]
</p><p>51 2 Maximization of the F1 Criterion In this section, we derive the generalized discriminant analysis by maximizing the F1 criterion deﬁned in Eq. [sent-194, score-0.257]
</p><p>52 The main technique applied is the simultaneous diagonalization of scatter matrices from last section. [sent-196, score-0.305]
</p><p>53 We show in this section that the solutions to the proposed criterion F1 can be characterized as G = Xq M, where Xq is the matrix consisting of the ﬁrst q columns of X, deﬁned in Eq. [sent-197, score-0.122]
</p><p>54 Then, for any M ∈ IRm×s (s ≤ m) with orthonormal columns, the following inequality holds, trace M T AM ≤ λ1 + · · · + λs , where the equality holds if M = [x1 , · · · , xs ]Q, for any orthogonal matrix Q ∈ IRs×s . [sent-208, score-0.169]
</p><p>55 (15) and Xq be the matrix consisting of the ﬁrst q columns of X, where q = rank(Sb ). [sent-212, score-0.078]
</p><p>56 Then G = Xq M, for any nonsingular M, maximizes F1 deﬁned in Eq. [sent-213, score-0.087]
</p><p>57 Proof By the simultaneous diagonalization of the three scatter matrices in Eq. [sent-215, score-0.305]
</p><p>58 1  Hence F1 = trace (GT G1 )+ (GT Σb G1 ) = trace (G1 G+ )T Σb (G1 G+ ) , 1 1 1 1 490  (16)  G ENERALIZED D ISCRIMINANT A NALYSIS  where the second equality follows from Lemma 3. [sent-219, score-0.11]
</p><p>59 It follows that 1 1 0 0 0 0 F1 = trace (G1 G+ )T Σb (G1 G+ ) = trace R 1 1 = trace  Iδ 0 0 0  RT Σb R  Iδ 0 0 0  Iδ 0 0 0  RT Σb R  Iδ 0 0 0  RT  = trace RT Σb Rδ ≤ λ1 + · · · + λq . [sent-224, score-0.22]
</p><p>60 Note that the orthogonal matrices W and S, and the diagonal matrix Σq are arbitrary. [sent-232, score-0.157]
</p><p>61 It follows that G = X G = Xq M, for any nonsingular M, maximizes F1 . [sent-234, score-0.087]
</p><p>62 Uncorrelated LDA Versus Orthogonal LDA From last section, G = Xq M, for any nonsingular M maximizes the F1 criterion. [sent-240, score-0.087]
</p><p>63 491  YE  Algorithm 1: Uncorrelated LDA Input: data matrix A Output: transformation matrix G 1. [sent-256, score-0.123]
</p><p>64 G ← Xq ;  ULDA was originally proposed to compute the optimal discriminant vectors that are St -orthogonal. [sent-263, score-0.195]
</p><p>65 A key property of ULDA is that the features in the reduced space are uncorrelated to each other, as stated in the following proposition. [sent-267, score-0.099]
</p><p>66 1 Let the transformation matrix for ULDA be G = [g1 , · · · , gd ], for some d > 0. [sent-269, score-0.082]
</p><p>67 That is, Zi and Z j are uncorrelated to each other. [sent-275, score-0.099]
</p><p>68 More speciﬁcally, we show that the discriminant vectors of ULDA are eigenvectors of St+ Sb corresponding to nonzero eigenvalues. [sent-298, score-0.213]
</p><p>69 Recall that classical LDA computes the optimal discriminant vectors by solving an eigenvalue problem on St−1 Sb , assuming St is nonsingular (See Section 2). [sent-299, score-0.321]
</p><p>70 This equivalence result shows that ULDA is a natural extension of classical LDA by replacing inverse with pseudo-inverse, when dealing with singular St . [sent-300, score-0.084]
</p><p>71 493  YE  Algorithm 2: Orthogonal LDA Input: data matrix A Output: transformation matrix G 1. [sent-308, score-0.123]
</p><p>72 2 Orthogonal LDA ˜˜ LDA with orthogonal discriminant vectors is a natural alternative to ULDA. [sent-312, score-0.268]
</p><p>73 Let Xq = QR be the QR ˜ ˜ −1 so that the columns of G = Xq M = Q are decomposition of Xq , then we can simply choose M = R orthogonal to each other. [sent-313, score-0.092]
</p><p>74 Note that in the literature of LDA, Foley-Sammon LDA (FSLDA) is also known for its orthogonal discriminant vectors. [sent-315, score-0.268]
</p><p>75 It is worthwhile to point out that both ULDA and FSLDA use the same Fisher criterion function, and the main difference is that the optimal discriminant vectors generated by ULDA are St orthogonal to each other, while the optimal discriminant vectors of FSLDA are orthogonal to each other. [sent-323, score-0.58]
</p><p>76 The common point of the proposed OLDA algorithm and the FSLDA algorithm described above is that the transformation matrix has orthogonal columns. [sent-324, score-0.155]
</p><p>77 1 Let G be the optimal transformation matrix for ULDA, and let h be any test point. [sent-332, score-0.082]
</p><p>78 ULDA can be considered as an extension of classical LDA for singular scatter matrices. [sent-346, score-0.275]
</p><p>79 However, with whitened total scatter matrix, that is if St is an identity matrix, OLDA is equivalent to ULDA. [sent-348, score-0.191]
</p><p>80 On the other hand, OLDA applies or˜ ˜˜ ˜ thogonal transformation Q, by factoring out the R matrix through the QR decomposition of Xq = QR. [sent-351, score-0.082]
</p><p>81 Recall that G = Xq M, for any nonsingular M maximizes the F1 criterion. [sent-362, score-0.087]
</p><p>82 1 Data Sets We have three types of data for the evaluation: text documents, including tr41 and re0; face images, including PIX and AR; and gene expression data, including GCM and ALL. [sent-368, score-0.14]
</p><p>83 • PIX1 is a face image data set, which contains 300 face images of 30 persons. [sent-376, score-0.161]
</p><p>84 This subset contains 1638 face images of 126 individuals. [sent-382, score-0.09]
</p><p>85 • GCM is a gene expression data set consisting of 198 human tumor samples spanning fourteen different cancer types. [sent-385, score-0.111]
</p><p>86 The breakdown of the samples is: 15 samples for BCR, 27 samples for E2A, 64 samples for Hyperdip, 20 samples for MLL, 43 samples for T, and 79 samples for TEL. [sent-401, score-0.168]
</p><p>87 It is interesting to note that OLDA achieves higher accuracies than ULDA for the two face image data sets and two gene expression data sets, while it achieves accuracies close to those of ULDA for the two text document data sets. [sent-417, score-0.24]
</p><p>88 Recall that the solution to the proposed criterion is G = Xq M, for any nonsingular M. [sent-423, score-0.114]
</p><p>89 In this experiment, we randomly generated 100 matrices for M and computed the accuracies using the corresponding transformation matrices. [sent-425, score-0.134]
</p><p>90 Figure 1 shows the histogram of the resulting accuracies on GCM, where the x-axis represents the range of resulting accuracies (divided into small intervals), and the y-axis represents the number (count) for each interval. [sent-426, score-0.1]
</p><p>91 Conclusions and Future Directions In this paper, a new optimization criterion for discriminant analysis is presented. [sent-484, score-0.239]
</p><p>92 The new criterion extends the optimization criteria of the classical LDA when the scatter matrices are singular. [sent-485, score-0.334]
</p><p>93 It is based on the simultaneous diagonalization of the three scatter matrices. [sent-503, score-0.262]
</p><p>94 ULDA has the property that the features in the reduced space are uncorrelated, while OLDA has the property that the discriminant vectors obtained are orthogonal to each other. [sent-506, score-0.268]
</p><p>95 One of our future work is to incorporate the sparsity criterion in discriminant analysis. [sent-512, score-0.239]
</p><p>96 Deﬁnition 2 The pseudo-inverse of a matrix A, denoted as A+ , refers to the unique matrix satisfying the following four conditions: (1)A+ AA+ = A+ ,  (2)AA+ A = A,  (3) (AA+ )T = AA+ ,  (4)(A+ A)T = A+ A. [sent-517, score-0.082]
</p><p>97 Regularized discriminant analysis and its application to face recognition. [sent-550, score-0.266]
</p><p>98 An optimal transformation for discriminant and principal component analysis. [sent-576, score-0.236]
</p><p>99 Structure preserving dimension reduction for clustered text data based on the generalized singular value decomposition. [sent-651, score-0.105]
</p><p>100 An optimization criterion for generalized discriminant analysis on undersampled problems. [sent-783, score-0.343]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ulda', 0.48), ('lda', 0.389), ('sb', 0.345), ('olda', 0.334), ('discriminant', 0.195), ('scatter', 0.191), ('sw', 0.174), ('st', 0.173), ('xq', 0.172), ('ye', 0.14), ('gt', 0.137), ('uncorrelated', 0.099), ('fslda', 0.094), ('gcm', 0.094), ('rlda', 0.094), ('undersampled', 0.086), ('eneralized', 0.077), ('iscriminant', 0.077), ('stl', 0.077), ('orthogonal', 0.073), ('face', 0.071), ('nonsingular', 0.07), ('irm', 0.065), ('nalysis', 0.065), ('duchene', 0.06), ('classical', 0.056), ('trace', 0.055), ('gene', 0.052), ('krzanowski', 0.051), ('accuracies', 0.05), ('hb', 0.046), ('criterion', 0.044), ('matrices', 0.043), ('diagonalization', 0.043), ('foley', 0.043), ('pix', 0.043), ('skurichina', 0.043), ('swets', 0.043), ('xdt', 0.043), ('transformation', 0.041), ('matrix', 0.041), ('im', 0.041), ('svd', 0.04), ('jin', 0.038), ('fukunaga', 0.038), ('hastie', 0.038), ('dt', 0.036), ('leclerq', 0.034), ('ht', 0.032), ('singularity', 0.032), ('pca', 0.032), ('qr', 0.029), ('xit', 0.029), ('centroid', 0.029), ('duin', 0.029), ('belhumeur', 0.029), ('sammon', 0.029), ('singular', 0.028), ('db', 0.028), ('simultaneous', 0.028), ('dudoit', 0.026), ('jieping', 0.026), ('kopf', 0.026), ('plda', 0.026), ('raudys', 0.026), ('aa', 0.026), ('hw', 0.026), ('weng', 0.026), ('semide', 0.025), ('samples', 0.024), ('ni', 0.024), ('rank', 0.024), ('arg', 0.024), ('golub', 0.023), ('rt', 0.023), ('dimension', 0.022), ('recognition', 0.022), ('army', 0.022), ('howland', 0.022), ('zhao', 0.022), ('regularized', 0.021), ('ut', 0.021), ('reduction', 0.02), ('loan', 0.02), ('images', 0.019), ('ar', 0.019), ('columns', 0.019), ('consisting', 0.018), ('nonzero', 0.018), ('generalized', 0.018), ('maximizes', 0.017), ('tumor', 0.017), ('text', 0.017), ('aspremont', 0.017), ('baudat', 0.017), ('ggt', 0.017), ('htt', 0.017), ('irq', 0.017), ('irt', 0.017), ('janardan', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="18-tfidf-1" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>Author: Jieping Ye</p><p>Abstract: A generalized discriminant analysis based on a new optimization criterion is presented. The criterion extends the optimization criteria of the classical Linear Discriminant Analysis (LDA) when the scatter matrices are singular. An efﬁcient algorithm for the new optimization problem is presented. The solutions to the proposed criterion form a family of algorithms for generalized LDA, which can be characterized in a closed form. We study two speciﬁc algorithms, namely Uncorrelated LDA (ULDA) and Orthogonal LDA (OLDA). ULDA was previously proposed for feature extraction and dimension reduction, whereas OLDA is a novel algorithm proposed in this paper. The features in the reduced space of ULDA are uncorrelated, while the discriminant vectors of OLDA are orthogonal to each other. We have conducted a comparative study on a variety of real-world data sets to evaluate ULDA and OLDA in terms of classiﬁcation accuracy. Keywords: dimension reduction, linear discriminant analysis, uncorrelated LDA, orthogonal LDA, singular value decomposition</p><p>2 0.17991564 <a title="18-tfidf-2" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>Author: Hyunsoo Kim, Peg Howland, Haesun Park</p><p>Abstract: Support vector machines (SVMs) have been recognized as one of the most successful classiﬁcation methods for many applications including text classiﬁcation. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efﬁciently handle a large number of terms in practical applications of text classiﬁcation. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classiﬁcation algorithm and support vector classiﬁers to handle the classiﬁcation problem where a document may belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efﬁciency for both training and testing can be achieved without sacriﬁcing prediction accuracy of text classiﬁcation even when the dimension of the input space is signiﬁcantly reduced. Keywords: dimension reduction, support vector machines, text classiﬁcation, linear discriminant analysis, centroids</p><p>3 0.055426732 <a title="18-tfidf-3" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>Author: Luís B. Almeida</p><p>Abstract: When acquiring an image of a paper document, the image printed on the back page sometimes shows through. The mixture of the front- and back-page images thus obtained is markedly nonlinear, and thus constitutes a good real-life test case for nonlinear blind source separation. This paper addresses a difﬁcult version of this problem, corresponding to the use of “onion skin” paper, which results in a relatively strong nonlinearity of the mixture, which becomes close to singular in the lighter regions of the images. The separation is achieved through the MISEP technique, which is an extension of the well known INFOMAX method. The separation results are assessed with objective quality measures. They show an improvement over the results obtained with linear separation, but have room for further improvement. Keywords: ICA, blind source separation, nonlinear mixtures, nonlinear separation, image mixture, image separation</p><p>4 0.049984369 <a title="18-tfidf-4" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>Author: Lior Wolf, Amnon Shashua</p><p>Abstract: The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classiﬁcation tasks, for example, it is not uncommon to have 104 to 107 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a deﬁnition of “relevancy” based on spectral properties of the Laplacian of the features’ measurement matrix. The feature selection process is then based on a continuous ranking of the features deﬁned by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a “biased non-negativity” of a key matrix in the process. As a result, a simple leastsquares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.</p><p>5 0.046186969 <a title="18-tfidf-5" href="./jmlr-2005-Learning_the_Kernel_Function_via_Regularization.html">48 jmlr-2005-Learning the Kernel Function via Regularization</a></p>
<p>Author: Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We study the problem of ﬁnding an optimal kernel from a prescribed convex set of kernels K for learning a real-valued function by regularization. We establish for a wide variety of regularization functionals that this leads to a convex optimization problem and, for square loss regularization, we characterize the solution of this problem. We show that, although K may be an uncountable set, the optimal kernel is always obtained as a convex combination of at most m + 2 basic kernels, where m is the number of data examples. In particular, our results apply to learning the optimal radial kernel or the optimal dot product kernel.</p><p>6 0.041985143 <a title="18-tfidf-6" href="./jmlr-2005-Change_Point_Problems_in_Linear_Dynamical_Systems.html">17 jmlr-2005-Change Point Problems in Linear Dynamical Systems</a></p>
<p>7 0.03178063 <a title="18-tfidf-7" href="./jmlr-2005-Multiclass_Boosting_for_Weak_Classifiers.html">57 jmlr-2005-Multiclass Boosting for Weak Classifiers</a></p>
<p>8 0.031489458 <a title="18-tfidf-8" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>9 0.030215448 <a title="18-tfidf-9" href="./jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error.html">13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</a></p>
<p>10 0.029410891 <a title="18-tfidf-10" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>11 0.028450795 <a title="18-tfidf-11" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>12 0.028070463 <a title="18-tfidf-12" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>13 0.025717752 <a title="18-tfidf-13" href="./jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines.html">73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</a></p>
<p>14 0.024879064 <a title="18-tfidf-14" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>15 0.023500931 <a title="18-tfidf-15" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>16 0.022415278 <a title="18-tfidf-16" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>17 0.022018012 <a title="18-tfidf-17" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>18 0.021915665 <a title="18-tfidf-18" href="./jmlr-2005-Matrix_Exponentiated_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">55 jmlr-2005-Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>19 0.020494143 <a title="18-tfidf-19" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>20 0.019270949 <a title="18-tfidf-20" href="./jmlr-2005-On_the_Nystr%C3%B6m_Method_for_Approximating_a_Gram_Matrix_for_Improved_Kernel-Based_Learning.html">60 jmlr-2005-On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.134), (1, 0.126), (2, 0.081), (3, -0.015), (4, 0.06), (5, -0.018), (6, -0.147), (7, 0.344), (8, -0.067), (9, -0.055), (10, -0.448), (11, 0.242), (12, 0.078), (13, -0.162), (14, 0.037), (15, -0.019), (16, 0.002), (17, -0.197), (18, 0.109), (19, 0.016), (20, 0.131), (21, -0.142), (22, 0.022), (23, -0.12), (24, -0.023), (25, -0.005), (26, 0.01), (27, -0.089), (28, 0.153), (29, 0.018), (30, -0.104), (31, 0.053), (32, 0.029), (33, -0.013), (34, 0.012), (35, -0.071), (36, 0.078), (37, -0.004), (38, -0.006), (39, 0.096), (40, -0.071), (41, -0.051), (42, 0.026), (43, 0.066), (44, 0.07), (45, -0.031), (46, 0.033), (47, -0.001), (48, 0.041), (49, -0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97321081 <a title="18-lsi-1" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>Author: Jieping Ye</p><p>Abstract: A generalized discriminant analysis based on a new optimization criterion is presented. The criterion extends the optimization criteria of the classical Linear Discriminant Analysis (LDA) when the scatter matrices are singular. An efﬁcient algorithm for the new optimization problem is presented. The solutions to the proposed criterion form a family of algorithms for generalized LDA, which can be characterized in a closed form. We study two speciﬁc algorithms, namely Uncorrelated LDA (ULDA) and Orthogonal LDA (OLDA). ULDA was previously proposed for feature extraction and dimension reduction, whereas OLDA is a novel algorithm proposed in this paper. The features in the reduced space of ULDA are uncorrelated, while the discriminant vectors of OLDA are orthogonal to each other. We have conducted a comparative study on a variety of real-world data sets to evaluate ULDA and OLDA in terms of classiﬁcation accuracy. Keywords: dimension reduction, linear discriminant analysis, uncorrelated LDA, orthogonal LDA, singular value decomposition</p><p>2 0.75064152 <a title="18-lsi-2" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>Author: Hyunsoo Kim, Peg Howland, Haesun Park</p><p>Abstract: Support vector machines (SVMs) have been recognized as one of the most successful classiﬁcation methods for many applications including text classiﬁcation. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efﬁciently handle a large number of terms in practical applications of text classiﬁcation. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classiﬁcation algorithm and support vector classiﬁers to handle the classiﬁcation problem where a document may belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efﬁciency for both training and testing can be achieved without sacriﬁcing prediction accuracy of text classiﬁcation even when the dimension of the input space is signiﬁcantly reduced. Keywords: dimension reduction, support vector machines, text classiﬁcation, linear discriminant analysis, centroids</p><p>3 0.15920201 <a title="18-lsi-3" href="./jmlr-2005-Change_Point_Problems_in_Linear_Dynamical_Systems.html">17 jmlr-2005-Change Point Problems in Linear Dynamical Systems</a></p>
<p>Author: Onno Zoeter, Tom Heskes</p><p>Abstract: We study the problem of learning two regimes (we have a normal and a prefault regime in mind) based on a train set of non-Markovian observation sequences. Key to the model is that we assume that once the system switches from the normal to the prefault regime it cannot restore and will eventually result in a fault. We refer to the particular setting as semi-supervised since we assume the only information given to the learner is whether a particular sequence ended with a stop (implying that the sequence was generated by the normal regime) or with a fault (implying that there was a switch from the normal to the fault regime). In the latter case the particular time point at which a switch occurred is not known. The underlying model used is a switching linear dynamical system (SLDS). The constraints in the regime transition probabilities result in an exact inference procedure that scales quadratically with the length of a sequence. Maximum aposteriori (MAP) parameter estimates can be found using an expectation maximization (EM) algorithm with this inference algorithm in the E-step. For long sequences this will not be practically feasible and an approximate inference and an approximate EM procedure is called for. We describe a Ä?Ĺš&sbquo;exible class of approximations corresponding to different choices of clusters in a Kikuchi free energy with weak consistency constraints. Keywords: change point problems, switching linear dynamical systems, strong junction trees, approximate inference, expectation propagation, Kikuchi free energies</p><p>4 0.15350839 <a title="18-lsi-4" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>Author: Luís B. Almeida</p><p>Abstract: When acquiring an image of a paper document, the image printed on the back page sometimes shows through. The mixture of the front- and back-page images thus obtained is markedly nonlinear, and thus constitutes a good real-life test case for nonlinear blind source separation. This paper addresses a difﬁcult version of this problem, corresponding to the use of “onion skin” paper, which results in a relatively strong nonlinearity of the mixture, which becomes close to singular in the lighter regions of the images. The separation is achieved through the MISEP technique, which is an extension of the well known INFOMAX method. The separation results are assessed with objective quality measures. They show an improvement over the results obtained with linear separation, but have room for further improvement. Keywords: ICA, blind source separation, nonlinear mixtures, nonlinear separation, image mixture, image separation</p><p>5 0.14921831 <a title="18-lsi-5" href="./jmlr-2005-Learning_the_Kernel_Function_via_Regularization.html">48 jmlr-2005-Learning the Kernel Function via Regularization</a></p>
<p>Author: Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We study the problem of ﬁnding an optimal kernel from a prescribed convex set of kernels K for learning a real-valued function by regularization. We establish for a wide variety of regularization functionals that this leads to a convex optimization problem and, for square loss regularization, we characterize the solution of this problem. We show that, although K may be an uncountable set, the optimal kernel is always obtained as a convex combination of at most m + 2 basic kernels, where m is the number of data examples. In particular, our results apply to learning the optimal radial kernel or the optimal dot product kernel.</p><p>6 0.14229102 <a title="18-lsi-6" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>7 0.12995107 <a title="18-lsi-7" href="./jmlr-2005-Multiclass_Boosting_for_Weak_Classifiers.html">57 jmlr-2005-Multiclass Boosting for Weak Classifiers</a></p>
<p>8 0.12429205 <a title="18-lsi-8" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>9 0.11414029 <a title="18-lsi-9" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>10 0.10596885 <a title="18-lsi-10" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>11 0.10402444 <a title="18-lsi-11" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>12 0.10053711 <a title="18-lsi-12" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>13 0.098729201 <a title="18-lsi-13" href="./jmlr-2005-Matrix_Exponentiated_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">55 jmlr-2005-Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>14 0.096225642 <a title="18-lsi-14" href="./jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error.html">13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</a></p>
<p>15 0.095997885 <a title="18-lsi-15" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>16 0.089411043 <a title="18-lsi-16" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>17 0.080310427 <a title="18-lsi-17" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>18 0.080303714 <a title="18-lsi-18" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>19 0.078657776 <a title="18-lsi-19" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>20 0.075720258 <a title="18-lsi-20" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.016), (17, 0.026), (19, 0.021), (26, 0.503), (36, 0.021), (37, 0.014), (42, 0.07), (43, 0.023), (47, 0.014), (52, 0.079), (70, 0.02), (88, 0.053), (94, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73669684 <a title="18-lda-1" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>Author: Jieping Ye</p><p>Abstract: A generalized discriminant analysis based on a new optimization criterion is presented. The criterion extends the optimization criteria of the classical Linear Discriminant Analysis (LDA) when the scatter matrices are singular. An efﬁcient algorithm for the new optimization problem is presented. The solutions to the proposed criterion form a family of algorithms for generalized LDA, which can be characterized in a closed form. We study two speciﬁc algorithms, namely Uncorrelated LDA (ULDA) and Orthogonal LDA (OLDA). ULDA was previously proposed for feature extraction and dimension reduction, whereas OLDA is a novel algorithm proposed in this paper. The features in the reduced space of ULDA are uncorrelated, while the discriminant vectors of OLDA are orthogonal to each other. We have conducted a comparative study on a variety of real-world data sets to evaluate ULDA and OLDA in terms of classiﬁcation accuracy. Keywords: dimension reduction, linear discriminant analysis, uncorrelated LDA, orthogonal LDA, singular value decomposition</p><p>2 0.25862864 <a title="18-lda-2" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>Author: Hyunsoo Kim, Peg Howland, Haesun Park</p><p>Abstract: Support vector machines (SVMs) have been recognized as one of the most successful classiﬁcation methods for many applications including text classiﬁcation. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efﬁciently handle a large number of terms in practical applications of text classiﬁcation. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classiﬁcation algorithm and support vector classiﬁers to handle the classiﬁcation problem where a document may belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efﬁciency for both training and testing can be achieved without sacriﬁcing prediction accuracy of text classiﬁcation even when the dimension of the input space is signiﬁcantly reduced. Keywords: dimension reduction, support vector machines, text classiﬁcation, linear discriminant analysis, centroids</p><p>3 0.22007877 <a title="18-lda-3" href="./jmlr-2005-Combining_Information_Extraction_Systems_Using_Voting_and_Stacked_Generalization.html">21 jmlr-2005-Combining Information Extraction Systems Using Voting and Stacked Generalization</a></p>
<p>Author: Georgios Sigletos, Georgios Paliouras, Constantine D. Spyropoulos, Michalis Hatzopoulos</p><p>Abstract: This article investigates the effectiveness of voting and stacked generalization -also known as stacking- in the context of information extraction (IE). A new stacking framework is proposed that accommodates well-known approaches for IE. The key idea is to perform cross-validation on the base-level data set, which consists of text documents annotated with relevant information, in order to create a meta-level data set that consists of feature vectors. A classifier is then trained using the new vectors. Therefore, base-level IE systems are combined with a common classifier at the metalevel. Various voting schemes are presented for comparing against stacking in various IE domains. Well known IE systems are employed at the base-level, together with a variety of classifiers at the meta-level. Results show that both voting and stacking work better when relying on probabilistic estimates by the base-level systems. Voting proved to be effective in most domains in the experiments. Stacking, on the other hand, proved to be consistently effective over all domains, doing comparably or better than voting and always better than the best base-level systems. Particular emphasis is also given to explaining the results obtained by voting and stacking at the meta-level, with respect to the varying degree of similarity in the output of the base-level systems. Keywords: stacking, voting, information extraction, cross-validation 1</p><p>4 0.21047027 <a title="18-lda-4" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>Author: Antoine Bordes, Seyda Ertekin, Jason Weston, Léon Bottou</p><p>Abstract: Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efﬁcient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We ﬁrst present an online SVM algorithm based on this premise. LASVM yields competitive misclassiﬁcation rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.</p><p>5 0.20334768 <a title="18-lda-5" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>Author: Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, Suvrit Sra</p><p>Abstract: Several large scale data mining applications, such as text categorization and gene expression analysis, involve high-dimensional data that is also inherently directional in nature. Often such data is L2 normalized so that it lies on the surface of a unit hypersphere. Popular models such as (mixtures of) multi-variate Gaussians are inadequate for characterizing such data. This paper proposes a generative mixture-model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. In particular, we derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the mean and concentration parameters of this mixture. Numerical estimation of the concentration parameters is non-trivial in high dimensions since it involves functional inversion of ratios of Bessel functions. We also formulate two clustering algorithms corresponding to the variants of EM that we derive. Our approach provides a theoretical basis for the use of cosine similarity that has been widely employed by the information retrieval community, and obtains the spherical kmeans algorithm (kmeans with cosine similarity) as a special case of both variants. Empirical results on clustering of high-dimensional text and gene-expression data based on a mixture of vMF distributions show that the ability to estimate the concentration parameter for each vMF component, which is not present in existing approaches, yields superior results, especially for difﬁcult clustering tasks in high-dimensional spaces. Keywords: clustering, directional distributions, mixtures, von Mises-Fisher, expectation maximization, maximum likelihood, high dimensional data c 2005 Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh and Suvrit Sra. BANERJEE , D HILLON , G HOSH AND S RA</p><p>6 0.20268872 <a title="18-lda-6" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>7 0.20108502 <a title="18-lda-7" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>8 0.20048182 <a title="18-lda-8" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>9 0.20027231 <a title="18-lda-9" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>10 0.20004238 <a title="18-lda-10" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>11 0.1994594 <a title="18-lda-11" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>12 0.19745603 <a title="18-lda-12" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>13 0.19599836 <a title="18-lda-13" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>14 0.19538365 <a title="18-lda-14" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>15 0.19447935 <a title="18-lda-15" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>16 0.19359654 <a title="18-lda-16" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>17 0.19254519 <a title="18-lda-17" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>18 0.1917875 <a title="18-lda-18" href="./jmlr-2005-An_MDP-Based_Recommender_System.html">12 jmlr-2005-An MDP-Based Recommender System</a></p>
<p>19 0.19160821 <a title="18-lda-19" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>20 0.18915695 <a title="18-lda-20" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
