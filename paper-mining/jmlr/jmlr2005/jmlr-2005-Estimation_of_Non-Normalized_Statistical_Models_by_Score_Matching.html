<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-31" href="#">jmlr2005-31</a> knowledge-graph by maker-knowledge-mining</p><h1>31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</h1>
<br/><p>Source: <a title="jmlr-2005-31-pdf" href="http://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">pdf</a></p><p>Author: Aapo Hyvärinen</p><p>Abstract: One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data. Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence</p><p>Reference: <a title="jmlr-2005-31-reference" href="../jmlr2005_reference/jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('px', 0.427), ('scor', 0.411), ('wk', 0.328), ('pdf', 0.31), ('overcomplet', 0.226), ('di', 0.203), ('hyv', 0.191), ('hyvarin', 0.189), ('rin', 0.167), ('ic', 0.153), ('match', 0.141), ('wj', 0.135), ('mcmc', 0.128), ('mii', 0.108), ('bracket', 0.094), ('teh', 0.09), ('wi', 0.083), ('erenty', 0.081), ('estim', 0.079), ('mm', 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="31-tfidf-1" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>2 0.19274716 <a title="31-tfidf-2" href="./jmlr-2005-Convergence_Theorems_for_Generalized_Alternating_Minimization_Procedures.html">23 jmlr-2005-Convergence Theorems for Generalized Alternating Minimization Procedures</a></p>
<p>3 0.14602526 <a title="31-tfidf-3" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>4 0.13701992 <a title="31-tfidf-4" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>5 0.092291526 <a title="31-tfidf-5" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>6 0.089102559 <a title="31-tfidf-6" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>7 0.081884719 <a title="31-tfidf-7" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>8 0.074149996 <a title="31-tfidf-8" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>9 0.063760281 <a title="31-tfidf-9" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>10 0.058053013 <a title="31-tfidf-10" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>11 0.055484209 <a title="31-tfidf-11" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>12 0.055406563 <a title="31-tfidf-12" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>13 0.046542574 <a title="31-tfidf-13" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>14 0.04398492 <a title="31-tfidf-14" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>15 0.043790042 <a title="31-tfidf-15" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>16 0.039201617 <a title="31-tfidf-16" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>17 0.036530796 <a title="31-tfidf-17" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>18 0.035227153 <a title="31-tfidf-18" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>19 0.034830444 <a title="31-tfidf-19" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>20 0.033570644 <a title="31-tfidf-20" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.225), (1, 0.199), (2, 0.421), (3, 0.215), (4, 0.108), (5, 0.096), (6, -0.112), (7, -0.23), (8, 0.138), (9, -0.074), (10, -0.137), (11, -0.086), (12, -0.096), (13, -0.09), (14, 0.087), (15, -0.041), (16, 0.072), (17, -0.049), (18, 0.088), (19, 0.032), (20, -0.04), (21, -0.034), (22, 0.033), (23, 0.013), (24, -0.028), (25, -0.071), (26, -0.082), (27, -0.003), (28, -0.024), (29, -0.051), (30, 0.023), (31, -0.033), (32, 0.052), (33, 0.047), (34, 0.07), (35, 0.022), (36, -0.016), (37, 0.031), (38, 0.019), (39, -0.026), (40, -0.008), (41, 0.071), (42, 0.073), (43, -0.154), (44, -0.079), (45, -0.037), (46, 0.085), (47, -0.096), (48, 0.073), (49, 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95684922 <a title="31-lsi-1" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>2 0.70684481 <a title="31-lsi-2" href="./jmlr-2005-Convergence_Theorems_for_Generalized_Alternating_Minimization_Procedures.html">23 jmlr-2005-Convergence Theorems for Generalized Alternating Minimization Procedures</a></p>
<p>3 0.46197718 <a title="31-lsi-3" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>4 0.45074281 <a title="31-lsi-4" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>5 0.28899154 <a title="31-lsi-5" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>6 0.28830206 <a title="31-lsi-6" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>7 0.24298184 <a title="31-lsi-7" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>8 0.22901082 <a title="31-lsi-8" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>9 0.2246033 <a title="31-lsi-9" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>10 0.22108348 <a title="31-lsi-10" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>11 0.2025153 <a title="31-lsi-11" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>12 0.19956194 <a title="31-lsi-12" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>13 0.18526396 <a title="31-lsi-13" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<p>14 0.17896271 <a title="31-lsi-14" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>15 0.17698272 <a title="31-lsi-15" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>16 0.16235062 <a title="31-lsi-16" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>17 0.16043973 <a title="31-lsi-17" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>18 0.1602769 <a title="31-lsi-18" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>19 0.15937576 <a title="31-lsi-19" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>20 0.15532948 <a title="31-lsi-20" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.013), (37, 0.096), (44, 0.585), (62, 0.086), (68, 0.027), (78, 0.019), (79, 0.022), (88, 0.02), (97, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93107545 <a title="31-lda-1" href="./jmlr-2005-Concentration_Bounds_for_Unigram_Language_Models.html">22 jmlr-2005-Concentration Bounds for Unigram Language Models</a></p>
<p>2 0.90655375 <a title="31-lda-2" href="./jmlr-2005-Combining_Information_Extraction_Systems_Using_Voting_and_Stacked_Generalization.html">21 jmlr-2005-Combining Information Extraction Systems Using Voting and Stacked Generalization</a></p>
<p>same-paper 3 0.89084065 <a title="31-lda-3" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>4 0.47016704 <a title="31-lda-4" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>5 0.41269973 <a title="31-lda-5" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>6 0.40973502 <a title="31-lda-6" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>7 0.39588156 <a title="31-lda-7" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>8 0.36716133 <a title="31-lda-8" href="./jmlr-2005-Generalization_Bounds_and_Complexities_Based_on_Sparsity_and_Clustering_for_Convex_Combinations_of_Functions_from_Random_Classes.html">37 jmlr-2005-Generalization Bounds and Complexities Based on Sparsity and Clustering for Convex Combinations of Functions from Random Classes</a></p>
<p>9 0.36483511 <a title="31-lda-9" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>10 0.35089034 <a title="31-lda-10" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>11 0.34566167 <a title="31-lda-11" href="./jmlr-2005-Asymptotics_in_Empirical_Risk_Minimization.html">16 jmlr-2005-Asymptotics in Empirical Risk Minimization</a></p>
<p>12 0.34105548 <a title="31-lda-12" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>13 0.34067693 <a title="31-lda-13" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>14 0.3386867 <a title="31-lda-14" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>15 0.33294952 <a title="31-lda-15" href="./jmlr-2005-Efficient_Margin_Maximizing_with_Boosting.html">29 jmlr-2005-Efficient Margin Maximizing with Boosting</a></p>
<p>16 0.32572111 <a title="31-lda-16" href="./jmlr-2005-Convergence_Theorems_for_Generalized_Alternating_Minimization_Procedures.html">23 jmlr-2005-Convergence Theorems for Generalized Alternating Minimization Procedures</a></p>
<p>17 0.32278177 <a title="31-lda-17" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>18 0.32179517 <a title="31-lda-18" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>19 0.31701934 <a title="31-lda-19" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>20 0.31606573 <a title="31-lda-20" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
