<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-31" href="#">jmlr2005-31</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</h1>
<br/><p>Source: <a title="jmlr-2005-31-pdf" href="http://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">pdf</a></p><p>Author: Aapo Hyvärinen</p><p>Abstract: One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data. Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence</p><p>Reference: <a title="jmlr-2005-31-reference" href="../jmlr2005_reference/jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. [sent-5, score-0.278]
</p><p>2 While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. [sent-6, score-0.18]
</p><p>3 The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data. [sent-8, score-0.261]
</p><p>4 Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence  1. [sent-9, score-0.25]
</p><p>5 Assume we observe a random vector x ∈ Rn which has a probability density function (pdf) denoted by px (. [sent-12, score-0.399]
</p><p>6 ) The problem we consider here is that we only are able to compute the pdf given by the model up to a multiplicative constant Z(θ): p(ξ; θ) =  1 q(ξ; θ). [sent-22, score-0.298]
</p><p>7 Usually, estimation of non-normalized models is approached by Markov Chain Monte Carlo (MCMC) methods, which are very slow, or by making some approximations, which may be quite poor (Mackay, 2003). [sent-26, score-0.128]
</p><p>8 Other recent work in image modelling also includes non-normalized models (Hyv¨rinen and a Hoyer, 2001; Teh et al. [sent-31, score-0.108]
</p><p>9 Non-normalized models have been avoided because their estimation has been considered too diﬃcult; the advent of eﬃcient estimation methods may signiﬁcantly increase their utility. [sent-34, score-0.23]
</p><p>10 This is based on minimizing the expected squared distance of the score function of x and the score function given by the model. [sent-36, score-0.937]
</p><p>11 (By score function, we mean here the gradient of log-density. [sent-37, score-0.473]
</p><p>12 ) We show that this distance can be estimated by a very simple formula involving only sample averages of some derivatives of the logarithm of the pdf given by the model. [sent-38, score-0.399]
</p><p>13 Minimization of the proposed objective function thus provides an estimation method that is computationally simple yet statistically locally consistent. [sent-41, score-0.131]
</p><p>14 For simplicity, we call this the score function, although according the conventional deﬁnition, it is actually the score function with respect to a hypothetical location parameter (Schervish, 1995). [sent-44, score-0.848]
</p><p>15 For the model density, we denote the score function by ψ(ξ; θ):     ∂ log p(ξ;θ) ψ1 (ξ; θ) ∂ξ1     . [sent-45, score-0.502]
</p><p>16   ∂ log p(ξ;θ) ψn (ξ; θ) ∂ξn  The point in using the score function is that it does not depend on Z(θ). [sent-52, score-0.469]
</p><p>17 ) the score function of the distribution of observed data x. [sent-56, score-0.424]
</p><p>18 This could in principle be estimated by computing the gradient of the logarithm of 696  Estimation by Score Matching  a non-parametric estimate of the pdf—but we will see below that no such computation is necessary. [sent-57, score-0.124]
</p><p>19 Note that score functions are mappings from Rn to Rn . [sent-58, score-0.424]
</p><p>20 We now propose that the model is estimated by minimizing the expected squared distance between the model score function ψ(. [sent-59, score-0.611]
</p><p>21 We deﬁne this squared distance as J(θ) =  1 2  ξ∈Rn  px (ξ) ψ(ξ; θ) − ψ x (ξ) 2 dξ. [sent-62, score-0.432]
</p><p>22 (2)  Thus, our score matching estimator of θ is given by ˆ θ = arg min J(θ). [sent-63, score-0.785]
</p><p>23 θ  The motivation for this estimator is that the score function can be directly computed from q as in (1), and we do not need to compute Z. [sent-64, score-0.493]
</p><p>24 However, this may still seem to be a very diﬃcult way of estimating θ, since we might have to compute an estimator of the data score function ψ x from the observed sample, which is basically a non-parametric estimation problem. [sent-65, score-0.621]
</p><p>25 This is because we can use a simple trick of partial integration to compute the objective function very easily, as shown by the following theorem: Theorem 1 Assume that the model score function ψ(ξ; θ) is diﬀerentiable, as well as some weak regularity conditions. [sent-67, score-0.602]
</p><p>26 1 Then, the objective function J in (2) can be expressed as n  J(θ) = ξ∈Rn  px (ξ) i=1  1 ∂i ψi (ξ; θ) + ψi (ξ; θ)2 dξ + const. [sent-68, score-0.394]
</p><p>27 2  (3)  where the constant does not depend on θ, ψi (ξ; θ) =  ∂ log q(ξ; θ) ∂ξi  is the i-th element of the model score function, and ∂i ψi (ξ; θ) =  ∂ψi (ξ; θ) ∂ 2 log q(ξ; θ) = 2 ∂ξi ∂ξi  is the partial derivative of the i-th element of the model score function with respect to the i-th variable. [sent-69, score-1.03]
</p><p>28 The proof, given in the Appendix, is based a simple trick of partial integration that has previously been used in the theory of independent component analysis for modelling the densities of the independent components (Pham and Garrat, 1997). [sent-70, score-0.145]
</p><p>29 We have thus proven the remarkable fact that the squared distance of the model score function from the data score function can be computed as a simple expectation of certain 1. [sent-71, score-0.983]
</p><p>30 Namely: the data pdf px (ξ) is diﬀerentiable, the expectations Ex { ψ(x; θ) 2 } and Ex { ψ x (x) 2 } are ﬁnite for any θ, and px (ξ)ψ(ξ; θ) goes to zero for any θ when ξ → ∞. [sent-72, score-0.995]
</p><p>31 One may wonder whether it is enough to minimize J to estimate the model, or whether the distance of the score functions can be zero for diﬀerent parameter values. [sent-82, score-0.454]
</p><p>32 If we assume that the model is not degenerate, and that q > 0 always, we have local consistency as shown by the following theorem and the corollary: Theorem 2 Assume the pdf of x follows the model: px (. [sent-84, score-0.746]
</p><p>33 Assume further that no other parameter value gives a pdf that is equal2 to p(. [sent-87, score-0.265]
</p><p>34 Corollary 3 Under the assumptions of the preceding Theorems, the score matching esti˜ mator obtained by minimization of J is consistent, i. [sent-91, score-0.761]
</p><p>35 Examples Here, we provide three simulations to illustrate how score matching works, as well as to conﬁrm its consistency and applicability to real data. [sent-105, score-0.799]
</p><p>36 1 Multivariate Gaussian Density As a very simple illustrative example, we consider estimation of the parameters of the multivariate Gaussian density. [sent-109, score-0.13]
</p><p>37 Interestingly, we see that score matching gives exactly the same estimator as maximum likelihood estimation. [sent-120, score-0.853]
</p><p>38 The maximum likelihood estimator is known to be consistent, so the score matching estimator is consistent as well. [sent-122, score-0.922]
</p><p>39 2 Intuitive Interpretation This example also gives some intuitive insight into the principle of score matching. [sent-125, score-0.424]
</p><p>40 , the model variances are inﬁnite and the pdf is completely ﬂat. [sent-131, score-0.298]
</p><p>41 Therefore, the ﬁrst term in (3) and (6), involving second derivatives of the logarithm of q, seems to act as a kind of a normalization term. [sent-134, score-0.13]
</p><p>42 The second term in (3), expectation of the norm of score function, is closely related to maximization of nonnormalized likelihood: if the norm of this gradient is zero, then in fact the data point is in a local extremum of the non-normalized log-likelihood. [sent-140, score-0.61]
</p><p>43 2 Estimation of Basic Independent Component Analysis Model Next, we show the validity of score matching in estimating the following model n T G(wk x) + Z(w1 , . [sent-147, score-0.775]
</p><p>44 , wn ),  log p(x) =  (7)  k=1  which is the basic form of the independent component analysis (ICA) model. [sent-150, score-0.102]
</p><p>45 Again, the normalization constant is well-known and equal to − log | det W| where the matrix W has the vectors wi as rows, but this serves as an illustration of our method. [sent-151, score-0.174]
</p><p>46 , n are independently distributed and have the pdf given by exp(G(si )), the linear transformation x = As  (8)  with A = W−1 follows the pdf’s given in (7), see e. [sent-156, score-0.265]
</p><p>47 Thus, we a will be estimating the generative model in (8) using the non-normalized likelihood in (7). [sent-160, score-0.127]
</p><p>48 The score function of the model in (7 is given by n T wk g(wk x),  ψ(x; W) =  (9)  k=1  where the scalar nonlinear function g is given by π π g(s) = − tanh( √ s). [sent-163, score-0.738]
</p><p>49 (10)  t=1  We performed simulations to validate the consistency of score matching estimation, and to compare its eﬃciency with respect to maximum likelihood estimation. [sent-165, score-0.867]
</p><p>50 ˜ Score matching estimation consisted of minimizing J in (10) by a simple gradient descent; likelihood was maximized using a natural gradient method (Amari et al. [sent-167, score-0.614]
</p><p>51 We repeated the estimation for several diﬀerent sample sizes: 500, 1000, 2000, 4000, 8000, and 16000. [sent-170, score-0.102]
</p><p>52 For each sample size, the estimation was repeated 11 times using diﬀerent random initial points in the optimization, and diﬀerent random data sets. [sent-171, score-0.102]
</p><p>53 For each sample size and estimator type (score matching vs. [sent-178, score-0.361]
</p><p>54 The error of score matching seems to go to zero, which validates the theoretical consistency result of Theorem 2. [sent-181, score-0.799]
</p><p>55 Score matching gives slightly larger errors than maximum likelihood, which is to be expected because of the eﬃciency results of maximum likelihood estimation (Pham and Garrat, 1997). [sent-182, score-0.485]
</p><p>56 In the preceding simulation, we knew exactly the proper function g to be used in the score function. [sent-183, score-0.469]
</p><p>57 To investigate the robustness of the method to misspeciﬁcation of the score 701  ¨ Hyvarinen  -0. [sent-184, score-0.424]
</p><p>58 4  Figure 1: The estimation errors of score matching (solid line) compared with errors of maximum likelihood estimation (dashed line) for the basic ICA model. [sent-201, score-1.057]
</p><p>59 function (a well-known problem in ICA estimation), we ran the same estimation methods, score matching and maximum likelihood, for data that was generated by a slightly diﬀerent distribution. [sent-204, score-0.818]
</p><p>60 Interestingly, it now performs slightly better than maximum likelihood estimation (which would more properly be called quasi-maximum likelihood estimation due to the misspeciﬁcation (Pham and Garrat, 1997)). [sent-210, score-0.34]
</p><p>61 3 Estimation of an Overcomplete Model for Image Data Finally, we show image analysis results using an overcomplete version of the ICA model. [sent-212, score-0.207]
</p><p>62 , αn ),  log p(x) =  (11)  k=1  where the vectors wk = (wk1 , . [sent-223, score-0.326]
</p><p>63 4  Figure 2: The estimation errors of score matching compared with errors of maximum likelihood estimation for the basic ICA model. [sent-245, score-1.057]
</p><p>64 This time, the pdf of the independent components was slightly misspeciﬁed. [sent-246, score-0.265]
</p><p>65 and allowing the wk to have any norm, this becomes the basic ICA model of the preceding subsection. [sent-249, score-0.382]
</p><p>66 The model is related to ICA with overcomplete bases (Hyv¨rinen et al. [sent-250, score-0.212]
</p><p>67 In contrast to most ICA models, the overcompleteness is expressed as overcompleteness of ﬁlters wk which seems to make the problem a bit simpler because no latent variables need to be inferred. [sent-254, score-0.397]
</p><p>68 a We have the score function m T αk wk g(wk x),  ψ(x; W, α1 , . [sent-259, score-0.705]
</p><p>69 t=1  (12)  ¨ Hyvarinen  Figure 3: The overcomplete set of ﬁlters wi estimated from natural image data. [sent-264, score-0.31]
</p><p>70 a  We estimated the model for image patches of 8 × 8 pixels taken from natural images, see P. [sent-267, score-0.118]
</p><p>71 To show that the method correctly found diﬀerent vectors and not duplicates of a smaller set of vectors, we computed the dot-products between the vectors, and for each w i , we T selected the largest absolute value of dot-product |wi wj |, j = i. [sent-288, score-0.115]
</p><p>72 Since the vectors wi were normalized to unit norm, this shows that no two wi were close to equal, and we did ﬁnd m diﬀerent vectors. [sent-293, score-0.142]
</p><p>73 5  Figure 4: The distribution of maximal dot-products of a ﬁlter wi with all other ﬁlters, computed in the whitened space. [sent-308, score-0.125]
</p><p>74 The pdf is approximated by n  log ppseudo (x) = i=1  p(xi |x1 , . [sent-312, score-0.31]
</p><p>75 However, compared to score matching, this is a computationally expensive method since score matching avoids the need for numerical integration altogether. [sent-324, score-1.182]
</p><p>76 The question of consistency of pseudo-likelihood estimation seems to be unclear. [sent-325, score-0.185]
</p><p>77 Suﬃciently general consistency results on pseudo-likelihood estimation seem to be lacking. [sent-327, score-0.185]
</p><p>78 This is another disadvantage with respect to score matching, which was shown above to be (locally) consistent. [sent-328, score-0.424]
</p><p>79 2 Comparison with Contrastive Divergence An interesting approximative MCMC method called contrastive divergence was recently proposed by Hinton (2002). [sent-330, score-0.272]
</p><p>80 The basic principle is to use an MCMC method for computing the derivative of the logarithm of the normalization factor Z, but the MCMC is allowed to run for only a single iteration (or a few iterations) before doing the gradient step. [sent-331, score-0.173]
</p><p>81 Score matching is thus preferable if a consistent estimator is n´ wanted. [sent-333, score-0.361]
</p><p>82 The computational eﬃciency of contrastive divergence is diﬃcult to evaluate since it is not really a single method but a family of methods, depending on the MCMC method used. [sent-334, score-0.226]
</p><p>83 Nevertheless, contrastive divergence is a much more general method than score matching since it is applicable to intractable latent variable models. [sent-336, score-0.966]
</p><p>84 Extension of score matching to these two cases is an important problem for future research. [sent-338, score-0.716]
</p><p>85 3 Conclusion We have proposed a new method, score matching, to estimate statistical models in the case where the normalization constant is unknown. [sent-340, score-0.508]
</p><p>86 Although the estimation of the score function is computationally diﬃcult, we showed that the distance of data and model score functions is very easy to compute. [sent-341, score-1.013]
</p><p>87 The main assumptions in the method are: 1) all the variables are continuous-valued and deﬁned over Rn , 2) the model pdf is smooth enough. [sent-342, score-0.298]
</p><p>88 Score matching provides a computationally simple yet locally consistent alternative to existing methods, such as MCMC and various approximative methods. [sent-343, score-0.338]
</p><p>89 Proof of Theorem 1 Deﬁnition (2) gives J(θ) =  px (ξ)  1 ψ (ξ) 2 x  2  +  1 ψ(ξ; θ) 2  2  − ψ x (ξ)T ψ(ξ; θ) dξ. [sent-347, score-0.365]
</p><p>90 The integral of the second term is simply the integral 706  Estimation by Score Matching  of the sum of the second terms in brackets in (3). [sent-350, score-0.198]
</p><p>91 Thus, the diﬃcult thing to prove is that integral of the third term in brackets in (14) equals the integral of the sum of the ﬁrst terms in brackets in (3). [sent-351, score-0.311]
</p><p>92 This term equals −  px (ξ)ψx,i (ξ)ψi (ξ; θ)dξ, i  where ψx,i (ξ) denotes the i-th element of the vector ψ x (ξ). [sent-352, score-0.365]
</p><p>93 We can consider the integral for a single i separately, which equals −  px (ξ)  px (ξ) ∂px (ξ) ψi (ξ; θ)dξ = − px (ξ) ∂ξi  ∂ log px (ξ) ψi (ξ; θ)dξ = − ∂ξi  ∂px (ξ) ψi (ξ; θ)dξ. [sent-353, score-1.561]
</p><p>94 ∂ξi  The basic trick of partial integration needed the proof is simple: for any one-dimensional pdf p and any function f , we have p(x)(log p) (x)f (x)dx =  p(x)  p (x) f (x)dx = p(x)  p (x)f (x)dx = −  p(x)f (x)dx  under some regularity assumptions that will be dealt with below. [sent-354, score-0.404]
</p><p>95 Now, we can apply this lemma on px and ψ1 (ξ; θ) which were both assumed to be diﬀerentiable in the theorem, and we obtain: −  ∂px (ξ) ψ1 (ξ; θ)dξ1 d(ξ2 , . [sent-372, score-0.365]
</p><p>96 , ξn , θ because we assumed that px (ξ)ψ(ξ; θ) goes to zero at inﬁnity. [sent-395, score-0.365]
</p><p>97 Thus, we have proven that −  ∂px (ξ) ψi (ξ; θ)dξ = ∂ξi  ∂ψi (ξ; θ) px (ξ)dξ, ∂ξi  that is, integral of the the third term in brackets in (14) equals the integral of the sum of the ﬁrst terms in brackets in (3), and the proof of the theorem is complete. [sent-396, score-0.684]
</p><p>98 Then, the assumption q > 0 implies px (ξ) > 0 for all ξ, which implies that ψ x (. [sent-399, score-0.365]
</p><p>99 Estimating overcomplete independent component bases from a image windows. [sent-462, score-0.232]
</p><p>100 Sparse coding with an overcomplete basis set: A strategy employed by V1? [sent-486, score-0.183]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('score', 0.424), ('px', 0.365), ('matching', 0.292), ('wk', 0.281), ('pdf', 0.265), ('contrastive', 0.184), ('di', 0.173), ('erent', 0.173), ('hyv', 0.164), ('hyvarinen', 0.161), ('overcomplete', 0.154), ('rinen', 0.151), ('ica', 0.138), ('wj', 0.115), ('mcmc', 0.109), ('estimation', 0.102), ('mii', 0.092), ('brackets', 0.086), ('consistency', 0.083), ('teh', 0.077), ('wi', 0.071), ('estimator', 0.069), ('erentiable', 0.069), ('extremum', 0.069), ('likelihood', 0.068), ('mm', 0.062), ('pham', 0.062), ('garrat', 0.058), ('wki', 0.058), ('lters', 0.058), ('normalization', 0.058), ('integral', 0.056), ('whitened', 0.054), ('image', 0.053), ('misspeci', 0.051), ('hoyer', 0.051), ('gradient', 0.049), ('approximative', 0.046), ('besag', 0.046), ('bouman', 0.046), ('inki', 0.046), ('nonnormalized', 0.046), ('overcompleteness', 0.046), ('preceding', 0.045), ('log', 0.045), ('logarithm', 0.043), ('hinton', 0.043), ('divergence', 0.042), ('integration', 0.042), ('markov', 0.037), ('squared', 0.037), ('proven', 0.035), ('rn', 0.035), ('wn', 0.034), ('density', 0.034), ('model', 0.033), ('maximized', 0.032), ('estimated', 0.032), ('dx', 0.031), ('finland', 0.031), ('helsinki', 0.031), ('olshausen', 0.031), ('cosh', 0.031), ('carlo', 0.03), ('distance', 0.03), ('monte', 0.029), ('modelling', 0.029), ('derivatives', 0.029), ('coding', 0.029), ('obviously', 0.029), ('objective', 0.029), ('multivariate', 0.028), ('thing', 0.027), ('academy', 0.027), ('aapo', 0.027), ('elds', 0.027), ('vision', 0.027), ('models', 0.026), ('partial', 0.026), ('estimating', 0.026), ('densities', 0.025), ('su', 0.025), ('wa', 0.025), ('regularity', 0.025), ('bases', 0.025), ('blind', 0.025), ('tanh', 0.025), ('chain', 0.024), ('latent', 0.024), ('notational', 0.024), ('law', 0.024), ('trick', 0.023), ('basic', 0.023), ('errors', 0.023), ('analytical', 0.023), ('field', 0.022), ('resort', 0.022), ('lter', 0.022), ('maximization', 0.022), ('minimizing', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="31-tfidf-1" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>Author: Aapo Hyvärinen</p><p>Abstract: One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data. Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence</p><p>2 0.16002508 <a title="31-tfidf-2" href="./jmlr-2005-Convergence_Theorems_for_Generalized_Alternating_Minimization_Procedures.html">23 jmlr-2005-Convergence Theorems for Generalized Alternating Minimization Procedures</a></p>
<p>Author: Asela Gunawardana, William Byrne</p><p>Abstract: The EM algorithm is widely used to develop iterative parameter estimation procedures for statistical models. In cases where these procedures strictly follow the EM formulation, the convergence properties of the estimation procedures are well understood. In some instances there are practical reasons to develop procedures that do not strictly fall within the EM framework. We study EM variants in which the E-step is not performed exactly, either to obtain improved rates of convergence, or due to approximations needed to compute statistics under a model family over which E-steps cannot be realized. Since these variants are not EM procedures, the standard (G)EM convergence results do not apply to them. We present an information geometric framework for describing such algorithms and analyzing their convergence properties. We apply this framework to analyze the convergence properties of incremental EM and variational EM. For incremental EM, we discuss conditions under these algorithms converge in likelihood. For variational EM, we show how the E-step approximation prevents convergence to local maxima in likelihood. Keywords: EM, variational EM, incremental EM, convergence, information geometry</p><p>3 0.13119859 <a title="31-tfidf-3" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>Author: Motoaki Kawanabe, Klaus-Robert Müller</p><p>Abstract: A blind separation problem where the sources are not independent, but have variance dependencies is discussed. For this scenario Hyv¨ rinen and Hurri (2004) proposed an algorithm which requires a no assumption on distributions of sources and no parametric model of dependencies between components. In this paper, we extend the semiparametric approach of Amari and Cardoso (1997) to variance dependencies and study estimating functions for blind separation of such dependent sources. In particular, we show that many ICA algorithms are applicable to the variance-dependent model as well under mild conditions, although they should in principle not. Our results indicate that separation can be done based only on normalized sources which are adjusted to have stationary variances and is not affected by the dependent activity levels. We also study the asymptotic distribution of the quasi maximum likelihood method and the stability of the natural gradient learning in detail. Simulation results of artiﬁcial and realistic examples match well with our theoretical ﬁndings. Keywords: blind source separation, variance dependencies, independent component analysis, semiparametric statistical models, estimating functions</p><p>4 0.12784609 <a title="31-tfidf-4" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>Author: Arthur Gretton, Ralf Herbrich, Alexander Smola, Olivier Bousquet, Bernhard Schölkopf</p><p>Abstract: We introduce two new functionals, the constrained covariance and the kernel mutual information, to measure the degree of independence of random variables. These quantities are both based on the covariance between functions of the random variables in reproducing kernel Hilbert spaces (RKHSs). We prove that when the RKHSs are universal, both functionals are zero if and only if the random variables are pairwise independent. We also show that the kernel mutual information is an upper bound near independence on the Parzen window estimate of the mutual information. Analogous results apply for two correlation-based dependence functionals introduced earlier: we show the kernel canonical correlation and the kernel generalised variance to be independence measures for universal kernels, and prove the latter to be an upper bound on the mutual information near independence. The performance of the kernel dependence functionals in measuring independence is veriﬁed in the context of independent component analysis. Keywords: independence, covariance operator, mutual information, kernel, Parzen window estimate, independent component analysis c 2005 Arthur Gretton, Ralf Herbrich, Alexander Smola, Olivier Bousquet and Bernhard Schölkopf . G RETTON , H ERBRICH , S MOLA , B OUSQUET AND S CHÖLKOPF</p><p>5 0.077875182 <a title="31-tfidf-5" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>Author: Luís B. Almeida</p><p>Abstract: When acquiring an image of a paper document, the image printed on the back page sometimes shows through. The mixture of the front- and back-page images thus obtained is markedly nonlinear, and thus constitutes a good real-life test case for nonlinear blind source separation. This paper addresses a difﬁcult version of this problem, corresponding to the use of “onion skin” paper, which results in a relatively strong nonlinearity of the mixture, which becomes close to singular in the lighter regions of the images. The separation is achieved through the MISEP technique, which is an extension of the well known INFOMAX method. The separation results are assessed with objective quality measures. They show an improvement over the results obtained with linear separation, but have room for further improvement. Keywords: ICA, blind source separation, nonlinear mixtures, nonlinear separation, image mixture, image separation</p><p>6 0.076587476 <a title="31-tfidf-6" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>7 0.071597613 <a title="31-tfidf-7" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>8 0.06988477 <a title="31-tfidf-8" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>9 0.068964772 <a title="31-tfidf-9" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>10 0.064712904 <a title="31-tfidf-10" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>11 0.05899772 <a title="31-tfidf-11" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>12 0.058984268 <a title="31-tfidf-12" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>13 0.046313088 <a title="31-tfidf-13" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>14 0.044844583 <a title="31-tfidf-14" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>15 0.044474185 <a title="31-tfidf-15" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>16 0.04368668 <a title="31-tfidf-16" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>17 0.043500248 <a title="31-tfidf-17" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>18 0.03897867 <a title="31-tfidf-18" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>19 0.037464757 <a title="31-tfidf-19" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>20 0.034801882 <a title="31-tfidf-20" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.24), (1, 0.195), (2, -0.326), (3, -0.08), (4, -0.084), (5, 0.031), (6, 0.004), (7, -0.372), (8, -0.058), (9, 0.061), (10, -0.23), (11, -0.0), (12, 0.074), (13, 0.124), (14, 0.064), (15, 0.066), (16, 0.052), (17, -0.092), (18, 0.104), (19, 0.037), (20, -0.003), (21, 0.013), (22, -0.025), (23, 0.029), (24, 0.036), (25, -0.028), (26, 0.012), (27, 0.045), (28, -0.066), (29, 0.015), (30, 0.057), (31, -0.059), (32, -0.04), (33, -0.109), (34, -0.01), (35, 0.049), (36, 0.044), (37, 0.044), (38, 0.173), (39, 0.091), (40, -0.127), (41, 0.069), (42, 0.083), (43, 0.092), (44, 0.04), (45, -0.127), (46, 0.044), (47, 0.058), (48, -0.032), (49, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97668898 <a title="31-lsi-1" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>Author: Aapo Hyvärinen</p><p>Abstract: One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data. Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence</p><p>2 0.61117005 <a title="31-lsi-2" href="./jmlr-2005-Convergence_Theorems_for_Generalized_Alternating_Minimization_Procedures.html">23 jmlr-2005-Convergence Theorems for Generalized Alternating Minimization Procedures</a></p>
<p>Author: Asela Gunawardana, William Byrne</p><p>Abstract: The EM algorithm is widely used to develop iterative parameter estimation procedures for statistical models. In cases where these procedures strictly follow the EM formulation, the convergence properties of the estimation procedures are well understood. In some instances there are practical reasons to develop procedures that do not strictly fall within the EM framework. We study EM variants in which the E-step is not performed exactly, either to obtain improved rates of convergence, or due to approximations needed to compute statistics under a model family over which E-steps cannot be realized. Since these variants are not EM procedures, the standard (G)EM convergence results do not apply to them. We present an information geometric framework for describing such algorithms and analyzing their convergence properties. We apply this framework to analyze the convergence properties of incremental EM and variational EM. For incremental EM, we discuss conditions under these algorithms converge in likelihood. For variational EM, we show how the E-step approximation prevents convergence to local maxima in likelihood. Keywords: EM, variational EM, incremental EM, convergence, information geometry</p><p>3 0.43293038 <a title="31-lsi-3" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>Author: Arthur Gretton, Ralf Herbrich, Alexander Smola, Olivier Bousquet, Bernhard Schölkopf</p><p>Abstract: We introduce two new functionals, the constrained covariance and the kernel mutual information, to measure the degree of independence of random variables. These quantities are both based on the covariance between functions of the random variables in reproducing kernel Hilbert spaces (RKHSs). We prove that when the RKHSs are universal, both functionals are zero if and only if the random variables are pairwise independent. We also show that the kernel mutual information is an upper bound near independence on the Parzen window estimate of the mutual information. Analogous results apply for two correlation-based dependence functionals introduced earlier: we show the kernel canonical correlation and the kernel generalised variance to be independence measures for universal kernels, and prove the latter to be an upper bound on the mutual information near independence. The performance of the kernel dependence functionals in measuring independence is veriﬁed in the context of independent component analysis. Keywords: independence, covariance operator, mutual information, kernel, Parzen window estimate, independent component analysis c 2005 Arthur Gretton, Ralf Herbrich, Alexander Smola, Olivier Bousquet and Bernhard Schölkopf . G RETTON , H ERBRICH , S MOLA , B OUSQUET AND S CHÖLKOPF</p><p>4 0.41966072 <a title="31-lsi-4" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>Author: Motoaki Kawanabe, Klaus-Robert Müller</p><p>Abstract: A blind separation problem where the sources are not independent, but have variance dependencies is discussed. For this scenario Hyv¨ rinen and Hurri (2004) proposed an algorithm which requires a no assumption on distributions of sources and no parametric model of dependencies between components. In this paper, we extend the semiparametric approach of Amari and Cardoso (1997) to variance dependencies and study estimating functions for blind separation of such dependent sources. In particular, we show that many ICA algorithms are applicable to the variance-dependent model as well under mild conditions, although they should in principle not. Our results indicate that separation can be done based only on normalized sources which are adjusted to have stationary variances and is not affected by the dependent activity levels. We also study the asymptotic distribution of the quasi maximum likelihood method and the stability of the natural gradient learning in detail. Simulation results of artiﬁcial and realistic examples match well with our theoretical ﬁndings. Keywords: blind source separation, variance dependencies, independent component analysis, semiparametric statistical models, estimating functions</p><p>5 0.27772421 <a title="31-lsi-5" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>Author: Ofer Dekel, Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We describe new loss functions for regression problems along with an accompanying algorithmic framework which utilizes these functions. These loss functions are derived by symmetrization of margin-based losses commonly used in boosting algorithms, namely, the logistic loss and the exponential loss. The resulting symmetric logistic loss can be viewed as a smooth approximation to the ε-insensitive hinge loss used in support vector regression. We describe and analyze two parametric families of batch learning algorithms for minimizing these symmetric losses. The ﬁrst family employs an iterative log-additive update which can be viewed as a regression counterpart to recent boosting algorithms. The second family utilizes an iterative additive update step. We also describe and analyze online gradient descent (GD) and exponentiated gradient (EG) algorithms for the symmetric logistic loss. A byproduct of our work is a new simple form of regularization for boosting-based classiﬁcation and regression algorithms. Our regression framework also has implications on classiﬁcation algorithms, namely, a new additive update boosting algorithm for classiﬁcation. We demonstrate the merits of our algorithms in a series of experiments.</p><p>6 0.26124492 <a title="31-lsi-6" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>7 0.25599232 <a title="31-lsi-7" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>8 0.24369437 <a title="31-lsi-8" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>9 0.21604654 <a title="31-lsi-9" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>10 0.21294859 <a title="31-lsi-10" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>11 0.21081445 <a title="31-lsi-11" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>12 0.20591661 <a title="31-lsi-12" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>13 0.1952242 <a title="31-lsi-13" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>14 0.17343159 <a title="31-lsi-14" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>15 0.16758347 <a title="31-lsi-15" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<p>16 0.16058111 <a title="31-lsi-16" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>17 0.15442502 <a title="31-lsi-17" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>18 0.15273976 <a title="31-lsi-18" href="./jmlr-2005-A_Unifying_View_of_Sparse_Approximate_Gaussian_Process_Regression.html">7 jmlr-2005-A Unifying View of Sparse Approximate Gaussian Process Regression</a></p>
<p>19 0.14777522 <a title="31-lsi-19" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>20 0.13608406 <a title="31-lsi-20" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.02), (17, 0.031), (19, 0.027), (36, 0.038), (37, 0.027), (42, 0.015), (43, 0.05), (47, 0.017), (49, 0.412), (52, 0.071), (59, 0.019), (70, 0.045), (80, 0.01), (88, 0.069), (90, 0.06), (94, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.67740452 <a title="31-lda-1" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>Author: Aapo Hyvärinen</p><p>Abstract: One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data. Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence</p><p>2 0.30962113 <a title="31-lda-2" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>Author: Arthur Gretton, Ralf Herbrich, Alexander Smola, Olivier Bousquet, Bernhard Schölkopf</p><p>Abstract: We introduce two new functionals, the constrained covariance and the kernel mutual information, to measure the degree of independence of random variables. These quantities are both based on the covariance between functions of the random variables in reproducing kernel Hilbert spaces (RKHSs). We prove that when the RKHSs are universal, both functionals are zero if and only if the random variables are pairwise independent. We also show that the kernel mutual information is an upper bound near independence on the Parzen window estimate of the mutual information. Analogous results apply for two correlation-based dependence functionals introduced earlier: we show the kernel canonical correlation and the kernel generalised variance to be independence measures for universal kernels, and prove the latter to be an upper bound on the mutual information near independence. The performance of the kernel dependence functionals in measuring independence is veriﬁed in the context of independent component analysis. Keywords: independence, covariance operator, mutual information, kernel, Parzen window estimate, independent component analysis c 2005 Arthur Gretton, Ralf Herbrich, Alexander Smola, Olivier Bousquet and Bernhard Schölkopf . G RETTON , H ERBRICH , S MOLA , B OUSQUET AND S CHÖLKOPF</p><p>3 0.29702887 <a title="31-lda-3" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>Author: Cheng Soon Ong, Alexander J. Smola, Robert C. Williamson</p><p>Abstract: This paper addresses the problem of choosing a kernel suitable for estimation with a support vector machine, hence further automating machine learning. This goal is achieved by deﬁning a reproducing kernel Hilbert space on the space of kernels itself. Such a formulation leads to a statistical estimation problem similar to the problem of minimizing a regularized risk functional. We state the equivalent representer theorem for the choice of kernels and present a semideﬁnite programming formulation of the resulting optimization problem. Several recipes for constructing hyperkernels are provided, as well as the details of common machine learning problems. Experimental results for classiﬁcation, regression and novelty detection on UCI data show the feasibility of our approach. Keywords: learning the kernel, capacity control, kernel methods, support vector machines, representer theorem, semideﬁnite programming</p><p>4 0.29122341 <a title="31-lda-4" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>Author: Wei Chu, Zoubin Ghahramani</p><p>Abstract: We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative ﬁltering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach. Keywords: Gaussian processes, ordinal regression, approximate Bayesian inference, collaborative ﬁltering, gene expression analysis, feature selection</p><p>5 0.289211 <a title="31-lda-5" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>Author: Ofer Dekel, Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We describe new loss functions for regression problems along with an accompanying algorithmic framework which utilizes these functions. These loss functions are derived by symmetrization of margin-based losses commonly used in boosting algorithms, namely, the logistic loss and the exponential loss. The resulting symmetric logistic loss can be viewed as a smooth approximation to the ε-insensitive hinge loss used in support vector regression. We describe and analyze two parametric families of batch learning algorithms for minimizing these symmetric losses. The ﬁrst family employs an iterative log-additive update which can be viewed as a regression counterpart to recent boosting algorithms. The second family utilizes an iterative additive update step. We also describe and analyze online gradient descent (GD) and exponentiated gradient (EG) algorithms for the symmetric logistic loss. A byproduct of our work is a new simple form of regularization for boosting-based classiﬁcation and regression algorithms. Our regression framework also has implications on classiﬁcation algorithms, namely, a new additive update boosting algorithm for classiﬁcation. We demonstrate the merits of our algorithms in a series of experiments.</p><p>6 0.28879738 <a title="31-lda-6" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>7 0.28869677 <a title="31-lda-7" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>8 0.28695825 <a title="31-lda-8" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>9 0.28669757 <a title="31-lda-9" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>10 0.28543788 <a title="31-lda-10" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>11 0.28242883 <a title="31-lda-11" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>12 0.2804864 <a title="31-lda-12" href="./jmlr-2005-Frames%2C_Reproducing_Kernels%2C_Regularization_and_Learning.html">35 jmlr-2005-Frames, Reproducing Kernels, Regularization and Learning</a></p>
<p>13 0.27914694 <a title="31-lda-13" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>14 0.27913797 <a title="31-lda-14" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>15 0.27809694 <a title="31-lda-15" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>16 0.27749771 <a title="31-lda-16" href="./jmlr-2005-Clustering_with_Bregman_Divergences.html">20 jmlr-2005-Clustering with Bregman Divergences</a></p>
<p>17 0.27739885 <a title="31-lda-17" href="./jmlr-2005-Learning_the_Kernel_Function_via_Regularization.html">48 jmlr-2005-Learning the Kernel Function via Regularization</a></p>
<p>18 0.27554706 <a title="31-lda-18" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>19 0.27522641 <a title="31-lda-19" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>20 0.27492759 <a title="31-lda-20" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
