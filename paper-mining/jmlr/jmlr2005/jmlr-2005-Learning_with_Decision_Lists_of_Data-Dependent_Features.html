<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>50 jmlr-2005-Learning with Decision Lists of Data-Dependent Features</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-50" href="#">jmlr2005-50</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>50 jmlr-2005-Learning with Decision Lists of Data-Dependent Features</h1>
<br/><p>Source: <a title="jmlr-2005-50-pdf" href="http://jmlr.org/papers/volume6/marchand05a/marchand05a.pdf">pdf</a></p><p>Author: Mario Marchand, Marina Sokolova</p><p>Abstract: We present a learning algorithm for decision lists which allows features that are constructed from the data and allows a trade-off between accuracy and complexity. We provide bounds on the generalization error of this learning algorithm in terms of the number of errors and the size of the classiﬁer it ﬁnds on the training data. We also compare its performance on some natural data sets with the set covering machine and the support vector machine. Furthermore, we show that the proposed bounds on the generalization error provide effective guides for model selection. Keywords: decision list machines, set covering machines, sparsity, data-dependent features, sample compression, model selection, learning theory</p><p>Reference: <a title="jmlr-2005-50-reference" href="../jmlr2005_reference/jmlr-2005-Learning_with_Decision_Lists_of_Data-Dependent_Features_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We will see that the proposed learning algorithm for the DLM with data-dependent features is effectively compressing the training data into a small subset of examples which is called the compression set. [sent-35, score-0.41]
</p><p>2 The proposed bound will apply to any compression set-dependent distribution of messages (see the deﬁnition in Section 4) and allows for the message set to be of variable size (in contrast with the sample compression bound of Littlestone and Warmuth (1986) that requires ﬁxed size). [sent-37, score-0.764]
</p><p>3 Else If (hr (x)) then br Else br+1 , where each bi ∈ {0, 1} deﬁnes the output of f (x) if and only if hi is the ﬁrst feature to be satisﬁed on x (i. [sent-49, score-0.313]
</p><p>4 To compute a conjunction of hi s, we simply place in f the negation of each hi with bi = 0 for i = 1 . [sent-57, score-0.327]
</p><p>5 With this prescription, given any example x, we always have that r  sgn  ∑ wi hi (x) − θ  i=1  = 2bk − 1,  where k is the smallest integer for which hk (x) = 1 in the DLM or k = r + 1 if hi (x) = 0 ∀i ∈ {1, . [sent-76, score-0.336]
</p><p>6 For a given set S = P ∪ N of examples (where P ⊆ P and N ⊆ N) and a given set H of features, consider only the features hi ∈ H which either have hi (x) = 0 for all x ∈ P or hi (x) = 0 for all x ∈ N . [sent-83, score-0.505]
</p><p>7 Let Qi be the subset of examples on which hi = 1 (our constraint on the choice of hi implies that Qi contains only examples having the same class label). [sent-84, score-0.34]
</p><p>8 We therefore deﬁne the usefulness Ui of feature hi by def  Ui = max {|Pi | − pn |Ni |, |Ni | − p p |Pi |} , where pn denotes the penalty of making an error on a negative example whereas p p denotes the penalty of making an error on a positive example. [sent-99, score-0.361]
</p><p>9 Indeed, whenever we add to a DLM a feature hi for which Pi and Ni are both non empty, the output bi associated with hi will be 1 if |Pi | − pn |Ni | ≥ |Ni | − p p |Pi | or 0 otherwise. [sent-100, score-0.408]
</p><p>10 Output: A decision list f consisting of an ordered set R = {(hi , bi )}r of features hi with their i=1 corresponding output values bi , and a default value br+1 . [sent-113, score-0.393]
</p><p>11 Hence, a ball is a feature that covers the examples that are located inside the ball; whereas a hole covers the examples that are located outside. [sent-166, score-0.311]
</p><p>12 Hence, each ball and hole is identiﬁed by only two training examples: a center c and a border b that identiﬁes the radius with d(c, b). [sent-174, score-0.507]
</p><p>13 To avoid having examples directly on the decision surface of the DLM, the radius ρ of a ball of center c will always be given by ρ = d(c, b) − ε for some training example b chosen for the border and some ﬁxed and very small positive value ε. [sent-185, score-0.485]
</p><p>14 We have not chosen to assign the radius values “in between” two training example since this would have required three examples per ball and hole and would have decreased substantially the tightness of our generalization error bound without providing a signiﬁcant increase of discriminative power. [sent-187, score-0.352]
</p><p>15 Whenever a half-space hc is chosen to be appended to the DLM, we must also provide an a,b output value b which will be the output of the DLM on example x when hc is the ﬁrst feature of a,b the DLM having hc (x) = 1. [sent-203, score-0.383]
</p><p>16 Hence, in this section, we provide a general risk bound that depends on the number of examples that are used in the ﬁnal classiﬁer and the size of the information message needed to identify the ﬁnal classiﬁer from the compression set. [sent-211, score-0.506]
</p><p>17 Later, we apply this general risk bound to DLMs by making appropriate choices for a compression set-dependent distribution of messages. [sent-215, score-0.359]
</p><p>18 This implies that there exists a reconstruction function R , associated to A, that outputs a classiﬁer R (σ, zi ) when given an arbitrary compression set zi and message string σ chosen from the set M (zi ) of all distinct messages that can be supplied to R with the compression set zi . [sent-224, score-2.007]
</p><p>19 It is only when such an R exists that the classiﬁer returned by A(S) is always identiﬁed by a compression set zi and a message string σ. [sent-225, score-0.834]
</p><p>20 Given a training set S, the compression set zi is deﬁned by a vector i of indices such that def  i = (i1 , i2 , . [sent-226, score-0.737]
</p><p>21 In contrast, we will see in the next section that the reconstruction function for DLMs needs both a compression set and a message string. [sent-238, score-0.407]
</p><p>22 We seek a tight risk bound for arbitrary reconstruction functions that holds uniformly for all compression sets and message strings. [sent-239, score-0.51]
</p><p>23 Recall that classiﬁer A(zm ) is described entirely in terms of a compression set zi ⊂ zm and a message string σ ∈ M (zi ). [sent-253, score-0.876]
</p><p>24 Let M (zi ) be the set of all messages σ that can be attached to compression set zi . [sent-255, score-0.737]
</p><p>25 (3)  We will now stratify PZi |Zi (R(R (σ, Zi )) > ε) in terms of the errors that R (σ, Zi ) can make on the training examples that are not used for the compression set. [sent-260, score-0.319]
</p><p>26 Given a training sample zm and a compression set zi , we denote by Rzi ( f ) the vector of indices pointing to the examples in zi which are misclassiﬁed by f . [sent-262, score-1.153]
</p><p>27 Given any compression set zi , let us now use any function PM (zi ) (σ) which has the property that  ∑  σ∈M (zi )  PM (zi ) (σ) ≤ 1  (7)  and can, therefore, be interpreted as compression set-dependent distribution of messages when it sums to one. [sent-266, score-0.993]
</p><p>28 However, a reconstruction function will generally need less additional information when it is constrained to produce a classiﬁer making no errors with the compression set. [sent-273, score-0.319]
</p><p>29 Hence, the above risk bound will generally be smaller for sample-compression learning algorithms that always return a classiﬁer making no errors on the compression set. [sent-274, score-0.359]
</p><p>30 Finally note that the risk bound is small for classiﬁers making a small number |j| of training errors, having a small compression set size |i|, and having a message string σ with large prior “probability” PM (Zi ) (σ). [sent-276, score-0.568]
</p><p>31 Comparison with Other Risk Bounds Although the risk bound of Theorem 1 is basically a sample compression bound it, nevertheless, applies to a much broader class of learning algorithms than just sample compression learning algorithms. [sent-279, score-0.643]
</p><p>32 Indeed the risk bound depends on two complementary sources of information used to identify the classiﬁer: the sample compression set zi and the message string σ. [sent-280, score-0.937]
</p><p>33 In fact, the bound still holds when the sample compression set vanishes and when the classiﬁer h = R (σ) is described entirely in terms of a message string σ. [sent-281, score-0.466]
</p><p>34 1 Comparison with Data-Independent Bounds The risk bound of Theorem 1 can be qualiﬁed as “data-dependent” when the learning algorithm is searching among a class of functions (classiﬁers) described in terms of a subset zi of the training set. [sent-284, score-0.526]
</p><p>35 Consequently, the bound of Theorem 1 is a generalization of the Occam’s razor bound to the case where the classiﬁers are identiﬁed by two complementary sources of information: the message string σ and the compression set zi . [sent-293, score-0.89]
</p><p>36 The proposed bound is obtained by using a union bound over the possible compression subsets of the training set and over the possible messages σ ∈ M (zi ). [sent-294, score-0.424]
</p><p>37 The ln(m + 1) terms are absent from the Littlestone and Warmuth compression bounds because their bounds hold uniformly for all compression sets of a ﬁxed size |i| and for all conﬁgurations of training error points of a ﬁxed amount |j|. [sent-313, score-0.585]
</p><p>38 The bound of Theorem 1 holds uniformly for all compression sets of arbitrary sizes and for all conﬁgurations of training error points of an arbitrary amount. [sent-316, score-0.311]
</p><p>39 When the classiﬁer is allowed to make training errors, the bound of Equation 14 is tighter than the lossy compression bound of Theorem 5. [sent-326, score-0.339]
</p><p>40 Consequently, the bound of Theorem 1 is tighter than the above-mentioned sample compression bounds for three reasons. [sent-328, score-0.307]
</p><p>41 Third, in contrast with the other sample compression bounds, the bound of Theorem 1 is valid for any a priori deﬁned sample compression-dependent distribution of messages PM (zi ) (σ). [sent-331, score-0.369]
</p><p>42 Indeed, we feel that it is important to allow the set of possible messages and the message set size to depend on the sample compression zi since the class labels of the compression set examples give information about the set of possible data-dependent features that can be constructed from zi . [sent-333, score-1.603]
</p><p>43 Indeed, it is conceivable that for some zi , very little extra information may be needed to identify the classiﬁer whereas for some other zi , more information may be needed. [sent-334, score-0.792]
</p><p>44 Consider, for example, the case where the compression set consists of two examples that are used by the reconstruction function R to obtain a single-ball classiﬁer. [sent-335, score-0.332]
</p><p>45 For the reconstruction function of the set covering machine (described in the next section), a ball border must be a positive example whereas both positive and negative examples are allowed for ball centers. [sent-336, score-0.484]
</p><p>46 In that case, if the two examples in the compression set have a positive label, the reconstruction function needs a message string of at least one bit that indicates which example is the ball center. [sent-337, score-0.613]
</p><p>47 If the two examples have opposite class labels, then the negative example is necessarily the ball center and no message at all is needed to reconstruct the classiﬁer. [sent-338, score-0.351]
</p><p>48 More generally, the set of messages that we use for all types of DLMs proposed in this paper depends on some properties of zi like its number n(zi ) of negative examples. [sent-339, score-0.481]
</p><p>49 Without such a dependency on zi , the set of possible messages M could be unnecessarily too large and would then loosen the risk bound. [sent-340, score-0.556]
</p><p>50 3 Comparison with the Set Covering Machine Risk Bound The risk bound for the set covering machine (SCM) (Marchand and Shawe-Taylor, 2001, 2002) is not a general-purposed sample compression risk bound as the one provided by Theorem 1. [sent-342, score-0.495]
</p><p>51 For the case of data-dependent balls and holes, each feature is identiﬁed by a training example called a center (xc , yc ), and another training example called a border (xb , yb ). [sent-349, score-0.477]
</p><p>52 h(x) =  In this case, given a compression set zi , we need to specify the examples in zi that are used for a border point without being used as a center. [sent-351, score-1.29]
</p><p>53 As explained by Marchand and Shawe-Taylor (2001), no additional amount of information is required to pair each center with its border point whenever the reconstruction function R is constrained to produce a classiﬁer that always correctly classiﬁes the compression set. [sent-352, score-0.6]
</p><p>54 In that case, each message σ ∈ M (zi ) just needs to specify the positive examples that are a border point without being a center. [sent-354, score-0.353]
</p><p>55 Let n(zi ) and p(zi ) be, respectively, the number of negative and the number of positive examples in compression set zi . [sent-355, score-0.688]
</p><p>56 Let b(σ) be the number of border point examples speciﬁed in message σ and let ζ(a) be deﬁned by Equation 9. [sent-356, score-0.324]
</p><p>57 We can then use PM (Zi ) (σ) = ζ(b(σ)) ·  p(zi ) b(σ)  −1  (15)  since, in that case, we have for any compression set zi :  ∑  σ∈M (zi )  p(zi )  PM (zi ) (σ) =  ∑ ζ(b) ∑  σ:b(σ)=b  b=0  p(zi ) b(σ)  −1  ≤ 1. [sent-357, score-0.652]
</p><p>58 1 DLMs Containing Only Balls Even in this simplest case, the compression set zi alone does not contain enough information to identify a DLM classiﬁer (the hypothesis). [sent-371, score-0.652]
</p><p>59 Recall that, in this case, zi contains ball centers and border points. [sent-373, score-0.721]
</p><p>60 Moreover, each center can only be the center of one ball since the center is removed from the data when a ball is added to the DLM. [sent-375, score-0.444]
</p><p>61 But a center can be the border of a previous ball in the DLM and a border can be the border of more than one ball (since the border of a ball is not removed from the data when that ball is added to the DLM). [sent-376, score-1.186]
</p><p>62 Hence, σ needs to specify the border points in zi that are a border without being the center of another ball. [sent-377, score-0.861]
</p><p>63 Since we expect that most of the compression sets will contain roughly the same number of centers and borders, we assign, to each example of zi , an equal a priori probability to be a center or a border. [sent-379, score-0.783]
</p><p>64 441  M ARCHAND AND S OKOLOVA  Once σ1 is speciﬁed, the centers and borders of zi are identiﬁed. [sent-383, score-0.445]
</p><p>65 But to identify each ball we need to pair each center with a border point (which could possibly be the center of another ball). [sent-384, score-0.44]
</p><p>66 For this operation, recall that the border and the center of each ball must have opposite class labels. [sent-385, score-0.381]
</p><p>67 Let n(zi ) and p(zi ) be, respectively, the number of negative and the number of positive examples in compression set zi . [sent-387, score-0.688]
</p><p>68 Since a border point can be used for more that one ball and a center can also be used as a border, we assign an equal probability of 1/n(zi ) to each negative example of zi to be paired with x. [sent-389, score-0.794]
</p><p>69 Similarly, we assign an equal probability of 1/p(zi ) to each positive example to be paired with a negative center of zi . [sent-390, score-0.518]
</p><p>70 Let c p (zi ) and cn (zi ) be, respectively, the number of positive centers and negative centers in zi (this is known once σ1 is speciﬁed). [sent-391, score-0.494]
</p><p>71 For an arbitrary compression set zi , we thus assign the following a priori probability to each possible pairing information string σ2 : c p (zi )  1 n(zi )  P2 (σ2 |σ1 ) =  cn (zi )  1 p(zi )  ∀σ2 | n(zi ) = 0 and p(zi ) = 0. [sent-392, score-0.763]
</p><p>72 However, since the border and center of each ball must have opposite labels, this condition is the same as |i| = 0. [sent-394, score-0.381]
</p><p>73 For an arbitrary compression set zi , we assign an equal a priori probability to each possible ball ordering by using P3 (σ3 |σ2 , σ1 ) =  1 r(zi )! [sent-401, score-0.791]
</p><p>74 Indeed, in the worst case, each pair of (distinct) examples taken from the compression 442  D ECISION L ISTS OF DATA -D EPENDENT F EATURES  set zi could be used for two holes: giving a total of |i|(|i| − 1) features. [sent-410, score-0.688]
</p><p>75 The ﬁrst part σ1 of the (complete) message string σ will specify the number r(zi ) of features present in compression set zi . [sent-411, score-0.93]
</p><p>76 For this, we give an equal probability of 1/|i| to each example in zi of being chosen (whenever |i| = 0). [sent-422, score-0.396]
</p><p>77 3 Constrained DLMs Containing Only Balls A constrained DLM is a DLM that has the property of correctly classifying each example of its compression set zi with the exception of the compression set examples who’s output is determined by the default value. [sent-427, score-0.989]
</p><p>78 Every training example (x, y) ∈ Qi that is either a border point of a previous feature (ball or hole) in the DLM or a center of a previous hole in the DLM must have y = bi and thus be correctly classiﬁed by hi . [sent-430, score-0.641]
</p><p>79 As before, we use a string σ1 , with the same probability P1 (σ1 ) = 2−|i| ∀σ1 to specify if each example of the compression set zi is a center or a border point. [sent-435, score-1.011]
</p><p>80 ∀σ2 where r(zi ) denotes the number of balls for zi (an information given by σ1 ). [sent-440, score-0.519]
</p><p>81 But now, since each feature was constrained to correctly classify the examples of zi that it covers (and which were not covered by the features above), we do not need any additional information to specify the border for each center. [sent-441, score-0.821]
</p><p>82 Given a compression set zi , let P and N denote, respectively, the set of positive and the set of negative 443  M ARCHAND AND S OKOLOVA  examples in zi . [sent-443, score-1.084]
</p><p>83 If center c is positive, then its border b is given by argminx∈N d(c, x) and we remove from P (to ﬁnd the border of the other balls) the center c and all other positive examples covered by that feature and used by the previous features. [sent-445, score-0.654]
</p><p>84 If center c is negative, then its border b is given by argminx∈P d(c, x) and we remove from N the center c and all other negative examples covered by that feature and used by the previous features. [sent-446, score-0.477]
</p><p>85 2, we use a string σ1 to specify the number r(zi ) of features present in compression set zi . [sent-451, score-0.819]
</p><p>86 Here, however, we only need to specify the center of each feature, since, as we will see below, no additional information is needed to ﬁnd the border of each feature when the DLM is constrained to classify correctly each example in zi . [sent-455, score-0.77]
</p><p>87 Given a compression set zi , let P and N denote, respectively, the set of positive and the set of negative examples in zi . [sent-458, score-1.084]
</p><p>88 If the feature is a ball with a positive center c, then its border is given by argminx∈N d(c, x) and we remove from P the center c and all other positive examples covered by that feature and used by the previous features. [sent-460, score-0.617]
</p><p>89 If the feature is a hole with a positive center c, then its border is given by argmaxx∈P −{c} d(c, x) and we remove from N all the negative examples covered by that feature and used by the previous features. [sent-461, score-0.535]
</p><p>90 If the feature is a ball with a negative center c, then its border is given by argminx∈P d(c, x) and we remove from N the center c and all other negative examples covered by that feature and used by the previous features. [sent-462, score-0.617]
</p><p>91 If the feature is a hole with a negative center c, then its border is given by argmaxx∈N −{c} d(c, x) and we remove from P all the positive examples covered by that feature and used by the previous features. [sent-463, score-0.535]
</p><p>92 The ﬁrst part of the message will be a string σ1 that speciﬁes the number r(zi ) of half-spaces used in the compression set zi . [sent-467, score-0.834]
</p><p>93 As before, let p(zi ) and n(zi ) denote, respectively, the number of positive examples and the number of negative examples in the compression set zi . [sent-468, score-0.724]
</p><p>94 Let P(zi ) and N(zi ) denote, respectively, the set of positive examples and the set of negative examples in the compression set zi . [sent-469, score-0.724]
</p><p>95 Therefore, for the string σ1 that speciﬁes the number r(zi ) of half-spaces used in the compression set zi , we could assign the same probability to each number between zero and |i|p(zi )n(zi ). [sent-474, score-0.763]
</p><p>96 Finally, as for the other constrained DLMs, we do not need any additional message string to identify the threshold point xc ∈ zi for each w of the DLM. [sent-483, score-0.682]
</p><p>97 Let P and N denote, respectively, the set of positive and the set of negative examples in zi . [sent-485, score-0.432]
</p><p>98 5, we use a string σ1 to specify the number r(zi ) of half-spaces present in compression set zi . [sent-493, score-0.752]
</p><p>99 For this, we give an equal probability of 1/|i| to each example in zi of being chosen (when |i| = 0). [sent-498, score-0.396]
</p><p>100 We have proposed a general risk bound that depends on the number of examples that are used in the ﬁnal classiﬁer and the size of the information message needed to identify the ﬁnal classiﬁer from the compression set. [sent-847, score-0.506]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dlm', 0.565), ('zi', 0.396), ('dlms', 0.282), ('compression', 0.256), ('border', 0.177), ('scm', 0.134), ('hi', 0.134), ('balls', 0.123), ('marchand', 0.113), ('message', 0.111), ('err', 0.11), ('hc', 0.106), ('ball', 0.099), ('hole', 0.099), ('hsp', 0.099), ('archand', 0.092), ('holes', 0.092), ('pm', 0.088), ('messages', 0.085), ('eatures', 0.085), ('ecision', 0.085), ('ists', 0.085), ('okolova', 0.085), ('center', 0.082), ('br', 0.079), ('risk', 0.075), ('string', 0.071), ('ependent', 0.071), ('features', 0.067), ('ni', 0.06), ('bi', 0.059), ('def', 0.058), ('xc', 0.057), ('credit', 0.053), ('pi', 0.052), ('builddlm', 0.049), ('haberman', 0.049), ('msfrombound', 0.049), ('msfromcv', 0.049), ('centers', 0.049), ('ln', 0.049), ('lists', 0.047), ('xa', 0.046), ('xb', 0.044), ('er', 0.044), ('zm', 0.042), ('breastw', 0.042), ('bupa', 0.042), ('prunedlm', 0.042), ('pzm', 0.042), ('pima', 0.042), ('mario', 0.041), ('decision', 0.041), ('feature', 0.041), ('pn', 0.04), ('reconstruction', 0.04), ('assign', 0.04), ('qi', 0.04), ('examples', 0.036), ('bh', 0.036), ('disjunction', 0.036), ('votes', 0.036), ('pzi', 0.035), ('sokolova', 0.035), ('list', 0.033), ('covering', 0.033), ('glass', 0.032), ('littlestone', 0.031), ('covered', 0.03), ('specify', 0.029), ('remove', 0.029), ('bound', 0.028), ('argminx', 0.028), ('dlmb', 0.028), ('scms', 0.028), ('training', 0.027), ('rivest', 0.026), ('warmuth', 0.026), ('hk', 0.025), ('sequentially', 0.025), ('threshold', 0.024), ('classi', 0.024), ('penalty', 0.024), ('appended', 0.024), ('compressing', 0.024), ('append', 0.024), ('specialized', 0.024), ('opposite', 0.023), ('bounds', 0.023), ('radius', 0.023), ('constrained', 0.023), ('correctly', 0.022), ('heart', 0.022), ('wi', 0.022), ('dhagat', 0.021), ('dlmbh', 0.021), ('dlmhsp', 0.021), ('qk', 0.021), ('rzi', 0.021), ('sgn', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="50-tfidf-1" href="./jmlr-2005-Learning_with_Decision_Lists_of_Data-Dependent_Features.html">50 jmlr-2005-Learning with Decision Lists of Data-Dependent Features</a></p>
<p>Author: Mario Marchand, Marina Sokolova</p><p>Abstract: We present a learning algorithm for decision lists which allows features that are constructed from the data and allows a trade-off between accuracy and complexity. We provide bounds on the generalization error of this learning algorithm in terms of the number of errors and the size of the classiﬁer it ﬁnds on the training data. We also compare its performance on some natural data sets with the set covering machine and the support vector machine. Furthermore, we show that the proposed bounds on the generalization error provide effective guides for model selection. Keywords: decision list machines, set covering machines, sparsity, data-dependent features, sample compression, model selection, learning theory</p><p>2 0.11395214 <a title="50-tfidf-2" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>Author: Dmitry Rusakov, Dan Geiger</p><p>Abstract: We develop a closed form asymptotic formula to compute the marginal likelihood of data given a naive Bayesian network model with two hidden states and binary features. This formula deviates from the standard BIC score. Our work provides a concrete example that the BIC score is generally incorrect for statistical models that belong to stratiﬁed exponential families. This claim stands in contrast to linear and curved exponential families, where the BIC score has been proven to provide a correct asymptotic approximation for the marginal likelihood. Keywords: Bayesian networks, asymptotic model selection, Bayesian information criterion (BIC)</p><p>3 0.10023837 <a title="50-tfidf-3" href="./jmlr-2005-Stability_of_Randomized_Learning_Algorithms.html">67 jmlr-2005-Stability of Randomized Learning Algorithms</a></p>
<p>Author: Andre Elisseeff, Theodoros Evgeniou, Massimiliano Pontil</p><p>Abstract: We extend existing theory on stability, namely how much changes in the training data inﬂuence the estimated models, and generalization performance of deterministic learning algorithms to the case of randomized algorithms. We give formal deﬁnitions of stability for randomized algorithms and prove non-asymptotic bounds on the difference between the empirical and expected error as well as the leave-one-out and expected error of such algorithms that depend on their random stability. The setup we develop for this purpose can be also used for generally studying randomized learning algorithms. We then use these general results to study the effects of bagging on the stability of a learning method and to prove non-asymptotic bounds on the predictive performance of bagging which have not been possible to prove with the existing theory of stability for deterministic learning algorithms.1 Keywords: stability, randomized learning algorithms, sensitivity analysis, bagging, bootstrap methods, generalization error, leave-one-out error.</p><p>4 0.090010971 <a title="50-tfidf-4" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>Author: John Langford</p><p>Abstract: We discuss basic prediction theory and its impact on classiﬁcation success evaluation, implications for learning algorithm design, and uses in learning algorithm execution. This tutorial is meant to be a comprehensive compilation of results which are both theoretically rigorous and quantitatively useful. There are two important implications of the results presented here. The ﬁrst is that common practices for reporting results in classiﬁcation should change to use the test set bound. The second is that train set bounds can sometimes be used to directly motivate learning algorithms. Keywords: sample complexity bounds, classiﬁcation, quantitative bounds</p><p>5 0.086542323 <a title="50-tfidf-5" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>Author: Wei Chu, Zoubin Ghahramani</p><p>Abstract: We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative ﬁltering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach. Keywords: Gaussian processes, ordinal regression, approximate Bayesian inference, collaborative ﬁltering, gene expression analysis, feature selection</p><p>6 0.075165272 <a title="50-tfidf-6" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>7 0.063533813 <a title="50-tfidf-7" href="./jmlr-2005-Generalization_Bounds_and_Complexities_Based_on_Sparsity_and_Clustering_for_Convex_Combinations_of_Functions_from_Random_Classes.html">37 jmlr-2005-Generalization Bounds and Complexities Based on Sparsity and Clustering for Convex Combinations of Functions from Random Classes</a></p>
<p>8 0.041830737 <a title="50-tfidf-8" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<p>9 0.04007059 <a title="50-tfidf-9" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>10 0.039376352 <a title="50-tfidf-10" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>11 0.038579445 <a title="50-tfidf-11" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>12 0.037113097 <a title="50-tfidf-12" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>13 0.034584198 <a title="50-tfidf-13" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>14 0.031763289 <a title="50-tfidf-14" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>15 0.030977966 <a title="50-tfidf-15" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>16 0.030301763 <a title="50-tfidf-16" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>17 0.029235182 <a title="50-tfidf-17" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>18 0.027666701 <a title="50-tfidf-18" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>19 0.027611533 <a title="50-tfidf-19" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>20 0.025742203 <a title="50-tfidf-20" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.197), (1, -0.152), (2, -0.033), (3, 0.169), (4, -0.084), (5, -0.103), (6, 0.069), (7, 0.039), (8, 0.037), (9, -0.289), (10, 0.091), (11, 0.097), (12, 0.041), (13, 0.048), (14, -0.239), (15, 0.287), (16, -0.244), (17, -0.114), (18, 0.174), (19, 0.035), (20, 0.114), (21, -0.089), (22, -0.152), (23, -0.028), (24, -0.006), (25, 0.126), (26, 0.012), (27, -0.022), (28, 0.006), (29, -0.053), (30, 0.097), (31, -0.083), (32, -0.025), (33, -0.035), (34, 0.004), (35, 0.038), (36, -0.121), (37, -0.029), (38, 0.043), (39, 0.018), (40, 0.092), (41, 0.014), (42, 0.032), (43, -0.253), (44, -0.11), (45, -0.083), (46, 0.071), (47, -0.107), (48, -0.163), (49, -0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97038639 <a title="50-lsi-1" href="./jmlr-2005-Learning_with_Decision_Lists_of_Data-Dependent_Features.html">50 jmlr-2005-Learning with Decision Lists of Data-Dependent Features</a></p>
<p>Author: Mario Marchand, Marina Sokolova</p><p>Abstract: We present a learning algorithm for decision lists which allows features that are constructed from the data and allows a trade-off between accuracy and complexity. We provide bounds on the generalization error of this learning algorithm in terms of the number of errors and the size of the classiﬁer it ﬁnds on the training data. We also compare its performance on some natural data sets with the set covering machine and the support vector machine. Furthermore, we show that the proposed bounds on the generalization error provide effective guides for model selection. Keywords: decision list machines, set covering machines, sparsity, data-dependent features, sample compression, model selection, learning theory</p><p>2 0.4465045 <a title="50-lsi-2" href="./jmlr-2005-Stability_of_Randomized_Learning_Algorithms.html">67 jmlr-2005-Stability of Randomized Learning Algorithms</a></p>
<p>Author: Andre Elisseeff, Theodoros Evgeniou, Massimiliano Pontil</p><p>Abstract: We extend existing theory on stability, namely how much changes in the training data inﬂuence the estimated models, and generalization performance of deterministic learning algorithms to the case of randomized algorithms. We give formal deﬁnitions of stability for randomized algorithms and prove non-asymptotic bounds on the difference between the empirical and expected error as well as the leave-one-out and expected error of such algorithms that depend on their random stability. The setup we develop for this purpose can be also used for generally studying randomized learning algorithms. We then use these general results to study the effects of bagging on the stability of a learning method and to prove non-asymptotic bounds on the predictive performance of bagging which have not been possible to prove with the existing theory of stability for deterministic learning algorithms.1 Keywords: stability, randomized learning algorithms, sensitivity analysis, bagging, bootstrap methods, generalization error, leave-one-out error.</p><p>3 0.41035858 <a title="50-lsi-3" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>Author: Dmitry Rusakov, Dan Geiger</p><p>Abstract: We develop a closed form asymptotic formula to compute the marginal likelihood of data given a naive Bayesian network model with two hidden states and binary features. This formula deviates from the standard BIC score. Our work provides a concrete example that the BIC score is generally incorrect for statistical models that belong to stratiﬁed exponential families. This claim stands in contrast to linear and curved exponential families, where the BIC score has been proven to provide a correct asymptotic approximation for the marginal likelihood. Keywords: Bayesian networks, asymptotic model selection, Bayesian information criterion (BIC)</p><p>4 0.32895565 <a title="50-lsi-4" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>Author: John Langford</p><p>Abstract: We discuss basic prediction theory and its impact on classiﬁcation success evaluation, implications for learning algorithm design, and uses in learning algorithm execution. This tutorial is meant to be a comprehensive compilation of results which are both theoretically rigorous and quantitatively useful. There are two important implications of the results presented here. The ﬁrst is that common practices for reporting results in classiﬁcation should change to use the test set bound. The second is that train set bounds can sometimes be used to directly motivate learning algorithms. Keywords: sample complexity bounds, classiﬁcation, quantitative bounds</p><p>5 0.29035372 <a title="50-lsi-5" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>Author: John Winn, Christopher M. Bishop</p><p>Abstract: Bayesian inference is now widely established as one of the principal foundations for machine learning. In practice, exact inference is rarely possible, and so a variety of approximation techniques have been developed, one of the most widely used being a deterministic framework called variational inference. In this paper we introduce Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to Bayesian Networks. Like belief propagation, VMP proceeds by sending messages between nodes in the network and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing additional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational message passing has been implemented in the form of a general purpose inference engine called VIBES (‘Variational Inference for BayEsian networkS’) which allows models to be speciﬁed graphically and then solved variationally without recourse to coding. Keywords: Bayesian networks, variational inference, message passing</p><p>6 0.25974467 <a title="50-lsi-6" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>7 0.23779762 <a title="50-lsi-7" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>8 0.22409143 <a title="50-lsi-8" href="./jmlr-2005-Generalization_Bounds_and_Complexities_Based_on_Sparsity_and_Clustering_for_Convex_Combinations_of_Functions_from_Random_Classes.html">37 jmlr-2005-Generalization Bounds and Complexities Based on Sparsity and Clustering for Convex Combinations of Functions from Random Classes</a></p>
<p>9 0.20553641 <a title="50-lsi-9" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<p>10 0.16516969 <a title="50-lsi-10" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>11 0.14330968 <a title="50-lsi-11" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>12 0.14258724 <a title="50-lsi-12" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>13 0.13730007 <a title="50-lsi-13" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>14 0.13556589 <a title="50-lsi-14" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>15 0.12990831 <a title="50-lsi-15" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>16 0.12847753 <a title="50-lsi-16" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>17 0.12106089 <a title="50-lsi-17" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>18 0.11779577 <a title="50-lsi-18" href="./jmlr-2005-Efficient_Computation_of_Gapped_Substring_Kernels_on_Large_Alphabets.html">28 jmlr-2005-Efficient Computation of Gapped Substring Kernels on Large Alphabets</a></p>
<p>19 0.11022265 <a title="50-lsi-19" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>20 0.10418882 <a title="50-lsi-20" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(17, 0.017), (19, 0.015), (36, 0.023), (37, 0.014), (43, 0.016), (47, 0.012), (52, 0.697), (59, 0.01), (70, 0.014), (88, 0.055), (92, 0.012), (94, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9797157 <a title="50-lda-1" href="./jmlr-2005-Learning_with_Decision_Lists_of_Data-Dependent_Features.html">50 jmlr-2005-Learning with Decision Lists of Data-Dependent Features</a></p>
<p>Author: Mario Marchand, Marina Sokolova</p><p>Abstract: We present a learning algorithm for decision lists which allows features that are constructed from the data and allows a trade-off between accuracy and complexity. We provide bounds on the generalization error of this learning algorithm in terms of the number of errors and the size of the classiﬁer it ﬁnds on the training data. We also compare its performance on some natural data sets with the set covering machine and the support vector machine. Furthermore, we show that the proposed bounds on the generalization error provide effective guides for model selection. Keywords: decision list machines, set covering machines, sparsity, data-dependent features, sample compression, model selection, learning theory</p><p>2 0.97169316 <a title="50-lda-2" href="./jmlr-2005-An_MDP-Based_Recommender_System.html">12 jmlr-2005-An MDP-Based Recommender System</a></p>
<p>Author: Guy Shani, David Heckerman, Ronen I. Brafman</p><p>Abstract: Typical recommender systems adopt a static view of the recommendation process and treat it as a prediction problem. We argue that it is more appropriate to view the problem of generating recommendations as a sequential optimization problem and, consequently, that Markov decision processes (MDPs) provide a more appropriate model for recommender systems. MDPs introduce two beneﬁts: they take into account the long-term effects of each recommendation and the expected value of each recommendation. To succeed in practice, an MDP-based recommender system must employ a strong initial model, must be solvable quickly, and should not consume too much memory. In this paper, we describe our particular MDP model, its initialization using a predictive model, the solution and update algorithm, and its actual performance on a commercial site. We also describe the particular predictive model we used which outperforms previous models. Our system is one of a small number of commercially deployed recommender systems. As far as we know, it is the ﬁrst to report experimental analysis conducted on a real commercial site. These results validate the commercial value of recommender systems, and in particular, of our MDP-based approach. Keywords: recommender systems, Markov decision processes, learning, commercial applications</p><p>3 0.96887767 <a title="50-lda-3" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>Author: Lior Wolf, Amnon Shashua</p><p>Abstract: The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classiﬁcation tasks, for example, it is not uncommon to have 104 to 107 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a deﬁnition of “relevancy” based on spectral properties of the Laplacian of the features’ measurement matrix. The feature selection process is then based on a continuous ranking of the features deﬁned by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a “biased non-negativity” of a key matrix in the process. As a result, a simple leastsquares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.</p><p>4 0.90650016 <a title="50-lda-4" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>Author: Tong Luo, Kurt Kramer, Dmitry B. Goldgof, Lawrence O. Hall, Scott Samson, Andrew Remsen, Thomas Hopkins</p><p>Abstract: This paper presents an active learning method which reduces the labeling effort of domain experts in multi-class classiﬁcation problems. Active learning is applied in conjunction with support vector machines to recognize underwater zooplankton from higher-resolution, new generation SIPPER II images. Most previous work on active learning with support vector machines only deals with two class problems. In this paper, we propose an active learning approach “breaking ties” for multiclass support vector machines using the one-vs-one approach with a probability approximation. Experimental results indicate that our approach often requires signiﬁcantly less labeled images to reach a given accuracy than the approach of labeling the least certain test example and random sampling. It can also be applied in batch mode resulting in an accuracy comparable to labeling one image at a time and retraining. Keywords: active learning, support vector machine, plankton recognition, probabilistic output, multi-class support vector machine</p><p>5 0.76662332 <a title="50-lda-5" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>Author: Ivor W. Tsang, James T. Kwok, Pak-Ming Cheung</p><p>Abstract: O(m3 ) Standard SVM training has time and O(m2 ) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such “approximateness” in this paper. We ﬁrst show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efﬁcient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and realworld data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about ﬁve million training patterns, in only 1.4 seconds on a 3.2GHz Pentium–4 PC. Keywords: kernel methods, approximation algorithm, minimum enclosing ball, core set, scalability</p><p>6 0.76295775 <a title="50-lda-6" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>7 0.72313666 <a title="50-lda-7" href="./jmlr-2005-Active_Coevolutionary_Learning_of_Deterministic_Finite_Automata.html">8 jmlr-2005-Active Coevolutionary Learning of Deterministic Finite Automata</a></p>
<p>8 0.69333053 <a title="50-lda-8" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>9 0.6830579 <a title="50-lda-9" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>10 0.67087811 <a title="50-lda-10" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>11 0.64523679 <a title="50-lda-11" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>12 0.64455289 <a title="50-lda-12" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>13 0.64069748 <a title="50-lda-13" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>14 0.62149292 <a title="50-lda-14" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>15 0.6129666 <a title="50-lda-15" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>16 0.60043234 <a title="50-lda-16" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>17 0.59948903 <a title="50-lda-17" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>18 0.59760809 <a title="50-lda-18" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>19 0.59686172 <a title="50-lda-19" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>20 0.59490305 <a title="50-lda-20" href="./jmlr-2005-Stability_of_Randomized_Learning_Algorithms.html">67 jmlr-2005-Stability of Randomized Learning Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
