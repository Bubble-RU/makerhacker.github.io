<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-70" href="#">jmlr2005-70</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</h1>
<br/><p>Source: <a title="jmlr-2005-70-pdf" href="http://jmlr.org/papers/volume6/binev05a/binev05a.pdf">pdf</a></p><p>Author: Peter Binev, Albert Cohen, Wolfgang Dahmen, Ronald DeVore, Vladimir Temlyakov</p><p>Abstract: This paper is concerned with the construction and analysis of a universal estimator for the regression problem in supervised learning. Universal means that the estimator does not depend on any a priori assumptions about the regression function to be estimated. The universal estimator studied in this paper consists of a least-square ﬁtting procedure using piecewise constant functions on a partition which depends adaptively on the data. The partition is generated by a splitting procedure which differs from those used in CART algorithms. It is proven that this estimator performs at the optimal convergence rate for a wide class of priors on the regression function. Namely, as will be made precise in the text, if the regression function is in any one of a certain class of approximation spaces (or smoothness spaces of order not exceeding one – a limitation resulting because the estimator uses piecewise constants) measured relative to the marginal measure, then the estimator converges to the regression function (in the least squares sense) with an optimal rate of convergence in terms of the number of samples. The estimator is also numerically feasible and can be implemented on-line. Keywords: distribution-free learning theory, nonparametric regression, universal algorithms, adaptive approximation, on-line algorithms c 2005 Peter Binev, Albert Cohen, Wolfgang Dahmen, Ronald DeVore and Vladimir Temlyakov. B INEV, C OHEN , DAHMEN , D E VORE AND T EMLYAKOV</p><p>Reference: <a title="jmlr-2005-70-reference" href="../jmlr2005_reference/jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fz', 0.433), ('dahm', 0.277), ('emlyakov', 0.207), ('inev', 0.207), ('nivers', 0.207), ('vor', 0.207), ('ci', 0.203), ('partit', 0.203), ('nem', 0.176), ('oh', 0.174), ('heory', 0.174), ('dyad', 0.173), ('cm', 0.169), ('wavelet', 0.164), ('piecew', 0.129), ('donoho', 0.116), ('smooth', 0.108), ('binev', 0.104), ('rfy', 0.104), ('thresholding', 0.103)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="70-tfidf-1" href="./jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</a></p>
<p>2 0.18347788 <a title="70-tfidf-2" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>3 0.067272976 <a title="70-tfidf-3" href="./jmlr-2005-Prioritization_Methods_for_Accelerating_MDP_Solvers.html">61 jmlr-2005-Prioritization Methods for Accelerating MDP Solvers</a></p>
<p>4 0.057425365 <a title="70-tfidf-4" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>5 0.055659886 <a title="70-tfidf-5" href="./jmlr-2005-Frames%2C_Reproducing_Kernels%2C_Regularization_and_Learning.html">35 jmlr-2005-Frames, Reproducing Kernels, Regularization and Learning</a></p>
<p>6 0.046284106 <a title="70-tfidf-6" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>7 0.041314237 <a title="70-tfidf-7" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>8 0.041023601 <a title="70-tfidf-8" href="./jmlr-2005-Local_Propagation_in_Conditional_Gaussian_Bayesian_Networks.html">51 jmlr-2005-Local Propagation in Conditional Gaussian Bayesian Networks</a></p>
<p>9 0.038401145 <a title="70-tfidf-9" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>10 0.037565511 <a title="70-tfidf-10" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>11 0.037387207 <a title="70-tfidf-11" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>12 0.036965955 <a title="70-tfidf-12" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>13 0.036706239 <a title="70-tfidf-13" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>14 0.03627646 <a title="70-tfidf-14" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>15 0.036051452 <a title="70-tfidf-15" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>16 0.033623941 <a title="70-tfidf-16" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>17 0.03293689 <a title="70-tfidf-17" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>18 0.032861333 <a title="70-tfidf-18" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>19 0.03152794 <a title="70-tfidf-19" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>20 0.030727098 <a title="70-tfidf-20" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.188), (1, 0.097), (2, -0.065), (3, -0.17), (4, -0.056), (5, -0.11), (6, -0.332), (7, -0.044), (8, 0.064), (9, -0.055), (10, 0.199), (11, -0.038), (12, -0.057), (13, 0.068), (14, -0.085), (15, -0.098), (16, 0.027), (17, -0.3), (18, 0.013), (19, 0.016), (20, -0.04), (21, 0.083), (22, -0.161), (23, -0.07), (24, 0.241), (25, 0.239), (26, -0.043), (27, 0.054), (28, -0.222), (29, 0.079), (30, 0.145), (31, 0.003), (32, 0.073), (33, 0.094), (34, -0.173), (35, 0.047), (36, 0.001), (37, 0.211), (38, -0.013), (39, -0.15), (40, -0.158), (41, 0.015), (42, 0.01), (43, -0.168), (44, 0.028), (45, 0.018), (46, -0.176), (47, 0.04), (48, -0.013), (49, -0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94939631 <a title="70-lsi-1" href="./jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</a></p>
<p>2 0.53942764 <a title="70-lsi-2" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>3 0.34085149 <a title="70-lsi-3" href="./jmlr-2005-Prioritization_Methods_for_Accelerating_MDP_Solvers.html">61 jmlr-2005-Prioritization Methods for Accelerating MDP Solvers</a></p>
<p>4 0.20182802 <a title="70-lsi-4" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>5 0.16875291 <a title="70-lsi-5" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>6 0.16777912 <a title="70-lsi-6" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>7 0.16620423 <a title="70-lsi-7" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>8 0.15293705 <a title="70-lsi-8" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>9 0.15225402 <a title="70-lsi-9" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>10 0.14869781 <a title="70-lsi-10" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>11 0.1463303 <a title="70-lsi-11" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>12 0.14271937 <a title="70-lsi-12" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>13 0.14229344 <a title="70-lsi-13" href="./jmlr-2005-Frames%2C_Reproducing_Kernels%2C_Regularization_and_Learning.html">35 jmlr-2005-Frames, Reproducing Kernels, Regularization and Learning</a></p>
<p>14 0.13872443 <a title="70-lsi-14" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>15 0.13771057 <a title="70-lsi-15" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<p>16 0.13624045 <a title="70-lsi-16" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>17 0.13587226 <a title="70-lsi-17" href="./jmlr-2005-Asymptotics_in_Empirical_Risk_Minimization.html">16 jmlr-2005-Asymptotics in Empirical Risk Minimization</a></p>
<p>18 0.13287085 <a title="70-lsi-18" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>19 0.132636 <a title="70-lsi-19" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>20 0.13021062 <a title="70-lsi-20" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.013), (35, 0.018), (37, 0.019), (44, 0.024), (47, 0.018), (60, 0.013), (62, 0.114), (68, 0.028), (78, 0.602), (88, 0.035), (97, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91376686 <a title="70-lda-1" href="./jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</a></p>
<p>2 0.85602236 <a title="70-lda-2" href="./jmlr-2005-Frames%2C_Reproducing_Kernels%2C_Regularization_and_Learning.html">35 jmlr-2005-Frames, Reproducing Kernels, Regularization and Learning</a></p>
<p>3 0.50573319 <a title="70-lda-3" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>4 0.49609149 <a title="70-lda-4" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>5 0.492576 <a title="70-lda-5" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>6 0.47915637 <a title="70-lda-6" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>7 0.45968691 <a title="70-lda-7" href="./jmlr-2005-Asymptotics_in_Empirical_Risk_Minimization.html">16 jmlr-2005-Asymptotics in Empirical Risk Minimization</a></p>
<p>8 0.45667794 <a title="70-lda-8" href="./jmlr-2005-Generalization_Bounds_and_Complexities_Based_on_Sparsity_and_Clustering_for_Convex_Combinations_of_Functions_from_Random_Classes.html">37 jmlr-2005-Generalization Bounds and Complexities Based on Sparsity and Clustering for Convex Combinations of Functions from Random Classes</a></p>
<p>9 0.454916 <a title="70-lda-9" href="./jmlr-2005-Learning_the_Kernel_Function_via_Regularization.html">48 jmlr-2005-Learning the Kernel Function via Regularization</a></p>
<p>10 0.43412149 <a title="70-lda-10" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>11 0.41944566 <a title="70-lda-11" href="./jmlr-2005-Stability_of_Randomized_Learning_Algorithms.html">67 jmlr-2005-Stability of Randomized Learning Algorithms</a></p>
<p>12 0.41290021 <a title="70-lda-12" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>13 0.40511951 <a title="70-lda-13" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>14 0.39945635 <a title="70-lda-14" href="./jmlr-2005-Generalization_Bounds_for_the_Area_Under_the_ROC_Curve.html">38 jmlr-2005-Generalization Bounds for the Area Under the ROC Curve</a></p>
<p>15 0.39062858 <a title="70-lda-15" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>16 0.38888952 <a title="70-lda-16" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>17 0.38614115 <a title="70-lda-17" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>18 0.38570818 <a title="70-lda-18" href="./jmlr-2005-Clustering_with_Bregman_Divergences.html">20 jmlr-2005-Clustering with Bregman Divergences</a></p>
<p>19 0.38059375 <a title="70-lda-19" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>20 0.3756876 <a title="70-lda-20" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
