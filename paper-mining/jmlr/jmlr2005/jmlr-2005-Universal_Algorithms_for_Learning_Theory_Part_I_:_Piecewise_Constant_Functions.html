<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-70" href="#">jmlr2005-70</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</h1>
<br/><p>Source: <a title="jmlr-2005-70-pdf" href="http://jmlr.org/papers/volume6/binev05a/binev05a.pdf">pdf</a></p><p>Author: Peter Binev, Albert Cohen, Wolfgang Dahmen, Ronald DeVore, Vladimir Temlyakov</p><p>Abstract: This paper is concerned with the construction and analysis of a universal estimator for the regression problem in supervised learning. Universal means that the estimator does not depend on any a priori assumptions about the regression function to be estimated. The universal estimator studied in this paper consists of a least-square ﬁtting procedure using piecewise constant functions on a partition which depends adaptively on the data. The partition is generated by a splitting procedure which differs from those used in CART algorithms. It is proven that this estimator performs at the optimal convergence rate for a wide class of priors on the regression function. Namely, as will be made precise in the text, if the regression function is in any one of a certain class of approximation spaces (or smoothness spaces of order not exceeding one – a limitation resulting because the estimator uses piecewise constants) measured relative to the marginal measure, then the estimator converges to the regression function (in the least squares sense) with an optimal rate of convergence in terms of the number of samples. The estimator is also numerically feasible and can be implemented on-line. Keywords: distribution-free learning theory, nonparametric regression, universal algorithms, adaptive approximation, on-line algorithms c 2005 Peter Binev, Albert Cohen, Wolfgang Dahmen, Ronald DeVore and Vladimir Temlyakov. B INEV, C OHEN , DAHMEN , D E VORE AND T EMLYAKOV</p><p>Reference: <a title="jmlr-2005-70-reference" href="../jmlr2005_reference/jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Industrial Mathematics Institute Department of Mathematics University of South Carolina Columbia, SC 29208, USA  Editor: Peter Bartlett  Abstract This paper is concerned with the construction and analysis of a universal estimator for the regression problem in supervised learning. [sent-11, score-0.199]
</p><p>2 Universal means that the estimator does not depend on any a priori assumptions about the regression function to be estimated. [sent-12, score-0.131]
</p><p>3 The universal estimator studied in this paper consists of a least-square ﬁtting procedure using piecewise constant functions on a partition which depends adaptively on the data. [sent-13, score-0.389]
</p><p>4 The partition is generated by a splitting procedure which differs from those used in CART algorithms. [sent-14, score-0.153]
</p><p>5 It is proven that this estimator performs at the optimal convergence rate for a wide class of priors on the regression function. [sent-15, score-0.16]
</p><p>6 Keywords: distribution-free learning theory, nonparametric regression, universal algorithms, adaptive approximation, on-line algorithms c 2005 Peter Binev, Albert Cohen, Wolfgang Dahmen, Ronald DeVore and Vladimir Temlyakov. [sent-18, score-0.142]
</p><p>7 (2)  Our objective is therefore to ﬁnd an estimator fz for fρ based on z such that the quantity fz − fρ is small. [sent-36, score-0.717]
</p><p>8 A common approach to this problem is to choose an hypothesis (or model) class H and then to deﬁne fz , in analogy to (1), as the minimizer of the empirical risk 1 m fz = fz,H := argmin Ez ( f ), with Ez ( f ) := ∑ (yi − f (xi ))2 . [sent-37, score-0.65]
</p><p>9 The usual way of evaluating the performance of the estimator fz is by studying its convergence either in probability or in expectation, i. [sent-42, score-0.407]
</p><p>10 the rate of decay of the quantities Prob{ fρ − fz ≥ η},  η > 0 or E( fρ − fz 2 )  (4)  as the sample size m increases. [sent-44, score-0.717]
</p><p>11 A recent survey on distribution free regression theory is provided in the book by Gy¨ rfy et al. [sent-50, score-0.108]
</p><p>12 There are three common ways to measure the compactness of a set Θ: (i) minimal coverings, (ii) smoothness conditions on the elements of Θ, (iii) the rate of approximation of the elements of Θ by a speciﬁc approximation process. [sent-54, score-0.21]
</p><p>13 One can therefore quantify the level of compactness of Θ by an assumption on the rate of decay of εn (Θ, L2 (X, ρX )). [sent-61, score-0.11]
</p><p>14 It has been communicated to us by Lucien Birg´ that one can derive from one of his forthcoming papers (Birg´ , 2004) that for any e e class Θ satisfying (5) with B = L2 (X, ρX ), there is an estimator fz satisfying 2r  E( fρ − fz 2 ) ≤ Cm− 2r+1 ,  m = 1, 2, . [sent-68, score-0.717]
</p><p>15 One can also use ε nets to give bounds for Prob( fρ − fz ). [sent-77, score-0.341]
</p><p>16 However, there is no general approach to deﬁning smoothness spaces with respect to general Borel measures ρX which precludes the direct use of classiﬁcation according to (ii). [sent-89, score-0.173]
</p><p>17 One way to circumvent this is to deﬁne smoothness in C(X), that is systematically use the spaces W r (L∞ ), but then this suffers from the same deﬁciency of not giving a full array of compact subsets in L2 (X, ρX ). [sent-90, score-0.227]
</p><p>18 If the partitions are set in advance this corresponds to the linear space approximation above. [sent-98, score-0.146]
</p><p>19 In nonlinear methods the partitions are allowed to vary but their size is speciﬁed. [sent-99, score-0.146]
</p><p>20 1300  U NIVERSAL A LGORITHMS FOR L EARNING T HEORY  Concrete algorithms have been constructed for the regression problem in learning by using approximation from speciﬁc linear spaces such as piecewise polynomial on uniform partitions, convolution kernels, and spline functions. [sent-105, score-0.161]
</p><p>21 A very useful method for bounding the performance of such estimators is provided by the following result (see Gy¨ rfy et al. [sent-107, score-0.118]
</p><p>22 3): if H is taken as a linear space of dimension o N and if the least-square estimator (3) is post-processed by application of the truncation operator y → TM (y) = sign(y) min{|y|, M}, then E( fρ − fz 2 ) ≤ C  N log(m) + inf fρ − g 2 . [sent-109, score-0.407]
</p><p>23 For example, if Θ is a ball of the Sobolev space W r (L∞ ) and H is taken as a space of piecewise polynomial functions of degree no larger than r − 1 on uniform partitions of X, one derives E( fρ − fz 2 ) ≤ C(  m − 2r ) d+2r . [sent-111, score-0.548]
</p><p>24 Secondly, it uses linear methods of approximation and therefore misses our goal of giving an estimator which performs optimally for the full range of smoothness spaces in L2 (X, ρX ). [sent-115, score-0.27]
</p><p>25 The ﬁrst deﬁciency motivates the notion of adaptive or universal estimators: the estimation algorithm should be able to exhibit the optimal rate without the knowledge of the exact amount of smoothness r in the regression function fρ . [sent-116, score-0.343]
</p><p>26 In particular, one can construct one estimator which simultaneously obtains the optimal rate (7) for all ﬁnite balls in each of the class W r (L∞ ), 0 < r ≤ k where k is arbitrary but ﬁxed, by the selection of an appropriate uniform partition. [sent-120, score-0.126]
</p><p>27 These spaces correspond to smoothness spaces of order s in L p whenever s > d − d (see DeVore, 1998). [sent-122, score-0.208]
</p><p>28 The selection of an appropriate adaptive partition in the complexity regularization framework can be implemented by the CART algorithm (Breiman et al. [sent-124, score-0.181]
</p><p>29 , 1984), which limits the search within a set of admissible partitions based on a tree structured splitting rule. [sent-125, score-0.259]
</p><p>30 In the learning theory context, the wavelet thresholding has also been used by DeVore et al. [sent-130, score-0.183]
</p><p>31 In this paper, we propose an approach which allows us to circumvent these difﬁculties, while staying in spirit close to the ideas of wavelet thresholding. [sent-135, score-0.14]
</p><p>32 In our approach, the hypothesis classes H are spaces of piecewise constant functions associated to adaptive partitions Λ. [sent-136, score-0.347]
</p><p>33 Our partitions have the same tree structure as those used in the CART algorithm (Breiman et al. [sent-137, score-0.213]
</p><p>34 While the connection between CART and thresholding in one or several orthonormal bases is well understood in the ﬁxed design denoising context (Donoho, 1997), this connection is not clear to us in our present context. [sent-139, score-0.129]
</p><p>35 (iii) The proven convergence rates are optimal in probability and expectation (up to logarithmic factors) for the largest possible range of smoothness classes in L2 (X, ρX ). [sent-142, score-0.16]
</p><p>36 The present choice of piecewise constant functions limits the optimal convergence rate to classes of low or no pointwise regularity. [sent-144, score-0.121]
</p><p>37 This is so far a weakness of our approach from the theoretical perspective, compared to the complexity regularization approach for which optimal convergence results could be obtained in the piecewise polynomial context (using for instance Gy¨ rfy et al. [sent-146, score-0.166]
</p><p>38 1 Partitions and Adaptive Approximation A typical way of generating partitions Λ of X is through a reﬁnement strategy. [sent-163, score-0.146]
</p><p>39 We ﬁrst describe the prototypical example of dyadic partitions. [sent-164, score-0.124]
</p><p>40 For this, we assume that X = [0, 1]d and denote by D j = D j (X) the collection of dyadic subcubes of X of sidelength 2− j and D := ∪∞ D j . [sent-165, score-0.124]
</p><p>41 If I ∈ D j , then its children are the 2d dyadic cubes of J ⊂ D j+1 with J ⊂ I. [sent-168, score-0.212]
</p><p>42 The cubes in D j (X) form a uniform partition in which every cube has the same measure 2− jd . [sent-171, score-0.144]
</p><p>43 ˜ More general adaptive partitions are deﬁned as follow. [sent-172, score-0.22]
</p><p>44 Any ﬁnite proper subtree T is associated to a unique partition Λ = Λ(T ) ˜ ˜ parent P (I) is also in T ˜ but P (J) is in T . [sent-174, score-0.169]
</p><p>45 ˜ which consists of its outer leaves, by which we mean those J ∈ T such that J ∈ T / One way of generating adaptive partitions is through some reﬁnement strategy. [sent-175, score-0.22]
</p><p>46 If X is subdivided then one examines each child and decides whether or not to reﬁne such a child based on the reﬁnement strategy. [sent-179, score-0.107]
</p><p>47 We assume that if X is to be reﬁned then its children consist of a subsets of X which are a partition of X. [sent-183, score-0.158]
</p><p>48 We assume that the child is also reﬁned into a sets which form a partition of the child. [sent-185, score-0.15]
</p><p>49 Such a reﬁnement strategy also results in a tree T (called the master tree) and children, parents, proper trees and partitions are deﬁned as above for the special case of dyadic partitions. [sent-186, score-0.371]
</p><p>50 We denote by T j the proper subtree consisting of all nodes with level < j and we denote by Λ j the partition associated to T j , which coincides with D j (X) in the above described dyadic partition case. [sent-188, score-0.4]
</p><p>51 Note that in contrast to this case, the a children may not be similar in which case the partitions Λ j are not spatially uniform (we could also work with even more generality and allow the number of children to depend on the cell to be reﬁned, while remaining globally bounded by some ﬁxed a). [sent-189, score-0.248]
</p><p>52 It is important to note that the cardinalities of ˜ ˜ a proper tree T and of its associated partition Λ(T ) are equivalent. [sent-190, score-0.208]
</p><p>53 Given a partition Λ, let us denote by SΛ the space of piecewise constant functions subordinate to Λ. [sent-192, score-0.199]
</p><p>54 We shall be interested in two types of approximation corresponding to uniform reﬁnement and adaptive reﬁnement. [sent-200, score-0.117]
</p><p>55 The decay of this error to zero is connected with the smoothness of f as measured in L2 (X, ρX ). [sent-206, score-0.176]
</p><p>56 The space A s can be viewed as a smoothness space of order s > 0 with smoothness measured with respect to ρX . [sent-213, score-0.276]
</p><p>57 For example, if ρX is the Lebesgue measure and we use dyadic partitioning then A s/d = Bs (L2 ), ∞ 0 < s ≤ 1, with equivalent norms. [sent-214, score-0.124]
</p><p>58 Instead of working with a-priori ﬁxed partitions there is a second kind of approximation where the partition is generated adaptively and will vary with f . [sent-216, score-0.278]
</p><p>59 Adaptive partitions are typically generated by using some reﬁnement criterion that determines whether or not to subdivide a given cell. [sent-217, score-0.171]
</p><p>60 We shall use a reﬁnement criteria that is motivated by adaptive wavelet constructions such as those given by Cohen et al. [sent-218, score-0.226]
</p><p>61 The criteria we shall use to decide when to reﬁne is analogous to thresholding wavelet coefﬁcients. [sent-220, score-0.226]
</p><p>62 Indeed, it would be exactly this criteria if we were to construct a wavelet (Haar like) bases for L2 (X, ρX ). [sent-221, score-0.136]
</p><p>63 If we were in a classical situation of Lebesgue measure and dyadic reﬁnement, then εI ( f )2 would be exactly the sum of squares of the Haar coefﬁcients of f corresponding to I. [sent-225, score-0.124]
</p><p>64 Corresponding to this tree we have the partition Λ( f , η) consisting of the outer leaves of T ( f , η). [sent-230, score-0.174]
</p><p>65 We shall deﬁne some new smoothness spaces B s which measure the regularity of a given function f by the size of the tree T ( f , η). [sent-231, score-0.283]
</p><p>66 (2001) where a similar result is proven for dyadic partitioning. [sent-235, score-0.124]
</p><p>67 It follows that every function f ∈ B s can be approximated to order O(N −s ) by PΛ f for some partition Λ with #(Λ) = N. [sent-236, score-0.107]
</p><p>68 For example, in the case of Lebesgue measure and dyadic partitions we know that each Besov space Bs (Lτ ) with τ > (s/d + 1/2)−1 and 0 < q ≤ ∞ arbitrary, is contained in B s/d (see Cohen et al. [sent-240, score-0.27]
</p><p>69 The distinction between these two forms of approximation is that in the ﬁrst, the partitions are ﬁxed in advance regardless of f but in the second form the partition can adapt to f . [sent-243, score-0.253]
</p><p>70 There is actually a slightly better strategy described in the paper by Binev and DeVore (2004) which is guaranteed to give near optimal adaptive partitions (independent of the reﬁnement strategy and hence not necessarily of the above form) for each individual f . [sent-246, score-0.22]
</p><p>71 I  As in (3) we deﬁne the estimator fz,Λ of fρ on SΛ as the empirical counterpart of PΛ fρ obtained as the solution of the least-squares problem fz,Λ := argmin Ez ( f ) = argmin f ∈SΛ  f ∈SΛ  1 m ∑ (yi − f (xi ))2 . [sent-255, score-0.157]
</p><p>72 1306  U NIVERSAL A LGORITHMS FOR L EARNING T HEORY  Theorem 1 For any partition Λ and any η > 0, Prob  PΛ fρ − fz,Λ > η ≤ 4Ne−c  mη2 N  ,  (9)  where N := #(Λ) and c depends only on M. [sent-264, score-0.107]
</p><p>73 Theorem 2 Assume that fρ ∈ A s and deﬁne the estimator fz := fz,Λ j with j chosen as the smallest m ˜ ˜ integer such that a j(1+2s) ≥ log m . [sent-269, score-0.462]
</p><p>74 Then, given any β > 0, there is a constant c = c(M, β, a) such that fρ − fz  Prob  log m > (c + | fρ | ) ˜ m  s 2s+1  As  and E  fρ − fz  2  log m ≤ (C + | fρ |A s ) m 2  ≤ Cm−β , 2s 2s+1  . [sent-270, score-0.73]
</p><p>75 Theorem 2 is satisfactory in the sense that it is obtained under no assumption on the measure ρX and the assumption fρ ∈ A s is measuring smoothness (and hence compactness) in L2 (X, ρX ), i. [sent-277, score-0.138]
</p><p>76 However, it is unsatisfactory in the sense that the estimation procedure requires the a-priori knowledge of the smoothness parameter s which appears in the choice of the resolution level j. [sent-281, score-0.138]
</p><p>77 Moreover, as noted before, the smoothness assumption fρ ∈ A s is too severe. [sent-282, score-0.138]
</p><p>78 In the context of density estimation or denoising, it is well known that adaptive methods based on wavelet thresholding (Donoho and Johnstone, 1998, 1995; Donoho et al. [sent-283, score-0.257]
</p><p>79 3 A Universal Algorithm Based on Adaptive Partitions The main feature of our algorithm is to adaptively choose a partition Λ = Λ(z) depending on the data z. [sent-287, score-0.132]
</p><p>80 It will not require a priori knowledge of the smoothness of fρ but rather will learn the smoothness from the data. [sent-288, score-0.276]
</p><p>81 Thus, it will automatically choose the right size for the partition Λ. [sent-289, score-0.107]
</p><p>82 We then deﬁne the partition Λ = Λ(z, m) associated to this tree and the corresponding estimator fz := fz,Λ . [sent-300, score-0.581]
</p><p>83 (iv) Compute the estimator fz by empirical risk minimization on the partition Λ(z, m). [sent-304, score-0.514]
</p><p>84 Indeed, the algorithm automatically exploits this unknown smoothness through the samples z. [sent-311, score-0.138]
</p><p>85 It is actually possible to build an algorithm without assuming knowledge of a γ > 0 by using the adaptive tree algorithm by Binev and DeVore (2004). [sent-313, score-0.141]
</p><p>86 4 Remarks on Algorithmic Aspects and On-Line Implementation Our ﬁrst remarks concern the construction of the adaptive partition Λ(z, m) for a ﬁxed m which −1/γ requires the computation of the numbers εI (z) for I ∈ Λ j when j satisﬁes a j ≤ τm . [sent-316, score-0.204]
</p><p>87 Hence, using Theorem 1, we see that the probability on the left of (11) is bounded from above by Prob  PΛ fρ − fΛ,z  log m >c ˜ m  s 2s+1  2 log m a2  ˜ − cc  ≤ 4a2 me  which does not exceed Cm−β provided c2 c > a2 (1 + β). [sent-355, score-0.11]
</p><p>88 Recall that the tree T ( fρ , η) is the smallest tree which contains all I for which εI = εI ( fρ ) is larger than η. [sent-360, score-0.134]
</p><p>89 If Λ0 and Λ1 are two adaptive partitions respectively associated to trees T0 and T1 we denote by Λ0 ∨ Λ1 and Λ0 ∧ Λ1 the partitions associated to the trees T0 ∪ T1 and T0 ∩ T1 , respectively. [sent-364, score-0.366]
</p><p>90 Given any η > 0, we deﬁne the partitions Λ(η) := Λ( fρ , η) ∧ Λ j0 and Λ(η, z) associated with the smallest trees containing those I such that εI ≥ η and εI (z) ≥ η, respectively, and such that the reﬁnement level j of any I in either one of these two partitions satisﬁes j ≤ j0 . [sent-365, score-0.292]
</p><p>91 In these terms our estimator fz is given by fz = fz,m = fz,Λ(τm ,z) . [sent-366, score-0.717]
</p><p>92 The remaining terms e1 and e3 respectively correspond to the bias and variance of oracle estimators based on partitions obtained by thresholding the unknown coefﬁcients εI . [sent-371, score-0.264]
</p><p>93 Let η1 := c( log m ) 2s+1 with c from (14) and η2 := c0 ( log m ) 2s+1 with c0 := ˜ m ˜ m  Cs (κ 2s+1 + aγ κ) max {| fρ |A γ , | fρ |B s }. [sent-380, score-0.11]
</p><p>94 For the expectation estimate (15), we recall that according to Corollary 1, we have  E(e2 ) ≤ C 3  m log m  N log N ≤C m  1 2s+1  log m  m  =C  log m m  2s 1+2s  . [sent-384, score-0.264]
</p><p>95 Universal Consistency of the Estimator In this last section, we discuss the consistency of our estimator when no smoothness assumption is made on the regression function fρ ∈ L2 (X, ρX ). [sent-436, score-0.269]
</p><p>96 We ﬁrst remark that the proof of the estimate E(e2 ) + E(e2 ) ≤ Cm−β , 2 4 remains unchanged under no smoothness assumption made on fρ . [sent-440, score-0.196]
</p><p>97 Under no smoothness assumptions, the convergence to 0 of these two terms still occurs when j0 → +∞ and τm → 0, and therefore as m → +∞. [sent-442, score-0.138]
</p><p>98 This is ensured by imposing natural restrictions on the splitting procedure generating the partitions which should be such that lim sup |I| = 0,  j→+∞ I∈Λ j  1318  U NIVERSAL A LGORITHMS FOR L EARNING T HEORY  where |I| is the Lebesgue measure of I. [sent-444, score-0.192]
</p><p>99 This is obviously true for dyadic partitions, and more generally when the splitting rule is such that  ∑  J∈C (I)  |J| ≤ ν|I|,  with ν < 1 independent of I ∈ T . [sent-445, score-0.17]
</p><p>100 It follows that if the threshold τm is modiﬁed into log m τm := √ , m we ﬁnd that E(e2 ) goes to 0 according to 3 E(e2 ) ≤ C 3  −2 τ−2 ϕ(τm ) log m log(τ−2 ϕ(τm ) log m) m m ˜ τ ϕ(τm ) log m = Cϕ(τm ). [sent-457, score-0.22]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('prob', 0.606), ('fz', 0.31), ('dahmen', 0.198), ('devore', 0.198), ('emlyakov', 0.148), ('inev', 0.148), ('niversal', 0.148), ('ohen', 0.148), ('vore', 0.148), ('partitions', 0.146), ('ci', 0.146), ('smoothness', 0.138), ('heory', 0.124), ('dyadic', 0.124), ('cm', 0.121), ('nement', 0.117), ('wavelet', 0.109), ('partition', 0.107), ('estimator', 0.097), ('piecewise', 0.092), ('donoho', 0.083), ('binev', 0.074), ('rfy', 0.074), ('adaptive', 0.074), ('thresholding', 0.074), ('lgorithms', 0.072), ('universal', 0.068), ('tree', 0.067), ('birg', 0.062), ('konyagin', 0.062), ('ai', 0.062), ('pk', 0.057), ('carolina', 0.055), ('earning', 0.055), ('log', 0.055), ('sc', 0.052), ('lebesgue', 0.051), ('children', 0.051), ('re', 0.05), ('besov', 0.049), ('imi', 0.049), ('temlyakov', 0.049), ('south', 0.046), ('splitting', 0.046), ('cohen', 0.045), ('estimators', 0.044), ('shall', 0.043), ('compactness', 0.043), ('child', 0.043), ('cart', 0.042), ('johnstone', 0.041), ('kerkyacharian', 0.041), ('sobolev', 0.041), ('decay', 0.038), ('gy', 0.038), ('aachen', 0.037), ('coverings', 0.037), ('cubes', 0.037), ('cucker', 0.037), ('remark', 0.036), ('spaces', 0.035), ('proper', 0.034), ('regression', 0.034), ('bs', 0.033), ('bernstein', 0.031), ('circumvent', 0.031), ('nets', 0.031), ('argmin', 0.03), ('quantities', 0.03), ('rate', 0.029), ('denoising', 0.028), ('contract', 0.028), ('subtree', 0.028), ('preprint', 0.028), ('smale', 0.028), ('bases', 0.027), ('estimates', 0.026), ('pj', 0.025), ('adaptively', 0.025), ('contracts', 0.025), ('preprints', 0.025), ('rwth', 0.025), ('subdivide', 0.025), ('ei', 0.024), ('compact', 0.023), ('jn', 0.023), ('remarks', 0.023), ('procedures', 0.022), ('ce', 0.022), ('expectation', 0.022), ('estimate', 0.022), ('coef', 0.021), ('concentration', 0.021), ('picard', 0.021), ('massart', 0.021), ('wolfgang', 0.021), ('cj', 0.021), ('haar', 0.021), ('remarking', 0.021), ('decides', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="70-tfidf-1" href="./jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</a></p>
<p>Author: Peter Binev, Albert Cohen, Wolfgang Dahmen, Ronald DeVore, Vladimir Temlyakov</p><p>Abstract: This paper is concerned with the construction and analysis of a universal estimator for the regression problem in supervised learning. Universal means that the estimator does not depend on any a priori assumptions about the regression function to be estimated. The universal estimator studied in this paper consists of a least-square ﬁtting procedure using piecewise constant functions on a partition which depends adaptively on the data. The partition is generated by a splitting procedure which differs from those used in CART algorithms. It is proven that this estimator performs at the optimal convergence rate for a wide class of priors on the regression function. Namely, as will be made precise in the text, if the regression function is in any one of a certain class of approximation spaces (or smoothness spaces of order not exceeding one – a limitation resulting because the estimator uses piecewise constants) measured relative to the marginal measure, then the estimator converges to the regression function (in the least squares sense) with an optimal rate of convergence in terms of the number of samples. The estimator is also numerically feasible and can be implemented on-line. Keywords: distribution-free learning theory, nonparametric regression, universal algorithms, adaptive approximation, on-line algorithms c 2005 Peter Binev, Albert Cohen, Wolfgang Dahmen, Ronald DeVore and Vladimir Temlyakov. B INEV, C OHEN , DAHMEN , D E VORE AND T EMLYAKOV</p><p>2 0.13674702 <a title="70-tfidf-2" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>Author: Ernesto De Vito, Lorenzo Rosasco, Andrea Caponnetto, Umberto De Giovannini, Francesca Odone</p><p>Abstract: Many works related learning from examples to regularization techniques for inverse problems, emphasizing the strong algorithmic and conceptual analogy of certain learning algorithms with regularization algorithms. In particular it is well known that regularization schemes such as Tikhonov regularization can be effectively used in the context of learning and are closely related to algorithms such as support vector machines. Nevertheless the connection with inverse problem was considered only for the discrete (ﬁnite sample) problem and the probabilistic aspects of learning from examples were not taken into account. In this paper we provide a natural extension of such analysis to the continuous (population) case and study the interplay between the discrete and continuous problems. From a theoretical point of view, this allows to draw a clear connection between the consistency approach in learning theory and the stability convergence property in ill-posed inverse problems. The main mathematical result of the paper is a new probabilistic bound for the regularized least-squares algorithm. By means of standard results on the approximation term, the consistency of the algorithm easily follows. Keywords: statistical learning, inverse problems, regularization theory, consistency c 2005 Ernesto De Vito, Lorenzo Rosasco, Andrea Caponnetto, Umberto De Giovannini and Francesca Odone. D E V ITO , ROSASCO , C APONNETTO , D E G IOVANNINI AND O DONE</p><p>3 0.048078593 <a title="70-tfidf-3" href="./jmlr-2005-Prioritization_Methods_for_Accelerating_MDP_Solvers.html">61 jmlr-2005-Prioritization Methods for Accelerating MDP Solvers</a></p>
<p>Author: David Wingate, Kevin D. Seppi</p><p>Abstract: The performance of value and policy iteration can be dramatically improved by eliminating redundant or useless backups, and by backing up states in the right order. We study several methods designed to accelerate these iterative solvers, including prioritization, partitioning, and variable reordering. We generate a family of algorithms by combining several of the methods discussed, and present extensive empirical evidence demonstrating that performance can improve by several orders of magnitude for many problems, while preserving accuracy and convergence guarantees. Keywords: Markov Decision Processes, value iteration, policy iteration, prioritized sweeping, dynamic programming</p><p>4 0.041738335 <a title="70-tfidf-4" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>Author: John Langford</p><p>Abstract: We discuss basic prediction theory and its impact on classiﬁcation success evaluation, implications for learning algorithm design, and uses in learning algorithm execution. This tutorial is meant to be a comprehensive compilation of results which are both theoretically rigorous and quantitatively useful. There are two important implications of the results presented here. The ﬁrst is that common practices for reporting results in classiﬁcation should change to use the test set bound. The second is that train set bounds can sometimes be used to directly motivate learning algorithms. Keywords: sample complexity bounds, classiﬁcation, quantitative bounds</p><p>5 0.038498376 <a title="70-tfidf-5" href="./jmlr-2005-Frames%2C_Reproducing_Kernels%2C_Regularization_and_Learning.html">35 jmlr-2005-Frames, Reproducing Kernels, Regularization and Learning</a></p>
<p>Author: Alain Rakotomamonjy,  Stéphane Canu</p><p>Abstract: This work deals with a method for building a reproducing kernel Hilbert space (RKHS) from a Hilbert space with frame elements having special properties. Conditions on existence and a method of construction are given. Then, these RKHS are used within the framework of regularization theory for function approximation. Implications on semiparametric estimation are discussed and a multiscale scheme of regularization is also proposed. Results on toy and real-world approximation problems illustrate the effectiveness of such methods. Keywords: regularization, kernel, frames, wavelets</p><p>6 0.038419344 <a title="70-tfidf-6" href="./jmlr-2005-Local_Propagation_in_Conditional_Gaussian_Bayesian_Networks.html">51 jmlr-2005-Local Propagation in Conditional Gaussian Bayesian Networks</a></p>
<p>7 0.037087154 <a title="70-tfidf-7" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>8 0.03659714 <a title="70-tfidf-8" href="./jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error.html">13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</a></p>
<p>9 0.032311205 <a title="70-tfidf-9" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>10 0.03118033 <a title="70-tfidf-10" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>11 0.029123979 <a title="70-tfidf-11" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>12 0.028823281 <a title="70-tfidf-12" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>13 0.028439634 <a title="70-tfidf-13" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>14 0.028399128 <a title="70-tfidf-14" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>15 0.027673401 <a title="70-tfidf-15" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>16 0.027462408 <a title="70-tfidf-16" href="./jmlr-2005-Asymptotics_in_Empirical_Risk_Minimization.html">16 jmlr-2005-Asymptotics in Empirical Risk Minimization</a></p>
<p>17 0.027456552 <a title="70-tfidf-17" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>18 0.026882032 <a title="70-tfidf-18" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>19 0.026835173 <a title="70-tfidf-19" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>20 0.026752746 <a title="70-tfidf-20" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.16), (1, 0.055), (2, 0.074), (3, 0.151), (4, -0.026), (5, 0.256), (6, 0.06), (7, -0.079), (8, 0.115), (9, -0.087), (10, 0.035), (11, 0.202), (12, -0.054), (13, 0.029), (14, 0.167), (15, -0.105), (16, 0.198), (17, 0.071), (18, 0.09), (19, -0.172), (20, -0.162), (21, -0.043), (22, -0.19), (23, 0.012), (24, -0.063), (25, 0.233), (26, 0.015), (27, 0.236), (28, 0.016), (29, -0.063), (30, 0.262), (31, 0.21), (32, 0.101), (33, 0.039), (34, 0.018), (35, -0.163), (36, 0.026), (37, 0.023), (38, 0.131), (39, 0.177), (40, 0.068), (41, -0.09), (42, -0.273), (43, -0.058), (44, 0.05), (45, 0.084), (46, 0.078), (47, 0.009), (48, -0.005), (49, 0.186)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96205425 <a title="70-lsi-1" href="./jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</a></p>
<p>Author: Peter Binev, Albert Cohen, Wolfgang Dahmen, Ronald DeVore, Vladimir Temlyakov</p><p>Abstract: This paper is concerned with the construction and analysis of a universal estimator for the regression problem in supervised learning. Universal means that the estimator does not depend on any a priori assumptions about the regression function to be estimated. The universal estimator studied in this paper consists of a least-square ﬁtting procedure using piecewise constant functions on a partition which depends adaptively on the data. The partition is generated by a splitting procedure which differs from those used in CART algorithms. It is proven that this estimator performs at the optimal convergence rate for a wide class of priors on the regression function. Namely, as will be made precise in the text, if the regression function is in any one of a certain class of approximation spaces (or smoothness spaces of order not exceeding one – a limitation resulting because the estimator uses piecewise constants) measured relative to the marginal measure, then the estimator converges to the regression function (in the least squares sense) with an optimal rate of convergence in terms of the number of samples. The estimator is also numerically feasible and can be implemented on-line. Keywords: distribution-free learning theory, nonparametric regression, universal algorithms, adaptive approximation, on-line algorithms c 2005 Peter Binev, Albert Cohen, Wolfgang Dahmen, Ronald DeVore and Vladimir Temlyakov. B INEV, C OHEN , DAHMEN , D E VORE AND T EMLYAKOV</p><p>2 0.46028382 <a title="70-lsi-2" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>Author: Ernesto De Vito, Lorenzo Rosasco, Andrea Caponnetto, Umberto De Giovannini, Francesca Odone</p><p>Abstract: Many works related learning from examples to regularization techniques for inverse problems, emphasizing the strong algorithmic and conceptual analogy of certain learning algorithms with regularization algorithms. In particular it is well known that regularization schemes such as Tikhonov regularization can be effectively used in the context of learning and are closely related to algorithms such as support vector machines. Nevertheless the connection with inverse problem was considered only for the discrete (ﬁnite sample) problem and the probabilistic aspects of learning from examples were not taken into account. In this paper we provide a natural extension of such analysis to the continuous (population) case and study the interplay between the discrete and continuous problems. From a theoretical point of view, this allows to draw a clear connection between the consistency approach in learning theory and the stability convergence property in ill-posed inverse problems. The main mathematical result of the paper is a new probabilistic bound for the regularized least-squares algorithm. By means of standard results on the approximation term, the consistency of the algorithm easily follows. Keywords: statistical learning, inverse problems, regularization theory, consistency c 2005 Ernesto De Vito, Lorenzo Rosasco, Andrea Caponnetto, Umberto De Giovannini and Francesca Odone. D E V ITO , ROSASCO , C APONNETTO , D E G IOVANNINI AND O DONE</p><p>3 0.21083543 <a title="70-lsi-3" href="./jmlr-2005-Prioritization_Methods_for_Accelerating_MDP_Solvers.html">61 jmlr-2005-Prioritization Methods for Accelerating MDP Solvers</a></p>
<p>Author: David Wingate, Kevin D. Seppi</p><p>Abstract: The performance of value and policy iteration can be dramatically improved by eliminating redundant or useless backups, and by backing up states in the right order. We study several methods designed to accelerate these iterative solvers, including prioritization, partitioning, and variable reordering. We generate a family of algorithms by combining several of the methods discussed, and present extensive empirical evidence demonstrating that performance can improve by several orders of magnitude for many problems, while preserving accuracy and convergence guarantees. Keywords: Markov Decision Processes, value iteration, policy iteration, prioritized sweeping, dynamic programming</p><p>4 0.17376941 <a title="70-lsi-4" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>Author: Jaakko Särelä, Harri Valpola</p><p>Abstract: A new algorithmic framework called denoising source separation (DSS) is introduced. The main beneﬁt of this framework is that it allows for the easy development of new source separation algorithms which can be optimised for speciﬁc problems. In this framework, source separation algorithms are constructed around denoising procedures. The resulting algorithms can range from almost blind to highly specialised source separation algorithms. Both simple linear and more complex nonlinear or adaptive denoising schemes are considered. Some existing independent component analysis algorithms are reinterpreted within the DSS framework and new, robust blind source separation algorithms are suggested. The framework is derived as a one-unit equivalent to an EM algorithm for source separation. However, in the DSS framework it is easy to utilise various kinds of denoising procedures which need not be based on generative models. In the experimental section, various DSS schemes are applied extensively to artiﬁcial data, to real magnetoencephalograms and to simulated CDMA mobile network signals. Finally, various extensions to the proposed DSS algorithms are considered. These include nonlinear observation mappings, hierarchical models and over-complete, nonorthogonal feature spaces. With these extensions, DSS appears to have relevance to many existing models of neural information processing. Keywords: blind source separation, BSS, prior information, denoising, denoising source separation, DSS, independent component analysis, ICA, magnetoencephalograms, MEG, CDMA</p><p>5 0.14758363 <a title="70-lsi-5" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>Author: Hyunsoo Kim, Peg Howland, Haesun Park</p><p>Abstract: Support vector machines (SVMs) have been recognized as one of the most successful classiﬁcation methods for many applications including text classiﬁcation. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efﬁciently handle a large number of terms in practical applications of text classiﬁcation. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classiﬁcation algorithm and support vector classiﬁers to handle the classiﬁcation problem where a document may belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efﬁciency for both training and testing can be achieved without sacriﬁcing prediction accuracy of text classiﬁcation even when the dimension of the input space is signiﬁcantly reduced. Keywords: dimension reduction, support vector machines, text classiﬁcation, linear discriminant analysis, centroids</p><p>6 0.13663059 <a title="70-lsi-6" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>7 0.13252072 <a title="70-lsi-7" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>8 0.12979096 <a title="70-lsi-8" href="./jmlr-2005-Local_Propagation_in_Conditional_Gaussian_Bayesian_Networks.html">51 jmlr-2005-Local Propagation in Conditional Gaussian Bayesian Networks</a></p>
<p>9 0.12757385 <a title="70-lsi-9" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>10 0.12742537 <a title="70-lsi-10" href="./jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error.html">13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</a></p>
<p>11 0.1198629 <a title="70-lsi-11" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>12 0.11720081 <a title="70-lsi-12" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>13 0.11319514 <a title="70-lsi-13" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>14 0.1105498 <a title="70-lsi-14" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>15 0.10414352 <a title="70-lsi-15" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<p>16 0.10222282 <a title="70-lsi-16" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>17 0.10133605 <a title="70-lsi-17" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>18 0.10087454 <a title="70-lsi-18" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>19 0.10074928 <a title="70-lsi-19" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>20 0.098177001 <a title="70-lsi-20" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.013), (17, 0.016), (19, 0.018), (36, 0.016), (37, 0.029), (43, 0.588), (47, 0.015), (52, 0.049), (59, 0.016), (70, 0.015), (76, 0.013), (88, 0.095), (90, 0.014), (94, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90382814 <a title="70-lda-1" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>Author: Rie Kubota Ando, Tong Zhang</p><p>Abstract: One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don’t have a complete understanding of their eﬀectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Speciﬁcally we consider learning predictive structures on hypothesis spaces (that is, what kind of classiﬁers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the eﬀectiveness of the proposed algorithms in the semi-supervised learning setting.</p><p>same-paper 2 0.87718344 <a title="70-lda-2" href="./jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</a></p>
<p>Author: Peter Binev, Albert Cohen, Wolfgang Dahmen, Ronald DeVore, Vladimir Temlyakov</p><p>Abstract: This paper is concerned with the construction and analysis of a universal estimator for the regression problem in supervised learning. Universal means that the estimator does not depend on any a priori assumptions about the regression function to be estimated. The universal estimator studied in this paper consists of a least-square ﬁtting procedure using piecewise constant functions on a partition which depends adaptively on the data. The partition is generated by a splitting procedure which differs from those used in CART algorithms. It is proven that this estimator performs at the optimal convergence rate for a wide class of priors on the regression function. Namely, as will be made precise in the text, if the regression function is in any one of a certain class of approximation spaces (or smoothness spaces of order not exceeding one – a limitation resulting because the estimator uses piecewise constants) measured relative to the marginal measure, then the estimator converges to the regression function (in the least squares sense) with an optimal rate of convergence in terms of the number of samples. The estimator is also numerically feasible and can be implemented on-line. Keywords: distribution-free learning theory, nonparametric regression, universal algorithms, adaptive approximation, on-line algorithms c 2005 Peter Binev, Albert Cohen, Wolfgang Dahmen, Ronald DeVore and Vladimir Temlyakov. B INEV, C OHEN , DAHMEN , D E VORE AND T EMLYAKOV</p><p>3 0.45068711 <a title="70-lda-3" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>Author: Theodoros Evgeniou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we deﬁne is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Speciﬁc kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can signiﬁcantly outperform standard single-task learning particularly when there are many related tasks but few data per task. Keywords: multi-task learning, kernels, vector-valued functions, regularization, learning algorithms</p><p>4 0.43803105 <a title="70-lda-4" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of ﬁnding level sets for the data generating density. We interpret this learning problem as a binary classiﬁcation problem and compare the corresponding classiﬁcation risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classiﬁcation risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justiﬁcation for the well-known heuristic of artiﬁcially sampling “labeled” samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM. Keywords: unsupervised learning, anomaly detection, density levels, classiﬁcation, SVMs</p><p>5 0.43174097 <a title="70-lda-5" href="./jmlr-2005-Frames%2C_Reproducing_Kernels%2C_Regularization_and_Learning.html">35 jmlr-2005-Frames, Reproducing Kernels, Regularization and Learning</a></p>
<p>Author: Alain Rakotomamonjy,  Stéphane Canu</p><p>Abstract: This work deals with a method for building a reproducing kernel Hilbert space (RKHS) from a Hilbert space with frame elements having special properties. Conditions on existence and a method of construction are given. Then, these RKHS are used within the framework of regularization theory for function approximation. Implications on semiparametric estimation are discussed and a multiscale scheme of regularization is also proposed. Results on toy and real-world approximation problems illustrate the effectiveness of such methods. Keywords: regularization, kernel, frames, wavelets</p><p>6 0.42629582 <a title="70-lda-6" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>7 0.41293469 <a title="70-lda-7" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>8 0.4099704 <a title="70-lda-8" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>9 0.40195528 <a title="70-lda-9" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>10 0.39204422 <a title="70-lda-10" href="./jmlr-2005-Clustering_with_Bregman_Divergences.html">20 jmlr-2005-Clustering with Bregman Divergences</a></p>
<p>11 0.38549107 <a title="70-lda-11" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>12 0.3812924 <a title="70-lda-12" href="./jmlr-2005-Learning_the_Kernel_Function_via_Regularization.html">48 jmlr-2005-Learning the Kernel Function via Regularization</a></p>
<p>13 0.37953743 <a title="70-lda-13" href="./jmlr-2005-Adaptive_Online_Prediction_by_Following_the_Perturbed_Leader.html">10 jmlr-2005-Adaptive Online Prediction by Following the Perturbed Leader</a></p>
<p>14 0.37622288 <a title="70-lda-14" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>15 0.37165076 <a title="70-lda-15" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>16 0.366108 <a title="70-lda-16" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>17 0.36414164 <a title="70-lda-17" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>18 0.35179666 <a title="70-lda-18" href="./jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error.html">13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</a></p>
<p>19 0.34024349 <a title="70-lda-19" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>20 0.33904943 <a title="70-lda-20" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
