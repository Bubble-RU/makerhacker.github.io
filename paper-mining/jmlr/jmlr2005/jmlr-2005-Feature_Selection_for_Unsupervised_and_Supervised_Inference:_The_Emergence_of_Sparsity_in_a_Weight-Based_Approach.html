<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-34" href="#">jmlr2005-34</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</h1>
<br/><p>Source: <a title="jmlr-2005-34-pdf" href="http://jmlr.org/papers/volume6/wolf05a/wolf05a.pdf">pdf</a></p><p>Author: Lior Wolf, Amnon Shashua</p><p>Abstract: The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classiﬁcation tasks, for example, it is not uncommon to have 104 to 107 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a deﬁnition of “relevancy” based on spectral properties of the Laplacian of the features’ measurement matrix. The feature selection process is then based on a continuous ranking of the features deﬁned by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a “biased non-negativity” of a key matrix in the process. As a result, a simple leastsquares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.</p><p>Reference: <a title="jmlr-2005-34-reference" href="../jmlr2005_reference/jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mi', 0.37), ('hashu', 0.223), ('olf', 0.223), ('eigenvect', 0.208), ('eat', 0.199), ('nsuperv', 0.199), ('nfer', 0.187), ('feat', 0.186), ('qgh', 0.153), ('qq', 0.153), ('uperv', 0.151), ('unsuperv', 0.131), ('nc', 0.125), ('clust', 0.123), ('cos', 0.122), ('trac', 0.118), ('matrix', 0.117), ('irrelev', 0.109), ('spect', 0.103), ('eigenvalu', 0.101)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="34-tfidf-1" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>2 0.13412353 <a title="34-tfidf-2" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>3 0.11685732 <a title="34-tfidf-3" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>4 0.11346573 <a title="34-tfidf-4" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>5 0.090177707 <a title="34-tfidf-5" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>6 0.083543442 <a title="34-tfidf-6" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>7 0.083321795 <a title="34-tfidf-7" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>8 0.080282129 <a title="34-tfidf-8" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>9 0.078485571 <a title="34-tfidf-9" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>10 0.074322708 <a title="34-tfidf-10" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>11 0.074028984 <a title="34-tfidf-11" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>12 0.072898954 <a title="34-tfidf-12" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>13 0.07193353 <a title="34-tfidf-13" href="./jmlr-2005-Efficient_Computation_of_Gapped_Substring_Kernels_on_Large_Alphabets.html">28 jmlr-2005-Efficient Computation of Gapped Substring Kernels on Large Alphabets</a></p>
<p>14 0.059708163 <a title="34-tfidf-14" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>15 0.056243557 <a title="34-tfidf-15" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>16 0.055931885 <a title="34-tfidf-16" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>17 0.054269221 <a title="34-tfidf-17" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>18 0.05382375 <a title="34-tfidf-18" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>19 0.053648829 <a title="34-tfidf-19" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>20 0.051225603 <a title="34-tfidf-20" href="./jmlr-2005-Local_Propagation_in_Conditional_Gaussian_Bayesian_Networks.html">51 jmlr-2005-Local Propagation in Conditional Gaussian Bayesian Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.33), (1, 0.11), (2, -0.049), (3, 0.084), (4, -0.232), (5, -0.051), (6, 0.15), (7, 0.1), (8, -0.032), (9, -0.023), (10, 0.078), (11, -0.066), (12, 0.176), (13, -0.192), (14, -0.018), (15, 0.076), (16, 0.024), (17, -0.112), (18, -0.059), (19, 0.123), (20, -0.161), (21, -0.187), (22, -0.256), (23, 0.069), (24, -0.067), (25, -0.1), (26, -0.09), (27, 0.062), (28, -0.049), (29, -0.036), (30, -0.014), (31, -0.027), (32, 0.016), (33, 0.077), (34, 0.016), (35, 0.071), (36, 0.07), (37, -0.115), (38, 0.074), (39, -0.074), (40, -0.052), (41, -0.022), (42, -0.085), (43, -0.124), (44, -0.043), (45, 0.083), (46, -0.003), (47, 0.028), (48, 0.161), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94190216 <a title="34-lsi-1" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>2 0.60076851 <a title="34-lsi-2" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>3 0.4633854 <a title="34-lsi-3" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>4 0.4246285 <a title="34-lsi-4" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>5 0.36700556 <a title="34-lsi-5" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>6 0.32149419 <a title="34-lsi-6" href="./jmlr-2005-Efficient_Computation_of_Gapped_Substring_Kernels_on_Large_Alphabets.html">28 jmlr-2005-Efficient Computation of Gapped Substring Kernels on Large Alphabets</a></p>
<p>7 0.30938047 <a title="34-lsi-7" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>8 0.30923274 <a title="34-lsi-8" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>9 0.28744966 <a title="34-lsi-9" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>10 0.2816495 <a title="34-lsi-10" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>11 0.27790707 <a title="34-lsi-11" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>12 0.27382392 <a title="34-lsi-12" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>13 0.26552176 <a title="34-lsi-13" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>14 0.2620717 <a title="34-lsi-14" href="./jmlr-2005-On_the_Nystr%C3%B6m_Method_for_Approximating_a_Gram_Matrix_for_Improved_Kernel-Based_Learning.html">60 jmlr-2005-On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning</a></p>
<p>15 0.26087371 <a title="34-lsi-15" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>16 0.25637352 <a title="34-lsi-16" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>17 0.2550405 <a title="34-lsi-17" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>18 0.25406647 <a title="34-lsi-18" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>19 0.22944577 <a title="34-lsi-19" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>20 0.22886766 <a title="34-lsi-20" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.013), (11, 0.016), (35, 0.017), (37, 0.024), (44, 0.034), (60, 0.012), (62, 0.116), (68, 0.044), (74, 0.015), (78, 0.024), (79, 0.015), (88, 0.033), (97, 0.556)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8809427 <a title="34-lda-1" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>2 0.41983184 <a title="34-lda-2" href="./jmlr-2005-On_the_Nystr%C3%B6m_Method_for_Approximating_a_Gram_Matrix_for_Improved_Kernel-Based_Learning.html">60 jmlr-2005-On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning</a></p>
<p>3 0.41189843 <a title="34-lda-3" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>4 0.38371351 <a title="34-lda-4" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>5 0.36972344 <a title="34-lda-5" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>6 0.36507583 <a title="34-lda-6" href="./jmlr-2005-Matrix_Exponentiated_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">55 jmlr-2005-Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>7 0.36367017 <a title="34-lda-7" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>8 0.36355597 <a title="34-lda-8" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>9 0.35785511 <a title="34-lda-9" href="./jmlr-2005-Generalization_Bounds_for_the_Area_Under_the_ROC_Curve.html">38 jmlr-2005-Generalization Bounds for the Area Under the ROC Curve</a></p>
<p>10 0.35025746 <a title="34-lda-10" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>11 0.34382343 <a title="34-lda-11" href="./jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines.html">73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</a></p>
<p>12 0.3409144 <a title="34-lda-12" href="./jmlr-2005-Learning_with_Decision_Lists_of_Data-Dependent_Features.html">50 jmlr-2005-Learning with Decision Lists of Data-Dependent Features</a></p>
<p>13 0.33976004 <a title="34-lda-13" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>14 0.33803657 <a title="34-lda-14" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>15 0.33754209 <a title="34-lda-15" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>16 0.33689654 <a title="34-lda-16" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>17 0.33243787 <a title="34-lda-17" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>18 0.3310461 <a title="34-lda-18" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>19 0.33027035 <a title="34-lda-19" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>20 0.32957375 <a title="34-lda-20" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
