<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-34" href="#">jmlr2005-34</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</h1>
<br/><p>Source: <a title="jmlr-2005-34-pdf" href="http://jmlr.org/papers/volume6/wolf05a/wolf05a.pdf">pdf</a></p><p>Author: Lior Wolf, Amnon Shashua</p><p>Abstract: The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classiﬁcation tasks, for example, it is not uncommon to have 104 to 107 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a deﬁnition of “relevancy” based on spectral properties of the Laplacian of the features’ measurement matrix. The feature selection process is then based on a continuous ranking of the features deﬁned by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a “biased non-negativity” of a key matrix in the process. As a result, a simple leastsquares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.</p><p>Reference: <a title="jmlr-2005-34-reference" href="../jmlr2005_reference/jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The feature selection process is then based on a continuous ranking of the features deﬁned by a least-squares optimization process. [sent-10, score-0.359]
</p><p>2 The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant. [sent-15, score-0.687]
</p><p>3 In Genomics, a typical example is gene selection from micro-array data where the features are gene expression coefﬁcients corresponding to the abundance of cellular mRNA taken from sample tissues. [sent-28, score-0.487]
</p><p>4 First, most learning problems do not scale well with the growth of irrelevant features — in many cases the number of training examples grows exponentially with the number of irrelevant features (Langley and Iba, 1993). [sent-33, score-0.568]
</p><p>5 In this paper the inference algorithm is not employed directly in the feature selection process but instead general properties are being gathered which indirectly indicate whether a feature subset would be appropriate or not. [sent-57, score-0.333]
</p><p>6 A remarkable property of the energy function is that the feature weights come out positive as a result of a “biased non-negativity” of a key matrix in the process and sharply decay at the border between relevant and non-relevant features. [sent-61, score-0.407]
</p><p>7 These properties make the algorithm ideal for “feature weighting” applications and for feature selection as the boundary between relevant and non-relevant features is typically clearly expressed by the decaying property of the feature weights. [sent-62, score-0.566]
</p><p>8 Instead, we take an algebraic approach and measure the relevance of a subset of features against its inﬂuence on the cluster arrangement of the data points with the goal of introducing an energy function which receives its optimal value on the desired feature selection. [sent-72, score-0.453]
</p><p>9 1 The Standard Spectrum Consider a n × q data set M consisting of q samples (columns) over n-dimensional feature space Rn representing n features x1 , . [sent-75, score-0.29]
</p><p>10 , mn pre-processed such that each row is centered around zero and is of unit L2 norm mi = 1. [sent-82, score-0.333]
</p><p>11 , xil } be a subset of (relevant) features from the set of n features and let αi ∈ {0, 1} be the indicator value associated with feature xi , i. [sent-86, score-0.448]
</p><p>12 , As = ∑n αi mi mi where i=1 1857  W OLF AND S HASHUA  Figure 1: An illustration of variable-selection using our matrix notation. [sent-92, score-0.771]
</p><p>13 In an idealized variable selection process, ˆ rows of the matrix M are selected to construct the matrix M (middle), whose columns form well coherent clusters. [sent-101, score-0.343]
</p><p>14 mi mi is the rank-1 matrix deﬁned by the outer-product between mi and itself. [sent-102, score-1.104]
</p><p>15 Finally, let Qs be a q × k matrix whose columns are the ﬁrst k eigenvectors of As associated with the leading (highest) eigenvalues λ1 ≥ . [sent-103, score-0.279]
</p><p>16 We measure cluster coherence by analyzing the (standard) spectral properties of the afﬁnity matrix As . [sent-111, score-0.311]
</p><p>17 Therefore, we deﬁne the relevance of the subset S as: rel(S ) = trace(Qs As As Qs ) =  ∑ αi αi (mi mi )mi Qs Qs mi r  s  r,s  1858  r  s  r  s  F EATURE S ELECTION FOR U NSUPERVISED AND S UPERVISED I NFERENCE  k  =  ∑ λ2j ,  j=1  where λ j are the leading eigenvalues of As . [sent-120, score-0.768]
</p><p>18 To conclude, achieving a high score on the combined energy of the ﬁrst k eigenvalues of As indicate (although indirectly) that the q input points projected onto the l-dimensional feature space are “well clustered” and that in turn suggests that S is a relevant subset of features. [sent-122, score-0.308]
</p><p>19 Let Aα = ∑n αi mi mi for some unknown scalars α1 , . [sent-133, score-0.666]
</p><p>20 We will describe in Section 3 an efﬁcient algorithm for ﬁnding a local maximum of the optimization (1) and later address the issue of sparsity and positivity of the resulting weight vector α. [sent-143, score-0.296]
</p><p>21 2 The Laplacian Spectrum Given the standard afﬁnity matrix A, consider the Laplacian matrix: L = A − D + dmax I where D is a diagonal matrix D = diag(∑ j ai j ) and dmax is a scalar larger or equal to the maximal element of D. [sent-147, score-0.294]
</p><p>22 A quadratic complexity is the best that one can expect when performing feature selection in an unsupervised manner since all pairs of feature vectors need to be compared to each other. [sent-172, score-0.451]
</p><p>23 Let q = ∑i γi vi be represented with respect to the orthonormal set of eigenvectors vi of the matrix A. [sent-206, score-0.338]
</p><p>24 Let G(r) be a matrix whose (i, j) components are mi Q(r) Q(r) m j . [sent-227, score-0.438]
</p><p>25 Deﬁnition 5 (Laplacian Power-Embedded Q − α Method) In addition to the deﬁnition of the Stan(0) dard method, let di = max diag(mi mi ) and Li = mi mi − diag(mi mi 1) + di I. [sent-239, score-1.332]
</p><p>26 Let d (r) = (max diag(∑n αi mi mi ))/(∑n αi ) i=1 i=1 (r)  4. [sent-248, score-0.666]
</p><p>27 For each i let Li = mi mi − diag(mi mi 1) + d (r) I (r) (r)  5. [sent-249, score-0.999]
</p><p>28 One of the strengths of our approach is that the feature selection method can handle both unsupervised and supervised data sets. [sent-259, score-0.397]
</p><p>29 We analyze next the propj i ij i j erties of the unsupervised Q − α algorithm with regard to sparsity and positivity of the weight vector α and then proceed to experimental analysis. [sent-286, score-0.414]
</p><p>30 In other words, if one could guarantee that the weights would come out non-negative then Q − α would provide feature weights which could be used for selection or for simply weighting the features as they are being fed into the inference engine of choice. [sent-295, score-0.461]
</p><p>31 , the gap between the high and low values of the weights is high, then the weights could be used for selecting the relevant features as well. [sent-298, score-0.419]
</p><p>32 For most feature weighting schemes, the conditions of positivity and sparsity should be specifically presented into the optimization criterion one way or the other. [sent-302, score-0.379]
</p><p>33 Since Aα = ∑i αi mi mi is represented by the sum of rank-1 matrices one can combine only a small number of them if the ﬁrst term is desired to be small. [sent-311, score-0.717]
</p><p>34 This makes the point that the feature selection scheme looks for relevant features but not necessarily the minimal set of relevant features. [sent-315, score-0.509]
</p><p>35 However, from a probabilistic point of view the probability that the leading eigenvector of G will come out positive rapidly approaches 1 with the growth of the number of features — this under a couple of simplifying assumptions. [sent-327, score-0.391]
</p><p>36 , the expected values of the entries of G are biased towards a positive value — and we should expect a bias towards a positive leading eigenvector of G. [sent-335, score-0.273]
</p><p>37 We will derive below the expectation on the entries of G (assuming independence and uniformity) and prove the main theorem showing that a random matrix whose entries are sampled i. [sent-336, score-0.319]
</p><p>38 form some distribution with positive mean and bounded variance has a positive leading eigenvector with probability 1 when the number of features n is sufﬁciently large. [sent-339, score-0.343]
</p><p>39 Proposition 9 The expected value of f = ∑k (a b)(a ci )(b ci ) where a, b ∈ ℜq and ci are ori=1 thonormal vectors uniformly sampled over the unit hypersphere in ℜq is (k/q)(a b)2 . [sent-352, score-0.361]
</p><p>40 2  0  0  20  40  60  (a)  80  100  120  140  160  180  200  (b)  Figure 2: (a) Probability, as computed using simulations, of positive leading eigenvector of a symmetric √ matrix G with i. [sent-366, score-0.29]
</p><p>41 (b) Positivity and sparsity demonstrated on the synthetic feature selection problem described in Section 6 (6 relevant features out of 202) and of a random data matrix. [sent-370, score-0.618]
</p><p>42 Our task is to derive the asymptotic behavior of the leading eigenvector when µ > 0 under the assumption that the entries of G are i. [sent-375, score-0.273]
</p><p>43 To summarize the positivity issue, the weight vector α comes out positive due to the fact that it is the leading eigenvector of a matrix whose entries have a positive mean (Propositions 7 and 8). [sent-414, score-0.529]
</p><p>44 Theorem 10 made the connection between matrices which have the property of a positive mean and the positivity of its leading eigenvector in a probabilistic setting. [sent-415, score-0.338]
</p><p>45 Below, we will establish a relationship between the spectral properties of the relevant and the irrelevant feature sets, and the sparsity of the feature vector. [sent-420, score-0.679]
</p><p>46 Assume that the rows of the matrix have been sorted such that the ﬁrst n1 rows are relevant features, and the next n2 = n−n1 features are irrelevant. [sent-422, score-0.466]
</p><p>47 Let the matrix containing the ﬁrst n1 rows be noted as M1 , and let the matrix containing M1 the rest of the rows be M2 , i. [sent-423, score-0.338]
</p><p>48 Recall the weight vector α is the ﬁrst eigenvector of the matrix Gi j = (mi m j )mi QQ m j , where mi are the rows of the matrix M, and Q is a matrix containing k orthonormal columns qi , i = 1. [sent-431, score-0.94]
</p><p>49 n  ∑  αi =  i=n1 +1  0 1  0 1  α=  0 1  0 ≤ 1  αα  be the vector with n1 zeros and n2 0 1  G λ  0 1  ,  where the last inequality follows from the spectral decomposition of the positive deﬁnite matrix G, to which α is an eigenvector with an eigenvalue of λ. [sent-441, score-0.386]
</p><p>50 Therefore the addition of more relevant features reduces the weights of the irrelevant features. [sent-457, score-0.386]
</p><p>51 Therefore, the rate in which the bound on the sum of squares of weights of irrelevant features grow is mostly linear. [sent-475, score-0.311]
</p><p>52 The implication is that the Q − α algorithm is robust to many irrelevant features: to a ﬁrst approximation, the bound on the average squared weight of an irrelevant feature remains mostly the same, as the number of irrelevant features increases. [sent-476, score-0.645]
</p><p>53 2(b) shows the weight vector α for a random data matrix M, and for a synthetic experiment (6 relevant features out of 202). [sent-480, score-0.426]
</p><p>54 One can clearly observe the positivity and sparsity of the recovered weight vector — even for a random matrix. [sent-481, score-0.296]
</p><p>55 3 Sparsity and Generalization The sparsity of the returned vector of weights does more than just directly ensure that the irrelevant features are left out; it also helps the generalization ability of the returned kernel by lowering the trace of the kernel matrix. [sent-483, score-0.668]
</p><p>56 Consider the matrix Aα , the linear kernel matrix based on the weights returned by the Q − α algorithm. [sent-487, score-0.305]
</p><p>57 Aα , which equals ∑ αi mi mi , is a weighted sum of rank-one matrices. [sent-488, score-0.666]
</p><p>58 Since our features are normalized to have norm-1, each such rank-one matrix mi mi has a trace of 1. [sent-489, score-1.053]
</p><p>59 In supervised learning, feature selection can be a major cause of over ﬁtting. [sent-500, score-0.279]
</p><p>60 Consequently, cumulants of the original data matrix M which are of higher order than two are not being considered by the feature selection scheme. [sent-511, score-0.348]
</p><p>61 , MM is diagonal) the matrix G would be diagonal and the feature selection scheme would select only a single feature rather than a feature subset. [sent-514, score-0.57]
</p><p>62 In this section we employ the so called “kernel trick” to allow for cumulants of higher orders among the feature vectors to be included in the feature selection process. [sent-515, score-0.375]
</p><p>63 1 Kernel Q − α We will consider mapping the rows mi of the data matrix M such that the rows of the mapped data matrix become φ(m1 ) , . [sent-530, score-0.671]
</p><p>64 Since the entries of G consist of inner-products between pairs of mapped feature vectors, the interaction will be no longer bilinear and will contain higher-order cumulants whose nature depends on the choice of the kernel function. [sent-534, score-0.306]
</p><p>65 The matrix Q holding the eigenvectors of Aα cannot be explicitly evaluated as well and likewise the matrix Z = Aα Q (in step 4). [sent-537, score-0.282]
</p><p>66 Let V = MM be the n × n matrix whose entries are evaluated using the kernel vi j = k(mi , m j ). [sent-540, score-0.298]
</p><p>67 Our main focus in the experiments below is in the unsupervised domain, which has received much less attention in the feature selection literature than the supervised one. [sent-584, score-0.397]
</p><p>68 Two experiments were designed, one with 6 relevant features out of 202 referred to as “linear” problem, and the other experiment with 2 relevant features out of 52 designed in a more complex manner and referred to as “non-linear” problem. [sent-589, score-0.466]
</p><p>69 In the supervised case the selected features were used to train an SVM and in the unsupervised case the class labels were not used for the Q − α feature selection but were used for the SVM training. [sent-605, score-0.555]
</p><p>70 The unsupervised test appears artiﬁcial but is important for appreciating the strength of the approach as the results of the unsupervised are only slightly inferior to the supervised test. [sent-606, score-0.314]
</p><p>71 The performance of the supervised Q − α closely agrees with the performance of the wrapper 1872  F EATURE S ELECTION FOR U NSUPERVISED AND S UPERVISED I NFERENCE  SVM feature selection algorithms. [sent-611, score-0.344]
</p><p>72 classes we sampled nc cluster centers in 5D space (5 coordinates per center) in the 5D cube where each cluster center coordinate is uniformly sampled in the interval [−1, 1]. [sent-617, score-0.382]
</p><p>73 Around each of the nc class centers we sampled 60 points according to a normal distribution nc whose mean is the class center and with a the random covariance matrix. [sent-620, score-0.306]
</p><p>74 We ran the Q − α algorithm on the data matrix and obtained the weight vector α and computed the sparsity gap - i. [sent-624, score-0.383]
</p><p>75 Good performance up to 6 clusters and a sparsity gap around 5 − 10 are not “magical numbers”. [sent-647, score-0.276]
</p><p>76 (b) Performance of a test with ﬁve relevant features and 120 irrelevant ones with nc clusters represented by the x-axis of the graph. [sent-665, score-0.494]
</p><p>77 The three graphs are solid blue for k = nc , dashed green for k = nc + 2 and dotted red for k = max(1, nc − 2). [sent-667, score-0.377]
</p><p>78 An interesting unsupervised feature selection problem in the context of visual processing is the one of automatic selection of relevant features which discriminate among perceptual classes. [sent-670, score-0.621]
</p><p>79 We would like to automatically, in an unsupervised manner, select the relevant features such that a new picture could be classiﬁed to the correct class membership. [sent-673, score-0.351]
</p><p>80 The resulting α weight vector forms a feature selection from which we create a submatrix of data points and construct its afﬁnity matrix and the associated matrix of eigenvectors Q. [sent-680, score-0.532]
</p><p>81 K ERNEL Q − α E XPERIMENTS One of the possible scenarios for which a polynomial (for example) kernel is useful is when hidden variables affect the original feature measurements and thus create non-linear interactions among the feature vectors. [sent-698, score-0.308]
</p><p>82 We consider the situation in which the original measurement matrix M is multiplied, element wise, with a hidden variable matrix whose entries are ±1. [sent-699, score-0.298]
</p><p>83 (a),(b) the ﬁrst 20 features from pictures containing the American frog and the Green frog ranked by the α weight vector. [sent-712, score-0.283]
</p><p>84 07  (b)  Figure 6: (a) 2D slice out of the relevant features in the original data matrix used in the synthetic experiment, showing three clusters. [sent-744, score-0.377]
</p><p>85 The kernel we used in this experiment was a sum of second-order polynomial kernels each over a portion of 8 entries of the feature vector: k(mi , m j ) = ∑(mk mk )2 , i j k  where mk represents the k’th section of 8 successive entries of the feature vector mi . [sent-748, score-0.817]
</p><p>86 The irrelevant features were generated in a similar manner, where for each irrelevant feature the sample points were permuted independently in order to break the interactions between the irrelevant features. [sent-753, score-0.596]
</p><p>87 This way it is impossible to distinguish between a single relevant feature and a single irrelevant feature. [sent-754, score-0.309]
</p><p>88 We considered an experiment to be successful if among the 12 features with the highest α values, at least 10 were from the relevant features subset. [sent-755, score-0.391]
</p><p>89 While MSA was able to handle values of s reaching 2, our algorithm was robust to s, and was at least 30 times more likely to choose a relevant feature than an irrelevant one, even for s > 1000. [sent-783, score-0.309]
</p><p>90 Real genomics data sets We evaluated the performance of the Q − α algorithm for the problem of gene selection on four data sets containing treatment outcome or status studies (see Wolf et al. [sent-784, score-0.386]
</p><p>91 We compared the leave-one-out error on these data sets with that achieved by both supervised and unsupervised methods of gene selection. [sent-798, score-0.326]
</p><p>92 In the unsupervised mode the class labels were ignored — and thus in general one should expect the supervised approaches to produce superior results than the unsupervised ones. [sent-805, score-0.314]
</p><p>93 For the breast me data set and for the lymph status data set we took only the ﬁrst 7, 000 features to reduce the computation complexity. [sent-812, score-0.368]
</p><p>94 homa 27 40 27 29 33 33 33 33 30 31 33 15 18 14 14  Table 1: The table entries show the Leave-one-out classiﬁcation errors for the supervised and unsupervised algorithms on the various data sets. [sent-818, score-0.284]
</p><p>95 The approach has the advantage of being suitable to unsupervised feature selection, but can also be applied in the supervised settings. [sent-823, score-0.328]
</p><p>96 The gap that exists between the algebraic and the probabilistic tools of machine learning make a direct comparison to information-based feature selection criteria a subject for future work. [sent-826, score-0.329]
</p><p>97 From linearity of the trace and from the equality trace(cc ) = c c the trace of this R R matrix is c cdσ(c) = 1. [sent-863, score-0.353]
</p><p>98 Proposition 9 The expected value of f = ∑k (a b)(a ci )(b ci ) where a, b ∈ ℜq and ci are ori=1 thonormal vectors uniformly sampled over the unit hypersphere in ℜq is (k/q)(a b)2 . [sent-866, score-0.361]
</p><p>99 Nevertheless, R ci are uniformly sampled subject to if the orthogonality constraint, the sum of integrals J = ∑k i=1 ci ci dσ(ci |c1 . [sent-874, score-0.287]
</p><p>100 Kernel feature selection with side data using a spectral approach. [sent-1246, score-0.294]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mi', 0.333), ('eature', 0.213), ('hashua', 0.201), ('olf', 0.201), ('election', 0.179), ('nsupervised', 0.179), ('nference', 0.168), ('features', 0.158), ('sparsity', 0.145), ('eigenvector', 0.14), ('qgh', 0.138), ('qq', 0.138), ('upervised', 0.136), ('feature', 0.132), ('gene', 0.13), ('trace', 0.124), ('unsupervised', 0.118), ('nc', 0.112), ('cos', 0.11), ('matrix', 0.105), ('positivity', 0.102), ('irrelevant', 0.102), ('relevancy', 0.095), ('wolf', 0.095), ('spectral', 0.093), ('lymph', 0.088), ('entries', 0.088), ('gap', 0.084), ('ci', 0.083), ('supervised', 0.078), ('cluster', 0.075), ('agh', 0.075), ('ql', 0.075), ('relevant', 0.075), ('shashua', 0.075), ('hypersphere', 0.074), ('status', 0.074), ('laplacian', 0.073), ('gi', 0.072), ('eigenvectors', 0.072), ('weston', 0.071), ('selection', 0.069), ('wrapper', 0.065), ('af', 0.065), ('rows', 0.064), ('patches', 0.063), ('kgh', 0.063), ('outcome', 0.063), ('vi', 0.061), ('qr', 0.059), ('eigenvalues', 0.057), ('decays', 0.056), ('weights', 0.051), ('matrices', 0.051), ('rq', 0.051), ('genomics', 0.05), ('msa', 0.05), ('qll', 0.05), ('wigner', 0.05), ('weight', 0.049), ('eigenvalue', 0.048), ('breast', 0.048), ('cycle', 0.048), ('growth', 0.048), ('genes', 0.047), ('clusters', 0.047), ('leading', 0.045), ('kernel', 0.044), ('algebraic', 0.044), ('energy', 0.044), ('centers', 0.044), ('hoeffding', 0.043), ('qs', 0.042), ('mm', 0.042), ('tumors', 0.042), ('cumulants', 0.042), ('aq', 0.042), ('dmax', 0.042), ('diag', 0.042), ('green', 0.041), ('nity', 0.041), ('sin', 0.039), ('synthetic', 0.039), ('orthonormal', 0.039), ('spectrum', 0.039), ('sampled', 0.038), ('coherence', 0.038), ('elephants', 0.038), ('frog', 0.038), ('furedi', 0.038), ('gaasenbeek', 0.038), ('ject', 0.038), ('mesirov', 0.038), ('overwhelming', 0.038), ('rfe', 0.038), ('rmb', 0.038), ('semicircle', 0.038), ('uncomputable', 0.038), ('unsuccessful', 0.038), ('patients', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999857 <a title="34-tfidf-1" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>Author: Lior Wolf, Amnon Shashua</p><p>Abstract: The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classiﬁcation tasks, for example, it is not uncommon to have 104 to 107 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a deﬁnition of “relevancy” based on spectral properties of the Laplacian of the features’ measurement matrix. The feature selection process is then based on a continuous ranking of the features deﬁned by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a “biased non-negativity” of a key matrix in the process. As a result, a simple leastsquares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.</p><p>2 0.10650966 <a title="34-tfidf-2" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>Author: Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, Suvrit Sra</p><p>Abstract: Several large scale data mining applications, such as text categorization and gene expression analysis, involve high-dimensional data that is also inherently directional in nature. Often such data is L2 normalized so that it lies on the surface of a unit hypersphere. Popular models such as (mixtures of) multi-variate Gaussians are inadequate for characterizing such data. This paper proposes a generative mixture-model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. In particular, we derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the mean and concentration parameters of this mixture. Numerical estimation of the concentration parameters is non-trivial in high dimensions since it involves functional inversion of ratios of Bessel functions. We also formulate two clustering algorithms corresponding to the variants of EM that we derive. Our approach provides a theoretical basis for the use of cosine similarity that has been widely employed by the information retrieval community, and obtains the spherical kmeans algorithm (kmeans with cosine similarity) as a special case of both variants. Empirical results on clustering of high-dimensional text and gene-expression data based on a mixture of vMF distributions show that the ability to estimate the concentration parameter for each vMF component, which is not present in existing approaches, yields superior results, especially for difﬁcult clustering tasks in high-dimensional spaces. Keywords: clustering, directional distributions, mixtures, von Mises-Fisher, expectation maximization, maximum likelihood, high dimensional data c 2005 Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh and Suvrit Sra. BANERJEE , D HILLON , G HOSH AND S RA</p><p>3 0.089217305 <a title="34-tfidf-3" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>Author: Hal Daumé III, Daniel Marcu</p><p>Abstract: We develop a Bayesian framework for tackling the supervised clustering problem, the generic problem encountered in tasks such as reference matching, coreference resolution, identity uncertainty and record linkage. Our clustering model is based on the Dirichlet process prior, which enables us to deﬁne distributions over the countably inﬁnite sets that naturally arise in this problem. We add supervision to our model by positing the existence of a set of unobserved random variables (we call these “reference types”) that are generic across all clusters. Inference in our framework, which requires integrating over inﬁnitely many parameters, is solved using Markov chain Monte Carlo techniques. We present algorithms for both conjugate and non-conjugate priors. We present a simple—but general—parameterization of our model based on a Gaussian assumption. We evaluate this model on one artiﬁcial task and three real-world tasks, comparing it against both unsupervised and state-of-the-art supervised algorithms. Our results show that our model is able to outperform other models across a variety of tasks and performance metrics. Keywords: supervised clustering, record linkage, citation matching, coreference, Dirichlet process, non-parametric Bayesian</p><p>4 0.087308608 <a title="34-tfidf-4" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>Author: Gal Chechik, Amir Globerson, Naftali Tishby, Yair Weiss</p><p>Abstract: The problem of extracting the relevant aspects of data was previously addressed through the information bottleneck (IB) method, through (soft) clustering one variable while preserving information about another - relevance - variable. The current work extends these ideas to obtain continuous representations that preserve relevant information, rather than discrete clusters, for the special case of multivariate Gaussian variables. While the general continuous IB problem is difﬁcult to solve, we provide an analytic solution for the optimal representation and tradeoff between compression and relevance for the this important case. The obtained optimal representation is a noisy linear projection to eigenvectors of the normalized regression matrix Σx|y Σ−1 , which is also the basis obtained x in canonical correlation analysis. However, in Gaussian IB, the compression tradeoff parameter uniquely determines the dimension, as well as the scale of each eigenvector, through a cascade of structural phase transitions. This introduces a novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level. Our analysis also provides a complete analytic expression of the preserved information as a function of the compression (the “information-curve”), in terms of the eigenvalue spectrum of the data. As in the discrete case, the information curve is concave and smooth, though it is made of different analytic segments for each optimal dimension. Finally, we show how the algorithmic theory developed in the IB framework provides an iterative algorithm for obtaining the optimal Gaussian projections. Keywords: information bottleneck, Gaussian processes, dimensionality reduction, canonical correlation analysis</p><p>5 0.085037217 <a title="34-tfidf-5" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>Author: Wei Chu, Zoubin Ghahramani</p><p>Abstract: We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative ﬁltering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach. Keywords: Gaussian processes, ordinal regression, approximate Bayesian inference, collaborative ﬁltering, gene expression analysis, feature selection</p><p>6 0.080478959 <a title="34-tfidf-6" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>7 0.077455893 <a title="34-tfidf-7" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>8 0.07627178 <a title="34-tfidf-8" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>9 0.06714914 <a title="34-tfidf-9" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>10 0.06693887 <a title="34-tfidf-10" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>11 0.062395401 <a title="34-tfidf-11" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>12 0.058930628 <a title="34-tfidf-12" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>13 0.058177367 <a title="34-tfidf-13" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>14 0.057949904 <a title="34-tfidf-14" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>15 0.053515114 <a title="34-tfidf-15" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>16 0.052057534 <a title="34-tfidf-16" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>17 0.049984369 <a title="34-tfidf-17" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>18 0.04967168 <a title="34-tfidf-18" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>19 0.049184453 <a title="34-tfidf-19" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>20 0.048018098 <a title="34-tfidf-20" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.311), (1, 0.065), (2, 0.045), (3, -0.042), (4, 0.152), (5, -0.061), (6, -0.241), (7, 0.079), (8, 0.078), (9, -0.062), (10, 0.026), (11, 0.13), (12, 0.139), (13, 0.099), (14, -0.038), (15, -0.019), (16, 0.043), (17, 0.029), (18, -0.156), (19, -0.012), (20, -0.056), (21, 0.064), (22, -0.034), (23, 0.211), (24, 0.015), (25, 0.134), (26, -0.093), (27, -0.138), (28, -0.12), (29, 0.014), (30, -0.053), (31, -0.119), (32, 0.013), (33, -0.089), (34, -0.003), (35, 0.14), (36, -0.125), (37, -0.022), (38, 0.276), (39, -0.008), (40, -0.123), (41, -0.027), (42, -0.192), (43, -0.022), (44, 0.071), (45, -0.034), (46, -0.256), (47, 0.077), (48, -0.152), (49, -0.144)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95890713 <a title="34-lsi-1" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>Author: Lior Wolf, Amnon Shashua</p><p>Abstract: The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classiﬁcation tasks, for example, it is not uncommon to have 104 to 107 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a deﬁnition of “relevancy” based on spectral properties of the Laplacian of the features’ measurement matrix. The feature selection process is then based on a continuous ranking of the features deﬁned by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a “biased non-negativity” of a key matrix in the process. As a result, a simple leastsquares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.</p><p>2 0.38780385 <a title="34-lsi-2" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>Author: Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, Suvrit Sra</p><p>Abstract: Several large scale data mining applications, such as text categorization and gene expression analysis, involve high-dimensional data that is also inherently directional in nature. Often such data is L2 normalized so that it lies on the surface of a unit hypersphere. Popular models such as (mixtures of) multi-variate Gaussians are inadequate for characterizing such data. This paper proposes a generative mixture-model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. In particular, we derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the mean and concentration parameters of this mixture. Numerical estimation of the concentration parameters is non-trivial in high dimensions since it involves functional inversion of ratios of Bessel functions. We also formulate two clustering algorithms corresponding to the variants of EM that we derive. Our approach provides a theoretical basis for the use of cosine similarity that has been widely employed by the information retrieval community, and obtains the spherical kmeans algorithm (kmeans with cosine similarity) as a special case of both variants. Empirical results on clustering of high-dimensional text and gene-expression data based on a mixture of vMF distributions show that the ability to estimate the concentration parameter for each vMF component, which is not present in existing approaches, yields superior results, especially for difﬁcult clustering tasks in high-dimensional spaces. Keywords: clustering, directional distributions, mixtures, von Mises-Fisher, expectation maximization, maximum likelihood, high dimensional data c 2005 Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh and Suvrit Sra. BANERJEE , D HILLON , G HOSH AND S RA</p><p>3 0.36570549 <a title="34-lsi-3" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>Author: Gal Chechik, Amir Globerson, Naftali Tishby, Yair Weiss</p><p>Abstract: The problem of extracting the relevant aspects of data was previously addressed through the information bottleneck (IB) method, through (soft) clustering one variable while preserving information about another - relevance - variable. The current work extends these ideas to obtain continuous representations that preserve relevant information, rather than discrete clusters, for the special case of multivariate Gaussian variables. While the general continuous IB problem is difﬁcult to solve, we provide an analytic solution for the optimal representation and tradeoff between compression and relevance for the this important case. The obtained optimal representation is a noisy linear projection to eigenvectors of the normalized regression matrix Σx|y Σ−1 , which is also the basis obtained x in canonical correlation analysis. However, in Gaussian IB, the compression tradeoff parameter uniquely determines the dimension, as well as the scale of each eigenvector, through a cascade of structural phase transitions. This introduces a novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level. Our analysis also provides a complete analytic expression of the preserved information as a function of the compression (the “information-curve”), in terms of the eigenvalue spectrum of the data. As in the discrete case, the information curve is concave and smooth, though it is made of different analytic segments for each optimal dimension. Finally, we show how the algorithmic theory developed in the IB framework provides an iterative algorithm for obtaining the optimal Gaussian projections. Keywords: information bottleneck, Gaussian processes, dimensionality reduction, canonical correlation analysis</p><p>4 0.31105202 <a title="34-lsi-4" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>Author: Wei Chu, Zoubin Ghahramani</p><p>Abstract: We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative ﬁltering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach. Keywords: Gaussian processes, ordinal regression, approximate Bayesian inference, collaborative ﬁltering, gene expression analysis, feature selection</p><p>5 0.30046564 <a title="34-lsi-5" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>Author: Hal Daumé III, Daniel Marcu</p><p>Abstract: We develop a Bayesian framework for tackling the supervised clustering problem, the generic problem encountered in tasks such as reference matching, coreference resolution, identity uncertainty and record linkage. Our clustering model is based on the Dirichlet process prior, which enables us to deﬁne distributions over the countably inﬁnite sets that naturally arise in this problem. We add supervision to our model by positing the existence of a set of unobserved random variables (we call these “reference types”) that are generic across all clusters. Inference in our framework, which requires integrating over inﬁnitely many parameters, is solved using Markov chain Monte Carlo techniques. We present algorithms for both conjugate and non-conjugate priors. We present a simple—but general—parameterization of our model based on a Gaussian assumption. We evaluate this model on one artiﬁcial task and three real-world tasks, comparing it against both unsupervised and state-of-the-art supervised algorithms. Our results show that our model is able to outperform other models across a variety of tasks and performance metrics. Keywords: supervised clustering, record linkage, citation matching, coreference, Dirichlet process, non-parametric Bayesian</p><p>6 0.29818636 <a title="34-lsi-6" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>7 0.28167602 <a title="34-lsi-7" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>8 0.26485237 <a title="34-lsi-8" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>9 0.26273644 <a title="34-lsi-9" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>10 0.25958246 <a title="34-lsi-10" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>11 0.24981451 <a title="34-lsi-11" href="./jmlr-2005-On_the_Nystr%C3%B6m_Method_for_Approximating_a_Gram_Matrix_for_Improved_Kernel-Based_Learning.html">60 jmlr-2005-On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning</a></p>
<p>12 0.22886842 <a title="34-lsi-12" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>13 0.21657887 <a title="34-lsi-13" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>14 0.20737487 <a title="34-lsi-14" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>15 0.20695612 <a title="34-lsi-15" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>16 0.18902645 <a title="34-lsi-16" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>17 0.18892623 <a title="34-lsi-17" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>18 0.18756321 <a title="34-lsi-18" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>19 0.18671869 <a title="34-lsi-19" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>20 0.18544418 <a title="34-lsi-20" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.012), (17, 0.02), (19, 0.017), (36, 0.021), (37, 0.022), (42, 0.021), (43, 0.03), (47, 0.017), (52, 0.632), (59, 0.018), (70, 0.019), (80, 0.01), (88, 0.063), (90, 0.013), (94, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98414487 <a title="34-lda-1" href="./jmlr-2005-Learning_with_Decision_Lists_of_Data-Dependent_Features.html">50 jmlr-2005-Learning with Decision Lists of Data-Dependent Features</a></p>
<p>Author: Mario Marchand, Marina Sokolova</p><p>Abstract: We present a learning algorithm for decision lists which allows features that are constructed from the data and allows a trade-off between accuracy and complexity. We provide bounds on the generalization error of this learning algorithm in terms of the number of errors and the size of the classiﬁer it ﬁnds on the training data. We also compare its performance on some natural data sets with the set covering machine and the support vector machine. Furthermore, we show that the proposed bounds on the generalization error provide effective guides for model selection. Keywords: decision list machines, set covering machines, sparsity, data-dependent features, sample compression, model selection, learning theory</p><p>2 0.9804697 <a title="34-lda-2" href="./jmlr-2005-An_MDP-Based_Recommender_System.html">12 jmlr-2005-An MDP-Based Recommender System</a></p>
<p>Author: Guy Shani, David Heckerman, Ronen I. Brafman</p><p>Abstract: Typical recommender systems adopt a static view of the recommendation process and treat it as a prediction problem. We argue that it is more appropriate to view the problem of generating recommendations as a sequential optimization problem and, consequently, that Markov decision processes (MDPs) provide a more appropriate model for recommender systems. MDPs introduce two beneﬁts: they take into account the long-term effects of each recommendation and the expected value of each recommendation. To succeed in practice, an MDP-based recommender system must employ a strong initial model, must be solvable quickly, and should not consume too much memory. In this paper, we describe our particular MDP model, its initialization using a predictive model, the solution and update algorithm, and its actual performance on a commercial site. We also describe the particular predictive model we used which outperforms previous models. Our system is one of a small number of commercially deployed recommender systems. As far as we know, it is the ﬁrst to report experimental analysis conducted on a real commercial site. These results validate the commercial value of recommender systems, and in particular, of our MDP-based approach. Keywords: recommender systems, Markov decision processes, learning, commercial applications</p><p>same-paper 3 0.97747493 <a title="34-lda-3" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>Author: Lior Wolf, Amnon Shashua</p><p>Abstract: The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classiﬁcation tasks, for example, it is not uncommon to have 104 to 107 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a deﬁnition of “relevancy” based on spectral properties of the Laplacian of the features’ measurement matrix. The feature selection process is then based on a continuous ranking of the features deﬁned by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a “biased non-negativity” of a key matrix in the process. As a result, a simple leastsquares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.</p><p>4 0.92351812 <a title="34-lda-4" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>Author: Tong Luo, Kurt Kramer, Dmitry B. Goldgof, Lawrence O. Hall, Scott Samson, Andrew Remsen, Thomas Hopkins</p><p>Abstract: This paper presents an active learning method which reduces the labeling effort of domain experts in multi-class classiﬁcation problems. Active learning is applied in conjunction with support vector machines to recognize underwater zooplankton from higher-resolution, new generation SIPPER II images. Most previous work on active learning with support vector machines only deals with two class problems. In this paper, we propose an active learning approach “breaking ties” for multiclass support vector machines using the one-vs-one approach with a probability approximation. Experimental results indicate that our approach often requires signiﬁcantly less labeled images to reach a given accuracy than the approach of labeling the least certain test example and random sampling. It can also be applied in batch mode resulting in an accuracy comparable to labeling one image at a time and retraining. Keywords: active learning, support vector machine, plankton recognition, probabilistic output, multi-class support vector machine</p><p>5 0.78722662 <a title="34-lda-5" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>Author: Ivor W. Tsang, James T. Kwok, Pak-Ming Cheung</p><p>Abstract: O(m3 ) Standard SVM training has time and O(m2 ) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such “approximateness” in this paper. We ﬁrst show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efﬁcient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and realworld data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about ﬁve million training patterns, in only 1.4 seconds on a 3.2GHz Pentium–4 PC. Keywords: kernel methods, approximation algorithm, minimum enclosing ball, core set, scalability</p><p>6 0.78715008 <a title="34-lda-6" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>7 0.74054265 <a title="34-lda-7" href="./jmlr-2005-Active_Coevolutionary_Learning_of_Deterministic_Finite_Automata.html">8 jmlr-2005-Active Coevolutionary Learning of Deterministic Finite Automata</a></p>
<p>8 0.71944588 <a title="34-lda-8" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>9 0.70681483 <a title="34-lda-9" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>10 0.69745833 <a title="34-lda-10" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>11 0.67054582 <a title="34-lda-11" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>12 0.6628024 <a title="34-lda-12" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>13 0.6593104 <a title="34-lda-13" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>14 0.64610106 <a title="34-lda-14" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>15 0.63762283 <a title="34-lda-15" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>16 0.62977064 <a title="34-lda-16" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>17 0.62641531 <a title="34-lda-17" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>18 0.6263299 <a title="34-lda-18" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>19 0.62244546 <a title="34-lda-19" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>20 0.61620671 <a title="34-lda-20" href="./jmlr-2005-Stability_of_Randomized_Learning_Algorithms.html">67 jmlr-2005-Stability of Randomized Learning Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
