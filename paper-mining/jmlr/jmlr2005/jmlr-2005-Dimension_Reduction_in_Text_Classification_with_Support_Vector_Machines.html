<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-27" href="#">jmlr2005-27</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</h1>
<br/><p>Source: <a title="jmlr-2005-27-pdf" href="http://jmlr.org/papers/volume6/kim05a/kim05a.pdf">pdf</a></p><p>Author: Hyunsoo Kim, Peg Howland, Haesun Park</p><p>Abstract: Support vector machines (SVMs) have been recognized as one of the most successful classiﬁcation methods for many applications including text classiﬁcation. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efﬁciently handle a large number of terms in practical applications of text classiﬁcation. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classiﬁcation algorithm and support vector classiﬁers to handle the classiﬁcation problem where a document may belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efﬁciency for both training and testing can be achieved without sacriﬁcing prediction accuracy of text classiﬁcation even when the dimension of the input space is signiﬁcantly reduced. Keywords: dimension reduction, support vector machines, text classiﬁcation, linear discriminant analysis, centroids</p><p>Reference: <a title="jmlr-2005-27-reference" href="../jmlr2005_reference/jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. [sent-11, score-0.668]
</p><p>2 Keywords: dimension reduction, support vector machines, text classiﬁcation, linear discriminant analysis, centroids  1. [sent-14, score-0.425]
</p><p>3 Several characteristics have been observed in vector space based methods for text classiﬁcation (20; 21), including the high dimensionality of the input space, sparsity of document vectors, linear separability in most text classiﬁcation problems, and the belief that few features are irrelevant. [sent-17, score-0.389]
</p><p>4 It has been conjectured that an aggressive dimension reduction may result in a signiﬁcant loss of information, and therefore, result in poor classiﬁcation results (13). [sent-18, score-0.349]
</p><p>5 The evaluation of the kernel function depends on the dimension of the input data, since the kernel functions contain the inner product of two input vectors for the linear or polynomial kernels or the distance of two vectors for the Gaussian RBF kernel. [sent-32, score-0.254]
</p><p>6 Throughout the paper, we will assume that the document set is represented in an m × n termdocument matrix A = (ai j ), in which each column represents a document, and each entry ai j represents the weighted frequency of term i in document j (1; 2). [sent-36, score-0.409]
</p><p>7 Although the truncated SVD provides the closest approximation to A in Frobenius or L2 norm, LSI ignores the cluster structure while reducing the dimension of the data. [sent-39, score-0.348]
</p><p>8 With dimension reduction, computational complexity can be dramatically reduced for all classiﬁers including support vector machines and k-nearest neighbor classiﬁcation. [sent-41, score-0.312]
</p><p>9 In many document data sets, documents can be assigned to more than one cluster upon classiﬁcation. [sent-44, score-0.438]
</p><p>10 Our numerical experiments illustrate that the clusterpreserving dimension reduction algorithms we employ reduce the data dimension without any signiﬁcant loss of information. [sent-46, score-0.501]
</p><p>11 In fact, in many cases, they seem to have the effect of noise reduction, since prediction accuracy becomes better after dimension reduction when compared to that in the original high dimensional input space. [sent-47, score-0.452]
</p><p>12 Low-Rank Approximation Using Latent Semantic Indexing LSI is based on the assumption that there is some underlying latent semantic structure in the termdocument matrix that is corrupted by the wide variety of words used in documents and queries. [sent-49, score-0.27]
</p><p>13 The basic idea is that if two document vectors represent the same topic, they will share many associating words with a keyword, and they will have very close semantic structures after dimension reduction via SVD. [sent-51, score-0.588]
</p><p>14 Thus LSI/SVD breaks the original relationship of the data into linearly independent components (6), where the original term vectors are represented by left singular vectors and document vectors by right singular vectors. [sent-52, score-0.345]
</p><p>15 That is, if l ≤ rank(A), then A ≈ Ul Σl VlT  , where the columns of Ul are the leading l left singular vectors, Σl is an l × l diagonal matrix with the l largest singular values in nonincreasing order along its diagonal, and the columns of Vl are the leading l right singular vectors. [sent-53, score-0.41]
</p><p>16 Then Σl VlT is the reduced dimensional representation of A, or ˆ equivalently, a new document q ∈ Rm×1 can be represented in the l-dimensional space as q = UlT q. [sent-54, score-0.317]
</p><p>17 Since the complete orthogonal decomposition such as ULV or URV has computational advantages over the SVD including easier updating (22; 23; 24) and downdating (17), dimension reduction by these faster low-rank orthogonal decompositions has also been exploited (3). [sent-56, score-0.709]
</p><p>18 In addition, since there is no theoretical optimum value for the reduced dimension, potentially expensive experimentation may be required to determine a reduced dimension l. [sent-58, score-0.31]
</p><p>19 The experimental results conﬁrm that when the data set is already clustered, the dimension reduction methods we present in the next section are more effective for classiﬁcation of new data. [sent-60, score-0.349]
</p><p>20 Compute the centroid ci of the ith cluster, 1 ≤ i ≤ p 2. [sent-63, score-0.697]
</p><p>21 Compute the centroid ci of the ith cluster, 1 ≤ i ≤ p 2. [sent-67, score-0.697]
</p><p>22 Dimension Reduction Algorithms for Clustered Data To achieve greater efﬁciency in manipulating data represented in a high dimensional space, it is often necessary to reduce the dimension dramatically. [sent-71, score-0.223]
</p><p>23 In this section, several dimension reduction methods that preserve the cluster structure are reviewed. [sent-72, score-0.496]
</p><p>24 Each method attempts to choose a projection to a reduced dimensional space that will capture the cluster structure of the data collection as much as possible. [sent-73, score-0.297]
</p><p>25 Instead of treating each column of the matrix A equally regardless of its membership in a speciﬁc cluster as in LSI/SVD, we want to ﬁnd a lower dimensional representation Y of A so that the p clusters are preserved in Y . [sent-76, score-0.393]
</p><p>26 Given a term-document matrix, the problem is to ﬁnd a transformation that maps each document vector in the m dimensional space to a vector in the l dimensional space for some l < m. [sent-77, score-0.309]
</p><p>27 For this, either the dimension reducing transformation GT ∈ Rl×m is computed explicitly or the problem is formulated as a rank reducing approximation where the given matrix A is to be decomposed into two matrices B and Y . [sent-78, score-0.323]
</p><p>28 The matrix B accounts for the dimension reducing transformation. [sent-80, score-0.244]
</p><p>29 However, it is not necessary to compute the dimension reducing transformation G from B explicitly, as long as we can ﬁnd the reduced dimensional representation of a given data item. [sent-81, score-0.351]
</p><p>30 (3)  Any given document q ∈ Rm×1 can be transformed to the lower dimensional space by solving the minimization problem ˆ (4) min Bq − q 2 . [sent-83, score-0.238]
</p><p>31 ˆ q∈Rl×1  (5)  In the Centroid dimension reduction algorithm (see Algorithm 1), the ith column of B is the centroid vector of the ith cluster, which is the average of the data items in the ith cluster, for 1 ≤ i ≤ p. [sent-86, score-1.001]
</p><p>32 Then, any vector q ∈ Rm×1 can be represented in the ˆ p dimensional space as q, the solution of the least squares problem (4), where B is the centroid matrix. [sent-88, score-0.624]
</p><p>33 In the Orthogonal Centroid algorithm (see Algorithm 2), the p dimensional representation ˆ of a data vector q ∈ Rm×1 is given as q = QT q where Q p is an orthonormal basis for the centroid p matrix obtained from its QR decomposition. [sent-89, score-0.667]
</p><p>34 The centroid-based dimension reduction algorithms are computationally less costly than LSI/SVD. [sent-90, score-0.349]
</p><p>35 Although the centroid-based schemes can be applied only when the data are linearly separable, they are suitable for text classiﬁcation problems, since text data is usually linearly separable in the original dimensional space (13). [sent-92, score-0.335]
</p><p>36 2 Generalized Discriminant Analysis based on the Generalized Singular Value Decomposition Recently, a new algorithm has been developed for cluster-preserving dimension reduction based on the generalized singular value decomposition (GSVD) (10). [sent-95, score-0.488]
</p><p>37 Classical discriminant analysis (7; 25) preserves cluster structure by maximizing the scatter between clusters while minimizing the scatter within clusters. [sent-97, score-0.346]
</p><p>38 If we denote by Ni the set of column indices that belong to the cluster i, ni the number of columns in cluster i, and c the global centroid, then p  Sw = ∑  ∑ (a j − ci )(a j − ci )T ,  Sb =  ∑ ∑ (ci − c)(ci − c)T  i=1 j∈Ni  and p  i=1 j∈Ni p  =  ∑ ni (ci − c)(ci − c)T . [sent-99, score-0.699]
</p><p>39 In fact, when the reduced dimension −1 l ≥ p − 1, trace(Sw Sb ) is exactly preserved upon dimension reduction, and equals λ1 + · · · + λ p−1 , where each λi ≥ 0. [sent-117, score-0.462]
</p><p>40 Without loss of generality, we assume that the term-document matrix A is partitioned as A = [A1 , · · · , A p ] 42  D IMENSION R EDUCTION IN T EXT C LASSIFICATION WITH SVM S  where the columns of each block Ai ∈ Rm×ni belong to the cluster i. [sent-118, score-0.269]
</p><p>41 As the product of an m × n matrix with an n × m matrix, Sw will be singular when the number of terms m exceeds the number of documents n. [sent-126, score-0.227]
</p><p>42 Classiﬁcation Methods To test the effect of dimension reduction in text classiﬁcation, three different classiﬁcation methods were used: centroid-based classiﬁcation, k-nearest neighbor (kNN), and support vector machines (SVMs). [sent-139, score-0.541]
</p><p>43 43  K IM , H OWLAND AND PARK  Algorithm 4 : Centroid-based Classiﬁcation Given a data matrix A with p clusters and p corresponding centroids, ci , 1 ≤ i ≤ p, and a vector q ∈ Rm×1 , this method ﬁnds the index j of the cluster in which the vector q belongs. [sent-142, score-0.35]
</p><p>44 • ﬁnd the index j such that sim(q, ci ), 1 ≤ i ≤ p, is minimum (or maximum), where sim(q, ci ) is the similarity measure between q and ci . [sent-143, score-0.398]
</p><p>45 (For example, sim(q, ci ) = q − ci 2 using the L2 norm, and we take the index with the minimum value. [sent-144, score-0.222]
</p><p>46 Using the cosine measure, qT ci q 2 ci  sim(q, ci ) = cos(q, ci ) =  , 2  and we take the index with the maximum value. [sent-145, score-0.661]
</p><p>47 Using the cosine similarity measure, we can classify a test document q by computing arg max  1≤i≤p  qT ci q 2 ci  (8) 2  where ci is the centroid of the ith cluster of the training data. [sent-149, score-1.515]
</p><p>48 When dimension reduction is performed by the Centroid algorithm, the centroids of the full space become the columns ei ∈ R p×1 of the identity matrix. [sent-150, score-0.55]
</p><p>49 Then the decision rule becomes arg max  1≤i≤p  ˆ qT ei ˆ q 2 ei  ,  (9)  2  ˆ where q is the reduced dimensional representation of the document q. [sent-151, score-0.317]
</p><p>50 We can also classify using the L2 norm similarity measure by ﬁnding the centroid that is closest to q in L2 norm. [sent-154, score-0.618]
</p><p>51 The original form of centroid-based classiﬁcation ﬁnds the nearest centroid and assigns the corresponding class as the predicted class. [sent-155, score-0.584]
</p><p>52 In this way, document x will be a member of class j if its similarity to the centroid vector c j for the class is above the threshold. [sent-157, score-0.785]
</p><p>53 Experimental Results Prediction results are compared for the test documents in the full space without any dimension reduction as well as those in the reduced space obtained by LSI/SVD, Centroid, Orthogonal Centroid, and LDA/GSVD dimension reduction methods. [sent-179, score-0.915]
</p><p>54 For SVMs, we optimized the regularization parameter C, polynomial degree d for the polynomial kernel, and γ for the Gaussian RBF (radial basis function) kernel for each full and reduced dimension data set. [sent-180, score-0.376]
</p><p>55 45  K IM , H OWLAND AND PARK  classiﬁcation methods centroid (L2 ) centroid (Cosine) 5NN (L2 ) 15NN (L2 ) 30NN (L2 ) 5NN (Cosine) 15NN (Cosine) 30NN (Cosine) SVM  l=5 71. [sent-181, score-1.106]
</p><p>56 9  Table 1: Text classiﬁcation accuracy (%) using centroid-based classiﬁcation, k-nearest neighbor classiﬁcation, and SVMs, with LSI/SVD dimension reduction on the MEDLINE data set. [sent-262, score-0.433]
</p><p>57 The Euclidean norm (L2 ) and the cosine similarity measure (Cosine) were used for the centroid-based and kNN classiﬁcation. [sent-263, score-0.282]
</p><p>58 Table 1 reports text classiﬁcation accuracy for the MEDLINE data set using LSI/SVD with a range of values for the reduced dimension. [sent-275, score-0.222]
</p><p>59 The smallest reduced dimension, l = 5, is included in order to compare with centroid-based and LDA/GSVD methods, which reduce the dimension to 5 and 4, respectively. [sent-276, score-0.231]
</p><p>60 For a training set of size 1250, the reduced dimension l = 300 is generous. [sent-278, score-0.231]
</p><p>61 This is consistent with the common belief that cosine similarity performs better with unnormalized text data. [sent-280, score-0.393]
</p><p>62 Also, classiﬁcation accuracy using 5NN lags that for higher values of k, suggesting that k=5 is too small for classes 46  D IMENSION R EDUCTION IN T EXT C LASSIFICATION WITH SVM S  kernel  Dimension reduction methods Centroid Orthogonal LDA/ Centroid GSVD4 22095×1250 5×1250 5×1250 4×1250 88. [sent-281, score-0.263]
</p><p>63 3  Table 2: Text classiﬁcation accuracy (%) with different kernels in SVMs with and without dimension reduction on the MEDLINE data set. [sent-352, score-0.381]
</p><p>64 It is noteworthy that even with LSI, which makes no attempt to preserve the cluster structure upon dimension reduction, SVM classiﬁcation achieves very consistent classiﬁcation results for reduced dimensions of 100 or greater, and the SVM accuracy exceeds that of the other classiﬁcation methods. [sent-357, score-0.439]
</p><p>65 Table 2 shows text classiﬁcation accuracy (%) with different kernels in SVMs, with and without dimension reduction on the MEDLINE data set. [sent-358, score-0.492]
</p><p>66 This table shows that the prediction results in the reduced dimension are similar to those in the original full dimensional space, while achieving a signiﬁcant reduction in time and space complexity. [sent-360, score-0.542]
</p><p>67 In the reduced space obtained by the Orthogonal Centroid dimension reduction algorithm, the classiﬁcation accuracy is insensitive to the choice of the kernel. [sent-361, score-0.46]
</p><p>68 Table 3 shows classiﬁcation accuracy obtained by all three classiﬁcation methods – centroidbased, kNN with three different values of k, and the optimal result from SVM – for each dimension reduced data set and the full space. [sent-363, score-0.306]
</p><p>69 For the LDA/GSVD dimension reduction method, the classiﬁcation accuracy with cosine similarity measure is lower with centroid-based classiﬁcation as well as with kNN, while the results with L2 norm are better. [sent-364, score-0.663]
</p><p>70 With LDA/GSVD, documents from the same class in 47  K IM , H OWLAND AND PARK  classiﬁcation methods  centroid (L2 ) centroid (Cosine) 5NN (L2 ) 15NN (L2 ) 30NN (L2 ) 5NN (Cosine) 15NN (Cosine) 30NN (Cosine) SVM  Full 22095×1250 85. [sent-366, score-1.201]
</p><p>71 4  Table 3: Text classiﬁcation accuracy (%) using centroid-based classiﬁcation, k-nearest neighbor classiﬁcation, and SVMs, with and without dimension reduction on the MEDLINE data set. [sent-411, score-0.433]
</p><p>72 The Euclidean norm (L2 ) and the cosine similarity measure (Cosine) were used for centroid-based and kNN classiﬁcation. [sent-412, score-0.282]
</p><p>73 the full dimensional space tend to be transformed to a very tight cluster or even to a single point in the reduced space, since the LDA/GSVD algorithm tends to minimize the trace of the within cluster scatter. [sent-445, score-0.532]
</p><p>74 Table 4 shows text classiﬁcation accuracy for the 5 classes using SVMs with and without dimension reduction methods on the MEDLINE data set. [sent-447, score-0.492]
</p><p>75 The REUTERS data set has many documents that are classiﬁed to more than 2 classes, whereas no document is classiﬁed to belong to more than one class in the MEDLINE data set. [sent-449, score-0.291]
</p><p>76 03  Table 5: Comparison of micro-averaged F1 scores for 3 different classiﬁcation methods with and without dimension reduction on the REUTERS data set. [sent-468, score-0.349]
</p><p>77 The Euclidean norm (L2 ) and the cosine similarity measure (Cosine) were used for the centroid-based classiﬁcation. [sent-469, score-0.282]
</p><p>78 The cosine similarity measure was used for the kNN classiﬁcation. [sent-470, score-0.282]
</p><p>79 The dimension of the full training term-document matrix is 11941×9579 and that of the reduced matrix is 90×9579. [sent-471, score-0.36]
</p><p>80 could handle relatively large matrices using a sparse matrix representation and sparse QR decomposition in the Centroid and Orthogonal Centroid dimension reduction methods, results for the LDA/GSVD dimension reduction method are not reported, since we ran out of memory while computing the GSVD. [sent-472, score-0.791]
</p><p>81 Table 5 shows that the effectiveness of classiﬁcation was preserved for the Orthogonal Centroid dimension reduction algorithm, while it became worse for the Centroid dimension reduction algorithm. [sent-476, score-0.778]
</p><p>82 This is due to a property of the Centroid algorithm that the centroids of the full space are projected to the columns of the identity matrix in the reduced space. [sent-477, score-0.323]
</p><p>83 This orthogonality between the centroids may make it difﬁcult to represent the multiclass membership of a document by separating closely related classes after dimension reduction. [sent-478, score-0.46]
</p><p>84 Conclusion and Discussion In this paper, we applied three methods, Centroid, Orthogonal Centroid, and LDA/GSVD, which are designed for reducing the dimension of clustered data. [sent-482, score-0.262]
</p><p>85 For comparison, we also applied LSI/SVD, which does not attempt to preserve cluster structure upon dimension reduction. [sent-483, score-0.328]
</p><p>86 We tested the effectiveness in classiﬁcation with dimension reduction using three different classiﬁcation methods: 49  K IM , H OWLAND AND PARK  class Full  earn acq money-fx grain crude trade interest ship wheat corn microavg (top 10) avg (top 10) microavg(all)  11941×9579 98. [sent-484, score-0.397]
</p><p>87 The dimension of the full training term-document matrix is 11941×9579 and that of the reduced matrix is 90×9579. [sent-525, score-0.36]
</p><p>88 They justify dimension reduction as a worthwhile preprocessing stage for achieving high efﬁciency and effectiveness. [sent-528, score-0.349]
</p><p>89 Especially for kNN classiﬁcation, the savings in computational complexity in classiﬁcation after dimension reduction are signiﬁcant. [sent-529, score-0.389]
</p><p>90 In the case of SVM the savings are also clear, since the distance between two pairs of input data points need to be computed repeatedly with and without the use of the kernel function, and the vectors become signiﬁcantly shorter with dimension reduction. [sent-530, score-0.226]
</p><p>91 Prediction results with the Centroid dimension reduction method became better compared to those from the full space for the completely disjoint MEDLINE data set, but became worse for the REUTERS data set. [sent-532, score-0.452]
</p><p>92 Since the Centroid dimension reduction method maps the centroids to unit vectors ei which are orthogonal to each other, it is helpful for the disjoint data set, but not for a data set which contains documents belonging multiple classes. [sent-533, score-0.677]
</p><p>93 We observed that prediction accuracy with the Orthogonal Centroid dimension reduction algorithm was preserved for SVMs as well as with centroid-based classiﬁcation. [sent-534, score-0.431]
</p><p>94 Another way to handle non-linearly separable data is to apply nonlinear extensions of the dimension reduction methods, including those presented in (18; 19). [sent-539, score-0.391]
</p><p>95 All of the dimension reduction methods presented here can also be applied to visualize the higher dimensional structure by reducing the dimension to 2- or 3-dimensional space. [sent-540, score-0.621]
</p><p>96 We conclude that dramatic dimension reduction of text documents can be achieved, without sacriﬁcing classiﬁcation accuracy. [sent-541, score-0.555]
</p><p>97 For the document sets we tested, the Orthogonal Centroid method did particularly well at preserving the cluster structure from the full dimensional representation. [sent-542, score-0.428]
</p><p>98 That is, the prediction accuracies for Orthogonal Centroid rival those of the full space, even though the dimension is reduced to the number of clusters. [sent-543, score-0.274]
</p><p>99 Dimensional reduction based on centroids and least squares for efﬁcient processing of text data. [sent-623, score-0.416]
</p><p>100 Lower dimensional representation of text data based on centroids and least squares, BIT Numerical Mathematics, 42(2):1–22, 2003. [sent-648, score-0.29]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('centroid', 0.553), ('knn', 0.269), ('cosine', 0.217), ('reduction', 0.197), ('document', 0.167), ('medline', 0.16), ('dimension', 0.152), ('cluster', 0.147), ('ext', 0.128), ('imension', 0.128), ('owland', 0.128), ('orthogonal', 0.125), ('sb', 0.121), ('sw', 0.119), ('ci', 0.111), ('text', 0.111), ('rm', 0.11), ('centroids', 0.108), ('eduction', 0.107), ('park', 0.099), ('sim', 0.098), ('lsi', 0.096), ('hw', 0.095), ('documents', 0.095), ('singular', 0.089), ('im', 0.087), ('hb', 0.087), ('classi', 0.086), ('lassification', 0.08), ('reduced', 0.079), ('svms', 0.073), ('semantic', 0.072), ('dimensional', 0.071), ('qt', 0.07), ('ul', 0.067), ('howland', 0.067), ('svd', 0.066), ('rl', 0.065), ('similarity', 0.065), ('urv', 0.064), ('clustered', 0.061), ('gt', 0.058), ('sv', 0.058), ('qr', 0.054), ('discriminant', 0.054), ('neighbor', 0.052), ('ni', 0.052), ('columns', 0.05), ('preserved', 0.05), ('decomposition', 0.05), ('svm', 0.049), ('clusters', 0.049), ('reducing', 0.049), ('microavg', 0.048), ('vlt', 0.048), ('rh', 0.048), ('scatter', 0.048), ('rbf', 0.045), ('trace', 0.045), ('indexing', 0.043), ('matrix', 0.043), ('full', 0.043), ('separable', 0.042), ('minnesota', 0.04), ('savings', 0.04), ('umn', 0.04), ('qh', 0.04), ('reuters', 0.04), ('siam', 0.039), ('berry', 0.036), ('jeon', 0.036), ('di', 0.035), ('polynomial', 0.034), ('kernel', 0.034), ('membership', 0.033), ('ith', 0.033), ('cation', 0.033), ('accuracy', 0.032), ('bq', 0.032), ('centroidbased', 0.032), ('colon', 0.032), ('downdating', 0.032), ('haesun', 0.032), ('hyunsoo', 0.032), ('linearopt', 0.032), ('oral', 0.032), ('peg', 0.032), ('rbfopt', 0.032), ('termdocument', 0.032), ('nearest', 0.031), ('became', 0.03), ('rank', 0.03), ('cancer', 0.03), ('machines', 0.029), ('belong', 0.029), ('upon', 0.029), ('latent', 0.028), ('decompositions', 0.028), ('ult', 0.027), ('paige', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="27-tfidf-1" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>Author: Hyunsoo Kim, Peg Howland, Haesun Park</p><p>Abstract: Support vector machines (SVMs) have been recognized as one of the most successful classiﬁcation methods for many applications including text classiﬁcation. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efﬁciently handle a large number of terms in practical applications of text classiﬁcation. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classiﬁcation algorithm and support vector classiﬁers to handle the classiﬁcation problem where a document may belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efﬁciency for both training and testing can be achieved without sacriﬁcing prediction accuracy of text classiﬁcation even when the dimension of the input space is signiﬁcantly reduced. Keywords: dimension reduction, support vector machines, text classiﬁcation, linear discriminant analysis, centroids</p><p>2 0.17991564 <a title="27-tfidf-2" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>Author: Jieping Ye</p><p>Abstract: A generalized discriminant analysis based on a new optimization criterion is presented. The criterion extends the optimization criteria of the classical Linear Discriminant Analysis (LDA) when the scatter matrices are singular. An efﬁcient algorithm for the new optimization problem is presented. The solutions to the proposed criterion form a family of algorithms for generalized LDA, which can be characterized in a closed form. We study two speciﬁc algorithms, namely Uncorrelated LDA (ULDA) and Orthogonal LDA (OLDA). ULDA was previously proposed for feature extraction and dimension reduction, whereas OLDA is a novel algorithm proposed in this paper. The features in the reduced space of ULDA are uncorrelated, while the discriminant vectors of OLDA are orthogonal to each other. We have conducted a comparative study on a variety of real-world data sets to evaluate ULDA and OLDA in terms of classiﬁcation accuracy. Keywords: dimension reduction, linear discriminant analysis, uncorrelated LDA, orthogonal LDA, singular value decomposition</p><p>3 0.081298448 <a title="27-tfidf-3" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>Author: Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, Suvrit Sra</p><p>Abstract: Several large scale data mining applications, such as text categorization and gene expression analysis, involve high-dimensional data that is also inherently directional in nature. Often such data is L2 normalized so that it lies on the surface of a unit hypersphere. Popular models such as (mixtures of) multi-variate Gaussians are inadequate for characterizing such data. This paper proposes a generative mixture-model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. In particular, we derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the mean and concentration parameters of this mixture. Numerical estimation of the concentration parameters is non-trivial in high dimensions since it involves functional inversion of ratios of Bessel functions. We also formulate two clustering algorithms corresponding to the variants of EM that we derive. Our approach provides a theoretical basis for the use of cosine similarity that has been widely employed by the information retrieval community, and obtains the spherical kmeans algorithm (kmeans with cosine similarity) as a special case of both variants. Empirical results on clustering of high-dimensional text and gene-expression data based on a mixture of vMF distributions show that the ability to estimate the concentration parameter for each vMF component, which is not present in existing approaches, yields superior results, especially for difﬁcult clustering tasks in high-dimensional spaces. Keywords: clustering, directional distributions, mixtures, von Mises-Fisher, expectation maximization, maximum likelihood, high dimensional data c 2005 Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh and Suvrit Sra. BANERJEE , D HILLON , G HOSH AND S RA</p><p>4 0.07627178 <a title="27-tfidf-4" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>Author: Lior Wolf, Amnon Shashua</p><p>Abstract: The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classiﬁcation tasks, for example, it is not uncommon to have 104 to 107 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a deﬁnition of “relevancy” based on spectral properties of the Laplacian of the features’ measurement matrix. The feature selection process is then based on a continuous ranking of the features deﬁned by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a “biased non-negativity” of a key matrix in the process. As a result, a simple leastsquares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.</p><p>5 0.055829067 <a title="27-tfidf-5" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>Author: Rie Kubota Ando, Tong Zhang</p><p>Abstract: One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don’t have a complete understanding of their eﬀectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Speciﬁcally we consider learning predictive structures on hypothesis spaces (that is, what kind of classiﬁers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the eﬀectiveness of the proposed algorithms in the semi-supervised learning setting.</p><p>6 0.054803681 <a title="27-tfidf-6" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>7 0.050897971 <a title="27-tfidf-7" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>8 0.046129413 <a title="27-tfidf-8" href="./jmlr-2005-Local_Propagation_in_Conditional_Gaussian_Bayesian_Networks.html">51 jmlr-2005-Local Propagation in Conditional Gaussian Bayesian Networks</a></p>
<p>9 0.045209214 <a title="27-tfidf-9" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>10 0.042697843 <a title="27-tfidf-10" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>11 0.04163852 <a title="27-tfidf-11" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>12 0.04106278 <a title="27-tfidf-12" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>13 0.040618151 <a title="27-tfidf-13" href="./jmlr-2005-On_the_Nystr%C3%B6m_Method_for_Approximating_a_Gram_Matrix_for_Improved_Kernel-Based_Learning.html">60 jmlr-2005-On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning</a></p>
<p>14 0.039860331 <a title="27-tfidf-14" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>15 0.039335914 <a title="27-tfidf-15" href="./jmlr-2005-A_Generalization_Error_for_Q-Learning.html">5 jmlr-2005-A Generalization Error for Q-Learning</a></p>
<p>16 0.037510339 <a title="27-tfidf-16" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>17 0.033027574 <a title="27-tfidf-17" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>18 0.032714657 <a title="27-tfidf-18" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>19 0.03270191 <a title="27-tfidf-19" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>20 0.032173462 <a title="27-tfidf-20" href="./jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines.html">73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.211), (1, 0.106), (2, 0.126), (3, -0.067), (4, 0.166), (5, -0.052), (6, -0.247), (7, 0.291), (8, -0.017), (9, -0.125), (10, -0.432), (11, 0.182), (12, 0.075), (13, -0.039), (14, 0.046), (15, -0.068), (16, -0.001), (17, -0.086), (18, 0.098), (19, -0.022), (20, 0.001), (21, -0.007), (22, -0.029), (23, -0.122), (24, 0.007), (25, 0.033), (26, 0.07), (27, 0.031), (28, 0.081), (29, 0.04), (30, -0.015), (31, 0.024), (32, -0.023), (33, 0.073), (34, -0.007), (35, -0.061), (36, 0.036), (37, 0.079), (38, 0.012), (39, 0.032), (40, 0.014), (41, -0.005), (42, 0.012), (43, -0.059), (44, -0.043), (45, 0.019), (46, 0.087), (47, 0.053), (48, -0.023), (49, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96064478 <a title="27-lsi-1" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>Author: Hyunsoo Kim, Peg Howland, Haesun Park</p><p>Abstract: Support vector machines (SVMs) have been recognized as one of the most successful classiﬁcation methods for many applications including text classiﬁcation. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efﬁciently handle a large number of terms in practical applications of text classiﬁcation. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classiﬁcation algorithm and support vector classiﬁers to handle the classiﬁcation problem where a document may belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efﬁciency for both training and testing can be achieved without sacriﬁcing prediction accuracy of text classiﬁcation even when the dimension of the input space is signiﬁcantly reduced. Keywords: dimension reduction, support vector machines, text classiﬁcation, linear discriminant analysis, centroids</p><p>2 0.84766114 <a title="27-lsi-2" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>Author: Jieping Ye</p><p>Abstract: A generalized discriminant analysis based on a new optimization criterion is presented. The criterion extends the optimization criteria of the classical Linear Discriminant Analysis (LDA) when the scatter matrices are singular. An efﬁcient algorithm for the new optimization problem is presented. The solutions to the proposed criterion form a family of algorithms for generalized LDA, which can be characterized in a closed form. We study two speciﬁc algorithms, namely Uncorrelated LDA (ULDA) and Orthogonal LDA (OLDA). ULDA was previously proposed for feature extraction and dimension reduction, whereas OLDA is a novel algorithm proposed in this paper. The features in the reduced space of ULDA are uncorrelated, while the discriminant vectors of OLDA are orthogonal to each other. We have conducted a comparative study on a variety of real-world data sets to evaluate ULDA and OLDA in terms of classiﬁcation accuracy. Keywords: dimension reduction, linear discriminant analysis, uncorrelated LDA, orthogonal LDA, singular value decomposition</p><p>3 0.25914615 <a title="27-lsi-3" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>Author: Lior Wolf, Amnon Shashua</p><p>Abstract: The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classiﬁcation tasks, for example, it is not uncommon to have 104 to 107 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a deﬁnition of “relevancy” based on spectral properties of the Laplacian of the features’ measurement matrix. The feature selection process is then based on a continuous ranking of the features deﬁned by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a “biased non-negativity” of a key matrix in the process. As a result, a simple leastsquares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.</p><p>4 0.25806659 <a title="27-lsi-4" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>Author: Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, Suvrit Sra</p><p>Abstract: Several large scale data mining applications, such as text categorization and gene expression analysis, involve high-dimensional data that is also inherently directional in nature. Often such data is L2 normalized so that it lies on the surface of a unit hypersphere. Popular models such as (mixtures of) multi-variate Gaussians are inadequate for characterizing such data. This paper proposes a generative mixture-model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. In particular, we derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the mean and concentration parameters of this mixture. Numerical estimation of the concentration parameters is non-trivial in high dimensions since it involves functional inversion of ratios of Bessel functions. We also formulate two clustering algorithms corresponding to the variants of EM that we derive. Our approach provides a theoretical basis for the use of cosine similarity that has been widely employed by the information retrieval community, and obtains the spherical kmeans algorithm (kmeans with cosine similarity) as a special case of both variants. Empirical results on clustering of high-dimensional text and gene-expression data based on a mixture of vMF distributions show that the ability to estimate the concentration parameter for each vMF component, which is not present in existing approaches, yields superior results, especially for difﬁcult clustering tasks in high-dimensional spaces. Keywords: clustering, directional distributions, mixtures, von Mises-Fisher, expectation maximization, maximum likelihood, high dimensional data c 2005 Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh and Suvrit Sra. BANERJEE , D HILLON , G HOSH AND S RA</p><p>5 0.23576 <a title="27-lsi-5" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>Author: Rie Kubota Ando, Tong Zhang</p><p>Abstract: One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don’t have a complete understanding of their eﬀectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Speciﬁcally we consider learning predictive structures on hypothesis spaces (that is, what kind of classiﬁers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the eﬀectiveness of the proposed algorithms in the semi-supervised learning setting.</p><p>6 0.2039104 <a title="27-lsi-6" href="./jmlr-2005-On_the_Nystr%C3%B6m_Method_for_Approximating_a_Gram_Matrix_for_Improved_Kernel-Based_Learning.html">60 jmlr-2005-On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning</a></p>
<p>7 0.20196629 <a title="27-lsi-7" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>8 0.18676385 <a title="27-lsi-8" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>9 0.18487333 <a title="27-lsi-9" href="./jmlr-2005-A_Generalization_Error_for_Q-Learning.html">5 jmlr-2005-A Generalization Error for Q-Learning</a></p>
<p>10 0.18431322 <a title="27-lsi-10" href="./jmlr-2005-Local_Propagation_in_Conditional_Gaussian_Bayesian_Networks.html">51 jmlr-2005-Local Propagation in Conditional Gaussian Bayesian Networks</a></p>
<p>11 0.1792748 <a title="27-lsi-11" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>12 0.17811403 <a title="27-lsi-12" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>13 0.17310628 <a title="27-lsi-13" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>14 0.16724533 <a title="27-lsi-14" href="./jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</a></p>
<p>15 0.16142565 <a title="27-lsi-15" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>16 0.16003367 <a title="27-lsi-16" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>17 0.15382908 <a title="27-lsi-17" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>18 0.15245211 <a title="27-lsi-18" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>19 0.14448775 <a title="27-lsi-19" href="./jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines.html">73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</a></p>
<p>20 0.14394076 <a title="27-lsi-20" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.024), (17, 0.016), (19, 0.013), (26, 0.027), (36, 0.02), (37, 0.018), (42, 0.53), (43, 0.035), (47, 0.028), (52, 0.077), (70, 0.028), (88, 0.084), (94, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95213944 <a title="27-lda-1" href="./jmlr-2005-Combining_Information_Extraction_Systems_Using_Voting_and_Stacked_Generalization.html">21 jmlr-2005-Combining Information Extraction Systems Using Voting and Stacked Generalization</a></p>
<p>Author: Georgios Sigletos, Georgios Paliouras, Constantine D. Spyropoulos, Michalis Hatzopoulos</p><p>Abstract: This article investigates the effectiveness of voting and stacked generalization -also known as stacking- in the context of information extraction (IE). A new stacking framework is proposed that accommodates well-known approaches for IE. The key idea is to perform cross-validation on the base-level data set, which consists of text documents annotated with relevant information, in order to create a meta-level data set that consists of feature vectors. A classifier is then trained using the new vectors. Therefore, base-level IE systems are combined with a common classifier at the metalevel. Various voting schemes are presented for comparing against stacking in various IE domains. Well known IE systems are employed at the base-level, together with a variety of classifiers at the meta-level. Results show that both voting and stacking work better when relying on probabilistic estimates by the base-level systems. Voting proved to be effective in most domains in the experiments. Stacking, on the other hand, proved to be consistently effective over all domains, doing comparably or better than voting and always better than the best base-level systems. Particular emphasis is also given to explaining the results obtained by voting and stacking at the meta-level, with respect to the varying degree of similarity in the output of the base-level systems. Keywords: stacking, voting, information extraction, cross-validation 1</p><p>same-paper 2 0.90162253 <a title="27-lda-2" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>Author: Hyunsoo Kim, Peg Howland, Haesun Park</p><p>Abstract: Support vector machines (SVMs) have been recognized as one of the most successful classiﬁcation methods for many applications including text classiﬁcation. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efﬁciently handle a large number of terms in practical applications of text classiﬁcation. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classiﬁcation algorithm and support vector classiﬁers to handle the classiﬁcation problem where a document may belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efﬁciency for both training and testing can be achieved without sacriﬁcing prediction accuracy of text classiﬁcation even when the dimension of the input space is signiﬁcantly reduced. Keywords: dimension reduction, support vector machines, text classiﬁcation, linear discriminant analysis, centroids</p><p>3 0.42059818 <a title="27-lda-3" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>Author: Jieping Ye</p><p>Abstract: A generalized discriminant analysis based on a new optimization criterion is presented. The criterion extends the optimization criteria of the classical Linear Discriminant Analysis (LDA) when the scatter matrices are singular. An efﬁcient algorithm for the new optimization problem is presented. The solutions to the proposed criterion form a family of algorithms for generalized LDA, which can be characterized in a closed form. We study two speciﬁc algorithms, namely Uncorrelated LDA (ULDA) and Orthogonal LDA (OLDA). ULDA was previously proposed for feature extraction and dimension reduction, whereas OLDA is a novel algorithm proposed in this paper. The features in the reduced space of ULDA are uncorrelated, while the discriminant vectors of OLDA are orthogonal to each other. We have conducted a comparative study on a variety of real-world data sets to evaluate ULDA and OLDA in terms of classiﬁcation accuracy. Keywords: dimension reduction, linear discriminant analysis, uncorrelated LDA, orthogonal LDA, singular value decomposition</p><p>4 0.32765505 <a title="27-lda-4" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>Author: Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, Suvrit Sra</p><p>Abstract: Several large scale data mining applications, such as text categorization and gene expression analysis, involve high-dimensional data that is also inherently directional in nature. Often such data is L2 normalized so that it lies on the surface of a unit hypersphere. Popular models such as (mixtures of) multi-variate Gaussians are inadequate for characterizing such data. This paper proposes a generative mixture-model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. In particular, we derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the mean and concentration parameters of this mixture. Numerical estimation of the concentration parameters is non-trivial in high dimensions since it involves functional inversion of ratios of Bessel functions. We also formulate two clustering algorithms corresponding to the variants of EM that we derive. Our approach provides a theoretical basis for the use of cosine similarity that has been widely employed by the information retrieval community, and obtains the spherical kmeans algorithm (kmeans with cosine similarity) as a special case of both variants. Empirical results on clustering of high-dimensional text and gene-expression data based on a mixture of vMF distributions show that the ability to estimate the concentration parameter for each vMF component, which is not present in existing approaches, yields superior results, especially for difﬁcult clustering tasks in high-dimensional spaces. Keywords: clustering, directional distributions, mixtures, von Mises-Fisher, expectation maximization, maximum likelihood, high dimensional data c 2005 Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh and Suvrit Sra. BANERJEE , D HILLON , G HOSH AND S RA</p><p>5 0.29396647 <a title="27-lda-5" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>Author: Aharon Bar-Hillel, Tomer Hertz, Noam Shental, Daphna Weinshall</p><p>Abstract: Many learning algorithms use a metric deﬁned over the input space as a principal tool, and their performance critically depends on the quality of this metric. We address the problem of learning metrics using side-information in the form of equivalence constraints. Unlike labels, we demonstrate that this type of side-information can sometimes be automatically obtained without the need of human intervention. We show how such side-information can be used to modify the representation of the data, leading to improved clustering and classiﬁcation. Speciﬁcally, we present the Relevant Component Analysis (RCA) algorithm, which is a simple and efﬁcient algorithm for learning a Mahalanobis metric. We show that RCA is the solution of an interesting optimization problem, founded on an information theoretic basis. If dimensionality reduction is allowed within RCA, we show that it is optimally accomplished by a version of Fisher’s linear discriminant that uses constraints. Moreover, under certain Gaussian assumptions, RCA can be viewed as a Maximum Likelihood estimation of the within class covariance matrix. We conclude with extensive empirical evaluations of RCA, showing its advantage over alternative methods. Keywords: clustering, metric learning, dimensionality reduction, equivalence constraints, side information.</p><p>6 0.29373032 <a title="27-lda-6" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>7 0.2828601 <a title="27-lda-7" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>8 0.28230804 <a title="27-lda-8" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>9 0.28167328 <a title="27-lda-9" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>10 0.28118703 <a title="27-lda-10" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>11 0.27508157 <a title="27-lda-11" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>12 0.27306357 <a title="27-lda-12" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>13 0.27293968 <a title="27-lda-13" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>14 0.27193934 <a title="27-lda-14" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>15 0.26861519 <a title="27-lda-15" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>16 0.26832408 <a title="27-lda-16" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>17 0.26808944 <a title="27-lda-17" href="./jmlr-2005-Frames%2C_Reproducing_Kernels%2C_Regularization_and_Learning.html">35 jmlr-2005-Frames, Reproducing Kernels, Regularization and Learning</a></p>
<p>18 0.26463202 <a title="27-lda-18" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>19 0.25988537 <a title="27-lda-19" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>20 0.25849715 <a title="27-lda-20" href="./jmlr-2005-Generalization_Bounds_for_the_Area_Under_the_ROC_Curve.html">38 jmlr-2005-Generalization Bounds for the Area Under the ROC Curve</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
