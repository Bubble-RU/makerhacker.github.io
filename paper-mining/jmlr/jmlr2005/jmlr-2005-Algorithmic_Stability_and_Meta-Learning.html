<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>11 jmlr-2005-Algorithmic Stability and Meta-Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-11" href="#">jmlr2005-11</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>11 jmlr-2005-Algorithmic Stability and Meta-Learning</h1>
<br/><p>Source: <a title="jmlr-2005-11-pdf" href="http://jmlr.org/papers/volume6/maurer05a/maurer05a.pdf">pdf</a></p><p>Author: Andreas Maurer</p><p>Abstract: A mechnism of transfer learning is analysed, where samples drawn from different learning tasks of an environment are used to improve the learners performance on a new task. We give a general method to prove generalisation error bounds for such meta-algorithms. The method can be applied to the bias learning model of J. Baxter and to derive novel generalisation bounds for metaalgorithms searching spaces of uniformly stable algorithms. We also present an application to regularized least squares regression. Keywords: algorithmic stability, meta-learning, learning to learn</p><p>Reference: <a title="jmlr-2005-11-reference" href="../jmlr2005_reference/jmlr-2005-Algorithmic_Stability_and_Meta-Learning_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lemp', 0.78), ('dm', 0.305), ('lloo', 0.26), ('ah', 0.18), ('baxt', 0.169), ('environ', 0.141), ('aur', 0.14), ('sn', 0.112), ('transf', 0.111), ('ordin', 0.105), ('gi', 0.082), ('zm', 0.071), ('hk', 0.071), ('hh', 0.067), ('elisseeff', 0.066), ('si', 0.063), ('hypothes', 0.06), ('ln', 0.053), ('bousquet', 0.048), ('met', 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="11-tfidf-1" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>2 0.082700171 <a title="11-tfidf-2" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>3 0.080524951 <a title="11-tfidf-3" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>4 0.053261202 <a title="11-tfidf-4" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>5 0.040783748 <a title="11-tfidf-5" href="./jmlr-2005-Stability_of_Randomized_Learning_Algorithms.html">67 jmlr-2005-Stability of Randomized Learning Algorithms</a></p>
<p>6 0.039167672 <a title="11-tfidf-6" href="./jmlr-2005-Generalization_Bounds_and_Complexities_Based_on_Sparsity_and_Clustering_for_Convex_Combinations_of_Functions_from_Random_Classes.html">37 jmlr-2005-Generalization Bounds and Complexities Based on Sparsity and Clustering for Convex Combinations of Functions from Random Classes</a></p>
<p>7 0.037298381 <a title="11-tfidf-7" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>8 0.033733152 <a title="11-tfidf-8" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>9 0.030585572 <a title="11-tfidf-9" href="./jmlr-2005-Learning_with_Decision_Lists_of_Data-Dependent_Features.html">50 jmlr-2005-Learning with Decision Lists of Data-Dependent Features</a></p>
<p>10 0.027393948 <a title="11-tfidf-10" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>11 0.027338047 <a title="11-tfidf-11" href="./jmlr-2005-Asymptotics_in_Empirical_Risk_Minimization.html">16 jmlr-2005-Asymptotics in Empirical Risk Minimization</a></p>
<p>12 0.025694149 <a title="11-tfidf-12" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>13 0.025501041 <a title="11-tfidf-13" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth Îµ-Insensitive Regression by Loss Symmetrization</a></p>
<p>14 0.024848493 <a title="11-tfidf-14" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>15 0.02378563 <a title="11-tfidf-15" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>16 0.02313282 <a title="11-tfidf-16" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>17 0.022573801 <a title="11-tfidf-17" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>18 0.022113692 <a title="11-tfidf-18" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>19 0.021818243 <a title="11-tfidf-19" href="./jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</a></p>
<p>20 0.021099901 <a title="11-tfidf-20" href="./jmlr-2005-Efficient_Margin_Maximizing_with_Boosting.html">29 jmlr-2005-Efficient Margin Maximizing with Boosting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.139), (1, -0.051), (2, -0.005), (3, -0.159), (4, -0.029), (5, 0.09), (6, -0.003), (7, -0.027), (8, 0.066), (9, -0.078), (10, 0.103), (11, 0.097), (12, 0.084), (13, 0.029), (14, 0.203), (15, -0.039), (16, 0.125), (17, -0.041), (18, 0.124), (19, 0.062), (20, 0.153), (21, -0.374), (22, 0.134), (23, -0.286), (24, -0.118), (25, 0.17), (26, 0.021), (27, 0.178), (28, 0.034), (29, -0.161), (30, -0.019), (31, 0.269), (32, 0.001), (33, -0.124), (34, 0.126), (35, -0.047), (36, -0.035), (37, 0.246), (38, 0.204), (39, -0.068), (40, 0.278), (41, -0.057), (42, 0.002), (43, 0.004), (44, 0.064), (45, 0.133), (46, 0.006), (47, 0.154), (48, 0.129), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92057467 <a title="11-lsi-1" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>2 0.2684285 <a title="11-lsi-2" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>3 0.23029937 <a title="11-lsi-3" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>4 0.19252381 <a title="11-lsi-4" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>5 0.14892118 <a title="11-lsi-5" href="./jmlr-2005-Efficient_Margin_Maximizing_with_Boosting.html">29 jmlr-2005-Efficient Margin Maximizing with Boosting</a></p>
<p>6 0.1223595 <a title="11-lsi-6" href="./jmlr-2005-Stability_of_Randomized_Learning_Algorithms.html">67 jmlr-2005-Stability of Randomized Learning Algorithms</a></p>
<p>7 0.12224701 <a title="11-lsi-7" href="./jmlr-2005-Asymptotics_in_Empirical_Risk_Minimization.html">16 jmlr-2005-Asymptotics in Empirical Risk Minimization</a></p>
<p>8 0.11655791 <a title="11-lsi-8" href="./jmlr-2005-Generalization_Bounds_for_the_Area_Under_the_ROC_Curve.html">38 jmlr-2005-Generalization Bounds for the Area Under the ROC Curve</a></p>
<p>9 0.11172543 <a title="11-lsi-9" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>10 0.10241352 <a title="11-lsi-10" href="./jmlr-2005-Adaptive_Online_Prediction_by_Following_the_Perturbed_Leader.html">10 jmlr-2005-Adaptive Online Prediction by Following the Perturbed Leader</a></p>
<p>11 0.10220417 <a title="11-lsi-11" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>12 0.10171026 <a title="11-lsi-12" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>13 0.099170171 <a title="11-lsi-13" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>14 0.098098904 <a title="11-lsi-14" href="./jmlr-2005-Generalization_Bounds_and_Complexities_Based_on_Sparsity_and_Clustering_for_Convex_Combinations_of_Functions_from_Random_Classes.html">37 jmlr-2005-Generalization Bounds and Complexities Based on Sparsity and Clustering for Convex Combinations of Functions from Random Classes</a></p>
<p>15 0.097461745 <a title="11-lsi-15" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>16 0.093067437 <a title="11-lsi-16" href="./jmlr-2005-Learning_with_Decision_Lists_of_Data-Dependent_Features.html">50 jmlr-2005-Learning with Decision Lists of Data-Dependent Features</a></p>
<p>17 0.091601044 <a title="11-lsi-17" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>18 0.090810835 <a title="11-lsi-18" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>19 0.087357767 <a title="11-lsi-19" href="./jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</a></p>
<p>20 0.086542569 <a title="11-lsi-20" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth Îµ-Insensitive Regression by Loss Symmetrization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.01), (11, 0.033), (35, 0.023), (37, 0.037), (44, 0.034), (47, 0.01), (60, 0.026), (62, 0.109), (68, 0.057), (74, 0.011), (78, 0.041), (79, 0.01), (81, 0.424), (88, 0.025), (97, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.59058166 <a title="11-lda-1" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>2 0.31396595 <a title="11-lda-2" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>3 0.31182966 <a title="11-lda-3" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>4 0.31063005 <a title="11-lda-4" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>5 0.31010202 <a title="11-lda-5" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth Îµ-Insensitive Regression by Loss Symmetrization</a></p>
<p>6 0.30874708 <a title="11-lda-6" href="./jmlr-2005-Efficient_Margin_Maximizing_with_Boosting.html">29 jmlr-2005-Efficient Margin Maximizing with Boosting</a></p>
<p>7 0.30847743 <a title="11-lda-7" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>8 0.30782345 <a title="11-lda-8" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>9 0.30689785 <a title="11-lda-9" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>10 0.30591363 <a title="11-lda-10" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>11 0.30568886 <a title="11-lda-11" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels Â Â Â Â (Kernel Machines Section)</a></p>
<p>12 0.3056711 <a title="11-lda-12" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>13 0.30532786 <a title="11-lda-13" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>14 0.30500969 <a title="11-lda-14" href="./jmlr-2005-Clustering_with_Bregman_Divergences.html">20 jmlr-2005-Clustering with Bregman Divergences</a></p>
<p>15 0.30422124 <a title="11-lda-15" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>16 0.30364123 <a title="11-lda-16" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>17 0.30352953 <a title="11-lda-17" href="./jmlr-2005-A_Unifying_View_of_Sparse_Approximate_Gaussian_Process_Regression.html">7 jmlr-2005-A Unifying View of Sparse Approximate Gaussian Process Regression</a></p>
<p>18 0.30220592 <a title="11-lda-18" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>19 0.3019599 <a title="11-lda-19" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>20 0.30141196 <a title="11-lda-20" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
