<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-13" href="#">jmlr2005-13</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</h1>
<br/><p>Source: <a title="jmlr-2005-13-pdf" href="http://jmlr.org/papers/volume6/markatou05a/markatou05a.pdf">pdf</a></p><p>Author: Marianthi Markatou, Hong Tian, Shameek Biswas, George Hripcsak</p><p>Abstract: This paper brings together methods from two different disciplines: statistics and machine learning. We address the problem of estimating the variance of cross-validation (CV) estimators of the generalization error. In particular, we approach the problem of variance estimation of the CV estimators of generalization error as a problem in approximating the moments of a statistic. The approximation illustrates the role of training and test sets in the performance of the algorithm. It provides a unifying approach to evaluation of various methods used in obtaining training and test sets and it takes into account the variability due to different training and test sets. For the simple problem of predicting the sample mean and in the case of smooth loss functions, we show that the variance of the CV estimator of the generalization error is a function of the moments of the random T T variables Y = Card(S j S j ) and Y ∗ = Card(Sc Sc ), where S j , S j are two training sets, and Sc , j j j c are the corresponding test sets. We prove that the distribution of Y and Y* is hypergeometric Sj and we compare our estimator with the one proposed by Nadeau and Bengio (2003). We extend these results in the regression case and the case of absolute error loss, and indicate how the methods can be extended to the classiﬁcation case. We illustrate the results through simulation. Keywords: cross-validation, generalization error, moment approximation, prediction, variance estimation</p><p>Reference: <a title="jmlr-2005-13-reference" href="../jmlr2005_reference/jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, providing a variance estimate of the estimator of this generalization error is a more difﬁcult problem. [sent-24, score-0.301]
</p><p>2 This is because the generalization error depends on the loss function involved, and the mathematics needed to analyze the variance of the estimator are complicated. [sent-25, score-0.338]
</p><p>3 An estimator of variance of the cross-validation estimator of the generalization error is proposed by Nadeau and Bengio (2003). [sent-26, score-0.477]
</p><p>4 In this paper we address estimation of the variance of the cross validation estimator of the generalization error, using the method of moment approximation. [sent-28, score-0.453]
</p><p>5 The cross validation estimator of the generalization error is viewed as a statistic. [sent-30, score-0.291]
</p><p>6 We present a framework that allows computation of the variance estimator of the generalization error for k fold cross validation, as well as the usual random set selection in cross validation. [sent-33, score-0.401]
</p><p>7 We then obtain variance estimators of the generalization error for the k-fold cross validation estimator, and extend the results to the regression case. [sent-36, score-0.312]
</p><p>8 Section 2 introduces the framework and discusses existing literature on the problem of variance estimation of the cross validation estimators of the generalization error. [sent-39, score-0.288]
</p><p>9 Section 4 presents computer experiments and compares our estimator with the estimator proposed by Nadeau and Bengio (2003). [sent-41, score-0.364]
</p><p>10 The variance estimate of the cross validation estimator of the generalization error can be computed under the following cross validation schemes. [sent-57, score-0.469]
</p><p>11 We now describe in detail the cross validation estimator of the generalization error whose variance we will study. [sent-63, score-0.385]
</p><p>12 2)  j=1  This version of the cross validation estimator of the generalization error depends on the value of J, the size of the training and test sets and the size of the data universe. [sent-78, score-0.313]
</p><p>13 In a later section we will see that, when J is chosen appropriately, then the Nadeau and Bengio (2003) estimator is close to and performs similarly with the moment approximation estimator in some of the cases we study. [sent-82, score-0.449]
</p><p>14 The propose an approximation to the correlation, ρ n ﬁnal estimator of the variance of n2 µJ is given as n1 ˆ  Nadeau and Bengio (2003) note that the above suggested estimator is simple but it may have a positive or negative bias with respect to the actual Var(n2 µJ ). [sent-95, score-0.482]
</p><p>15 Nadeau and Bengio (2003) also suggested another estimator of the variance of the cross-validation estimator of the generalization error. [sent-98, score-0.463]
</p><p>16 Denote by µ1 the estimator n2 µJ computed on the ﬁrst data subset and ˆ ˆ n1 n2 µ2 the estimator n µJ computed on the second data subset. [sent-105, score-0.352]
</p><p>17 To obtain an estimator of the variance of ˆ ˆ 1 the cross validation estimator of the generalization error compute the sample variance of µ1 and µ2 . [sent-106, score-0.665]
</p><p>18 Here, we derive estimators of the variance of the k-fold cross validation estimator of the generalization error that are almost unbiased. [sent-112, score-0.478]
</p><p>19 Our work has similarities with the work by McLachlan in the sense that we derive approximations to the moments of the distribution of the cross validation estimator of the generalization error and use these to obtain a variance estimator. [sent-117, score-0.399]
</p><p>20 A second reason in favor of this case is because, under square error loss, we obtain a “ golden standard” against which we can compare the new empirically computed variance estimator and the Nadeau and Bengio (2003) estimator. [sent-137, score-0.295]
</p><p>21 The following proposition establishes the approximation formula for the covariance terms that enter the computation of the variance of the cross validation estimators of the generalization error. [sent-215, score-0.35]
</p><p>22 An additional random variable that enters the variance estimator of the cross validation estiT mator of the generalization error is Y ∗ = Card(Sc Sc ), the cardinality of the intersection of two j j different test sets. [sent-223, score-0.399]
</p><p>23 n 4n1 n1 1136  VARIANCE OF C ROSS -VALIDATION E STIMATORS OF THE G ENERALIZATION E RROR  The ﬁnal estimator of the variance of n2 µJ is a plug-in estimator and it can be computed using n1 ˆ theorem (3. [sent-260, score-0.446]
</p><p>24 there are many different training sets, take as an estimator of the sample mean X J j=1 n1 ¯ ˆj Moreover, σ2 = n11 ∑l=1 (Xl − XS j )2 , thus the variance estimate of the population variance will be −1 2= 1 J 2. [sent-264, score-0.4]
</p><p>25 10)  If the data are from a N(0, σ2 ) then the moment approximation estimator of the variance of n2 µJ n1 ˆ is given by 2(n1 + 2) 1 J − 1 2(n + 2) 1 ˆ σ4 { +( )[ − 2 ]}, n1 n2 J J n2 n1 ˆ where σ is the sample standard deviation. [sent-269, score-0.377]
</p><p>26 Thus the estimator of the variance n2 µJ is a multiple n1 ˆ of the sample variance and the multiplication factor indicates the dependence of the estimator on n1 , n2 and n. [sent-270, score-0.559]
</p><p>27 Variance estimator of the k-fold CV estimator of the generalization error. [sent-271, score-0.369]
</p><p>28 Here we present a variance estimator of the k-fold cross validation estimator of the generalization error of a learning algorithm. [sent-272, score-0.561]
</p><p>29 To illustrate, if we further assume a normal population then Var[(Xi − µ)2 ] = 2σ4 and the variance estimator of n2 µJ is given as n1 ˆ ˆ 3k σ4 (2 + ), n n(k − 1) ˆ where σ is the sample standard deviation. [sent-292, score-0.298]
</p><p>30 We consider here the problem of estimating the variance of the cross validation estimator of the generalization error n2 µJ in the case of n1 ˆ regression. [sent-295, score-0.385]
</p><p>31 Then for a new observation (yi , xi ) ∈ Sc denote j T β , where β indicates the estimator of β computed by using the data in the training ˆS by yi,S j = xi ˆ S j ˆ j set S j . [sent-299, score-0.329]
</p><p>32 The loss function L is then dependent on yi,S j and yi , that is L(yi,S j , yi ). [sent-300, score-0.297]
</p><p>33 ˆ ˆ To derive the estimator of Var(n2 µJ ) we need to use the moment approximation method to obn1 ˆ tain approximations for the moments of the statistic n2 µJ . [sent-301, score-0.287]
</p><p>34 Then E[L(yi,S j , yi )] = E[L(xiT β0 , yi )] + ˆ  σ2 E[L (xiT β0 , yi )]tr[(xi xiT )(XT j XS j )−1 ] + Rn , S 2  1 where the remainder term is of order O( n2 ), and the prime indicates derivative with respect to the 1 ﬁrst argument of the loss function. [sent-316, score-0.453]
</p><p>35 Proof: First expand L(yi,S j , yi ) with respect to the ﬁrst argument to obtain: ˆ ˆ L(yi,S j , yi ) = L(xiT β0 , yi ) + L (xiT β0 , yi )xiT (βS j − β0 ) ˆ 1 ˆ ˆ L (xiT β0 , yi )(βS j − β0 )T xi xiT (βS j − β0 ) + Rn , + 2  (3. [sent-317, score-0.718]
</p><p>36 Therefore  E[L(yi,S j , yi )] = E[L(xiT β0 , yi )] + ˆ  σ2 E[L (xiT β0 , yi )]tr[(xi xiT )(XT j XS j )−1 ] + Rn , S 2 1139  M ARKATOU , T IAN , B ISWAS AND H RIPCSAK  where the expectations are taken with respect to the distribution of the data. [sent-321, score-0.39]
</p><p>37 Then Var(L(yi,S j , yi )) can be approxiˆ mated as follows: Var{L(yi,S j , yi )} = Var[L(xiT β0 , yi )]} + σ2tr[(xi xiT )(XT j XS j )−1 {Cov(L(xiT β0 , yi ), ˆ S L (xiT β0 , yi )) + E[L (xiT β0 , yi )]2 } + Rn ,  n where σ2 = VarZ1 (Yi |Xi ) and Rn is the remaining term of order  1 . [sent-327, score-0.78]
</p><p>38 4 to and we use the fact that  L2 (yi,S j , yi ) ˆ  [L2 (yi,S j , yi )] ˆ  = 2L(yi,S j , yi )L (yi,S j , yi ) + 2[L (yi,S j , yi )]2 , ˆ ˆ ˆ  where prime indicates derivative with respect to the ﬁrst argument of the loss function. [sent-330, score-0.705]
</p><p>39 To verify the above approximations we use L(yi,S j , yi ) = (yi,S j − yi )2 , the square error ˆ ˆ loss and the case of simple regression, that is yi = a + bzi + εi = xiT β + εi , ˆ ˆ where xiT = (1, zi ), βT = (a, b) and (yi , xi ) ∈ Sc . [sent-332, score-0.52]
</p><p>40 j ˆ The exact expectation of L(yi,S j , yi ) = (xiT βS j − yi )2 is given as: ˆ E[L(yi,S j , yi )] = σ2 + σ2 xiT (XT j XS j )−1 xi . [sent-334, score-0.478]
</p><p>41 To illustrate further the formulas assume that yi ∼ N(xiT β, σ2 ), then the exact calculation gives the variance of L(yi,S j , yi ), ˆ Var(L(yi,S j , yi )) = 2σ4 + 4σ4 xiT (XT j XS j )−1 xi + 2σ4 (xi (XT j XS j )−1 xi )2 . [sent-337, score-0.633]
</p><p>42 1140  1 ), n2 1  VARIANCE OF C ROSS -VALIDATION E STIMATORS OF THE G ENERALIZATION E RROR  To complete the variance approximation of the estimator n2 µJ we need an approximation of the n1 ˆ covariance between L(yi,S j , yi ) and L(yi ,S j , yi ). [sent-339, score-0.572]
</p><p>43 The following proposition expresses the approxiˆ ˆ mation of Cov(L(yi,S j , yi ), L(yi,S j , yi )). [sent-340, score-0.284]
</p><p>44 Then for i = i Cov(L(yi,S j , yi ), L(yi ,S j , yi )) = σ2 (E[L (xiT β0 , yi )])2tr[(xi xiT )(XT j XS j )−1 ] ˆ ˆ S +  σ4 (E[L (xiT β0 , yi )])2tr[(xi xiT )(XT j XS j )−1 (xi xiT )(XT j XS j )−1 ]. [sent-349, score-0.52]
</p><p>45 Remark: If the loss is square error, Cov(L(yi,S j , yi ), L(yi ,S j , yi )) = 2σ4tr[(xi xiT )(XT j XS j )−1 (xi xiT )(XT j XS j )−1 ]. [sent-353, score-0.308]
</p><p>46 To µ ˆ ˆ obtain an estimator of σ2 , we ﬁt the regression model and obtain yi . [sent-364, score-0.316]
</p><p>47 Then σ2 is the sample variance ˆ ˆ of the errors εi = yi − yi , that is the residual mean square. [sent-365, score-0.364]
</p><p>48 The simulation experiments compare the proposed estimators with the Nadeau and Bengio estimator under two different error losses, the square error and the absolute error loss. [sent-378, score-0.338]
</p><p>49 We then computed 1 2 Sµ j = J−1 ∑J (ˆ j − n2 µJ )2 and the estimator of the variance of the generalization error, given as n1 ˆ j=1 µ ˆ n2 2 1 ( J + n1 )Sµ j . [sent-384, score-0.287]
</p><p>50 The variance is computed by simply taking the sample variance of the estimator that was computed over the 100 independent data sets. [sent-397, score-0.374]
</p><p>51 The fourth column of the table reports the value of the moment approximation estimator of the variance of the cross validation estimator of the generalization error, while the ﬁfth column reports the sample variance of the moment approximation estimator. [sent-398, score-0.89]
</p><p>52 Nadeau-Bengio (NB) and moment approximation (MA) estimators of the variance of the cross validation estimator of the generalization error, and their sample variances. [sent-436, score-0.571]
</p><p>53 We notice that the variance of the moment approximation estimator is at least one order of magnitude smaller than the variance of the Nadeau- Bengio estimator, thereby increasing the accuracy of the moment estimator. [sent-439, score-0.558]
</p><p>54 026  nb ma  10  20  30  40  50  size of the test set  Figure 1: Simple mean case n=100, J=15  seems to ﬂuctuate (this also is indicated by the value of the sample variance associated with the estimator and reported in table 1. [sent-448, score-0.461]
</p><p>55 Moment approximation (MA) and Nadeau-Bengio (NB) estimators of the variance the cross validation estimator of the generalization error and their sample variances. [sent-486, score-0.503]
</p><p>56 In this case we notice that the variance of the moment approximation estimator is about half of the variance of the Nadeau-Bengio estimator. [sent-492, score-0.476]
</p><p>57 022  nb ma  10  20  30  40  50  size of the test set  Figure 2: Simple mean case n=100, J=50 Figure 2 shows a plot of Nadeau-Bengio and moment approximation estimate of the variance as a function of the size of the test set. [sent-495, score-0.373]
</p><p>58 Table 3 presents the values of the two variance estimators as well as their variance when the data universe has size n = 1000, for the case J = 15 and J = 50. [sent-497, score-0.384]
</p><p>59 We notice that the performance, in terms of variance, of the moment approximation estimator is, in both cases, superior to the performance of the Nadeau-Bengio estimator, always having variance that is smaller than the NB variance by one order of magnitude. [sent-498, score-0.476]
</p><p>60 To address the problem of bias we computed the exact (and theoretical) value of the variance estimator of n2 µJ . [sent-499, score-0.291]
</p><p>61 Moment approximation (MA) and NadeauBengio (NB) estimators of the variance of the cross validation estimator of the generalization error under random selection, and their sample variances. [sent-579, score-0.503]
</p><p>62 We observe that the moment approximation estimator has a very small bias, consistently smaller than the bias of the Nadeau-Bengio estimator. [sent-617, score-0.294]
</p><p>63 Notice that when the sizes of the training and test sets are equal (n1 = n2 = 50) the bias of the Nadeau-Bengio estimator is four times higher, in absolute value, than that of the moment approximation estimator. [sent-618, score-0.332]
</p><p>64 Table 6 presents the moment approximation variance estimators together with their variance and the corresponding NB estimators. [sent-642, score-0.39]
</p><p>65 Moment approximation and Nadeau-Bengio variance estimators for k-fold cross-validation estimators of the generalization error and their variances. [sent-728, score-0.326]
</p><p>66 MA estimator is used to estimate the variance of the cross -validation estimator of the generalization error. [sent-743, score-0.513]
</p><p>67 We see that the reduction in variance between the 4-fold and 10fold CV variance estimator is not appreciably different. [sent-747, score-0.364]
</p><p>68 Notice that the Nadeau-Bengio estimate was computed using L(XS j , Xi ) = |Xi − XS j |, 2 + d]1/2 , ¯ ¯ while the moment approximation estimator uses the loss function L(XS j , Xi ) = [(Xi − XS j ) which is almost the same with the absolute error loss. [sent-756, score-0.34]
</p><p>69 The ﬁrst observation we make is that the effect of d on the moment approximation estimator and its sample variance is almost undetectable, as the values of the estimator and its sample variance 1 (averaged over 100 different data sets) do not change with d being 1 or n2 . [sent-761, score-0.657]
</p><p>70 Secondly, we see that the n variance of the Nadeau-Bengio estimator is larger than the variance of the moment approximation estimator by one order of magnitude. [sent-762, score-0.637]
</p><p>71 1149  M ARKATOU , T IAN , B ISWAS AND H RIPCSAK  n2 d=1 n 10 15 20 25 30 35 40 45 50 1 d = n2 10 15 20 25 30 35 40 45 50  NB estimator  var(NB)  MA estimator  var(MA)  0. [sent-763, score-0.352]
</p><p>72 Notice that, in contrast with the square error loss case, the Nadeau-Bengio estimator has a higher variance than the moment approximation estimator. [sent-840, score-0.429]
</p><p>73 Its variance is still an order of magnitude higher than the variance of the moment approximation estimator. [sent-841, score-0.285]
</p><p>74 Notice that for J = 15 the NB estimate has larger, by two orders of magnitude, variance than the moment approximation estimator, while J = 50 it still maintains a larger than the moment approximation estimator variance, only this time by one order of magnitude. [sent-844, score-0.464]
</p><p>75 Notice that the moment approximation estimator has variance that is at least one order of magnitude smaller than the variance of Nadeau-Bengio estimator. [sent-929, score-0.461]
</p><p>76 Table 13 computes the NB and moment approximation variance estimators of the generalization error when the size of the data universe is 500. [sent-930, score-0.406]
</p><p>77 We see that the moment approximation estimator still maintains a variance of an order of magnitude lower than the NB estimator. [sent-931, score-0.367]
</p><p>78 We also computed the variance estimators for k-fold cross validation estimators of the generalization error in the regression case. [sent-932, score-0.405]
</p><p>79 Table 14 shows the value of the moment approximation and Nadeau-Bengio estimator and their sample variances computed over 100 different data sets of size 100. [sent-933, score-0.298]
</p><p>80 Moment approximation (MA) and Nadeau-Bengio (NB) estimators of the variance of the cross validation estimator of the generalization error and their sample variances in the regression case. [sent-1065, score-0.528]
</p><p>81 Moment approximation (MA) and Nadeau-Bengio (NB) estimators of the variance of the cross validation estimator of the generalization error and their sample variances in the regression case. [sent-1109, score-0.528]
</p><p>82 Moment approximation (MA) and Nadeau-Bengio (NB) estimators of the variance of the cross validation estimator of the generalization error and their sample variances in the regression case. [sent-1141, score-0.528]
</p><p>83 Variance estimators of k-fold cross-validation estimator of the generalization error and their sample variances, in regression. [sent-1168, score-0.31]
</p><p>84 Table 15 shows the moment approximation variance estimator and NB estimator for various values of the data universe. [sent-1180, score-0.543]
</p><p>85 Moment approximation (MA) and Nadeau-Bengio (NB) estimators of the variance of the cross validation estimator of the generalization error and their sample variances in the simple classiﬁcation case. [sent-1195, score-0.518]
</p><p>86 Discussion and Conclusion We presented a method for deriving variance estimators of the cross validation estimator of the generalization error in the cases of smooth loss functions and the absolute error loss. [sent-1202, score-0.545]
</p><p>87 We also provide a unifying framework, under which we can obtain variance estimators of the estimators of the generalization error for both, complete random sampling and non-random test set selection. [sent-1204, score-0.325]
</p><p>88 We compared the moment approximation estimators with an estimator proposed by Nadeau and Bengio (2003). [sent-1205, score-0.366]
</p><p>89 On the other hand, the Nadeau and Bengio estimator is computationally simpler than the moment approximation estimator for general loss functions, as it does not require the computation of the derivatives of the loss function. [sent-1208, score-0.523]
</p><p>90 The moment approximation estimator in this case is a reasonable estimator and can be computed. [sent-1210, score-0.449]
</p><p>91 3 and the fact that E[xiT (βS j − β0 )] = 0 the above relationship becomes: ˆ ˆ |E[L(xi β0 , yi )L (xiT β0 , yi )xiT (βS j − β0 )] − E[L(xi β0 , yi )]E[L (xiT β0 , yi )]E[xiT (βS j − β0 )]| ≤  ˆ ˆ E([L(xiT β0 , yi )]2 ) E[L2 (xiT β0 , yi )(βS j − β0 )xi xiT (βS j − β0 )]  Apply once more Lemma A2. [sent-1305, score-0.78]
</p><p>92 j sup||β ˆ  √ S j −β0 ||≤k/ n1  ˆ ˆ |E[L(xiT β0 , yi )L (xiT β0 , yi )(βS j − β0 )xi xiT (βS j − β0 )]  ˆ ˆ −E[L(xiT β0 , yi )]E[L (xiT β0 , yi )]E[(βS j − β0 )xi xiT (βS j − β0 )]| = o(1) Proof. [sent-1310, score-0.52]
</p><p>93 The second term is ˆ ˆ |E[L(xiT β0 , yi )]E[L (xiT β0 , yi )]E[(βS j − β0 )xi xiT (βS j − β0 )]|  ˆ ≤ E[|L(xiT β0 , yi )L (xiT β0 , yi )|]E[xiT (βS j − β0 )]2 c1 ≤ |E[|L(xiT β0 , yi )|]E[|L (xiT β0 , yi )|] n1 c∗ ≤ n1 1161  M ARKATOU , T IAN , B ISWAS AND H RIPCSAK  where c∗ is a constant. [sent-1312, score-0.78]
</p><p>94 5: To obtain the approximation given above we need ﬁrst an approximation for the product L(yi,S j , yi )L(yi ,S j , yi ). [sent-1322, score-0.29]
</p><p>95 Then the ﬁrst term of j j the above expansion is E[L(xi β0 , yi )]E[L(xiT β0 , yi )] = (E[L(xi β0 , yi )])2 . [sent-1328, score-0.399]
</p><p>96 Similarly, the expectation of the third term is σ2 E[L(xiT β0 , yi )]E[L (xiT β0 , yi )]tr[(xi xiT )(XT j XS j )−1 ], S 2  (5)  in both cases, when (yi , xi ) ∈ S j and when (yi , xi ) ∈ S j . [sent-1338, score-0.416]
</p><p>97 Both cases can be treated using the following expression for the expectation of the seventh term: σ2 ˆ ˆ E[L(xiT β0 , yi )](E[L (xiT β0 , yi )])E[(βS j − β0 )xi xiT (βS j − β0 )] 2 σ2 = E[L(xiT β0 , yi )](E[L (xiT β0 , yi )])tr[(xi xiT )(XT j XS j )−1 ]. [sent-1351, score-0.54]
</p><p>98 4  Now, ˆ xiT (βS j − β0 ) T (β − β ) xi ˆ S j 0  ∼ MV N(  0 , Σ) 0  (12)  where Σ = σ2  xiT (XT j XS j )−1 xi xiT (XT j XS j )−1 xi S S xiT (XT j XS j )−1 xi xiT (XT j XS j )−1 xi S S  . [sent-1363, score-0.34]
</p><p>99 S S 4 1166  VARIANCE OF C ROSS -VALIDATION E STIMATORS OF THE G ENERALIZATION E RROR  Therefore, Cov(L(yi,S j , yi ), L(yi ,S j , yi )) = σ2 (E[L (xiT β0 , yi )])2tr[(xi xiT )(XT j XS j )−1 ] ˆ ˆ S +  σ4 (E[L (xiT β0 , yi )])2tr[(xi xiT )(XT j XS j )−1 (xi xiT )(XT j XS j )−1 ]. [sent-1369, score-0.52]
</p><p>100 No unbiased estimator of the variance of k-fold cross validation. [sent-1373, score-0.333]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xit', 0.805), ('xs', 0.323), ('estimator', 0.176), ('var', 0.17), ('xt', 0.134), ('yi', 0.13), ('nb', 0.124), ('sc', 0.116), ('nadeau', 0.107), ('variance', 0.094), ('estimators', 0.093), ('universe', 0.091), ('arkatou', 0.083), ('iswas', 0.083), ('ripcsak', 0.083), ('moment', 0.082), ('stimators', 0.079), ('xi', 0.068), ('cov', 0.068), ('rror', 0.066), ('bengio', 0.065), ('ian', 0.062), ('eneralization', 0.054), ('ross', 0.054), ('cross', 0.05), ('card', 0.041), ('loss', 0.037), ('validation', 0.034), ('loocv', 0.032), ('ma', 0.03), ('tr', 0.025), ('proposition', 0.024), ('bias', 0.021), ('expectation', 0.02), ('iv', 0.02), ('scj', 0.02), ('normality', 0.02), ('cv', 0.019), ('population', 0.018), ('columbia', 0.018), ('zk', 0.017), ('generalization', 0.017), ('absolute', 0.016), ('lr', 0.016), ('nk', 0.015), ('approximation', 0.015), ('eu', 0.015), ('notice', 0.015), ('variances', 0.015), ('rn', 0.014), ('test', 0.014), ('error', 0.014), ('moments', 0.014), ('lemma', 0.014), ('xk', 0.013), ('hypergeometric', 0.013), ('table', 0.013), ('formulas', 0.013), ('unbiased', 0.013), ('differentiable', 0.012), ('lemmas', 0.012), ('presents', 0.012), ('uvt', 0.012), ('covariance', 0.012), ('bv', 0.012), ('averages', 0.012), ('fourth', 0.012), ('gk', 0.012), ('sj', 0.012), ('formula', 0.011), ('square', 0.011), ('mclachlan', 0.011), ('mse', 0.011), ('regression', 0.01), ('variability', 0.01), ('sample', 0.01), ('discriminant', 0.01), ('uut', 0.01), ('cu', 0.01), ('ai', 0.01), ('reports', 0.01), ('complement', 0.01), ('indicates', 0.009), ('group', 0.009), ('expansion', 0.009), ('sen', 0.009), ('prime', 0.009), ('remainder', 0.008), ('asymptotic', 0.008), ('training', 0.008), ('biswas', 0.008), ('diminishes', 0.008), ('grandvalet', 0.008), ('marianthi', 0.008), ('markatou', 0.008), ('mv', 0.008), ('noether', 0.008), ('nonoverlapping', 0.008), ('shameek', 0.008), ('tian', 0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="13-tfidf-1" href="./jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error.html">13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</a></p>
<p>Author: Marianthi Markatou, Hong Tian, Shameek Biswas, George Hripcsak</p><p>Abstract: This paper brings together methods from two different disciplines: statistics and machine learning. We address the problem of estimating the variance of cross-validation (CV) estimators of the generalization error. In particular, we approach the problem of variance estimation of the CV estimators of generalization error as a problem in approximating the moments of a statistic. The approximation illustrates the role of training and test sets in the performance of the algorithm. It provides a unifying approach to evaluation of various methods used in obtaining training and test sets and it takes into account the variability due to different training and test sets. For the simple problem of predicting the sample mean and in the case of smooth loss functions, we show that the variance of the CV estimator of the generalization error is a function of the moments of the random T T variables Y = Card(S j S j ) and Y ∗ = Card(Sc Sc ), where S j , S j are two training sets, and Sc , j j j c are the corresponding test sets. We prove that the distribution of Y and Y* is hypergeometric Sj and we compare our estimator with the one proposed by Nadeau and Bengio (2003). We extend these results in the regression case and the case of absolute error loss, and indicate how the methods can be extended to the classiﬁcation case. We illustrate the results through simulation. Keywords: cross-validation, generalization error, moment approximation, prediction, variance estimation</p><p>2 0.10337016 <a title="13-tfidf-2" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<p>Author: Alexander T. Ihler, John W. Fisher III, Alan S. Willsky</p><p>Abstract: Belief propagation (BP) is an increasingly popular method of performing approximate inference on arbitrary graphical models. At times, even further approximations are required, whether due to quantization of the messages or model parameters, from other simpliﬁed message or model representations, or from stochastic approximation methods. The introduction of such errors into the BP message computations has the potential to affect the solution obtained adversely. We analyze the effect resulting from message approximation under two particular measures of error, and show bounds on the accumulation of errors in the system. This analysis leads to convergence conditions for traditional BP message passing, and both strict bounds and estimates of the resulting error in systems of approximate BP message passing. Keywords: belief propagation, sum-product, convergence, approximate inference, quantization</p><p>3 0.063756689 <a title="13-tfidf-3" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>Author: S. Sathiya Keerthi, Dennis DeCoste</p><p>Abstract: This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classiﬁcation. This is done by modifying the ﬁnite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight , SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modiﬁed Huber’s loss function and the L1 loss function, and also for solving ordinal regression. Keywords: linear SVMs, classiﬁcation, conjugate gradient</p><p>4 0.047853094 <a title="13-tfidf-4" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>Author: Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, Yasemin Altun</p><p>Abstract: Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing ﬂexible and powerful input representations, this paper addresses the complementary issue of designing classiﬁcation algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classiﬁcation problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.</p><p>5 0.038983405 <a title="13-tfidf-5" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>Author: Ofer Dekel, Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We describe new loss functions for regression problems along with an accompanying algorithmic framework which utilizes these functions. These loss functions are derived by symmetrization of margin-based losses commonly used in boosting algorithms, namely, the logistic loss and the exponential loss. The resulting symmetric logistic loss can be viewed as a smooth approximation to the ε-insensitive hinge loss used in support vector regression. We describe and analyze two parametric families of batch learning algorithms for minimizing these symmetric losses. The ﬁrst family employs an iterative log-additive update which can be viewed as a regression counterpart to recent boosting algorithms. The second family utilizes an iterative additive update step. We also describe and analyze online gradient descent (GD) and exponentiated gradient (EG) algorithms for the symmetric logistic loss. A byproduct of our work is a new simple form of regularization for boosting-based classiﬁcation and regression algorithms. Our regression framework also has implications on classiﬁcation algorithms, namely, a new additive update boosting algorithm for classiﬁcation. We demonstrate the merits of our algorithms in a series of experiments.</p><p>6 0.03659714 <a title="13-tfidf-6" href="./jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</a></p>
<p>7 0.034572948 <a title="13-tfidf-7" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>8 0.030386498 <a title="13-tfidf-8" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>9 0.030280657 <a title="13-tfidf-9" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>10 0.030215448 <a title="13-tfidf-10" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>11 0.029580485 <a title="13-tfidf-11" href="./jmlr-2005-Matrix_Exponentiated_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">55 jmlr-2005-Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>12 0.029425606 <a title="13-tfidf-12" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>13 0.029330347 <a title="13-tfidf-13" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>14 0.028895512 <a title="13-tfidf-14" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>15 0.023055328 <a title="13-tfidf-15" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>16 0.02210284 <a title="13-tfidf-16" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>17 0.021123132 <a title="13-tfidf-17" href="./jmlr-2005-Change_Point_Problems_in_Linear_Dynamical_Systems.html">17 jmlr-2005-Change Point Problems in Linear Dynamical Systems</a></p>
<p>18 0.020162676 <a title="13-tfidf-18" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>19 0.018869229 <a title="13-tfidf-19" href="./jmlr-2005-Generalization_Bounds_and_Complexities_Based_on_Sparsity_and_Clustering_for_Convex_Combinations_of_Functions_from_Random_Classes.html">37 jmlr-2005-Generalization Bounds and Complexities Based on Sparsity and Clustering for Convex Combinations of Functions from Random Classes</a></p>
<p>20 0.01807189 <a title="13-tfidf-20" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.122), (1, 0.028), (2, 0.076), (3, -0.036), (4, -0.204), (5, 0.042), (6, 0.115), (7, 0.065), (8, -0.075), (9, 0.016), (10, -0.015), (11, 0.132), (12, 0.086), (13, -0.083), (14, -0.056), (15, 0.204), (16, 0.501), (17, 0.211), (18, 0.126), (19, 0.055), (20, -0.084), (21, -0.263), (22, 0.086), (23, -0.078), (24, 0.112), (25, -0.011), (26, -0.059), (27, 0.036), (28, -0.017), (29, 0.115), (30, -0.07), (31, -0.074), (32, -0.003), (33, -0.079), (34, 0.063), (35, 0.061), (36, -0.049), (37, -0.148), (38, 0.03), (39, -0.021), (40, 0.138), (41, 0.049), (42, -0.109), (43, 0.036), (44, -0.119), (45, 0.085), (46, 0.051), (47, 0.062), (48, 0.085), (49, -0.223)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9728002 <a title="13-lsi-1" href="./jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error.html">13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</a></p>
<p>Author: Marianthi Markatou, Hong Tian, Shameek Biswas, George Hripcsak</p><p>Abstract: This paper brings together methods from two different disciplines: statistics and machine learning. We address the problem of estimating the variance of cross-validation (CV) estimators of the generalization error. In particular, we approach the problem of variance estimation of the CV estimators of generalization error as a problem in approximating the moments of a statistic. The approximation illustrates the role of training and test sets in the performance of the algorithm. It provides a unifying approach to evaluation of various methods used in obtaining training and test sets and it takes into account the variability due to different training and test sets. For the simple problem of predicting the sample mean and in the case of smooth loss functions, we show that the variance of the CV estimator of the generalization error is a function of the moments of the random T T variables Y = Card(S j S j ) and Y ∗ = Card(Sc Sc ), where S j , S j are two training sets, and Sc , j j j c are the corresponding test sets. We prove that the distribution of Y and Y* is hypergeometric Sj and we compare our estimator with the one proposed by Nadeau and Bengio (2003). We extend these results in the regression case and the case of absolute error loss, and indicate how the methods can be extended to the classiﬁcation case. We illustrate the results through simulation. Keywords: cross-validation, generalization error, moment approximation, prediction, variance estimation</p><p>2 0.47465768 <a title="13-lsi-2" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<p>Author: Alexander T. Ihler, John W. Fisher III, Alan S. Willsky</p><p>Abstract: Belief propagation (BP) is an increasingly popular method of performing approximate inference on arbitrary graphical models. At times, even further approximations are required, whether due to quantization of the messages or model parameters, from other simpliﬁed message or model representations, or from stochastic approximation methods. The introduction of such errors into the BP message computations has the potential to affect the solution obtained adversely. We analyze the effect resulting from message approximation under two particular measures of error, and show bounds on the accumulation of errors in the system. This analysis leads to convergence conditions for traditional BP message passing, and both strict bounds and estimates of the resulting error in systems of approximate BP message passing. Keywords: belief propagation, sum-product, convergence, approximate inference, quantization</p><p>3 0.31324607 <a title="13-lsi-3" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>Author: S. Sathiya Keerthi, Dennis DeCoste</p><p>Abstract: This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classiﬁcation. This is done by modifying the ﬁnite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight , SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modiﬁed Huber’s loss function and the L1 loss function, and also for solving ordinal regression. Keywords: linear SVMs, classiﬁcation, conjugate gradient</p><p>4 0.22186455 <a title="13-lsi-4" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>Author: Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, Yasemin Altun</p><p>Abstract: Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing ﬂexible and powerful input representations, this paper addresses the complementary issue of designing classiﬁcation algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classiﬁcation problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.</p><p>5 0.15461265 <a title="13-lsi-5" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>Author: Andreas Maurer</p><p>Abstract: A mechnism of transfer learning is analysed, where samples drawn from different learning tasks of an environment are used to improve the learners performance on a new task. We give a general method to prove generalisation error bounds for such meta-algorithms. The method can be applied to the bias learning model of J. Baxter and to derive novel generalisation bounds for metaalgorithms searching spaces of uniformly stable algorithms. We also present an application to regularized least squares regression. Keywords: algorithmic stability, meta-learning, learning to learn</p><p>6 0.1526159 <a title="13-lsi-6" href="./jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</a></p>
<p>7 0.14364421 <a title="13-lsi-7" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>8 0.12074675 <a title="13-lsi-8" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>9 0.11904854 <a title="13-lsi-9" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>10 0.11810254 <a title="13-lsi-10" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>11 0.11620327 <a title="13-lsi-11" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>12 0.11187121 <a title="13-lsi-12" href="./jmlr-2005-Multiclass_Boosting_for_Weak_Classifiers.html">57 jmlr-2005-Multiclass Boosting for Weak Classifiers</a></p>
<p>13 0.10483543 <a title="13-lsi-13" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>14 0.099779084 <a title="13-lsi-14" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>15 0.098591737 <a title="13-lsi-15" href="./jmlr-2005-Matrix_Exponentiated_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">55 jmlr-2005-Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>16 0.095850728 <a title="13-lsi-16" href="./jmlr-2005-Concentration_Bounds_for_Unigram_Language_Models.html">22 jmlr-2005-Concentration Bounds for Unigram Language Models</a></p>
<p>17 0.094906487 <a title="13-lsi-17" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>18 0.087364502 <a title="13-lsi-18" href="./jmlr-2005-Generalization_Bounds_and_Complexities_Based_on_Sparsity_and_Clustering_for_Convex_Combinations_of_Functions_from_Random_Classes.html">37 jmlr-2005-Generalization Bounds and Complexities Based on Sparsity and Clustering for Convex Combinations of Functions from Random Classes</a></p>
<p>19 0.083192982 <a title="13-lsi-19" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>20 0.081400894 <a title="13-lsi-20" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.021), (17, 0.022), (19, 0.06), (24, 0.442), (36, 0.019), (37, 0.024), (43, 0.024), (47, 0.013), (52, 0.047), (70, 0.042), (88, 0.101), (90, 0.03), (94, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.66740936 <a title="13-lda-1" href="./jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error.html">13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</a></p>
<p>Author: Marianthi Markatou, Hong Tian, Shameek Biswas, George Hripcsak</p><p>Abstract: This paper brings together methods from two different disciplines: statistics and machine learning. We address the problem of estimating the variance of cross-validation (CV) estimators of the generalization error. In particular, we approach the problem of variance estimation of the CV estimators of generalization error as a problem in approximating the moments of a statistic. The approximation illustrates the role of training and test sets in the performance of the algorithm. It provides a unifying approach to evaluation of various methods used in obtaining training and test sets and it takes into account the variability due to different training and test sets. For the simple problem of predicting the sample mean and in the case of smooth loss functions, we show that the variance of the CV estimator of the generalization error is a function of the moments of the random T T variables Y = Card(S j S j ) and Y ∗ = Card(Sc Sc ), where S j , S j are two training sets, and Sc , j j j c are the corresponding test sets. We prove that the distribution of Y and Y* is hypergeometric Sj and we compare our estimator with the one proposed by Nadeau and Bengio (2003). We extend these results in the regression case and the case of absolute error loss, and indicate how the methods can be extended to the classiﬁcation case. We illustrate the results through simulation. Keywords: cross-validation, generalization error, moment approximation, prediction, variance estimation</p><p>2 0.276254 <a title="13-lda-2" href="./jmlr-2005-Clustering_with_Bregman_Divergences.html">20 jmlr-2005-Clustering with Bregman Divergences</a></p>
<p>Author: Arindam Banerjee, Srujana Merugu, Inderjit S. Dhillon, Joydeep Ghosh</p><p>Abstract: A wide variety of distortion functions, such as squared Euclidean distance, Mahalanobis distance, Itakura-Saito distance and relative entropy, have been used for clustering. In this paper, we propose and analyze parametric hard and soft clustering algorithms based on a large class of distortion functions known as Bregman divergences. The proposed algorithms unify centroid-based parametric clustering approaches, such as classical kmeans, the Linde-Buzo-Gray (LBG) algorithm and information-theoretic clustering, which arise by special choices of the Bregman divergence. The algorithms maintain the simplicity and scalability of the classical kmeans algorithm, while generalizing the method to a large class of clustering loss functions. This is achieved by ﬁrst posing the hard clustering problem in terms of minimizing the loss in Bregman information, a quantity motivated by rate distortion theory, and then deriving an iterative algorithm that monotonically decreases this loss. In addition, we show that there is a bijection between regular exponential families and a large class of Bregman divergences, that we call regular Bregman divergences. This result enables the development of an alternative interpretation of an efﬁcient EM scheme for learning mixtures of exponential family distributions, and leads to a simple soft clustering algorithm for regular Bregman divergences. Finally, we discuss the connection between rate distortion theory and Bregman clustering and present an information theoretic analysis of Bregman clustering algorithms in terms of a trade-off between compression and loss in Bregman information. Keywords: clustering, Bregman divergences, Bregman information, exponential families, expectation maximization, information theory</p><p>3 0.27328733 <a title="13-lda-3" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>Author: Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, Yasemin Altun</p><p>Abstract: Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing ﬂexible and powerful input representations, this paper addresses the complementary issue of designing classiﬁcation algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classiﬁcation problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.</p><p>4 0.27190885 <a title="13-lda-4" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>Author: Cheng Soon Ong, Alexander J. Smola, Robert C. Williamson</p><p>Abstract: This paper addresses the problem of choosing a kernel suitable for estimation with a support vector machine, hence further automating machine learning. This goal is achieved by deﬁning a reproducing kernel Hilbert space on the space of kernels itself. Such a formulation leads to a statistical estimation problem similar to the problem of minimizing a regularized risk functional. We state the equivalent representer theorem for the choice of kernels and present a semideﬁnite programming formulation of the resulting optimization problem. Several recipes for constructing hyperkernels are provided, as well as the details of common machine learning problems. Experimental results for classiﬁcation, regression and novelty detection on UCI data show the feasibility of our approach. Keywords: learning the kernel, capacity control, kernel methods, support vector machines, representer theorem, semideﬁnite programming</p><p>5 0.27131847 <a title="13-lda-5" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>Author: John Winn, Christopher M. Bishop</p><p>Abstract: Bayesian inference is now widely established as one of the principal foundations for machine learning. In practice, exact inference is rarely possible, and so a variety of approximation techniques have been developed, one of the most widely used being a deterministic framework called variational inference. In this paper we introduce Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to Bayesian Networks. Like belief propagation, VMP proceeds by sending messages between nodes in the network and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing additional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational message passing has been implemented in the form of a general purpose inference engine called VIBES (‘Variational Inference for BayEsian networkS’) which allows models to be speciﬁed graphically and then solved variationally without recourse to coding. Keywords: Bayesian networks, variational inference, message passing</p><p>6 0.26968506 <a title="13-lda-6" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>7 0.2676121 <a title="13-lda-7" href="./jmlr-2005-Matrix_Exponentiated_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">55 jmlr-2005-Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>8 0.26742846 <a title="13-lda-8" href="./jmlr-2005-Learning_the_Kernel_Function_via_Regularization.html">48 jmlr-2005-Learning the Kernel Function via Regularization</a></p>
<p>9 0.26554054 <a title="13-lda-9" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>10 0.26489928 <a title="13-lda-10" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<p>11 0.2641688 <a title="13-lda-11" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>12 0.26285961 <a title="13-lda-12" href="./jmlr-2005-On_the_Nystr%C3%B6m_Method_for_Approximating_a_Gram_Matrix_for_Improved_Kernel-Based_Learning.html">60 jmlr-2005-On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning</a></p>
<p>13 0.2616345 <a title="13-lda-13" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>14 0.26095706 <a title="13-lda-14" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>15 0.26072094 <a title="13-lda-15" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>16 0.26060238 <a title="13-lda-16" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>17 0.25873491 <a title="13-lda-17" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>18 0.25652477 <a title="13-lda-18" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>19 0.25604418 <a title="13-lda-19" href="./jmlr-2005-Generalization_Bounds_for_the_Area_Under_the_ROC_Curve.html">38 jmlr-2005-Generalization Bounds for the Area Under the ROC Curve</a></p>
<p>20 0.25467709 <a title="13-lda-20" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
