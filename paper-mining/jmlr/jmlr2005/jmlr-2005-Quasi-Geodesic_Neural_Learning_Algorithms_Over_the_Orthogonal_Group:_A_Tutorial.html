<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-63" href="#">jmlr2005-63</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</h1>
<br/><p>Source: <a title="jmlr-2005-63-pdf" href="http://jmlr.org/papers/volume6/fiori05a/fiori05a.pdf">pdf</a></p><p>Author: Simone Fiori</p><p>Abstract: The aim of this contribution is to present a tutorial on learning algorithms for a single neural layer whose connection matrix belongs to the orthogonal group. The algorithms exploit geodesics appropriately connected as piece-wise approximate integrals of the exact differential learning equation. The considered learning equations essentially arise from the Riemannian-gradient-based optimization theory with deterministic and diffusion-type gradient. The paper aims speciﬁcally at reviewing the relevant mathematics (and at presenting it in as much transparent way as possible in order to make it accessible to readers that do not possess a background in differential geometry), at bringing together modern optimization methods on manifolds and at comparing the different algorithms on a common machine learning problem. As a numerical case-study, we consider an application to non-negative independent component analysis, although it should be recognized that Riemannian gradient methods give rise to general-purpose algorithms, by no means limited to ICA-related applications. Keywords: differential geometry, diffusion-type gradient, Lie groups, non-negative independent component analysis, Riemannian gradient</p><p>Reference: <a title="jmlr-2005-63-reference" href="../jmlr2005_reference/jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Often, such differential equations are deﬁned over parameter spaces that may be endowed with a speciﬁc geometry, such as the general linear group, the compact Stiefel manifold, the orthogonal group, the Grassman manifold and the manifold of FIR ﬁlters1 (Amari, 1998; Fiori, 2001, 2002; Liu et al. [sent-9, score-1.032]
</p><p>2 F IORI  With the present contribution, we aim at studying and illustrating learning algorithms for a single neural layer whose connection matrix belongs to the orthogonal group, that is the group of square orthogonal matrices. [sent-17, score-0.463]
</p><p>3 • In order to improve the numerical performances of the learning algorithm, we might tentatively try adding some stochasticity to the standard gradient (through annealed-MCMC method) and try a geodesic search. [sent-28, score-0.432]
</p><p>4 The results on this sides are so far disappointing, because numerical simulations shown that standard Riemannian gradient with no geodesic search nor stochasticity added outperforms the other methods on the considered ICA+ problem. [sent-30, score-0.46]
</p><p>5 In particular, the concepts of algebraic groups, differential manifolds and Lie groups are recalled, along with the concepts of right-translation, Riemannian gradient and geodesic 744  Q UASI -G EODESIC N EURAL L EARNING A LGORITHMS OVER THE O RTHOGONAL G ROUP  curves. [sent-35, score-0.549]
</p><p>6 Geodesic-based approximations of gradient-type learning differential equations over the orthogonal group are also explained. [sent-37, score-0.478]
</p><p>7 Learning Over the Orthogonal Group: Gradient-Based Differential Systems and Their Integration The aims of the present section are to recall some basic concepts from differential geometry and to derive the general form of gradient-based learning differential equations over the orthogonal group. [sent-49, score-0.501]
</p><p>8 1 Basic Differential Geometry Preliminaries In order to better explain the subsequent issues, it would be beneﬁcial to recall some basic concepts from differential geometry related to the orthogonal group O(p). [sent-52, score-0.499]
</p><p>9 An algebraic group (G, m, i, e) is a set G that is endowed with an internal operation m : G × G → G, usually referred to as group multiplication, an inverse operation i : G → G, and an identity element e with respect to the group multiplication. [sent-53, score-0.592]
</p><p>10 The ﬁrst group is the Z, set of all integer numbers endowed with the standard addition as group multiplication. [sent-58, score-0.42]
</p><p>11 In the second example, we considered the set of non-singular matrices: def  Gl(p) ={X ∈ IR p×p | det(X) = 0}, 745  (1)  F IORI  endowed with standard matrix multiplication ‘·’ as group multiplication operation. [sent-60, score-0.406]
</p><p>12 The formal deﬁnition of a differential manifold is quite involved, because it requires precise deﬁnitions from mathematical topology theory and advanced calculus (Olver, 2003). [sent-66, score-0.47]
</p><p>13 More practically, a manifold may be essentially regarded as a generalization of curves and surfaces in high-dimensional space, that is endowed with the noticeable property of being locally similar to a ﬂat (Euclidean) space. [sent-67, score-0.401]
</p><p>14 Let us consider a differential manifold M and a point ξ on it. [sent-68, score-0.47]
</p><p>15 About these concepts, two short notes are in order: • Borrowing terms from maritime terminology, a triple (ψ,U, p) is termed coordinate chart associated to the manifold M . [sent-75, score-0.425]
</p><p>16 Such notation evidences that the elements ψ and U ⊂ M are necessary to coordinatize a point on the manifold and that the coordinate space has dimension p. [sent-76, score-0.394]
</p><p>17 This is a smooth manifold of dimension p embedded in the Euclidean space IR p+1 , in fact with only p coordinates we can identify any point on the sphere. [sent-88, score-0.39]
</p><p>18 Olver (2003) shows how to coordinatize such manifold through for example, the stereographic projection, which requires two coordinate maps applied to two convenient neighborhoods on the sphere. [sent-89, score-0.394]
</p><p>19 An interesting object we may think to on a differential manifold M is a smooth curve γ : [a, b] → M . [sent-90, score-0.571]
</p><p>20 In coordinates,2 x = γ(t) describes a curve on the manifold M delimited by the endpoints γ(a) and γ(b). [sent-91, score-0.392]
</p><p>21 Here, the manifold is supposed to be immersed in a suitable ambient Euclidean space A of suitable dimension (for instance, the sphere S p may be though of as immersed in the ambient space A = IR p+1 ). [sent-92, score-0.443]
</p><p>22 The smooth curve admits a tangent vector vx at the point x on the manifold, which is deﬁned by γ(t) − γ(0) def ∈ A. [sent-94, score-0.454]
</p><p>23 vx = lim t→0 t Clearly, the vector vx does not belong to the curved manifold M but is tangent to it in the point x. [sent-95, score-0.555]
</p><p>24 Let us imagine to consider every possible smooth curve on a manifold of dimension p passing through the point x and to compute the tangent vectors to these curves in the point x. [sent-96, score-0.586]
</p><p>25 The collection of these vectors span a linear space of dimension p, which is referred to as tangent space to the manifold M at the point x, and is denoted with Tx M ⊆ A . [sent-97, score-0.485]
</p><p>26 As a further safety note, it might deserve to recall that, in differential geometry, the main way to regard for example, tangent spaces and vector ﬁelds is based on differential operators (Olver, 2003). [sent-98, score-0.45]
</p><p>27 This means, for instance, that a tangent vector v ∈ Tx M of some smooth manifold M is deﬁned in such a way that if F denotes a smooth functional space then for instance v : F → IR, namely v( f ) is a scalar for f ∈ F . [sent-99, score-0.553]
</p><p>28 It is now possible to give the deﬁnition of Riemannian manifold, which is a pair (M , g) formed by a differential manifold M and an inner product gx (vx , ux ), locally deﬁned in every point x of the manifold as a bilinear function from Tx M × Tx M to IR. [sent-105, score-0.894]
</p><p>29 It is important to remark that the inner product gx (·, ·) acts on elements from the tangent space to the manifold at some given point, it therefore depends (smoothly) on the point x. [sent-106, score-0.584]
</p><p>30 On a Riemannian manifold (M , g), we can measure the length of a vector v ∈ Tx M as def  v =  gx (v, v). [sent-107, score-0.582]
</p><p>31 747  F IORI  Also, a remarkable property of Riemannian manifolds is that we can measure the length of a curve γ : [a, b] → M on the manifold through the local metric on its tangent spaces. [sent-110, score-0.642]
</p><p>32 γ  (2)  The net result of this argument is that, through a deﬁnition of an inner product on the tangent spaces to a Riemannian manifold, we are able to measure the length of paths in the manifold itself, and this turns the manifold into a metric space. [sent-113, score-0.856]
</p><p>33 A vector ﬁeld vx on manifold M speciﬁes a vector belonging to the tangent space Tx M to the manifold at every point x. [sent-114, score-0.845]
</p><p>34 A geodesic on a smooth manifold may be intuitively looked upon in at least three different ways: • On a general manifold, the concept of geodesic extends the concept of straight line on a ﬂat space to a curved space. [sent-116, score-0.891]
</p><p>35 An informal interpretation of this property is that a geodesic is a curve on a manifold that would resemble a straight line in an inﬁnitesimal neighborhood of any of its points. [sent-117, score-0.658]
</p><p>36 • On a Riemannian manifold, a geodesic among two points is locally deﬁned as the shortest curve on the manifold connecting these endpoints. [sent-119, score-0.658]
</p><p>37 • Another intuitive interpretation is based on the observation that a geodesic emanating from a point x on the manifold coincides to the path followed by a particle sliding on the manifold itself with constant scalar speed speciﬁed by the norm of the vector vx . [sent-122, score-0.951]
</p><p>38 For a manifold embedded in a Euclidean space, this is equivalent to require that the acceleration of the particle is either zero or perpendicular to the tangent space to the manifold in every point. [sent-123, score-0.841]
</p><p>39 The concept of geodesic and geodesic equation are recalled here only informally. [sent-124, score-0.602]
</p><p>40 748  Q UASI -G EODESIC N EURAL L EARNING A LGORITHMS OVER THE O RTHOGONAL G ROUP  Formally, the concept of gradient on a Riemannian manifold may be deﬁned as follows. [sent-129, score-0.419]
</p><p>41 Let us consider a Riemannian manifold (M , g) and, for every point x, the tangent space Tx M . [sent-130, score-0.485]
</p><p>42 The Riemannian gradient gradM f of the x x ∂x function f over the manifold M in the point x is uniquely deﬁned by the following two conditions: • Tangency condition. [sent-132, score-0.419]
</p><p>43 x x The tangency condition expresses the fact that a gradient vector is always tangent to the basemanifold, while the compatibility condition states that the inner product, under a metric on a manifold, of a gradient vector with any other tangent vector is invariant with the chosen metric. [sent-136, score-0.634]
</p><p>44 In order to facilitate the use of the compatibility condition for gradient computation, it is sometimes useful to introduce the concept of normal space of a Riemannian manifold in a given point under a chosen metric gA : def  Nx M ={n ∈ A |gA (n, v) = 0 , ∀v ∈ Tx M }. [sent-141, score-0.666]
</p><p>45 x It represents the orthogonal complement of the tangent space with respect to an Euclidean ambient space A that the manifold M is embedded within. [sent-142, score-0.707]
</p><p>46 A Lie group conjugates the properties of an algebraic group and of a smooth manifold, as it is a set endowed with both group properties and manifold structure. [sent-144, score-0.951]
</p><p>47 An example of a Lie group that we are interested in within the paper is the orthogonal group: def O(p) ={X ∈ IR p×p |XT X = I p }. [sent-145, score-0.462]
</p><p>48 A particular tangent space is Te G, namely the tangent at identity, which, properly endowed with a binary operator termed Lie bracket, has the structure of a Lie algebra and is denoted with g. [sent-148, score-0.499]
</p><p>49 γ This operation closely resembles a translation of a curve into a convenient neighborhood of the group identity, so that we can deﬁne a special operator referred to as right translation as def  Rx : G → G , Rx (γ) =m(γ, i(x)). [sent-152, score-0.397]
</p><p>50 749  F IORI  ˜ It is clear that every tangent vector vx to the curve γ at x is also translated to a tangent vector v of ˜ the curve γ(t) by a conveniently deﬁned operator: ˜ dRx : Tx G → Te G , v = dRx (v), which is commonly referred to as tangent map associated to the (right) translation Rx . [sent-153, score-0.649]
</p><p>51 2 Gradient Flows on the Orthogonal Group As mentioned, the orthogonal group O(p) is a Lie group, therefore it is endowed with a manifold structure. [sent-159, score-0.705]
</p><p>52 Some useful facts about the geometrical structure of the orthogonal group O(p) are: • The standard group multiplication on O(p) is non-commutative (for p ≥ 3). [sent-161, score-0.476]
</p><p>53 • The group O(p) manifold structure has dimension p(p−1) . [sent-162, score-0.497]
</p><p>54 • The Lie algebra associated to the orthogonal group is the set of skew-symmetric matrices def ˜ ˜ ˜ so(p) ={V ∈ IR p×p |V+ VT = 0 p }. [sent-169, score-0.505]
</p><p>55 2  First, it is necessary to compute the gradient of a function f : O(p) → IR over the group O(p) in view of computing the geodesic that emanates from a point X ∈ O(p) with velocity proportional to O(p) gradX f . [sent-172, score-0.562]
</p><p>56 750  Q UASI -G EODESIC N EURAL L EARNING A LGORITHMS OVER THE O RTHOGONAL G ROUP  O(p)  def  Let the manifold O(p) be equipped with the canonical induced metric gO(p) , that is gX (U, V) = tr[UT V], for every X ∈ O(p) and every U, V ∈ TX O(p). [sent-177, score-0.529]
</p><p>57 Having endowed the manifold O(p) with a metric, it is possible to describe completely its normal space, provided the ambient space A is endowed with the canonical Euclidean metric. [sent-179, score-0.536]
</p><p>58 1, when a manifold is embedded in a Euclidean space, the second derivative of the geodesic with resepct to the parameter is either zero or perpendicular to ˜ the tangent space to the manifold in every point (see Appendix A). [sent-200, score-1.107]
</p><p>59 Therefore, a geodesic γ(t) on p×p the Riemannian manifold (O(p), gO(p) ) embedded in the Euclidean ambient space (IR p×p , gIR ), ¨ ˜ departing from the identity I p , should be such that γ(t) ∈ NI p O(p), therefore it should hold: ¨ ˜ ˜ γ(t) = γ(t)S(t) , with ST (t) = S(t). [sent-201, score-0.681]
</p><p>60 The expression of the geodesic in the position of interest may be made explicit by taking advantage of the Lie-group structure of the orthogonal group endowed with the canonical metric. [sent-206, score-0.646]
</p><p>61 When applied to the manifold M = O(p), the general gradient-based learning equation (3) has the inherent property of keeping the connection matrix X within the group O(p) at any time. [sent-221, score-0.551]
</p><p>62 Moreover, as opposed to the Euclidean space IR p and the general-linear group Gl(p), the orthogonal group O(p) is a compact space. [sent-225, score-0.476]
</p><p>63 This means that the above parameterization spans one of the two components of the orthogonal group termed special orthogonal group and denoted by SO(p). [sent-229, score-0.668]
</p><p>64 This curve may always def ˜ be translated into a neighborhood of the identity of the group by the left-translation γ(t) =X−1 γ(t),  753  F IORI  in fact, the inverse X−1 surely exists because Gl(p) is the set of all invertible p × p matrices by ˜ deﬁnition and now γ(0) = I p . [sent-244, score-0.397]
</p><p>65 Therefore, if V ∈ TX Gl(p) denotes the tangent vector to the curve γ(t) ˜ ∈ gl(p) denotes the tangent vector to the curve γ(t) at t = 0, they are related by the ˜ at t = 0 and V ˜ corresponding tangent map V → V = X−1 V. [sent-245, score-0.614]
</p><p>66 The ‘natural gradient’ theory for Gl(p) and the Riemannian-gradient-theory for the group O(p) are thus somewhat unrelated, even if ultimately the ‘natural gradient’ is a Riemannian gradient on the group Gl(p) arising from a speciﬁc metric. [sent-253, score-0.438]
</p><p>67 4 In the spirit of information geometry, the natural gradient works on a manifold of parameterized likelihood. [sent-256, score-0.419]
</p><p>68 However, if applied to differential equations based on curved manifolds, such ordinary discretization methods produce updating rules that do not satisfy the manifold constraints. [sent-266, score-0.499]
</p><p>69 More formally, this method consists in embedding the manifold M of interest into a Euclidean space of proper dimension A and to discretize the differential equation whose variable is regarded as belonging to A through any suitable ordinary method. [sent-269, score-0.497]
</p><p>70 The principle behind the geodesic method is to replace the line approximation to the original differential equation by the geodesic approximation in the manifold. [sent-273, score-0.704]
</p><p>71 From a geometrical point of view, this seems a natural approximation because a geodesic on a manifold is a counterpart of a line in the Euclidean space. [sent-274, score-0.591]
</p><p>72 Furthermore, a geodesic on a Riemannian manifold is a length-minimizing curve between two points, which looks quite appealing if we regard an optimization process as connecting an initial solution to a stationary point of a criterion function through the shortest path. [sent-275, score-0.685]
</p><p>73 In particular, we suppose to approximate the ﬂow of the differential learning equation (3) through geodesic arcs properly connected, so as to obtain a piece-wise geodesic-type approximation of the exact gradient ﬂow. [sent-278, score-0.532]
</p><p>74 The aim of the present section is to consider three Riemannian gradient algorithms over the Lie group of orthogonal matrices. [sent-283, score-0.398]
</p><p>75 1, the concept of geodesic is essentially local, therefore the discrete steps (11) on the orthogonal group should be extended for small values of ηn . [sent-294, score-0.57]
</p><p>76 It is worth underlining at this point that the numerical evaluation of the geodesic curve through the exponential map, as well as the effective movement along a geodesic, are computationally expensive operations. [sent-296, score-0.444]
</p><p>77 In this case, moving along the geodesic of a small random quantity does not ensure monotonic decreasing of the cost function, but it might help moving to another zone of the parameter space in which the geodesic learning might be effective. [sent-324, score-0.585]
</p><p>78 In the present context, however, the base manifold O(p) is curved, therefore it is sensible to perturb the gradient in the Lie algebra and then apply the formulas explained in the Section 2. [sent-338, score-0.462]
</p><p>79 The learning differential equation on the orthogonal group associated to the gradient (13) reads dW ˜ = −Vdiff (t)W(t), dt  (14)  is a Langevin-type stochastic differential equation (LDE). [sent-349, score-0.798]
</p><p>80 The theoretical foundations of the non-negative independent component analysis (ICA+ ) have been given by Plumbley (2002), and then Plumbley (2003) proposed an optimization algorithm for non-negative ICA based on geodesic learning and applied it to the blind separation of three gray-level images. [sent-436, score-0.455]
</p><p>81 As the orthogonal separation matrix W is of size 2 × 2, it may be easily parameterized, as in equation (10), by cos β − sin β , W(β) = sin β cos β with β ∈ [−π, π[ being the separation angle. [sent-579, score-0.413]
</p><p>82 The Figure 2 shows the two-image mixtures, the behavior of the cost function f and of the separation index Q as well as the separated images obtained with the optimal separation angle, which is deﬁned as the angle corresponding to the minimal criterion function value. [sent-584, score-0.472]
</p><p>83 The results of this analysis are shown in the Figures 4 and 5, which illustrate the behavior of the cost function f and of the separation index Q as well as the separated images obtained with the optimal separation angle, for two different noisy mixtures. [sent-597, score-0.445]
</p><p>84 The ﬁrst line of the above algorithm moves the connection pattern at step n from the matrix Wn over the orthogonal group toward the direction of the Euclidean gradient of the ICA+ cost function ˜ ˜ to the new point Wn+1 . [sent-622, score-0.478]
</p><p>85 However, the matrix Wn+1 does not belong to the orthogonal group so it is necessary to project it back to the group with the help of a suitable projector (according to what granted in Section 2. [sent-623, score-0.476]
</p><p>86 3 N UMERICAL A NALYSIS  AND  C OMPARISON OF THE ICA+ A LGORITHMS  The ﬁrst experiment of this section aims at investigating a 4 × 4 ICA+ case tackled with the help of the deterministic-gradient-based algorithm endowed with geodesic search (Algorithm 2). [sent-633, score-0.401]
</p><p>87 In particular, the Figure 6 shows the behavior of the (normalized) separation index Qn /Q0 and of the cost function fn versus the iteration index n. [sent-639, score-0.452]
</p><p>88 of the cost function is smaller than the value of the cost function achieved in the previous iteration, otherwise the result of the geodesic search is ignored and a small random step-size is selected. [sent-652, score-0.4]
</p><p>89 The second experiment of this section aims at investigating a 9 × 9 ICA+ case tackled with the help of the deterministic-gradient-based algorithm endowed with geodesic search (Algorithm 2). [sent-658, score-0.401]
</p><p>90 The same separation problem was also tackled through the deterministic-gradient-based algorithm without geodesic search (Algorithm 1). [sent-665, score-0.452]
</p><p>91 Two possible explanations of the observed behavior are that: • The projection operation wastes the most part of the time in canceling out the component of the Euclidean gradient that is normal to the manifold instead of advancing the solution toward the most appropriate direction. [sent-692, score-0.449]
</p><p>92 A promising alternative solution would be to exploit the latest advancements in the ﬁeld of numerical calculus on manifold for exponential maps computation, which should allegedly lead to a considerable saving of computational effort without detriment of separation effectiveness. [sent-750, score-0.48]
</p><p>93 1, all the learning equations/algorithms developed in this manuscript are based on a particular choice of the metric that turns the Lie-algebra associated to the Lie group of orthogonal matrices into a metric space. [sent-755, score-0.396]
</p><p>94 Geodesic Equation and Relevant Properties In the present appendix, we consider the problem of constructing a geodesic curve on a Riemannian manifold (M , g) and illustrate some relevant properties of geodesics on Riemannian manifolds embedded in a Euclidean ambient space IR p . [sent-778, score-0.792]
</p><p>95 ˙˙  Let us further deﬁne the Christoffel (or afﬁne connection) coefﬁcients as ∂g j ∂gi j ∂gi + − ∂x j ∂xi ∂x  def 1  Γkj = i  gk 2∑  ,  through which the geodesic equation assumes the classical expression: xk + ∑ ∑ Γkj xi x j = 0 , k = 1 , 2 , · · · , p. [sent-800, score-0.451]
</p><p>96 ˙ A result we make use of in the paper is that, when a Riemannian manifold is embedded into an Euclidean space, the second derivative of the geodesic (x) belongs to the normal space to the ¨ embedded manifold at x. [sent-803, score-0.978]
</p><p>97 We have d gx (x, x) = ˙ ˙ dt  d gab xa xb ˙ ˙ dt ∑ ∑ a b  ∑∑  =  a  gab xa xb + gab xa xb + ¨ ˙ ˙ ¨  b  = 2 ∑ ∑ gab xa xb + ∑ ∑ ¨ ˙ a  a  b  b  dgab xa xb ˙ ˙ dt  dgab xa xb . [sent-805, score-1.101]
</p><p>98 ˙ ˙ dt  By replacing the expression of xa from the geodesic equation (26) into the last expression, we get ¨ d dgab gx (x, x) = −2 ∑ ∑ ∑ ∑ gab Γaj xb xi x j + ∑ ∑ ˙ ˙ xa xb . [sent-806, score-0.716]
</p><p>99 The last step consists in recalling that the manifold has been supposed to be embedded in a def d ˙ ˙ ¨ ˙ Euclidean ambient space and we assume gx (x, x) = xT x. [sent-812, score-0.672]
</p><p>100 Learning algorithm for ICA by geodesic ﬂows on orthogonal group. [sent-916, score-0.398]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('manifold', 0.325), ('geodesic', 0.266), ('gl', 0.264), ('ir', 0.257), ('gradx', 0.205), ('riemannian', 0.202), ('wn', 0.175), ('group', 0.172), ('ica', 0.16), ('tangent', 0.16), ('def', 0.158), ('differential', 0.145), ('eodesic', 0.139), ('eural', 0.139), ('iori', 0.139), ('roup', 0.139), ('rthogonal', 0.139), ('uasi', 0.139), ('orthogonal', 0.132), ('separation', 0.127), ('nnic', 0.125), ('tx', 0.102), ('gx', 0.099), ('gradient', 0.094), ('fiori', 0.088), ('iex', 0.088), ('plumbley', 0.086), ('lie', 0.083), ('mcmc', 0.079), ('endowed', 0.076), ('lde', 0.073), ('lgorithms', 0.072), ('fn', 0.071), ('curve', 0.067), ('img', 0.066), ('db', 0.064), ('termed', 0.06), ('index', 0.059), ('ambient', 0.059), ('gab', 0.059), ('dt', 0.056), ('xa', 0.056), ('iteration', 0.056), ('cost', 0.053), ('worth', 0.052), ('earning', 0.051), ('xt', 0.051), ('images', 0.051), ('geometry', 0.05), ('euclidean', 0.049), ('metric', 0.046), ('gik', 0.044), ('stochasticity', 0.044), ('manifolds', 0.044), ('instrumental', 0.044), ('algebra', 0.043), ('recalled', 0.043), ('compatibility', 0.043), ('coordinate', 0.04), ('rejection', 0.038), ('celledoni', 0.037), ('gradir', 0.037), ('gradm', 0.037), ('tangency', 0.037), ('gi', 0.035), ('tensor', 0.035), ('vx', 0.035), ('xb', 0.034), ('smooth', 0.034), ('mixing', 0.032), ('mix', 0.032), ('blind', 0.032), ('vn', 0.032), ('embedded', 0.031), ('tackled', 0.031), ('srivastava', 0.031), ('underlining', 0.031), ('disturbance', 0.031), ('recti', 0.031), ('component', 0.03), ('velocity', 0.03), ('coordinatize', 0.029), ('dgab', 0.029), ('gradientbased', 0.029), ('orthogonalization', 0.029), ('pertaining', 0.029), ('stepsize', 0.029), ('equations', 0.029), ('search', 0.028), ('numerical', 0.028), ('separated', 0.028), ('annealing', 0.028), ('liu', 0.027), ('equation', 0.027), ('noiseless', 0.027), ('connection', 0.027), ('signals', 0.027), ('criterion', 0.027), ('versus', 0.027), ('temperature', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="63-tfidf-1" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>Author: Simone Fiori</p><p>Abstract: The aim of this contribution is to present a tutorial on learning algorithms for a single neural layer whose connection matrix belongs to the orthogonal group. The algorithms exploit geodesics appropriately connected as piece-wise approximate integrals of the exact differential learning equation. The considered learning equations essentially arise from the Riemannian-gradient-based optimization theory with deterministic and diffusion-type gradient. The paper aims speciﬁcally at reviewing the relevant mathematics (and at presenting it in as much transparent way as possible in order to make it accessible to readers that do not possess a background in differential geometry), at bringing together modern optimization methods on manifolds and at comparing the different algorithms on a common machine learning problem. As a numerical case-study, we consider an application to non-negative independent component analysis, although it should be recognized that Riemannian gradient methods give rise to general-purpose algorithms, by no means limited to ICA-related applications. Keywords: differential geometry, diffusion-type gradient, Lie groups, non-negative independent component analysis, Riemannian gradient</p><p>2 0.20932806 <a title="63-tfidf-2" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>Author: John Lafferty, Guy Lebanon</p><p>Abstract: A family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models. The kernels are based on the heat equation on the Riemannian manifold deﬁned by the Fisher information metric associated with a statistical family, and generalize the Gaussian kernel of Euclidean space. As an important special case, kernels based on the geometry of multinomial families are derived, leading to kernel-based learning algorithms that apply naturally to discrete data. Bounds on covering numbers and Rademacher averages for the kernels are proved using bounds on the eigenvalues of the Laplacian on Riemannian manifolds. Experimental results are presented for document classiﬁcation, for which the use of multinomial geometry is natural and well motivated, and improvements are obtained over the standard use of Gaussian or linear kernels, which have been the standard for text classiﬁcation. Keywords: kernels, heat equation, diffusion, information geometry, text classiﬁcation</p><p>3 0.13844617 <a title="63-tfidf-3" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>Author: Luís B. Almeida</p><p>Abstract: When acquiring an image of a paper document, the image printed on the back page sometimes shows through. The mixture of the front- and back-page images thus obtained is markedly nonlinear, and thus constitutes a good real-life test case for nonlinear blind source separation. This paper addresses a difﬁcult version of this problem, corresponding to the use of “onion skin” paper, which results in a relatively strong nonlinearity of the mixture, which becomes close to singular in the lighter regions of the images. The separation is achieved through the MISEP technique, which is an extension of the well known INFOMAX method. The separation results are assessed with objective quality measures. They show an improvement over the results obtained with linear separation, but have room for further improvement. Keywords: ICA, blind source separation, nonlinear mixtures, nonlinear separation, image mixture, image separation</p><p>4 0.10214231 <a title="63-tfidf-4" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>Author: Motoaki Kawanabe, Klaus-Robert Müller</p><p>Abstract: A blind separation problem where the sources are not independent, but have variance dependencies is discussed. For this scenario Hyv¨ rinen and Hurri (2004) proposed an algorithm which requires a no assumption on distributions of sources and no parametric model of dependencies between components. In this paper, we extend the semiparametric approach of Amari and Cardoso (1997) to variance dependencies and study estimating functions for blind separation of such dependent sources. In particular, we show that many ICA algorithms are applicable to the variance-dependent model as well under mild conditions, although they should in principle not. Our results indicate that separation can be done based only on normalized sources which are adjusted to have stationary variances and is not affected by the dependent activity levels. We also study the asymptotic distribution of the quasi maximum likelihood method and the stability of the natural gradient learning in detail. Simulation results of artiﬁcial and realistic examples match well with our theoretical ﬁndings. Keywords: blind source separation, variance dependencies, independent component analysis, semiparametric statistical models, estimating functions</p><p>5 0.085482143 <a title="63-tfidf-5" href="./jmlr-2005-Learning_the_Kernel_Function_via_Regularization.html">48 jmlr-2005-Learning the Kernel Function via Regularization</a></p>
<p>Author: Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We study the problem of ﬁnding an optimal kernel from a prescribed convex set of kernels K for learning a real-valued function by regularization. We establish for a wide variety of regularization functionals that this leads to a convex optimization problem and, for square loss regularization, we characterize the solution of this problem. We show that, although K may be an uncountable set, the optimal kernel is always obtained as a convex combination of at most m + 2 basic kernels, where m is the number of data examples. In particular, our results apply to learning the optimal radial kernel or the optimal dot product kernel.</p><p>6 0.068964772 <a title="63-tfidf-6" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>7 0.05740419 <a title="63-tfidf-7" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>8 0.057325218 <a title="63-tfidf-8" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>9 0.056267537 <a title="63-tfidf-9" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>10 0.046440225 <a title="63-tfidf-10" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>11 0.043001156 <a title="63-tfidf-11" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>12 0.042729791 <a title="63-tfidf-12" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>13 0.040833309 <a title="63-tfidf-13" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>14 0.040442277 <a title="63-tfidf-14" href="./jmlr-2005-Matrix_Exponentiated_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">55 jmlr-2005-Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>15 0.039167486 <a title="63-tfidf-15" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>16 0.038770009 <a title="63-tfidf-16" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>17 0.035589024 <a title="63-tfidf-17" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>18 0.032982528 <a title="63-tfidf-18" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>19 0.032566436 <a title="63-tfidf-19" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>20 0.032436129 <a title="63-tfidf-20" href="./jmlr-2005-Prioritization_Methods_for_Accelerating_MDP_Solvers.html">61 jmlr-2005-Prioritization Methods for Accelerating MDP Solvers</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.244), (1, 0.307), (2, -0.184), (3, 0.106), (4, -0.024), (5, 0.075), (6, 0.005), (7, 0.232), (8, 0.121), (9, 0.009), (10, 0.029), (11, -0.452), (12, 0.105), (13, -0.124), (14, 0.106), (15, 0.12), (16, -0.008), (17, 0.018), (18, 0.202), (19, -0.03), (20, -0.07), (21, 0.048), (22, 0.023), (23, 0.07), (24, 0.004), (25, -0.015), (26, -0.019), (27, -0.066), (28, 0.018), (29, 0.034), (30, -0.039), (31, 0.024), (32, 0.013), (33, -0.001), (34, 0.016), (35, 0.012), (36, 0.019), (37, 0.092), (38, -0.006), (39, -0.039), (40, -0.017), (41, 0.008), (42, -0.008), (43, -0.054), (44, -0.012), (45, -0.027), (46, 0.008), (47, -0.034), (48, 0.015), (49, -0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96673298 <a title="63-lsi-1" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>Author: Simone Fiori</p><p>Abstract: The aim of this contribution is to present a tutorial on learning algorithms for a single neural layer whose connection matrix belongs to the orthogonal group. The algorithms exploit geodesics appropriately connected as piece-wise approximate integrals of the exact differential learning equation. The considered learning equations essentially arise from the Riemannian-gradient-based optimization theory with deterministic and diffusion-type gradient. The paper aims speciﬁcally at reviewing the relevant mathematics (and at presenting it in as much transparent way as possible in order to make it accessible to readers that do not possess a background in differential geometry), at bringing together modern optimization methods on manifolds and at comparing the different algorithms on a common machine learning problem. As a numerical case-study, we consider an application to non-negative independent component analysis, although it should be recognized that Riemannian gradient methods give rise to general-purpose algorithms, by no means limited to ICA-related applications. Keywords: differential geometry, diffusion-type gradient, Lie groups, non-negative independent component analysis, Riemannian gradient</p><p>2 0.78302675 <a title="63-lsi-2" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>Author: John Lafferty, Guy Lebanon</p><p>Abstract: A family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models. The kernels are based on the heat equation on the Riemannian manifold deﬁned by the Fisher information metric associated with a statistical family, and generalize the Gaussian kernel of Euclidean space. As an important special case, kernels based on the geometry of multinomial families are derived, leading to kernel-based learning algorithms that apply naturally to discrete data. Bounds on covering numbers and Rademacher averages for the kernels are proved using bounds on the eigenvalues of the Laplacian on Riemannian manifolds. Experimental results are presented for document classiﬁcation, for which the use of multinomial geometry is natural and well motivated, and improvements are obtained over the standard use of Gaussian or linear kernels, which have been the standard for text classiﬁcation. Keywords: kernels, heat equation, diffusion, information geometry, text classiﬁcation</p><p>3 0.37231731 <a title="63-lsi-3" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>Author: Luís B. Almeida</p><p>Abstract: When acquiring an image of a paper document, the image printed on the back page sometimes shows through. The mixture of the front- and back-page images thus obtained is markedly nonlinear, and thus constitutes a good real-life test case for nonlinear blind source separation. This paper addresses a difﬁcult version of this problem, corresponding to the use of “onion skin” paper, which results in a relatively strong nonlinearity of the mixture, which becomes close to singular in the lighter regions of the images. The separation is achieved through the MISEP technique, which is an extension of the well known INFOMAX method. The separation results are assessed with objective quality measures. They show an improvement over the results obtained with linear separation, but have room for further improvement. Keywords: ICA, blind source separation, nonlinear mixtures, nonlinear separation, image mixture, image separation</p><p>4 0.33735248 <a title="63-lsi-4" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>Author: Motoaki Kawanabe, Klaus-Robert Müller</p><p>Abstract: A blind separation problem where the sources are not independent, but have variance dependencies is discussed. For this scenario Hyv¨ rinen and Hurri (2004) proposed an algorithm which requires a no assumption on distributions of sources and no parametric model of dependencies between components. In this paper, we extend the semiparametric approach of Amari and Cardoso (1997) to variance dependencies and study estimating functions for blind separation of such dependent sources. In particular, we show that many ICA algorithms are applicable to the variance-dependent model as well under mild conditions, although they should in principle not. Our results indicate that separation can be done based only on normalized sources which are adjusted to have stationary variances and is not affected by the dependent activity levels. We also study the asymptotic distribution of the quasi maximum likelihood method and the stability of the natural gradient learning in detail. Simulation results of artiﬁcial and realistic examples match well with our theoretical ﬁndings. Keywords: blind source separation, variance dependencies, independent component analysis, semiparametric statistical models, estimating functions</p><p>5 0.28466159 <a title="63-lsi-5" href="./jmlr-2005-Learning_the_Kernel_Function_via_Regularization.html">48 jmlr-2005-Learning the Kernel Function via Regularization</a></p>
<p>Author: Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We study the problem of ﬁnding an optimal kernel from a prescribed convex set of kernels K for learning a real-valued function by regularization. We establish for a wide variety of regularization functionals that this leads to a convex optimization problem and, for square loss regularization, we characterize the solution of this problem. We show that, although K may be an uncountable set, the optimal kernel is always obtained as a convex combination of at most m + 2 basic kernels, where m is the number of data examples. In particular, our results apply to learning the optimal radial kernel or the optimal dot product kernel.</p><p>6 0.25378066 <a title="63-lsi-6" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>7 0.19047587 <a title="63-lsi-7" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>8 0.1838719 <a title="63-lsi-8" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>9 0.17216267 <a title="63-lsi-9" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>10 0.16847907 <a title="63-lsi-10" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>11 0.1532948 <a title="63-lsi-11" href="./jmlr-2005-Prioritization_Methods_for_Accelerating_MDP_Solvers.html">61 jmlr-2005-Prioritization Methods for Accelerating MDP Solvers</a></p>
<p>12 0.15222909 <a title="63-lsi-12" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>13 0.14676493 <a title="63-lsi-13" href="./jmlr-2005-Matrix_Exponentiated_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">55 jmlr-2005-Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>14 0.14450654 <a title="63-lsi-14" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>15 0.13684237 <a title="63-lsi-15" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>16 0.1340403 <a title="63-lsi-16" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<p>17 0.13217962 <a title="63-lsi-17" href="./jmlr-2005-Multiclass_Boosting_for_Weak_Classifiers.html">57 jmlr-2005-Multiclass Boosting for Weak Classifiers</a></p>
<p>18 0.13192873 <a title="63-lsi-18" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>19 0.12759463 <a title="63-lsi-19" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>20 0.12708096 <a title="63-lsi-20" href="./jmlr-2005-A_Bayes_Optimal_Approach_for_Partitioning_the_Values_of_Categorical_Attributes.html">1 jmlr-2005-A Bayes Optimal Approach for Partitioning the Values of Categorical Attributes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.011), (11, 0.418), (13, 0.034), (17, 0.026), (19, 0.026), (36, 0.041), (37, 0.04), (42, 0.022), (43, 0.031), (47, 0.021), (52, 0.074), (59, 0.021), (70, 0.042), (80, 0.015), (88, 0.072), (90, 0.021), (94, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75402206 <a title="63-lda-1" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>Author: Simone Fiori</p><p>Abstract: The aim of this contribution is to present a tutorial on learning algorithms for a single neural layer whose connection matrix belongs to the orthogonal group. The algorithms exploit geodesics appropriately connected as piece-wise approximate integrals of the exact differential learning equation. The considered learning equations essentially arise from the Riemannian-gradient-based optimization theory with deterministic and diffusion-type gradient. The paper aims speciﬁcally at reviewing the relevant mathematics (and at presenting it in as much transparent way as possible in order to make it accessible to readers that do not possess a background in differential geometry), at bringing together modern optimization methods on manifolds and at comparing the different algorithms on a common machine learning problem. As a numerical case-study, we consider an application to non-negative independent component analysis, although it should be recognized that Riemannian gradient methods give rise to general-purpose algorithms, by no means limited to ICA-related applications. Keywords: differential geometry, diffusion-type gradient, Lie groups, non-negative independent component analysis, Riemannian gradient</p><p>2 0.2944417 <a title="63-lda-2" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>Author: Antoine Bordes, Seyda Ertekin, Jason Weston, Léon Bottou</p><p>Abstract: Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efﬁcient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We ﬁrst present an online SVM algorithm based on this premise. LASVM yields competitive misclassiﬁcation rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.</p><p>3 0.29432121 <a title="63-lda-3" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>Author: John Lafferty, Guy Lebanon</p><p>Abstract: A family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models. The kernels are based on the heat equation on the Riemannian manifold deﬁned by the Fisher information metric associated with a statistical family, and generalize the Gaussian kernel of Euclidean space. As an important special case, kernels based on the geometry of multinomial families are derived, leading to kernel-based learning algorithms that apply naturally to discrete data. Bounds on covering numbers and Rademacher averages for the kernels are proved using bounds on the eigenvalues of the Laplacian on Riemannian manifolds. Experimental results are presented for document classiﬁcation, for which the use of multinomial geometry is natural and well motivated, and improvements are obtained over the standard use of Gaussian or linear kernels, which have been the standard for text classiﬁcation. Keywords: kernels, heat equation, diffusion, information geometry, text classiﬁcation</p><p>4 0.29333028 <a title="63-lda-4" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>Author: Cheng Soon Ong, Alexander J. Smola, Robert C. Williamson</p><p>Abstract: This paper addresses the problem of choosing a kernel suitable for estimation with a support vector machine, hence further automating machine learning. This goal is achieved by deﬁning a reproducing kernel Hilbert space on the space of kernels itself. Such a formulation leads to a statistical estimation problem similar to the problem of minimizing a regularized risk functional. We state the equivalent representer theorem for the choice of kernels and present a semideﬁnite programming formulation of the resulting optimization problem. Several recipes for constructing hyperkernels are provided, as well as the details of common machine learning problems. Experimental results for classiﬁcation, regression and novelty detection on UCI data show the feasibility of our approach. Keywords: learning the kernel, capacity control, kernel methods, support vector machines, representer theorem, semideﬁnite programming</p><p>5 0.29149875 <a title="63-lda-5" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>Author: Wei Chu, Zoubin Ghahramani</p><p>Abstract: We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative ﬁltering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach. Keywords: Gaussian processes, ordinal regression, approximate Bayesian inference, collaborative ﬁltering, gene expression analysis, feature selection</p><p>6 0.29146129 <a title="63-lda-6" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>7 0.29144201 <a title="63-lda-7" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>8 0.28692538 <a title="63-lda-8" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>9 0.28422356 <a title="63-lda-9" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>10 0.28334159 <a title="63-lda-10" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>11 0.28229842 <a title="63-lda-11" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>12 0.2819905 <a title="63-lda-12" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>13 0.27923617 <a title="63-lda-13" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>14 0.2750634 <a title="63-lda-14" href="./jmlr-2005-Frames%2C_Reproducing_Kernels%2C_Regularization_and_Learning.html">35 jmlr-2005-Frames, Reproducing Kernels, Regularization and Learning</a></p>
<p>15 0.27442551 <a title="63-lda-15" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>16 0.27432889 <a title="63-lda-16" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>17 0.27389434 <a title="63-lda-17" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>18 0.27384409 <a title="63-lda-18" href="./jmlr-2005-Learning_the_Kernel_Function_via_Regularization.html">48 jmlr-2005-Learning the Kernel Function via Regularization</a></p>
<p>19 0.27350745 <a title="63-lda-19" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>20 0.27296925 <a title="63-lda-20" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
