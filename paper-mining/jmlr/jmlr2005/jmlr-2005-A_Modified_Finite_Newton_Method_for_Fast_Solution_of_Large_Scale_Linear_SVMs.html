<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-6" href="#">jmlr2005-6</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</h1>
<br/><p>Source: <a title="jmlr-2005-6-pdf" href="http://jmlr.org/papers/volume6/keerthi05a/keerthi05a.pdf">pdf</a></p><p>Author: S. Sathiya Keerthi, Dennis DeCoste</p><p>Abstract: This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classiﬁcation. This is done by modifying the ﬁnite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight , SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modiﬁed Huber’s loss function and the L1 loss function, and also for solving ordinal regression. Keywords: linear SVMs, classiﬁcation, conjugate gradient</p><p>Reference: <a title="jmlr-2005-6-reference" href="../jmlr2005_reference/jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The paper also suggests ways of extending the method to other loss functions such as the modiﬁed Huber’s loss function and the L1 loss function, and also for solving ordinal regression. [sent-12, score-0.316]
</p><p>2 Traditionally, linear SVMs have been trained using decomposition techniques such as SVMlight (Joachims, 1999), SMO (Platt, 1999), and BSVM (Hsu and Lin, 2002), which solve the dual problem by optimizing a small subset of the variables in each iteration. [sent-22, score-0.114]
</p><p>3 Each iteration costs O(nnz ) time, where nnz is the number of non-zeros in the data matrix (nnz = mn if the data matrix is full where m is the number of examples and n is the number of features). [sent-23, score-0.157]
</p><p>4 With SVMs two particular loss functions for imposing penalties on slacks (violations on the wrong side of the margin, usually denoted by ξ) have been popularly used. [sent-25, score-0.176]
</p><p>5 Since linear SVMs are directly formulated in the input space, it is possible and worthwhile to think of direct methods of solving the primal problem without using the kernel trick. [sent-31, score-0.153]
</p><p>6 Primal approaches are attractive because they assure a continuous decrease in the primal objective function. [sent-32, score-0.153]
</p><p>7 Recently some promising primal algorithms have been given for training linear classiﬁers. [sent-33, score-0.153]
</p><p>8 Fung and Mangasarian (Fung and Mangasarian, 2001) have given a primal version of the least squares formulation of SVMs given by Suykens and Vandewalle (1999). [sent-34, score-0.236]
</p><p>9 A direct primal algorithm for L2 -SVMs that exploits the sparsity property, called the ﬁnite Newton method, was given by Mangasarian (2002). [sent-39, score-0.201]
</p><p>10 We show that the method that we develop for L2 -SVMs can be extended in a straight-forward way to the modiﬁed Huber’s loss function (Zhang, 2004) and, in a slightly more complicated way to the L1 loss function. [sent-45, score-0.16]
</p><p>11 We also show how the algorithm can be modifed to solve ordinal regression. [sent-46, score-0.124]
</p><p>12 Section 6 suggests ways for extending the method to other loss functions and Section 7 explains how ordinal regression can be solved. [sent-52, score-0.156]
</p><p>13 Problem Formulation and Some Basic Results Consider a binary classiﬁcation problem with training examples, {xi ,ti }m where xi ∈ Rn and ti ∈ i=1 {+1, −1}. [sent-55, score-0.242]
</p><p>14 To obtain a linear classiﬁer y = w · x + b, L2 -SVM solves the following primal problem: 1 min ( w (w,b) 2  2  + b2 ) +  C m 2 ξ s. [sent-56, score-0.153]
</p><p>15 ti (w · xi + b) ≥ 1 − ξi ∀ i 2∑ i i  (1)  where C is the regularization parameter. [sent-58, score-0.242]
</p><p>16 We have included the b2 /2 term so that standard regularized least squares algorithms can be used directly. [sent-59, score-0.12]
</p><p>17 This gives1 min f (β) = β  λ β 2  2  +  1 ∑ d 2 (β) 2 i∈I(β) i  (2)  where β = (w, b), λ = 1/C, di (β) = yi (β) − ti , yi (β) = w · xi + b, and I(β) = {i : ti yi (β) < 1}. [sent-64, score-0.849]
</p><p>18 Least Squares SVM (LS-SVM) (Suykens and Vandewalle, 1999) corresponds to (1) with the inequality constraints replaced by the equality constraints, ti (w · xi + b) = 1 − ξi for all i. [sent-65, score-0.242]
</p><p>19 , m}; thus, LS-SVM is solved via a single regularized least squares solution. [sent-69, score-0.12]
</p><p>20 f is continuously differentiable in spite of the jumps in I(β), the reason for this being that when an index i causes a jump in I(β) at some β, its di is 0. [sent-77, score-0.199]
</p><p>21 The Modiﬁed Finite Newton Algorithm Mangasarian’s ﬁnite newton method (Mangasarian, 2002) does iterations of the form βk+1 = βk + δk pk , 1. [sent-88, score-0.284]
</p><p>22 343  K EERTHI AND D E C OSTE  where the search direction pk is based on a second order approximation of the objective function at βk :  pk = −H(βk )−1 ∇ f (βk ). [sent-93, score-0.196]
</p><p>23 ) The step size δk is chosen to satisfy an Armijo condition that ensures convergence, and it is found by applying a halving method of line search in the [0, 1] interval. [sent-96, score-0.143]
</p><p>24 First, we avoid doing anything special for cases where ti yi (β) = 1 occurs. [sent-99, score-0.351]
</p><p>25 More precisely, instead of computing the Newton direction pk , we compute the Newton point, βk + pk , which is the solution of a regularized least squares problem. [sent-102, score-0.264]
</p><p>26 Then an exact line search on the ray from β to β yields the next point of the method. [sent-108, score-0.153]
</p><p>27 Practical Implementation In this section we discuss details associated with the implementation of the various steps of the modiﬁed ﬁnite Newton algorithm and also introduce some useful heuristics for speeding up the algorithm. [sent-146, score-0.123]
</p><p>28 For this point we have yi = 0 ¯ for all i and so I0 = {1, . [sent-155, score-0.109]
</p><p>29 Suppose we also assume that I, a guess of ˜ the optimal set of active indices, is available. [sent-187, score-0.116]
</p><p>30 ) Then choose γ and b0 to minimize the cost λ 2 [γ w ˜ 2  2  + b2 ] + 0  1 [γw · xi + b0 − ti ]2 . [sent-189, score-0.242]
</p><p>31 (9)  where p11 = λ w 2 + ∑i∈I (w · xi )2 , p22 = λ + |I|, p12 = ∑i∈I w · xi , q1 = ∑i∈I ti w · xi , q2 = ∑i∈I ti ˜ ˜ ˜ ˜ 2 . [sent-191, score-0.484]
</p><p>32 2 Step 2: Checking Convergence Checking of the optimality of βk is done by ﬁrst calculating yi (βk ) and di (βk ) for all i, determining the active index set Ik and then checking if ∇ fIk (βk ) = 0. [sent-211, score-0.288]
</p><p>33 To setup the details of the CG method, let: X be the matrix whose rows are (xiT , 1), i ∈ Ik ; and t be a vector whose elements are ti , i ∈ Ik . [sent-217, score-0.242]
</p><p>34 Then (6) is the same as the regularized least squares problem λ 1 min fIk (β) = β 2 + Xβ − t 2 . [sent-218, score-0.12]
</p><p>35 An algorithm with better numerical properties can be easily derived by an algorithmic rearrangement that is special to the regularized least squares solution, which makes use of the intermediate vector X p. [sent-224, score-0.12]
</p><p>36 Almost always, termination of L2 -SVM-MFN occurs when, after the least squares solu¯ tion at step 3, exact line search in step 4 gives βk+1 = β (i. [sent-262, score-0.268]
</p><p>37 In their proximal SVM implementation of LS-SVM, Fung and Mangasarian (2001) solve (12) using Matlab routines that employ factorization techniques on X T X. [sent-272, score-0.232]
</p><p>38 The complexity of the original proximal SVM implementation is O(nnz n+n3 ) whereas the complexity of the CGLS implementation is O(nnz l), where l is the number of iterations needed by the CGLS algorithm. [sent-284, score-0.241]
</p><p>39 For any given i, let us ¯ deﬁne: δi = (ti − yk )/(yi − yk ), where yi = yi (β) and yk = yi (βk ). [sent-298, score-0.756]
</p><p>40 The jump points mentioned above ¯ ¯ i i i are given by ∆ = ∆1 ∪ ∆2 , (14) where ∆1 = {δi : i ∈ Ik ,ti (yi − yk ) > 0} and ∆2 = {δi : i ∈ Ik ,ti (yi − yk ) < 0}. [sent-299, score-0.286]
</p><p>41 ¯ ¯ i i  (15)  For ∆1 we are not using i with ti (yi − yk ) ≤ 0 because they do not cause switching at a positive δ; ¯ i similarly, for ∆2 we are not using i with ti (yi − yk ) ≥ 0. [sent-300, score-0.77]
</p><p>42 It is useful to note here that the proximal SVM implementation solves (12) exactly while LS-SVM-CG uses the practical stopping condition (13) that contributes further to its efﬁciency. [sent-316, score-0.183]
</p><p>43 349  K EERTHI AND D E C OSTE  Let us call the ordinate values at these two meeting points as li and ri respectively. [sent-319, score-0.137]
</p><p>44 It is very easy to keep track of the changes in li and ri as indices get dropped and added to the active set of indices. [sent-320, score-0.202]
</p><p>45 It is easy to get, from the deﬁnition of φ(δ) that ¯ ¯ l0 = λβk · (β − βk ) + ∑ (yk − ti )(yi − yk ) i i  (16)  ¯ ¯ r0 = λβ · (β − βk ) + ∑ (yi − ti )(yi − yk ). [sent-324, score-0.77]
</p><p>46 Thus we keep moving to the right until we get a zero satisfying the condition that the root determined by interpolating (0, li ) and (1, ri ) lies between δi and δi+1 . [sent-333, score-0.137]
</p><p>47 Since the least squares solution (step 3) is much more expensive, the cost of exact line search is negligible. [sent-338, score-0.184]
</p><p>48 5 Complexity Analysis The bulk of the cost of the algorithm is associated with step 3, which only deals with examples that are active at the current point. [sent-340, score-0.142]
</p><p>49 Thus, the empirical complexity of the algorithm is O(nnz lav ) where lav , the average number of CG iterations in step 3, is bounded by the rank of the data matrix and so lav ≤ min{m, n}. [sent-346, score-0.353]
</p><p>50 , m}, step 3 corresponds to 350  F INITE N EWTON METHOD FOR LINEAR SVM S  No heuristics Heuristic 1 Heuristic 2 Both heuristics SV fraction  Adult-9 7. [sent-356, score-0.184]
</p><p>51 solving an unnecessarily large least squares problem; it is wasteful to solve it accurately. [sent-385, score-0.131]
</p><p>52 Whenever β0 is a crude approximation (say, β0 = 0), terminate the least squares solution of (6) after a ﬁxed, small number (say, 10) of CGLS iterations at the ﬁrst call to step 3. [sent-388, score-0.21]
</p><p>53 ¯ Even with the crude β thus generated, the following step 4 usually leads to a point β1 with |I(β1 )| much smaller than m, and a good bulk of the non-support vectors get identiﬁed correctly. [sent-389, score-0.112]
</p><p>54 We end this section on implementation by explaining how a solution of the SVM dual can be obtained after L2 -SVM-MFN solves the primal. [sent-402, score-0.118]
</p><p>55 After ˆ ˆ computing α as mentioned above, set β = ∑i αiti (xiT , 1)T , ξi = λαi ∀i, gi = ti yi (β) + ξi − 1 ∀i, and obtain the maximum dual KKT violation as max{maxi:αi >0 |gi |, maxi:αi =0 max{0, −gi }}. [sent-413, score-0.417]
</p><p>56 If, keeping the maximum dual KKT violation within some speciﬁed tolerance (say, τ = 0. [sent-414, score-0.134]
</p><p>57 First solve L2 -SVM-MFN using ε = 10−3 and then 351  K EERTHI AND D E C OSTE  check the maximum dual KKT violation as described above. [sent-416, score-0.114]
</p><p>58 In order to make a proper comparison L2 -SVM-MFN was forced to satisfy the same dual KKT tolerance of τ = 0. [sent-420, score-0.134]
</p><p>59 , 2004) in Section 4 of their paper via the fact that, for the linear SVM implementation, the cost of updating the gradient of the dual is independent of the number of dual variables that are optimized in each basic iteration. [sent-426, score-0.132]
</p><p>60 34  Table 5: Results for Web-8  353  L2 -SVM-MFN secs CV% 3. [sent-606, score-0.192]
</p><p>61 , the SVM primal problem that uses the L2 loss function: λ min f (β) = β 2 + ∑ L(ξi ) (19) 2 β i where ξi = 1 − ti yi (β) and L = L2 where L2 (ξi ) =  0 if ξi ≤ 0 ξ2 /2 if ξi > 0 i  (20)  The modiﬁed Newton algorithm can be adapted for other loss functions too. [sent-719, score-0.664]
</p><p>62 We brieﬂy explain how to do this for the following loss functions: the modifed Huber’s loss function (Zhang, 2004) and the L1 loss function. [sent-720, score-0.24]
</p><p>63 The solution for this loss function forms the basis of the solution for the L1 loss function. [sent-722, score-0.16]
</p><p>64 The loss function is given by  if ξi ≤ 0  0 ξ2 /2 if 0 < ξi < 2 (21) Lh (ξi ) =  i 2(ξi − 1) if ξi ≥ 2 With L = Lh , the primal objective function f in (19) is strictly convex, continuously differentiable and piecewise quadratic, very much as when (20) is used. [sent-724, score-0.338]
</p><p>65 , min f˜(β) = β  λ β 2  2  +  1 ∑ (yi (β) − ti )2 − 2 ∑ ti yi (β). [sent-730, score-0.593]
</p><p>66 2 i∈I1 i∈I2 355  (22)  K EERTHI AND D E C OSTE  Let q=  2 ∑ ti λ i∈I2  xi 1  ˜ and β = β − q so that (22) can be equivalently rewritten as the solution of λ min f¯(β) = β − q 2 β  2  +  1 ∑ (yi (β) − ti )2 . [sent-731, score-0.484]
</p><p>67 2 i∈I1  (23)  This is nothing but a regularized least squares solution that is shifted in β space; the CG techniques ˜ described in Section 4 can be used to solve for β = β − q and then β can be obtained. [sent-732, score-0.168]
</p><p>68 The exact line search for minimizing f on a ray is only slightly more complicated than the one in Section 4: with (21) we need to watch for jumps of examples from/to three sets of the type I0 , I1 and I2 deﬁned above. [sent-733, score-0.197]
</p><p>69 The L1 loss function can be ˜ Thus, we can solve the primal problem corresponding to the L1 loss function approximated by Lh (ξ). [sent-737, score-0.361]
</p><p>70 Use the β thus obtained to seed the solution of the problem for j = 1 and so on until a solution that approximates the true solution of the L1 loss function satisfactorily is obtained. [sent-744, score-0.119]
</p><p>71 , 2003) gave a primal algorithm for SVMs with L1 loss function in which a modiﬁed logistic regression function is used to approximate the L1 loss function and a sequential approximation scheme similar to what we described above is employed. [sent-749, score-0.359]
</p><p>72 Our method is expected to be more efﬁcient since the approximating loss function (modiﬁed Huber) helps keep the sparsity propery, i. [sent-750, score-0.128]
</p><p>73 Extension to Ordinal Regression In this section we explain how the L2 -SVM-MFN algorithm can be adapted to solve ordinal regression problems. [sent-755, score-0.124]
</p><p>74 In ordinal regression the target variable, ti takes a value from a ﬁnite set, say, {1, 2, . [sent-756, score-0.318]
</p><p>75 Let w denote the weight vector and yi (w) = w · xi denote the ‘score’ of the SVM for the i-th example. [sent-762, score-0.109]
</p><p>76 To set up the SVM formulation we follow the approach given in Chu and Keerthi (2005) and use p − 1 thresholds, bs , s = 1, . [sent-763, score-0.142]
</p><p>77 , p − 1 to divide the scores into p bins so that the interval, (bs−1 , bs ) is assigned for examples which have ti = s. [sent-766, score-0.384]
</p><p>78 356  F INITE N EWTON METHOD FOR LINEAR SVM S  bs , s = 1, . [sent-769, score-0.142]
</p><p>79 , p − 1} deﬁne the following ‘margin-violating’ index sets: Ls (β) = {i : i ∈ Jl for some l ≤ s and yi (w) − bs > −1}  Us (β) = {i : i ∈ Jl for some l > s and yi (w) − bs < 1}. [sent-776, score-0.538]
</p><p>80 Then the primal SVM problem can be written as min f (β) = β  λ β 2  p−1 2  +  1  ∑ (2 ∑  s=1  (yi (w) − bs + 1)2 +  i∈Ls (w)  1 ∑ (yi (w) − bs − 1)2 ). [sent-777, score-0.437]
</p><p>81 , p − 1 and solve the following quadratic approximation of f corresponding to keeping those index sets unchanged: λ min f˜(β) = β 2 β  p−1 2  +  1  1  ∑ ( 2 ∑ (yi (w) − bs + 1)2 + 2 ∑ (yi (w) − bs − 1)2 ). [sent-785, score-0.368]
</p><p>82 Exact line search to minimize f on the ray from βk to β is more complicated than the line search we described in Section 4, but it is quite easy to program in code; also, if p is small, the algorithm is not expensive. [sent-787, score-0.254]
</p><p>83 , l − 1, calculate δsi such ¯ ¯ ¯ ¯ that yi (βk ) + δsi (yi − yi (βk )) = bs + 1. [sent-797, score-0.36]
</p><p>84 , p − 1, calculate δsi such that ¯ yi (βk ) + δsi (yi − yi (βk )) = bs − 1. [sent-801, score-0.36]
</p><p>85 The ideas outlined above for the L2 loss function can be extended to other loss functions such as the modiﬁed Huber’s loss function and the L1 loss function using the ideas of Section 6. [sent-805, score-0.32]
</p><p>86 That would be an interesting primal algorithm that is implemented using dual variables. [sent-815, score-0.219]
</p><p>87 • If no initial guess of β is available, set ini = 0, β = 0, yi = 0 ∀i ∈ Iall and I = Iall . [sent-827, score-0.265]
</p><p>88 • If a guess of w is obtained from another method (say, the Naive Bayes method), set ini = 0, use (9) to form β and then compute yi ∀i ∈ Iall and the active index set I at β. [sent-828, score-0.366]
</p><p>89 • If continuing the solution from one C value to another nearby C value, set ini = 1 and simply start with the β, yi , i ∈ Iall and I available from the previous C solution. [sent-829, score-0.214]
</p><p>90 Deﬁne X to be a restricted data matrix whose rows are (xiT , 1), i ∈ Ik and t to be the corresponding target vector whose elements are ti , i ∈ Ik . [sent-837, score-0.242]
</p><p>91 15 ) Set: ¯ iter = iter + 1, β = β, z = t − Xβ, r = X T z − λβ, φ1 = r 2 , p = r, φ2 = φ1 . [sent-840, score-0.118]
</p><p>92 If (ini = 0 and iter = 1) set cgitermax = 10; else set cgitermax = 5000. [sent-841, score-0.163]
</p><p>93 Check if the following conditions hold: (a) optimality = 1; ¯ (b) ti yi ≤ 1 + tol ∀i ∈ I; and (c) ti yi ≥ 1 − tol ∀i ∈ I. [sent-850, score-0.842]
</p><p>94 Repeat the following steps until exit = 1 occurs: j = j + 1, δ = δi j delslope = ls + δ(rs − ls) If delslope ≥ 0 set δ = −δ ls/(delslope − ls) and exit = 1 Use (18) to update ls and rs using i = i j . [sent-861, score-0.51]
</p><p>95 Set β := β + δ (β − β), y := y + δ (y − y), and compute the new active index set: I = {i ∈ Iall : ¯ ti yi < 1}. [sent-863, score-0.452]
</p><p>96 If Need Second Round=0 stop with β = β, y = y and Ik as the optimal active index set. [sent-868, score-0.14]
</p><p>97 A Description of Data Sets Used As in the main paper, let m, n and nnz denote, respectively, the number of examples, the number of features and the number of non-zero elements in the data matrix. [sent-871, score-0.157]
</p><p>98 Let s = nnz /(mn) denote the sparsity in the data matrix. [sent-872, score-0.205]
</p><p>99 For example, p can be used to store the yi ¯ computed in step 5 and r can be used to store the δi computed in step 6. [sent-893, score-0.193]
</p><p>100 To match the notations given there, take: yi in this algorithm to be yk of the main paper; ls in this pseudocode to stand for l0 , li , li+1 etc; and rs in this pseudocode to stand for r0 , i ri , ri+1 etc. [sent-899, score-0.651]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cgls', 0.244), ('ti', 0.242), ('bsvm', 0.227), ('svmlight', 0.203), ('eerthi', 0.192), ('oste', 0.192), ('secs', 0.192), ('cg', 0.182), ('ewton', 0.174), ('newton', 0.162), ('nnz', 0.157), ('primal', 0.153), ('inite', 0.146), ('yk', 0.143), ('bs', 0.142), ('ik', 0.132), ('ls', 0.132), ('cv', 0.13), ('iall', 0.122), ('mangasarian', 0.114), ('yi', 0.109), ('ini', 0.105), ('fik', 0.102), ('svm', 0.097), ('lav', 0.087), ('proximal', 0.087), ('squares', 0.083), ('yahoo', 0.083), ('loss', 0.08), ('ri', 0.077), ('huber', 0.076), ('ordinal', 0.076), ('pk', 0.072), ('exit', 0.071), ('heuristics', 0.071), ('svms', 0.07), ('cgiter', 0.07), ('frommer', 0.07), ('lh', 0.07), ('maa', 0.07), ('tol', 0.07), ('tolerance', 0.068), ('dual', 0.066), ('pseudocode', 0.065), ('active', 0.065), ('bertsekas', 0.065), ('li', 0.06), ('iter', 0.059), ('piecewise', 0.059), ('massachussetts', 0.058), ('paige', 0.058), ('kkt', 0.054), ('zhang', 0.054), ('implementation', 0.052), ('cgitermax', 0.052), ('decoste', 0.052), ('delslope', 0.052), ('financial', 0.052), ('fprevious', 0.052), ('popularly', 0.052), ('hsu', 0.052), ('ray', 0.052), ('search', 0.052), ('guess', 0.051), ('adult', 0.051), ('iterations', 0.05), ('line', 0.049), ('sparsity', 0.048), ('solve', 0.048), ('fung', 0.047), ('xit', 0.047), ('logistic', 0.046), ('differentiable', 0.046), ('tried', 0.046), ('factorization', 0.045), ('smo', 0.045), ('stopping', 0.044), ('jumps', 0.044), ('slacks', 0.044), ('suykens', 0.044), ('step', 0.042), ('initialization', 0.041), ('platt', 0.04), ('checking', 0.04), ('kao', 0.039), ('seed', 0.039), ('stop', 0.039), ('di', 0.038), ('fi', 0.038), ('regularized', 0.037), ('keerthi', 0.036), ('round', 0.036), ('index', 0.036), ('crude', 0.035), ('bulk', 0.035), ('saunders', 0.035), ('spite', 0.035), ('arrays', 0.035), ('chakrabarti', 0.035), ('dii', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="6-tfidf-1" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>Author: S. Sathiya Keerthi, Dennis DeCoste</p><p>Abstract: This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classiﬁcation. This is done by modifying the ﬁnite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight , SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modiﬁed Huber’s loss function and the L1 loss function, and also for solving ordinal regression. Keywords: linear SVMs, classiﬁcation, conjugate gradient</p><p>2 0.098982513 <a title="6-tfidf-2" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>Author: Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, Yasemin Altun</p><p>Abstract: Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing ﬂexible and powerful input representations, this paper addresses the complementary issue of designing classiﬁcation algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classiﬁcation problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.</p><p>3 0.090479873 <a title="6-tfidf-3" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>Author: Wei Chu, Zoubin Ghahramani</p><p>Abstract: We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative ﬁltering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach. Keywords: Gaussian processes, ordinal regression, approximate Bayesian inference, collaborative ﬁltering, gene expression analysis, feature selection</p><p>4 0.077731565 <a title="6-tfidf-4" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>Author: Antoine Bordes, Seyda Ertekin, Jason Weston, Léon Bottou</p><p>Abstract: Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efﬁcient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We ﬁrst present an online SVM algorithm based on this premise. LASVM yields competitive misclassiﬁcation rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.</p><p>5 0.074948601 <a title="6-tfidf-5" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>Author: Ivor W. Tsang, James T. Kwok, Pak-Ming Cheung</p><p>Abstract: O(m3 ) Standard SVM training has time and O(m2 ) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such “approximateness” in this paper. We ﬁrst show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efﬁcient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and realworld data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about ﬁve million training patterns, in only 1.4 seconds on a 3.2GHz Pentium–4 PC. Keywords: kernel methods, approximation algorithm, minimum enclosing ball, core set, scalability</p><p>6 0.064984567 <a title="6-tfidf-6" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>7 0.063756689 <a title="6-tfidf-7" href="./jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error.html">13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</a></p>
<p>8 0.061630879 <a title="6-tfidf-8" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>9 0.057381496 <a title="6-tfidf-9" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>10 0.057005912 <a title="6-tfidf-10" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>11 0.049184453 <a title="6-tfidf-11" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>12 0.048949067 <a title="6-tfidf-12" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>13 0.048597138 <a title="6-tfidf-13" href="./jmlr-2005-Local_Propagation_in_Conditional_Gaussian_Bayesian_Networks.html">51 jmlr-2005-Local Propagation in Conditional Gaussian Bayesian Networks</a></p>
<p>14 0.04818086 <a title="6-tfidf-14" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>15 0.044432838 <a title="6-tfidf-15" href="./jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines.html">73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</a></p>
<p>16 0.042697843 <a title="6-tfidf-16" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>17 0.041273654 <a title="6-tfidf-17" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>18 0.039700232 <a title="6-tfidf-18" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>19 0.035148505 <a title="6-tfidf-19" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>20 0.030927293 <a title="6-tfidf-20" href="./jmlr-2005-Matrix_Exponentiated_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">55 jmlr-2005-Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.233), (1, 0.068), (2, 0.191), (3, -0.114), (4, 0.03), (5, -0.078), (6, 0.101), (7, -0.003), (8, -0.209), (9, 0.004), (10, 0.124), (11, 0.104), (12, -0.019), (13, 0.089), (14, 0.272), (15, 0.202), (16, 0.08), (17, 0.159), (18, 0.139), (19, 0.047), (20, -0.052), (21, 0.015), (22, -0.016), (23, -0.032), (24, 0.102), (25, -0.0), (26, -0.001), (27, -0.015), (28, -0.01), (29, 0.09), (30, -0.012), (31, -0.001), (32, 0.024), (33, 0.298), (34, -0.125), (35, 0.128), (36, -0.039), (37, -0.085), (38, -0.022), (39, -0.004), (40, -0.06), (41, 0.013), (42, 0.063), (43, -0.186), (44, 0.184), (45, 0.014), (46, 0.046), (47, 0.201), (48, 0.193), (49, -0.164)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94622743 <a title="6-lsi-1" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>Author: S. Sathiya Keerthi, Dennis DeCoste</p><p>Abstract: This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classiﬁcation. This is done by modifying the ﬁnite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight , SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modiﬁed Huber’s loss function and the L1 loss function, and also for solving ordinal regression. Keywords: linear SVMs, classiﬁcation, conjugate gradient</p><p>2 0.39354205 <a title="6-lsi-2" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>Author: Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, Yasemin Altun</p><p>Abstract: Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing ﬂexible and powerful input representations, this paper addresses the complementary issue of designing classiﬁcation algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classiﬁcation problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.</p><p>3 0.37564757 <a title="6-lsi-3" href="./jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error.html">13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</a></p>
<p>Author: Marianthi Markatou, Hong Tian, Shameek Biswas, George Hripcsak</p><p>Abstract: This paper brings together methods from two different disciplines: statistics and machine learning. We address the problem of estimating the variance of cross-validation (CV) estimators of the generalization error. In particular, we approach the problem of variance estimation of the CV estimators of generalization error as a problem in approximating the moments of a statistic. The approximation illustrates the role of training and test sets in the performance of the algorithm. It provides a unifying approach to evaluation of various methods used in obtaining training and test sets and it takes into account the variability due to different training and test sets. For the simple problem of predicting the sample mean and in the case of smooth loss functions, we show that the variance of the CV estimator of the generalization error is a function of the moments of the random T T variables Y = Card(S j S j ) and Y ∗ = Card(Sc Sc ), where S j , S j are two training sets, and Sc , j j j c are the corresponding test sets. We prove that the distribution of Y and Y* is hypergeometric Sj and we compare our estimator with the one proposed by Nadeau and Bengio (2003). We extend these results in the regression case and the case of absolute error loss, and indicate how the methods can be extended to the classiﬁcation case. We illustrate the results through simulation. Keywords: cross-validation, generalization error, moment approximation, prediction, variance estimation</p><p>4 0.37467596 <a title="6-lsi-4" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>Author: Wei Chu, Zoubin Ghahramani</p><p>Abstract: We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative ﬁltering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach. Keywords: Gaussian processes, ordinal regression, approximate Bayesian inference, collaborative ﬁltering, gene expression analysis, feature selection</p><p>5 0.34815571 <a title="6-lsi-5" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>Author: Ivor W. Tsang, James T. Kwok, Pak-Ming Cheung</p><p>Abstract: O(m3 ) Standard SVM training has time and O(m2 ) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such “approximateness” in this paper. We ﬁrst show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efﬁcient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and realworld data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about ﬁve million training patterns, in only 1.4 seconds on a 3.2GHz Pentium–4 PC. Keywords: kernel methods, approximation algorithm, minimum enclosing ball, core set, scalability</p><p>6 0.33462673 <a title="6-lsi-6" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>7 0.25978541 <a title="6-lsi-7" href="./jmlr-2005-Local_Propagation_in_Conditional_Gaussian_Bayesian_Networks.html">51 jmlr-2005-Local Propagation in Conditional Gaussian Bayesian Networks</a></p>
<p>8 0.25036007 <a title="6-lsi-8" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>9 0.22759247 <a title="6-lsi-9" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>10 0.21824329 <a title="6-lsi-10" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>11 0.21723382 <a title="6-lsi-11" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>12 0.2130807 <a title="6-lsi-12" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>13 0.20146507 <a title="6-lsi-13" href="./jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines.html">73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</a></p>
<p>14 0.17658634 <a title="6-lsi-14" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>15 0.16612868 <a title="6-lsi-15" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>16 0.16446605 <a title="6-lsi-16" href="./jmlr-2005-Multiclass_Boosting_for_Weak_Classifiers.html">57 jmlr-2005-Multiclass Boosting for Weak Classifiers</a></p>
<p>17 0.16338761 <a title="6-lsi-17" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>18 0.16280395 <a title="6-lsi-18" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>19 0.16057514 <a title="6-lsi-19" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>20 0.13988921 <a title="6-lsi-20" href="./jmlr-2005-An_MDP-Based_Recommender_System.html">12 jmlr-2005-An MDP-Based Recommender System</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(17, 0.013), (19, 0.034), (36, 0.012), (37, 0.034), (42, 0.016), (43, 0.026), (47, 0.608), (52, 0.07), (59, 0.02), (70, 0.02), (88, 0.051), (90, 0.018), (94, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87559891 <a title="6-lda-1" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>Author: S. Sathiya Keerthi, Dennis DeCoste</p><p>Abstract: This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classiﬁcation. This is done by modifying the ﬁnite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight , SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modiﬁed Huber’s loss function and the L1 loss function, and also for solving ordinal regression. Keywords: linear SVMs, classiﬁcation, conjugate gradient</p><p>2 0.36316025 <a title="6-lda-2" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>Author: Ivor W. Tsang, James T. Kwok, Pak-Ming Cheung</p><p>Abstract: O(m3 ) Standard SVM training has time and O(m2 ) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such “approximateness” in this paper. We ﬁrst show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efﬁcient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and realworld data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about ﬁve million training patterns, in only 1.4 seconds on a 3.2GHz Pentium–4 PC. Keywords: kernel methods, approximation algorithm, minimum enclosing ball, core set, scalability</p><p>3 0.35074583 <a title="6-lda-3" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>Author: Antoine Bordes, Seyda Ertekin, Jason Weston, Léon Bottou</p><p>Abstract: Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efﬁcient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We ﬁrst present an online SVM algorithm based on this premise. LASVM yields competitive misclassiﬁcation rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.</p><p>4 0.27735206 <a title="6-lda-4" href="./jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines.html">73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</a></p>
<p>Author: Rong-En Fan, Pai-Hsuen Chen, Chih-Jen Lin</p><p>Abstract: Working set selection is an important step in decomposition methods for training support vector machines (SVMs). This paper develops a new technique for working set selection in SMO-type decomposition methods. It uses second order information to achieve fast convergence. Theoretical properties such as linear convergence are established. Experiments demonstrate that the proposed method is faster than existing selection methods using ﬁrst order information. Keywords: support vector machines, decomposition methods, sequential minimal optimization, working set selection</p><p>5 0.26778561 <a title="6-lda-5" href="./jmlr-2005-Stability_of_Randomized_Learning_Algorithms.html">67 jmlr-2005-Stability of Randomized Learning Algorithms</a></p>
<p>Author: Andre Elisseeff, Theodoros Evgeniou, Massimiliano Pontil</p><p>Abstract: We extend existing theory on stability, namely how much changes in the training data inﬂuence the estimated models, and generalization performance of deterministic learning algorithms to the case of randomized algorithms. We give formal deﬁnitions of stability for randomized algorithms and prove non-asymptotic bounds on the difference between the empirical and expected error as well as the leave-one-out and expected error of such algorithms that depend on their random stability. The setup we develop for this purpose can be also used for generally studying randomized learning algorithms. We then use these general results to study the effects of bagging on the stability of a learning method and to prove non-asymptotic bounds on the predictive performance of bagging which have not been possible to prove with the existing theory of stability for deterministic learning algorithms.1 Keywords: stability, randomized learning algorithms, sensitivity analysis, bagging, bootstrap methods, generalization error, leave-one-out error.</p><p>6 0.25027105 <a title="6-lda-6" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>7 0.24696612 <a title="6-lda-7" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>8 0.24617182 <a title="6-lda-8" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>9 0.24590087 <a title="6-lda-9" href="./jmlr-2005-Efficient_Computation_of_Gapped_Substring_Kernels_on_Large_Alphabets.html">28 jmlr-2005-Efficient Computation of Gapped Substring Kernels on Large Alphabets</a></p>
<p>10 0.23853147 <a title="6-lda-10" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>11 0.23738931 <a title="6-lda-11" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>12 0.22634594 <a title="6-lda-12" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>13 0.22520417 <a title="6-lda-13" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>14 0.22516879 <a title="6-lda-14" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>15 0.22270259 <a title="6-lda-15" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>16 0.22180113 <a title="6-lda-16" href="./jmlr-2005-Frames%2C_Reproducing_Kernels%2C_Regularization_and_Learning.html">35 jmlr-2005-Frames, Reproducing Kernels, Regularization and Learning</a></p>
<p>17 0.22107087 <a title="6-lda-17" href="./jmlr-2005-Multiclass_Boosting_for_Weak_Classifiers.html">57 jmlr-2005-Multiclass Boosting for Weak Classifiers</a></p>
<p>18 0.22083694 <a title="6-lda-18" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>19 0.22053447 <a title="6-lda-19" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>20 0.21763448 <a title="6-lda-20" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
