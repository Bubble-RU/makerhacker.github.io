<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-73" href="#">jmlr2005-73</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</h1>
<br/><p>Source: <a title="jmlr-2005-73-pdf" href="http://jmlr.org/papers/volume6/fan05a/fan05a.pdf">pdf</a></p><p>Author: Rong-En Fan, Pai-Hsuen Chen, Chih-Jen Lin</p><p>Abstract: Working set selection is an important step in decomposition methods for training support vector machines (SVMs). This paper develops a new technique for working set selection in SMO-type decomposition methods. It uses second order information to achieve fast convergence. Theoretical properties such as linear convergence are established. Experiments demonstrate that the proposed method is faster than existing selection methods using ﬁrst order information. Keywords: support vector machines, decomposition methods, sequential minimal optimization, working set selection</p><p>Reference: <a title="jmlr-2005-73-reference" href="../jmlr2005_reference/jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This paper develops a new technique for working set selection in SMO-type decomposition methods. [sent-11, score-0.218]
</p><p>2 Keywords: support vector machines, decomposition methods, sequential minimal optimization, working set selection  1. [sent-15, score-0.218]
</p><p>3 , l,  (1)  T  y α = 0, where e is the vector of all ones, C is the upper bound of all variables, Q is an l by l symmetric matrix with Qij = yi yj K(xi , xj ), and K(xi , xj ) is the kernel function. [sent-24, score-0.257]
</p><p>4 Better methods of working set selection could reduce the number of iterations and hence are an important research issue. [sent-55, score-0.209]
</p><p>5 , 2003a,b) to ﬁnd working sets based on the reduction of the objective value, but these selection methods are only heuristics without convergence proofs. [sent-64, score-0.207]
</p><p>6 This paper develops a simple working set selection using second order information. [sent-66, score-0.184]
</p><p>7 In Section 2, we discuss existing methods of working set selection and propose a new strategy. [sent-70, score-0.184]
</p><p>8 Existing and New Working Set Selections In this section, we discuss existing methods of working set selection and then propose a new approach. [sent-78, score-0.184]
</p><p>9 1 Existing Selections Currently a popular way to select the working set B is via the “maximal violating pair:” WSS 1 (Working set selection via the “maximal violating pair”) 1. [sent-80, score-0.308]
</p><p>10 Select i ∈ arg max{−yt f (αk )t | t ∈ Iup (αk )}, t  j ∈ arg min{−yt f (αk )t | t ∈ Ilow (αk )}, t  where Iup (α) ≡ {t | αt < C, yt = 1 or αt > 0, yt = −1}, and Ilow (α) ≡ {t | αt < C, yt = −1 or αt > 0, yt = 1}. [sent-81, score-0.308]
</p><p>11 It is known that violating pairs are important in the working set selection: Theorem 2 (Hush and Scovel, 2003) Assume Q is positive semi-deﬁnite. [sent-99, score-0.198]
</p><p>12 Note that except the working set selection, the main task per decomposition iteration is on calculating the two kernel columns Qti and Qtj , t = 1, . [sent-122, score-0.243]
</p><p>13 Therefore, each iteration can become l times more expensive if an O(l2 ) working set selection is used. [sent-127, score-0.219]
</p><p>14 Therefore, an O(l2 ) working set selection is impractical. [sent-129, score-0.184]
</p><p>15 The following theorem shows that one could eﬃciently solve (11), so the working set selection WSS 2 does not cost a lot more than WSS 1. [sent-148, score-0.184]
</p><p>16 Theorem 3 If B = {i, j} is a violating pair and Kii + Kjj − 2Kij > 0, then (11) has the optimal objective value (−yi f (αk )i + yj f (αk )j )2 . [sent-149, score-0.216]
</p><p>17 − 2(Kii + Kjj − 2Kij ) T ˆ ˆ ˆ ˆ Proof Deﬁne di ≡ yi di and dj ≡ yj dj . [sent-150, score-0.81]
</p><p>18 From yB dB = 0, we have di = −dj and  =  1 Qii Qij di di di dj f (αk )j + f (αk )i Qij Qjj dj dj 2 1 ˆ ˆ (Kii + Kjj − 2Kij )d2 + (−yi f (αk )i + yj f (αk )j )dj . [sent-151, score-1.227]
</p><p>19 j 2  (13)  Since Kii + Kjj − 2Kij > 0 and B is a violating pair, we can deﬁne aij ≡ Kii + Kjj − 2Kij > 0 and bij ≡ −yi f (αk )i + yj f (αk )j > 0. [sent-152, score-0.301]
</p><p>20 (14)  Then (13) has the minimum at bij ˆ ˆ < 0, dj = −di = − aij and  (15)  b2 ij the objective function (11a) = − . [sent-153, score-0.323]
</p><p>21 ˆ ˆ di and dj (di and dj ) indeed satisfy (11c)-(11d). [sent-156, score-0.514]
</p><p>22 If ˆ = −1 and hence dj = yj dj > 0, a condition required ˆ ˆ Thus di and di deﬁned in (15) are optimal for (11). [sent-157, score-0.775]
</p><p>23 Note that (8) and (11) are used only for selecting the working set, so they do not have k to maintain the feasibility 0 ≤ αi + di ≤ C, ∀i ∈ B. [sent-162, score-0.307]
</p><p>24 By deﬁning di ≡ yi (αi − αi ) and dj ≡ yj (αj − αj ), (16)’s objective function, in a form similar to (13), is  1 ˆ2 ˆ τ d + bij dj , 2 j  (17)  where bij is deﬁned as in (14). [sent-186, score-0.775]
</p><p>25 If {i, j} ˆ is a violating pair, then a careful check shows that there is dj < 0 which leads to a negative value in (17) and maintains the feasibility of (16). [sent-188, score-0.326]
</p><p>26 For selecting the working set, we consider a similar modiﬁcation: If B = {i, j} and aij is deﬁned as in (14), then (11) is modiﬁed to: Sub(B) ≡ min dB  subject to  1 T 2 d f (αk )BB dB + 2 B constraints of (11). [sent-192, score-0.223]
</p><p>27 2τ Therefore, a generalized working set selection is as the following: WSS 3 (Working set selection using second order information: any symmetric K) 1. [sent-197, score-0.232]
</p><p>28 In summary, an SMO-type decomposition method using WSS 3 for the working set selection is: Algorithm 2 (An SMO-type decomposition method using WSS 3) 1. [sent-202, score-0.252]
</p><p>29 It considers Algorithm 2 but replaces WSS 3 with a general working set selection2 : WSS 4 (A general working set selection discussed in Chen et al. [sent-220, score-0.32]
</p><p>30 Since WSS 3 selects i ∈ arg m(αk ), with aij > 0 j ¯ and ai¯ > 0, (20) in WSS 3 implies ¯j −(−yi f (αk )i + yj f (αk )j )2 −(m(αk ) − M (αk ))2 . [sent-232, score-0.237]
</p><p>31 ≤ aij ¯ ai¯ ¯j Thus, −yi f (αk )i + yj f (αk )j ≥  mint,s at,s ¯ (m(αk ) − M (αk )), maxt,s at,s ¯  ¯ ¯ an inequality satisfying (21) for σ = mint,s at,s / maxt,s at,s . [sent-233, score-0.203]
</p><p>32 (22)  Alternatively, one may check if the selected working set {i, j} satisﬁes −yi f (αk )i + yj f (αk )j ≤ ,  (23)  because (21) implies m(αk ) − M (αk ) ≤ /σ. [sent-245, score-0.298]
</p><p>33 In an SMO-type method for ν-SVM the selected working set B = {i, j} must satisfy yi = yj . [sent-292, score-0.302]
</p><p>34 Otherwise, if yi = yj , then the two linear equalities make the sub-problem have only one feasible point αk . [sent-293, score-0.188]
</p><p>35 For each step we check time and iterations using the two methods of working set selection. [sent-361, score-0.21]
</p><p>36 It is important to check how WSS 3 performs after incorporating shrinking and caching strategies. [sent-380, score-0.185]
</p><p>37 Diﬀerent cache size: First a 40MB cache allows the whole kernel matrix to be stored in the computer memory. [sent-385, score-0.463]
</p><p>38 Second, we allocate only 100K, so cache misses may happen and more kernel evaluations are needed. [sent-386, score-0.243]
</p><p>39 We further separate each ﬁgure to two scenarios: without/with shrinking, and present three ratios between using WSS 3 and using WSS 1: ratio 1 ≡ ratio 2 ≡ ratio 3 ≡  # iter. [sent-401, score-0.208]
</p><p>40 2 (WSS 1, 40M cache)  Note that the number of iterations is independent of the cache size. [sent-410, score-0.245]
</p><p>41 Before describing other results, we explain an interesting observation: In these ﬁgures, if shrinking is not used, in general ratio 1 ≤ ratio 2 ≤ ratio 3. [sent-416, score-0.292]
</p><p>42 (30)  Under the two very diﬀerent cache sizes, one is too small to store the kernel matrix, but the other is large enough. [sent-417, score-0.243]
</p><p>43 6  m g  sp ac e_ ga  cp us m al l  ca da ta  ab al on e  fo ur c  la ss  an ce r  di ab et es  br ea st -c  au st ra lia n  a1 a  tre e  sp lic e  0  im ag e  0. [sent-429, score-2.723]
</p><p>44 6  ga e_  g  ac  m  sp  cp  us  m  ta ca  da  al ab  w  ge  1a  rm  on  an  e  al  l  . [sent-436, score-1.048]
</p><p>45 n  s as cl ur fo  di  br  ab  ea  et  st  -c  es  an  n lia ra st au  a a1  e tre  sp  lic  e  e ag im  0  um  ce  r  0. [sent-437, score-1.853]
</p><p>46 6  ga  l  e_  al  m  sp  g  ac  m us cp  ca  da  ta  e ab  1a w  al  on  an rm ge  fo  di  . [sent-444, score-1.31]
</p><p>47 n  s as cl ur  ab  br ea  et  st  -c  es  an  n lia st ra au  a a1  tre  e  e lic sp  ag e im  0  um  ce  r  0. [sent-445, score-1.591]
</p><p>48 6  m  g  ga ac e_ sp  al us m cp  da ca  on al ab  1a w  ta  e  l  . [sent-451, score-0.923]
</p><p>49 n ge  rm  an  ss ur fo  di  ab  et  cl a  es  st -c an  n br  ea  ia ra l st au  a a1  e tre  sp  lic  e  e ag im  0  um  ce  r  0. [sent-452, score-1.978]
</p><p>50 6  m g  sp ac e_ ga  cp us m al l  ca da ta  ab al on e  fo ur c  la ss  an ce r  di ab et es  br ea st -c  au st ra lia n  a1 a  tre e  sp lic e  0  im ag e  0. [sent-459, score-2.723]
</p><p>51 6  ga e_  g  ac  m  sp  cp  us  m  ta ca  da  al ab  w  ge  1a  rm  on  an  e  al  l  . [sent-466, score-1.048]
</p><p>52 n  s as cl ur fo  di  br  ab  ea  et  st  -c  es  an  n lia ra st au  a a1  e tre  sp  lic  e  e ag im  0  um  ce  r  0. [sent-467, score-1.853]
</p><p>53 6  ga  l  e_  al  m  sp  g  ac  m us cp  ca  da  ta  e ab  1a w  al  on  an rm ge  fo  di  . [sent-474, score-1.31]
</p><p>54 n  s as cl ur  ab  br ea  et  st  -c  es  an  n lia st ra au  a a1  tre  e  e lic sp  ag e im  0  um  ce  r  0. [sent-475, score-1.591]
</p><p>55 6  m  g  ga ac e_ sp  al us m cp  da ca  on al ab  1a w  ta  e  l  . [sent-481, score-0.923]
</p><p>56 n ge  rm  an  ss ur fo  di  ab  et  cl a  es  st -c an  n br  ea  ia ra l st au  a a1  e tre  sp  lic  e  e ag im  0  um  ce  r  0. [sent-482, score-1.978]
</p><p>57 6  m g  sp ac e_ ga  cp us m al l  ca da ta  ab al on e  fo ur c  la ss  an ce r  di ab et es  br ea st -c  au st ra lia n  a1 a  tre e  sp lic e  0  im ag e  0. [sent-489, score-2.723]
</p><p>58 6  ga e_  g  ac  m  sp  cp  us  m  ta ca  da  al ab  w  ge  1a  rm  on  an  e  al  l  . [sent-496, score-1.048]
</p><p>59 n  s as cl ur fo  di  br  ab  ea  et  st  -c  es  an  n lia ra st au  a a1  e tre  sp  lic  e  e ag im  0  um  ce  r  0. [sent-497, score-1.853]
</p><p>60 6  ga  l  e_  al  m  sp  g  ac  m us cp  ca  da  ta  e ab  1a w  al  on  an rm ge  fo  di  . [sent-504, score-1.31]
</p><p>61 n  s as cl ur  ab  br ea  et  st  -c  es  an  n lia st ra au  a a1  tre  e  e lic sp  ag e im  0  um  ce  r  0. [sent-505, score-1.591]
</p><p>62 6  m  g  ga ac e_ sp  al us m cp  da ca  on al ab  1a w  ta  e  l  . [sent-511, score-0.923]
</p><p>63 n ge  rm  an  ss ur fo  di  ab  et  cl a  es  st -c an  n br  ea  ia ra l st au  a a1  e tre  sp  lic  e  e ag im  0  um  ce  r  0. [sent-512, score-1.978]
</p><p>64 6  m g  sp ac e_ ga  cp us m al l  ca da ta  ab al on e  fo ur c  la ss  an ce r  di ab et es  br ea st -c  au st ra lia n  a1 a  tre e  sp lic e  0  im ag e  0. [sent-519, score-2.723]
</p><p>65 6  ga e_  g  ac  m  sp  cp  us  m  ta ca  da  al ab  w  ge  1a  rm  on  an  e  al  l  . [sent-526, score-1.048]
</p><p>66 n  s as cl ur fo  di  br  ab  ea  et  st  -c  es  an  n lia ra st au  a a1  e tre  sp  lic  e  e ag im  0  um  ce  r  0. [sent-527, score-1.853]
</p><p>67 6  ga  l  e_  al  m  sp  g  ac  m us cp  ca  da  ta  e ab  1a w  al  on  an rm ge  fo  di  . [sent-534, score-1.31]
</p><p>68 n  s as cl ur  ab  br ea  et  st  -c  es  an  n lia st ra au  a a1  tre  e  e lic sp  ag e im  0  um  ce  r  0. [sent-535, score-1.591]
</p><p>69 6  m  g  ga ac e_ sp  al us m cp  da ca  on al ab  1a w  ta  e  l  . [sent-541, score-0.923]
</p><p>70 n ge  rm  an  ss ur fo  di  ab  et  cl a  es  st -c an  n br  ea  ia ra l st au  a a1  e tre  sp  lic  e  e ag im  0  um  ce  r  0. [sent-542, score-1.978]
</p><p>71 When shrinking is incorporated, the cost per iteration varies and (32) may not hold. [sent-590, score-0.186]
</p><p>72 When the cache is not enough to store the whole kernel matrix, kernel evaluations are the main cost per iteration. [sent-598, score-0.281]
</p><p>73 Then similar to having enough cache, the time reduction does not match that of iterations due to the higher cost on selecting the working set per iteration. [sent-604, score-0.194]
</p><p>74 Therefore, results in Figures 1-8 indicate that with eﬀective shrinking and caching implementations, it is diﬃcult to have a new selection rule systematically surpassing WSS 1. [sent-605, score-0.202]
</p><p>75 The cache 1907  Fan, Chen, and Lin  size is 350M except 800M for covtype. [sent-610, score-0.22]
</p><p>76 To be more precise, if ˆ ˆ B = {i, j}, using dj = −di = yj dj = −yi di and a derivation similar to (13), we now must 1 ˆ2 + bij dj under the constraints ˆ ¯ minimize 2 aij dj k k k k ˆ ˆ −αj ≤ dj = yj dj ≤ C − αj and − αi ≤ di = −yi dj ≤ C − αi . [sent-627, score-1.974]
</p><p>77 Next, by dj = −yi yj di , we check the ﬁrst constraint. [sent-629, score-0.484]
</p><p>78 Equation (20) in WSS 3 is thus modiﬁed to j ∈ arg min t  1 ˆ2 ˆ ait dt + bit dt | t ∈ Ilow (αk ), −yt f (αk )t < −yi f (αk )i , ¯ 2  (35)  k k k k ˆ dt = yt max(−αt , min(C − αt , −yt yi max(−αi , min(C − αi , yi bit /¯it )))) . [sent-630, score-0.314]
</p><p>79 7  ag e sp lic e tre e a1 a au st ra br lian ea s di t-ca ab n et cer fo es ur cla ge ss rm a w1 n. [sent-647, score-1.688]
</p><p>80 n um a er ab al on e ca da ta cp us m sp all ac e m _ga g  0. [sent-648, score-0.837]
</p><p>81 9  ag e sp lic e tre e a1 a au st ra br lian ea s di t-ca ab n et cer fo es ur cla ge ss rm a w1 n. [sent-649, score-1.688]
</p><p>82 n um a er ab al on ca e da ta cp us m sp all ac e m _ga g  0. [sent-650, score-0.837]
</p><p>83 3  Data sets  Data sets  (a) The “parameter selection” step without shrinking  (b) The “ﬁnal training” step without shrinking  1. [sent-652, score-0.272]
</p><p>84 7  ag e sp lic e tre e a1 a au st ra br lian ea s di t-ca ab n et cer fo es ur cla ge ss rm a w1 n. [sent-665, score-1.688]
</p><p>85 n um a er ab al on e ca da ta cp us m sp all ac e m _ga g  0. [sent-666, score-0.837]
</p><p>86 9  ag e sp lic e tre e a1 a au st ra br lian ea s di t-ca ab n et cer fo es ur cla ge ss rm a w1 n. [sent-667, score-1.688]
</p><p>87 n um a er ab al on ca e da ta cp us m sp all ac e m _ga g  0. [sent-668, score-0.837]
</p><p>88 3  Data sets  Data sets  (c) The “parameter selection” step with shrinking  (d) The “ﬁnal training” step with shrinking  Figure 9: Iteration and time ratios between using (11) and (33) in WSS 2. [sent-670, score-0.342]
</p><p>89 (25) of Theorem 5, there is  δ k |αi − αi | < , ∀i ∈ I , ¯ 2 | − yi f (αk )i + yj f (αk )j | δ < , ∀i, j ∈ I , Kii + Kjj − 2Kij > 0, Kii + Kjj − 2Kij 2  (37) (38)  and all violating pairs come from I . [sent-683, score-0.228]
</p><p>90 If B = {i, j} is a violating pair selected by WSS 2 at the kth iteration, (37)-(39) imply that di and dj deﬁned in (15) satisfy k+1 k+1 k k 0 < αi = αi + di < C and 0 < αj = αj + dj < C. [sent-685, score-0.706]
</p><p>91 This theorem indicates that the two methods of working set selection in general lead to a similar number of iterations. [sent-692, score-0.184]
</p><p>92 Especially when the cache is large enough to store all kernel elements, selecting working sets is the main cost and hence the ratio is lower. [sent-704, score-0.431]
</p><p>93 Then the reduced problem can be stored in the small cache (100K), so kernel evaluations are largely saved. [sent-709, score-0.243]
</p><p>94 n  s  an  1a  rm  w  fo  ge  ur  ab di  br  cl  et  as  es  an -c st ea  ra st au  a a1  tre  e  e lic sp  im  0. [sent-755, score-1.671]
</p><p>95 n  s  an  1a  rm  w  ge  cl ur fo  di  ab  et  as  es  an -c st ea br  ra st au  a a1  tre  e  e lic sp  im  0. [sent-766, score-1.671]
</p><p>96 WSS 1 Solves Problem (7): the Proof ˆ ˆ For any given {i, j}, we can substitute di ≡ yi di and dj ≡ yj dj to (8), so the objective function becomes ˆ (−yi f (αk )i + yj f (αk )j )dj . [sent-801, score-0.964]
</p><p>97 (44) As di = dj = 0 is feasible for (8), the minimum of (44) is zero or a negative number. [sent-802, score-0.344]
</p><p>98 If ˆ ˆ −yi f (αk )i > −yj f (αk )j , using the condition di + dj = 0, the only possibility for (44) to ˆ ˆ be negative is dj < 0 and di > 0. [sent-803, score-0.644]
</p><p>99 Moreover, the minimum occurs at dj = −1 and di = 1. [sent-805, score-0.322]
</p><p>100 −yi f (α i j f (α j Therefore, solving (7) is essentially the same as min min yi f (αk )i − yj f (αk )j , 0  i ∈ Iup (αk ), j ∈ Ilow (αk )  = min −m(αk ) + M (αk ), 0 . [sent-807, score-0.211]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wss', 0.677), ('cache', 0.22), ('dj', 0.192), ('sp', 0.187), ('shrinking', 0.136), ('working', 0.136), ('fo', 0.132), ('lic', 0.132), ('tre', 0.132), ('yj', 0.131), ('di', 0.13), ('ilow', 0.126), ('sub', 0.122), ('al', 0.111), ('ab', 0.111), ('ta', 0.101), ('chen', 0.098), ('ur', 0.098), ('ea', 0.098), ('ag', 0.098), ('db', 0.092), ('im', 0.089), ('cp', 0.089), ('iup', 0.084), ('lia', 0.084), ('ra', 0.082), ('ge', 0.082), ('br', 0.082), ('iter', 0.081), ('da', 0.081), ('fan', 0.08), ('ga', 0.08), ('kjj', 0.078), ('au', 0.077), ('um', 0.077), ('st', 0.072), ('aij', 0.072), ('kii', 0.065), ('violating', 0.062), ('ss', 0.06), ('yt', 0.06), ('lin', 0.055), ('cl', 0.052), ('ratio', 0.052), ('ratios', 0.052), ('selection', 0.048), ('ce', 0.048), ('rm', 0.043), ('feasibility', 0.041), ('qij', 0.04), ('dt', 0.037), ('bij', 0.036), ('yi', 0.035), ('iteration', 0.035), ('arg', 0.034), ('xj', 0.034), ('decomposition', 0.034), ('check', 0.031), ('qbn', 0.03), ('ac', 0.029), ('er', 0.028), ('svms', 0.027), ('iterations', 0.025), ('lai', 0.025), ('ait', 0.024), ('ats', 0.024), ('cer', 0.024), ('cla', 0.024), ('ia', 0.024), ('len', 0.024), ('lian', 0.024), ('objective', 0.023), ('kernel', 0.023), ('ca', 0.023), ('yb', 0.022), ('mp', 0.022), ('feasible', 0.022), ('taiwan', 0.02), ('bb', 0.02), ('maximal', 0.019), ('rbf', 0.019), ('sigmoid', 0.019), ('covtype', 0.018), ('oldai', 0.018), ('oldaj', 0.018), ('qbb', 0.018), ('qii', 0.018), ('splice', 0.018), ('caching', 0.018), ('time', 0.018), ('ip', 0.017), ('libsvm', 0.017), ('violation', 0.017), ('checking', 0.017), ('la', 0.016), ('chang', 0.015), ('per', 0.015), ('min', 0.015), ('qjj', 0.015), ('tau', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="73-tfidf-1" href="./jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines.html">73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</a></p>
<p>Author: Rong-En Fan, Pai-Hsuen Chen, Chih-Jen Lin</p><p>Abstract: Working set selection is an important step in decomposition methods for training support vector machines (SVMs). This paper develops a new technique for working set selection in SMO-type decomposition methods. It uses second order information to achieve fast convergence. Theoretical properties such as linear convergence are established. Experiments demonstrate that the proposed method is faster than existing selection methods using ﬁrst order information. Keywords: support vector machines, decomposition methods, sequential minimal optimization, working set selection</p><p>2 0.044432838 <a title="73-tfidf-2" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>Author: S. Sathiya Keerthi, Dennis DeCoste</p><p>Abstract: This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classiﬁcation. This is done by modifying the ﬁnite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight , SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modiﬁed Huber’s loss function and the L1 loss function, and also for solving ordinal regression. Keywords: linear SVMs, classiﬁcation, conjugate gradient</p><p>3 0.039891109 <a title="73-tfidf-3" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of ﬁnding level sets for the data generating density. We interpret this learning problem as a binary classiﬁcation problem and compare the corresponding classiﬁcation risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classiﬁcation risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justiﬁcation for the well-known heuristic of artiﬁcially sampling “labeled” samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM. Keywords: unsupervised learning, anomaly detection, density levels, classiﬁcation, SVMs</p><p>4 0.038655888 <a title="73-tfidf-4" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>Author: Antoine Bordes, Seyda Ertekin, Jason Weston, Léon Bottou</p><p>Abstract: Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efﬁcient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We ﬁrst present an online SVM algorithm based on this premise. LASVM yields competitive misclassiﬁcation rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.</p><p>5 0.036299154 <a title="73-tfidf-5" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>Author: Fabio Aiolli, Alessandro Sperduti</p><p>Abstract: Winner-take-all multiclass classiﬁers are built on the top of a set of prototypes each representing one of the available classes. A pattern is then classiﬁed with the label associated to the most ‘similar’ prototype. Recent proposal of SVM extensions to multiclass can be considered instances of the same strategy with one prototype per class. The multi-prototype SVM proposed in this paper extends multiclass SVM to multiple prototypes per class. It allows to combine several vectors in a principled way to obtain large margin decision functions. For this problem, we give a compact constrained quadratic formulation and we propose a greedy optimization algorithm able to ﬁnd locally optimal solutions for the non convex objective function. This algorithm proceeds by reducing the overall problem into a series of simpler convex problems. For the solution of these reduced problems an efﬁcient optimization algorithm is proposed. A number of pattern selection strategies are then discussed to speed-up the optimization process. In addition, given the combinatorial nature of the overall problem, stochastic search strategies are suggested to escape from local minima which are not globally optimal. Finally, we report experiments on a number of datasets. The performance obtained using few simple linear prototypes is comparable to that obtained by state-of-the-art kernel-based methods but with a signiﬁcant reduction (of one or two orders) in response time. Keywords: multiclass classiﬁcation, multi-prototype support vector machines, kernel machines, stochastic search optimization, large margin classiﬁers</p><p>6 0.032587677 <a title="73-tfidf-6" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>7 0.032173462 <a title="73-tfidf-7" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>8 0.032114241 <a title="73-tfidf-8" href="./jmlr-2005-Local_Propagation_in_Conditional_Gaussian_Bayesian_Networks.html">51 jmlr-2005-Local Propagation in Conditional Gaussian Bayesian Networks</a></p>
<p>9 0.031975586 <a title="73-tfidf-9" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>10 0.030918468 <a title="73-tfidf-10" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>11 0.030391688 <a title="73-tfidf-11" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>12 0.029402943 <a title="73-tfidf-12" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>13 0.026195051 <a title="73-tfidf-13" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>14 0.025717752 <a title="73-tfidf-14" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>15 0.023793101 <a title="73-tfidf-15" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>16 0.020964483 <a title="73-tfidf-16" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>17 0.020625519 <a title="73-tfidf-17" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>18 0.019146005 <a title="73-tfidf-18" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>19 0.018936774 <a title="73-tfidf-19" href="./jmlr-2005-Change_Point_Problems_in_Linear_Dynamical_Systems.html">17 jmlr-2005-Change Point Problems in Linear Dynamical Systems</a></p>
<p>20 0.018929679 <a title="73-tfidf-20" href="./jmlr-2005-Matrix_Exponentiated_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">55 jmlr-2005-Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.114), (1, 0.06), (2, 0.084), (3, -0.072), (4, -0.041), (5, -0.047), (6, 0.017), (7, -0.011), (8, -0.141), (9, -0.035), (10, -0.135), (11, 0.039), (12, 0.006), (13, -0.018), (14, 0.211), (15, 0.001), (16, -0.125), (17, 0.1), (18, 0.12), (19, 0.042), (20, -0.062), (21, 0.178), (22, -0.23), (23, 0.168), (24, -0.246), (25, -0.077), (26, -0.218), (27, 0.013), (28, 0.025), (29, -0.121), (30, 0.034), (31, -0.355), (32, -0.161), (33, -0.049), (34, 0.396), (35, 0.222), (36, 0.087), (37, -0.13), (38, -0.258), (39, 0.119), (40, 0.219), (41, 0.062), (42, 0.005), (43, 0.051), (44, 0.055), (45, 0.121), (46, -0.011), (47, 0.045), (48, -0.023), (49, 0.091)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97455221 <a title="73-lsi-1" href="./jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines.html">73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</a></p>
<p>Author: Rong-En Fan, Pai-Hsuen Chen, Chih-Jen Lin</p><p>Abstract: Working set selection is an important step in decomposition methods for training support vector machines (SVMs). This paper develops a new technique for working set selection in SMO-type decomposition methods. It uses second order information to achieve fast convergence. Theoretical properties such as linear convergence are established. Experiments demonstrate that the proposed method is faster than existing selection methods using ﬁrst order information. Keywords: support vector machines, decomposition methods, sequential minimal optimization, working set selection</p><p>2 0.15540788 <a title="73-lsi-2" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>Author: Antoine Bordes, Seyda Ertekin, Jason Weston, Léon Bottou</p><p>Abstract: Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efﬁcient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We ﬁrst present an online SVM algorithm based on this premise. LASVM yields competitive misclassiﬁcation rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.</p><p>3 0.13638321 <a title="73-lsi-3" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>Author: S. Sathiya Keerthi, Dennis DeCoste</p><p>Abstract: This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classiﬁcation. This is done by modifying the ﬁnite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight , SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modiﬁed Huber’s loss function and the L1 loss function, and also for solving ordinal regression. Keywords: linear SVMs, classiﬁcation, conjugate gradient</p><p>4 0.10370331 <a title="73-lsi-4" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>Author: Ofer Dekel, Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We describe new loss functions for regression problems along with an accompanying algorithmic framework which utilizes these functions. These loss functions are derived by symmetrization of margin-based losses commonly used in boosting algorithms, namely, the logistic loss and the exponential loss. The resulting symmetric logistic loss can be viewed as a smooth approximation to the ε-insensitive hinge loss used in support vector regression. We describe and analyze two parametric families of batch learning algorithms for minimizing these symmetric losses. The ﬁrst family employs an iterative log-additive update which can be viewed as a regression counterpart to recent boosting algorithms. The second family utilizes an iterative additive update step. We also describe and analyze online gradient descent (GD) and exponentiated gradient (EG) algorithms for the symmetric logistic loss. A byproduct of our work is a new simple form of regularization for boosting-based classiﬁcation and regression algorithms. Our regression framework also has implications on classiﬁcation algorithms, namely, a new additive update boosting algorithm for classiﬁcation. We demonstrate the merits of our algorithms in a series of experiments.</p><p>5 0.097045437 <a title="73-lsi-5" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>Author: Hyunsoo Kim, Peg Howland, Haesun Park</p><p>Abstract: Support vector machines (SVMs) have been recognized as one of the most successful classiﬁcation methods for many applications including text classiﬁcation. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efﬁciently handle a large number of terms in practical applications of text classiﬁcation. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classiﬁcation algorithm and support vector classiﬁers to handle the classiﬁcation problem where a document may belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efﬁciency for both training and testing can be achieved without sacriﬁcing prediction accuracy of text classiﬁcation even when the dimension of the input space is signiﬁcantly reduced. Keywords: dimension reduction, support vector machines, text classiﬁcation, linear discriminant analysis, centroids</p><p>6 0.094561093 <a title="73-lsi-6" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>7 0.094203874 <a title="73-lsi-7" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>8 0.093158662 <a title="73-lsi-8" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>9 0.09117081 <a title="73-lsi-9" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>10 0.088816546 <a title="73-lsi-10" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>11 0.08268927 <a title="73-lsi-11" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>12 0.079347938 <a title="73-lsi-12" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>13 0.077139899 <a title="73-lsi-13" href="./jmlr-2005-Local_Propagation_in_Conditional_Gaussian_Bayesian_Networks.html">51 jmlr-2005-Local Propagation in Conditional Gaussian Bayesian Networks</a></p>
<p>14 0.070833683 <a title="73-lsi-14" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>15 0.06956856 <a title="73-lsi-15" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>16 0.068554543 <a title="73-lsi-16" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>17 0.067019209 <a title="73-lsi-17" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>18 0.062116783 <a title="73-lsi-18" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>19 0.061765417 <a title="73-lsi-19" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>20 0.060246982 <a title="73-lsi-20" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.015), (19, 0.025), (31, 0.558), (36, 0.02), (37, 0.024), (42, 0.013), (43, 0.014), (47, 0.044), (52, 0.065), (70, 0.016), (80, 0.011), (88, 0.047), (90, 0.026), (94, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77293926 <a title="73-lda-1" href="./jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines.html">73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</a></p>
<p>Author: Rong-En Fan, Pai-Hsuen Chen, Chih-Jen Lin</p><p>Abstract: Working set selection is an important step in decomposition methods for training support vector machines (SVMs). This paper develops a new technique for working set selection in SMO-type decomposition methods. It uses second order information to achieve fast convergence. Theoretical properties such as linear convergence are established. Experiments demonstrate that the proposed method is faster than existing selection methods using ﬁrst order information. Keywords: support vector machines, decomposition methods, sequential minimal optimization, working set selection</p><p>2 0.52775347 <a title="73-lda-2" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>Author: Aharon Bar-Hillel, Tomer Hertz, Noam Shental, Daphna Weinshall</p><p>Abstract: Many learning algorithms use a metric deﬁned over the input space as a principal tool, and their performance critically depends on the quality of this metric. We address the problem of learning metrics using side-information in the form of equivalence constraints. Unlike labels, we demonstrate that this type of side-information can sometimes be automatically obtained without the need of human intervention. We show how such side-information can be used to modify the representation of the data, leading to improved clustering and classiﬁcation. Speciﬁcally, we present the Relevant Component Analysis (RCA) algorithm, which is a simple and efﬁcient algorithm for learning a Mahalanobis metric. We show that RCA is the solution of an interesting optimization problem, founded on an information theoretic basis. If dimensionality reduction is allowed within RCA, we show that it is optimally accomplished by a version of Fisher’s linear discriminant that uses constraints. Moreover, under certain Gaussian assumptions, RCA can be viewed as a Maximum Likelihood estimation of the within class covariance matrix. We conclude with extensive empirical evaluations of RCA, showing its advantage over alternative methods. Keywords: clustering, metric learning, dimensionality reduction, equivalence constraints, side information.</p><p>3 0.17524073 <a title="73-lda-3" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>Author: Antoine Bordes, Seyda Ertekin, Jason Weston, Léon Bottou</p><p>Abstract: Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efﬁcient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We ﬁrst present an online SVM algorithm based on this premise. LASVM yields competitive misclassiﬁcation rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.</p><p>4 0.16318458 <a title="73-lda-4" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>Author: S. Sathiya Keerthi, Dennis DeCoste</p><p>Abstract: This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classiﬁcation. This is done by modifying the ﬁnite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight , SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modiﬁed Huber’s loss function and the L1 loss function, and also for solving ordinal regression. Keywords: linear SVMs, classiﬁcation, conjugate gradient</p><p>5 0.16315851 <a title="73-lda-5" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>Author: Ivor W. Tsang, James T. Kwok, Pak-Ming Cheung</p><p>Abstract: O(m3 ) Standard SVM training has time and O(m2 ) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such “approximateness” in this paper. We ﬁrst show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efﬁcient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and realworld data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about ﬁve million training patterns, in only 1.4 seconds on a 3.2GHz Pentium–4 PC. Keywords: kernel methods, approximation algorithm, minimum enclosing ball, core set, scalability</p><p>6 0.16132937 <a title="73-lda-6" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>7 0.16064699 <a title="73-lda-7" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>8 0.15791005 <a title="73-lda-8" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>9 0.15541744 <a title="73-lda-9" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>10 0.15532614 <a title="73-lda-10" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>11 0.1546683 <a title="73-lda-11" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>12 0.15450147 <a title="73-lda-12" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>13 0.15393879 <a title="73-lda-13" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>14 0.15339588 <a title="73-lda-14" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>15 0.15232956 <a title="73-lda-15" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>16 0.15106159 <a title="73-lda-16" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>17 0.15104219 <a title="73-lda-17" href="./jmlr-2005-Stability_of_Randomized_Learning_Algorithms.html">67 jmlr-2005-Stability of Randomized Learning Algorithms</a></p>
<p>18 0.15046796 <a title="73-lda-18" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>19 0.15027843 <a title="73-lda-19" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>20 0.15017155 <a title="73-lda-20" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
