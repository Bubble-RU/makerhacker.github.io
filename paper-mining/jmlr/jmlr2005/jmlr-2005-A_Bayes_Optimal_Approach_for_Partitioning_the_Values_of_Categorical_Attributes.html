<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 jmlr-2005-A Bayes Optimal Approach for Partitioning the Values of Categorical Attributes</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-1" href="#">jmlr2005-1</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 jmlr-2005-A Bayes Optimal Approach for Partitioning the Values of Categorical Attributes</h1>
<br/><p>Source: <a title="jmlr-2005-1-pdf" href="http://jmlr.org/papers/volume6/boulle05a/boulle05a.pdf">pdf</a></p><p>Author: Marc Boullé</p><p>Abstract: In supervised machine learning, the partitioning of the values (also called grouping) of a categorical attribute aims at constructing a new synthetic attribute which keeps the information of the initial attribute and reduces the number of its values. In this paper, we propose a new grouping method MODL1 founded on a Bayesian approach. The method relies on a model space of grouping models and on a prior distribution defined on this model space. This results in an evaluation criterion of grouping, which is minimal for the most probable grouping given the data, i.e. the Bayes optimal grouping. We propose new super-linear optimization heuristics that yields near-optimal groupings. Extensive comparative experiments demonstrate that the MODL grouping method builds high quality groupings in terms of predictive quality, robustness and small number of groups. Keywords: data preparation, grouping, Bayesianism, model selection, classification, naïve Bayes 1</p><p>Reference: <a title="jmlr-2005-1-reference" href="../jmlr2005_reference/jmlr-2005-A_Bayes_Optimal_Approach_for_Partitioning_the_Values_of_Categorical_Attributes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The method relies on a model space of grouping models and on a prior distribution defined on this model space. [sent-5, score-0.664]
</p><p>2 This results in an evaluation criterion of grouping, which is minimal for the most probable grouping given the data, i. [sent-6, score-0.679]
</p><p>3 Extensive comparative experiments demonstrate that the MODL grouping method builds high quality groupings in terms of predictive quality, robustness and small number of groups. [sent-10, score-0.883]
</p><p>4 Many induction algorithms rely on discrete attributes and need to discretize continuous attributes or to group the values of categorical attributes when they are too numerous. [sent-12, score-0.772]
</p><p>5 While the discretization problem has been studied extensively in the past, the grouping problem has not been explored so deeply in the literature. [sent-13, score-0.598]
</p><p>6 The grouping problem consists in partitioning the set of values of a categorical attribute into a finite number of groups. [sent-14, score-1.06]
</p><p>7 For example, most decision trees exploit a grouping method to handle categorical attributes, in order to increase the number of instances in each node of the tree (Zighed and Rakotomalala, 2000). [sent-15, score-0.963]
</p><p>8 When the categories are too numerous, this encoding scheme might be replaced by a grouping method. [sent-17, score-0.598]
</p><p>9 Moreover, the grouping is a general-purpose method that is intrinsically useful in the data preparation step of the data mining process (Pyle, 1999). [sent-19, score-0.594]
</p><p>10 The grouping methods can be clustered according to the search strategy of the best partition and to the grouping criterion used to evaluate the partitions. [sent-20, score-1.265]
</p><p>11 In the general case of more than two class values, there is no algorithm to find the optimal grouping with K groups, apart from exhaustive search. [sent-32, score-0.664]
</p><p>12 Decision tree algorithms often manage the grouping problem with a greedy heuristic based on a bottom-up classification of the categories. [sent-34, score-0.663]
</p><p>13 The process is reiterated until no further merge can improve the grouping criterion. [sent-36, score-0.678]
</p><p>14 In this paper, we present a new grouping method called MODL, which results from a similar approach as that of the MODL discretization method (Boullé, 2004c). [sent-44, score-0.598]
</p><p>15 This method is founded on a Bayesian approach to find the most probable grouping model given the data. [sent-45, score-0.646]
</p><p>16 We first define a general family of grouping models, and second propose a prior distribution on this model space. [sent-46, score-0.596]
</p><p>17 As the grouping problem has been turned into a minimization problem, the method automatically stops merging groups as soon as the evaluation of the resulting grouping does not decrease anymore. [sent-51, score-1.338]
</p><p>18 The 10 categorical values of the explanatory attribute CapColor are sorted by decreasing frequency; the proportions of the class values are reported for each explanatory value. [sent-61, score-0.843]
</p><p>19 Example of a grouping of the categorical values of the attribute CapColor of data set Mushroom  In data preparation for supervised learning, the problem of grouping is to produce the smallest possible number of groups with the slightest decay of information concerning the class values. [sent-86, score-1.842]
</p><p>20 Producing a good grouping is harder with large numbers of values since the risk of overfitting the data increases. [sent-91, score-0.608]
</p><p>21 In the limit situation where the number of values is the same as the number of instances, overfitting is obviously so important that efficient grouping methods should produce one single group, leading to the elimination of the attribute. [sent-92, score-0.649]
</p><p>22 In real applications, there are some domains that require grouping of the categorical attributes. [sent-93, score-0.939]
</p><p>23 2 Definition of a Grouping Model The objective of the grouping process is to induce a set of groups from the set of values of a categorical explanatory attribute. [sent-97, score-1.25]
</p><p>24 We propose in Definition 1 the following formal definition of a grouping model. [sent-100, score-0.632]
</p><p>25 Such a model is a pattern that describes both the partition of the categorical values into groups and the proportions of the class values in each group. [sent-101, score-0.679]
</p><p>26 Definition 1: A standard grouping model is defined by the following properties: 1. [sent-102, score-0.635]
</p><p>27 the grouping model allows to describe a partition of the categorical values into groups, 2. [sent-103, score-1.0]
</p><p>28 All the information concerning the explanatory attribute, such as the size of the data set, the number of explanatory values and their distribution might be used by a grouping model. [sent-107, score-0.839]
</p><p>29 The grouping model has to describe the partition of the explanatory values into groups and the distribution of the class values in each group. [sent-109, score-0.974]
</p><p>30 A SGM grouping model is completely defined by the parameters { K , {k (i )}1≤i≤ I , {nkj }1≤k ≤ K , 1≤ j ≤ J }. [sent-110, score-0.635]
</p><p>31 The grouping model pictured in Figure 1 is defined by K=5 groups, the description of the partition of the 10 values into 5 groups (for example, k(1)=2 since the BROWN CapColor belongs to the G2 group) and the description of the distribution of the class values in each group. [sent-113, score-0.938]
</p><p>32 This hypothesis is often assumed (at least implicitly) by many grouping methods that try to merge similar groups and separate groups with significantly different distributions of class values. [sent-122, score-1.063]
</p><p>33 This is the case for example with the CHAID grouping method (Kass, 1980), which merges two adjacent groups if their distributions of class values are statistically similar (using the chi-square test of independence). [sent-123, score-0.848]
</p><p>34 for a given number of groups K, every division of the I categorical values into K groups is equiprobable, 3. [sent-126, score-0.722]
</p><p>35 There is a subtlety in the three-stage prior, where choosing a grouping with K groups incorporates the case of potentially empty groups. [sent-146, score-0.742]
</p><p>36 The intuition behind the choice of this prior is that if K groups are chosen and if the categorical values are dropped in the groups independently from each other, empty groups are likely to appear. [sent-148, score-0.926]
</p><p>37 Furthermore, grouping algorithms can exploit this property in a preprocessing step and considerably reduce their overall computational complexity. [sent-154, score-0.616]
</p><p>38 Theorem 3: In a SGM model distributed according to the three-stage prior and in the case of two classes, the Bayes optimal grouping model consists of a single group when each instance has a different categorical value. [sent-155, score-1.089]
</p><p>39 Conjecture 1: In a Bayes optimal SGM model distributed according to the three-stage prior and in the case of two classes, any categorical value whose class proportion is between the class proportions of two categorical values belonging to the same group necessary belongs to this group. [sent-158, score-1.0]
</p><p>40 1435  BOULLE  This conjecture has been proven for other grouping criterion such as Gini (Breiman, 1984) or Kolmogorov-Smirnov (Asseraf, 2000) and experimentally validated in extensive experiments for the MODL criterion. [sent-159, score-0.612]
</p><p>41 The grouping algorithms, such as greedy bottom-up merge algorithms, can take benefit from this conjecture. [sent-161, score-0.714]
</p><p>42 4 Optimization of a Grouping Model Once the optimality of an evaluation criterion is established, the problem is to design a search algorithm in order to find a grouping model that minimizes the criterion. [sent-164, score-0.682]
</p><p>43 This merge is performed if the MODL evaluation criterion of the grouping decreases after the merge and the process is reiterated until not further merge can decrease the criterion. [sent-170, score-0.974]
</p><p>44 Once a grouping is evaluated, the value of a new grouping resulting from the merge between two adjacent groups can be evaluated in a single step, without scanning all the other groups. [sent-176, score-1.441]
</p><p>45 The subparts of the groups shared by all the J groupings can easily be identified and represent very good candidate subgroups of the global grouping problem. [sent-189, score-0.912]
</p><p>46 Furthermore, in the case of noisy attribute where the optimal grouping consists of a single group, this heuristic guarantees to find the optimal solution. [sent-201, score-0.749]
</p><p>47 3  Experiments  In our experimental study, we compare the MODL grouping method with other supervised grouping algorithms. [sent-211, score-1.134]
</p><p>48 In this section, we introduce the evaluation protocol, the alternative evaluated grouping methods and the evaluation results on artificial and real data sets. [sent-212, score-0.735]
</p><p>49 Finally, we present the impact of grouping as a preprocessing step to the Naïve Bayes classifier. [sent-213, score-0.616]
</p><p>50 The grouping problem is a bi-criteria problem that tries to compromise between the predictive quality and the number of groups. [sent-216, score-0.629]
</p><p>51 The optimal classifier is the Bayes classifier: in the case of an univariate classifier based on a single categorical attribute, the optimal grouping is to do nothing, 1437  BOULLE  i. [sent-217, score-1.154]
</p><p>52 In the case of categorical attributes, we have the unique opportunity of observing the class conditional distribution on the test data set: for each categorical value in the test data set, the observed distribution of the class values can be estimated by counting. [sent-227, score-0.814]
</p><p>53 The grouping methods allow to induce the class conditional distribution from the train data set: for each learnt group on the train data set, the learnt distribution of the class values can be estimated by counting. [sent-228, score-0.901]
</p><p>54 The objective of grouping is to minimize the distance between the learnt distribution and the observed distribution. [sent-229, score-0.592]
</p><p>55 The MODL grouping methods exploits a space of class conditional distribution models (the SGM models) and searches the most probable model given the train data. [sent-232, score-0.697]
</p><p>56 The MODL method exploits all the available train data to build its grouping model. [sent-234, score-0.624]
</p><p>57 For a given categorical value i, let pij be the probability of the jth class value estimated on the train data set (on the basis of the group containing the categorical value), and qij be the probability of the jth class value observed on the test data set (using directly the categorical value). [sent-240, score-1.402]
</p><p>58 j =1  In a first experiment, we compare the behavior of the evaluated grouping method on synthetic data sets, where the ideal grouping pattern is known in advance. [sent-244, score-1.155]
</p><p>59 In the second experiments, we 1438  A BAYES OPTIMAL GROUPING METHOD  use real data sets to compare the grouping methods considered as univariate classifiers. [sent-245, score-0.608]
</p><p>60 In a third experiment, we use the same data sets to evaluate the results of Naïve Bayes classifiers using the grouping methods to preprocess the categorical attributes. [sent-246, score-0.984]
</p><p>61 In order to increase the number of categorical attributes candidate for grouping, the continuous attributes have been discretized in a preprocessing step with a 10 equalwidth unsupervised discretization. [sent-264, score-0.621]
</p><p>62 The CHAID method is the grouping method used in the CHAID decision tree classifier (Kass, 1980). [sent-276, score-0.633]
</p><p>63 Furthermore, the Khiops method provides a guaranteed resistance to noise: any categorical attribute independent from the class attribute is grouped in a single terminal group with a user defined probability. [sent-282, score-0.773]
</p><p>64 1 The Noise Pattern The purpose of this experiment is to compare the robustness of the grouping methods. [sent-297, score-0.612]
</p><p>65 The noise pattern data set consists of an explanatory categorical attribute independent from the class attribute. [sent-298, score-0.642]
</p><p>66 The most comprehensible grouping consists of a single group whereas the worst one is to build one group per value. [sent-305, score-0.767]
</p><p>67 Mean of the number of unnecessary groups (K-1) and of the Kullback-Leibler divergence of the groupings of an explanatory attribute independent from the class attribute  The GainRatio method is constrained to produce at least 2 groups. [sent-321, score-0.762]
</p><p>68 2 The Mixture Pattern The objective of this experiment is to evaluate the sensibility of the grouping methods. [sent-339, score-0.616]
</p><p>69 The mixture pattern data set consists of an explanatory categorical attribute distributed according to several mixtures of class values. [sent-340, score-0.642]
</p><p>70 The BIC method manages to find the optimal number of groups when the number of categorical values is below 100. [sent-351, score-0.609]
</p><p>71 For small numbers of categorical values, it builds a constant number of groups equal to the number of class values, and beyond one hundred values, the number of groups raises sharply. [sent-354, score-0.796]
</p><p>72 The Khiops method benefits from its robustness and correctly identifies the 8 artificial groups as long as the frequencies of the categorical values are sufficient. [sent-356, score-0.717]
</p><p>73 5 The Real Data Sets Experiments The goal of this experiment is to evaluate the intrinsic performance of the grouping methods, without the bias of the choice of a specific induction algorithm. [sent-382, score-0.592]
</p><p>74 The grouping are performed on all the attributes of the UCI data sets presented in Table 1, using a stratified tenfold crossvalidation. [sent-383, score-0.667]
</p><p>75 As the purpose of the experiments is to evaluate the grouping methods according to the way they compromise between the quality and the size of the groupings, we also added three basic grouping methods for comparison reasons. [sent-384, score-1.189]
</p><p>76 For each grouping method, this represents 2300 measures for the univariate analysis (230 attributes) and 26140 measures for the bivariate analysis (2614 pairs of attributes). [sent-389, score-0.674]
</p><p>77 The gray line highlights the Pareto curve of the results obtained by the grouping methods. [sent-393, score-0.59]
</p><p>78 Mean of the number of groups and of the Kullback-Leibler divergence of the groupings performed on the UCI data sets, in univariate analysis (on the left) and bivariate analysis (on the right)  The MODL method gets the lowest number of group without discarding the predictive quality. [sent-427, score-0.632]
</p><p>79 6 Impact of Groupings on the Naïve Bayes Classifier The aim of this experiment is to evaluate the impact of grouping methods on the Naïve Bayes classifier. [sent-430, score-0.592]
</p><p>80 High quality grouping tend to increase the frequency in each group and to decrease the variance in the estimation of the conditional class density. [sent-436, score-0.77]
</p><p>81 On the opposite, the probabilities for categorical attribute are estimated using the Laplace's estimator directly on the categorical values, without any preprocessing such as grouping. [sent-441, score-0.892]
</p><p>82 It is interesting to study whether grouping could produce a more robust estimation of the class distributions and enhance the performance of the Naïve Bayes classifier. [sent-442, score-0.602]
</p><p>83 The experiment is performed on the 12 data sets presented in Table 1, using the univariate (all categorical attributes) and bivariate (all pairwise interactions of categorical attributes) sets of data sets. [sent-443, score-0.851]
</p><p>84 4  Conclusion  When categorical attributes contain few values, typically less than 10 values, using a grouping method is not required. [sent-546, score-1.039]
</p><p>85 The issue of grouping methods is to reduce the number of groups of values while maintaining the conditional class information. [sent-548, score-0.777]
</p><p>86 The MODL grouping method exploits the precise definition of a family of grouping models with a general prior. [sent-549, score-1.199]
</p><p>87 An optimization heuristics including preprocessing and post-optimizations is proposed in this paper to optimize the grouping with super-linear time complexity. [sent-553, score-0.616]
</p><p>88 Although understandability is hard to evaluate, the method is theoretically founded to produce correct explanations of the explanatory categorical attributes on the basis of the partition of their values, and even the most probable "grouping based" explanation given the train data. [sent-559, score-0.764]
</p><p>89 K  K  k  k =1  k =1  Proof: The prior probability of a grouping model M can be defined by the prior probability of the parameters of the model. [sent-576, score-0.693]
</p><p>90 Let us introduce some notations: • p ( K ) : prior probability of the number of groups K, •  (  )  p {k ( i )} : prior probability of a partition (defined by  {k ( i )} )  values into K groups,  ({ }) : prior probability of the set of parameters {n  •  p nkj  •  p nkj  ({  11 ,. [sent-577, score-0.615]
</p><p>91 of the categorical  },  The objective is to find the grouping model M that maximizes the probability p ( M | D ) for a given train data set D. [sent-587, score-1.037]
</p><p>92 I  The second hypothesis is that all the partition of the categorical values into at most K groups are equiprobable for a given K. [sent-593, score-0.608]
</p><p>93 = ∏ p nkj k =1  | K , {k ( i )}  }k | K ,{k ( i )})  = ∏ p nkj k =1 K  kj K  2  The frequencies per group nk derive from the frequencies per categorical values ni for a given partition of the values into groups. [sent-602, score-0.976]
</p><p>94 The cost variation ∆Cost1 of the grouping is ∆Cost1 = ∆PartitionCost1  + log ( ( n A0 + J − 1) ! [sent-629, score-0.645]
</p><p>95 This time, the cost variation ∆Cost2 of the grouping is  n −1  ∆Cost 2 − ∆PartitionCost 2 = log  ∏ ( nB1,1 − n ) ( nB1 − n + J − 1)     n =0  n   − log  ∏ ( n A1,1 + n ) ( n A1 + n + J − 1)  . [sent-682, score-0.699]
</p><p>96 Since the variations of partition costs are always non-negative, at least one of the two cost variations ∆Cost1 or ∆Cost2 is negative and the initial grouping could not be optimal. [sent-688, score-0.652]
</p><p>97 Theorem 3: In a SGM model distributed according to the three-stage prior and in the case of two classes, the Bayes optimal grouping model consists of a single group when each instance has a different categorical value. [sent-691, score-1.089]
</p><p>98 1449  BOULLE  Proof: Since each instance has a different categorical value, all the categorical values are pure values associated with one among the J class values. [sent-692, score-0.808]
</p><p>99 The total variation of the grouping cost is ∆Cost = ∆PartitionCost  ( ( − ( log ( C − ( log ( C  )  J + log Cn −1 + J −1 + log ( n A∪ B ! [sent-698, score-0.807]
</p><p>100 In particular, when the class values are n equi-distributed, the grouping cost in the case of a single group ( log(n) + log(n + 1) + log ( Cn 2 ) ) is higher than the grouping cost in the case of one group per categorical value ( log(n) + n log ( 2 ) ). [sent-737, score-1.897]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('grouping', 0.567), ('categorical', 0.372), ('modl', 0.324), ('groups', 0.175), ('groupings', 0.17), ('chaid', 0.162), ('khiops', 0.146), ('nkj', 0.146), ('na', 0.137), ('explanatory', 0.136), ('tschuprow', 0.122), ('merge', 0.111), ('sgm', 0.105), ('bic', 0.102), ('attributes', 0.1), ('group', 0.1), ('attribute', 0.099), ('boulle', 0.097), ('artificial', 0.089), ('bayes', 0.083), ('nk', 0.079), ('gainratio', 0.073), ('nogrouping', 0.073), ('merges', 0.071), ('defined', 0.068), ('bivariate', 0.066), ('classifier', 0.066), ('boull', 0.065), ('definition', 0.065), ('partition', 0.061), ('nb', 0.061), ('classification', 0.06), ('train', 0.057), ('log', 0.054), ('preprocessing', 0.049), ('partitioncost', 0.049), ('divergence', 0.048), ('robustness', 0.045), ('criterion', 0.045), ('owing', 0.044), ('univariate', 0.041), ('contingency', 0.041), ('find', 0.041), ('capcolor', 0.041), ('efficient', 0.041), ('overfits', 0.041), ('overfitting', 0.041), ('builds', 0.039), ('probable', 0.038), ('frequency', 0.038), ('pp', 0.036), ('proportions', 0.036), ('greedy', 0.036), ('frequencies', 0.036), ('bell', 0.035), ('class', 0.035), ('wins', 0.034), ('buff', 0.032), ('exhaustivechaid', 0.032), ('pictured', 0.032), ('pij', 0.032), ('ritschard', 0.032), ('predictive', 0.032), ('categories', 0.031), ('discretization', 0.031), ('kass', 0.03), ('ve', 0.03), ('quality', 0.03), ('evaluation', 0.029), ('sorted', 0.029), ('prior', 0.029), ('pure', 0.029), ('preparation', 0.027), ('cap', 0.027), ('qij', 0.027), ('yellow', 0.027), ('brown', 0.025), ('learnt', 0.025), ('evaluate', 0.025), ('gain', 0.025), ('caps', 0.024), ('cnj', 0.024), ('equidistributed', 0.024), ('exhaustivemodl', 0.024), ('mushroom', 0.024), ('pink', 0.024), ('preprocessed', 0.024), ('sensibility', 0.024), ('zighed', 0.024), ('quinlan', 0.024), ('cost', 0.024), ('instances', 0.024), ('gray', 0.023), ('des', 0.022), ('partitioning', 0.022), ('uci', 0.021), ('evaluated', 0.021), ('optimal', 0.021), ('classifiers', 0.02), ('dougherty', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="1-tfidf-1" href="./jmlr-2005-A_Bayes_Optimal_Approach_for_Partitioning_the_Values_of_Categorical_Attributes.html">1 jmlr-2005-A Bayes Optimal Approach for Partitioning the Values of Categorical Attributes</a></p>
<p>Author: Marc Boullé</p><p>Abstract: In supervised machine learning, the partitioning of the values (also called grouping) of a categorical attribute aims at constructing a new synthetic attribute which keeps the information of the initial attribute and reduces the number of its values. In this paper, we propose a new grouping method MODL1 founded on a Bayesian approach. The method relies on a model space of grouping models and on a prior distribution defined on this model space. This results in an evaluation criterion of grouping, which is minimal for the most probable grouping given the data, i.e. the Bayes optimal grouping. We propose new super-linear optimization heuristics that yields near-optimal groupings. Extensive comparative experiments demonstrate that the MODL grouping method builds high quality groupings in terms of predictive quality, robustness and small number of groups. Keywords: data preparation, grouping, Bayesianism, model selection, classification, naïve Bayes 1</p><p>2 0.056026924 <a title="1-tfidf-2" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>Author: Joseph F. Murray, Gordon F. Hughes, Kenneth Kreutz-Delgado</p><p>Abstract: We compare machine learning methods applied to a difﬁcult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classiﬁer (mi-NB) which is speciﬁcally designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum signiﬁcance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures. Keywords: hard drive failure prediction, rank-sum test, support vector machines (SVM), exact nonparametric statistics, multiple instance naive-Bayes</p><p>3 0.04047199 <a title="1-tfidf-3" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>Author: Dmitry Rusakov, Dan Geiger</p><p>Abstract: We develop a closed form asymptotic formula to compute the marginal likelihood of data given a naive Bayesian network model with two hidden states and binary features. This formula deviates from the standard BIC score. Our work provides a concrete example that the BIC score is generally incorrect for statistical models that belong to stratiﬁed exponential families. This claim stands in contrast to linear and curved exponential families, where the BIC score has been proven to provide a correct asymptotic approximation for the marginal likelihood. Keywords: Bayesian networks, asymptotic model selection, Bayesian information criterion (BIC)</p><p>4 0.036260691 <a title="1-tfidf-4" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>Author: Rie Kubota Ando, Tong Zhang</p><p>Abstract: One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don’t have a complete understanding of their eﬀectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Speciﬁcally we consider learning predictive structures on hypothesis spaces (that is, what kind of classiﬁers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the eﬀectiveness of the proposed algorithms in the semi-supervised learning setting.</p><p>5 0.031981107 <a title="1-tfidf-5" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>Author: Simone Fiori</p><p>Abstract: The aim of this contribution is to present a tutorial on learning algorithms for a single neural layer whose connection matrix belongs to the orthogonal group. The algorithms exploit geodesics appropriately connected as piece-wise approximate integrals of the exact differential learning equation. The considered learning equations essentially arise from the Riemannian-gradient-based optimization theory with deterministic and diffusion-type gradient. The paper aims speciﬁcally at reviewing the relevant mathematics (and at presenting it in as much transparent way as possible in order to make it accessible to readers that do not possess a background in differential geometry), at bringing together modern optimization methods on manifolds and at comparing the different algorithms on a common machine learning problem. As a numerical case-study, we consider an application to non-negative independent component analysis, although it should be recognized that Riemannian gradient methods give rise to general-purpose algorithms, by no means limited to ICA-related applications. Keywords: differential geometry, diffusion-type gradient, Lie groups, non-negative independent component analysis, Riemannian gradient</p><p>6 0.03156548 <a title="1-tfidf-6" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<p>7 0.029964909 <a title="1-tfidf-7" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>8 0.026322691 <a title="1-tfidf-8" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>9 0.026234444 <a title="1-tfidf-9" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>10 0.024138046 <a title="1-tfidf-10" href="./jmlr-2005-Combining_Information_Extraction_Systems_Using_Voting_and_Stacked_Generalization.html">21 jmlr-2005-Combining Information Extraction Systems Using Voting and Stacked Generalization</a></p>
<p>11 0.023724893 <a title="1-tfidf-11" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>12 0.023158465 <a title="1-tfidf-12" href="./jmlr-2005-Prioritization_Methods_for_Accelerating_MDP_Solvers.html">61 jmlr-2005-Prioritization Methods for Accelerating MDP Solvers</a></p>
<p>13 0.022907073 <a title="1-tfidf-13" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>14 0.021122023 <a title="1-tfidf-14" href="./jmlr-2005-A_Unifying_View_of_Sparse_Approximate_Gaussian_Process_Regression.html">7 jmlr-2005-A Unifying View of Sparse Approximate Gaussian Process Regression</a></p>
<p>15 0.02078191 <a title="1-tfidf-15" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>16 0.019917406 <a title="1-tfidf-16" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>17 0.019844955 <a title="1-tfidf-17" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>18 0.019317139 <a title="1-tfidf-18" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>19 0.019291671 <a title="1-tfidf-19" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>20 0.018905269 <a title="1-tfidf-20" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.117), (1, 0.004), (2, 0.02), (3, -0.064), (4, 0.079), (5, -0.004), (6, 0.01), (7, -0.074), (8, 0.082), (9, -0.079), (10, 0.098), (11, -0.1), (12, 0.033), (13, -0.295), (14, 0.017), (15, 0.008), (16, 0.156), (17, -0.081), (18, 0.018), (19, 0.036), (20, 0.111), (21, -0.051), (22, -0.065), (23, -0.041), (24, -0.164), (25, 0.026), (26, 0.262), (27, 0.253), (28, -0.007), (29, -0.124), (30, -0.006), (31, -0.364), (32, 0.54), (33, 0.006), (34, 0.015), (35, 0.027), (36, 0.217), (37, -0.109), (38, -0.07), (39, -0.058), (40, -0.128), (41, 0.001), (42, 0.01), (43, 0.012), (44, 0.079), (45, -0.184), (46, -0.092), (47, -0.001), (48, -0.015), (49, -0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98611987 <a title="1-lsi-1" href="./jmlr-2005-A_Bayes_Optimal_Approach_for_Partitioning_the_Values_of_Categorical_Attributes.html">1 jmlr-2005-A Bayes Optimal Approach for Partitioning the Values of Categorical Attributes</a></p>
<p>Author: Marc Boullé</p><p>Abstract: In supervised machine learning, the partitioning of the values (also called grouping) of a categorical attribute aims at constructing a new synthetic attribute which keeps the information of the initial attribute and reduces the number of its values. In this paper, we propose a new grouping method MODL1 founded on a Bayesian approach. The method relies on a model space of grouping models and on a prior distribution defined on this model space. This results in an evaluation criterion of grouping, which is minimal for the most probable grouping given the data, i.e. the Bayes optimal grouping. We propose new super-linear optimization heuristics that yields near-optimal groupings. Extensive comparative experiments demonstrate that the MODL grouping method builds high quality groupings in terms of predictive quality, robustness and small number of groups. Keywords: data preparation, grouping, Bayesianism, model selection, classification, naïve Bayes 1</p><p>2 0.17417718 <a title="1-lsi-2" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>Author: Joseph F. Murray, Gordon F. Hughes, Kenneth Kreutz-Delgado</p><p>Abstract: We compare machine learning methods applied to a difﬁcult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classiﬁer (mi-NB) which is speciﬁcally designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum signiﬁcance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures. Keywords: hard drive failure prediction, rank-sum test, support vector machines (SVM), exact nonparametric statistics, multiple instance naive-Bayes</p><p>3 0.14176612 <a title="1-lsi-3" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>Author: Dmitry Rusakov, Dan Geiger</p><p>Abstract: We develop a closed form asymptotic formula to compute the marginal likelihood of data given a naive Bayesian network model with two hidden states and binary features. This formula deviates from the standard BIC score. Our work provides a concrete example that the BIC score is generally incorrect for statistical models that belong to stratiﬁed exponential families. This claim stands in contrast to linear and curved exponential families, where the BIC score has been proven to provide a correct asymptotic approximation for the marginal likelihood. Keywords: Bayesian networks, asymptotic model selection, Bayesian information criterion (BIC)</p><p>4 0.12566863 <a title="1-lsi-4" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>Author: Hal Daumé III, Daniel Marcu</p><p>Abstract: We develop a Bayesian framework for tackling the supervised clustering problem, the generic problem encountered in tasks such as reference matching, coreference resolution, identity uncertainty and record linkage. Our clustering model is based on the Dirichlet process prior, which enables us to deﬁne distributions over the countably inﬁnite sets that naturally arise in this problem. We add supervision to our model by positing the existence of a set of unobserved random variables (we call these “reference types”) that are generic across all clusters. Inference in our framework, which requires integrating over inﬁnitely many parameters, is solved using Markov chain Monte Carlo techniques. We present algorithms for both conjugate and non-conjugate priors. We present a simple—but general—parameterization of our model based on a Gaussian assumption. We evaluate this model on one artiﬁcial task and three real-world tasks, comparing it against both unsupervised and state-of-the-art supervised algorithms. Our results show that our model is able to outperform other models across a variety of tasks and performance metrics. Keywords: supervised clustering, record linkage, citation matching, coreference, Dirichlet process, non-parametric Bayesian</p><p>5 0.12483498 <a title="1-lsi-5" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>Author: Simone Fiori</p><p>Abstract: The aim of this contribution is to present a tutorial on learning algorithms for a single neural layer whose connection matrix belongs to the orthogonal group. The algorithms exploit geodesics appropriately connected as piece-wise approximate integrals of the exact differential learning equation. The considered learning equations essentially arise from the Riemannian-gradient-based optimization theory with deterministic and diffusion-type gradient. The paper aims speciﬁcally at reviewing the relevant mathematics (and at presenting it in as much transparent way as possible in order to make it accessible to readers that do not possess a background in differential geometry), at bringing together modern optimization methods on manifolds and at comparing the different algorithms on a common machine learning problem. As a numerical case-study, we consider an application to non-negative independent component analysis, although it should be recognized that Riemannian gradient methods give rise to general-purpose algorithms, by no means limited to ICA-related applications. Keywords: differential geometry, diffusion-type gradient, Lie groups, non-negative independent component analysis, Riemannian gradient</p><p>6 0.10096911 <a title="1-lsi-6" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>7 0.092050329 <a title="1-lsi-7" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>8 0.090780824 <a title="1-lsi-8" href="./jmlr-2005-A_Unifying_View_of_Sparse_Approximate_Gaussian_Process_Regression.html">7 jmlr-2005-A Unifying View of Sparse Approximate Gaussian Process Regression</a></p>
<p>9 0.083158955 <a title="1-lsi-9" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>10 0.082064532 <a title="1-lsi-10" href="./jmlr-2005-Prioritization_Methods_for_Accelerating_MDP_Solvers.html">61 jmlr-2005-Prioritization Methods for Accelerating MDP Solvers</a></p>
<p>11 0.08121036 <a title="1-lsi-11" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>12 0.075870603 <a title="1-lsi-12" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>13 0.075342208 <a title="1-lsi-13" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>14 0.072365999 <a title="1-lsi-14" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>15 0.070072398 <a title="1-lsi-15" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>16 0.06959901 <a title="1-lsi-16" href="./jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</a></p>
<p>17 0.063847087 <a title="1-lsi-17" href="./jmlr-2005-Matrix_Exponentiated_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">55 jmlr-2005-Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>18 0.06288296 <a title="1-lsi-18" href="./jmlr-2005-Clustering_with_Bregman_Divergences.html">20 jmlr-2005-Clustering with Bregman Divergences</a></p>
<p>19 0.062602833 <a title="1-lsi-19" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>20 0.062422872 <a title="1-lsi-20" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(17, 0.04), (19, 0.016), (36, 0.017), (37, 0.038), (42, 0.019), (43, 0.027), (47, 0.02), (52, 0.059), (70, 0.608), (88, 0.041), (94, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93422937 <a title="1-lda-1" href="./jmlr-2005-A_Bayes_Optimal_Approach_for_Partitioning_the_Values_of_Categorical_Attributes.html">1 jmlr-2005-A Bayes Optimal Approach for Partitioning the Values of Categorical Attributes</a></p>
<p>Author: Marc Boullé</p><p>Abstract: In supervised machine learning, the partitioning of the values (also called grouping) of a categorical attribute aims at constructing a new synthetic attribute which keeps the information of the initial attribute and reduces the number of its values. In this paper, we propose a new grouping method MODL1 founded on a Bayesian approach. The method relies on a model space of grouping models and on a prior distribution defined on this model space. This results in an evaluation criterion of grouping, which is minimal for the most probable grouping given the data, i.e. the Bayes optimal grouping. We propose new super-linear optimization heuristics that yields near-optimal groupings. Extensive comparative experiments demonstrate that the MODL grouping method builds high quality groupings in terms of predictive quality, robustness and small number of groups. Keywords: data preparation, grouping, Bayesianism, model selection, classification, naïve Bayes 1</p><p>2 0.9208988 <a title="1-lda-2" href="./jmlr-2005-New_Horn_Revision_Algorithms.html">59 jmlr-2005-New Horn Revision Algorithms</a></p>
<p>Author: Judy Goldsmith, Robert H. Sloan</p><p>Abstract: A revision algorithm is a learning algorithm that identiﬁes the target concept, starting from an initial concept. Such an algorithm is considered efﬁcient if its complexity (in terms of the measured resource) is polynomial in the syntactic distance between the initial and the target concept, but only polylogarithmic in the number of variables in the universe. We give efﬁcient revision algorithms in the model of learning with equivalence and membership queries. The algorithms work in a general revision model where both deletion and addition revision operators are allowed. In this model one of the main open problems is the efﬁcient revision of Horn formulas. Two revision algorithms are presented for special cases of this problem: for depth-1 acyclic Horn formulas, and for deﬁnite Horn formulas with unique heads. Keywords: theory revision, Horn formulas, query learning, exact learning, computational learning theory</p><p>3 0.85998595 <a title="1-lda-3" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>Author: Malte Kuss, Carl Edward Rasmussen</p><p>Abstract: Gaussian process priors can be used to deﬁne ﬂexible, probabilistic classiﬁcation models. Unfortunately exact Bayesian inference is analytically intractable and various approximation techniques have been proposed. In this work we review and compare Laplace’s method and Expectation Propagation for approximate Bayesian inference in the binary Gaussian process classiﬁcation model. We present a comprehensive comparison of the approximations, their predictive performance and marginal likelihood estimates to results obtained by MCMC sampling. We explain theoretically and corroborate empirically the advantages of Expectation Propagation compared to Laplace’s method. Keywords: Gaussian process priors, probabilistic classiﬁcation, Laplace’s approximation, expectation propagation, marginal likelihood, evidence, MCMC</p><p>4 0.37124076 <a title="1-lda-4" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>Author: Wei Chu, Zoubin Ghahramani</p><p>Abstract: We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative ﬁltering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach. Keywords: Gaussian processes, ordinal regression, approximate Bayesian inference, collaborative ﬁltering, gene expression analysis, feature selection</p><p>5 0.32671311 <a title="1-lda-5" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>Author: Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, Suvrit Sra</p><p>Abstract: Several large scale data mining applications, such as text categorization and gene expression analysis, involve high-dimensional data that is also inherently directional in nature. Often such data is L2 normalized so that it lies on the surface of a unit hypersphere. Popular models such as (mixtures of) multi-variate Gaussians are inadequate for characterizing such data. This paper proposes a generative mixture-model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. In particular, we derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the mean and concentration parameters of this mixture. Numerical estimation of the concentration parameters is non-trivial in high dimensions since it involves functional inversion of ratios of Bessel functions. We also formulate two clustering algorithms corresponding to the variants of EM that we derive. Our approach provides a theoretical basis for the use of cosine similarity that has been widely employed by the information retrieval community, and obtains the spherical kmeans algorithm (kmeans with cosine similarity) as a special case of both variants. Empirical results on clustering of high-dimensional text and gene-expression data based on a mixture of vMF distributions show that the ability to estimate the concentration parameter for each vMF component, which is not present in existing approaches, yields superior results, especially for difﬁcult clustering tasks in high-dimensional spaces. Keywords: clustering, directional distributions, mixtures, von Mises-Fisher, expectation maximization, maximum likelihood, high dimensional data c 2005 Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh and Suvrit Sra. BANERJEE , D HILLON , G HOSH AND S RA</p><p>6 0.32442001 <a title="1-lda-6" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>7 0.31559795 <a title="1-lda-7" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>8 0.31106839 <a title="1-lda-8" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>9 0.29592329 <a title="1-lda-9" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>10 0.29561484 <a title="1-lda-10" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>11 0.29309577 <a title="1-lda-11" href="./jmlr-2005-Active_Coevolutionary_Learning_of_Deterministic_Finite_Automata.html">8 jmlr-2005-Active Coevolutionary Learning of Deterministic Finite Automata</a></p>
<p>12 0.28504986 <a title="1-lda-12" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>13 0.27708751 <a title="1-lda-13" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>14 0.27701208 <a title="1-lda-14" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>15 0.27602187 <a title="1-lda-15" href="./jmlr-2005-Local_Propagation_in_Conditional_Gaussian_Bayesian_Networks.html">51 jmlr-2005-Local Propagation in Conditional Gaussian Bayesian Networks</a></p>
<p>16 0.27573991 <a title="1-lda-16" href="./jmlr-2005-A_Unifying_View_of_Sparse_Approximate_Gaussian_Process_Regression.html">7 jmlr-2005-A Unifying View of Sparse Approximate Gaussian Process Regression</a></p>
<p>17 0.27349487 <a title="1-lda-17" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>18 0.26207557 <a title="1-lda-18" href="./jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error.html">13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</a></p>
<p>19 0.2616002 <a title="1-lda-19" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>20 0.25446123 <a title="1-lda-20" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
