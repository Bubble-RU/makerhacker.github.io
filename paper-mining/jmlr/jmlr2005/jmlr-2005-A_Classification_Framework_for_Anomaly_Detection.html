<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 jmlr-2005-A Classification Framework for Anomaly Detection</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-3" href="#">jmlr2005-3</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3 jmlr-2005-A Classification Framework for Anomaly Detection</h1>
<br/><p>Source: <a title="jmlr-2005-3-pdf" href="http://jmlr.org/papers/volume6/steinwart05a/steinwart05a.pdf">pdf</a></p><p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of ﬁnding level sets for the data generating density. We interpret this learning problem as a binary classiﬁcation problem and compare the corresponding classiﬁcation risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classiﬁcation risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justiﬁcation for the well-known heuristic of artiﬁcially sampling “labeled” samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM. Keywords: unsupervised learning, anomaly detection, density levels, classiﬁcation, SVMs</p><p>Reference: <a title="jmlr-2005-3-reference" href="../jmlr2005_reference/jmlr-2005-A_Classification_Framework_for_Anomaly_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 GOV  Modeling, Algorithms and Informatics Group, CCS-3 Los Alamos National Laboratory Los Alamos, NM 87545, USA  Editor: Bernhard Sch¨ lkopf o  Abstract One way to describe anomalies is by saying that anomalies are not concentrated. [sent-4, score-0.316]
</p><p>2 We interpret this learning problem as a binary classiﬁcation problem and compare the corresponding classiﬁcation risk with the standard performance measure for the density level problem. [sent-6, score-0.282]
</p><p>3 In particular it turns out that the empirical classiﬁcation risk can serve as an empirical performance measure for the anomaly detection problem. [sent-7, score-0.727]
</p><p>4 This allows us to compare different anomaly detection algorithms empirically, i. [sent-8, score-0.522]
</p><p>5 In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. [sent-12, score-0.563]
</p><p>6 Keywords: unsupervised learning, anomaly detection, density levels, classiﬁcation, SVMs  1. [sent-14, score-0.378]
</p><p>7 Introduction Anomaly (or novelty) detection aims to detect anomalous observations from a system. [sent-15, score-0.342]
</p><p>8 This anomaly detection learning problem has many important applications including the detection of e. [sent-18, score-0.769]
</p><p>9 It is important to note that a typical feature of these applications is that only unlabeled samples are available, and hence one has to make some a-priori assumptions on anomalies in order to be able to distinguish between normal and anomalous future oberservations. [sent-28, score-0.33]
</p><p>10 One of the most common ways to deﬁne anomalies is by saying that anomalies are not concentrated (see e. [sent-29, score-0.316]
</p><p>11 Obviously, the density level sets {h > ρ}, ρ > 0, describe the concentration of Q. [sent-34, score-0.165]
</p><p>12 Therefore to deﬁne anomalies in terms of the concentration one only has to ﬁx a threshold level ρ > 0 so that a sample x ∈ X is considered to be anomalous whenever h(x) ≤ ρ. [sent-35, score-0.315]
</p><p>13 We emphasize that given the data-generating distribution Q the choice of µ determines the density h, and consequently anomalies are actually modeled by both µ and ρ. [sent-37, score-0.287]
</p><p>14 Gaussian mixtures, Parzen windows and k-nearest neighbors density estimates) and therefore for these algorithms deﬁning anomalies is restricted to the choice of ρ. [sent-40, score-0.29]
</p><p>15 In particular, this is true if we consider a modiﬁcation of the anomaly detection problem where µ is not known but can be sampled from. [sent-42, score-0.522]
</p><p>16 Finding level sets of an unknown density is also a well known problem in statistics which has some important applications different from anomaly detection. [sent-44, score-0.408]
</p><p>17 Unfortunately, the algorithms considered in these articles cannot be used for the anomaly detection problem since the imposed assumptions on h are often tailored to the above applications and are in general unrealistic for anomalies. [sent-50, score-0.522]
</p><p>18 One of the main problems of anomaly detection—or more precisely density level detection—is the lack of an empirical performance measure which allows us to compare the generalization performance of different algorithms by test samples. [sent-51, score-0.471]
</p><p>19 By interpreting the density level detection problem as binary classiﬁcation with respect to an appropriate measure, we show that the corresponding empirical classiﬁcation risk can serve as such an empirical performance measure for anomaly detection. [sent-52, score-0.897]
</p><p>20 Furthermore, we compare the excess classiﬁcation risk with the standard performance measure for the density level detection problem. [sent-53, score-0.608]
</p><p>21 , 2003) for anomaly detection is to generate a labeled data set by assigning one label to the original unlabeled data and another label to a set of artiﬁcially generated data, and then apply a binary classiﬁcation algorithm. [sent-60, score-0.557]
</p><p>22 By interpreting the density level detection problem as a binary classiﬁcation problem we can show that this heuristic can be strongly justiﬁed provided that the sampling plan for the artiﬁcial samples is chosen in a certain way and the used classiﬁcation algorithm is well-adopted to this plan. [sent-61, score-0.532]
</p><p>23 Detecting Density Levels is a Classiﬁcation Problem We begin with rigorously deﬁning the density level detection (DLD) problem. [sent-66, score-0.38]
</p><p>24 For the density level detection problem and related tasks this is a common assumption (see e. [sent-73, score-0.38]
</p><p>25 Furthermore, for a sequence of functions fn : X → R with Sµ,h,ρ ( fn ) → 0 we easily see that sign fn (x) → 1{h>ρ} (x) for µ-almost all x ∈ X, and since Q is absolutely continuous with respect to µ the same convergence holds Q-almost surely. [sent-91, score-0.306]
</p><p>26 Then the probability measure Q s µ on X ×Y is deﬁned by Q  s µ (A)  := sEx∼Q 1A (x, 1) + (1 − s)Ex∼µ 1A (x, −1)  for all measurable subsets A ⊂ X ×Y . [sent-110, score-0.141]
</p><p>27 Inspired by this interpretation let us recall that the binary classiﬁcation risk for a measurable function f : X → R and a distribution P on X ×Y is deﬁned by  RP ( f ) = P {(x, y) : sign f (x) = y} , where we deﬁne signt := 1 if t > 0 and signt = −1 otherwise. [sent-114, score-0.284]
</p><p>28 Furthermore, the Bayes risk RP of P is the smallest possible classiﬁcation risk with respect to P, i. [sent-115, score-0.228]
</p><p>29 Then for all sequences ( fn ) of measurable functions fn : X → R the following are equivalent: i) SP ( fn ) → 0. [sent-146, score-0.304]
</p><p>30 Since by Corollary 3 we know µ({h > ρ} 1 {η > 2 }) = 0 it is easy to see that the classiﬁcation risk of fn can be computed by  RP ( fn ) = RP +  Z  En  |2η − 1|dPX . [sent-150, score-0.246]
</p><p>31 Therefore, the assertion follows from SP ( fn ) = µ(En ). [sent-156, score-0.151]
</p><p>32 Theorem 4 shows that instead of using SP as a performance measure for the density level detection problem one can alternatively use the classiﬁcation risk RP (. [sent-157, score-0.529]
</p><p>33 Then for all measurable f : X → R we have  RP ( f ) =  1 1+ρ  and deﬁne  1 ρ EQ I(1, sign f ) + Eµ I(−1, sign f ) . [sent-163, score-0.234]
</p><p>34 1+ρ 1+ρ  Proof The ﬁrst assertion directly follows from  RP ( f ) = P {(x, y) : sign f (x) = y} = P {(x, 1) : sign f (x) = −1} + P {(x, −1) : sign f (x) = 1} = sQ {sign f = −1} + (1 − s)µ {sign f = 1} = sEQ I(1, sign f ) + (1 − s)Eµ I(−1, sign f ) . [sent-165, score-0.405]
</p><p>35 As described at the beginning of this section our main goal is to ﬁnd a performance measure for the density level detection problem which has an empirical counterpart. [sent-168, score-0.443]
</p><p>36 , xn ) ∈ X n and a measurable function f : X → R we deﬁne  RT ( f ) :=  n ρ 1 ∑ I(1, sign f (xi )) + 1 + ρ Eµ I(−1, sign f ) . [sent-174, score-0.234]
</p><p>37 (1 + ρ)n i=1  If we identify T with the corresponding empirical measure it is easy to see that RT ( f ) is the 1 classiﬁcation risk with respect to the measure T s µ for s := 1+ρ . [sent-175, score-0.212]
</p><p>38 ) is strongly connected with another approach for the density level detection problem which is based on the so-called excess mass (see e. [sent-180, score-0.515]
</p><p>39 To be more precise let us ﬁrst recall that the excess mass of a measurable function f : X → R is deﬁned by EP ( f ) := Q({ f > 0}) − ρµ({ f > 0}) , where Q, ρ and µ have the usual meaning. [sent-183, score-0.241]
</p><p>40 By the above proposition we have  ET ( f ) = 1 −  1 n ∑ I(1, sign f (xi )) − ρEµ I(−1, sign f ) = 1 − (1 + ρ)RT ( f ) n i=1  (2)  for all measurable f : X → R. [sent-193, score-0.287]
</p><p>41 Now, given a class F of measurable functions from X to R the (empirical) excess mass approach considered e. [sent-194, score-0.241]
</p><p>42 By equation (2) we see that this approach is actually a type of empirical risk minimization (ERM). [sent-198, score-0.142]
</p><p>43 956) used (3) for the analysis of a density level detection method which is based on a localized version of the empirical excess mass approach. [sent-213, score-0.543]
</p><p>44 Mammen and Tsybakov, 1999; Tsybakov, 2004; Steinwart and Scovel, 2004) as the following proposition proved in the appendix shows: Proposition 9 Let µ and Q be distributions on X such that Q has a density h with respect to µ. [sent-216, score-0.156]
</p><p>45 However it appears that only the “RP ( fn ) → RP ⇒ SP ( fn ) → 0” assertion of Theorem 4 holds instead of equivalence. [sent-245, score-0.217]
</p><p>46 Therefore we immediately obtain the following corollary Corollary 13 Under the assumptions of Theorem 12 both the DLD-SVM with offset and without ˜ offset are universally consistent with respect to SP (. [sent-322, score-0.153]
</p><p>47 This gives a strong justiﬁcation for the well-known heuristic of adding artiﬁcial samples to anomaly detection problems with unlabeled data. [sent-328, score-0.625]
</p><p>48 Experiments We present experimental results for anomaly detection problems where the set X is a subset of Rd . [sent-331, score-0.522]
</p><p>49 Based on Deﬁnition 6 we deﬁne the empirical risk of f with respect to (S, S ) to be  R(S,S ) ( f ) =  ρ 1 ∑ I(1, sign f (x)) + (1 + ρ)|S | ∑ I(−1, sign f (x)). [sent-335, score-0.27]
</p><p>50 the rate at which samples will be labeled anomalous by f ), and the quantity 1 |S | ∑x∈S I(−1, sign f (x)) is an estimate of µ({ f > 0}) which we call the volume of the predicted normal set. [sent-340, score-0.201]
</p><p>51 Also, from the expression for the risk in Proposition 5 it is clear that for any two functions with the same alarm rate we prefer the function with the smaller volume and vice versa. [sent-344, score-0.18]
</p><p>52 We consider three different anomaly detection problems, two are synthetic and one is an application in cybersecurity. [sent-347, score-0.522]
</p><p>53 In each case we deﬁne a problem instance to be a triplet consisting of samples from Q, samples from µ, and a value for the density level ρ. [sent-348, score-0.217]
</p><p>54 (2001) and the o others (including the Parzen windows method) are based on some of the most common parametric and non-parametric statistical methods for density-based anomaly detection in Rd . [sent-351, score-0.551]
</p><p>55 The core procedures are run on the training sets and the values of the free parameters are chosen to minimize the empirical risk (8) on the validation sets. [sent-358, score-0.169]
</p><p>56 The regularization parameters λ and σ2 are chosen to (approximately) minimize the empirical risk R(V,V ) ( f ) on the validation sets. [sent-363, score-0.169]
</p><p>57 In particular, for each value of λ from a ﬁxed grid we seek a minimizer over σ2 by evaluating the validation risk at a coarse grid of σ2 values and then performing a Golden search over the interval deﬁned by the two σ2 values on either side of the coarse grid minimum. [sent-365, score-0.309]
</p><p>58 2 As the overall search proceeds the (λ, σ2 ) pair with the smallest validation risk is retained. [sent-366, score-0.141]
</p><p>59 In particular both ν and σ2 are chosen to (approximately) minimize the validation risk using the search procedure described above for the DLD–SVM where the grid search for λ is replaced by a Golden search (over [0, 1]) for ν. [sent-376, score-0.197]
</p><p>60 The GML algorithm produces a function f = g − t where t is an offset and g is a Gaussian probability density function whose mean and inverse covariance are determined from maximum likelihood estimates formed from the training data T (see e. [sent-377, score-0.166]
</p><p>61 Once the parameters of g are determined the offset t is chosen to minimize the training risk R(T,T ) . [sent-382, score-0.177]
</p><p>62 The regularization parameter λ is chosen to (approximately) minimize the validation risk by searching a ﬁxed grid of λ values. [sent-383, score-0.197]
</p><p>63 222  A C LASSIFICATION F RAMEWORK FOR A NOMALY D ETECTION  Number of Q samples Number of µ samples λ grid (DLD–SVM/GML/MGML) σ2 grid (DLD–SVM/1CLASS–SVM)  Train 1000 2000  Validate 500 2000  Test 100,000 100,000  1. [sent-388, score-0.196]
</p><p>64 for all inverse covariance estimates and both λ and K are chosen to (approximately) minimize the validation risk by searching a ﬁxed grid of (λ, K) values. [sent-408, score-0.197]
</p><p>65 Figures 1(a) and 1(c) plot the empirical risk R(W,W ) versus ρ while Figures 1(b) and 1(d) plot the corresponding performance curves. [sent-427, score-0.142]
</p><p>66 These results are signiﬁcant because values of ρ substantially larger than one appear to have little utility here since they yield alarm rates that do not conform to our notion that anomalies are rare events. [sent-434, score-0.224]
</p><p>67 In addition ρ 1 appears to have little utility in the general anomaly detection problem since it deﬁnes anomalies in regions where the concentration of Q is much larger than the concentration of µ, which is contrary to our premise that anomalies are not concentrated. [sent-435, score-0.902]
</p><p>68 005 to 50 and the results are summarized by the empirical risk curve in Figure 2(a) and the corresponding performance curve in Figure 2(b). [sent-490, score-0.142]
</p><p>69 The empirical risk values for DLD–SVM and MGML are nearly identical except for ρ = 0. [sent-491, score-0.142]
</p><p>70 Number of Q samples Number of µ samples λ grid (DLD–SVM/GML/MGML) σ2 grid (DLD–SVM/1CLASS–SVM)  Train 4000 10,000  Validate 2000 100,000  Test 5664 100,000  0. [sent-493, score-0.196]
</p><p>71 Except for this case the empirical risk values for DLD–SVM and MGML are much better than 1CLASS–SVM and GML at nearly all values of ρ. [sent-514, score-0.142]
</p><p>72 The performance curves conﬁrm the superiority of DLD–SVM and MGML, but also reveal differences not easily seen in the empirical risk curves. [sent-515, score-0.169]
</p><p>73 For example, all four methods produced some solutions with identical performance estimates for different values of ρ which is reﬂected by the fact that the performance curves show fewer points than the corresponding empirical risk curves. [sent-516, score-0.169]
</p><p>74 Discussion A review of the literature on anomaly detection suggests that there are many ways to characterize anomalies (see e. [sent-533, score-0.68]
</p><p>75 In this work we assumed that anomalies are not concentrated. [sent-536, score-0.158]
</p><p>76 This assumption can be speciﬁed by choosing a reference measure µ which determines a density and a level value ρ. [sent-537, score-0.194]
</p><p>77 The density then quantiﬁes the degree of concentration and the density level ρ establishes a threshold on the degree that determines anomalies. [sent-538, score-0.294]
</p><p>78 In practice the user chooses µ and ρ to capture some notion of anomaly that he deems relevant to the application. [sent-540, score-0.275]
</p><p>79 This paper advances the existing state of “density based” anomaly detection in the following ways. [sent-541, score-0.522]
</p><p>80 Therefore we accommodate a larger class of anomaly detection problems. [sent-543, score-0.522]
</p><p>81 Consequently, it has been difﬁcult to compare different methods for anomaly detection in practice. [sent-549, score-0.522]
</p><p>82 We have demonstrated this approach which is a rigorous variant of a well-known heuristic for anomaly detection in the formulation of the DLD-SVM. [sent-554, score-0.548]
</p><p>83 These advances have created a situation in which much of the knowledge on classiﬁcation can now be used for anomaly detection. [sent-555, score-0.275]
</p><p>84 Consequently, we expect substantial advances in anomaly detection in the future. [sent-556, score-0.522]
</p><p>85 Finally let us consider a different learning scenario in which anomaly detection methods are also commonly employed. [sent-557, score-0.522]
</p><p>86 Hidden classiﬁcation problems for example occur in network intrusion detection problems where it is impractical to obtain labels. [sent-561, score-0.275]
</p><p>87 This means for example that when an anomaly detection method is used to produce a classiﬁer f for a HCP its anomaly detection performance RP ( f ) with 1 P := Q s µ and s := 1+ρ may be very different from its hidden classiﬁcation performance Rν ( f ). [sent-573, score-1.044]
</p><p>88 Another consequence of the above considerations is that the common practice of measuring the performance of anomaly detection algorithms on (hidden) binary classiﬁcation problems is problematic. [sent-579, score-0.522]
</p><p>89 Indeed, the obtained classiﬁcation errors depend on the model error and thus they provide an inadequate description how well the algorithms solve the anomaly detection problem. [sent-580, score-0.522]
</p><p>90 In conclusion although there are clear similiarities between the use of the DLD formalism for anomaly detection and its use for the HCP there is also an important difference. [sent-584, score-0.522]
</p><p>91 In the ﬁrst case the speciﬁcation of µ and ρ determines the deﬁnition of anomalies and therefore there is no model error, whereas in the second case the model error is determined by the choice of µ and ρ. [sent-585, score-0.184]
</p><p>92 2) gives a sufﬁcient condition for the existence of a regular conditional probability: 228  A C LASSIFICATION F RAMEWORK FOR A NOMALY D ETECTION  Theorem 17 If Y is a Polish space then a regular conditional probability P( . [sent-614, score-0.178]
</p><p>93 Therefore we ﬁnd  1+t 1−t ρ≤h≤ ρ 1+t 1−t  |2η − 1| ≤ t  ≤ PX  (1 − tr )ρ ≤ h ≤ (1 + tr )ρ  =  PX  (1 − tl )ρ ≤ h ≤ (1 + tr )ρ  ⊂  Hence for all sufﬁciently small t > 0 with t <  =  |h − ρ| ≤ tr ρ . [sent-644, score-0.182]
</p><p>94 Applications of probability density estimation to the detection of abnormal conditions in engineering. [sent-704, score-0.35]
</p><p>95 Using artiﬁcial anomalies to detect unknown and known network intrusions. [sent-728, score-0.158]
</p><p>96 Support vector novelty detection applied to o jet engine vibration spectra. [sent-755, score-0.345]
</p><p>97 The use of novelty detection techniques for monitoring high-integrity plant. [sent-773, score-0.307]
</p><p>98 Network intrusion and fault detection: a statistical anomaly approach. [sent-786, score-0.303]
</p><p>99 Measuring mass concentrations and estimating density contour clusters—an excess mass aproach. [sent-819, score-0.294]
</p><p>100 Novelty detection for the identiﬁcation of masses in mammograms. [sent-882, score-0.247]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dld', 0.478), ('rp', 0.323), ('anomaly', 0.275), ('detection', 0.247), ('mgml', 0.189), ('gml', 0.164), ('anomalies', 0.158), ('px', 0.158), ('sp', 0.15), ('tsybakov', 0.14), ('covel', 0.138), ('teinwart', 0.138), ('etection', 0.126), ('nomaly', 0.126), ('ramework', 0.126), ('sh', 0.116), ('ush', 0.116), ('risk', 0.114), ('measurable', 0.106), ('density', 0.103), ('anomalous', 0.095), ('svm', 0.092), ('destination', 0.088), ('assertion', 0.085), ('excess', 0.079), ('lassification', 0.079), ('hcp', 0.076), ('steinwart', 0.066), ('fn', 0.066), ('alarm', 0.066), ('sign', 0.064), ('hd', 0.063), ('offset', 0.063), ('novelty', 0.06), ('grid', 0.056), ('mass', 0.056), ('proposition', 0.053), ('polonik', 0.053), ('bytes', 0.05), ('hayton', 0.05), ('markou', 0.05), ('sawitzki', 0.05), ('sq', 0.05), ('tarassenko', 0.05), ('theiler', 0.05), ('regular', 0.049), ('plan', 0.047), ('absolutely', 0.044), ('dp', 0.043), ('king', 0.042), ('mammen', 0.042), ('samples', 0.042), ('universal', 0.041), ('conditional', 0.04), ('exponent', 0.039), ('session', 0.039), ('ft', 0.039), ('dx', 0.038), ('classi', 0.038), ('dpx', 0.038), ('gov', 0.038), ('hartigan', 0.038), ('ingo', 0.038), ('jet', 0.038), ('lanl', 0.038), ('interpreting', 0.037), ('tr', 0.037), ('measure', 0.035), ('unlabeled', 0.035), ('tl', 0.034), ('en', 0.034), ('ep', 0.033), ('concentration', 0.032), ('level', 0.03), ('windows', 0.029), ('ex', 0.029), ('consistency', 0.029), ('empirical', 0.028), ('fan', 0.028), ('intrusion', 0.028), ('parzen', 0.028), ('sessions', 0.028), ('traf', 0.028), ('furthermore', 0.028), ('corollary', 0.027), ('validation', 0.027), ('curves', 0.027), ('ller', 0.026), ('determines', 0.026), ('heuristic', 0.026), ('alamos', 0.025), ('announced', 0.025), ('cuevas', 0.025), ('cybersecurity', 0.025), ('desforges', 0.025), ('gonz', 0.025), ('lez', 0.025), ('manikopoulos', 0.025), ('nairac', 0.025), ('packet', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="3-tfidf-1" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of ﬁnding level sets for the data generating density. We interpret this learning problem as a binary classiﬁcation problem and compare the corresponding classiﬁcation risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classiﬁcation risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justiﬁcation for the well-known heuristic of artiﬁcially sampling “labeled” samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM. Keywords: unsupervised learning, anomaly detection, density levels, classiﬁcation, SVMs</p><p>2 0.076587476 <a title="3-tfidf-2" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>Author: Aapo Hyvärinen</p><p>Abstract: One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data. Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence</p><p>3 0.074405946 <a title="3-tfidf-3" href="./jmlr-2005-Convergence_Theorems_for_Generalized_Alternating_Minimization_Procedures.html">23 jmlr-2005-Convergence Theorems for Generalized Alternating Minimization Procedures</a></p>
<p>Author: Asela Gunawardana, William Byrne</p><p>Abstract: The EM algorithm is widely used to develop iterative parameter estimation procedures for statistical models. In cases where these procedures strictly follow the EM formulation, the convergence properties of the estimation procedures are well understood. In some instances there are practical reasons to develop procedures that do not strictly fall within the EM framework. We study EM variants in which the E-step is not performed exactly, either to obtain improved rates of convergence, or due to approximations needed to compute statistics under a model family over which E-steps cannot be realized. Since these variants are not EM procedures, the standard (G)EM convergence results do not apply to them. We present an information geometric framework for describing such algorithms and analyzing their convergence properties. We apply this framework to analyze the convergence properties of incremental EM and variational EM. For incremental EM, we discuss conditions under these algorithms converge in likelihood. For variational EM, we show how the E-step approximation prevents convergence to local maxima in likelihood. Keywords: EM, variational EM, incremental EM, convergence, information geometry</p><p>4 0.071198076 <a title="3-tfidf-4" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<p>Author: Weng-Keen Wong, Andrew Moore, Gregory Cooper, Michael Wagner</p><p>Abstract: Traditional biosurveillance algorithms detect disease outbreaks by looking for peaks in a univariate time series of health-care data. Current health-care surveillance data, however, are no longer simply univariate data streams. Instead, a wealth of spatial, temporal, demographic and symptomatic information is available. We present an early disease outbreak detection algorithm called What’s Strange About Recent Events (WSARE), which uses a multivariate approach to improve its timeliness of detection. WSARE employs a rule-based technique that compares recent health-care data against data from a baseline distribution and ﬁnds subgroups of the recent data whose proportions have changed the most from the baseline data. In addition, health-care data also pose difﬁculties for surveillance algorithms because of inherent temporal trends such as seasonal effects and day of week variations. WSARE approaches this problem using a Bayesian network to produce a baseline distribution that accounts for these temporal trends. The algorithm itself incorporates a wide range of ideas, including association rules, Bayesian networks, hypothesis testing and permutation tests to produce a detection algorithm that is careful to evaluate the signiﬁcance of the alarms that it raises. Keywords: anomaly detection, syndromic surveillance, biosurveillance, Bayesian networks, applications</p><p>5 0.066645555 <a title="3-tfidf-5" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>Author: Joseph F. Murray, Gordon F. Hughes, Kenneth Kreutz-Delgado</p><p>Abstract: We compare machine learning methods applied to a difﬁcult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classiﬁer (mi-NB) which is speciﬁcally designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum signiﬁcance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures. Keywords: hard drive failure prediction, rank-sum test, support vector machines (SVM), exact nonparametric statistics, multiple instance naive-Bayes</p><p>6 0.060046822 <a title="3-tfidf-6" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>7 0.05763571 <a title="3-tfidf-7" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>8 0.054264855 <a title="3-tfidf-8" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>9 0.04618784 <a title="3-tfidf-9" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>10 0.045611884 <a title="3-tfidf-10" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>11 0.041701552 <a title="3-tfidf-11" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>12 0.040340614 <a title="3-tfidf-12" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>13 0.039891109 <a title="3-tfidf-13" href="./jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines.html">73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</a></p>
<p>14 0.036283188 <a title="3-tfidf-14" href="./jmlr-2005-Asymptotics_in_Empirical_Risk_Minimization.html">16 jmlr-2005-Asymptotics in Empirical Risk Minimization</a></p>
<p>15 0.035630517 <a title="3-tfidf-15" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>16 0.034844317 <a title="3-tfidf-16" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>17 0.034315035 <a title="3-tfidf-17" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>18 0.034249853 <a title="3-tfidf-18" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>19 0.034083635 <a title="3-tfidf-19" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>20 0.032683335 <a title="3-tfidf-20" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.196), (1, 0.08), (2, 0.019), (3, -0.035), (4, 0.058), (5, -0.015), (6, -0.008), (7, -0.37), (8, -0.194), (9, -0.057), (10, -0.189), (11, -0.065), (12, -0.071), (13, -0.194), (14, -0.093), (15, -0.01), (16, -0.07), (17, 0.045), (18, -0.01), (19, -0.08), (20, -0.147), (21, 0.034), (22, -0.113), (23, -0.032), (24, -0.082), (25, -0.009), (26, -0.15), (27, -0.021), (28, 0.069), (29, 0.085), (30, 0.036), (31, 0.032), (32, -0.123), (33, -0.038), (34, 0.001), (35, -0.109), (36, -0.072), (37, 0.029), (38, -0.016), (39, -0.111), (40, 0.091), (41, -0.252), (42, -0.106), (43, -0.034), (44, -0.113), (45, -0.133), (46, -0.107), (47, -0.32), (48, 0.002), (49, -0.288)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95146388 <a title="3-lsi-1" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of ﬁnding level sets for the data generating density. We interpret this learning problem as a binary classiﬁcation problem and compare the corresponding classiﬁcation risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classiﬁcation risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justiﬁcation for the well-known heuristic of artiﬁcially sampling “labeled” samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM. Keywords: unsupervised learning, anomaly detection, density levels, classiﬁcation, SVMs</p><p>2 0.28146866 <a title="3-lsi-2" href="./jmlr-2005-Convergence_Theorems_for_Generalized_Alternating_Minimization_Procedures.html">23 jmlr-2005-Convergence Theorems for Generalized Alternating Minimization Procedures</a></p>
<p>Author: Asela Gunawardana, William Byrne</p><p>Abstract: The EM algorithm is widely used to develop iterative parameter estimation procedures for statistical models. In cases where these procedures strictly follow the EM formulation, the convergence properties of the estimation procedures are well understood. In some instances there are practical reasons to develop procedures that do not strictly fall within the EM framework. We study EM variants in which the E-step is not performed exactly, either to obtain improved rates of convergence, or due to approximations needed to compute statistics under a model family over which E-steps cannot be realized. Since these variants are not EM procedures, the standard (G)EM convergence results do not apply to them. We present an information geometric framework for describing such algorithms and analyzing their convergence properties. We apply this framework to analyze the convergence properties of incremental EM and variational EM. For incremental EM, we discuss conditions under these algorithms converge in likelihood. For variational EM, we show how the E-step approximation prevents convergence to local maxima in likelihood. Keywords: EM, variational EM, incremental EM, convergence, information geometry</p><p>3 0.26117566 <a title="3-lsi-3" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>Author: Ivor W. Tsang, James T. Kwok, Pak-Ming Cheung</p><p>Abstract: O(m3 ) Standard SVM training has time and O(m2 ) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such “approximateness” in this paper. We ﬁrst show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efﬁcient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and realworld data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about ﬁve million training patterns, in only 1.4 seconds on a 3.2GHz Pentium–4 PC. Keywords: kernel methods, approximation algorithm, minimum enclosing ball, core set, scalability</p><p>4 0.23949853 <a title="3-lsi-4" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>Author: Cheng Soon Ong, Alexander J. Smola, Robert C. Williamson</p><p>Abstract: This paper addresses the problem of choosing a kernel suitable for estimation with a support vector machine, hence further automating machine learning. This goal is achieved by deﬁning a reproducing kernel Hilbert space on the space of kernels itself. Such a formulation leads to a statistical estimation problem similar to the problem of minimizing a regularized risk functional. We state the equivalent representer theorem for the choice of kernels and present a semideﬁnite programming formulation of the resulting optimization problem. Several recipes for constructing hyperkernels are provided, as well as the details of common machine learning problems. Experimental results for classiﬁcation, regression and novelty detection on UCI data show the feasibility of our approach. Keywords: learning the kernel, capacity control, kernel methods, support vector machines, representer theorem, semideﬁnite programming</p><p>5 0.2343553 <a title="3-lsi-5" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<p>Author: Weng-Keen Wong, Andrew Moore, Gregory Cooper, Michael Wagner</p><p>Abstract: Traditional biosurveillance algorithms detect disease outbreaks by looking for peaks in a univariate time series of health-care data. Current health-care surveillance data, however, are no longer simply univariate data streams. Instead, a wealth of spatial, temporal, demographic and symptomatic information is available. We present an early disease outbreak detection algorithm called What’s Strange About Recent Events (WSARE), which uses a multivariate approach to improve its timeliness of detection. WSARE employs a rule-based technique that compares recent health-care data against data from a baseline distribution and ﬁnds subgroups of the recent data whose proportions have changed the most from the baseline data. In addition, health-care data also pose difﬁculties for surveillance algorithms because of inherent temporal trends such as seasonal effects and day of week variations. WSARE approaches this problem using a Bayesian network to produce a baseline distribution that accounts for these temporal trends. The algorithm itself incorporates a wide range of ideas, including association rules, Bayesian networks, hypothesis testing and permutation tests to produce a detection algorithm that is careful to evaluate the signiﬁcance of the alarms that it raises. Keywords: anomaly detection, syndromic surveillance, biosurveillance, Bayesian networks, applications</p><p>6 0.21298641 <a title="3-lsi-6" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>7 0.21158741 <a title="3-lsi-7" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>8 0.18545221 <a title="3-lsi-8" href="./jmlr-2005-Asymptotics_in_Empirical_Risk_Minimization.html">16 jmlr-2005-Asymptotics in Empirical Risk Minimization</a></p>
<p>9 0.18526343 <a title="3-lsi-9" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>10 0.17431366 <a title="3-lsi-10" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>11 0.16815002 <a title="3-lsi-11" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>12 0.15641285 <a title="3-lsi-12" href="./jmlr-2005-Multiclass_Boosting_for_Weak_Classifiers.html">57 jmlr-2005-Multiclass Boosting for Weak Classifiers</a></p>
<p>13 0.15201381 <a title="3-lsi-13" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>14 0.15070547 <a title="3-lsi-14" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>15 0.13928394 <a title="3-lsi-15" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>16 0.13799319 <a title="3-lsi-16" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>17 0.13701035 <a title="3-lsi-17" href="./jmlr-2005-Local_Propagation_in_Conditional_Gaussian_Bayesian_Networks.html">51 jmlr-2005-Local Propagation in Conditional Gaussian Bayesian Networks</a></p>
<p>18 0.13294597 <a title="3-lsi-18" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>19 0.13137582 <a title="3-lsi-19" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>20 0.12965196 <a title="3-lsi-20" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.427), (13, 0.021), (17, 0.035), (19, 0.027), (36, 0.038), (37, 0.034), (43, 0.05), (47, 0.023), (52, 0.073), (59, 0.027), (70, 0.035), (88, 0.083), (90, 0.028), (94, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.63510722 <a title="3-lda-1" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of ﬁnding level sets for the data generating density. We interpret this learning problem as a binary classiﬁcation problem and compare the corresponding classiﬁcation risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classiﬁcation risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justiﬁcation for the well-known heuristic of artiﬁcially sampling “labeled” samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM. Keywords: unsupervised learning, anomaly detection, density levels, classiﬁcation, SVMs</p><p>2 0.30325183 <a title="3-lda-2" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>Author: Cheng Soon Ong, Alexander J. Smola, Robert C. Williamson</p><p>Abstract: This paper addresses the problem of choosing a kernel suitable for estimation with a support vector machine, hence further automating machine learning. This goal is achieved by deﬁning a reproducing kernel Hilbert space on the space of kernels itself. Such a formulation leads to a statistical estimation problem similar to the problem of minimizing a regularized risk functional. We state the equivalent representer theorem for the choice of kernels and present a semideﬁnite programming formulation of the resulting optimization problem. Several recipes for constructing hyperkernels are provided, as well as the details of common machine learning problems. Experimental results for classiﬁcation, regression and novelty detection on UCI data show the feasibility of our approach. Keywords: learning the kernel, capacity control, kernel methods, support vector machines, representer theorem, semideﬁnite programming</p><p>3 0.29875922 <a title="3-lda-3" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>Author: John Winn, Christopher M. Bishop</p><p>Abstract: Bayesian inference is now widely established as one of the principal foundations for machine learning. In practice, exact inference is rarely possible, and so a variety of approximation techniques have been developed, one of the most widely used being a deterministic framework called variational inference. In this paper we introduce Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to Bayesian Networks. Like belief propagation, VMP proceeds by sending messages between nodes in the network and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing additional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational message passing has been implemented in the form of a general purpose inference engine called VIBES (‘Variational Inference for BayEsian networkS’) which allows models to be speciﬁed graphically and then solved variationally without recourse to coding. Keywords: Bayesian networks, variational inference, message passing</p><p>4 0.29599014 <a title="3-lda-4" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>Author: Antoine Bordes, Seyda Ertekin, Jason Weston, Léon Bottou</p><p>Abstract: Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efﬁcient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We ﬁrst present an online SVM algorithm based on this premise. LASVM yields competitive misclassiﬁcation rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.</p><p>5 0.29537264 <a title="3-lda-5" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>Author: Aapo Hyvärinen</p><p>Abstract: One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data. Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence</p><p>6 0.29489136 <a title="3-lda-6" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>7 0.29470089 <a title="3-lda-7" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>8 0.29419604 <a title="3-lda-8" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>9 0.29414722 <a title="3-lda-9" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>10 0.29257408 <a title="3-lda-10" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>11 0.28860435 <a title="3-lda-11" href="./jmlr-2005-Clustering_with_Bregman_Divergences.html">20 jmlr-2005-Clustering with Bregman Divergences</a></p>
<p>12 0.28844291 <a title="3-lda-12" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>13 0.28800881 <a title="3-lda-13" href="./jmlr-2005-Learning_the_Kernel_Function_via_Regularization.html">48 jmlr-2005-Learning the Kernel Function via Regularization</a></p>
<p>14 0.28734013 <a title="3-lda-14" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>15 0.28687397 <a title="3-lda-15" href="./jmlr-2005-Frames%2C_Reproducing_Kernels%2C_Regularization_and_Learning.html">35 jmlr-2005-Frames, Reproducing Kernels, Regularization and Learning</a></p>
<p>16 0.28680798 <a title="3-lda-16" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>17 0.2833904 <a title="3-lda-17" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>18 0.28333658 <a title="3-lda-18" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<p>19 0.28008795 <a title="3-lda-19" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>20 0.27814439 <a title="3-lda-20" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
