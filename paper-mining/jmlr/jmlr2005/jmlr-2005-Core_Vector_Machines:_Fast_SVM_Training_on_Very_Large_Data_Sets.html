<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-24" href="#">jmlr2005-24</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</h1>
<br/><p>Source: <a title="jmlr-2005-24-pdf" href="http://jmlr.org/papers/volume6/tsang05a/tsang05a.pdf">pdf</a></p><p>Author: Ivor W. Tsang, James T. Kwok, Pak-Ming Cheung</p><p>Abstract: O(m3 ) Standard SVM training has time and O(m2 ) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such “approximateness” in this paper. We ﬁrst show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efﬁcient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and realworld data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about ﬁve million training patterns, in only 1.4 seconds on a 3.2GHz Pentium–4 PC. Keywords: kernel methods, approximation algorithm, minimum enclosing ball, core set, scalability</p><p>Reference: <a title="jmlr-2005-24-reference" href="../jmlr2005_reference/jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such “approximateness” in this paper. [sent-11, score-0.103]
</p><p>2 We ﬁrst show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. [sent-12, score-0.173]
</p><p>3 Then, by adopting an efﬁcient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. [sent-13, score-0.222]
</p><p>4 For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about ﬁve million training patterns, in only 1. [sent-16, score-0.196]
</p><p>5 Keywords: kernel methods, approximation algorithm, minimum enclosing ball, core set, scalability  1. [sent-19, score-0.338]
</p><p>6 T SANG , K WOK AND C HEUNG  Another approach to scale up kernel methods is by chunking (Vapnik, 1998) or more sophisticated decomposition methods (Chang and Lin, 2004; Osuna et al. [sent-31, score-0.095]
</p><p>7 (1997b) suggested optimizing only a ﬁxed-size subset (working set) of the training data each time, while the variables corresponding to the other patterns are frozen. [sent-36, score-0.144]
</p><p>8 o In practice, state-of-the-art SVM implementations typically have a training time complexity that scales between O(m) and O(m2. [sent-58, score-0.103]
</p><p>9 In this paper, we will utilize an approximation algorithm for the minimum enclosing ball (MEB) problem in computational geometry. [sent-78, score-0.132]
</p><p>10 The MEB problem computes the ball of minimum radius enclosing a given set of points (or, more generally, balls). [sent-79, score-0.159]
</p><p>11 Consequently, recent attention has shifted to the development of approximation algorithms (B˘ doiu a and Clarkson, 2002; Kumar et al. [sent-83, score-0.093]
</p><p>12 In particular, a breakthrough was obtained by B˘ doiu and Clarkson (2002), who showed that an (1 + ε)-approximation of the a MEB can be efﬁciently obtained using core sets. [sent-85, score-0.315]
</p><p>13 Generally speaking, in an optimization problem, a core set is a subset of input points such that we can get a good approximation to the original input by solving the optimization problem directly on the core set. [sent-86, score-0.444]
</p><p>14 A surprising property of (B˘ doiu and a Clarkson, 2002) is that the size of its core set can be shown to be independent of both d and the size of the point set. [sent-87, score-0.315]
</p><p>15 Inspired from the core set-based approximate MEB algorithms, we will then develop an approximation algorithm for SVM training that has an approximation ratio of (1 + ε)2 . [sent-89, score-0.263]
</p><p>16 The core set in CVM plays a similar role as the working set in decomposition algorithms, which will be reviewed brieﬂy in Section 5. [sent-96, score-0.248]
</p><p>17 , xm }, where each xi ∈ Rd , the minimum enclosing ball of S (denoted MEB(S )) is the smallest ball that contains all the points in S . [sent-104, score-0.189]
</p><p>18 The MEB problem can be dated back as early as in 1857, when Sylvester (1857) ﬁrst investigated the smallest radius disk enclosing m points on the plane. [sent-105, score-0.102]
</p><p>19 In many shape ﬁtting problems, it is found that solving the problem on a subset, called the core set, Q of points from S can often give an accurate and efﬁcient approximation. [sent-116, score-0.222]
</p><p>20 More formally, a subset Q ⊆ S is a core set of S if an expansion by a factor (1 + ε) of its MEB contains S , i. [sent-117, score-0.222]
</p><p>21 They used a simple iterative scheme: At the tth iteration, the current estimate B(ct , rt ) is expanded incrementally by including the furthest point outside the (1 + ε)-ball B(ct , (1 + ε)rt ). [sent-123, score-0.172]
</p><p>22 Despite its simplicity, a surprising property is that the number of iterations, and hence the size of the ﬁnal core set, depends only on ε but not on d or m. [sent-125, score-0.222]
</p><p>23 Another heuristic approach for scaling up the soft-margin SVDD using core sets has also been proposed in (Chu et al. [sent-140, score-0.245]
</p><p>24 All the patterns are then mapped to a sphere in the feature space. [sent-167, score-0.103]
</p><p>25 1 O NE -C LASS L2-SVM Given a set of unlabeled patterns {zi }m where zi only has the input part xi , the one-class L2-SVM i=1 separates outliers from the normal data by solving the primal problem: minw,ρ,ξi  w  2  m  − 2ρ +C ∑ ξ2 i i=1  s. [sent-196, score-0.184]
</p><p>26 Again, we can recover m  m  i=1  i=1  w = ∑ αi yi ϕ(xi ), b = ∑ αi yi , ξi =  αi , C  (14)  from the optimal α and then ρ = yi (w ϕ(xi ) + b) + αi from any support vector zi . [sent-224, score-0.163]
</p><p>27 Equating the primal (12) and dual (13), we have w  2  m  + b2 − 2ρ +C ∑ ξ2 = − i i=1  m  ∑ αi α j  yi y j k(xi , x j ) + yi y j +  i, j=1  δi j C  . [sent-226, score-0.107]
</p><p>28 Thus, this two-class L2-SVM can also be viewed as a MEB problem (1) in which ϕ is ˜ replaced by ϕ, with   yi ϕ(xi )  yi ˜ ϕ(zi ) =  1 √ ei C for any training point zi . [sent-229, score-0.136]
</p><p>29 Moreover, all the support vectors of this L2-SVM, including those deﬁning the margin and those that are misclassiﬁed, now reside on the surface of the ball in the feature space induced by ˜ k. [sent-231, score-0.102]
</p><p>30 As mentioned in Section 2, the idea is to incrementally expand the ball by including the point furthest away from the current center. [sent-241, score-0.154]
</p><p>31 In the following, we denote the core set, the ball’s center and radius at the tth iteration by St , ct and Rt respectively. [sent-242, score-0.397]
</p><p>32 In the sequel, points that are added to the core set will be called core vectors. [sent-255, score-0.444]
</p><p>33 370  C ORE V ECTOR M ACHINES  We start with an arbitrary point z ∈ S and ﬁnd za ∈ S that is furthest away from z in the feature ˜ ˜ space F . [sent-267, score-0.164]
</p><p>34 Then, we ﬁnd another point zb ∈ S that is furthest away from za in F . [sent-268, score-0.231]
</p><p>35 The initial core set ˜ ) has center c0 = 1 (ϕ(za ) + ϕ(zb )) On ˜ ˜ is then set to be S0 = {za , zb }. [sent-269, score-0.289]
</p><p>36 The initial radius is R0 = = =  1 ˜ ˜ ϕ(za ) − ϕ(zb ) 2 1 ˜ ˜ ϕ(za ) 2 + ϕ(zb ) 2 1 ˜ ˜ 2κ − 2k(za , zb ). [sent-271, score-0.094]
</p><p>37 2  2 − 2ϕ(z ) ˜ a  ˜ ϕ(zb )  In a classiﬁcation problem, one may further require za and zb to come from different classes. [sent-272, score-0.134]
</p><p>38 On using c = ∑zi ∈St αi ϕ(zi ) in (3), we have ˜ ct − ϕ(z )  2  =  ∑  zi ,z j ∈St  ˜ αi α j k(zi , z j ) − 2  ˜ ˜ ∑ αi k(zi , z ) + k(z , z ). [sent-279, score-0.134]
</p><p>39 The idea is to randomly o sample a sufﬁciently large subset S from S , and then take the point in S that is furthest away from ct as the approximate furthest point over S . [sent-286, score-0.26]
</p><p>40 95 among the furthest 5% of points from the whole S . [sent-288, score-0.104]
</p><p>41 Our step 3, however, takes a greedy approach by including the point furthest away from the current center. [sent-298, score-0.097]
</p><p>42 2), we have arg  max  z ∈B(ct ,(1+ε)Rt ) /  ˜ ct − ϕ(z )  2  = arg  min  ∑ αi yi y (k(xi , x ) + 1)  min  y (w ϕ(x ) + b),  z ∈B(ct ,(1+ε)Rt ) z ∈S / i  = arg  z ∈B(ct ,(1+ε)Rt ) /  t  (20)  on using (14) and (17). [sent-304, score-0.108]
</p><p>43 As the size |St | of the core set is much smaller than m in practice (Section 6), the computational complexity of each QP sub-problem is much lower than solving the whole QP. [sent-314, score-0.248]
</p><p>44 Besides, as only one core vector is added at each iteration, efﬁcient rank-one update procedures (Cauwenberghs and Poggio, 2001; Vishwanathan et al. [sent-315, score-0.222]
</p><p>45 As will be demonstrated in Section 6, the size of the core set is usually small to medium even for very large data sets. [sent-318, score-0.222]
</p><p>46 But as the number of core vectors increases in each iteration and the training set 372  C ORE V ECTOR M ACHINES  size is ﬁnite, so CVM must terminate in a ﬁnite number (say, τ) of iterations, With ε = 0, MEB(Sτ ) ˜ is an enclosing ball for all the (ϕ-transformed) points on termination. [sent-325, score-0.443]
</p><p>47 It can be shown that when CVM terminates, all the training patterns also satisfy similar loose KKT conditions. [sent-343, score-0.144]
</p><p>48 Its α is zero (by initialization) and ˜ ct − ϕ(z )  2  =  ∑  αi α j yi y j k(xi , x j ) + yi y j +  zi ,z j ∈St  −2  ∑ αi  yi y k(xi , x ) + yi y +  zi ∈St  ˜ = ρt + κ − 2y (wt ϕ(x ) + bt ),  δi j C  δi C  ˜ + k(z , z ) (23)  on using (14), (15), (17) and (18). [sent-351, score-0.275]
</p><p>49 Recall that all the αi ’s (except those of the two initial core vectors) are initialized to zero. [sent-357, score-0.222]
</p><p>50 In other words, the a total number of iterations, and consequently the size of the ﬁnal core set, are of τ = O(1/ε). [sent-373, score-0.222]
</p><p>51 In practice, it has often been observed that the size of the core set is much smaller than this worstcase theoretical upper bound3 (Kumar et al. [sent-374, score-0.222]
</p><p>52 As only one core vector is added at each iteration, |St | = t + 2. [sent-376, score-0.222]
</p><p>53 As the m training patterns may be stored outside the core memory, the O(m) space required will be ignored in the following. [sent-382, score-0.366]
</p><p>54 Since only the core vectors are involved in the QP, the space complexity for the tth iteration is O(|St |2 ). [sent-383, score-0.285]
</p><p>55 As probabilistic speeedup may not ﬁnd the furthest point in each iteration, τ may be larger than 2/ε though it can still be bounded by O(1/ε2 ) (B˘ doiu et al. [sent-390, score-0.171]
</p><p>56 Related Work The core set in CVM plays a similar role as the working set in other decomposition algorithms, and so these algorithms will be reviewed brieﬂy in this Section. [sent-402, score-0.248]
</p><p>57 , 1997b), the working set will be denoted B while the remaining subset of training patterns denoted N. [sent-404, score-0.144]
</p><p>58 Support vectors in the chunk are retained while non-support vectors are replaced by patterns in N violating the KKT conditions. [sent-407, score-0.126]
</p><p>59 At each iteration, variables corresponding to patterns in N are frozen, while those in B are optimized in a QP sub-problem. [sent-412, score-0.103]
</p><p>60 Moreover, while decomposition algorithms allow training patterns to join and leave the working set multiple times, patterns once recruited as core vectors by the CVM will remain there for the whole training process. [sent-431, score-0.562]
</p><p>61 Besides, as in LIBSVM, our CVM uses caching (with the same cache size as in the other LIBSVM implementations above) and stores all the training patterns in main memory. [sent-455, score-0.206]
</p><p>62 1 Checkerboard Data We ﬁrst experiment on the 4 × 4 checkerboard data (Figure 2) commonly used for evaluating large-scale SVM implementations (Lee and Mangasarian, 2001; Mangasarian and Musicant, 2001b; Schwaighofer and Tresp, 2001). [sent-491, score-0.104]
</p><p>63 In particular, one million patterns can be processed in under 13 seconds. [sent-512, score-0.124]
</p><p>64 Figure 3(b) also shows the core set size, which can be seen to be small and its curve basically overlaps with that of the CVM. [sent-515, score-0.222]
</p><p>65 Thus, almost all the core vectors are useful support vectors. [sent-516, score-0.267]
</p><p>66 Note that when the training set is small, more training patterns bring in additional information useful for classiﬁcation and so the number of core vectors increases with training set size. [sent-538, score-0.448]
</p><p>67 With hindsight, one might simply sample 100K training patterns and hope to obtain comparable results. [sent-540, score-0.144]
</p><p>68 Time for reading the training patterns into main memory is not included. [sent-549, score-0.144]
</p><p>69 In fact, we tried both LIBSVM implementations on a random sample of 100K training patterns, but their testing accuracies are inferior to that of CVM. [sent-558, score-0.123]
</p><p>70 Figure 3: Results on the checkerboard data set (Except for the CVM, the other implementations have to terminate early because of not enough memory and/or the training time is too long). [sent-567, score-0.168]
</p><p>71 380  5  x 10  C ORE V ECTOR M ACHINES  266, 079 patterns while the extended test set has (359 + 264) × 112 = 753, 83 patterns. [sent-574, score-0.103]
</p><p>72 They then stabilize at around 30K patterns and CVM becomes faster than the other decomposition algorithms. [sent-579, score-0.129]
</p><p>73 4 Extended MIT Face Data In this Section, we perform face detection using an extended version of the MIT face database10 (Heisele et al. [sent-581, score-0.165]
</p><p>74 The original data set has 6,977 training images (with 2,429 faces and 4,548 nonfaces) and 24,045 test images (472 faces and 23,573 nonfaces). [sent-583, score-0.133]
</p><p>75 , 1997b), additional nonfaces are extracting from images that do not contain faces (e. [sent-588, score-0.105]
</p><p>76 Set A: This is obtained by adding 477,366 nonfaces to the original training set, with the nonface images extracted from 100 photos randomly collected from the web. [sent-595, score-0.1]
</p><p>77 Set B: Each training face is blurred by the arithmetic mean ﬁlter (with window sizes 2 × 2, 3 × 3 and 4 × 4, respectively) and added to set A. [sent-597, score-0.099]
</p><p>78 Recall that the intent of this experiment is on studying the scaling behavior rather than on obtaining state-of-the-art face detection performance. [sent-605, score-0.13]
</p><p>79 Nevertheless, the ability of CVM in handling very large data sets could make it a better base classiﬁer in powerful face detection systems such as the boosted cascade (Viola and Jones, 2001). [sent-606, score-0.107]
</p><p>80 training set original set A set B set C  # faces 2,429 2,429 19,432 408,072  # nonfaces 4,548 481,914 481,914 481,914  total 6,977 484,343 501,346 889,986  Table 2: Number of faces and nonfaces in the face detection data sets. [sent-607, score-0.358]
</p><p>81 Figure 5: Results on the extended USPS digits data set (Except for the CVM, the other implementations have to terminate early because of not enough memory and/or the training time is too long). [sent-623, score-0.126]
</p><p>82 Here, TP =  positives correctly classiﬁed , total positives  TN =  negatives correctly classiﬁed , total negatives  are the true positive and true negative rates respectively. [sent-633, score-0.11]
</p><p>83 The ROC on using CVM is shown in Figure 6, which demonstrates the usefulness of using extra faces and nonfaces in training. [sent-634, score-0.105]
</p><p>84 Even in this non-asymptotic case, the CVM still signiﬁcantly outperforms both LIBSVM implementations in terms of training time and number of support vectors, while the values of AUC and bal are again very competitive. [sent-636, score-0.183]
</p><p>85 Note also that the LIBSVM implementations of both L1- and L2-SVMs do not perform well (in terms of bal ) on the highly imbalanced set A. [sent-637, score-0.125]
</p><p>86 , 1996) to reduce the number of patterns in SVM training. [sent-654, score-0.103]
</p><p>87 Moreover, as the whole data set is stored in the core in our current implementation, we use a 3. [sent-679, score-0.248]
</p><p>88 However, as this data set is relatively small, more training patterns do carry more classiﬁcation information. [sent-694, score-0.144]
</p><p>89 2, the number of iterations, the core set size and consequently the CPU time all increase with the number of training patterns. [sent-696, score-0.263]
</p><p>90 From another perspective, recall that the worst case core set size is 2/ε, independent of m (Section 4. [sent-697, score-0.222]
</p><p>91 Besides, although we have seen that the actual size of the core set is often much smaller than this worst case value, however, when m 2/ε, the number of core vectors can still be dependent on m. [sent-700, score-0.444]
</p><p>92 1% 1% 5% active learning CB-SVM CVM  # training patterns input to SVM 47 515 4,917 49,204 245,364 747 4,090 4,898,431  # test errors 25,713 25,030 25,531 25,700 25,587 21,634 20,938 19,513  SVM training other processing time (in sec) time (in sec) 0. [sent-723, score-0.208]
</p><p>93 042  # core vectors 55  # support vectors 20  Table 4: More performance measures of CVM on the KDDCUP-99 intrusion detection data. [sent-747, score-0.36]
</p><p>94 Figure 9: Results on the UCI adult data set (The other implementations have to terminate early because of not enough memory and/or the training time is too long). [sent-771, score-0.126]
</p><p>95 We formulate kernel methods (including the soft-margin one-class and two-class SVMs) as equivalent MEB problems, and then obtain approximately optimal solutions efﬁciently with the use of core sets. [sent-775, score-0.263]
</p><p>96 The iterative recruitment of core vectors is also similar to incremental procedures (Cauwenberghs and Poggio, 2001; Fung and Mangasarian, 2002), and this connection will be further explored. [sent-789, score-0.222]
</p><p>97 Besides, although the CVM can obtain much fewer support vectors than standard SVM implementations on large data sets, the number of support vectors may still be too large for real-time testing. [sent-790, score-0.152]
</p><p>98 As the core vectors in CVM are added incrementally and never removed, it is thus possible that some of them might be redundant. [sent-791, score-0.222]
</p><p>99 Finally, all the training patterns are currently stored in the main memory. [sent-793, score-0.144]
</p><p>100 Training support vector machines: an application to face detection. [sent-1015, score-0.103]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cvm', 0.631), ('meb', 0.378), ('libsvm', 0.23), ('core', 0.222), ('svm', 0.181), ('simplesvm', 0.127), ('heung', 0.126), ('rsvm', 0.126), ('sang', 0.126), ('wok', 0.126), ('qp', 0.12), ('patterns', 0.103), ('achines', 0.099), ('ector', 0.099), ('doiu', 0.093), ('cpu', 0.088), ('ore', 0.088), ('ct', 0.085), ('furthest', 0.078), ('enclosing', 0.075), ('kkt', 0.068), ('clarkson', 0.067), ('za', 0.067), ('zb', 0.067), ('st', 0.063), ('implementations', 0.062), ('mangasarian', 0.06), ('nonfaces', 0.059), ('face', 0.058), ('ball', 0.057), ('pentium', 0.056), ('rt', 0.056), ('svdd', 0.05), ('osuna', 0.05), ('auc', 0.049), ('zi', 0.049), ('detection', 0.049), ('faces', 0.046), ('support', 0.045), ('intrusion', 0.044), ('smo', 0.043), ('checkerboard', 0.042), ('vishwanathan', 0.042), ('training', 0.041), ('kernel', 0.041), ('speedup', 0.039), ('tth', 0.038), ('kumar', 0.038), ('bal', 0.035), ('collobert', 0.035), ('tp', 0.035), ('roc', 0.034), ('complexities', 0.034), ('lsvm', 0.034), ('negatives', 0.034), ('pavlov', 0.034), ('yu', 0.032), ('primal', 0.032), ('tsang', 0.031), ('dual', 0.029), ('seconds', 0.029), ('imbalanced', 0.028), ('chunking', 0.028), ('sch', 0.027), ('platt', 0.027), ('radius', 0.027), ('besides', 0.027), ('whole', 0.026), ('decomposition', 0.026), ('sv', 0.026), ('approximateness', 0.025), ('bakir', 0.025), ('fp', 0.025), ('ivor', 0.025), ('rmeb', 0.025), ('forest', 0.025), ('kong', 0.025), ('iteration', 0.025), ('lkopf', 0.024), ('terminate', 0.023), ('violating', 0.023), ('active', 0.023), ('mining', 0.023), ('yi', 0.023), ('fung', 0.023), ('hong', 0.023), ('scaling', 0.023), ('chang', 0.022), ('musicant', 0.021), ('positives', 0.021), ('million', 0.021), ('tm', 0.021), ('ram', 0.021), ('smola', 0.021), ('tresp', 0.021), ('testing', 0.02), ('lee', 0.019), ('away', 0.019), ('kao', 0.019), ('cauwenberghs', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="24-tfidf-1" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>Author: Ivor W. Tsang, James T. Kwok, Pak-Ming Cheung</p><p>Abstract: O(m3 ) Standard SVM training has time and O(m2 ) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such “approximateness” in this paper. We ﬁrst show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efﬁcient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and realworld data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about ﬁve million training patterns, in only 1.4 seconds on a 3.2GHz Pentium–4 PC. Keywords: kernel methods, approximation algorithm, minimum enclosing ball, core set, scalability</p><p>2 0.094146386 <a title="24-tfidf-2" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>Author: Antoine Bordes, Seyda Ertekin, Jason Weston, Léon Bottou</p><p>Abstract: Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efﬁcient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We ﬁrst present an online SVM algorithm based on this premise. LASVM yields competitive misclassiﬁcation rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.</p><p>3 0.074948601 <a title="24-tfidf-3" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>Author: S. Sathiya Keerthi, Dennis DeCoste</p><p>Abstract: This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classiﬁcation. This is done by modifying the ﬁnite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight , SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modiﬁed Huber’s loss function and the L1 loss function, and also for solving ordinal regression. Keywords: linear SVMs, classiﬁcation, conjugate gradient</p><p>4 0.073697902 <a title="24-tfidf-4" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>Author: Fabio Aiolli, Alessandro Sperduti</p><p>Abstract: Winner-take-all multiclass classiﬁers are built on the top of a set of prototypes each representing one of the available classes. A pattern is then classiﬁed with the label associated to the most ‘similar’ prototype. Recent proposal of SVM extensions to multiclass can be considered instances of the same strategy with one prototype per class. The multi-prototype SVM proposed in this paper extends multiclass SVM to multiple prototypes per class. It allows to combine several vectors in a principled way to obtain large margin decision functions. For this problem, we give a compact constrained quadratic formulation and we propose a greedy optimization algorithm able to ﬁnd locally optimal solutions for the non convex objective function. This algorithm proceeds by reducing the overall problem into a series of simpler convex problems. For the solution of these reduced problems an efﬁcient optimization algorithm is proposed. A number of pattern selection strategies are then discussed to speed-up the optimization process. In addition, given the combinatorial nature of the overall problem, stochastic search strategies are suggested to escape from local minima which are not globally optimal. Finally, we report experiments on a number of datasets. The performance obtained using few simple linear prototypes is comparable to that obtained by state-of-the-art kernel-based methods but with a signiﬁcant reduction (of one or two orders) in response time. Keywords: multiclass classiﬁcation, multi-prototype support vector machines, kernel machines, stochastic search optimization, large margin classiﬁers</p><p>5 0.054264855 <a title="24-tfidf-5" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of ﬁnding level sets for the data generating density. We interpret this learning problem as a binary classiﬁcation problem and compare the corresponding classiﬁcation risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classiﬁcation risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justiﬁcation for the well-known heuristic of artiﬁcially sampling “labeled” samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM. Keywords: unsupervised learning, anomaly detection, density levels, classiﬁcation, SVMs</p><p>6 0.047377873 <a title="24-tfidf-6" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>7 0.0464187 <a title="24-tfidf-7" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>8 0.043871768 <a title="24-tfidf-8" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>9 0.042172089 <a title="24-tfidf-9" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>10 0.041901611 <a title="24-tfidf-10" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>11 0.039913118 <a title="24-tfidf-11" href="./jmlr-2005-Generalization_Bounds_for_the_Area_Under_the_ROC_Curve.html">38 jmlr-2005-Generalization Bounds for the Area Under the ROC Curve</a></p>
<p>12 0.03871543 <a title="24-tfidf-12" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>13 0.038579445 <a title="24-tfidf-13" href="./jmlr-2005-Learning_with_Decision_Lists_of_Data-Dependent_Features.html">50 jmlr-2005-Learning with Decision Lists of Data-Dependent Features</a></p>
<p>14 0.030918468 <a title="24-tfidf-14" href="./jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines.html">73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</a></p>
<p>15 0.028293135 <a title="24-tfidf-15" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>16 0.02760404 <a title="24-tfidf-16" href="./jmlr-2005-Stability_of_Randomized_Learning_Algorithms.html">67 jmlr-2005-Stability of Randomized Learning Algorithms</a></p>
<p>17 0.027271666 <a title="24-tfidf-17" href="./jmlr-2005-Multiclass_Boosting_for_Weak_Classifiers.html">57 jmlr-2005-Multiclass Boosting for Weak Classifiers</a></p>
<p>18 0.027058074 <a title="24-tfidf-18" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>19 0.026128653 <a title="24-tfidf-19" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>20 0.024879064 <a title="24-tfidf-20" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.166), (1, 0.072), (2, 0.19), (3, -0.048), (4, 0.056), (5, -0.155), (6, 0.111), (7, -0.022), (8, -0.246), (9, -0.12), (10, -0.006), (11, -0.031), (12, -0.128), (13, -0.042), (14, 0.05), (15, 0.216), (16, -0.232), (17, 0.1), (18, -0.038), (19, -0.091), (20, -0.041), (21, -0.071), (22, 0.071), (23, 0.194), (24, 0.155), (25, 0.079), (26, 0.063), (27, -0.017), (28, -0.003), (29, -0.073), (30, -0.011), (31, 0.126), (32, 0.055), (33, -0.053), (34, 0.076), (35, -0.218), (36, -0.037), (37, -0.096), (38, -0.218), (39, 0.126), (40, -0.199), (41, -0.115), (42, -0.096), (43, -0.016), (44, 0.071), (45, -0.238), (46, 0.192), (47, -0.037), (48, -0.071), (49, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93613058 <a title="24-lsi-1" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>Author: Ivor W. Tsang, James T. Kwok, Pak-Ming Cheung</p><p>Abstract: O(m3 ) Standard SVM training has time and O(m2 ) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such “approximateness” in this paper. We ﬁrst show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efﬁcient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and realworld data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about ﬁve million training patterns, in only 1.4 seconds on a 3.2GHz Pentium–4 PC. Keywords: kernel methods, approximation algorithm, minimum enclosing ball, core set, scalability</p><p>2 0.43791503 <a title="24-lsi-2" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>Author: Antoine Bordes, Seyda Ertekin, Jason Weston, Léon Bottou</p><p>Abstract: Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efﬁcient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We ﬁrst present an online SVM algorithm based on this premise. LASVM yields competitive misclassiﬁcation rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.</p><p>3 0.29416847 <a title="24-lsi-3" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>Author: Fabio Aiolli, Alessandro Sperduti</p><p>Abstract: Winner-take-all multiclass classiﬁers are built on the top of a set of prototypes each representing one of the available classes. A pattern is then classiﬁed with the label associated to the most ‘similar’ prototype. Recent proposal of SVM extensions to multiclass can be considered instances of the same strategy with one prototype per class. The multi-prototype SVM proposed in this paper extends multiclass SVM to multiple prototypes per class. It allows to combine several vectors in a principled way to obtain large margin decision functions. For this problem, we give a compact constrained quadratic formulation and we propose a greedy optimization algorithm able to ﬁnd locally optimal solutions for the non convex objective function. This algorithm proceeds by reducing the overall problem into a series of simpler convex problems. For the solution of these reduced problems an efﬁcient optimization algorithm is proposed. A number of pattern selection strategies are then discussed to speed-up the optimization process. In addition, given the combinatorial nature of the overall problem, stochastic search strategies are suggested to escape from local minima which are not globally optimal. Finally, we report experiments on a number of datasets. The performance obtained using few simple linear prototypes is comparable to that obtained by state-of-the-art kernel-based methods but with a signiﬁcant reduction (of one or two orders) in response time. Keywords: multiclass classiﬁcation, multi-prototype support vector machines, kernel machines, stochastic search optimization, large margin classiﬁers</p><p>4 0.28324988 <a title="24-lsi-4" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>Author: S. Sathiya Keerthi, Dennis DeCoste</p><p>Abstract: This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classiﬁcation. This is done by modifying the ﬁnite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight , SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modiﬁed Huber’s loss function and the L1 loss function, and also for solving ordinal regression. Keywords: linear SVMs, classiﬁcation, conjugate gradient</p><p>5 0.26036188 <a title="24-lsi-5" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of ﬁnding level sets for the data generating density. We interpret this learning problem as a binary classiﬁcation problem and compare the corresponding classiﬁcation risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classiﬁcation risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justiﬁcation for the well-known heuristic of artiﬁcially sampling “labeled” samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM. Keywords: unsupervised learning, anomaly detection, density levels, classiﬁcation, SVMs</p><p>6 0.25258654 <a title="24-lsi-6" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>7 0.21324793 <a title="24-lsi-7" href="./jmlr-2005-Learning_with_Decision_Lists_of_Data-Dependent_Features.html">50 jmlr-2005-Learning with Decision Lists of Data-Dependent Features</a></p>
<p>8 0.16701841 <a title="24-lsi-8" href="./jmlr-2005-Generalization_Bounds_for_the_Area_Under_the_ROC_Curve.html">38 jmlr-2005-Generalization Bounds for the Area Under the ROC Curve</a></p>
<p>9 0.14631791 <a title="24-lsi-9" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>10 0.13687919 <a title="24-lsi-10" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>11 0.13617352 <a title="24-lsi-11" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>12 0.12504777 <a title="24-lsi-12" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>13 0.1184842 <a title="24-lsi-13" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>14 0.11658721 <a title="24-lsi-14" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>15 0.11613274 <a title="24-lsi-15" href="./jmlr-2005-Change_Point_Problems_in_Linear_Dynamical_Systems.html">17 jmlr-2005-Change Point Problems in Linear Dynamical Systems</a></p>
<p>16 0.11593724 <a title="24-lsi-16" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>17 0.11539123 <a title="24-lsi-17" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>18 0.10958175 <a title="24-lsi-18" href="./jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines.html">73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</a></p>
<p>19 0.1004113 <a title="24-lsi-19" href="./jmlr-2005-Prioritization_Methods_for_Accelerating_MDP_Solvers.html">61 jmlr-2005-Prioritization Methods for Accelerating MDP Solvers</a></p>
<p>20 0.097179033 <a title="24-lsi-20" href="./jmlr-2005-On_the_Nystr%C3%B6m_Method_for_Approximating_a_Gram_Matrix_for_Improved_Kernel-Based_Learning.html">60 jmlr-2005-On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.022), (17, 0.024), (19, 0.025), (36, 0.03), (37, 0.038), (42, 0.018), (43, 0.026), (47, 0.066), (52, 0.096), (59, 0.014), (70, 0.022), (80, 0.011), (88, 0.063), (90, 0.014), (94, 0.059), (96, 0.387)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70080781 <a title="24-lda-1" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>Author: Ivor W. Tsang, James T. Kwok, Pak-Ming Cheung</p><p>Abstract: O(m3 ) Standard SVM training has time and O(m2 ) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such “approximateness” in this paper. We ﬁrst show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efﬁcient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and realworld data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about ﬁve million training patterns, in only 1.4 seconds on a 3.2GHz Pentium–4 PC. Keywords: kernel methods, approximation algorithm, minimum enclosing ball, core set, scalability</p><p>2 0.38322985 <a title="24-lda-2" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>Author: Antoine Bordes, Seyda Ertekin, Jason Weston, Léon Bottou</p><p>Abstract: Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efﬁcient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We ﬁrst present an online SVM algorithm based on this premise. LASVM yields competitive misclassiﬁcation rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.</p><p>3 0.34394512 <a title="24-lda-3" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>Author: Marco Cuturi, Kenji Fukumizu, Jean-Philippe Vert</p><p>Abstract: We present a family of positive deﬁnite kernels on measures, characterized by the fact that the value of the kernel between two measures is a function of their sum. These kernels can be used to derive kernels on structured objects, such as images and texts, by representing these objects as sets of components, such as pixels or words, or more generally as measures on the space of components. Several kernels studied in this work make use of common quantities deﬁned on measures such as entropy or generalized variance to detect similarities. Given an a priori kernel on the space of components itself, the approach is further extended by restating the previous results in a more efﬁcient and ﬂexible framework using the “kernel trick”. Finally, a constructive approach to such positive deﬁnite kernels through an integral representation theorem is proved, before presenting experimental results on a benchmark experiment of handwritten digits classiﬁcation to illustrate the validity of the approach. Keywords: kernels on measures, semigroup theory, Jensen divergence, generalized variance, reproducing kernel Hilbert space</p><p>4 0.33612531 <a title="24-lda-4" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>Author: Fabio Aiolli, Alessandro Sperduti</p><p>Abstract: Winner-take-all multiclass classiﬁers are built on the top of a set of prototypes each representing one of the available classes. A pattern is then classiﬁed with the label associated to the most ‘similar’ prototype. Recent proposal of SVM extensions to multiclass can be considered instances of the same strategy with one prototype per class. The multi-prototype SVM proposed in this paper extends multiclass SVM to multiple prototypes per class. It allows to combine several vectors in a principled way to obtain large margin decision functions. For this problem, we give a compact constrained quadratic formulation and we propose a greedy optimization algorithm able to ﬁnd locally optimal solutions for the non convex objective function. This algorithm proceeds by reducing the overall problem into a series of simpler convex problems. For the solution of these reduced problems an efﬁcient optimization algorithm is proposed. A number of pattern selection strategies are then discussed to speed-up the optimization process. In addition, given the combinatorial nature of the overall problem, stochastic search strategies are suggested to escape from local minima which are not globally optimal. Finally, we report experiments on a number of datasets. The performance obtained using few simple linear prototypes is comparable to that obtained by state-of-the-art kernel-based methods but with a signiﬁcant reduction (of one or two orders) in response time. Keywords: multiclass classiﬁcation, multi-prototype support vector machines, kernel machines, stochastic search optimization, large margin classiﬁers</p><p>5 0.32337809 <a title="24-lda-5" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>Author: S. Sathiya Keerthi, Dennis DeCoste</p><p>Abstract: This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classiﬁcation. This is done by modifying the ﬁnite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight , SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modiﬁed Huber’s loss function and the L1 loss function, and also for solving ordinal regression. Keywords: linear SVMs, classiﬁcation, conjugate gradient</p><p>6 0.32286128 <a title="24-lda-6" href="./jmlr-2005-Efficient_Margin_Maximizing_with_Boosting.html">29 jmlr-2005-Efficient Margin Maximizing with Boosting</a></p>
<p>7 0.32240993 <a title="24-lda-7" href="./jmlr-2005-A_Generalization_Error_for_Q-Learning.html">5 jmlr-2005-A Generalization Error for Q-Learning</a></p>
<p>8 0.3203522 <a title="24-lda-8" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>9 0.31668442 <a title="24-lda-9" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>10 0.31644645 <a title="24-lda-10" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>11 0.31510875 <a title="24-lda-11" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>12 0.3105557 <a title="24-lda-12" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>13 0.30875897 <a title="24-lda-13" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>14 0.30683807 <a title="24-lda-14" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>15 0.30536181 <a title="24-lda-15" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>16 0.30487335 <a title="24-lda-16" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>17 0.30299217 <a title="24-lda-17" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>18 0.30185589 <a title="24-lda-18" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>19 0.30014464 <a title="24-lda-19" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>20 0.2993823 <a title="24-lda-20" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
