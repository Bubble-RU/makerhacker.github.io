<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>16 jmlr-2005-Asymptotics in Empirical Risk Minimization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-16" href="#">jmlr2005-16</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>16 jmlr-2005-Asymptotics in Empirical Risk Minimization</h1>
<br/><p>Source: <a title="jmlr-2005-16-pdf" href="http://jmlr.org/papers/volume6/mohammadi05a/mohammadi05a.pdf">pdf</a></p><p>Author: Leila Mohammadi, Sara van de Geer</p><p>Abstract: In this paper, we study a two-category classiﬁcation problem. We indicate the categories by labels Y = 1 and Y = −1. We observe a covariate, or feature, X ∈ X ⊂ Rd . Consider a collection {ha } of classiﬁers indexed by a ﬁnite-dimensional parameter a, and the classiﬁer ha∗ that minimizes the prediction error over this class. The parameter a∗ is estimated by the empirical risk minimizer an over the class, where the empirical risk is calculated on a training sample of size n. We apply ˆ the Kim Pollard Theorem to show that under certain differentiability assumptions, an converges to ˆ a∗ with rate n−1/3 , and also present the asymptotic distribution of the renormalized estimator. For example, let V0 denote the set of x on which, given X = x, the label Y = 1 is more likely (than the label Y = −1). If X is one-dimensional, the set V0 is the union of disjoint intervals. The problem is then to estimate the thresholds of the intervals. We obtain the asymptotic distribution of the empirical risk minimizer when the classiﬁers have K thresholds, where K is ﬁxed. We furthermore consider an extension to higher-dimensional X, assuming basically that V0 has a smooth boundary in some given parametric class. We also discuss various rates of convergence when the differentiability conditions are possibly violated. Here, we again restrict ourselves to one-dimensional X. We show that the rate is n−1 in certain cases, and then also obtain the asymptotic distribution for the empirical prediction error. Keywords: asymptotic distribution, classiﬁcation theory, estimation error, nonparametric models, threshold-based classiﬁers</p><p>Reference: <a title="jmlr-2005-16-reference" href="../jmlr2005_reference/jmlr-2005-Asymptotics_in_Empirical_Risk_Minimization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 NL  EURANDOM Post Ofﬁce Box 513 5600 MB Eindhoven The Netherlands  Sara van de Geer  GEER @ STAT. [sent-3, score-0.118]
</p><p>2 The parameter a∗ is estimated by the empirical risk minimizer an over the class, where the empirical risk is calculated on a training sample of size n. [sent-10, score-0.238]
</p><p>3 We apply ˆ the Kim Pollard Theorem to show that under certain differentiability assumptions, an converges to ˆ a∗ with rate n−1/3 , and also present the asymptotic distribution of the renormalized estimator. [sent-11, score-0.26]
</p><p>4 The problem is then to estimate the thresholds of the intervals. [sent-14, score-0.066]
</p><p>5 We obtain the asymptotic distribution of the empirical risk minimizer when the classiﬁers have K thresholds, where K is ﬁxed. [sent-15, score-0.221]
</p><p>6 We furthermore consider an extension to higher-dimensional X, assuming basically that V0 has a smooth boundary in some given parametric class. [sent-16, score-0.07]
</p><p>7 We also discuss various rates of convergence when the differentiability conditions are possibly violated. [sent-17, score-0.153]
</p><p>8 We show that the rate is n−1 in certain cases, and then also obtain the asymptotic distribution for the empirical prediction error. [sent-19, score-0.11]
</p><p>9 Keywords: asymptotic distribution, classiﬁcation theory, estimation error, nonparametric models, threshold-based classiﬁers  1. [sent-20, score-0.063]
</p><p>10 Let the training set (X1 ,Y1 ), · · · , (Xn ,Yn ) consist of n independent copies of the couple (X,Y ) with distribution P, where X ∈ X ⊂ Rd is called a feature and Y ∈ {−1, 1} is the label of X. [sent-24, score-0.076]
</p><p>11 Following Vapnik (2000) and Vapnik (1998), we c 2005 Leila Mohammadi and Sara van de Geer. [sent-27, score-0.118]
</p><p>12 M OHAMMADI AND VAN DE G EER  consider the empirical counterpart of the risk which is the number of misclassiﬁed examples, i. [sent-28, score-0.08]
</p><p>13 We will study empirical risk minimization over a model class H of classiﬁers h. [sent-32, score-0.08]
</p><p>14 (2005), Koltchinskii (2003a), Koltchinskii (2003b), Mohammadi (2004) and Tsybakov and van de Geer (2005). [sent-43, score-0.118]
</p><p>15 Under regularity assumptions, one can establish rates, as well as the asymptotic distributions. [sent-48, score-0.091]
</p><p>16 In Section 2, we generalize the problem considered in Mohammadi and van de Geer (2003). [sent-52, score-0.118]
</p><p>17 It gives an application of the cube root asymptotics derived by Kim and Pollard (1990). [sent-53, score-0.126]
</p><p>18 2028  A SYMPTOTICS IN E MPIRICAL R ISK M INIMIZATION  A simple case, with just one threshold, has been presented in Mohammadi and van de Geer (2003). [sent-65, score-0.118]
</p><p>19 We will establish the asymptotic behavior of estimators of the thresholds, using the set of classiﬁers with K thresholds as model class. [sent-66, score-0.151]
</p><p>20 Here K is ﬁxed, and not bigger than, but not necessarily equal to, the number of thresholds of Bayes classiﬁer. [sent-67, score-0.066]
</p><p>21 We let X = (U,V ), with U ∈ Rd−1 and V ∈ R and minimize the empirical classiﬁcation error over the classiﬁers ha (u, v) := 2 {ka (u) ≥ v} − 1,    where a is an r-dimensional parameter and ka : Rd−1 → R is some given smooth function of a. [sent-74, score-0.941]
</p><p>22 Under differentiability conditions, this will again lead to cube root asymptotics. [sent-75, score-0.17]
</p><p>23 In Section 3, we study various other rates, and also the asymptotic distribution in the case of a (1/n)-rate. [sent-76, score-0.063]
</p><p>24 For example, in Corollary 2 the asymptotic distribution of the prediction error follows as a corollary. [sent-82, score-0.063]
</p><p>25 The conclusion is that by considering some assumptions on the distribution of the data, we can prove rates of convergence and asymptotic distributions. [sent-83, score-0.092]
</p><p>26 The results of the present paper give more insight in the dependency of the asymptotic behavior on the underlying distribution. [sent-85, score-0.063]
</p><p>27 We consider asymptotics as n → ∞, regarding the sample (X1 ,Y1 ), . [sent-86, score-0.057]
</p><p>28 2029  M OHAMMADI AND VAN DE G EER  Deﬁne the empirical risk Ln (a) := Pn (ha (X) = Y ),  (3)  L(a) := P(ha (X) = Y ). [sent-110, score-0.08]
</p><p>29 (4)  and the theoretical risk Moreover, let an = arg min Ln (a) ˆ a∈A  be the empirical risk minimizer, and let a∗ = arg min L(a) a∈A  be its theoretical counterpart. [sent-111, score-0.311]
</p><p>30 ˆ ˆ  (6)  Under regularity conditions L(a) − L(a∗ ) behaves like the squared distance a − a∗ 2 . [sent-120, score-0.096]
</p><p>31 Moreover, √ again under regularity conditions, the right hand side of (6) behaves in probability like σ(an )/ n, ˆ where σ(a) is the standard deviation of [νn (a) − νn (a∗ )]. [sent-121, score-0.073]
</p><p>32 Due to the fact that we are dealing with indicator functions, the standard deviation of [νn (a) − νn (a∗ )] behaves like the square root a − a∗ 1/2 of the distance between a and a∗ . [sent-122, score-0.072]
</p><p>33 Let us continue with a rough sketch of the arguments used for establishing the asymptotic distribution. [sent-125, score-0.063]
</p><p>34 We may write 1  2  an = arg min n 6 νn (a) − νn (a∗ ) + n 3 L(a) − L(a∗ ) . [sent-126, score-0.086]
</p><p>35 ˆ a  When we already have the n−1/3 -rate, it is convenient to renormalize to 1  1  1  2  1  ˆ n 3 (an − a∗ ) = arg min n 6 νn (a∗ + n− 3 t) − νn (a∗ ) + n 3 L(a∗ + n− 3 t) − L(a∗ ) . [sent-127, score-0.086]
</p><p>36 t  Now, under differentiability assumptions, 2  1  n 3 L(a∗ + n− 3 t) − L(a∗ ) ≈ t T V t/2, 2030  A SYMPTOTICS IN E MPIRICAL R ISK M INIMIZATION  1  where V is the matrix of second derivatives of L at a∗ . [sent-128, score-0.101]
</p><p>37 We call the locations of the K 1 sign changes thresholds. [sent-141, score-0.107]
</p><p>38 With a sign change we mean that the function has strictly opposite sign in sufﬁciently small intervals to the left and right side of each threshold. [sent-142, score-0.106]
</p><p>39 The boundary points a0 = 0 0 and a0 0 +1 = 1 are thus not considered as locations of a sign change. [sent-143, score-0.113]
</p><p>40 K+1  ha (x) :=  ∑ bk  k=1     Let for a ∈ UK  (7)  {ak−1 ≤ x < ak },  where a0 = 0, aK+1 = 1 and b1 = −1, bk+1 = −bk , k = 2, . [sent-151, score-0.618]
</p><p>41 (9)  Let The empirical risk minimizer is an := arg min Ln (a). [sent-157, score-0.244]
</p><p>42 ˆ a∈UK  (10)  We emphasize that we take the number of thresholds K in our model class ﬁxed. [sent-158, score-0.066]
</p><p>43 With a consistent estimator K in our model class, the asymptotics presented in this paper generally still go through. [sent-162, score-0.057]
</p><p>44 e The following theorem states that an converges to the minimizer a∗ of L(a) with rate n−1/3 ˆ and also provides its asymptotic distribution after renormalization. [sent-169, score-0.257]
</p><p>45 If K = K0 , one can show that when the minimizer a∗ is unique, it is equal to a0 , i. [sent-171, score-0.078]
</p><p>46 , a∗ ) := arg min L(a), 1 2 K  (11)  a∈UK  is the unique minimizer of L(a), that a∗ is in the interior of UK , and that L(a) is a continuous function of a. [sent-181, score-0.237]
</p><p>47 Suppose that F0 has non-zero derivative f0 in a neighborhood of a∗ , k = 1, . [sent-182, score-0.061]
</p><p>48 , K, where g, the density of G, is continuous in a neighborhood of a∗ . [sent-189, score-0.061]
</p><p>49 The theorem therefore also provides us the rate n−2/3 for the convergence of the prediction error L(an ) of the classiﬁer han , to the prediction error of ha∗ , and the asymptotic distribution ˆ ˆ of the prediction error L(an ) after renormalization. [sent-215, score-0.166]
</p><p>50 We present this asymptotic distribution in a ˆ corollary. [sent-216, score-0.063]
</p><p>51 Recall that one of the conditions in the above theorem is that L has a unique minimizer in the interior of UK . [sent-219, score-0.217]
</p><p>52 Note that 2 1 2 1 L(a) = P(Y = 1, ha (X) = −1) + P(Y = −1, ha (X) = 1) =  Z a 0  =  Z a 0  F0 dG +  Z 1 a  (2F0 − 1)dG +  (1 − F0 )dG  Z 1 0  (1 − F0 )dG. [sent-223, score-0.856]
</p><p>53 If a10 (2F0 − 1)dG > 0, then a∗ = a0 is the unique minimizer of L. [sent-224, score-0.104]
</p><p>54 The minimizer is not in the open interval (0, 1), and Theorem 1 indeed fails. [sent-226, score-0.078]
</p><p>55 Consider given functions ka : Rd−1 → R, a ∈ A , and classiﬁers    ha = 2 {Ca } − 1, a ∈ A , 2033  M OHAMMADI AND VAN DE G EER  where Ca := {(u, v) : v ≤ ka (u)}, a ∈ A . [sent-231, score-1.412]
</p><p>56 A famous example is when ka is linear in a, see for instance Hastie et al. [sent-234, score-0.492]
</p><p>57 Tsybakov and van de Geer (2005) consider this case for large r, depending on n. [sent-236, score-0.118]
</p><p>58 Let again a∗ = arg min L(a), a  be the minimizer of the theoretical risk L(a), and an = arg min Ln (a) ˆ a  be the empirical risk minimizer. [sent-238, score-0.389]
</p><p>59 We would like to know the asymptotic distribution of an . [sent-239, score-0.063]
</p><p>60 We also suppose that ka is a regular function of the parameter a ∈ Rr , i. [sent-243, score-0.492]
</p><p>61 , the gradient ∂ ka (u) = ka (u) (13) ∂a of ka (u) exists for all u, and also its Hessian ∂2 ka (u) = ka (u). [sent-245, score-2.46]
</p><p>62 It is called locally dominated integrable with respect to the measure µ and variable a if for each a there is a neighborhood I of a and a nonnegative µ-integrable function g1 such that for all u ∈ U and b ∈ I, | fb (u)| ≤ g1 (u). [sent-249, score-0.124]
</p><p>63 The probability of misclassiﬁcation using the classiﬁer ha is L(a) = P(ha (X) = Y ) = =  Z  Ca  Z  Ca  (1 − F0 )dG +  Z  c Ca  F0 dG  (1 − 2F0 )dG + P(Y = 1). [sent-250, score-0.428]
</p><p>64 ∂v  2034  (15)  A SYMPTOTICS IN E MPIRICAL R ISK M INIMIZATION  ∂ Assume furthermore that the functions m(u, ka (u))ka (u) and ∂aT [m(u, ka (u))ka (u)] are locally dominated integrable with respect to Lebesgue measure and variable a. [sent-254, score-1.047]
</p><p>65 Also, assume that the funcR tion ka (u)g(u, ka (u))du is uniformly bounded for a in a neighborhood of a∗ , and that for each u, m (u, ka (u)) and ka (u) are continuous in a neighborhood of a∗ . [sent-255, score-2.116]
</p><p>66 ∂a∂aT  Σa (u)m(u, ka (u))du,  where Σa (u) = ka (u)kaT (u)  m (u, ka (u)) + ka (u). [sent-257, score-1.968]
</p><p>67 m(u, ka (u))  (16)  (17)  1  In the following theorem, we show that n 3 (an − a∗ ) converges to the location of the minimum ˆ of some Gaussian process. [sent-258, score-0.539]
</p><p>68 As an example of Theorem 4, suppose r = d and ka is the linear function ka (u) := a1 u1 + . [sent-266, score-0.984]
</p><p>69 Other Rates of Convergence In this section, we will investigate the rates that can occur if we do not assume the differentiability conditions needed for the Kim Pollard Theorem. [sent-281, score-0.153]
</p><p>70 We ﬁrst assume K = 1, and that 2F0 − 1 has at most one sign change (i. [sent-283, score-0.053]
</p><p>71 Now, either 2F0 − 1 changes sign at a∗ ∈ (0, 1) or there are no sign changes in (0, 1), i. [sent-289, score-0.162]
</p><p>72 One easily veriﬁes that a∗ is the minimizer of L(a) over a ∈ [0, 1]. [sent-294, score-0.078]
</p><p>73 Throughout, a neighborhood of a∗ is some set of the form (a∗ − δ, a∗ + δ), δ > 0, intersected with [0, 1]. [sent-299, score-0.061]
</p><p>74 Assumption B: Let there exist c > 0 and ε ≥ 0 such that |1 − 2F0 (x)|g(x) ≥ c|x − a∗ |ε ,  (19)  for all x in a neighborhood of a∗ . [sent-300, score-0.061]
</p><p>75 In Section 2, we assumed differentiability of F0 in a neighborhood of a∗ ∈ (0, 1), with positive derivative f0 . [sent-301, score-0.162]
</p><p>76 This theorem then gives that for large T and large n, P  sup a−a∗ ≤δ2 n  |νn (a) − νn (a∗ )| ≥ C1 T 2 δn  ≤ C exp(−T ). [sent-326, score-0.079]
</p><p>77 Now, by the peeling device, see for example van de Geer (2000), we can show that lim lim sup P √ sup T →∞ n→∞ ∗ a−a  So,  |νn (a) − νn (a∗ )| ≥T a − a∗ >δn  = 0. [sent-327, score-0.19]
</p><p>78 Under the conditions of Theorem 5 with ε = 0, we derive the asymptotic distribution of the renormalized empirical risk, locally in a neighborhood of order 1/n of a∗ , the local empirical risk. [sent-338, score-0.212]
</p><p>79 However, since the local empirical risk has a limit law which has no unique minimum, n(an − a∗ ) generally does not converge in distribution. [sent-340, score-0.106]
</p><p>80 2 Extension to Several Thresholds and Sign Changes Recall that K0 is the number of sign changes of 2F0 − 1, and that K is the number of thresholds in the model class H deﬁned in (8). [sent-392, score-0.147]
</p><p>81 Below, whenever we mention the rate n−1/3 or n−1 , we mean the rate can be obtained under some conditions on F0 and g (see Theorem 1 (where ε = 1), and Theorem 5 with ε = 0). [sent-393, score-0.075]
</p><p>82 Recall that a0 denotes the K0 -vector of the locations of the sign changes of 2F0 − 1. [sent-394, score-0.107]
</p><p>83 Then, K0 of the elements of an converge to a0 , and either a1,n converges ˆ ˆ to 0 or aK,n converges to 1. [sent-401, score-0.094]
</p><p>84 The rate of convergence to the interior points is n−1/3 and the rate of ˆ convergence to the boundary point is n−1 . [sent-402, score-0.133]
</p><p>85 If ˆ K − K0 is odd, one element of an converges to one of the boundary points 0 or 1. [sent-406, score-0.081]
</p><p>86 k  (26)  The envelope GR of this class is deﬁned as GR (·) = sup |φ(·)|. [sent-420, score-0.121]
</p><p>87 If G is VC-subgraph, then a sufﬁcient condition for the class GR to be uniformly manageable is that its envelope function GR is uniformly square integrable for R near zero. [sent-424, score-0.278]
</p><p>88 Theorem 7 ( Kim and Pollard (1990)) Let {an } be a sequence of estimators for which ˆ (i) Ln (an ) ≤ infa∈A Ln (a) + oP (n−2/3 ), ˆ (ii) an converges in probability to the unique a∗ that minimizes L(a), ˆ (iii) a∗ is an interior point of A . [sent-425, score-0.165]
</p><p>89 If Z has non-degenerate increments, then n1/3 (an − a∗ ) converges in distribution to the (almost surely unique) random vector that minimizes ˆ {Z(t) : t ∈ Rr }. [sent-428, score-0.07]
</p><p>90 To check Condition (ii), we note that, because ˆ {φ(·, a) : a ∈ UK } is a uniformly bounded VC-subgraph class, we have the uniform law of large numbers sup |Ln (a) − L(a)| → 0, a. [sent-430, score-0.062]
</p><p>91 To check Condition (iv), for odd i, we have ∂ P(ha (X) = Y ) = (2F0 (ai ) − 1)g(ai ) ∂ai so  ∂2 P(ha (X) = Y ) ∂a2 i  =  2 f0 (ai )g(ai ) + (2F0 (ai ) − 1)g (ai )  ai =a∗ i  ai =a∗ i  = −2 f0 (a∗ )g(a∗ ). [sent-438, score-0.117]
</p><p>92   Now, a∗ minimizes L(a) for a in the interior of UK , so 2F0 − 1 changes sign from negative to positive at a∗ for odd k, and it changes sign from positive to negative at a∗ for even k. [sent-462, score-0.285]
</p><p>93 Finally, by (27) and (28), the limit of τEφ(X,Y, a∗ + s/τ)φ(X,Y, a∗ + t/τ) as τ → ∞ becomes  ∑     K  H(s,t) =  min(sk ,tk )g(a∗ ) (0 < sk ,tk ) k  k=1     − max(sk ,tk )g(a∗ ) (0 > sk ,tk ) . [sent-473, score-0.224]
</p><p>94 Now we calculate an upper bound for the envelope function. [sent-478, score-0.085]
</p><p>95 To maximize the function φ(x, y, a) = (ha (x) = y) − (ha∗ (x) = y)|, note that for y = 1, this function is increasing in ak ’s for even k and decreasing in ak ’s for odd k. [sent-480, score-0.285]
</p><p>96 1 2 3 K  (30)        Similarly, (ha∗ (x) = y) − (ha (x) = y) is maximized for y = 1, in case (30) and for y = −1, it is maximized in case (29). [sent-490, score-0.062]
</p><p>97 1 1 2 2 K K  and  So the envelope GR of GR satisﬁes GR ≤ GR where    GR =  x ∈ ∪K [a∗ − R, a∗ + R] . [sent-498, score-0.085]
</p><p>98 k=1 k k  Now, note that 2 E(GR ) ≤  and  K  ∑ P(a∗ − R ≤ X ≤ a∗ + R) k k  k=1  P(a∗ − R ≤ X ≤ a∗ + R) 2Rg(ak ) k k = < R∗ , ∃R∗ < ∞, R R  for some ak ∈ (a∗ − R, a∗ + R), when R is close to zero. [sent-499, score-0.116]
</p><p>99 Since GR R k k is bounded by one, it is also easy to see that GR is uniformly square integrable for R close to zero. [sent-501, score-0.068]
</p><p>100 Finally, since G is VC-subgraph, we conclude that GR is uniformly manageable for the envelope GR . [sent-502, score-0.154]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ka', 0.492), ('ha', 0.428), ('pollard', 0.237), ('gr', 0.211), ('dg', 0.201), ('rr', 0.18), ('eer', 0.152), ('geer', 0.152), ('ohammadi', 0.152), ('inimization', 0.135), ('isk', 0.135), ('mpirical', 0.135), ('symptotics', 0.135), ('van', 0.118), ('op', 0.118), ('kim', 0.118), ('ak', 0.116), ('sk', 0.112), ('differentiability', 0.101), ('mohammadi', 0.101), ('ln', 0.094), ('envelope', 0.085), ('minimizer', 0.078), ('jn', 0.074), ('bk', 0.074), ('va', 0.068), ('thresholds', 0.066), ('tsybakov', 0.063), ('asymptotic', 0.063), ('uk', 0.063), ('neighborhood', 0.061), ('rd', 0.06), ('risk', 0.059), ('asymptotics', 0.057), ('sign', 0.053), ('odd', 0.053), ('subsection', 0.052), ('koltchinskii', 0.05), ('converges', 0.047), ('interior', 0.047), ('arg', 0.046), ('cc', 0.046), ('hb', 0.046), ('behaves', 0.045), ('theorem', 0.043), ('manageable', 0.043), ('cube', 0.042), ('integrable', 0.042), ('min', 0.04), ('ers', 0.039), ('lugosi', 0.038), ('ur', 0.038), ('poisson', 0.037), ('parametric', 0.036), ('sup', 0.036), ('boundary', 0.034), ('barbour', 0.034), ('eurandom', 0.034), ('han', 0.034), ('leila', 0.034), ('manageability', 0.034), ('ai', 0.032), ('maximized', 0.031), ('iv', 0.031), ('ca', 0.031), ('near', 0.029), ('rates', 0.029), ('changes', 0.028), ('regularity', 0.028), ('copies', 0.028), ('classi', 0.027), ('condition', 0.027), ('root', 0.027), ('union', 0.027), ('uniformly', 0.026), ('lebesgue', 0.026), ('zn', 0.026), ('du', 0.026), ('unique', 0.026), ('rate', 0.026), ('locations', 0.026), ('jump', 0.025), ('vii', 0.025), ('label', 0.025), ('paths', 0.025), ('bayes', 0.024), ('indexed', 0.023), ('ez', 0.023), ('renormalized', 0.023), ('sara', 0.023), ('met', 0.023), ('feature', 0.023), ('conditions', 0.023), ('minimizes', 0.023), ('pn', 0.022), ('differentiable', 0.022), ('estimators', 0.022), ('empirical', 0.021), ('max', 0.021), ('dominated', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="16-tfidf-1" href="./jmlr-2005-Asymptotics_in_Empirical_Risk_Minimization.html">16 jmlr-2005-Asymptotics in Empirical Risk Minimization</a></p>
<p>Author: Leila Mohammadi, Sara van de Geer</p><p>Abstract: In this paper, we study a two-category classiﬁcation problem. We indicate the categories by labels Y = 1 and Y = −1. We observe a covariate, or feature, X ∈ X ⊂ Rd . Consider a collection {ha } of classiﬁers indexed by a ﬁnite-dimensional parameter a, and the classiﬁer ha∗ that minimizes the prediction error over this class. The parameter a∗ is estimated by the empirical risk minimizer an over the class, where the empirical risk is calculated on a training sample of size n. We apply ˆ the Kim Pollard Theorem to show that under certain differentiability assumptions, an converges to ˆ a∗ with rate n−1/3 , and also present the asymptotic distribution of the renormalized estimator. For example, let V0 denote the set of x on which, given X = x, the label Y = 1 is more likely (than the label Y = −1). If X is one-dimensional, the set V0 is the union of disjoint intervals. The problem is then to estimate the thresholds of the intervals. We obtain the asymptotic distribution of the empirical risk minimizer when the classiﬁers have K thresholds, where K is ﬁxed. We furthermore consider an extension to higher-dimensional X, assuming basically that V0 has a smooth boundary in some given parametric class. We also discuss various rates of convergence when the differentiability conditions are possibly violated. Here, we again restrict ourselves to one-dimensional X. We show that the rate is n−1 in certain cases, and then also obtain the asymptotic distribution for the empirical prediction error. Keywords: asymptotic distribution, classiﬁcation theory, estimation error, nonparametric models, threshold-based classiﬁers</p><p>2 0.085309543 <a title="16-tfidf-2" href="./jmlr-2005-Generalization_Bounds_and_Complexities_Based_on_Sparsity_and_Clustering_for_Convex_Combinations_of_Functions_from_Random_Classes.html">37 jmlr-2005-Generalization Bounds and Complexities Based on Sparsity and Clustering for Convex Combinations of Functions from Random Classes</a></p>
<p>Author: Savina Andonova Jaeger</p><p>Abstract: A uniﬁed approach is taken for deriving new generalization data dependent bounds for several classes of algorithms explored in the existing literature by different approaches. This uniﬁed approach is based on an extension of Vapnik’s inequality for VC classes of sets to random classes of sets - that is, classes depending on the random data, invariant under permutation of the data and possessing the increasing property. Generalization bounds are derived for convex combinations of functions from random classes with certain properties. Algorithms, such as SVMs (support vector machines), boosting with decision stumps, radial basis function networks, some hierarchies of kernel machines or convex combinations of indicator functions over sets with ﬁnite VC dimension, generate classiﬁer functions that fall into the above category. We also explore the individual complexities of the classiﬁers, such as sparsity of weights and weighted variance over clusters from the convex combination introduced by Koltchinskii and Panchenko (2004), and show sparsity-type and cluster-variance-type generalization bounds for random classes. Keywords: complexities of classiﬁers, generalization bounds, SVM, voting classiﬁers, random classes</p><p>3 0.068649992 <a title="16-tfidf-3" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>Author: Dmitry Rusakov, Dan Geiger</p><p>Abstract: We develop a closed form asymptotic formula to compute the marginal likelihood of data given a naive Bayesian network model with two hidden states and binary features. This formula deviates from the standard BIC score. Our work provides a concrete example that the BIC score is generally incorrect for statistical models that belong to stratiﬁed exponential families. This claim stands in contrast to linear and curved exponential families, where the BIC score has been proven to provide a correct asymptotic approximation for the marginal likelihood. Keywords: Bayesian networks, asymptotic model selection, Bayesian information criterion (BIC)</p><p>4 0.052436016 <a title="16-tfidf-4" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>Author: Manfred Opper, Ole Winther</p><p>Abstract: We propose a novel framework for approximations to intractable probabilistic models which is based on a free energy formulation. The approximation can be understood as replacing an average over the original intractable distribution with a tractable one. It requires two tractable probability distributions which are made consistent on a set of moments and encode diﬀerent features of the original intractable distribution. In this way we are able to use Gaussian approximations for models with discrete or bounded variables which allow us to include non-trivial correlations. These are neglected in many other methods. We test the framework on toy benchmark problems for binary variables on fully connected graphs and 2D grids and compare with other methods, such as loopy belief propagation. Good performance is already achieved by using single nodes as tractable substructures. Signiﬁcant improvements are obtained when a spanning tree is used instead.</p><p>5 0.036283188 <a title="16-tfidf-5" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of ﬁnding level sets for the data generating density. We interpret this learning problem as a binary classiﬁcation problem and compare the corresponding classiﬁcation risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classiﬁcation risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justiﬁcation for the well-known heuristic of artiﬁcially sampling “labeled” samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM. Keywords: unsupervised learning, anomaly detection, density levels, classiﬁcation, SVMs</p><p>6 0.035779607 <a title="16-tfidf-6" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>7 0.033605974 <a title="16-tfidf-7" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>8 0.033509169 <a title="16-tfidf-8" href="./jmlr-2005-Concentration_Bounds_for_Unigram_Language_Models.html">22 jmlr-2005-Concentration Bounds for Unigram Language Models</a></p>
<p>9 0.031863295 <a title="16-tfidf-9" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>10 0.031565424 <a title="16-tfidf-10" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>11 0.031442311 <a title="16-tfidf-11" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>12 0.030910371 <a title="16-tfidf-12" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>13 0.029131779 <a title="16-tfidf-13" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>14 0.028755836 <a title="16-tfidf-14" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>15 0.027462408 <a title="16-tfidf-15" href="./jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</a></p>
<p>16 0.027402677 <a title="16-tfidf-16" href="./jmlr-2005-Generalization_Bounds_for_the_Area_Under_the_ROC_Curve.html">38 jmlr-2005-Generalization Bounds for the Area Under the ROC Curve</a></p>
<p>17 0.026123725 <a title="16-tfidf-17" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>18 0.026032133 <a title="16-tfidf-18" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>19 0.02558862 <a title="16-tfidf-19" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>20 0.023326661 <a title="16-tfidf-20" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, -0.107), (2, -0.025), (3, 0.168), (4, -0.042), (5, -0.061), (6, -0.025), (7, -0.017), (8, -0.013), (9, -0.084), (10, -0.126), (11, -0.026), (12, -0.12), (13, -0.017), (14, 0.103), (15, -0.02), (16, 0.132), (17, 0.163), (18, -0.091), (19, -0.024), (20, -0.001), (21, 0.074), (22, -0.108), (23, -0.153), (24, -0.321), (25, 0.04), (26, -0.293), (27, 0.017), (28, -0.126), (29, 0.075), (30, -0.133), (31, -0.003), (32, -0.034), (33, -0.2), (34, -0.411), (35, -0.295), (36, -0.05), (37, -0.153), (38, -0.166), (39, -0.058), (40, -0.083), (41, 0.236), (42, 0.144), (43, -0.002), (44, 0.159), (45, 0.008), (46, 0.021), (47, 0.007), (48, -0.022), (49, -0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96440536 <a title="16-lsi-1" href="./jmlr-2005-Asymptotics_in_Empirical_Risk_Minimization.html">16 jmlr-2005-Asymptotics in Empirical Risk Minimization</a></p>
<p>Author: Leila Mohammadi, Sara van de Geer</p><p>Abstract: In this paper, we study a two-category classiﬁcation problem. We indicate the categories by labels Y = 1 and Y = −1. We observe a covariate, or feature, X ∈ X ⊂ Rd . Consider a collection {ha } of classiﬁers indexed by a ﬁnite-dimensional parameter a, and the classiﬁer ha∗ that minimizes the prediction error over this class. The parameter a∗ is estimated by the empirical risk minimizer an over the class, where the empirical risk is calculated on a training sample of size n. We apply ˆ the Kim Pollard Theorem to show that under certain differentiability assumptions, an converges to ˆ a∗ with rate n−1/3 , and also present the asymptotic distribution of the renormalized estimator. For example, let V0 denote the set of x on which, given X = x, the label Y = 1 is more likely (than the label Y = −1). If X is one-dimensional, the set V0 is the union of disjoint intervals. The problem is then to estimate the thresholds of the intervals. We obtain the asymptotic distribution of the empirical risk minimizer when the classiﬁers have K thresholds, where K is ﬁxed. We furthermore consider an extension to higher-dimensional X, assuming basically that V0 has a smooth boundary in some given parametric class. We also discuss various rates of convergence when the differentiability conditions are possibly violated. Here, we again restrict ourselves to one-dimensional X. We show that the rate is n−1 in certain cases, and then also obtain the asymptotic distribution for the empirical prediction error. Keywords: asymptotic distribution, classiﬁcation theory, estimation error, nonparametric models, threshold-based classiﬁers</p><p>2 0.28744772 <a title="16-lsi-2" href="./jmlr-2005-Generalization_Bounds_and_Complexities_Based_on_Sparsity_and_Clustering_for_Convex_Combinations_of_Functions_from_Random_Classes.html">37 jmlr-2005-Generalization Bounds and Complexities Based on Sparsity and Clustering for Convex Combinations of Functions from Random Classes</a></p>
<p>Author: Savina Andonova Jaeger</p><p>Abstract: A uniﬁed approach is taken for deriving new generalization data dependent bounds for several classes of algorithms explored in the existing literature by different approaches. This uniﬁed approach is based on an extension of Vapnik’s inequality for VC classes of sets to random classes of sets - that is, classes depending on the random data, invariant under permutation of the data and possessing the increasing property. Generalization bounds are derived for convex combinations of functions from random classes with certain properties. Algorithms, such as SVMs (support vector machines), boosting with decision stumps, radial basis function networks, some hierarchies of kernel machines or convex combinations of indicator functions over sets with ﬁnite VC dimension, generate classiﬁer functions that fall into the above category. We also explore the individual complexities of the classiﬁers, such as sparsity of weights and weighted variance over clusters from the convex combination introduced by Koltchinskii and Panchenko (2004), and show sparsity-type and cluster-variance-type generalization bounds for random classes. Keywords: complexities of classiﬁers, generalization bounds, SVM, voting classiﬁers, random classes</p><p>3 0.2257897 <a title="16-lsi-3" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>Author: Dmitry Rusakov, Dan Geiger</p><p>Abstract: We develop a closed form asymptotic formula to compute the marginal likelihood of data given a naive Bayesian network model with two hidden states and binary features. This formula deviates from the standard BIC score. Our work provides a concrete example that the BIC score is generally incorrect for statistical models that belong to stratiﬁed exponential families. This claim stands in contrast to linear and curved exponential families, where the BIC score has been proven to provide a correct asymptotic approximation for the marginal likelihood. Keywords: Bayesian networks, asymptotic model selection, Bayesian information criterion (BIC)</p><p>4 0.18568909 <a title="16-lsi-4" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>Author: Manfred Opper, Ole Winther</p><p>Abstract: We propose a novel framework for approximations to intractable probabilistic models which is based on a free energy formulation. The approximation can be understood as replacing an average over the original intractable distribution with a tractable one. It requires two tractable probability distributions which are made consistent on a set of moments and encode diﬀerent features of the original intractable distribution. In this way we are able to use Gaussian approximations for models with discrete or bounded variables which allow us to include non-trivial correlations. These are neglected in many other methods. We test the framework on toy benchmark problems for binary variables on fully connected graphs and 2D grids and compare with other methods, such as loopy belief propagation. Good performance is already achieved by using single nodes as tractable substructures. Signiﬁcant improvements are obtained when a spanning tree is used instead.</p><p>5 0.17926957 <a title="16-lsi-5" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of ﬁnding level sets for the data generating density. We interpret this learning problem as a binary classiﬁcation problem and compare the corresponding classiﬁcation risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classiﬁcation risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justiﬁcation for the well-known heuristic of artiﬁcially sampling “labeled” samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM. Keywords: unsupervised learning, anomaly detection, density levels, classiﬁcation, SVMs</p><p>6 0.1190635 <a title="16-lsi-6" href="./jmlr-2005-Generalization_Bounds_for_the_Area_Under_the_ROC_Curve.html">38 jmlr-2005-Generalization Bounds for the Area Under the ROC Curve</a></p>
<p>7 0.11215633 <a title="16-lsi-7" href="./jmlr-2005-Clustering_with_Bregman_Divergences.html">20 jmlr-2005-Clustering with Bregman Divergences</a></p>
<p>8 0.10897784 <a title="16-lsi-8" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>9 0.10866489 <a title="16-lsi-9" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>10 0.10846742 <a title="16-lsi-10" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>11 0.10538542 <a title="16-lsi-11" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>12 0.1038609 <a title="16-lsi-12" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>13 0.10074594 <a title="16-lsi-13" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>14 0.099706896 <a title="16-lsi-14" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>15 0.097408637 <a title="16-lsi-15" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>16 0.094776057 <a title="16-lsi-16" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>17 0.091513999 <a title="16-lsi-17" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>18 0.089592405 <a title="16-lsi-18" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>19 0.08782924 <a title="16-lsi-19" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>20 0.086077906 <a title="16-lsi-20" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(17, 0.016), (19, 0.014), (37, 0.013), (43, 0.037), (47, 0.012), (52, 0.036), (88, 0.739)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99274677 <a title="16-lda-1" href="./jmlr-2005-Asymptotics_in_Empirical_Risk_Minimization.html">16 jmlr-2005-Asymptotics in Empirical Risk Minimization</a></p>
<p>Author: Leila Mohammadi, Sara van de Geer</p><p>Abstract: In this paper, we study a two-category classiﬁcation problem. We indicate the categories by labels Y = 1 and Y = −1. We observe a covariate, or feature, X ∈ X ⊂ Rd . Consider a collection {ha } of classiﬁers indexed by a ﬁnite-dimensional parameter a, and the classiﬁer ha∗ that minimizes the prediction error over this class. The parameter a∗ is estimated by the empirical risk minimizer an over the class, where the empirical risk is calculated on a training sample of size n. We apply ˆ the Kim Pollard Theorem to show that under certain differentiability assumptions, an converges to ˆ a∗ with rate n−1/3 , and also present the asymptotic distribution of the renormalized estimator. For example, let V0 denote the set of x on which, given X = x, the label Y = 1 is more likely (than the label Y = −1). If X is one-dimensional, the set V0 is the union of disjoint intervals. The problem is then to estimate the thresholds of the intervals. We obtain the asymptotic distribution of the empirical risk minimizer when the classiﬁers have K thresholds, where K is ﬁxed. We furthermore consider an extension to higher-dimensional X, assuming basically that V0 has a smooth boundary in some given parametric class. We also discuss various rates of convergence when the differentiability conditions are possibly violated. Here, we again restrict ourselves to one-dimensional X. We show that the rate is n−1 in certain cases, and then also obtain the asymptotic distribution for the empirical prediction error. Keywords: asymptotic distribution, classiﬁcation theory, estimation error, nonparametric models, threshold-based classiﬁers</p><p>2 0.98624617 <a title="16-lda-2" href="./jmlr-2005-Generalization_Bounds_and_Complexities_Based_on_Sparsity_and_Clustering_for_Convex_Combinations_of_Functions_from_Random_Classes.html">37 jmlr-2005-Generalization Bounds and Complexities Based on Sparsity and Clustering for Convex Combinations of Functions from Random Classes</a></p>
<p>Author: Savina Andonova Jaeger</p><p>Abstract: A uniﬁed approach is taken for deriving new generalization data dependent bounds for several classes of algorithms explored in the existing literature by different approaches. This uniﬁed approach is based on an extension of Vapnik’s inequality for VC classes of sets to random classes of sets - that is, classes depending on the random data, invariant under permutation of the data and possessing the increasing property. Generalization bounds are derived for convex combinations of functions from random classes with certain properties. Algorithms, such as SVMs (support vector machines), boosting with decision stumps, radial basis function networks, some hierarchies of kernel machines or convex combinations of indicator functions over sets with ﬁnite VC dimension, generate classiﬁer functions that fall into the above category. We also explore the individual complexities of the classiﬁers, such as sparsity of weights and weighted variance over clusters from the convex combination introduced by Koltchinskii and Panchenko (2004), and show sparsity-type and cluster-variance-type generalization bounds for random classes. Keywords: complexities of classiﬁers, generalization bounds, SVM, voting classiﬁers, random classes</p><p>3 0.95742482 <a title="16-lda-3" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>Author: Gal Elidan, Nir Friedman</p><p>Abstract: A central challenge in learning probabilistic graphical models is dealing with domains that involve hidden variables. The common approach for learning model parameters in such domains is the expectation maximization (EM) algorithm. This algorithm, however, can easily get trapped in suboptimal local maxima. Learning the model structure is even more challenging. The structural EM algorithm can adapt the structure in the presence of hidden variables, but usually performs poorly without prior knowledge about the cardinality and location of the hidden variables. In this work, we present a general approach for learning Bayesian networks with hidden variables that overcomes these problems. The approach builds on the information bottleneck framework of Tishby et al. (1999). We start by proving formal correspondence between the information bottleneck objective and the standard parametric EM functional. We then use this correspondence to construct a learning algorithm that combines an information-theoretic smoothing term with a continuation procedure. Intuitively, the algorithm bypasses local maxima and achieves superior solutions by following a continuous path from a solution of, an easy and smooth, target function, to a solution of the desired likelihood function. As we show, our algorithmic framework allows learning of the parameters as well as the structure of a network. In addition, it also allows us to introduce new hidden variables during model selection and learn their cardinality. We demonstrate the performance of our procedure on several challenging real-life data sets. Keywords: Bayesian networks, hidden variables, information bottleneck, continuation, variational methods</p><p>4 0.94800574 <a title="16-lda-4" href="./jmlr-2005-On_the_Nystr%C3%B6m_Method_for_Approximating_a_Gram_Matrix_for_Improved_Kernel-Based_Learning.html">60 jmlr-2005-On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning</a></p>
<p>Author: Petros Drineas, Michael W. Mahoney</p><p>Abstract: A problem for many kernel-based methods is that the amount of computation required to ﬁnd the solution scales as O(n3 ), where n is the number of training examples. We develop and analyze an algorithm to compute an easily-interpretable low-rank approximation to an n × n Gram matrix G such that computations of interest may be performed more rapidly. The approximation is of ˜ the form Gk = CWk+CT , where C is a matrix consisting of a small number c of columns of G and Wk is the best rank-k approximation to W , the matrix formed by the intersection between those c columns of G and the corresponding c rows of G. An important aspect of the algorithm is the probability distribution used to randomly sample the columns; we will use a judiciously-chosen and data-dependent nonuniform probability distribution. Let · 2 and · F denote the spectral norm and the Frobenius norm, respectively, of a matrix, and let Gk be the best rank-k approximation to G. We prove that by choosing O(k/ε4 ) columns G −CWk+CT ξ ≤ G − Gk ξ +ε n ∑ G2 , ii i=1 both in expectation and with high probability, for both ξ = 2, F, and for all k : 0 ≤ k ≤ rank(W ). This approximation can be computed using O(n) additional space and time, after making two passes over the data from external storage. The relationships between this algorithm, other related matrix decompositions, and the Nystr¨ m method from integral equation theory are discussed.1 o Keywords: kernel methods, randomized algorithms, Gram matrix, Nystr¨ m method o</p><p>5 0.81046093 <a title="16-lda-5" href="./jmlr-2005-Generalization_Bounds_for_the_Area_Under_the_ROC_Curve.html">38 jmlr-2005-Generalization Bounds for the Area Under the ROC Curve</a></p>
<p>Author: Shivani Agarwal, Thore Graepel, Ralf Herbrich, Sariel Har-Peled, Dan Roth</p><p>Abstract: We study generalization properties of the area under the ROC curve (AUC), a quantity that has been advocated as an evaluation criterion for the bipartite ranking problem. The AUC is a different term than the error rate used for evaluation in classiﬁcation problems; consequently, existing generalization bounds for the classiﬁcation error rate cannot be used to draw conclusions about the AUC. In this paper, we deﬁne the expected accuracy of a ranking function (analogous to the expected error rate of a classiﬁcation function), and derive distribution-free probabilistic bounds on the deviation of the empirical AUC of a ranking function (observed on a ﬁnite data sequence) from its expected accuracy. We derive both a large deviation bound, which serves to bound the expected accuracy of a ranking function in terms of its empirical AUC on a test sequence, and a uniform convergence bound, which serves to bound the expected accuracy of a learned ranking function in terms of its empirical AUC on a training sequence. Our uniform convergence bound is expressed in terms of a new set of combinatorial parameters that we term the bipartite rank-shatter coefﬁcients; these play the same role in our result as do the standard VC-dimension related shatter coefﬁcients (also known as the growth function) in uniform convergence results for the classiﬁcation error rate. A comparison of our result with a recent uniform convergence result derived by Freund et al. (2003) for a quantity closely related to the AUC shows that the bound provided by our result can be considerably tighter. Keywords: generalization bounds, area under the ROC curve, ranking, large deviations, uniform convergence ∗. Parts of the results contained in this paper were presented at the 18th Annual Conference on Neural Information Processing Systems in December, 2004 (Agarwal et al., 2005a) and at the 10th International Workshop on Artiﬁcial Intelligence and Statistics in January, 2005 (Agarwal et al., 2005b). ©2005 Shivan</p><p>6 0.79039997 <a title="16-lda-6" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>7 0.784486 <a title="16-lda-7" href="./jmlr-2005-Clustering_with_Bregman_Divergences.html">20 jmlr-2005-Clustering with Bregman Divergences</a></p>
<p>8 0.76862431 <a title="16-lda-8" href="./jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error.html">13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</a></p>
<p>9 0.76280302 <a title="16-lda-9" href="./jmlr-2005-Convergence_Theorems_for_Generalized_Alternating_Minimization_Procedures.html">23 jmlr-2005-Convergence Theorems for Generalized Alternating Minimization Procedures</a></p>
<p>10 0.75852507 <a title="16-lda-10" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>11 0.75504887 <a title="16-lda-11" href="./jmlr-2005-Learning_the_Kernel_Function_via_Regularization.html">48 jmlr-2005-Learning the Kernel Function via Regularization</a></p>
<p>12 0.74425852 <a title="16-lda-12" href="./jmlr-2005-Stability_of_Randomized_Learning_Algorithms.html">67 jmlr-2005-Stability of Randomized Learning Algorithms</a></p>
<p>13 0.74252641 <a title="16-lda-13" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>14 0.71943992 <a title="16-lda-14" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>15 0.70905477 <a title="16-lda-15" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>16 0.70731217 <a title="16-lda-16" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<p>17 0.70718706 <a title="16-lda-17" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>18 0.69926023 <a title="16-lda-18" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>19 0.69844484 <a title="16-lda-19" href="./jmlr-2005-Concentration_Bounds_for_Unigram_Language_Models.html">22 jmlr-2005-Concentration Bounds for Unigram Language Models</a></p>
<p>20 0.67281806 <a title="16-lda-20" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
