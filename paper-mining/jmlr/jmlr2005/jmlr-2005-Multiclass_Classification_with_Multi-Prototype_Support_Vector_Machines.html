<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-58" href="#">jmlr2005-58</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</h1>
<br/><p>Source: <a title="jmlr-2005-58-pdf" href="http://jmlr.org/papers/volume6/aiolli05a/aiolli05a.pdf">pdf</a></p><p>Author: Fabio Aiolli, Alessandro Sperduti</p><p>Abstract: Winner-take-all multiclass classiﬁers are built on the top of a set of prototypes each representing one of the available classes. A pattern is then classiﬁed with the label associated to the most ‘similar’ prototype. Recent proposal of SVM extensions to multiclass can be considered instances of the same strategy with one prototype per class. The multi-prototype SVM proposed in this paper extends multiclass SVM to multiple prototypes per class. It allows to combine several vectors in a principled way to obtain large margin decision functions. For this problem, we give a compact constrained quadratic formulation and we propose a greedy optimization algorithm able to ﬁnd locally optimal solutions for the non convex objective function. This algorithm proceeds by reducing the overall problem into a series of simpler convex problems. For the solution of these reduced problems an efﬁcient optimization algorithm is proposed. A number of pattern selection strategies are then discussed to speed-up the optimization process. In addition, given the combinatorial nature of the overall problem, stochastic search strategies are suggested to escape from local minima which are not globally optimal. Finally, we report experiments on a number of datasets. The performance obtained using few simple linear prototypes is comparable to that obtained by state-of-the-art kernel-based methods but with a signiﬁcant reduction (of one or two orders) in response time. Keywords: multiclass classiﬁcation, multi-prototype support vector machines, kernel machines, stochastic search optimization, large margin classiﬁers</p><p>Reference: <a title="jmlr-2005-58-reference" href="../jmlr2005_reference/jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Belzoni 7 35131 Padova, Italy  Editor: Yoram Singer  Abstract Winner-take-all multiclass classiﬁers are built on the top of a set of prototypes each representing one of the available classes. [sent-7, score-0.551]
</p><p>2 Recent proposal of SVM extensions to multiclass can be considered instances of the same strategy with one prototype per class. [sent-9, score-0.397]
</p><p>3 The multi-prototype SVM proposed in this paper extends multiclass SVM to multiple prototypes per class. [sent-10, score-0.551]
</p><p>4 The performance obtained using few simple linear prototypes is comparable to that obtained by state-of-the-art kernel-based methods but with a signiﬁcant reduction (of one or two orders) in response time. [sent-18, score-0.381]
</p><p>5 Keywords: multiclass classiﬁcation, multi-prototype support vector machines, kernel machines, stochastic search optimization, large margin classiﬁers  1. [sent-19, score-0.253]
</p><p>6 Introduction In multiclass classiﬁcation, given a set of labelled examples with labels selected from a ﬁnite set, an inductive procedure builds a function that (hopefully) is able to map unseen instances to their appropriate classes. [sent-20, score-0.228]
</p><p>7 In this work, we exclusively focus on the single-label version of the multiclass classiﬁcation problem in which instances are associated with exactly one element of the label set. [sent-21, score-0.205]
</p><p>8 A scoring function f : X × M → R is then deﬁned, measuring the similarity of an element in X with prototypes deﬁned in a space M . [sent-27, score-0.381]
</p><p>9 Another class of methods for multiclass classiﬁcation are the so called prototype based methods, one of the most relevant of which is the learning vector quantization (LVQ) algorithm (Kohonen et al. [sent-34, score-0.397]
</p><p>10 In the simplest case, also known as LVQ1, at each step of the codewords learning, for each input pattern xi , the algorithm ﬁnds the element ck closest to xi . [sent-38, score-0.195]
</p><p>11 If that codeword is associated to a class which is the same as the class of the pattern, then ck is updated by ck ← ck + η(t)(xi − ck ) thus making the prototype get closer to the pattern, otherwise it is updated by ck ← ck − η(t)(xi − ck ) thus making the prototype farther away. [sent-39, score-0.822]
</p><p>12 The approach we propose here gives an alternative method to combine simple predictors together to obtain large margin multiclass classiﬁers. [sent-74, score-0.224]
</p><p>13 First, adding prototypes can produce higher margin decision functions without dramatically increasing the complexity of the generated model. [sent-76, score-0.435]
</p><p>14 This can be trivially shown by considering that the single-prototype margin is a lower bound on the margin for multi-prototype since it can be obtained when all the prototypes of the same class coincide. [sent-77, score-0.489]
</p><p>15 Then, in Section 3 we derive a convex quadratic formulation for the easier problem of learning one prototype per class. [sent-82, score-0.252]
</p><p>16 When multiple prototypes are introduced in Section 4, the problem becomes not convex in general. [sent-84, score-0.406]
</p><p>17 Moreover, three alternative methods are given for this optimization problem and heuristics for the ”smart” selection of patterns in the optimization process are proposed and compared. [sent-86, score-0.214]
</p><p>18 Namely, (Aiolli and Sperduti, 2003) which contains the basic idea and the theory of the multi-prototype SVM together with preliminary experimental work and (Aiolli and Sperduti, 2002a) which proposes and analyzes selection heuristics for the optimization of multiclass SVM. [sent-92, score-0.257]
</p><p>19 i Moreover, for a given example xi , Pi = {r ∈ Ω : yr = 1} is the set of ’positive’ prototypes for the i example xi , i. [sent-110, score-0.522]
</p><p>20 the set of prototype indices associated to the class of xi , while Ni = Ω \ Pi = {r ∈ Ω : yr = −1} is the set of ’negative’ prototypes, i. [sent-112, score-0.373]
</p><p>21 the set of prototype indices associated to classes i different from the class of xi . [sent-114, score-0.292]
</p><p>22 The dot product fr (x) = Mr , x is referred to as the similarity score  820  M ULTICLASS C LASSIFICATION WITH M ULTI -P ROTOTYPE S UPPORT V ECTOR M ACHINES  (or simply score) of the r-th prototype vector for the instance x. [sent-115, score-0.352]
</p><p>23 (2), in order to have a correct classiﬁcation, the prototype of the correct class is required to have a score greater than the maximum among the scores of the prototypes associated to incorrect classes. [sent-127, score-0.643]
</p><p>24 The multiclass margin for the example xi is then deﬁned by ρ(xi , ci |M) = Myi , xi − max Mr , xi , r=yi  where yi such that C (yi ) = ci , is the index of the prototype associated to the correct label for the example xi . [sent-128, score-0.606]
</p><p>25 In the single prototype case, with no loss of generality, we consider a prototype and the associated class indices to be coincident, that is yi = ci . [sent-129, score-0.489]
</p><p>26 Thus, a correct classiﬁcation of the example xi with a margin greater or equal to 1 requires the condition Myi , xi ≥ θi + 1 where θi = max Mr , xi . [sent-130, score-0.179]
</p><p>27 With these premises, a set of examples is said to be linearly separable by a multiclass classiﬁer if there exists a matrix M able to fulﬁll the above constraints for every pattern in the set. [sent-134, score-0.234]
</p><p>28 We thus formulate the problem in a SVM style by requiring a set of small norm prototypes to fulﬁll the soft constraints given by the classiﬁcation requirements. [sent-142, score-0.381]
</p><p>29 In this case, the Lagrangian is: L(M, ξ, θ, α, λ) =  1 2 2 ||M|| +C ∑i ξi + ∑i,r=yi αr ( Mr , xi − θi )+ i y ∑i αi i (θi + 1 − ξi − Myi , xi  )− ∑i λi ξi 1 = 2 ||M||2 − ∑i,r yr αr ( Mr , xi − θi )+ i i y y ∑i αi i + ∑i (C − αi i − λi )ξi ,  (5)  subject to the constraints αr , λi ≥ 0. [sent-148, score-0.171]
</p><p>30 Multi-Prototype Multi-Class SVM The SProtSVM model presented in the previous section is here extended to learn more than one prototypes per class. [sent-154, score-0.381]
</p><p>31 In this setting, one instance is correctly classiﬁed if and only if at least one of the prototypes associated to the correct class has a score greater than the maximum of the scores of the prototypes associated to incorrect classes. [sent-157, score-0.832]
</p><p>32 We can now give conditions for a correct classiﬁcation of an example xi with a margin greater or equal to 1 by requiring that: ∃r ∈ Pi : Mr , xi ≥ θi + 1 and θi = max Mr , xi . [sent-160, score-0.185]
</p><p>33 r∈Ni  (7)  To allow for margin violations, for each example xi , we introduce soft margin slack variables ξr ≥ 0, one for each positive prototype, such that i ∀r ∈ Pi , ξr = [θi + 1 − Mr , xi ]+ . [sent-161, score-0.227]
</p><p>34 i  Given a pattern xi , we arrange the soft margin slack variables ξr in a vector ξi ∈ R|Pi | . [sent-162, score-0.207]
</p><p>35 Let us now i introduce, for each example xi , a new vector having a number of components equal to the number of positive prototypes for xi , πi ∈ {0, 1}|Pi | , whose components are all zero except one component that is 1. [sent-163, score-0.441]
</p><p>36 Now, we are ready to formulate the general multi-prototype problem by requiring a set of prototypes of small norm and the best assignment for the examples able to fulﬁll the soft constraints given by the classiﬁcation requirements. [sent-166, score-0.439]
</p><p>37 Moreover, when kernels are used, the score function for the r-th prototype and a pattern x can be formulated as in the single-prototype case as n  fr (x) = Mr , φ(x) = ∑ yr αr k(x, xi ). [sent-183, score-0.527]
</p><p>38 i i i=1  Thus, when patterns are statically assigned to the prototypes via constant vectors πi , the convexity of the associated MProtSVM problem implies that the optimal solution for the primal problem in Eq. [sent-184, score-0.637]
</p><p>39 Assuming an equal number q of prototypes per class, the dual involves n × m × q variables which leads to a very large scale problem. [sent-187, score-0.412]
</p><p>40 In order to compute the optimal value for ν we ﬁrst observe that an additive update ∆Mr to the prototype r will affect the squared norm of the prototype vector Mr of an amount ∆||Mr ||2 = ||∆Mr ||2 + 2 Mr , ∆Mr . [sent-201, score-0.454]
</p><p>41 (Case 1) We ﬁrst show how to analytically solve the problem associated to an update involving y a single variable αr , r ∈ N p and the variable α pp . [sent-203, score-0.229]
</p><p>42 Thus, in this case we have: p y  y  αr ← αr + ν and α pp ← α pp + ν. [sent-205, score-0.388]
</p><p>43 p p  Since ∆Mr = −νx p , ∆My p = νx p and ∆Ms = 0 for s ∈ {r, y p }, we obtain / ∆||M||2 = ∆||Mr ||2 + ∆||My p ||2 = 2ν2 ||x p ||2 + 2ν( fy p (x p ) − fr (x p )) and the difference obtained in the Lagrangian value will be ∆L(ν) = 2ν(1 − fy p (x p ) + fr (x p ) − ν||x p ||2 ). [sent-206, score-0.444]
</p><p>44 p p  BasicUpdate(p, ra , rb , ν) αra = αra + yra ν; αrb = αrb + yrb ν; p p fra (x p ) = fra (x p ) + yra νKpp ; frb (x p ) = frb (x p ) − yrb νKpp ; p p Figure 2: Updates done after the basic optimization step has been performed and the optimal solution found. [sent-223, score-0.883]
</p><p>45 BasicOptimizeOnPattern(p, ϕV ) Heuristically choose two indexes ra = rb based on Eq. [sent-241, score-0.291]
</p><p>46 (14) ν = BasicStep(p, ra , rb ) BasicUpdate(p, ra , rb , ν) Figure 4: SMO-like algorithm for the optimization of statically assigned multi-prototype SVM. [sent-242, score-0.613]
</p><p>47 This method consists in ﬁxing an example and iterating multiple times the basic step described above on pairs of variables chosen among that associated to the pattern into consideration until some convergence conditions local to the pattern under consideration are matched. [sent-252, score-0.204]
</p><p>48 At each step, the algorithm applies the basic step to the m(m − 1)/2 pairs of variables associated with the pattern chosen for optimization until a certain condition on the value of the increment of the Lagrangian is veriﬁed. [sent-255, score-0.196]
</p><p>49 The optimization step of this reduced problem can require the optimization over all the q2 m(m − 1)/2 pairs of variables not constrained to 0 associated with the selected pattern. [sent-257, score-0.219]
</p><p>50 For the ﬁrst case, in order to be able to apply the step, it is necessary for one of the following two conditions to be veriﬁed: y  (α pp < C) ∧ ( fy p (x p ) < maxr∈N p fr (x p ) + 1)  (Ψ1 )  y  (Ψ2 ) (α pp > 0) ∧ ( fy p (x p ) > maxr∈N p ,αrp >0 fr (x p ) + 1) ˆ In fact, in Eq. [sent-265, score-0.873]
</p><p>51 (12), when there exists r ∈ N p such that fy p (x p ) < fr (x p ) + 1, the condition ν > 0 yp r ) can be chosen for optimization. [sent-266, score-0.286]
</p><p>52 Alternatively, if α pp > 0 and ˆ there exists an index r such that αr > 0 and fy p (x p ) > fr (x p ) + 1 then ν < 0 and (at least) the pair p yp k ) where k = arg max (α p , α p r∈N p ,αr >0 f r (x p ) can be chosen for optimization. [sent-269, score-0.445]
</p><p>53 (13), p ˆ we can observe that in order to have ν = 0, we need the last condition to be veriﬁed: y  (Ψ3 ) (α pp > 0) ∧ (maxr∈N p fr (x p ) > minr∈N p ,αrp >0 fr (x p )) In fact, in Eq. [sent-271, score-0.479]
</p><p>54 829  A IOLLI AND S PERDUTI  y  Note that in Ψ2 and Ψ3 the condition α pp > 0 is redundant and serves to assure that the second condition makes sense. [sent-273, score-0.264]
</p><p>55 (10) plus the so-called Karush-Kuhn-Tucker (KKT) complementarity conditions that in our case correspond to: (a) ∀p, r ∈ P p , αr (θ p + 1 − ξr − fr (x p )) = 0 p p (b) ∀p, r ∈ P p , λr ξr = 0 p p (c) ∀p, v ∈ N p , αv ( fv (x p ) − θ p ) = 0. [sent-284, score-0.209]
</p><p>56 First of all, we observe that for all the variables associated to a positive prototype r ∈ P p not assigned to the pattern x p , that is such that πr = 0, from Eq. [sent-293, score-0.326]
</p><p>57 (15)a, if not satisﬁed, implies fy p (x p ) ≥ θ p + 1, y y thus α pp = 0 and ξ pp = 0 thus satisfying the conditions in Eq. [sent-301, score-0.526]
</p><p>58 (15)a it must y be the case fy p (x p ) ≥ maxv∈N p fv (x p ) + 1 and so ξ pp = 0 thus verifying the condition in Eq. [sent-312, score-0.369]
</p><p>59 (15)c is not satisﬁed ∀v ∈ N p : αv > 0 ⇒ θ p = p maxr∈N p fr (x p ) ≤ fv (x p ) ⇒ θ p = fv (x p ) and the condition in Eq. [sent-318, score-0.246]
</p><p>60 (15)b we obtain fy p (x p ) ≤ θ p + 1 and ξ pp = θ p + 1 − fy p (x p ) thus implying the truth of the condition in Eq. [sent-321, score-0.423]
</p><p>61 In fact, this leads to a decrease of the total number of patterns selected for optimization and consequently to a decrease of the number of kernel computations. [sent-391, score-0.188]
</p><p>62 The algorithm consists of two steps: a step in which, ﬁxed the values for the set of variables α, we select the assignments π’s in such a way to minimize the primal value, followed by a step in which the optimization of the variables α is performed once ﬁxed the assignments. [sent-417, score-0.222]
</p><p>63 Once that the optimal value for the primal, let say Pπ(1) , has been reached, we can easily observe that the solution can be further improved by updating the assignments in such a way to associate each pattern xi to a positive prototype having associated the minimal slack value, i. [sent-421, score-0.448]
</p><p>64 For the optimization algorithm to succeed, however, KKT conditions on α have to be restored in order to return back to a feasible solution and then ﬁnally resuming the Lagrangian optimization with the new assignment π(2). [sent-432, score-0.223]
</p><p>65 One problem with this procedure is that it can result onerous when dealing with large datasets or when using many prototypes since, in this case, many complete Lagrangian optimizations have to be performed. [sent-436, score-0.42]
</p><p>66 For this, we can observe that for the procedure to work, at each step, it is sufﬁcient to stop the optimization of the Lagrangian when we ﬁnd a value for the primal which is better than the last found value and this is going to happen for sure since the last solution was found not optimal. [sent-437, score-0.189]
</p><p>67 In fact, since the induced problem is convex for each possible assignment, then there will exist a unique optimal primal value P∗ (π, α∗ (π)) associated with optimal solutions α∗ (π) for the assignments π. [sent-442, score-0.22]
</p><p>68 However, when assuming an equal number q of prototype vectors for each class, there are qn possible solutions with many trivial symmetries. [sent-444, score-0.227]
</p><p>69 2 i Let suppose to have a pattern xi having slack variables ξr , r ∈ Pi , and suppose that the probability i for the assignment to be in the state of nature s (i. [sent-454, score-0.211]
</p><p>70 with the s-th component set to 1) follows the law pi (s) ∝ e−∆Es /T where T is the temperature of the system and ∆Es = C(ξs − ξyi ) the variation of the system energy i i when the pattern xi is assigned to the s-th prototype. [sent-456, score-0.198]
</p><p>71 ∑r∈Pi pi (r) = 1, we obtain pi (s) = r  1 − C(ξs −ξ0 ) i i T e Zi  (17)  0  with Zi = ∑r∈Pi e−C(ξi −ξi )/T the partition function. [sent-459, score-0.208]
</p><p>72 Thus, when perturbing the assignment for a pattern xi , each positive prototype s will be selected with probability pi (s). [sent-460, score-0.515]
</p><p>73 (17) it clearly appears that, when the temperature of the system is low, the probability for a pattern to be assigned to a prototype different from the one having minimal slack value tends to 0 and we obtain a behavior similar to the deterministic version of the algorithm. [sent-462, score-0.35]
</p><p>74 For simplicity, we consider MProtSVM with a ﬁxed number q of prototypes per class. [sent-468, score-0.381]
</p><p>75 (17); compute the new primal E(t + 1) := Pπ(t+1) (α); restore KKT conditions on α /*see Section 6*/ end; Figure 11: Fast annealed algorithm for the optimization of MProtSVM. [sent-481, score-0.23]
</p><p>76 In fact, the condition ξr = 0 is true at least in the case r = y p , where y p is the p y positive prototype associated to the pattern x p , i. [sent-483, score-0.361]
</p><p>77 we show how the original multiclass problem can be reduced into one made of multiple binary decisions. [sent-489, score-0.198]
</p><p>78 The evaluation of new patterns is made by evaluating the pattern with the perceptron discriminating the classes in the ﬁrst and in the last position of the list. [sent-495, score-0.192]
</p><p>79 839  A IOLLI AND S PERDUTI  given a new pattern, we compare the scores obtained by the two prototypes in the head and in the tail of the list and the loser is removed from the list. [sent-508, score-0.381]
</p><p>80 This is done until only prototypes of the same class remain on the list and this is the class returned for the pattern under consideration. [sent-509, score-0.445]
</p><p>81 Going back to MProtSVM, for the following analysis we deﬁne the hyperplane wrs = Mr − Ms for each pair of prototypes indexes r, s such that C (r) < C (s). [sent-516, score-0.469]
</p><p>82 i i Now, we can deﬁne the margin of the classiﬁer hrs (x) = wrs , x as the minimum of the (geometrical) margins of the patterns associated to it, i. [sent-521, score-0.299]
</p><p>83 By now, we have demonstrated that the minimization of the term D = ∑r,s: C (r) < C (s), deﬁne the perceptron hrs with weight vector wrs = Mr − Ms (1) and θrs = 0; (2)  Second layer (AND): ∀u ∈ Y = {1, . [sent-527, score-0.248]
</p><p>84 , m}, ∀v ∈ Ω : C (v) = u deﬁne the perceptron huv , taking (1) input from all hrs such that r = v or s = v and connection equal to 1 if r = v, −1 otherwise; (2) set threshold to the value θuv = q(m − 1) − 0. [sent-530, score-0.189]
</p><p>85 With this setting, prototypes M1 and M2 are associated with class 1, prototypes M3 and M4 are associated with class 2, and prototypes M5 and M6 are associated with class 3. [sent-549, score-1.248]
</p><p>86 Theorem 5 For any 0 < δ < 1, any MProtSVM HM (·) with q prototypes for each of the m classes, given S a sample of size n drawn i. [sent-550, score-0.381]
</p><p>87 By the above construction, the class of functions computable by a MProtSVM with q prototypes for each of the m classes is contained in the class of functions computable by three-layer perceptrons deﬁned as above. [sent-558, score-0.423]
</p><p>88 In fact, by construction of the network, we have that if hrs (x) = 1 843  A IOLLI AND S PERDUTI  (1)  (1)  (1)  (1)  and hssˆ (x) = 1, then for sure hrsˆ (x) = 1, as well as, if hrs (x) = 0 and hssˆ (x) = 0, then for sure (1) hrsˆ (x) = 0. [sent-562, score-0.174]
</p><p>89 This is the result of the fact that, given an input vector x, the outputs of the ﬁrst layer perceptrons are fully determined by the total order over the MProtSVM prototypes induced by the score functions fr (·). [sent-563, score-0.588]
</p><p>90 Thus we can compute an upper bound on the proportion of ’legal’ conﬁgurations by considering all possible permutations of the qm prototypes divided by all possible conﬁgurations, i. [sent-564, score-0.497]
</p><p>91 Notice that this is an upper bound since when considering prototypes of the same class, we do not care about their relative order. [sent-567, score-0.381]
</p><p>92 Initially, we tested our model against three multiclass datasets that we brieﬂy describe in the following: NIST: it consists of a 10-class task of 10705 digits randomly taken from the NIST-3 dataset. [sent-584, score-0.209]
</p><p>93 Here and in the following experiments we report the value of the factor β = (m × q)/n deﬁned as the number of prototypes produced as a fraction of the cardinality of the training set. [sent-635, score-0.381]
</p><p>94 26 × 20 prototypes in the LETTER dataset gives β = 0. [sent-653, score-0.441]
</p><p>95 Moreover, as the number of prototypes per class increases, the choice of small τ tends to be more crucial. [sent-719, score-0.381]
</p><p>96 Notice that from the fact that the primal value is just a way to approximate the theoretical SRM principle and from the non-optimality of the parameter C in these experiments, better values for the primal does not necessarily correspond to better values for the test error. [sent-721, score-0.254]
</p><p>97 Conclusions We have proposed an extension of multiclass SVM able to deal with several prototypes per class. [sent-723, score-0.551]
</p><p>98 We suggested to solve this problem by using a novel efﬁcient optimization procedure within an annealing framework where the energy function corresponds to the primal of the problem. [sent-725, score-0.215]
</p><p>99 When one prototype per class is used in a binary problem, as in this case, MProtSVM actually generates two vectors that are the same with sign inverted. [sent-733, score-0.227]
</p><p>100 Reducing multiclass to binary: A unifying approach for margin classiﬁers. [sent-809, score-0.224]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('prototypes', 0.381), ('mr', 0.3), ('mprotsvm', 0.29), ('prototype', 0.227), ('pp', 0.194), ('multiclass', 0.17), ('iolli', 0.155), ('perduti', 0.155), ('rb', 0.146), ('rototype', 0.145), ('ulti', 0.145), ('upport', 0.145), ('primal', 0.127), ('fr', 0.125), ('crammer', 0.124), ('achines', 0.122), ('ector', 0.122), ('ulticlass', 0.122), ('qm', 0.116), ('ra', 0.115), ('aiolli', 0.106), ('pi', 0.104), ('kkt', 0.103), ('lagrangian', 0.102), ('kpp', 0.097), ('maxr', 0.097), ('yrb', 0.097), ('fy', 0.097), ('lassification', 0.091), ('hrs', 0.087), ('yra', 0.087), ('singer', 0.086), ('yr', 0.081), ('lvq', 0.077), ('rvm', 0.073), ('basicstep', 0.068), ('sperduti', 0.068), ('patterns', 0.065), ('pattern', 0.064), ('perceptron', 0.063), ('optimization', 0.062), ('dataset', 0.06), ('slack', 0.059), ('assignment', 0.058), ('basicupdate', 0.058), ('wrs', 0.058), ('cooling', 0.058), ('margin', 0.054), ('cache', 0.053), ('fra', 0.048), ('frb', 0.048), ('sprotsvm', 0.048), ('vp', 0.048), ('ful', 0.047), ('fv', 0.043), ('perceptrons', 0.042), ('ck', 0.042), ('conditions', 0.041), ('svm', 0.041), ('myi', 0.041), ('usps', 0.04), ('layer', 0.04), ('gurations', 0.039), ('hm', 0.039), ('datasets', 0.039), ('codeword', 0.039), ('ddag', 0.039), ('huv', 0.039), ('optimizeonpattern', 0.039), ('ll', 0.038), ('spent', 0.036), ('associated', 0.035), ('condition', 0.035), ('strategies', 0.034), ('rp', 0.034), ('assignments', 0.033), ('minr', 0.032), ('wta', 0.032), ('selected', 0.032), ('dual', 0.031), ('ni', 0.031), ('xi', 0.03), ('indexes', 0.03), ('letter', 0.03), ('admissibility', 0.029), ('codewords', 0.029), ('fabio', 0.029), ('ocr', 0.029), ('statically', 0.029), ('yp', 0.029), ('kernel', 0.029), ('rs', 0.028), ('reduced', 0.028), ('platt', 0.027), ('labelled', 0.026), ('annealing', 0.026), ('tipping', 0.026), ('ms', 0.026), ('convex', 0.025), ('heuristics', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="58-tfidf-1" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>Author: Fabio Aiolli, Alessandro Sperduti</p><p>Abstract: Winner-take-all multiclass classiﬁers are built on the top of a set of prototypes each representing one of the available classes. A pattern is then classiﬁed with the label associated to the most ‘similar’ prototype. Recent proposal of SVM extensions to multiclass can be considered instances of the same strategy with one prototype per class. The multi-prototype SVM proposed in this paper extends multiclass SVM to multiple prototypes per class. It allows to combine several vectors in a principled way to obtain large margin decision functions. For this problem, we give a compact constrained quadratic formulation and we propose a greedy optimization algorithm able to ﬁnd locally optimal solutions for the non convex objective function. This algorithm proceeds by reducing the overall problem into a series of simpler convex problems. For the solution of these reduced problems an efﬁcient optimization algorithm is proposed. A number of pattern selection strategies are then discussed to speed-up the optimization process. In addition, given the combinatorial nature of the overall problem, stochastic search strategies are suggested to escape from local minima which are not globally optimal. Finally, we report experiments on a number of datasets. The performance obtained using few simple linear prototypes is comparable to that obtained by state-of-the-art kernel-based methods but with a signiﬁcant reduction (of one or two orders) in response time. Keywords: multiclass classiﬁcation, multi-prototype support vector machines, kernel machines, stochastic search optimization, large margin classiﬁers</p><p>2 0.096794434 <a title="58-tfidf-2" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>Author: Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, Yasemin Altun</p><p>Abstract: Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing ﬂexible and powerful input representations, this paper addresses the complementary issue of designing classiﬁcation algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classiﬁcation problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.</p><p>3 0.08531934 <a title="58-tfidf-3" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>Author: Manfred Opper, Ole Winther</p><p>Abstract: We propose a novel framework for approximations to intractable probabilistic models which is based on a free energy formulation. The approximation can be understood as replacing an average over the original intractable distribution with a tractable one. It requires two tractable probability distributions which are made consistent on a set of moments and encode diﬀerent features of the original intractable distribution. In this way we are able to use Gaussian approximations for models with discrete or bounded variables which allow us to include non-trivial correlations. These are neglected in many other methods. We test the framework on toy benchmark problems for binary variables on fully connected graphs and 2D grids and compare with other methods, such as loopy belief propagation. Good performance is already achieved by using single nodes as tractable substructures. Signiﬁcant improvements are obtained when a spanning tree is used instead.</p><p>4 0.073697902 <a title="58-tfidf-4" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>Author: Ivor W. Tsang, James T. Kwok, Pak-Ming Cheung</p><p>Abstract: O(m3 ) Standard SVM training has time and O(m2 ) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such “approximateness” in this paper. We ﬁrst show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efﬁcient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and realworld data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about ﬁve million training patterns, in only 1.4 seconds on a 3.2GHz Pentium–4 PC. Keywords: kernel methods, approximation algorithm, minimum enclosing ball, core set, scalability</p><p>5 0.064984567 <a title="58-tfidf-5" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>Author: S. Sathiya Keerthi, Dennis DeCoste</p><p>Abstract: This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classiﬁcation. This is done by modifying the ﬁnite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight , SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modiﬁed Huber’s loss function and the L1 loss function, and also for solving ordinal regression. Keywords: linear SVMs, classiﬁcation, conjugate gradient</p><p>6 0.047272239 <a title="58-tfidf-6" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>7 0.046419229 <a title="58-tfidf-7" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>8 0.045993634 <a title="58-tfidf-8" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>9 0.044443782 <a title="58-tfidf-9" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>10 0.044111043 <a title="58-tfidf-10" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>11 0.04069598 <a title="58-tfidf-11" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>12 0.038861424 <a title="58-tfidf-12" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>13 0.038311083 <a title="58-tfidf-13" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>14 0.038153578 <a title="58-tfidf-14" href="./jmlr-2005-Multiclass_Boosting_for_Weak_Classifiers.html">57 jmlr-2005-Multiclass Boosting for Weak Classifiers</a></p>
<p>15 0.037510339 <a title="58-tfidf-15" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>16 0.036299154 <a title="58-tfidf-16" href="./jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines.html">73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</a></p>
<p>17 0.036226187 <a title="58-tfidf-17" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>18 0.035630517 <a title="58-tfidf-18" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>19 0.034388386 <a title="58-tfidf-19" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>20 0.032499325 <a title="58-tfidf-20" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.209), (1, -0.007), (2, 0.136), (3, -0.031), (4, -0.022), (5, -0.146), (6, 0.021), (7, -0.059), (8, -0.173), (9, -0.061), (10, 0.062), (11, 0.028), (12, 0.057), (13, -0.014), (14, 0.294), (15, -0.083), (16, -0.25), (17, 0.203), (18, 0.111), (19, -0.014), (20, 0.046), (21, -0.107), (22, 0.191), (23, 0.075), (24, -0.147), (25, -0.044), (26, 0.261), (27, -0.001), (28, -0.102), (29, 0.057), (30, -0.04), (31, 0.095), (32, 0.133), (33, -0.211), (34, 0.079), (35, -0.076), (36, -0.036), (37, 0.127), (38, 0.123), (39, -0.023), (40, 0.082), (41, -0.017), (42, 0.0), (43, 0.143), (44, -0.286), (45, 0.071), (46, 0.007), (47, 0.072), (48, -0.035), (49, -0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9250837 <a title="58-lsi-1" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>Author: Fabio Aiolli, Alessandro Sperduti</p><p>Abstract: Winner-take-all multiclass classiﬁers are built on the top of a set of prototypes each representing one of the available classes. A pattern is then classiﬁed with the label associated to the most ‘similar’ prototype. Recent proposal of SVM extensions to multiclass can be considered instances of the same strategy with one prototype per class. The multi-prototype SVM proposed in this paper extends multiclass SVM to multiple prototypes per class. It allows to combine several vectors in a principled way to obtain large margin decision functions. For this problem, we give a compact constrained quadratic formulation and we propose a greedy optimization algorithm able to ﬁnd locally optimal solutions for the non convex objective function. This algorithm proceeds by reducing the overall problem into a series of simpler convex problems. For the solution of these reduced problems an efﬁcient optimization algorithm is proposed. A number of pattern selection strategies are then discussed to speed-up the optimization process. In addition, given the combinatorial nature of the overall problem, stochastic search strategies are suggested to escape from local minima which are not globally optimal. Finally, we report experiments on a number of datasets. The performance obtained using few simple linear prototypes is comparable to that obtained by state-of-the-art kernel-based methods but with a signiﬁcant reduction (of one or two orders) in response time. Keywords: multiclass classiﬁcation, multi-prototype support vector machines, kernel machines, stochastic search optimization, large margin classiﬁers</p><p>2 0.41719651 <a title="58-lsi-2" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>Author: Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, Yasemin Altun</p><p>Abstract: Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing ﬂexible and powerful input representations, this paper addresses the complementary issue of designing classiﬁcation algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classiﬁcation problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.</p><p>3 0.39815953 <a title="58-lsi-3" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>Author: Manfred Opper, Ole Winther</p><p>Abstract: We propose a novel framework for approximations to intractable probabilistic models which is based on a free energy formulation. The approximation can be understood as replacing an average over the original intractable distribution with a tractable one. It requires two tractable probability distributions which are made consistent on a set of moments and encode diﬀerent features of the original intractable distribution. In this way we are able to use Gaussian approximations for models with discrete or bounded variables which allow us to include non-trivial correlations. These are neglected in many other methods. We test the framework on toy benchmark problems for binary variables on fully connected graphs and 2D grids and compare with other methods, such as loopy belief propagation. Good performance is already achieved by using single nodes as tractable substructures. Signiﬁcant improvements are obtained when a spanning tree is used instead.</p><p>4 0.31005546 <a title="58-lsi-4" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>Author: Ivor W. Tsang, James T. Kwok, Pak-Ming Cheung</p><p>Abstract: O(m3 ) Standard SVM training has time and O(m2 ) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such “approximateness” in this paper. We ﬁrst show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efﬁcient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and realworld data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about ﬁve million training patterns, in only 1.4 seconds on a 3.2GHz Pentium–4 PC. Keywords: kernel methods, approximation algorithm, minimum enclosing ball, core set, scalability</p><p>5 0.21638398 <a title="58-lsi-5" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>Author: Atsuyoshi Nakamura, Michael Schmitt, Niels Schmitt, Hans Ulrich Simon</p><p>Abstract: Bayesian networks have become one of the major models used for statistical inference. We study the question whether the decisions computed by a Bayesian network can be represented within a low-dimensional inner product space. We focus on two-label classiﬁcation tasks over the Boolean domain. As main results we establish upper and lower bounds on the dimension of the inner product space for Bayesian networks with an explicitly given (full or reduced) parameter collection. In particular, these bounds are tight up to a factor of 2. For some nontrivial cases of Bayesian networks we even determine the exact values of this dimension. We further consider logistic autoregressive Bayesian networks and show that every sufﬁciently expressive inner product space must have dimension at least Ω(n2 ), where n is the number of network nodes. We also derive the bound 2Ω(n) for an artiﬁcial variant of this network, thereby demonstrating the limits of our approach and raising an interesting open question. As a major technical contribution, this work reveals combinatorial and algebraic structures within Bayesian networks such that known methods for the derivation of lower bounds on the dimension of inner product spaces can be brought into play. Keywords: Bayesian network, inner product space, embedding, linear arrangement, Euclidean dimension</p><p>6 0.18542984 <a title="58-lsi-6" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>7 0.18265909 <a title="58-lsi-7" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>8 0.17630023 <a title="58-lsi-8" href="./jmlr-2005-Multiclass_Boosting_for_Weak_Classifiers.html">57 jmlr-2005-Multiclass Boosting for Weak Classifiers</a></p>
<p>9 0.16873236 <a title="58-lsi-9" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>10 0.16389689 <a title="58-lsi-10" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>11 0.15733588 <a title="58-lsi-11" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>12 0.1511157 <a title="58-lsi-12" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>13 0.14870469 <a title="58-lsi-13" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>14 0.13898529 <a title="58-lsi-14" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>15 0.13123046 <a title="58-lsi-15" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>16 0.12692612 <a title="58-lsi-16" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>17 0.12691462 <a title="58-lsi-17" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>18 0.1254293 <a title="58-lsi-18" href="./jmlr-2005-Working_Set_Selection_Using_Second_Order_Information_for_Training_Support_Vector_Machines.html">73 jmlr-2005-Working Set Selection Using Second Order Information for Training Support Vector Machines</a></p>
<p>19 0.12514956 <a title="58-lsi-19" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>20 0.12199548 <a title="58-lsi-20" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(17, 0.022), (19, 0.031), (36, 0.026), (37, 0.023), (42, 0.014), (43, 0.028), (47, 0.029), (52, 0.068), (59, 0.014), (70, 0.035), (88, 0.079), (90, 0.013), (94, 0.545)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83691037 <a title="58-lda-1" href="./jmlr-2005-Efficient_Margin_Maximizing_with_Boosting.html">29 jmlr-2005-Efficient Margin Maximizing with Boosting</a></p>
<p>Author: Gunnar Rätsch, Manfred K. Warmuth</p><p>Abstract: AdaBoost produces a linear combination of base hypotheses and predicts with the sign of this linear combination. The linear combination may be viewed as a hyperplane in feature space where the base hypotheses form the features. It has been observed that the generalization error of the algorithm continues to improve even after all examples are on the correct side of the current hyperplane. The improvement is attributed to the experimental observation that the distances (margins) of the examples to the separating hyperplane are increasing even after all examples are on the correct side. We introduce a new version of AdaBoost, called AdaBoost∗ , that explicitly maximizes the ν minimum margin of the examples up to a given precision. The algorithm incorporates a current estimate of the achievable margin into its calculation of the linear coefﬁcients of the base hypotheses. The bound on the number of iterations needed by the new algorithms is the same as the number needed by a known version of AdaBoost that must have an explicit estimate of the achievable margin as a parameter. We also illustrate experimentally that our algorithm requires considerably fewer iterations than other algorithms that aim to maximize the margin.</p><p>same-paper 2 0.81080329 <a title="58-lda-2" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>Author: Fabio Aiolli, Alessandro Sperduti</p><p>Abstract: Winner-take-all multiclass classiﬁers are built on the top of a set of prototypes each representing one of the available classes. A pattern is then classiﬁed with the label associated to the most ‘similar’ prototype. Recent proposal of SVM extensions to multiclass can be considered instances of the same strategy with one prototype per class. The multi-prototype SVM proposed in this paper extends multiclass SVM to multiple prototypes per class. It allows to combine several vectors in a principled way to obtain large margin decision functions. For this problem, we give a compact constrained quadratic formulation and we propose a greedy optimization algorithm able to ﬁnd locally optimal solutions for the non convex objective function. This algorithm proceeds by reducing the overall problem into a series of simpler convex problems. For the solution of these reduced problems an efﬁcient optimization algorithm is proposed. A number of pattern selection strategies are then discussed to speed-up the optimization process. In addition, given the combinatorial nature of the overall problem, stochastic search strategies are suggested to escape from local minima which are not globally optimal. Finally, we report experiments on a number of datasets. The performance obtained using few simple linear prototypes is comparable to that obtained by state-of-the-art kernel-based methods but with a signiﬁcant reduction (of one or two orders) in response time. Keywords: multiclass classiﬁcation, multi-prototype support vector machines, kernel machines, stochastic search optimization, large margin classiﬁers</p><p>3 0.80204993 <a title="58-lda-3" href="./jmlr-2005-A_Generalization_Error_for_Q-Learning.html">5 jmlr-2005-A Generalization Error for Q-Learning</a></p>
<p>Author: Susan A. Murphy</p><p>Abstract: Planning problems that involve learning a policy from a single training set of ﬁnite horizon trajectories arise in both social science and medical ﬁelds. We consider Q-learning with function approximation for this setting and derive an upper bound on the generalization error. This upper bound is in terms of quantities minimized by a Q-learning algorithm, the complexity of the approximation space and an approximation term due to the mismatch between Q-learning and the goal of learning a policy that maximizes the value function. Keywords: multistage decisions, dynamic programming, reinforcement learning, batch data</p><p>4 0.63714635 <a title="58-lda-4" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>Author: Marco Cuturi, Kenji Fukumizu, Jean-Philippe Vert</p><p>Abstract: We present a family of positive deﬁnite kernels on measures, characterized by the fact that the value of the kernel between two measures is a function of their sum. These kernels can be used to derive kernels on structured objects, such as images and texts, by representing these objects as sets of components, such as pixels or words, or more generally as measures on the space of components. Several kernels studied in this work make use of common quantities deﬁned on measures such as entropy or generalized variance to detect similarities. Given an a priori kernel on the space of components itself, the approach is further extended by restating the previous results in a more efﬁcient and ﬂexible framework using the “kernel trick”. Finally, a constructive approach to such positive deﬁnite kernels through an integral representation theorem is proved, before presenting experimental results on a benchmark experiment of handwritten digits classiﬁcation to illustrate the validity of the approach. Keywords: kernels on measures, semigroup theory, Jensen divergence, generalized variance, reproducing kernel Hilbert space</p><p>5 0.42542648 <a title="58-lda-5" href="./jmlr-2005-Multiclass_Boosting_for_Weak_Classifiers.html">57 jmlr-2005-Multiclass Boosting for Weak Classifiers</a></p>
<p>Author: Günther Eibl, Karl-Peter Pfeiffer</p><p>Abstract: AdaBoost.M2 is a boosting algorithm designed for multiclass problems with weak base classiﬁers. The algorithm is designed to minimize a very loose bound on the training error. We propose two alternative boosting algorithms which also minimize bounds on performance measures. These performance measures are not as strongly connected to the expected error as the training error, but the derived bounds are tighter than the bound on the training error of AdaBoost.M2. In experiments the methods have roughly the same performance in minimizing the training and test error rates. The new algorithms have the advantage that the base classiﬁer should minimize the conﬁdence-rated error, whereas for AdaBoost.M2 the base classiﬁer should minimize the pseudo-loss. This makes them more easily applicable to already existing base classiﬁers. The new algorithms also tend to converge faster than AdaBoost.M2. Keywords: boosting, multiclass, ensemble, classiﬁcation, decision stumps</p><p>6 0.34527665 <a title="58-lda-6" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>7 0.33736914 <a title="58-lda-7" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>8 0.33053192 <a title="58-lda-8" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>9 0.32838535 <a title="58-lda-9" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>10 0.32348078 <a title="58-lda-10" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>11 0.32200122 <a title="58-lda-11" href="./jmlr-2005-Stability_of_Randomized_Learning_Algorithms.html">67 jmlr-2005-Stability of Randomized Learning Algorithms</a></p>
<p>12 0.31981522 <a title="58-lda-12" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>13 0.30922681 <a title="58-lda-13" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<p>14 0.30282754 <a title="58-lda-14" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>15 0.29142112 <a title="58-lda-15" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>16 0.29137176 <a title="58-lda-16" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>17 0.28941342 <a title="58-lda-17" href="./jmlr-2005-Learning_the_Kernel_Function_via_Regularization.html">48 jmlr-2005-Learning the Kernel Function via Regularization</a></p>
<p>18 0.28772223 <a title="58-lda-18" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>19 0.28271669 <a title="58-lda-19" href="./jmlr-2005-Generalization_Bounds_for_the_Area_Under_the_ROC_Curve.html">38 jmlr-2005-Generalization Bounds for the Area Under the ROC Curve</a></p>
<p>20 0.28046566 <a title="58-lda-20" href="./jmlr-2005-Frames%2C_Reproducing_Kernels%2C_Regularization_and_Learning.html">35 jmlr-2005-Frames, Reproducing Kernels, Regularization and Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
