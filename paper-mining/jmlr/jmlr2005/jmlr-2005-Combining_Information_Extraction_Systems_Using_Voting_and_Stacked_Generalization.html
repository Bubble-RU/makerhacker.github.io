<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 jmlr-2005-Combining Information Extraction Systems Using Voting and Stacked Generalization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-21" href="#">jmlr2005-21</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>21 jmlr-2005-Combining Information Extraction Systems Using Voting and Stacked Generalization</h1>
<br/><p>Source: <a title="jmlr-2005-21-pdf" href="http://jmlr.org/papers/volume6/sigletos05a/sigletos05a.pdf">pdf</a></p><p>Author: Georgios Sigletos, Georgios Paliouras, Constantine D. Spyropoulos, Michalis Hatzopoulos</p><p>Abstract: This article investigates the effectiveness of voting and stacked generalization -also known as stacking- in the context of information extraction (IE). A new stacking framework is proposed that accommodates well-known approaches for IE. The key idea is to perform cross-validation on the base-level data set, which consists of text documents annotated with relevant information, in order to create a meta-level data set that consists of feature vectors. A classifier is then trained using the new vectors. Therefore, base-level IE systems are combined with a common classifier at the metalevel. Various voting schemes are presented for comparing against stacking in various IE domains. Well known IE systems are employed at the base-level, together with a variety of classifiers at the meta-level. Results show that both voting and stacking work better when relying on probabilistic estimates by the base-level systems. Voting proved to be effective in most domains in the experiments. Stacking, on the other hand, proved to be consistently effective over all domains, doing comparably or better than voting and always better than the best base-level systems. Particular emphasis is also given to explaining the results obtained by voting and stacking at the meta-level, with respect to the varying degree of similarity in the output of the base-level systems. Keywords: stacking, voting, information extraction, cross-validation 1</p><p>Reference: <a title="jmlr-2005-21-reference" href="../jmlr2005_reference/jmlr-2005-Combining_Information_Extraction_Systems_Using_Voting_and_Stacked_Generalization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 GR  Department of Informatics and Telecommunications University of Athens Panepistimiopolis, 157 71, Athens, Greece Editor: William Cohen Abstract This article investigates the effectiveness of voting and stacked generalization -also known as stacking- in the context of information extraction (IE). [sent-10, score-0.312]
</p><p>2 A new stacking framework is proposed that accommodates well-known approaches for IE. [sent-11, score-0.363]
</p><p>3 Various voting schemes are presented for comparing against stacking in various IE domains. [sent-15, score-0.536]
</p><p>4 Results show that both voting and stacking work better when relying on probabilistic estimates by the base-level systems. [sent-17, score-0.536]
</p><p>5 Particular emphasis is also given to explaining the results obtained by voting and stacking at the meta-level, with respect to the varying degree of similarity in the output of the base-level systems. [sent-20, score-0.556]
</p><p>6 Stacked generalization or stacking (Wolpert, 1992) is a common scheme that deals with the task of learning a meta-level classifier to combine the predictions of multiple base-level classifiers. [sent-25, score-0.503]
</p><p>7 The success of stacking arises from its ability to exploit the diversity in the predictions of base-level classifiers and thus predicting with higher accuracy at meta-level. [sent-26, score-0.466]
</p><p>8 Spyropoulos and Michalis Hatzopoulos  SIGLETOS, PALIOURAS, SPYROPOULOS AND HATZOPOULOS  place when voting on the predictions of multiple classifiers. [sent-28, score-0.226]
</p><p>9 Voting is typically used as a baseline against which the performance of stacking is compared. [sent-29, score-0.363]
</p><p>10 Research on voting and stacking has primarily focused on classification. [sent-30, score-0.536]
</p><p>11 x n > , the predictions of the base-level classifiers form a new feature vector, which is assigned the class value y either by the meta-level classifier or by voting. [sent-41, score-0.21]
</p><p>12 In this article we investigate the effectiveness of voting and stacking on the task of Information Extraction (IE). [sent-43, score-0.574]
</p><p>13 IE is a form of shallow text processing that involves the population of a predefined template with relevant fragments extracted from a text document. [sent-44, score-0.537]
</p><p>14 The key idea behind combining a set of IE systems through stacking is to learn a common meta-level classifier, such as a decision tree or a naiveBayes classifier, based on the output of the IE systems, towards higher extraction performance. [sent-49, score-0.489]
</p><p>15 In order to apply voting and stacking to IE, the base-level classifiers should be normally replaced by systems that model IE as a classification task. [sent-51, score-0.67]
</p><p>16 A typical IE system is trained using a set of sample documents, paired with templates that are filled with relevant text fragments from the documents. [sent-54, score-0.341]
</p><p>17 This way of modelling the IE task, however, results in an enormous increase in the number of candidate text fragments, where only the small number of annotated fragments is considered as positive examples, while all the other fragments are considered as negative examples during training. [sent-56, score-0.359]
</p><p>18 Table 1 shows the examples that are constructed from a hypothetical text fragment within a page describing laptop products. [sent-57, score-0.531]
</p><p>19 An indicative example of recognizing an instance of the field ram (highlighted in bold) from a page that describes a laptop product and the set of examples it generates. [sent-59, score-0.491]
</p><p>20 Although the size of the candidate text fragments can be somehow reduced by using various heuristics (Freitag, 2000), modelling IE in this manner, does not seem natural. [sent-60, score-0.214]
</p><p>21 The task here is to recognize starting and ending token boundaries of relevant fragments within a document and then extract the enclosed content. [sent-65, score-0.216]
</p><p>22 (a) Part of a Web page describing laptop products (b) The hand-filled template for this page. [sent-67, score-0.346]
</p><p>23 Recent effort on adaptive IE (Ciravegna, 2001; Ciravegna and Lavelli, 2003), motivates the development of IE systems that can handle different text types, from rigidly structured to almost free text -where common wrappers fail- including mixed types. [sent-91, score-0.212]
</p><p>24 < f > and < /f > tags for each relevant field f ∈ { f 1 . [sent-102, score-0.261]
</p><p>25 Despite its simplicity, stacking offers the advantage of being highly extensible to more algorithms at both base-level and meta-level, regardless of their internal structure. [sent-119, score-0.363]
</p><p>26 Specifically, for each relevant field f ∈ { f 1 . [sent-122, score-0.239]
</p><p>27 In case of more than one predictions of the field f for a fragment t ( s, e) , a combined probability is estimated for < t(s, e), f > using (1). [sent-126, score-0.526]
</p><p>28 PC =1 −  ∏ (1 − p  j  ),  (1)  j  where P C is the combined probabilistic estimate that the text fragment t ( s, e) belongs to the field f , and p j the probabilistic estimate that some IE system E j has predicted the field f for t ( s, e) . [sent-127, score-0.834]
</p><p>29 Actually, P C measures the probability that at least one of those IE systems that have predicted the field f for t ( s, e) , has predicted correctly, which equals the probability that not all predictions for f are wrong. [sent-128, score-0.395]
</p><p>30 For example, a page in the domain of computer science (CS) courses should describe only one course, and thus should contain only one instance of the field course title. [sent-131, score-0.24]
</p><p>31 Assuming in Table 3 that one hypothetical IE system predicts “256 MB” and “1 GB” as ram instances, while another system predicts only “256 MB” as ram. [sent-134, score-0.242]
</p><p>32 Supposing that the correct instance is <"256 MB" , ram >  The OPD field constraint, though useful in certain cases, is restrictive for IE in general and does not hold for all relevant fields. [sent-143, score-0.382]
</p><p>33 For example, a Web page may describe more than one laptop products, and thus more than one ram instances may exist. [sent-144, score-0.284]
</p><p>34 The motivation behind mapping confidence scores to probabilistic estimates is that confidence scores are not always reliable, since incorrect matches may be assigned high scores. [sent-149, score-0.22]
</p><p>35 Thus voting on different IE systems using confidence scores may not always be reliable. [sent-150, score-0.289]
</p><p>36 The concept of the merged template is introduced, which is important for combining different IE systems either through voting or stacking. [sent-158, score-0.506]
</p><p>37 3, against which the performance of stacking for IE will be compared. [sent-161, score-0.363]
</p><p>38 LN be a set of N learning algorithms, designed for IE, which are given a corpus D of training documents, annotated with relevant field instances. [sent-166, score-0.268]
</p><p>39 We suggest in this article that a merged template can be constructed from T 1 . [sent-183, score-0.346]
</p><p>40 T N as follows: all text fragments t ( s, e) identified by E 1 . [sent-186, score-0.317]
</p><p>41 Duplicate fragments are removed: two fragments differ if either their start or end boundary differs. [sent-193, score-0.232]
</p><p>42 E N are collected and inserted together with the correct field in the template. [sent-197, score-0.245]
</p><p>43 If some IE system does not predict a field for a text fragment, then the corresponding cell in the merged template is empty. [sent-198, score-0.614]
</p><p>44 If a text fragment does not exist in the hand-filled template, then the corresponding cell in the last column is also empty. [sent-199, score-0.343]
</p><p>45 Table 4 shows an illustrative example of a merged template that has been constructed by the output T 1 , T 2 of two IE systems E 1 , E 2 , for the page of Table 2(a). [sent-200, score-0.386]
</p><p>46 Each entry corresponds to a text fragment that has been identified by at least one system. [sent-204, score-0.494]
</p><p>47 For two text fragments (“TransPort ZX”, “1GB”) the predicted fields by E 1 and E 2 differ. [sent-206, score-0.366]
</p><p>48 Comparing to the hand-filled template of Table 2(b), we conclude that “TransPort ZX” has been correctly identified as model only by E 1 , while E 2 identified the same fragment as manuf (the manufacturer of the laptop). [sent-207, score-0.729]
</p><p>49 On the other hand, the fragment “1GB” does not exist in the handfilled template. [sent-208, score-0.266]
</p><p>50 Therefore, the fields predicted by the two systems for this fragment are false. [sent-209, score-0.464]
</p><p>51 Furthermore, some text fragments have been identified by only one of the two IE systems. [sent-210, score-0.317]
</p><p>52 The fragment “15''” has been identified only by E 1 , while the fragment “40 GB” has been identified only by E 2 . [sent-211, score-0.78]
</p><p>53 The desirable result is to automatically fill the last column in the merged template of Table 4 with the correct fields. [sent-214, score-0.348]
</p><p>54 In other words, we would like to assign the correct field to each text fragment that has been identified by at least one base-level system. [sent-215, score-0.694]
</p><p>55 2 Majority Voting A simple idea for combining the predictions of different IE systems is to use majority voting: for each entry in the merged template, we count the predicted fields by the available systems and select the field with the highest count. [sent-217, score-0.652]
</p><p>56 Note that Table 4 contains missing values, reflecting the natural fact that some system may not have predicted a field for a text fragment that has been identified by another system. [sent-219, score-0.795]
</p><p>57 For example, if some system predicts an incorrect field f for a text fragment t ( s, e) , while the remaining systems do not predict any field at all, then ignoring missing values during voting harms precision, since the incorrect field is returned. [sent-221, score-1.272]
</p><p>58 An alternative is to record a missing value as “false”, providing evidence that no field should be predicted for t ( s, e ) . [sent-222, score-0.306]
</p><p>59 If the value with the highest count is “false” then no field is assigned to t ( s, e) . [sent-223, score-0.207]
</p><p>60 If, however, f is the correct field for t ( s, e) , interpreting the missing predictions by the remaining systems as “false” values harms overall extraction performance, since the correct field is rejected. [sent-224, score-0.658]
</p><p>61 Therefore, two different settings of majority voting are defined, depending on whether missing values are ignored or encoded as “false” values that indicate rejection of prediction. [sent-225, score-0.265]
</p><p>62 3 Voting Using Probabilities The voting with probabilities scheme that is presented in this section shares many features with multistrategy learning, as described in (Freitag, 2000) and was briefly outlined in Section 2. [sent-227, score-0.271]
</p><p>63 Multistrategy learning considers each field in isolation during combination and relies on the OPD constraint for improving the extraction accuracy, as demonstrated by the example of Table 1759  SIGLETOS, PALIOURAS, SPYROPOULOS AND HATZOPOULOS  3. [sent-232, score-0.269]
</p><p>64 On the other hand, voting using probabilities takes place on a merged template, like the one in Table 4, while no OPD assumption is required for any relevant field. [sent-233, score-0.328]
</p><p>65 This allows the case of contradictory field predictions among different systems during combination, as demonstrated in Table 4, where the fragment “1GB” has been identified as ram by the first system and as HDcapacity by the second one. [sent-234, score-0.82]
</p><p>66 2 two different settings of majority voting were defined, depending on whether absence of prediction by some system for a text fragment, i. [sent-238, score-0.291]
</p><p>67 Thus, two different settings for voting using probabilities are defined as follows: In the first setting, missing values are ignored, similar to the first setting of majority voting. [sent-243, score-0.261]
</p><p>68 Given a fragment t ( s, e) , the field f with the highest probabilistic estimate by those systems that have predicted a field for t ( s, e) is returned. [sent-244, score-0.76]
</p><p>69 Then a new stacking framework for IE is presented, along with an extension that relies on using probabilistic estimates on the output of the base-level systems. [sent-253, score-0.383]
</p><p>70 1 Motivation for Performing Learning Examining the merged template of Table 4, we wonder whether we can learn to predict the correct field, based on the fields predicted by the available systems, rather than simply voting. [sent-255, score-0.501]
</p><p>71 For example, if a system correctly predicts ram for the hypothetical fragment “1,5 GB”, while the other systems erroneously predict HDcapacity, then voting chooses the latter value. [sent-257, score-0.66]
</p><p>72 Therefore, it would be desirable to perform learning in order to induce a rule of the form: if the first IE system predicts “ram” and the other systems predict “HDcapacity”, then the correct field is “ram”. [sent-258, score-0.298]
</p><p>73 The idea suggested in this article is to create a feature vector for every row entry of the merged template, i. [sent-260, score-0.208]
</p><p>74 for each text fragment that has been identified by at least one base-level system. [sent-262, score-0.467]
</p><p>75 Table 5 shows the new feature vectors created by the merged template of Table 4. [sent-263, score-0.328]
</p><p>76 , procSpeed, ram, HDcapacity, HDcapacity,  Class model screenSize screenType false procName procSpeed ram false HDcapacity  Table 5. [sent-268, score-0.281]
</p><p>77 Feature vectors created by the merged template of Table 4. [sent-269, score-0.308]
</p><p>78 If a text fragment does not exist in the hand-filled template, the class attribute of the corresponding vector takes the value “false” that indicates rejection of prediction. [sent-272, score-0.372]
</p><p>79 Having specified the format of the feature vector, the remaining issue is to construct the full set of vectors for training the meta-level classifier using cross-validation, as described for the stacking framework in Section 2. [sent-274, score-0.47]
</p><p>80 This disparity between base-level and meta-level data sets is handled by sampling from documents during cross-validation, rather than from feature vectors as in stacking for classification. [sent-277, score-0.419]
</p><p>81 2 Stacking Using Nominal Values The key idea behind stacking for IE, is to learn a meta-level classifier based on the output of baselevel systems via cross-validation as follows: At the jth fold, j = 1. [sent-279, score-0.514]
</p><p>82 Figure 2 presents an algorithmic description of the new stacking framework for IE. [sent-306, score-0.363]
</p><p>83 A vector classified as “false” indicates that the corresponding fragment t ( s, e) does not exist in the hand-filled template, and thus the available base-level systems should not have predicted a field for it (for example the fragment “1 GB” in Table 5). [sent-308, score-0.858]
</p><p>84 f Q , false} = the correct field for t ( s, e) MD j = MD j ∪ vector < f 1 ,. [sent-327, score-0.227]
</p><p>85 1761  SIGLETOS, PALIOURAS, SPYROPOULOS AND HATZOPOULOS  The key difference among stacking for IE and common stacking is that cross-validation operates on text documents paired with hand-filled templates, instead of feature vectors labelled with class values. [sent-335, score-0.859]
</p><p>86 This removes the constraint of performing common classification at base-level, thus allowing the application of stacking to IE. [sent-336, score-0.422]
</p><p>87 The size of the meta-level data set is not a-priori known in stacking for IE, unlike common stacking where there is a one-to-one correspondence between base-level and meta-level vectors. [sent-344, score-0.726]
</p><p>88 In IE, however, a text fragment that is relevant to our task may not have been identified by any of the available base-level systems. [sent-352, score-0.499]
</p><p>89 In that case, there is no possibility of identifying that fragment at meta-level, since no feature vector is created and thus resulting in loss of information. [sent-353, score-0.286]
</p><p>90 This observation suggests that IE systems biased towards recall (percentage of the annotated field instances that were identified), should be generally preferred for combination, expecting to reduce the loss of information at meta-level. [sent-354, score-0.261]
</p><p>91 Moreover, a meta-level vector in common stacking for classification does not contain missing values, since each base-level classifier predicts a nominal class value or a probability distribution over the relevant classes. [sent-355, score-0.609]
</p><p>92 On the other hand, a meta-level vector in stacking for IE may contain missing values, as shown in Table 5, since some system may not have predicted a field for a fragment that has been identified by another system. [sent-356, score-1.081]
</p><p>93 Another issue in stacking for IE is the choice of J in the J -fold cross-validation process depicted in Figure 2. [sent-366, score-0.363]
</p><p>94 E N are used to identify relevant field instances and fill the corresponding templates T 1 . [sent-394, score-0.302]
</p><p>95 f Q then the corresponding instance < t(s, e), f > is inserted in the final template for d . [sent-416, score-0.24]
</p><p>96 At runtime, the stacking framework for IE is graphically depicted in Figure 3. [sent-419, score-0.363]
</p><p>97 1762  INFORMATION EXTRACTION USING VOTING AND STACKING  E1 E2  New document d  T1 T2  Merged template  Feature vectors  CM  Final template  T  … EN  TN  Figure 3. [sent-420, score-0.405]
</p><p>98 In contrast, both input and output in the runtime stacking for classification architecture of Figure 1(b), consist of a single feature vector. [sent-423, score-0.495]
</p><p>99 4 Stacking Using Probabilities A straightforward extension of stacking with nominal values is to rely on the confidence scores by the base-level systems that have been converted into probabilistic estimates of correctness. [sent-425, score-0.479]
</p><p>100 The new framework is described as follows: • Instead of predicting one of the Q relevant fields for each fragment t ( s, e) , each system generates a confidence score c k for the predicted field f k . [sent-426, score-0.766]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ie', 0.628), ('stacking', 0.363), ('fragment', 0.266), ('field', 0.207), ('template', 0.185), ('voting', 0.173), ('laptop', 0.128), ('identified', 0.124), ('merged', 0.123), ('ram', 0.123), ('fields', 0.118), ('fragments', 0.116), ('freitag', 0.116), ('hdcapacity', 0.098), ('multistrategy', 0.098), ('classifier', 0.087), ('gb', 0.083), ('opd', 0.079), ('false', 0.079), ('text', 0.077), ('procname', 0.069), ('procspeed', 0.069), ('screentype', 0.069), ('sigletos', 0.069), ('confidence', 0.066), ('mb', 0.063), ('extraction', 0.062), ('ciravegna', 0.059), ('hatzopoulos', 0.059), ('paliouras', 0.059), ('spyropoulos', 0.059), ('transport', 0.059), ('classification', 0.059), ('predicted', 0.055), ('intel', 0.053), ('predictions', 0.053), ('pentium', 0.051), ('md', 0.051), ('classifiers', 0.05), ('screensize', 0.049), ('tft', 0.049), ('missing', 0.044), ('templates', 0.043), ('zx', 0.04), ('classified', 0.039), ('georgios', 0.039), ('kushmerick', 0.039), ('lm', 0.039), ('stacked', 0.039), ('article', 0.038), ('final', 0.037), ('documents', 0.036), ('document', 0.035), ('runtime', 0.033), ('token', 0.033), ('processor', 0.033), ('mhz', 0.033), ('wrappers', 0.033), ('page', 0.033), ('relevant', 0.032), ('athens', 0.03), ('bwi', 0.03), ('filled', 0.03), ('manuf', 0.03), ('predefined', 0.03), ('rejection', 0.029), ('annotated', 0.029), ('hypothetical', 0.027), ('entry', 0.027), ('table', 0.026), ('web', 0.025), ('scores', 0.025), ('systems', 0.025), ('defined', 0.025), ('populated', 0.025), ('predicts', 0.024), ('system', 0.022), ('tags', 0.022), ('trained', 0.021), ('modelling', 0.021), ('correct', 0.02), ('feature', 0.02), ('output', 0.02), ('califf', 0.02), ('constantine', 0.02), ('fill', 0.02), ('foreach', 0.02), ('greece', 0.02), ('harms', 0.02), ('intensified', 0.02), ('michalis', 0.02), ('muslea', 0.02), ('screen', 0.02), ('sdram', 0.02), ('shallow', 0.02), ('sonderland', 0.02), ('majority', 0.019), ('behind', 0.019), ('motivation', 0.019), ('inserted', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="21-tfidf-1" href="./jmlr-2005-Combining_Information_Extraction_Systems_Using_Voting_and_Stacked_Generalization.html">21 jmlr-2005-Combining Information Extraction Systems Using Voting and Stacked Generalization</a></p>
<p>Author: Georgios Sigletos, Georgios Paliouras, Constantine D. Spyropoulos, Michalis Hatzopoulos</p><p>Abstract: This article investigates the effectiveness of voting and stacked generalization -also known as stacking- in the context of information extraction (IE). A new stacking framework is proposed that accommodates well-known approaches for IE. The key idea is to perform cross-validation on the base-level data set, which consists of text documents annotated with relevant information, in order to create a meta-level data set that consists of feature vectors. A classifier is then trained using the new vectors. Therefore, base-level IE systems are combined with a common classifier at the metalevel. Various voting schemes are presented for comparing against stacking in various IE domains. Well known IE systems are employed at the base-level, together with a variety of classifiers at the meta-level. Results show that both voting and stacking work better when relying on probabilistic estimates by the base-level systems. Voting proved to be effective in most domains in the experiments. Stacking, on the other hand, proved to be consistently effective over all domains, doing comparably or better than voting and always better than the best base-level systems. Particular emphasis is also given to explaining the results obtained by voting and stacking at the meta-level, with respect to the varying degree of similarity in the output of the base-level systems. Keywords: stacking, voting, information extraction, cross-validation 1</p><p>2 0.027965039 <a title="21-tfidf-2" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>Author: Joseph F. Murray, Gordon F. Hughes, Kenneth Kreutz-Delgado</p><p>Abstract: We compare machine learning methods applied to a difﬁcult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classiﬁer (mi-NB) which is speciﬁcally designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum signiﬁcance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures. Keywords: hard drive failure prediction, rank-sum test, support vector machines (SVM), exact nonparametric statistics, multiple instance naive-Bayes</p><p>3 0.024181549 <a title="21-tfidf-3" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>Author: Rie Kubota Ando, Tong Zhang</p><p>Abstract: One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don’t have a complete understanding of their eﬀectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Speciﬁcally we consider learning predictive structures on hypothesis spaces (that is, what kind of classiﬁers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the eﬀectiveness of the proposed algorithms in the semi-supervised learning setting.</p><p>4 0.024138046 <a title="21-tfidf-4" href="./jmlr-2005-A_Bayes_Optimal_Approach_for_Partitioning_the_Values_of_Categorical_Attributes.html">1 jmlr-2005-A Bayes Optimal Approach for Partitioning the Values of Categorical Attributes</a></p>
<p>Author: Marc Boullé</p><p>Abstract: In supervised machine learning, the partitioning of the values (also called grouping) of a categorical attribute aims at constructing a new synthetic attribute which keeps the information of the initial attribute and reduces the number of its values. In this paper, we propose a new grouping method MODL1 founded on a Bayesian approach. The method relies on a model space of grouping models and on a prior distribution defined on this model space. This results in an evaluation criterion of grouping, which is minimal for the most probable grouping given the data, i.e. the Bayes optimal grouping. We propose new super-linear optimization heuristics that yields near-optimal groupings. Extensive comparative experiments demonstrate that the MODL grouping method builds high quality groupings in terms of predictive quality, robustness and small number of groups. Keywords: data preparation, grouping, Bayesianism, model selection, classification, naïve Bayes 1</p><p>5 0.024063375 <a title="21-tfidf-5" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>Author: Lior Wolf, Amnon Shashua</p><p>Abstract: The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classiﬁcation tasks, for example, it is not uncommon to have 104 to 107 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a deﬁnition of “relevancy” based on spectral properties of the Laplacian of the features’ measurement matrix. The feature selection process is then based on a continuous ranking of the features deﬁned by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a “biased non-negativity” of a key matrix in the process. As a result, a simple leastsquares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.</p><p>6 0.023432879 <a title="21-tfidf-6" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>7 0.022211662 <a title="21-tfidf-7" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>8 0.021235036 <a title="21-tfidf-8" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>9 0.017664047 <a title="21-tfidf-9" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>10 0.016274493 <a title="21-tfidf-10" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>11 0.015585806 <a title="21-tfidf-11" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>12 0.014369037 <a title="21-tfidf-12" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>13 0.013889803 <a title="21-tfidf-13" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<p>14 0.013597711 <a title="21-tfidf-14" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>15 0.013003636 <a title="21-tfidf-15" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>16 0.012115067 <a title="21-tfidf-16" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>17 0.011630906 <a title="21-tfidf-17" href="./jmlr-2005-Efficient_Computation_of_Gapped_Substring_Kernels_on_Large_Alphabets.html">28 jmlr-2005-Efficient Computation of Gapped Substring Kernels on Large Alphabets</a></p>
<p>18 0.010923137 <a title="21-tfidf-18" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>19 0.010738623 <a title="21-tfidf-19" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>20 0.010021875 <a title="21-tfidf-20" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.07), (1, 0.027), (2, 0.032), (3, -0.04), (4, 0.052), (5, -0.03), (6, -0.03), (7, 0.004), (8, 0.031), (9, -0.042), (10, 0.012), (11, -0.07), (12, 0.055), (13, -0.156), (14, 0.007), (15, -0.012), (16, 0.15), (17, -0.103), (18, 0.024), (19, 0.15), (20, 0.163), (21, 0.167), (22, -0.081), (23, 0.188), (24, -0.0), (25, 0.315), (26, 0.429), (27, 0.212), (28, -0.296), (29, -0.062), (30, -0.147), (31, 0.021), (32, -0.489), (33, 0.046), (34, -0.16), (35, -0.019), (36, -0.057), (37, -0.099), (38, -0.147), (39, -0.018), (40, 0.066), (41, -0.135), (42, -0.016), (43, 0.087), (44, -0.024), (45, -0.031), (46, 0.011), (47, 0.038), (48, 0.056), (49, -0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99068713 <a title="21-lsi-1" href="./jmlr-2005-Combining_Information_Extraction_Systems_Using_Voting_and_Stacked_Generalization.html">21 jmlr-2005-Combining Information Extraction Systems Using Voting and Stacked Generalization</a></p>
<p>Author: Georgios Sigletos, Georgios Paliouras, Constantine D. Spyropoulos, Michalis Hatzopoulos</p><p>Abstract: This article investigates the effectiveness of voting and stacked generalization -also known as stacking- in the context of information extraction (IE). A new stacking framework is proposed that accommodates well-known approaches for IE. The key idea is to perform cross-validation on the base-level data set, which consists of text documents annotated with relevant information, in order to create a meta-level data set that consists of feature vectors. A classifier is then trained using the new vectors. Therefore, base-level IE systems are combined with a common classifier at the metalevel. Various voting schemes are presented for comparing against stacking in various IE domains. Well known IE systems are employed at the base-level, together with a variety of classifiers at the meta-level. Results show that both voting and stacking work better when relying on probabilistic estimates by the base-level systems. Voting proved to be effective in most domains in the experiments. Stacking, on the other hand, proved to be consistently effective over all domains, doing comparably or better than voting and always better than the best base-level systems. Particular emphasis is also given to explaining the results obtained by voting and stacking at the meta-level, with respect to the varying degree of similarity in the output of the base-level systems. Keywords: stacking, voting, information extraction, cross-validation 1</p><p>2 0.088245563 <a title="21-lsi-2" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>Author: Hyunsoo Kim, Peg Howland, Haesun Park</p><p>Abstract: Support vector machines (SVMs) have been recognized as one of the most successful classiﬁcation methods for many applications including text classiﬁcation. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efﬁciently handle a large number of terms in practical applications of text classiﬁcation. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classiﬁcation algorithm and support vector classiﬁers to handle the classiﬁcation problem where a document may belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efﬁciency for both training and testing can be achieved without sacriﬁcing prediction accuracy of text classiﬁcation even when the dimension of the input space is signiﬁcantly reduced. Keywords: dimension reduction, support vector machines, text classiﬁcation, linear discriminant analysis, centroids</p><p>3 0.071851477 <a title="21-lsi-3" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>Author: Ofer Dekel, Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We describe new loss functions for regression problems along with an accompanying algorithmic framework which utilizes these functions. These loss functions are derived by symmetrization of margin-based losses commonly used in boosting algorithms, namely, the logistic loss and the exponential loss. The resulting symmetric logistic loss can be viewed as a smooth approximation to the ε-insensitive hinge loss used in support vector regression. We describe and analyze two parametric families of batch learning algorithms for minimizing these symmetric losses. The ﬁrst family employs an iterative log-additive update which can be viewed as a regression counterpart to recent boosting algorithms. The second family utilizes an iterative additive update step. We also describe and analyze online gradient descent (GD) and exponentiated gradient (EG) algorithms for the symmetric logistic loss. A byproduct of our work is a new simple form of regularization for boosting-based classiﬁcation and regression algorithms. Our regression framework also has implications on classiﬁcation algorithms, namely, a new additive update boosting algorithm for classiﬁcation. We demonstrate the merits of our algorithms in a series of experiments.</p><p>4 0.071745649 <a title="21-lsi-4" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>Author: Neil Lawrence</p><p>Abstract: Summarising a high dimensional data set with a low dimensional embedding is a standard approach for exploring its structure. In this paper we provide an overview of some existing techniques for discovering such embeddings. We then introduce a novel probabilistic interpretation of principal component analysis (PCA) that we term dual probabilistic PCA (DPPCA). The DPPCA model has the additional advantage that the linear mappings from the embedded space can easily be nonlinearised through Gaussian processes. We refer to this model as a Gaussian process latent variable model (GP-LVM). Through analysis of the GP-LVM objective function, we relate the model to popular spectral techniques such as kernel PCA and multidimensional scaling. We then review a practical algorithm for GP-LVMs in the context of large data sets and develop it to also handle discrete valued data and missing attributes. We demonstrate the model on a range of real-world and artiﬁcially generated data sets. Keywords: Gaussian processes, latent variable models, principal component analysis, spectral methods, unsupervised learning, visualisation</p><p>5 0.070259713 <a title="21-lsi-5" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>Author: Rie Kubota Ando, Tong Zhang</p><p>Abstract: One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don’t have a complete understanding of their eﬀectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Speciﬁcally we consider learning predictive structures on hypothesis spaces (that is, what kind of classiﬁers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the eﬀectiveness of the proposed algorithms in the semi-supervised learning setting.</p><p>6 0.068968944 <a title="21-lsi-6" href="./jmlr-2005-An_MDP-Based_Recommender_System.html">12 jmlr-2005-An MDP-Based Recommender System</a></p>
<p>7 0.065262489 <a title="21-lsi-7" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>8 0.05834873 <a title="21-lsi-8" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>9 0.052586183 <a title="21-lsi-9" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>10 0.050367333 <a title="21-lsi-10" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>11 0.049293783 <a title="21-lsi-11" href="./jmlr-2005-A_Bayes_Optimal_Approach_for_Partitioning_the_Values_of_Categorical_Attributes.html">1 jmlr-2005-A Bayes Optimal Approach for Partitioning the Values of Categorical Attributes</a></p>
<p>12 0.043760542 <a title="21-lsi-12" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>13 0.042210702 <a title="21-lsi-13" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>14 0.03996167 <a title="21-lsi-14" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<p>15 0.039075892 <a title="21-lsi-15" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>16 0.038818456 <a title="21-lsi-16" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>17 0.038537502 <a title="21-lsi-17" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>18 0.036398288 <a title="21-lsi-18" href="./jmlr-2005-Active_Coevolutionary_Learning_of_Deterministic_Finite_Automata.html">8 jmlr-2005-Active Coevolutionary Learning of Deterministic Finite Automata</a></p>
<p>19 0.035731371 <a title="21-lsi-19" href="./jmlr-2005-Efficient_Computation_of_Gapped_Substring_Kernels_on_Large_Alphabets.html">28 jmlr-2005-Efficient Computation of Gapped Substring Kernels on Large Alphabets</a></p>
<p>20 0.034651399 <a title="21-lsi-20" href="./jmlr-2005-Generalization_Bounds_and_Complexities_Based_on_Sparsity_and_Clustering_for_Convex_Combinations_of_Functions_from_Random_Classes.html">37 jmlr-2005-Generalization Bounds and Complexities Based on Sparsity and Clustering for Convex Combinations of Functions from Random Classes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(17, 0.028), (19, 0.012), (36, 0.02), (37, 0.017), (42, 0.647), (43, 0.039), (52, 0.052), (59, 0.011), (70, 0.02), (88, 0.034), (94, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91974503 <a title="21-lda-1" href="./jmlr-2005-Combining_Information_Extraction_Systems_Using_Voting_and_Stacked_Generalization.html">21 jmlr-2005-Combining Information Extraction Systems Using Voting and Stacked Generalization</a></p>
<p>Author: Georgios Sigletos, Georgios Paliouras, Constantine D. Spyropoulos, Michalis Hatzopoulos</p><p>Abstract: This article investigates the effectiveness of voting and stacked generalization -also known as stacking- in the context of information extraction (IE). A new stacking framework is proposed that accommodates well-known approaches for IE. The key idea is to perform cross-validation on the base-level data set, which consists of text documents annotated with relevant information, in order to create a meta-level data set that consists of feature vectors. A classifier is then trained using the new vectors. Therefore, base-level IE systems are combined with a common classifier at the metalevel. Various voting schemes are presented for comparing against stacking in various IE domains. Well known IE systems are employed at the base-level, together with a variety of classifiers at the meta-level. Results show that both voting and stacking work better when relying on probabilistic estimates by the base-level systems. Voting proved to be effective in most domains in the experiments. Stacking, on the other hand, proved to be consistently effective over all domains, doing comparably or better than voting and always better than the best base-level systems. Particular emphasis is also given to explaining the results obtained by voting and stacking at the meta-level, with respect to the varying degree of similarity in the output of the base-level systems. Keywords: stacking, voting, information extraction, cross-validation 1</p><p>2 0.84272492 <a title="21-lda-2" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>Author: Hyunsoo Kim, Peg Howland, Haesun Park</p><p>Abstract: Support vector machines (SVMs) have been recognized as one of the most successful classiﬁcation methods for many applications including text classiﬁcation. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efﬁciently handle a large number of terms in practical applications of text classiﬁcation. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classiﬁcation algorithm and support vector classiﬁers to handle the classiﬁcation problem where a document may belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efﬁciency for both training and testing can be achieved without sacriﬁcing prediction accuracy of text classiﬁcation even when the dimension of the input space is signiﬁcantly reduced. Keywords: dimension reduction, support vector machines, text classiﬁcation, linear discriminant analysis, centroids</p><p>3 0.30400848 <a title="21-lda-3" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>Author: Jieping Ye</p><p>Abstract: A generalized discriminant analysis based on a new optimization criterion is presented. The criterion extends the optimization criteria of the classical Linear Discriminant Analysis (LDA) when the scatter matrices are singular. An efﬁcient algorithm for the new optimization problem is presented. The solutions to the proposed criterion form a family of algorithms for generalized LDA, which can be characterized in a closed form. We study two speciﬁc algorithms, namely Uncorrelated LDA (ULDA) and Orthogonal LDA (OLDA). ULDA was previously proposed for feature extraction and dimension reduction, whereas OLDA is a novel algorithm proposed in this paper. The features in the reduced space of ULDA are uncorrelated, while the discriminant vectors of OLDA are orthogonal to each other. We have conducted a comparative study on a variety of real-world data sets to evaluate ULDA and OLDA in terms of classiﬁcation accuracy. Keywords: dimension reduction, linear discriminant analysis, uncorrelated LDA, orthogonal LDA, singular value decomposition</p><p>4 0.2257721 <a title="21-lda-4" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>Author: Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, Suvrit Sra</p><p>Abstract: Several large scale data mining applications, such as text categorization and gene expression analysis, involve high-dimensional data that is also inherently directional in nature. Often such data is L2 normalized so that it lies on the surface of a unit hypersphere. Popular models such as (mixtures of) multi-variate Gaussians are inadequate for characterizing such data. This paper proposes a generative mixture-model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. In particular, we derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the mean and concentration parameters of this mixture. Numerical estimation of the concentration parameters is non-trivial in high dimensions since it involves functional inversion of ratios of Bessel functions. We also formulate two clustering algorithms corresponding to the variants of EM that we derive. Our approach provides a theoretical basis for the use of cosine similarity that has been widely employed by the information retrieval community, and obtains the spherical kmeans algorithm (kmeans with cosine similarity) as a special case of both variants. Empirical results on clustering of high-dimensional text and gene-expression data based on a mixture of vMF distributions show that the ability to estimate the concentration parameter for each vMF component, which is not present in existing approaches, yields superior results, especially for difﬁcult clustering tasks in high-dimensional spaces. Keywords: clustering, directional distributions, mixtures, von Mises-Fisher, expectation maximization, maximum likelihood, high dimensional data c 2005 Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh and Suvrit Sra. BANERJEE , D HILLON , G HOSH AND S RA</p><p>5 0.19480295 <a title="21-lda-5" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>Author: Aharon Bar-Hillel, Tomer Hertz, Noam Shental, Daphna Weinshall</p><p>Abstract: Many learning algorithms use a metric deﬁned over the input space as a principal tool, and their performance critically depends on the quality of this metric. We address the problem of learning metrics using side-information in the form of equivalence constraints. Unlike labels, we demonstrate that this type of side-information can sometimes be automatically obtained without the need of human intervention. We show how such side-information can be used to modify the representation of the data, leading to improved clustering and classiﬁcation. Speciﬁcally, we present the Relevant Component Analysis (RCA) algorithm, which is a simple and efﬁcient algorithm for learning a Mahalanobis metric. We show that RCA is the solution of an interesting optimization problem, founded on an information theoretic basis. If dimensionality reduction is allowed within RCA, we show that it is optimally accomplished by a version of Fisher’s linear discriminant that uses constraints. Moreover, under certain Gaussian assumptions, RCA can be viewed as a Maximum Likelihood estimation of the within class covariance matrix. We conclude with extensive empirical evaluations of RCA, showing its advantage over alternative methods. Keywords: clustering, metric learning, dimensionality reduction, equivalence constraints, side information.</p><p>6 0.18610239 <a title="21-lda-6" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>7 0.18433055 <a title="21-lda-7" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>8 0.18116777 <a title="21-lda-8" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>9 0.17595956 <a title="21-lda-9" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>10 0.17540149 <a title="21-lda-10" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>11 0.17505002 <a title="21-lda-11" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>12 0.17330046 <a title="21-lda-12" href="./jmlr-2005-Efficient_Computation_of_Gapped_Substring_Kernels_on_Large_Alphabets.html">28 jmlr-2005-Efficient Computation of Gapped Substring Kernels on Large Alphabets</a></p>
<p>13 0.17030041 <a title="21-lda-13" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>14 0.16912392 <a title="21-lda-14" href="./jmlr-2005-Frames%2C_Reproducing_Kernels%2C_Regularization_and_Learning.html">35 jmlr-2005-Frames, Reproducing Kernels, Regularization and Learning</a></p>
<p>15 0.16730621 <a title="21-lda-15" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>16 0.16593665 <a title="21-lda-16" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>17 0.16555665 <a title="21-lda-17" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>18 0.16263559 <a title="21-lda-18" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>19 0.158636 <a title="21-lda-19" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>20 0.15790911 <a title="21-lda-20" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
