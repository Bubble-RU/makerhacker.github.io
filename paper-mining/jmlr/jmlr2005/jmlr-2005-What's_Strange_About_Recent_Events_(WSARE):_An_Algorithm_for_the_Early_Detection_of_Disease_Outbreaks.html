<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-72" href="#">jmlr2005-72</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</h1>
<br/><p>Source: <a title="jmlr-2005-72-pdf" href="http://jmlr.org/papers/volume6/wong05a/wong05a.pdf">pdf</a></p><p>Author: Weng-Keen Wong, Andrew Moore, Gregory Cooper, Michael Wagner</p><p>Abstract: Traditional biosurveillance algorithms detect disease outbreaks by looking for peaks in a univariate time series of health-care data. Current health-care surveillance data, however, are no longer simply univariate data streams. Instead, a wealth of spatial, temporal, demographic and symptomatic information is available. We present an early disease outbreak detection algorithm called What’s Strange About Recent Events (WSARE), which uses a multivariate approach to improve its timeliness of detection. WSARE employs a rule-based technique that compares recent health-care data against data from a baseline distribution and ﬁnds subgroups of the recent data whose proportions have changed the most from the baseline data. In addition, health-care data also pose difﬁculties for surveillance algorithms because of inherent temporal trends such as seasonal effects and day of week variations. WSARE approaches this problem using a Bayesian network to produce a baseline distribution that accounts for these temporal trends. The algorithm itself incorporates a wide range of ideas, including association rules, Bayesian networks, hypothesis testing and permutation tests to produce a detection algorithm that is careful to evaluate the signiﬁcance of the alarms that it raises. Keywords: anomaly detection, syndromic surveillance, biosurveillance, Bayesian networks, applications</p><p>Reference: <a title="jmlr-2005-72-reference" href="../jmlr2005_reference/jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We present an early disease outbreak detection algorithm called What’s Strange About Recent Events (WSARE), which uses a multivariate approach to improve its timeliness of detection. [sent-12, score-0.455]
</p><p>2 In addition, health-care data also pose difﬁculties for surveillance algorithms because of inherent temporal trends such as seasonal effects and day of week variations. [sent-14, score-0.515]
</p><p>3 In a similar manner, we would like to tackle the problem of early disease outbreak detection, in which the disease outbreak can be due to either natural causes or a bioterrorist attack. [sent-23, score-0.624]
</p><p>4 One of the challenges for early disease outbreak detection is ﬁnding readily available data that contains a useful signal (Tsui et al. [sent-24, score-0.455]
</p><p>5 Although we have posed our problem in an anomaly detection framework, traditional anomaly detection algorithms are inappropriate for this domain. [sent-39, score-0.364]
</p><p>6 Another common approach to early outbreak detection is to convert the multivariate ED database into a univariate time series by aggregating daily counts of a certain attribute or combination of attributes. [sent-46, score-0.499]
</p><p>7 This technique works well if we know beforehand which disease to monitor, since we can improve the timeliness of detection by monitoring speciﬁc attributes of the disease. [sent-49, score-0.342]
</p><p>8 Our approach to early disease outbreak detection uses a rule-based anomaly pattern detector called What’s Strange About Recent Events (WSARE) (Wong et al. [sent-54, score-0.524]
</p><p>9 0 if the current day is December 30, 2003  1963  W ONG , M OORE , C OOPER AND WAGNER  The basic question asked by all detection systems is whether anything strange has occurred in recent events. [sent-77, score-0.397]
</p><p>10 Our algorithm considers all patient records falling on the current day under evaluation to be recent events. [sent-79, score-0.423]
</p><p>11 0, baseline behavior is assumed to be captured by raw historical data from the same day of the week in order to avoid environmental effects such as weekend versus weekday differences in the number of ED cases. [sent-83, score-0.662]
</p><p>12 This distance is required in case an outbreak happens on the current day but it remains undetected. [sent-87, score-0.47]
</p><p>13 If the baseline period is too close to the current day, the baseline period will quickly incorporate the outbreak cases as time progresses. [sent-88, score-0.652]
</p><p>14 0 below, we assume that baseline behavior is captured by records that are in the set baseline days. [sent-90, score-0.44]
</p><p>15 Typically, baseline days contains the days that are 35, 42, 49, and 56 days prior to the day under consideration. [sent-91, score-0.707]
</p><p>16 We will refer to the events that ﬁt a certain rule for the current day as Crecent . [sent-95, score-0.337]
</p><p>17 WSARE ﬁrst ﬁnds the best scoring rule over events occurring on the current day using a greedy search. [sent-114, score-0.371]
</p><p>18 The score of a rule is determined by comparing the events on the current day against events in the past. [sent-117, score-0.405]
</p><p>19 More speciﬁcally, we are comparing if the ratio between certain events on the current day and the total number of events on the current day differ dramatically between the recent period and the past. [sent-118, score-0.622]
</p><p>20 Following the score calculation, the best rule for that day has its p-value estimated by a randomization test. [sent-119, score-0.401]
</p><p>21 The p-value for a rule is the likelihood of 1965  W ONG , M OORE , C OOPER AND WAGNER  ﬁnding a rule with as good a score under the hypothesis that the date and the other attributes are independent. [sent-120, score-0.328]
</p><p>22 For each day i in the surveillance period, retrieve the records belonging to DBi . [sent-133, score-0.462]
</p><p>23 Consequently, the case attributes in the data set DBi remain the same for each record but the date ﬁeld is shufﬂed between records from the current day and records from ﬁve to eight weeks ago. [sent-228, score-0.689]
</p><p>24 In addition, let there be NT records for today and NB records for the baseline period. [sent-241, score-0.5]
</p><p>25 On a typical data set where an outbreak is unlikely, the majority of days will result in insigniﬁcant p-values. [sent-259, score-0.326]
</p><p>26 When using our algorithm on a day-to-day basis, the compensated p-value CPVi obtained for the current day through the randomization tests can be interpreted at face value. [sent-263, score-0.361]
</p><p>27 Under this assumption, detection algorithms operate by subtracting away the baseline from recent data and raising an alarm if the deviations from the baseline are signiﬁcant. [sent-281, score-0.482]
</p><p>28 Disease outbreak detectors intended to detect epidemics such as SARS, West Nile Virus and anthrax are not interested in detecting the onset of ﬂu season and would be thrown off by it. [sent-286, score-0.476]
</p><p>29 If we choose the baseline distribution to be outside of the current ﬂu season, then a comparison with recent data will trigger many false anthrax alerts due to the ﬂu cases. [sent-294, score-0.406]
</p><p>30 For example, we chose data from 35, 42, 49, and 56 days prior to the current day under examination. [sent-308, score-0.364]
</p><p>31 These dates were chosen to incorporate enough data so that seasonal trends could be captured and they were also chosen to avoid weekend versus weekday effects by making all comparisons from the same day of week. [sent-309, score-0.336]
</p><p>32 These attributes are precisely those attributes that account for trends in the data, such as the season, the current ﬂu level and the day of week. [sent-315, score-0.511]
</p><p>33 1 Creating the Baseline Distribution Learning the baseline distribution involves taking all records prior to the past 24 hours and building a Bayesian network from this subset. [sent-317, score-0.32]
</p><p>34 If there are any latent environmental attributes 1971  W ONG , M OORE , C OOPER AND WAGNER  that are not accounted for in this model, the detection algorithm may have some difﬁculties. [sent-320, score-0.381]
</p><p>35 We have often referred to environmental attributes as attributes that cause periodic trends. [sent-331, score-0.341]
</p><p>36 For example, suppose we detect that a botulism outbreak has occurred and we would still like to be on alert for any anthrax releases. [sent-333, score-0.394]
</p><p>37 The data set corresponding to the records from the past 24 hours of the current day will be named DBrecent . [sent-353, score-0.396]
</p><p>38 0 assumes that the baseline distribution remains relatively stable, with the environmental attributes accounting for the only sources of variation. [sent-366, score-0.387]
</p><p>39 If the current day is less than a year after the ﬁrst case date, we consider that hospital to have insufﬁcient historical data for the baseline and we ignore all records from that hospital. [sent-383, score-0.714]
</p><p>40 In order to produce the baseline data set, we sample a total of 10000 records from all the hospital Bayesian networks. [sent-385, score-0.409]
</p><p>41 Let hospital h have nh records on the current day and suppose there are H hospitals with sufﬁcient historical data for the current date. [sent-386, score-0.638]
</p><p>42 Evaluation Validation of early outbreak detection algorithms is generally a difﬁcult task due to the type of data required. [sent-394, score-0.359]
</p><p>43 The environment of the city is not static, with weather, ﬂu levels and food conditions in the city changing from day to day. [sent-410, score-0.363]
</p><p>44 A bad food condition facilitates the outbreak of food poisoning in the area. [sent-415, score-0.358]
</p><p>45 The anthrax concentration remains high for the affected region for each subsequent day with an 80% probability. [sent-429, score-0.408]
</p><p>46 Two of the environmental variables, namely Region Grassiness and Region Food Condition, are hidden from the detection algorithm while the remaining environmental attributes are observed. [sent-441, score-0.516]
</p><p>47 We choose to hide these two attributes because the remaining four attributes that are observed are typically considered when trying to account for temporal trends in biosurveillance data. [sent-442, score-0.35]
</p><p>48 Only the attributes in Figure 5 labelled with uppercase letters are recorded, resulting in a great deal of information being hidden from the detection algorithm, including some latent environmental attributes. [sent-453, score-0.381]
</p><p>49 The detection algorithms train on data from the ﬁrst year until the day being monitored while the second year is used for evaluation. [sent-458, score-0.371]
</p><p>50 Figure 6 plots the total count of health-care cases on each day during the evaluation period while Figure 7 plots the total count of health-care cases involving respiratory symptoms for the same simulated data set. [sent-461, score-0.418]
</p><p>51 A naive detection algorithm would assume that the highest peak in this graph would be the date of the anthrax release. [sent-462, score-0.336]
</p><p>52 However, the anthrax release occurs on day index 74,409, which is clearly not the highest peak in either graph. [sent-463, score-0.42]
</p><p>53 Consequently, we only use data sets with more than eight reported anthrax cases on any day during the attack period. [sent-465, score-0.373]
</p><p>54 This detector determines the mean and variance of the total number of records on each day in the PS data set during the training period. [sent-471, score-0.4]
</p><p>55 An alarm level is generated by ﬁtting a Gaussian to data prior to the current day and obtaining a p-value for 1977  W ONG , M OORE , C OOPER AND WAGNER  the current day’s count. [sent-477, score-0.321]
</p><p>56 0 is also evaluated, using a baseline distribution of records from 35, 42, 49 and 56 days before the current day. [sent-485, score-0.427]
</p><p>57 For instance, suppose that the current day is the ﬁrst day of fall, making the environmental attribute Season = Fall. [sent-491, score-0.643]
</p><p>58 0 would notice that 100% of the records for the current day have Season = Fall while 0% of the records in the baseline data set match this rule. [sent-494, score-0.687]
</p><p>59 5 simply builds a baseline from all records prior to the current period with their environmental attributes equal to the current day’s. [sent-497, score-0.637]
</p><p>60 To clarify this algorithm, suppose for the current day we have the following values of these environmental attributes: Flu Level = High, Season = Winter, Day o f Week = Weekday and Weather = Cold. [sent-499, score-0.389]
</p><p>61 Then DBbaseline would contain only records before the current period with environmental attributes having exactly these values. [sent-500, score-0.462]
</p><p>62 5 can not make an informed decision when comparing the current day to the baseline and simply reports nothing for the current day. [sent-503, score-0.429]
</p><p>63 0 would detect the simulated anthrax outbreak sooner than WSARE 2. [sent-509, score-0.419]
</p><p>64 On the AMOC curves to follow, the x-axis indicates the number of false positives per month while the y-axis measures the detection time in days. [sent-522, score-0.335]
</p><p>65 Suppose there are two such alarms, with one alarm appearing 5 days before the simulated anthrax release, which would be considered a false positive, and the other appearing 3 days after the release, making the detection time 3 days. [sent-529, score-0.66]
</p><p>66 We add a one day delay to all detection times to simulate reality where current data is only available after a 24 hour delay. [sent-544, score-0.397]
</p><p>67 0 clearly outperform the univariate algorithms when the univariate algorithms operate on the total daily counts and also when the univariate algorithms operate on the daily counts of cases involving respiratory symptoms. [sent-549, score-0.337]
</p><p>68 Figures 11 to 14 illustrate the various outbreak sizes in the simulated data by plotting the number of anthrax cases per day during the outbreak period. [sent-579, score-0.83]
</p><p>69 Figure 11 represents a large scale outbreak which was easily detected on the ﬁrst day by most algorithms. [sent-582, score-0.444]
</p><p>70 0 detects the outbreak in Figure 13 on the third day with a very insensitive alarm threshold of 0. [sent-590, score-0.485]
</p><p>71 We use this model to generate 150 days worth of data in which each day contains 1000 records and each record contains 100 attributes. [sent-611, score-0.48]
</p><p>72 The environmental attributes used are month, day of week and the number of cases from the previous day with respiratory problems. [sent-682, score-0.837]
</p><p>73 However, by using a short enough baseline period, such as the standard baseline of 35, 42, 49, and 56 days prior to the current date, we can capture fairly recent trends and deal with a changing distribution as new hospitals submit data. [sent-729, score-0.532]
</p><p>74 We speculate that the missing hospital IDs in rules 3-14 are due to hospital 14 coming online and a new hospital code not being available. [sent-758, score-0.404]
</p><p>75 Both of these rules are unlikely to have been caused by environmental trends; they are simply anomalous patterns when compared against the baseline of WSARE 2. [sent-768, score-0.41]
</p><p>76 The attributes in this data set include the visit date, area code, ICD-9 code, age category, and day of week. [sent-777, score-0.39]
</p><p>77 The day of week was used as the only environmental attribute. [sent-778, score-0.435]
</p><p>78 The rules that characterized the two anomalous patterns consisted of the same three attributes of ICD-9 code, area code and age category, indicating that an anomalous pattern was found involving children aged 6-14 having viral symptoms within a speciﬁc geographic area. [sent-785, score-0.429]
</p><p>79 0 detected the outbreak on the second day from its onset. [sent-787, score-0.444]
</p><p>80 0 promising and concluded that the algorithm was indeed able to detect an actual outbreak in syndromic surveillance data. [sent-790, score-0.388]
</p><p>81 0 assumes that the environmental attributes are the only source of variation in the baseline distribution. [sent-815, score-0.387]
</p><p>82 5 because the algorithm is unable to make predictions for days in which the combination of environmental attributes do not exist in historical data. [sent-821, score-0.399]
</p><p>83 In computer security, anomaly detection has been most prominent in intrusion detection systems, which identify intrusions by distinguishing between normal system behavior and behavior when security has been compromised (Lane and Brodley, 1999; Warrender et al. [sent-858, score-0.368]
</p><p>84 Univariate algorithms based on regression and time series models, on the other hand, are able to model explicitly the seasonal and day of week effects in the data. [sent-908, score-0.33]
</p><p>85 A Poisson regression model that included a day of week term as a covariate was demonstrated to be a fairly capable detector in (Buckeridge et al. [sent-910, score-0.33]
</p><p>86 , 2004) is an algorithm speciﬁcally designed to detect an outbreak of inhalational anthrax due to atmospheric dispersion of anthrax spores. [sent-927, score-0.539]
</p><p>87 use association rules for hospital infection control and public health surveillance (Brossette et al. [sent-940, score-0.324]
</p><p>88 Conclusions WSARE approaches the problem of early outbreak detection on multivariate surveillance data using two key components. [sent-946, score-0.451]
</p><p>89 With this perspective in mind, the fundamental assumption to our association rule approach is that an outbreak in its early stages will manifest itself in categorical surveillance data as an anomalous cluster in attribute space. [sent-953, score-0.464]
</p><p>90 The rule search allows us to ﬁnd the combination of attributes that characterize the set of cases from recent data that are most anomalous when compared to the baseline data. [sent-956, score-0.382]
</p><p>91 The second major component of WSARE is the use of a Bayesian network to model a baseline that changes due to temporal ﬂuctuations such as seasonal trends and weekend versus weekday effects. [sent-960, score-0.328]
</p><p>92 Environmental attributes, such as season and day of week, are attributes which are responsible for the temporal trends while response attributes are the non-environmental attributes. [sent-963, score-0.609]
</p><p>93 We can then condition on the environmental attributes to produce the baseline given the environment for the current day. [sent-971, score-0.413]
</p><p>94 Although the simulators do not reﬂect real life, detecting an outbreak in our simulated data sets is a challenging problem for any detection algorithm. [sent-974, score-0.384]
</p><p>95 We evaluated WSARE on the CityBN simulator, which was implemented to generate surveillance data which contained temporal ﬂuctuations due to day of week effects and seasonal variations of background illnesses such as ﬂu, food poisoning and allergies. [sent-975, score-0.535]
</p><p>96 0 detected the anthrax outbreaks with nearly the optimal detection time and a very low false positive rate. [sent-978, score-0.413]
</p><p>97 0 outperformed three common univariate detection algorithms in terms of false positives per month and detection time. [sent-980, score-0.516]
</p><p>98 WSARE has been demonstrated to outperform traditional univariate methods on simulated data in terms of false positives per month and detection time. [sent-987, score-0.398]
</p><p>99 Early statistical detection of anthrax outbreaks by tracking over-the-counter medication sales. [sent-1084, score-0.327]
</p><p>100 Evaluation of syndromic surveillance for early detection of bioterrorism using a localized, summer outbreak of Inﬂuenza B. [sent-1120, score-0.498]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wsare', 0.683), ('day', 0.228), ('outbreak', 0.216), ('baseline', 0.149), ('anthrax', 0.145), ('detection', 0.143), ('records', 0.142), ('environmental', 0.135), ('hospital', 0.118), ('days', 0.11), ('attributes', 0.103), ('syndrome', 0.098), ('disease', 0.096), ('surveillance', 0.092), ('amoc', 0.086), ('false', 0.086), ('season', 0.082), ('randomization', 0.08), ('anomalous', 0.076), ('home', 0.075), ('ooper', 0.075), ('oore', 0.075), ('week', 0.072), ('wagner', 0.072), ('bout', 0.071), ('ecent', 0.071), ('food', 0.071), ('respiratory', 0.071), ('trange', 0.071), ('vents', 0.071), ('today', 0.067), ('cpvi', 0.067), ('health', 0.064), ('citybn', 0.063), ('spatial', 0.061), ('age', 0.059), ('hat', 0.059), ('bonferroni', 0.059), ('scan', 0.058), ('positives', 0.056), ('pvalue', 0.056), ('period', 0.056), ('rule', 0.054), ('historical', 0.051), ('trends', 0.051), ('biosurveillance', 0.051), ('rules', 0.05), ('ong', 0.05), ('month', 0.05), ('date', 0.048), ('decile', 0.047), ('hospitals', 0.047), ('release', 0.047), ('syndromic', 0.047), ('security', 0.043), ('daily', 0.043), ('temporal', 0.042), ('bayesian', 0.041), ('alarm', 0.041), ('fdr', 0.039), ('flu', 0.039), ('outbreaks', 0.039), ('anomaly', 0.039), ('score', 0.039), ('vn', 0.039), ('univariate', 0.038), ('symptoms', 0.038), ('uenza', 0.035), ('winter', 0.035), ('region', 0.035), ('moore', 0.034), ('scoring', 0.034), ('counts', 0.033), ('wong', 0.033), ('simulator', 0.033), ('detect', 0.033), ('andrew', 0.032), ('city', 0.032), ('dbi', 0.031), ('latitude', 0.031), ('hypothesis', 0.03), ('contingency', 0.03), ('seasonal', 0.03), ('detector', 0.03), ('weather', 0.03), ('events', 0.029), ('network', 0.029), ('kulldorff', 0.027), ('neill', 0.027), ('viral', 0.027), ('weekday', 0.027), ('tests', 0.027), ('patient', 0.027), ('fawcett', 0.026), ('symptom', 0.026), ('id', 0.026), ('attribute', 0.026), ('current', 0.026), ('statistic', 0.025), ('simulated', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="72-tfidf-1" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<p>Author: Weng-Keen Wong, Andrew Moore, Gregory Cooper, Michael Wagner</p><p>Abstract: Traditional biosurveillance algorithms detect disease outbreaks by looking for peaks in a univariate time series of health-care data. Current health-care surveillance data, however, are no longer simply univariate data streams. Instead, a wealth of spatial, temporal, demographic and symptomatic information is available. We present an early disease outbreak detection algorithm called What’s Strange About Recent Events (WSARE), which uses a multivariate approach to improve its timeliness of detection. WSARE employs a rule-based technique that compares recent health-care data against data from a baseline distribution and ﬁnds subgroups of the recent data whose proportions have changed the most from the baseline data. In addition, health-care data also pose difﬁculties for surveillance algorithms because of inherent temporal trends such as seasonal effects and day of week variations. WSARE approaches this problem using a Bayesian network to produce a baseline distribution that accounts for these temporal trends. The algorithm itself incorporates a wide range of ideas, including association rules, Bayesian networks, hypothesis testing and permutation tests to produce a detection algorithm that is careful to evaluate the signiﬁcance of the alarms that it raises. Keywords: anomaly detection, syndromic surveillance, biosurveillance, Bayesian networks, applications</p><p>2 0.095606886 <a title="72-tfidf-2" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>Author: Joseph F. Murray, Gordon F. Hughes, Kenneth Kreutz-Delgado</p><p>Abstract: We compare machine learning methods applied to a difﬁcult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classiﬁer (mi-NB) which is speciﬁcally designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum signiﬁcance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures. Keywords: hard drive failure prediction, rank-sum test, support vector machines (SVM), exact nonparametric statistics, multiple instance naive-Bayes</p><p>3 0.071198076 <a title="72-tfidf-3" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of ﬁnding level sets for the data generating density. We interpret this learning problem as a binary classiﬁcation problem and compare the corresponding classiﬁcation risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classiﬁcation risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justiﬁcation for the well-known heuristic of artiﬁcially sampling “labeled” samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM. Keywords: unsupervised learning, anomaly detection, density levels, classiﬁcation, SVMs</p><p>4 0.039562035 <a title="72-tfidf-4" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>Author: Hal Daumé III, Daniel Marcu</p><p>Abstract: We develop a Bayesian framework for tackling the supervised clustering problem, the generic problem encountered in tasks such as reference matching, coreference resolution, identity uncertainty and record linkage. Our clustering model is based on the Dirichlet process prior, which enables us to deﬁne distributions over the countably inﬁnite sets that naturally arise in this problem. We add supervision to our model by positing the existence of a set of unobserved random variables (we call these “reference types”) that are generic across all clusters. Inference in our framework, which requires integrating over inﬁnitely many parameters, is solved using Markov chain Monte Carlo techniques. We present algorithms for both conjugate and non-conjugate priors. We present a simple—but general—parameterization of our model based on a Gaussian assumption. We evaluate this model on one artiﬁcial task and three real-world tasks, comparing it against both unsupervised and state-of-the-art supervised algorithms. Our results show that our model is able to outperform other models across a variety of tasks and performance metrics. Keywords: supervised clustering, record linkage, citation matching, coreference, Dirichlet process, non-parametric Bayesian</p><p>5 0.03156548 <a title="72-tfidf-5" href="./jmlr-2005-A_Bayes_Optimal_Approach_for_Partitioning_the_Values_of_Categorical_Attributes.html">1 jmlr-2005-A Bayes Optimal Approach for Partitioning the Values of Categorical Attributes</a></p>
<p>Author: Marc Boullé</p><p>Abstract: In supervised machine learning, the partitioning of the values (also called grouping) of a categorical attribute aims at constructing a new synthetic attribute which keeps the information of the initial attribute and reduces the number of its values. In this paper, we propose a new grouping method MODL1 founded on a Bayesian approach. The method relies on a model space of grouping models and on a prior distribution defined on this model space. This results in an evaluation criterion of grouping, which is minimal for the most probable grouping given the data, i.e. the Bayes optimal grouping. We propose new super-linear optimization heuristics that yields near-optimal groupings. Extensive comparative experiments demonstrate that the MODL grouping method builds high quality groupings in terms of predictive quality, robustness and small number of groups. Keywords: data preparation, grouping, Bayesianism, model selection, classification, naïve Bayes 1</p><p>6 0.027621266 <a title="72-tfidf-6" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>7 0.025253043 <a title="72-tfidf-7" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>8 0.021301508 <a title="72-tfidf-8" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>9 0.020920783 <a title="72-tfidf-9" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>10 0.020833852 <a title="72-tfidf-10" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<p>11 0.020283977 <a title="72-tfidf-11" href="./jmlr-2005-An_MDP-Based_Recommender_System.html">12 jmlr-2005-An MDP-Based Recommender System</a></p>
<p>12 0.019142084 <a title="72-tfidf-12" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>13 0.019080473 <a title="72-tfidf-13" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>14 0.018221481 <a title="72-tfidf-14" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>15 0.018120725 <a title="72-tfidf-15" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>16 0.017846748 <a title="72-tfidf-16" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>17 0.017201493 <a title="72-tfidf-17" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>18 0.01678733 <a title="72-tfidf-18" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>19 0.01632701 <a title="72-tfidf-19" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>20 0.015425788 <a title="72-tfidf-20" href="./jmlr-2005-Prioritization_Methods_for_Accelerating_MDP_Solvers.html">61 jmlr-2005-Prioritization Methods for Accelerating MDP Solvers</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.107), (1, 0.034), (2, 0.031), (3, -0.096), (4, 0.088), (5, -0.012), (6, -0.008), (7, -0.224), (8, -0.012), (9, -0.184), (10, 0.055), (11, -0.073), (12, 0.007), (13, -0.5), (14, -0.09), (15, -0.024), (16, 0.03), (17, -0.039), (18, -0.141), (19, -0.011), (20, 0.008), (21, -0.024), (22, -0.082), (23, -0.09), (24, 0.056), (25, -0.161), (26, -0.109), (27, -0.086), (28, 0.012), (29, 0.164), (30, 0.113), (31, 0.15), (32, -0.125), (33, 0.048), (34, 0.07), (35, 0.053), (36, -0.087), (37, 0.118), (38, 0.089), (39, 0.102), (40, 0.073), (41, 0.067), (42, 0.028), (43, 0.074), (44, -0.006), (45, -0.242), (46, 0.05), (47, 0.403), (48, 0.144), (49, 0.086)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97781408 <a title="72-lsi-1" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<p>Author: Weng-Keen Wong, Andrew Moore, Gregory Cooper, Michael Wagner</p><p>Abstract: Traditional biosurveillance algorithms detect disease outbreaks by looking for peaks in a univariate time series of health-care data. Current health-care surveillance data, however, are no longer simply univariate data streams. Instead, a wealth of spatial, temporal, demographic and symptomatic information is available. We present an early disease outbreak detection algorithm called What’s Strange About Recent Events (WSARE), which uses a multivariate approach to improve its timeliness of detection. WSARE employs a rule-based technique that compares recent health-care data against data from a baseline distribution and ﬁnds subgroups of the recent data whose proportions have changed the most from the baseline data. In addition, health-care data also pose difﬁculties for surveillance algorithms because of inherent temporal trends such as seasonal effects and day of week variations. WSARE approaches this problem using a Bayesian network to produce a baseline distribution that accounts for these temporal trends. The algorithm itself incorporates a wide range of ideas, including association rules, Bayesian networks, hypothesis testing and permutation tests to produce a detection algorithm that is careful to evaluate the signiﬁcance of the alarms that it raises. Keywords: anomaly detection, syndromic surveillance, biosurveillance, Bayesian networks, applications</p><p>2 0.2688213 <a title="72-lsi-2" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>Author: Joseph F. Murray, Gordon F. Hughes, Kenneth Kreutz-Delgado</p><p>Abstract: We compare machine learning methods applied to a difﬁcult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classiﬁer (mi-NB) which is speciﬁcally designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum signiﬁcance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures. Keywords: hard drive failure prediction, rank-sum test, support vector machines (SVM), exact nonparametric statistics, multiple instance naive-Bayes</p><p>3 0.19000491 <a title="72-lsi-3" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>Author: Ingo Steinwart, Don Hush, Clint Scovel</p><p>Abstract: One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of ﬁnding level sets for the data generating density. We interpret this learning problem as a binary classiﬁcation problem and compare the corresponding classiﬁcation risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classiﬁcation risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justiﬁcation for the well-known heuristic of artiﬁcially sampling “labeled” samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM. Keywords: unsupervised learning, anomaly detection, density levels, classiﬁcation, SVMs</p><p>4 0.1757047 <a title="72-lsi-4" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>Author: Hal Daumé III, Daniel Marcu</p><p>Abstract: We develop a Bayesian framework for tackling the supervised clustering problem, the generic problem encountered in tasks such as reference matching, coreference resolution, identity uncertainty and record linkage. Our clustering model is based on the Dirichlet process prior, which enables us to deﬁne distributions over the countably inﬁnite sets that naturally arise in this problem. We add supervision to our model by positing the existence of a set of unobserved random variables (we call these “reference types”) that are generic across all clusters. Inference in our framework, which requires integrating over inﬁnitely many parameters, is solved using Markov chain Monte Carlo techniques. We present algorithms for both conjugate and non-conjugate priors. We present a simple—but general—parameterization of our model based on a Gaussian assumption. We evaluate this model on one artiﬁcial task and three real-world tasks, comparing it against both unsupervised and state-of-the-art supervised algorithms. Our results show that our model is able to outperform other models across a variety of tasks and performance metrics. Keywords: supervised clustering, record linkage, citation matching, coreference, Dirichlet process, non-parametric Bayesian</p><p>5 0.15063396 <a title="72-lsi-5" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>Author: Aapo Hyvärinen</p><p>Abstract: One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data. Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence</p><p>6 0.11219747 <a title="72-lsi-6" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>7 0.10182463 <a title="72-lsi-7" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>8 0.098813109 <a title="72-lsi-8" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>9 0.086758174 <a title="72-lsi-9" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>10 0.080502637 <a title="72-lsi-10" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>11 0.080407195 <a title="72-lsi-11" href="./jmlr-2005-Efficient_Margin_Maximizing_with_Boosting.html">29 jmlr-2005-Efficient Margin Maximizing with Boosting</a></p>
<p>12 0.079038523 <a title="72-lsi-12" href="./jmlr-2005-An_MDP-Based_Recommender_System.html">12 jmlr-2005-An MDP-Based Recommender System</a></p>
<p>13 0.074410394 <a title="72-lsi-13" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>14 0.071707733 <a title="72-lsi-14" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>15 0.07075987 <a title="72-lsi-15" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>16 0.06912563 <a title="72-lsi-16" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>17 0.066335656 <a title="72-lsi-17" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>18 0.065882497 <a title="72-lsi-18" href="./jmlr-2005-A_Unifying_View_of_Sparse_Approximate_Gaussian_Process_Regression.html">7 jmlr-2005-A Unifying View of Sparse Approximate Gaussian Process Regression</a></p>
<p>19 0.062513776 <a title="72-lsi-19" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>20 0.061086845 <a title="72-lsi-20" href="./jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">40 jmlr-2005-Inner Product Spaces for Bayesian Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.018), (17, 0.075), (19, 0.017), (36, 0.025), (37, 0.03), (43, 0.024), (47, 0.012), (52, 0.067), (59, 0.027), (70, 0.031), (71, 0.513), (88, 0.045), (94, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80845684 <a title="72-lda-1" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<p>Author: Weng-Keen Wong, Andrew Moore, Gregory Cooper, Michael Wagner</p><p>Abstract: Traditional biosurveillance algorithms detect disease outbreaks by looking for peaks in a univariate time series of health-care data. Current health-care surveillance data, however, are no longer simply univariate data streams. Instead, a wealth of spatial, temporal, demographic and symptomatic information is available. We present an early disease outbreak detection algorithm called What’s Strange About Recent Events (WSARE), which uses a multivariate approach to improve its timeliness of detection. WSARE employs a rule-based technique that compares recent health-care data against data from a baseline distribution and ﬁnds subgroups of the recent data whose proportions have changed the most from the baseline data. In addition, health-care data also pose difﬁculties for surveillance algorithms because of inherent temporal trends such as seasonal effects and day of week variations. WSARE approaches this problem using a Bayesian network to produce a baseline distribution that accounts for these temporal trends. The algorithm itself incorporates a wide range of ideas, including association rules, Bayesian networks, hypothesis testing and permutation tests to produce a detection algorithm that is careful to evaluate the signiﬁcance of the alarms that it raises. Keywords: anomaly detection, syndromic surveillance, biosurveillance, Bayesian networks, applications</p><p>2 0.2315536 <a title="72-lda-2" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>Author: Joseph F. Murray, Gordon F. Hughes, Kenneth Kreutz-Delgado</p><p>Abstract: We compare machine learning methods applied to a difﬁcult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classiﬁer (mi-NB) which is speciﬁcally designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum signiﬁcance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures. Keywords: hard drive failure prediction, rank-sum test, support vector machines (SVM), exact nonparametric statistics, multiple instance naive-Bayes</p><p>3 0.21856602 <a title="72-lda-3" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>Author: Dmitry Rusakov, Dan Geiger</p><p>Abstract: We develop a closed form asymptotic formula to compute the marginal likelihood of data given a naive Bayesian network model with two hidden states and binary features. This formula deviates from the standard BIC score. Our work provides a concrete example that the BIC score is generally incorrect for statistical models that belong to stratiﬁed exponential families. This claim stands in contrast to linear and curved exponential families, where the BIC score has been proven to provide a correct asymptotic approximation for the marginal likelihood. Keywords: Bayesian networks, asymptotic model selection, Bayesian information criterion (BIC)</p><p>4 0.19217148 <a title="72-lda-4" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>Author: Aharon Bar-Hillel, Tomer Hertz, Noam Shental, Daphna Weinshall</p><p>Abstract: Many learning algorithms use a metric deﬁned over the input space as a principal tool, and their performance critically depends on the quality of this metric. We address the problem of learning metrics using side-information in the form of equivalence constraints. Unlike labels, we demonstrate that this type of side-information can sometimes be automatically obtained without the need of human intervention. We show how such side-information can be used to modify the representation of the data, leading to improved clustering and classiﬁcation. Speciﬁcally, we present the Relevant Component Analysis (RCA) algorithm, which is a simple and efﬁcient algorithm for learning a Mahalanobis metric. We show that RCA is the solution of an interesting optimization problem, founded on an information theoretic basis. If dimensionality reduction is allowed within RCA, we show that it is optimally accomplished by a version of Fisher’s linear discriminant that uses constraints. Moreover, under certain Gaussian assumptions, RCA can be viewed as a Maximum Likelihood estimation of the within class covariance matrix. We conclude with extensive empirical evaluations of RCA, showing its advantage over alternative methods. Keywords: clustering, metric learning, dimensionality reduction, equivalence constraints, side information.</p><p>5 0.18917476 <a title="72-lda-5" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>Author: Wei Chu, Zoubin Ghahramani</p><p>Abstract: We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative ﬁltering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach. Keywords: Gaussian processes, ordinal regression, approximate Bayesian inference, collaborative ﬁltering, gene expression analysis, feature selection</p><p>6 0.18880652 <a title="72-lda-6" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>7 0.1874375 <a title="72-lda-7" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>8 0.18584922 <a title="72-lda-8" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>9 0.18534289 <a title="72-lda-9" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>10 0.18493247 <a title="72-lda-10" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>11 0.18453965 <a title="72-lda-11" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>12 0.18400989 <a title="72-lda-12" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>13 0.18343136 <a title="72-lda-13" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>14 0.1827082 <a title="72-lda-14" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>15 0.18170755 <a title="72-lda-15" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>16 0.18076599 <a title="72-lda-16" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>17 0.17723626 <a title="72-lda-17" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>18 0.17558891 <a title="72-lda-18" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>19 0.17553599 <a title="72-lda-19" href="./jmlr-2005-Managing_Diversity_in_Regression_Ensembles.html">54 jmlr-2005-Managing Diversity in Regression Ensembles</a></p>
<p>20 0.17462805 <a title="72-lda-20" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
