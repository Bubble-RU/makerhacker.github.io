<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-9" href="#">jmlr2005-9</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</h1>
<br/><p>Source: <a title="jmlr-2005-9-pdf" href="http://jmlr.org/papers/volume6/luo05a/luo05a.pdf">pdf</a></p><p>Author: Tong Luo, Kurt Kramer, Dmitry B. Goldgof, Lawrence O. Hall, Scott Samson, Andrew Remsen, Thomas Hopkins</p><p>Abstract: This paper presents an active learning method which reduces the labeling effort of domain experts in multi-class classiﬁcation problems. Active learning is applied in conjunction with support vector machines to recognize underwater zooplankton from higher-resolution, new generation SIPPER II images. Most previous work on active learning with support vector machines only deals with two class problems. In this paper, we propose an active learning approach “breaking ties” for multiclass support vector machines using the one-vs-one approach with a probability approximation. Experimental results indicate that our approach often requires signiﬁcantly less labeled images to reach a given accuracy than the approach of labeling the least certain test example and random sampling. It can also be applied in batch mode resulting in an accuracy comparable to labeling one image at a time and retraining. Keywords: active learning, support vector machine, plankton recognition, probabilistic output, multi-class support vector machine</p><p>Reference: <a title="jmlr-2005-9-reference" href="../jmlr2005_reference/jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Petersburg, FL 33701, USA  Editor: David Cohn  Abstract This paper presents an active learning method which reduces the labeling effort of domain experts in multi-class classiﬁcation problems. [sent-18, score-0.523]
</p><p>2 Most previous work on active learning with support vector machines only deals with two class problems. [sent-20, score-0.488]
</p><p>3 In this paper, we propose an active learning approach “breaking ties” for multiclass support vector machines using the one-vs-one approach with a probability approximation. [sent-21, score-0.488]
</p><p>4 Experimental results indicate that our approach often requires signiﬁcantly less labeled images to reach a given accuracy than the approach of labeling the least certain test example and random sampling. [sent-22, score-0.59]
</p><p>5 Keywords: active learning, support vector machine, plankton recognition, probabilistic output, multi-class support vector machine  1. [sent-24, score-0.786]
</p><p>6 For example, in a previous study using approximately 150,000 SIPPER images from a two hour sampling deployment it took over one month to manually classify the images (Remsen et al. [sent-28, score-0.629]
</p><p>7 Also, this automated system is expected to continuously evolve from an existing model to a more accurate model created by training after adding some new labeled images into the training set. [sent-30, score-0.465]
</p><p>8 Since it is impossible to manually label all images during the time they are acquired on the ship, active learning to label the most important images seems attractive. [sent-31, score-1.07]
</p><p>9 Recently, active learning with SVMs has been developed and applied in a variety of applications (Tong and Koller, 2000; Schohn and Cohn, 2000; Campbell et al. [sent-45, score-0.412]
</p><p>10 A similar active learning method for support vector machines (SVMs) in two class problems was independently developed by several researchers Tong and Koller (2000), Schohn and Cohn (2000), and Campbell et al. [sent-54, score-0.488]
</p><p>11 Compared to random sampling, “simple” reduced the required number of labeled images in experiments on text classiﬁcation. [sent-59, score-0.42]
</p><p>12 (2004) observed that there was no single winner from different active learning strategies on several data sets. [sent-70, score-0.412]
</p><p>13 Although Brinker (2003) did not provide a method to set the optimal value of λ, “combined” performed better than “simple” in batch mode on several data sets (when labeling several images at a time). [sent-76, score-0.449]
</p><p>14 Higher resolution SIPPER (SIPPER II) images provide relatively better quality images with clear contours. [sent-81, score-0.56]
</p><p>15 Moreover, little previous work in active learning has been done with multiple class SVMs, which is required in plankton recognition. [sent-85, score-0.688]
</p><p>16 A least certainty active learning approach was proposed and evaluated for multiple class SVMs. [sent-103, score-0.478]
</p><p>17 (2004a) and propose a new active learning strategy for one-versus-one multiclass SVMs. [sent-105, score-0.412]
</p><p>18 We compare our approach with other methods like random sampling and least certainty for the plankton recognition problem. [sent-107, score-0.437]
</p><p>19 Our proposed method can run in batch mode, labeling up to 20 images at a time, with an accuracy comparable to labeling one image at a time and retraining. [sent-110, score-0.622]
</p><p>20 In a simulation where plankton images come as a stream, active learning resulted in higher classiﬁcation accuracy than random sampling. [sent-111, score-1.055]
</p><p>21 Section 2 introduces our active learning approach for support vector machines and our approach to assigning a classiﬁcation probability for a multi-class support vector machine. [sent-113, score-0.537]
</p><p>22 The probability model was used in the development of an active learning model. [sent-119, score-0.412]
</p><p>23 The classiﬁcation probability can be used to develop an active learning strategy for a multi-class SVM. [sent-146, score-0.412]
</p><p>24 In real-time plankton recognition, the probability computation needs to be fast since retraining the probability model will be frequently needed as more plankton images are acquired on a cruise. [sent-167, score-0.865]
</p><p>25 3 Active Learning for Multi-Class SVMs The least certainty active learning approach for SVMs (Luo et al. [sent-210, score-0.478]
</p><p>26 In this paper, we propose another active learning approach–“breaking ties” (BT). [sent-216, score-0.412]
</p><p>27 Experiments The experimental data set consisted of 8440 SIPPER II images selected from the ﬁve most abundant types of plankton: 1688 images from each type of plankton. [sent-238, score-0.588]
</p><p>28 There were 1000 images (200 each type of plankton) randomly selected as the validation set used in the active learning experiments. [sent-239, score-0.692]
</p><p>29 Figures 1(a) to 1(e) are typical examples of the images produced by SIPPER II for the ﬁve most abundant plankton classes. [sent-241, score-0.609]
</p><p>30 In all the active learning experiments, we used the best 17 feature subset instead of the 49 feature set. [sent-258, score-0.46]
</p><p>31 The parameters (g, C, A) were optimized by performing a gridsearch across a randomly selected 1000 images consisting of 200 images per class. [sent-261, score-0.586]
</p><p>32 A series of retrainings were done for the two active learning methods and with random sampling. [sent-269, score-0.412]
</p><p>33 Instead of exhausting all of the unlabeled data set, we only labeled 750 more images in each experiment because exhausting all unlabeled data was not a fair criterion for comparing between different sample selection algorithms. [sent-271, score-0.69]
</p><p>34 For example, active learning labeled the most “informative” new examples, which were available in the beginning of the experiment. [sent-272, score-0.552]
</p><p>35 In contrast to active learning, random sampling labeled average “informative” examples throughout the whole experiment. [sent-276, score-0.646]
</p><p>36 It surely would catch up with active learning in the later stages when active learning only had “garbage” examples to label. [sent-277, score-0.849]
</p><p>37 Moreover, when the plankton recognition system is employed on a cruise, the unlabeled images come like a stream. [sent-278, score-0.684]
</p><p>38 The nature of such an application prevents one from exhausting all the unlabeled images because the time required to label them is prohibitive. [sent-279, score-0.464]
</p><p>39 We varied both the number of initial labeled images per class (IIPC) and the number of images selected for labeling at each retraining step (IPR). [sent-285, score-0.883]
</p><p>40 1 Experiments with IPR=1, IIPC Varied Figures 2–5 show the experimental results of active learning methods using different IIPC values. [sent-287, score-0.412]
</p><p>41 05, “BT” is statistically signiﬁcantly more accurate than “LC” and both active learning methods are statistically signiﬁcantly more accurate than random sampling. [sent-293, score-0.458]
</p><p>42 7 times the number of newly labeled images compared to “BT”. [sent-295, score-0.447]
</p><p>43 In general, an effective active learning approach ﬁnds more SVs than random sampling. [sent-298, score-0.412]
</p><p>44 Also, the slope of both active learning curves is about 0. [sent-301, score-0.412]
</p><p>45 9, which means that 90% of the labeled images turn out to be SVs. [sent-302, score-0.42]
</p><p>46 We note that a high slope of the support vector curve is not a sufﬁcient condition for effective active learning because there are many SVs to be added into the current model and different SVs lead to different improvements. [sent-304, score-0.482]
</p><p>47 Ideally, a very effective active learning method discovers the SVs which provide the most improvement to the current model. [sent-305, score-0.412]
</p><p>48 In contrast, an active learning method, which always ﬁnds the SVs misclassiﬁed by the current classiﬁer and far from its decision boundary, may result in poor performance because such SVs are very likely to be noise. [sent-306, score-0.437]
</p><p>49 Therefore, we cannot compare active learning methods based only on slight differences in the support vector curves. [sent-307, score-0.461]
</p><p>50 Compared to 10 IIPC, the accuracy for both active learning approaches improved faster than random sampling. [sent-309, score-0.476]
</p><p>51 The slopes of support vector curves for active learning are higher than those of random sampling. [sent-313, score-0.461]
</p><p>52 In Figures 4 and 5, the initial accuracy was greater than 80% when using 100 and 200 initial images from each class, and active learning was very effective. [sent-315, score-0.838]
</p><p>53 Random sampling required more than 3 times the number of images to reach the same level of accuracy as both active learning approaches. [sent-316, score-0.848]
</p><p>54 The two active learning methods effectively capture many more SVs than random sampling. [sent-317, score-0.412]
</p><p>55 Also, our newly proposed active learning approach, “BT”, requires less images to reach a given accuracy than “LC” after adding 450 labeled images. [sent-318, score-0.946]
</p><p>56 It seems reasonable that the accuracy of the initial classiﬁer affects the performance of active learning and random sampling. [sent-320, score-0.517]
</p><p>57 Therefore, if the initial classiﬁer helps active learning to choose examples more informative than average (random sampling), active learning will result in a more accurate classiﬁer with fewer labeled examples. [sent-325, score-1.115]
</p><p>58 When comparing the two active learning methods, “BT” outperformed “LC” under all four starting conditions. [sent-327, score-0.434]
</p><p>59 The justiﬁcation is that an accurate initial classiﬁer allows for less error reduction using active learning. [sent-329, score-0.476]
</p><p>60 602  ACTIVE L EARNING TO R ECOGNIZE M ULTIPLE T YPES OF P LANKTON  of available accuracy improvement was small, the difference in accuracy between the two active learning methods became insigniﬁcant. [sent-352, score-0.54]
</p><p>61 Also, given the total number of newly labeled images is U, it is approximately k times faster if we label k images at a time because it only requires a new model be learned U times. [sent-356, score-0.776]
</p><p>62 Alk though an incremental SVM training algorithm was proposed by Cauwenberghs and Poggio (2000) to reduce the retraining time, model updating by labeling one image at a time was still quite time consuming, especially when many images are to be labeled. [sent-357, score-0.462]
</p><p>63 Therefore, we expected active learning to be effective even when adding several labeled images at a time. [sent-358, score-0.832]
</p><p>64 The active learning method “BT” was good for adding only one “informative” example at a time, however there was no guarantee that adding several examples at a time would still favor “BT”. [sent-359, score-0.437]
</p><p>65 These results indicate that our active learning approach “BT” can run in batch mode, labeling tens of examples at a time, to achieve speedup with at most a little compromise in accuracy. [sent-371, score-0.592]
</p><p>66 In our experiment, we started off with 10 labeled images randomly selected from each class (IIPC=10). [sent-377, score-0.42]
</p><p>67 9%  Accuracy  79%  74% 1-IPR 5-IPR 10-IPR 69%  20-IPR 50-IPR Random  64% 0  100  200  300  400  500  600  700  Number of new images  Figure 6: Comparison of active learning and random sampling in terms of accuracy with different IPR: initial training images per class are 10. [sent-389, score-1.172]
</p><p>68 2%  82% 1-IPR 81%  5-IPR  80%  10-IPR  79%  20-IPR 50-IPR  78%  Random 77% 0  100  200  300  400  500  600  700  Number of new images  Figure 7: Comparison of active learning and random sampling in terms of accuracy with different IPR: initial training images per class are 50. [sent-393, score-1.172]
</p><p>69 5%  Random 80% 0  100  200  300  400  500  600  700  Number of new images  Figure 8: Comparison of active learning and random sampling in terms of accuracy with different IPR: initial training images per class are 100. [sent-398, score-1.172]
</p><p>70 4%  83% 0  100  200  300  400  500  600  700  Number of new images  Figure 9: Comparison of active learning and random sampling in terms of accuracy with different IPR: initial training images per class are 200. [sent-403, score-1.172]
</p><p>71 7%  Random 60% 0  100  200  300  400  500  600  700  Number of new images  Figure 10: Comparison of active learning and random sampling in a data streaming simulation: initial training images per class are 10, 10 newly labeled images added at a time. [sent-408, score-1.641]
</p><p>72 We started off with a total of 30 initial labeled images randomly taken from the above distribution and labeled one image at a time per retraining (IPR=1). [sent-421, score-0.726]
</p><p>73 The initial labeled images used unequal priors because they would typically be randomly sampled from the unlabeled pool and therefore would likely have the same class distribution. [sent-423, score-0.629]
</p><p>74 Discussion and Conclusions This paper presents an active learning approach to reduce domain experts’ labeling efforts in recognizing plankton from higher-resolution, new generation SIPPER II images. [sent-429, score-0.771]
</p><p>75 The “breaking ties” active learning method was proposed and applied to a multi-class SVM using the one-vs-one approach on newly developed, image features extracted from gray-scale SIPPER images. [sent-431, score-0.529]
</p><p>76 The experimental results showed that our proposed active learning approach successfully reduced the number of labeled images required to reach a given accuracy level when compared to random sampling. [sent-432, score-0.919]
</p><p>77 The new approach was also effective in batch mode, allowing for labeling up to 20 images at a time with classiﬁcation accuracy which was similar to that achieved when labeling one image at a time and retraining. [sent-434, score-0.622]
</p><p>78 In the following, we address and discuss several active learning in SVM issues which deserve further exploration. [sent-436, score-0.412]
</p><p>79 One critique of active learning is the overhead related to searching for the next candidate to label. [sent-437, score-0.412]
</p><p>80 Random sampling just selects an example to label at random, but active learning needs to evaluate every unlabeled example. [sent-438, score-0.632]
</p><p>81 Also, the experiment with a “data streaming” simulation indicated “BT” worked well when the evaluation and active learning was performed on a small subset of the data. [sent-445, score-0.412]
</p><p>82 The key reason is we do not have a good method to evaluate different kernel parameters as active learning proceeds. [sent-453, score-0.412]
</p><p>83 The standard methods like crossvalidation and leave-one-out tend to fail because active learning chooses biased data samples. [sent-454, score-0.412]
</p><p>84 An important future direction is to ﬁnd a good online performance evaluation method for active learning. [sent-457, score-0.412]
</p><p>85 Otherwise, one could take it as one of the biggest bottlenecks for using a SVM as the classiﬁer in active learning because a SVM depends heavily on good kernel parameters. [sent-458, score-0.412]
</p><p>86 An important thing omitted in most active learning+SVMs literature is to try active learning in batch mode. [sent-462, score-0.87]
</p><p>87 Unless labeling an example is extremely expensive, it is always convenient and practical to use active learning in batch mode, namely labeling several examples at a time and then retraining. [sent-463, score-0.649]
</p><p>88 A criterion for the best set of data to label in multi-class SVMs needs to be addressed in future active learning work. [sent-466, score-0.461]
</p><p>89 At the very least, existing active learning methods need to be shown to work well in batch mode. [sent-467, score-0.458]
</p><p>90 Fortunately, our proposed active learning method did work well in batch mode without requiring a new criterion for selecting a set of data to label. [sent-468, score-0.498]
</p><p>91 In general, active learning tries to minimize the redundancy of labeled examples to reach a given accuracy. [sent-470, score-0.6]
</p><p>92 In our case, we only selectively label a few images and we expect small labeling noise due to the relatively small labeling effort. [sent-472, score-0.495]
</p><p>93 Incorporating diversity in active learning with support vector machines. [sent-483, score-0.461]
</p><p>94 Recognizing plankton images from the shadow image particle proﬁling evaluation recorder. [sent-560, score-0.65]
</p><p>95 Segmentation of multispectral remote sensing images using active support vector machines. [sent-576, score-0.741]
</p><p>96 Convergence and application of online active sampling using orthogonal pillar vectors. [sent-593, score-0.481]
</p><p>97 Toward optimal active learning through sampling estimation of error reduction. [sent-620, score-0.481]
</p><p>98 An empirical study of active learning with support vector machines for japanese word segmentation. [sent-633, score-0.488]
</p><p>99 Bootstrapping SVM active learning by incorporating unlabelled images for image retrieval. [sent-669, score-0.758]
</p><p>100 Support vector machines for a active learning in the drug discovery process. [sent-678, score-0.439]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('active', 0.412), ('images', 0.28), ('ipr', 0.276), ('plankton', 0.276), ('bt', 0.221), ('luo', 0.221), ('iipc', 0.199), ('sipper', 0.199), ('lc', 0.166), ('labeled', 0.14), ('amson', 0.132), ('ecognize', 0.132), ('emsen', 0.132), ('lankton', 0.132), ('oldgof', 0.132), ('opkins', 0.132), ('ramer', 0.132), ('uo', 0.132), ('ypes', 0.132), ('ppq', 0.121), ('svs', 0.11), ('unlabeled', 0.102), ('ultiple', 0.098), ('samson', 0.088), ('labeling', 0.083), ('sampling', 0.069), ('svms', 0.068), ('certainty', 0.066), ('remsen', 0.066), ('unequal', 0.066), ('image', 0.066), ('streaming', 0.065), ('accuracy', 0.064), ('informative', 0.062), ('bars', 0.058), ('goldgof', 0.055), ('marine', 0.055), ('mitra', 0.055), ('unclassi', 0.055), ('conf', 0.049), ('label', 0.049), ('earning', 0.049), ('svm', 0.049), ('support', 0.049), ('baram', 0.046), ('batch', 0.046), ('brinker', 0.044), ('copepod', 0.044), ('csee', 0.044), ('garbage', 0.044), ('texture', 0.044), ('initial', 0.041), ('mode', 0.04), ('er', 0.038), ('kramer', 0.037), ('particles', 0.037), ('tong', 0.036), ('exhausting', 0.033), ('larvacean', 0.033), ('oithona', 0.033), ('ppk', 0.033), ('unidenti', 0.033), ('retraining', 0.033), ('built', 0.033), ('classi', 0.033), ('recognize', 0.028), ('abundant', 0.028), ('particle', 0.028), ('experts', 0.028), ('machines', 0.027), ('newly', 0.027), ('boundary', 0.027), ('per', 0.026), ('recognition', 0.026), ('speedup', 0.026), ('block', 0.026), ('examples', 0.025), ('decision', 0.025), ('florida', 0.025), ('ling', 0.025), ('schohn', 0.025), ('features', 0.024), ('feature', 0.024), ('reach', 0.023), ('accurate', 0.023), ('resulted', 0.023), ('campbell', 0.022), ('automated', 0.022), ('ties', 0.022), ('cem', 0.022), ('invariants', 0.022), ('iprs', 0.022), ('kurt', 0.022), ('recorder', 0.022), ('scott', 0.022), ('zooplankton', 0.022), ('breaking', 0.022), ('outperformed', 0.022), ('added', 0.021), ('south', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="9-tfidf-1" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>Author: Tong Luo, Kurt Kramer, Dmitry B. Goldgof, Lawrence O. Hall, Scott Samson, Andrew Remsen, Thomas Hopkins</p><p>Abstract: This paper presents an active learning method which reduces the labeling effort of domain experts in multi-class classiﬁcation problems. Active learning is applied in conjunction with support vector machines to recognize underwater zooplankton from higher-resolution, new generation SIPPER II images. Most previous work on active learning with support vector machines only deals with two class problems. In this paper, we propose an active learning approach “breaking ties” for multiclass support vector machines using the one-vs-one approach with a probability approximation. Experimental results indicate that our approach often requires signiﬁcantly less labeled images to reach a given accuracy than the approach of labeling the least certain test example and random sampling. It can also be applied in batch mode resulting in an accuracy comparable to labeling one image at a time and retraining. Keywords: active learning, support vector machine, plankton recognition, probabilistic output, multi-class support vector machine</p><p>2 0.12138735 <a title="9-tfidf-2" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>Author: Antoine Bordes, Seyda Ertekin, Jason Weston, Léon Bottou</p><p>Abstract: Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efﬁcient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We ﬁrst present an online SVM algorithm based on this premise. LASVM yields competitive misclassiﬁcation rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.</p><p>3 0.088465601 <a title="9-tfidf-3" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>Author: Luís B. Almeida</p><p>Abstract: When acquiring an image of a paper document, the image printed on the back page sometimes shows through. The mixture of the front- and back-page images thus obtained is markedly nonlinear, and thus constitutes a good real-life test case for nonlinear blind source separation. This paper addresses a difﬁcult version of this problem, corresponding to the use of “onion skin” paper, which results in a relatively strong nonlinearity of the mixture, which becomes close to singular in the lighter regions of the images. The separation is achieved through the MISEP technique, which is an extension of the well known INFOMAX method. The separation results are assessed with objective quality measures. They show an improvement over the results obtained with linear separation, but have room for further improvement. Keywords: ICA, blind source separation, nonlinear mixtures, nonlinear separation, image mixture, image separation</p><p>4 0.086966626 <a title="9-tfidf-4" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>Author: Rie Kubota Ando, Tong Zhang</p><p>Abstract: One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don’t have a complete understanding of their eﬀectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Speciﬁcally we consider learning predictive structures on hypothesis spaces (that is, what kind of classiﬁers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the eﬀectiveness of the proposed algorithms in the semi-supervised learning setting.</p><p>5 0.06418819 <a title="9-tfidf-5" href="./jmlr-2005-Active_Coevolutionary_Learning_of_Deterministic_Finite_Automata.html">8 jmlr-2005-Active Coevolutionary Learning of Deterministic Finite Automata</a></p>
<p>Author: Josh Bongard, Hod Lipson</p><p>Abstract: This paper describes an active learning approach to the problem of grammatical inference, specifically the inference of deterministic ﬁnite automata (DFAs). We refer to the algorithm as the estimation-exploration algorithm (EEA). This approach differs from previous passive and active learning approaches to grammatical inference in that training data is actively proposed by the algorithm, rather than passively receiving training data from some external teacher. Here we show that this algorithm outperforms one version of the most powerful set of algorithms for grammatical inference, evidence driven state merging (EDSM), on randomly-generated DFAs. The performance increase is due to the fact that the EDSM algorithm only works well for DFAs with speciﬁc balances (percentage of positive labelings), while the EEA is more consistent over a wider range of balances. Based on this ﬁnding we propose a more general method for generating DFAs to be used in the development of future grammatical inference algorithms. Keywords: grammatical inference, evolutionary computation, deterministic ﬁnite automata, active learning, system identiﬁcation</p><p>6 0.057005912 <a title="9-tfidf-6" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>7 0.053552836 <a title="9-tfidf-7" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>8 0.0464187 <a title="9-tfidf-8" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>9 0.039578501 <a title="9-tfidf-9" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>10 0.038775519 <a title="9-tfidf-10" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>11 0.037296966 <a title="9-tfidf-11" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>12 0.034445539 <a title="9-tfidf-12" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>13 0.033886436 <a title="9-tfidf-13" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>14 0.032683335 <a title="9-tfidf-14" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>15 0.029138014 <a title="9-tfidf-15" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>16 0.029027458 <a title="9-tfidf-16" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>17 0.028490271 <a title="9-tfidf-17" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>18 0.024852974 <a title="9-tfidf-18" href="./jmlr-2005-Prioritization_Methods_for_Accelerating_MDP_Solvers.html">61 jmlr-2005-Prioritization Methods for Accelerating MDP Solvers</a></p>
<p>19 0.024025757 <a title="9-tfidf-19" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>20 0.023870505 <a title="9-tfidf-20" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.168), (1, 0.152), (2, 0.135), (3, -0.09), (4, 0.127), (5, -0.149), (6, 0.116), (7, 0.071), (8, -0.166), (9, -0.086), (10, 0.134), (11, -0.173), (12, -0.284), (13, 0.21), (14, -0.015), (15, -0.052), (16, 0.132), (17, -0.293), (18, -0.086), (19, -0.009), (20, -0.105), (21, -0.091), (22, -0.064), (23, -0.035), (24, -0.111), (25, 0.006), (26, 0.011), (27, 0.018), (28, 0.197), (29, -0.109), (30, -0.005), (31, -0.136), (32, -0.069), (33, 0.018), (34, -0.059), (35, -0.121), (36, -0.059), (37, 0.018), (38, 0.25), (39, -0.037), (40, -0.042), (41, -0.045), (42, 0.111), (43, -0.048), (44, -0.141), (45, 0.067), (46, -0.056), (47, 0.095), (48, 0.09), (49, -0.0)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9706164 <a title="9-lsi-1" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>Author: Tong Luo, Kurt Kramer, Dmitry B. Goldgof, Lawrence O. Hall, Scott Samson, Andrew Remsen, Thomas Hopkins</p><p>Abstract: This paper presents an active learning method which reduces the labeling effort of domain experts in multi-class classiﬁcation problems. Active learning is applied in conjunction with support vector machines to recognize underwater zooplankton from higher-resolution, new generation SIPPER II images. Most previous work on active learning with support vector machines only deals with two class problems. In this paper, we propose an active learning approach “breaking ties” for multiclass support vector machines using the one-vs-one approach with a probability approximation. Experimental results indicate that our approach often requires signiﬁcantly less labeled images to reach a given accuracy than the approach of labeling the least certain test example and random sampling. It can also be applied in batch mode resulting in an accuracy comparable to labeling one image at a time and retraining. Keywords: active learning, support vector machine, plankton recognition, probabilistic output, multi-class support vector machine</p><p>2 0.55562288 <a title="9-lsi-2" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>Author: Antoine Bordes, Seyda Ertekin, Jason Weston, Léon Bottou</p><p>Abstract: Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efﬁcient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We ﬁrst present an online SVM algorithm based on this premise. LASVM yields competitive misclassiﬁcation rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.</p><p>3 0.40339512 <a title="9-lsi-3" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>Author: Rie Kubota Ando, Tong Zhang</p><p>Abstract: One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don’t have a complete understanding of their eﬀectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Speciﬁcally we consider learning predictive structures on hypothesis spaces (that is, what kind of classiﬁers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the eﬀectiveness of the proposed algorithms in the semi-supervised learning setting.</p><p>4 0.35626116 <a title="9-lsi-4" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>Author: Luís B. Almeida</p><p>Abstract: When acquiring an image of a paper document, the image printed on the back page sometimes shows through. The mixture of the front- and back-page images thus obtained is markedly nonlinear, and thus constitutes a good real-life test case for nonlinear blind source separation. This paper addresses a difﬁcult version of this problem, corresponding to the use of “onion skin” paper, which results in a relatively strong nonlinearity of the mixture, which becomes close to singular in the lighter regions of the images. The separation is achieved through the MISEP technique, which is an extension of the well known INFOMAX method. The separation results are assessed with objective quality measures. They show an improvement over the results obtained with linear separation, but have room for further improvement. Keywords: ICA, blind source separation, nonlinear mixtures, nonlinear separation, image mixture, image separation</p><p>5 0.24303363 <a title="9-lsi-5" href="./jmlr-2005-Active_Coevolutionary_Learning_of_Deterministic_Finite_Automata.html">8 jmlr-2005-Active Coevolutionary Learning of Deterministic Finite Automata</a></p>
<p>Author: Josh Bongard, Hod Lipson</p><p>Abstract: This paper describes an active learning approach to the problem of grammatical inference, specifically the inference of deterministic ﬁnite automata (DFAs). We refer to the algorithm as the estimation-exploration algorithm (EEA). This approach differs from previous passive and active learning approaches to grammatical inference in that training data is actively proposed by the algorithm, rather than passively receiving training data from some external teacher. Here we show that this algorithm outperforms one version of the most powerful set of algorithms for grammatical inference, evidence driven state merging (EDSM), on randomly-generated DFAs. The performance increase is due to the fact that the EDSM algorithm only works well for DFAs with speciﬁc balances (percentage of positive labelings), while the EEA is more consistent over a wider range of balances. Based on this ﬁnding we propose a more general method for generating DFAs to be used in the development of future grammatical inference algorithms. Keywords: grammatical inference, evolutionary computation, deterministic ﬁnite automata, active learning, system identiﬁcation</p><p>6 0.22050145 <a title="9-lsi-6" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>7 0.2150193 <a title="9-lsi-7" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>8 0.15961072 <a title="9-lsi-8" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>9 0.15626647 <a title="9-lsi-9" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>10 0.15070881 <a title="9-lsi-10" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>11 0.14387132 <a title="9-lsi-11" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>12 0.13928813 <a title="9-lsi-12" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>13 0.12750074 <a title="9-lsi-13" href="./jmlr-2005-Multiclass_Classification_with_Multi-Prototype_Support_Vector_Machines.html">58 jmlr-2005-Multiclass Classification with Multi-Prototype Support Vector Machines</a></p>
<p>14 0.12664011 <a title="9-lsi-14" href="./jmlr-2005-Local_Propagation_in_Conditional_Gaussian_Bayesian_Networks.html">51 jmlr-2005-Local Propagation in Conditional Gaussian Bayesian Networks</a></p>
<p>15 0.11396321 <a title="9-lsi-15" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>16 0.10779101 <a title="9-lsi-16" href="./jmlr-2005-Tutorial_on_Practical_Prediction_Theory_for_Classification.html">69 jmlr-2005-Tutorial on Practical Prediction Theory for Classification</a></p>
<p>17 0.10451671 <a title="9-lsi-17" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>18 0.10066949 <a title="9-lsi-18" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<p>19 0.09937425 <a title="9-lsi-19" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>20 0.095001481 <a title="9-lsi-20" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.35), (17, 0.015), (21, 0.011), (36, 0.016), (37, 0.032), (42, 0.013), (43, 0.047), (47, 0.025), (52, 0.274), (59, 0.015), (70, 0.031), (88, 0.044), (90, 0.013), (94, 0.019), (96, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8932156 <a title="9-lda-1" href="./jmlr-2005-Change_Point_Problems_in_Linear_Dynamical_Systems.html">17 jmlr-2005-Change Point Problems in Linear Dynamical Systems</a></p>
<p>Author: Onno Zoeter, Tom Heskes</p><p>Abstract: We study the problem of learning two regimes (we have a normal and a prefault regime in mind) based on a train set of non-Markovian observation sequences. Key to the model is that we assume that once the system switches from the normal to the prefault regime it cannot restore and will eventually result in a fault. We refer to the particular setting as semi-supervised since we assume the only information given to the learner is whether a particular sequence ended with a stop (implying that the sequence was generated by the normal regime) or with a fault (implying that there was a switch from the normal to the fault regime). In the latter case the particular time point at which a switch occurred is not known. The underlying model used is a switching linear dynamical system (SLDS). The constraints in the regime transition probabilities result in an exact inference procedure that scales quadratically with the length of a sequence. Maximum aposteriori (MAP) parameter estimates can be found using an expectation maximization (EM) algorithm with this inference algorithm in the E-step. For long sequences this will not be practically feasible and an approximate inference and an approximate EM procedure is called for. We describe a Ä?Ĺš&sbquo;exible class of approximations corresponding to different choices of clusters in a Kikuchi free energy with weak consistency constraints. Keywords: change point problems, switching linear dynamical systems, strong junction trees, approximate inference, expectation propagation, Kikuchi free energies</p><p>2 0.86010599 <a title="9-lda-2" href="./jmlr-2005-Diffusion_Kernels_on_Statistical_Manifolds.html">26 jmlr-2005-Diffusion Kernels on Statistical Manifolds</a></p>
<p>Author: John Lafferty, Guy Lebanon</p><p>Abstract: A family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models. The kernels are based on the heat equation on the Riemannian manifold deﬁned by the Fisher information metric associated with a statistical family, and generalize the Gaussian kernel of Euclidean space. As an important special case, kernels based on the geometry of multinomial families are derived, leading to kernel-based learning algorithms that apply naturally to discrete data. Bounds on covering numbers and Rademacher averages for the kernels are proved using bounds on the eigenvalues of the Laplacian on Riemannian manifolds. Experimental results are presented for document classiﬁcation, for which the use of multinomial geometry is natural and well motivated, and improvements are obtained over the standard use of Gaussian or linear kernels, which have been the standard for text classiﬁcation. Keywords: kernels, heat equation, diffusion, information geometry, text classiﬁcation</p><p>same-paper 3 0.81655145 <a title="9-lda-3" href="./jmlr-2005-Active_Learning_to_Recognize_Multiple_Types_of_Plankton.html">9 jmlr-2005-Active Learning to Recognize Multiple Types of Plankton</a></p>
<p>Author: Tong Luo, Kurt Kramer, Dmitry B. Goldgof, Lawrence O. Hall, Scott Samson, Andrew Remsen, Thomas Hopkins</p><p>Abstract: This paper presents an active learning method which reduces the labeling effort of domain experts in multi-class classiﬁcation problems. Active learning is applied in conjunction with support vector machines to recognize underwater zooplankton from higher-resolution, new generation SIPPER II images. Most previous work on active learning with support vector machines only deals with two class problems. In this paper, we propose an active learning approach “breaking ties” for multiclass support vector machines using the one-vs-one approach with a probability approximation. Experimental results indicate that our approach often requires signiﬁcantly less labeled images to reach a given accuracy than the approach of labeling the least certain test example and random sampling. It can also be applied in batch mode resulting in an accuracy comparable to labeling one image at a time and retraining. Keywords: active learning, support vector machine, plankton recognition, probabilistic output, multi-class support vector machine</p><p>4 0.64563513 <a title="9-lda-4" href="./jmlr-2005-An_MDP-Based_Recommender_System.html">12 jmlr-2005-An MDP-Based Recommender System</a></p>
<p>Author: Guy Shani, David Heckerman, Ronen I. Brafman</p><p>Abstract: Typical recommender systems adopt a static view of the recommendation process and treat it as a prediction problem. We argue that it is more appropriate to view the problem of generating recommendations as a sequential optimization problem and, consequently, that Markov decision processes (MDPs) provide a more appropriate model for recommender systems. MDPs introduce two beneﬁts: they take into account the long-term effects of each recommendation and the expected value of each recommendation. To succeed in practice, an MDP-based recommender system must employ a strong initial model, must be solvable quickly, and should not consume too much memory. In this paper, we describe our particular MDP model, its initialization using a predictive model, the solution and update algorithm, and its actual performance on a commercial site. We also describe the particular predictive model we used which outperforms previous models. Our system is one of a small number of commercially deployed recommender systems. As far as we know, it is the ﬁrst to report experimental analysis conducted on a real commercial site. These results validate the commercial value of recommender systems, and in particular, of our MDP-based approach. Keywords: recommender systems, Markov decision processes, learning, commercial applications</p><p>5 0.63968861 <a title="9-lda-5" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>Author: Lior Wolf, Amnon Shashua</p><p>Abstract: The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classiﬁcation tasks, for example, it is not uncommon to have 104 to 107 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a deﬁnition of “relevancy” based on spectral properties of the Laplacian of the features’ measurement matrix. The feature selection process is then based on a continuous ranking of the features deﬁned by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a “biased non-negativity” of a key matrix in the process. As a result, a simple leastsquares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.</p><p>6 0.62146449 <a title="9-lda-6" href="./jmlr-2005-Learning_with_Decision_Lists_of_Data-Dependent_Features.html">50 jmlr-2005-Learning with Decision Lists of Data-Dependent Features</a></p>
<p>7 0.61678147 <a title="9-lda-7" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>8 0.58636642 <a title="9-lda-8" href="./jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</a></p>
<p>9 0.55750155 <a title="9-lda-9" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>10 0.54393667 <a title="9-lda-10" href="./jmlr-2005-Active_Coevolutionary_Learning_of_Deterministic_Finite_Automata.html">8 jmlr-2005-Active Coevolutionary Learning of Deterministic Finite Automata</a></p>
<p>11 0.53608185 <a title="9-lda-11" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>12 0.53263772 <a title="9-lda-12" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>13 0.52768594 <a title="9-lda-13" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>14 0.52436751 <a title="9-lda-14" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>15 0.51504403 <a title="9-lda-15" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>16 0.51327002 <a title="9-lda-16" href="./jmlr-2005-Characterization_of_a_Family_of_Algorithms_for_Generalized_Discriminant_Analysis_on_Undersampled_Problems.html">18 jmlr-2005-Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</a></p>
<p>17 0.51205564 <a title="9-lda-17" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>18 0.50638402 <a title="9-lda-18" href="./jmlr-2005-Efficient_Computation_of_Gapped_Substring_Kernels_on_Large_Alphabets.html">28 jmlr-2005-Efficient Computation of Gapped Substring Kernels on Large Alphabets</a></p>
<p>19 0.50150919 <a title="9-lda-19" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>20 0.49773639 <a title="9-lda-20" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
