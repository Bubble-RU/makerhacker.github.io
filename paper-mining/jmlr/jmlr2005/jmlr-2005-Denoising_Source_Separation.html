<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>25 jmlr-2005-Denoising Source Separation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-25" href="#">jmlr2005-25</a> knowledge-graph by maker-knowledge-mining</p><h1>25 jmlr-2005-Denoising Source Separation</h1>
<br/><p>Source: <a title="jmlr-2005-25-pdf" href="http://jmlr.org/papers/volume6/sarela05a/sarela05a.pdf">pdf</a></p><p>Author: Jaakko Särelä, Harri Valpola</p><p>Abstract: A new algorithmic framework called denoising source separation (DSS) is introduced. The main beneﬁt of this framework is that it allows for the easy development of new source separation algorithms which can be optimised for speciﬁc problems. In this framework, source separation algorithms are constructed around denoising procedures. The resulting algorithms can range from almost blind to highly specialised source separation algorithms. Both simple linear and more complex nonlinear or adaptive denoising schemes are considered. Some existing independent component analysis algorithms are reinterpreted within the DSS framework and new, robust blind source separation algorithms are suggested. The framework is derived as a one-unit equivalent to an EM algorithm for source separation. However, in the DSS framework it is easy to utilise various kinds of denoising procedures which need not be based on generative models. In the experimental section, various DSS schemes are applied extensively to artiﬁcial data, to real magnetoencephalograms and to simulated CDMA mobile network signals. Finally, various extensions to the proposed DSS algorithms are considered. These include nonlinear observation mappings, hierarchical models and over-complete, nonorthogonal feature spaces. With these extensions, DSS appears to have relevance to many existing models of neural information processing. Keywords: blind source separation, BSS, prior information, denoising, denoising source separation, DSS, independent component analysis, ICA, magnetoencephalograms, MEG, CDMA</p><p>Reference: <a title="jmlr-2005-25-reference" href="../jmlr2005_reference/jmlr-2005-Denoising_Source_Separation_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dss', 0.656), ('deno', 0.567), ('sourc', 0.231), ('mask', 0.179), ('eno', 0.129), ('eigenvalu', 0.107), ('valpol', 0.106), ('ss', 0.097), ('arel', 0.092), ('ourc', 0.088), ('meg', 0.077), ('cdma', 0.066), ('blind', 0.063), ('spectrogram', 0.06), ('ic', 0.06), ('snr', 0.059), ('hyv', 0.053), ('shift', 0.052), ('qrs', 0.051), ('negentrop', 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="25-tfidf-1" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>2 0.1394427 <a title="25-tfidf-2" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>3 0.13287206 <a title="25-tfidf-3" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>4 0.046542574 <a title="25-tfidf-4" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>5 0.041182559 <a title="25-tfidf-5" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>6 0.040522691 <a title="25-tfidf-6" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>7 0.039649598 <a title="25-tfidf-7" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>8 0.038401145 <a title="25-tfidf-8" href="./jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</a></p>
<p>9 0.03800514 <a title="25-tfidf-9" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>10 0.037149884 <a title="25-tfidf-10" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>11 0.030447189 <a title="25-tfidf-11" href="./jmlr-2005-Matrix_Exponentiated_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">55 jmlr-2005-Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>12 0.027403297 <a title="25-tfidf-12" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>13 0.02732859 <a title="25-tfidf-13" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>14 0.026321078 <a title="25-tfidf-14" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>15 0.021489663 <a title="25-tfidf-15" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>16 0.015220286 <a title="25-tfidf-16" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>17 0.013595921 <a title="25-tfidf-17" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>18 0.013503937 <a title="25-tfidf-18" href="./jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</a></p>
<p>19 0.013158653 <a title="25-tfidf-19" href="./jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</a></p>
<p>20 0.013017564 <a title="25-tfidf-20" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.13), (1, 0.172), (2, 0.3), (3, 0.059), (4, 0.083), (5, -0.051), (6, 0.06), (7, 0.12), (8, -0.048), (9, 0.013), (10, 0.35), (11, -0.003), (12, 0.016), (13, 0.093), (14, -0.166), (15, 0.028), (16, -0.101), (17, 0.01), (18, -0.08), (19, 0.049), (20, 0.059), (21, 0.01), (22, -0.011), (23, -0.07), (24, -0.019), (25, 0.023), (26, 0.037), (27, 0.159), (28, -0.09), (29, 0.037), (30, 0.021), (31, -0.019), (32, 0.034), (33, -0.094), (34, -0.016), (35, 0.063), (36, -0.079), (37, 0.18), (38, 0.027), (39, 0.039), (40, -0.274), (41, -0.071), (42, -0.166), (43, 0.28), (44, -0.019), (45, 0.128), (46, 0.01), (47, -0.031), (48, -0.023), (49, -0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95462942 <a title="25-lsi-1" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>2 0.56068021 <a title="25-lsi-2" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>3 0.46206376 <a title="25-lsi-3" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>4 0.19992025 <a title="25-lsi-4" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>5 0.18864505 <a title="25-lsi-5" href="./jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</a></p>
<p>6 0.17481692 <a title="25-lsi-6" href="./jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p>
<p>7 0.1710725 <a title="25-lsi-7" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<p>8 0.13708186 <a title="25-lsi-8" href="./jmlr-2005-Matrix_Exponentiated_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">55 jmlr-2005-Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>9 0.1299544 <a title="25-lsi-9" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>10 0.11861235 <a title="25-lsi-10" href="./jmlr-2005-Machine_Learning_Methods_for_Predicting_Failures_in_Hard_Drives%3A__A_Multiple-Instance_Application.html">53 jmlr-2005-Machine Learning Methods for Predicting Failures in Hard Drives:  A Multiple-Instance Application</a></p>
<p>11 0.11610866 <a title="25-lsi-11" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>12 0.11293244 <a title="25-lsi-12" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>13 0.11071122 <a title="25-lsi-13" href="./jmlr-2005-Learning_Multiple_Tasks_with_Kernel_Methods.html">45 jmlr-2005-Learning Multiple Tasks with Kernel Methods</a></p>
<p>14 0.10918434 <a title="25-lsi-14" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>15 0.10654064 <a title="25-lsi-15" href="./jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</a></p>
<p>16 0.10174724 <a title="25-lsi-16" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>17 0.097925529 <a title="25-lsi-17" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>18 0.074630558 <a title="25-lsi-18" href="./jmlr-2005-Learning_from_Examples_as_an_Inverse_Problem.html">47 jmlr-2005-Learning from Examples as an Inverse Problem</a></p>
<p>19 0.068324558 <a title="25-lsi-19" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>20 0.067822777 <a title="25-lsi-20" href="./jmlr-2005-An_MDP-Based_Recommender_System.html">12 jmlr-2005-An MDP-Based Recommender System</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.027), (11, 0.021), (19, 0.371), (35, 0.019), (37, 0.112), (44, 0.041), (47, 0.01), (53, 0.027), (60, 0.014), (62, 0.067), (68, 0.027), (74, 0.015), (78, 0.028), (79, 0.024), (88, 0.036), (97, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.69357884 <a title="25-lda-1" href="./jmlr-2005-A_Bayes_Optimal_Approach_for_Partitioning_the_Values_of_Categorical_Attributes.html">1 jmlr-2005-A Bayes Optimal Approach for Partitioning the Values of Categorical Attributes</a></p>
<p>same-paper 2 0.65509719 <a title="25-lda-2" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>3 0.37232918 <a title="25-lda-3" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>4 0.36991924 <a title="25-lda-4" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>5 0.29549968 <a title="25-lda-5" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>6 0.29501668 <a title="25-lda-6" href="./jmlr-2005-Loopy_Belief_Propagation%3A_Convergence_and_Effects_of_Message_Errors.html">52 jmlr-2005-Loopy Belief Propagation: Convergence and Effects of Message Errors</a></p>
<p>7 0.29234907 <a title="25-lda-7" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>8 0.28358915 <a title="25-lda-8" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>9 0.28023174 <a title="25-lda-9" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>10 0.27875552 <a title="25-lda-10" href="./jmlr-2005-A_Unifying_View_of_Sparse_Approximate_Gaussian_Process_Regression.html">7 jmlr-2005-A Unifying View of Sparse Approximate Gaussian Process Regression</a></p>
<p>11 0.27794379 <a title="25-lda-11" href="./jmlr-2005-Asymptotics_in_Empirical_Risk_Minimization.html">16 jmlr-2005-Asymptotics in Empirical Risk Minimization</a></p>
<p>12 0.27573183 <a title="25-lda-12" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>13 0.27497631 <a title="25-lda-13" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>14 0.27296546 <a title="25-lda-14" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>15 0.27253389 <a title="25-lda-15" href="./jmlr-2005-Smooth_%CE%B5-Insensitive_Regression_by_Loss_Symmetrization.html">66 jmlr-2005-Smooth ε-Insensitive Regression by Loss Symmetrization</a></p>
<p>16 0.2725102 <a title="25-lda-16" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>17 0.27132481 <a title="25-lda-17" href="./jmlr-2005-Clustering_with_Bregman_Divergences.html">20 jmlr-2005-Clustering with Bregman Divergences</a></p>
<p>18 0.27056295 <a title="25-lda-18" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>19 0.27005377 <a title="25-lda-19" href="./jmlr-2005-Efficient_Margin_Maximizing_with_Boosting.html">29 jmlr-2005-Efficient Margin Maximizing with Boosting</a></p>
<p>20 0.26938629 <a title="25-lda-20" href="./jmlr-2005-Tree-Based_Batch_Mode_Reinforcement_Learning.html">68 jmlr-2005-Tree-Based Batch Mode Reinforcement Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
