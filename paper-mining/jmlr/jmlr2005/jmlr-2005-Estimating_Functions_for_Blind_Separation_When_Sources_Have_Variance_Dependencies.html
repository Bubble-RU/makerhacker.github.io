<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-30" href="#">jmlr2005-30</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</h1>
<br/><p>Source: <a title="jmlr-2005-30-pdf" href="http://jmlr.org/papers/volume6/kawanabe05a/kawanabe05a.pdf">pdf</a></p><p>Author: Motoaki Kawanabe, Klaus-Robert Müller</p><p>Abstract: A blind separation problem where the sources are not independent, but have variance dependencies is discussed. For this scenario Hyv¨ rinen and Hurri (2004) proposed an algorithm which requires a no assumption on distributions of sources and no parametric model of dependencies between components. In this paper, we extend the semiparametric approach of Amari and Cardoso (1997) to variance dependencies and study estimating functions for blind separation of such dependent sources. In particular, we show that many ICA algorithms are applicable to the variance-dependent model as well under mild conditions, although they should in principle not. Our results indicate that separation can be done based only on normalized sources which are adjusted to have stationary variances and is not affected by the dependent activity levels. We also study the asymptotic distribution of the quasi maximum likelihood method and the stability of the natural gradient learning in detail. Simulation results of artiﬁcial and realistic examples match well with our theoretical ﬁndings. Keywords: blind source separation, variance dependencies, independent component analysis, semiparametric statistical models, estimating functions</p><p>Reference: <a title="jmlr-2005-30-reference" href="../jmlr2005_reference/jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 IDA Kekul´ strasse 7 e 12489 Berlin, Germany and Department of Computer Science University of Potsdam August-Bebel-Strasse 89 14482 Potsdam, Germany  Editor: Aapo Hyv¨ rinen a  Abstract A blind separation problem where the sources are not independent, but have variance dependencies is discussed. [sent-7, score-0.77]
</p><p>2 For this scenario Hyv¨ rinen and Hurri (2004) proposed an algorithm which requires a no assumption on distributions of sources and no parametric model of dependencies between components. [sent-8, score-0.357]
</p><p>3 In this paper, we extend the semiparametric approach of Amari and Cardoso (1997) to variance dependencies and study estimating functions for blind separation of such dependent sources. [sent-9, score-0.68]
</p><p>4 Our results indicate that separation can be done based only on normalized sources which are adjusted to have stationary variances and is not affected by the dependent activity levels. [sent-11, score-0.434]
</p><p>5 We also study the asymptotic distribution of the quasi maximum likelihood method and the stability of the natural gradient learning in detail. [sent-12, score-0.285]
</p><p>6 Keywords: blind source separation, variance dependencies, independent component analysis, semiparametric statistical models, estimating functions  1. [sent-14, score-0.605]
</p><p>7 The basic model assumes that the observed signals are linear superpositions of underlying hidden source signals. [sent-22, score-0.272]
</p><p>8 Let us denote the n source signals by the vector c 2005 Motoaki Kawanabe and Klaus-Robert M¨ ller. [sent-23, score-0.272]
</p><p>9 For simplicity, we consider the case where the number of source signals equals that of observed signals (n = m). [sent-33, score-0.444]
</p><p>10 In most blind source separation (BSS) methods, the source signals are assumed to be statistically independent. [sent-35, score-0.738]
</p><p>11 By using non-Gaussianity of the sources, the mixing matrix can be estimated and the source signals can be extracted under appropriate conditions. [sent-37, score-0.337]
</p><p>12 The second-order methods are applicable to the case where the source signals have (lagged) auto-correlation. [sent-39, score-0.272]
</p><p>13 Among many extensions of the basic ICA models, several researchers have studied the case where the source signals are not independent (for example, Cardoso, 1998b; Hyv¨ rinen et al. [sent-41, score-0.41]
</p><p>14 They simply assume that the sources are dependent only through their variances and that the sources have temporal correlation. [sent-47, score-0.371]
</p><p>15 , 2001a), the dependencies of the a sources are also caused only by their variances, but in contrast to the double blind case, they are determined by a preﬁxed neighborhood relation. [sent-49, score-0.512]
</p><p>16 In particular, they showed that the quasi maximum likelihood (QML) estimation and the natural gradient learning give a correct solution regardless of the true source densities which satisfy certain mild conditions. [sent-54, score-0.315]
</p><p>17 Thus our consistency results indicate that separation can be done based only on normalized sources which are adjusted to have stationary variances and is not affected by the dependent activity levels. [sent-64, score-0.475]
</p><p>18 Among several ICA algorithms, the quasi maximum likelihood method and its online version, the natural gradient learning are discussed in detail. [sent-69, score-0.25]
</p><p>19 We study the asymptotic distributions of the quasi maximum likelihood method (Section 5. [sent-70, score-0.252]
</p><p>20 In particular, we carried out two experiments, where we extract two speech signals with high variance dependencies. [sent-77, score-0.271]
</p><p>21 Variance-Dependent BSS Model Hyv¨ rinen and Hurri (2004) formalized the probabilistic framework of variance-dependent blind a separation. [sent-81, score-0.387]
</p><p>22 Let us assume that each source signal si (t) is a product of non-negative activity level vi (t) and underlying i. [sent-82, score-0.473]
</p><p>23 In practice, the activity levels vi (t) are often dependent among different signals and each observed signal is expressed as n  xi (t) =  ∑ ai j v j (t)z j (t),  i = 1, . [sent-96, score-0.59]
</p><p>24 , n,  j=1  where vi (t) and zi (t) satisfy: (i) vi (t) and z j (t ) are independent for all i, j, t and t , (ii) each zi (t) is i. [sent-99, score-0.376]
</p><p>25 Regarding the general activity levels vi ’s, vi (t) and v j (t) are allowed to be statistically dependent, and furthermore, no particular assumption on these dependencies is made (double blind situation). [sent-107, score-0.801]
</p><p>26 455  ¨ K AWANABE AND M ULLER  =  ×  =  ×  source (s1 , s2 )  activity level (v1 , v2 )  normalized signal (z1 , z2 )  Figure 1: Sources (s1 , s2 ) with variance dependencies in the variance-dependent BSS model. [sent-111, score-0.41]
</p><p>27 However, since the sequences z1 and z2 are multiplied by extremely dependent activity levels v1 and v2 , respectively, the short-term variance of the source signals s1 and s2 are highly correlated. [sent-113, score-0.563]
</p><p>28 It is important to remark that the nonstationary algorithm by Pham and Cardoso (2000) was also designed for the same source model (1), except that vi (t)’s are assumed to be deterministic and slowly varying. [sent-121, score-0.324]
</p><p>29 , bn ) = A−1 is the demixing matrix to be estimated and ρs (s) = n  Π ρsi (si ) is the density of the sources s. [sent-129, score-0.321]
</p><p>30 , xn (t) )  source signals at t  v(t) = (v1 (t), . [sent-147, score-0.272]
</p><p>31 , vn (t) )  general activity levels of the sources s(t)  V = (v(1), . [sent-150, score-0.405]
</p><p>32 , zn (t) )  normalized source signals by the activity levels v(t)  A  n × n mixing matrix  B = (bi j ) = (b1 , . [sent-156, score-0.581]
</p><p>33 , bn )  demixing matrix which is equivalent to A−1  n  ρz (z) = Π ρzi (zi )  density of the normalized source signals z  ρV (V )  density of the entire sequence V = (v(1), . [sent-159, score-0.432]
</p><p>34 , v(T ) )  i=1  of the activity levels y(t) = Bx(t)  extracted sources by the demixing matrix B  ¯ F(x, B) or F(X, B)  estimating function which is an n × n matrix-valued function of the data and the parameter B  vec(F)  vectorization operator  = (F11 , . [sent-162, score-0.661]
</p><p>35 , Fnn )  Table 1: List of notations used in the variance-dependent BSS model In the variance-dependent BSS model which we consider, the sources s(t) are decomposed of two components, the normalized signals z(t) = (z1 (t), . [sent-171, score-0.333]
</p><p>36 Since the former has mutual independence like the ICA model, the density of the data X is factorized as T  n  1 ρz vi (t) i t=1 i=1  p(X|V ; B, ρz ) = | det B|T ∏ ∏  bi x(t) vi (t)  ,  (3)  when V = (v(1), . [sent-178, score-0.312]
</p><p>37 It should be noted that both in usual ICA models and in the variance-dependent BSS model, scales and orders of the sources cannot be determined, that is, two matrices B and PDB indicate the same distribution, when P and D are a permutation and a diagonal matrix respectively (Comon, 1994). [sent-234, score-0.251]
</p><p>38 In general the quasi maximum likelihood estimator is no longer consistent because of misspeciﬁed distribution. [sent-254, score-0.266]
</p><p>39 However, in the ICA model (2), Amari and Cardoso (1997) found that the quasi maximum likelihood method and its online version (the natural gradient learning) give an asymptotically consistent estimator, provided that F(x, B) = I − ϕ{Bx}(Bx) satisﬁes (9) and (10). [sent-255, score-0.277]
</p><p>40 1 that the quasi maximum likelihood method (11) still gives a consistent estimator even under this extended situation. [sent-259, score-0.266]
</p><p>41 s(t)) under ﬁxed activity levels V , while E[·|ρV ] denotes the expectation over the activity level V . [sent-286, score-0.4]
</p><p>42 1 Asymptotic Distribution of the Quasi Maximum Likelihood Estimator In this section, it is shown that the quasi maximum likelihood method (11) as for example Pham and Garrat (1997); Bell and Sejnowski (1995) still gives a consistent estimator even under the extended model (3) and (4). [sent-313, score-0.266]
</p><p>43 In that case, the quasi maximum likelihood estimator BQML de¯ rived from the equation F QML (X, BQML ) = 0 is consistent regardless of the true nuisance functions ∗ , ρ∗ ) under appropriate regularity conditions. [sent-339, score-0.329]
</p><p>44 The natural gradient learning (Amari, 1998) B(t + 1) = B(t) + η(t) I − ϕ{y(t)} y (t) B(t),  (29)  is an online algorithm based on the quasi maximum likelihood method, where y(t) = B(t)x(t) is the current estimator of the sources and η(t) is an appropriate learning constant. [sent-344, score-0.462]
</p><p>45 If we apply the online algorithm (29) to data with highly nonstationary variances like speech, the scale factor of the demixing matrix B changes substantially from time to time and never converges. [sent-352, score-0.252]
</p><p>46 If all eigenvalues have negative real parts, then the equilib∂vec(B) ∗ is asymptotically stable for the ﬁxed activity levels V . [sent-364, score-0.271]
</p><p>47 Since the matrix can be expressed rium B as ∂ vec{ E F NG (x(t), B∗ ) V B∗ } ∂ vec E F NG V ¯ ¯ = B∗ (B∗ )−1 , (33) ∂ vec(B) ∂ vec(χ) ¯ ¯i where B∗ = (B∗j;kl ) = (δik b∗j ), and derivative w. [sent-365, score-0.285]
</p><p>48 Theorem 4 If the stochastic process V = {v(t), t ≥ 0} of the activity levels satisﬁes the conditions ∗ (34) – (36) with probability 1 as for the true parameter (B∗ , ρ∗ , ρV ), then the true demixing matrix z ∗ becomes an asymptotically stable equilibrium of the ﬂow (30) with probability 1. [sent-371, score-0.465]
</p><p>49 p+1 p+1 E[ vi ] E[ v j ]  −1  , ﬁnally we  For the cubic function ϕi (yi ) = y3 , not as in the ICA model, the condition that all signals are subi Gaussian E[ |zi |4 ] <3 γ3i = 2 E[ |zi |2 ] is not enough, but the variation of activity levels vi from (1) should be taken into account. [sent-385, score-0.692]
</p><p>50 3 Properties of Other BSS Algorithms Although we concentrated on estimating functions of the form (20), we can deal with more general functions and investigate other ICA algorithms within the framework of estimating functions and asymptotic estimating functions (see also Cardoso, 1997). [sent-397, score-0.325]
</p><p>51 The double blind algorithm (Hyv¨ rinen and Hurri, 2004) cannot be applied to the case where the variance structures of sources a are the same or there is no temporal variance-dependency. [sent-406, score-0.688]
</p><p>52 The nonstationary algorithm by Pham and Cardoso (2000) is not applicable to the case where time courses of the activity levels are proportional to each other. [sent-407, score-0.301]
</p><p>53 The eight batch algorithms and the online versions of the quasi maximum likelihood methods listed in Table 3 were applied to those data sets. [sent-413, score-0.25]
</p><p>54 468  E STIMATING F UNCTIONS FOR VARIANCE -D EPENDENT BSS  algorithm FastICA Hyv¨ rinen (1999) a double blind Hyv¨ rinen and Hurri (2004) a JADE Cardoso and Souloumiac (1993) TDSEP/SOBI Ziehe and M¨ ller (1998) u Belouchrani et al. [sent-419, score-0.612]
</p><p>55 asymptotically yes  always (since we consider here only the case without auto-correlations)  yes  Time course of the activity levels are proportional to each other. [sent-423, score-0.271]
</p><p>56 1 Artiﬁcial Data Sets In all artiﬁcial data sets, ﬁve source signals of various types with length T = 10000 were generated and data after multiplying a random 5 × 5 mixing matrix were observed. [sent-429, score-0.337]
</p><p>57 In the third and the fourth data, the activity levels vi (t) are sinusoidal functions with different frequencies. [sent-452, score-0.369]
</p><p>58 ’Sepagaus’ also showed 470  E STIMATING F UNCTIONS FOR VARIANCE -D EPENDENT BSS  QML(tanh)  QML(pow3)  Online(tanh)  Online(pow3)  ’DoubleBlind’  ar subG ar uni sin supG sin subG com supG com subG exp supG uni subG sss v12  8. [sent-464, score-0.352]
</p><p>59 21  JADE  FastICA(tanh)  FastICA(pow3)  TDSEP/SOBI  ’Sepagaus’  ar subG ar uni sin supG sin subG com supG com subG exp supG uni subG sss v12  10. [sent-557, score-0.352]
</p><p>60 The double blind algorithm (’DoubleBlind’) by Hyv¨ rinen and Hurri (2004) does not work a when (i) all vi ’s have same temporal structure, and (ii) there exist no temporal dependencies in vi ’s. [sent-653, score-0.837]
</p><p>61 ’Sepagaus’ does not have a guarantee to separate sources either, because smoothed sequences of the activity levels are nearly proportional to each other (see Table 2). [sent-654, score-0.405]
</p><p>62 Speech and audio signals have often been used as sources s(t) even for experiments of the instantaneous ICA model. [sent-702, score-0.333]
</p><p>63 We used the separated signals of their second demo as the sources, because their separation quality is good enough. [sent-705, score-0.289]
</p><p>64 Figure 2 shows the sources and the estimators of their activity levels with 8. [sent-706, score-0.431]
</p><p>65 We inserted one short pause at different positions of both sequences to make correlation of the activity levels of the modiﬁed signals much larger (0. [sent-711, score-0.416]
</p><p>66 Figure 3 shows the sources and the estimators of the activity levels. [sent-714, score-0.343]
</p><p>67 Correlation of the activity levels of the arranged signals becomes 0. [sent-716, score-0.416]
</p><p>68 1  2 40000  80000  120000  40000  80000  120000  1  2  Figure 2: The sources of the data set ’sss’ and the estimators of their activity levels. [sent-718, score-0.343]
</p><p>69 1  2 10000  20000  30000  40000  10000  20000  30000  40000  1  2  Figure 3: The sources of the data set ’v12’ and the estimators of their activity levels. [sent-721, score-0.343]
</p><p>70 A 2 × 2 mixing matrix A was randomly generated 100 times and 100 different mixtures of the source signals were made. [sent-724, score-0.337]
</p><p>71 Conclusions In this paper, we discussed semiparametric estimation for blind source separation, when sources have variance dependencies. [sent-741, score-0.67]
</p><p>72 Hyv¨ rinen and Hurri (2004) introduced the double blind setting where, a in addition to source distributions, dependencies between components are not restricted by any parametric model. [sent-742, score-0.589]
</p><p>73 In the presence of these two nuisance parameters (densities of activity level and underlying signal), they proposed an algorithm based on lagged 4-th order cumulants. [sent-743, score-0.278]
</p><p>74 Although their algorithm works well in many cases, it fails if (i) all vi ’s have similar temporal structure, or (ii) there exist no temporal dependencies in vi ’s. [sent-744, score-0.406]
</p><p>75 Extending the semiparametric approach (Amari and Cardoso, 1997) under variance dependencies, we investigated estimating functions for the variance-dependent BSS model. [sent-746, score-0.256]
</p><p>76 In particular, we proved that the quasi maximum likelihood estimator is derived from an estimating function, and is hence consistent regardless of the true nuisance densities (which satisfy certain mild conditions). [sent-747, score-0.425]
</p><p>77 We also analyzed other ICA algorithms within the framework of (asymptotic) estimating functions and showed that many of them can separate sources with coherent variances. [sent-748, score-0.257]
</p><p>78 Comments on Other Selected BSS Algorithms We will discuss in the following the local consistency of ICA/BSS algorithms except the quasi maximum likelihood method. [sent-774, score-0.256]
</p><p>79 1 FastICA FastICA is one of the standard algorithms for blind source separation. [sent-776, score-0.349]
</p><p>80 We use, in the following the notation W for the demixing matrix after whitening in order to distinguish it from the total demixing matrix B = WC−1/2 including whitening process. [sent-782, score-0.32]
</p><p>81 , n,  (40)  t=1  then it is easy to show that the expectations of the left hand side of (38) and (39) vanish regardless of the nuisance functions ρz and ρV , in the same way as for the quasi maximum likelihood method. [sent-796, score-0.278]
</p><p>82 If the other regularity conditions hold, it becomes an estimating function and the estimator B derived from it converges to the correct demixing matrix B∗ = (A∗ )−1 with a permutation matrix P and a diagonal matrix D. [sent-798, score-0.407]
</p><p>83 Although the estimating function is similar to that of the quasi maximum likelihood, FastICA algorithm is based on the Newton’s algorithm, and therefore, it has globally more stable dynamics than the natural gradient learning. [sent-799, score-0.273]
</p><p>84 2 The Double Blind Algorithm by Hyv¨ rinen and Hurri (2004) a Hyv¨ rinen and Hurri (2004) proposed an algorithm for separating sources under the double blind a situation. [sent-801, score-0.73]
</p><p>85 Provided that the matrix K = (Ki j ) is non-singular, the quantity J is maximized when Q is a signed permutation matrix, that is, by maximizing the criterion J we can estimate the true demixing matrix B∗ = (A∗ )−1 up to signed permutation matrices. [sent-805, score-0.308]
</p><p>86 If the other regularity condition holds, F(X, B) turns out to be an asymptotic estimating function which is asymptotically equivalent to an estimating function and the estimator B converges to the correct demixing matrix B∗ = (A∗ )−1 . [sent-817, score-0.493]
</p><p>87 If W is the true demixing matrix and yi ’s are extracted signals with W , the components Ki jkl := cum(yi , y j , yk , yl ) of the expected cumulant are zero except for i = j = k = l or i = j = k = l or i = l = j = k. [sent-822, score-0.434]
</p><p>88 Thus, one needs only to show that the estimating equation is associated to the minimization of 2 ∑ |cum(yi , y j , yi , y j )|2 under the orthogonality constraints which is satisﬁed i= j  when yi equals the true sources (up to a scaling and a permutation). [sent-823, score-0.355]
</p><p>89 When the sources si ’s are mutually independent and have temporal covariance structure, the demixing matrix PD(A∗ )−1 can diagonalize all lagged covariance matrices R(∆t), where P is a permutation matrix and D is a diagonal matrix. [sent-829, score-0.538]
</p><p>90 This property has been used in blind separation methods with second order statistics (Tong et al. [sent-830, score-0.366]
</p><p>91 An information-maximization approach to blind separation and blind deconvolution. [sent-898, score-0.615]
</p><p>92 A blind source separation techinique based on second order statistics. [sent-906, score-0.466]
</p><p>93 Injecting noise for analysing the stability of ica u components. [sent-976, score-0.268]
</p><p>94 Blind separation of sources that have spatiotemporal variance depena dencies. [sent-998, score-0.325]
</p><p>95 Estimating functions for blind separation when source have u variance-dependencies. [sent-1021, score-0.466]
</p><p>96 On-line learning in Switching and Drifting u environments with application to blind source separation, pages 93–110. [sent-1084, score-0.349]
</p><p>97 Blind separation of instantaneous mixture sources via an independent component analysis. [sent-1121, score-0.278]
</p><p>98 Blind separation of mixture of independent sources through a quasimaximum likelihood approach. [sent-1137, score-0.316]
</p><p>99 Adaptive on-line learning algorithms for blind separation: Maximum entropy and minimum mutual information. [sent-1189, score-0.249]
</p><p>100 TDSEP – an efﬁcient algorithm for blind separation using time strucu ture. [sent-1195, score-0.366]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bss', 0.32), ('qml', 0.304), ('vec', 0.251), ('blind', 0.249), ('ica', 0.235), ('fastica', 0.188), ('quasi', 0.177), ('signals', 0.172), ('sources', 0.161), ('subg', 0.16), ('activity', 0.156), ('hyv', 0.143), ('rinen', 0.138), ('cardoso', 0.131), ('tanh', 0.129), ('hurri', 0.127), ('awanabe', 0.127), ('uller', 0.127), ('demixing', 0.126), ('vi', 0.125), ('bx', 0.12), ('stimating', 0.118), ('separation', 0.117), ('semiparametric', 0.113), ('source', 0.1), ('ependent', 0.099), ('amari', 0.097), ('estimating', 0.096), ('supg', 0.092), ('jade', 0.088), ('unctions', 0.088), ('levels', 0.088), ('doubleblind', 0.084), ('sepagaus', 0.084), ('ziehe', 0.069), ('unbiasedness', 0.067), ('meinecke', 0.064), ('nuisance', 0.063), ('zi', 0.063), ('cum', 0.059), ('lagged', 0.059), ('dependencies', 0.058), ('kawanabe', 0.057), ('pham', 0.057), ('nonstationary', 0.057), ('fii', 0.056), ('ar', 0.055), ('av', 0.053), ('speech', 0.052), ('estimator', 0.051), ('temporal', 0.049), ('signal', 0.049), ('yi', 0.049), ('variance', 0.047), ('ki', 0.045), ('double', 0.044), ('ller', 0.043), ('si', 0.043), ('sss', 0.042), ('vip', 0.042), ('remark', 0.042), ('consistency', 0.041), ('ng', 0.041), ('bi', 0.039), ('likelihood', 0.038), ('asymptotic', 0.037), ('sl', 0.035), ('online', 0.035), ('sin', 0.035), ('equilibrium', 0.034), ('matrix', 0.034), ('com', 0.034), ('fji', 0.034), ('jjade', 0.034), ('kil', 0.034), ('kli', 0.034), ('stability', 0.033), ('permutation', 0.032), ('cov', 0.031), ('uni', 0.031), ('mixing', 0.031), ('nonlinearity', 0.029), ('yk', 0.028), ('asymptotically', 0.027), ('bell', 0.026), ('estimators', 0.026), ('condition', 0.026), ('amariindex', 0.025), ('godambe', 0.025), ('jkl', 0.025), ('wc', 0.025), ('jl', 0.025), ('murata', 0.025), ('belouchrani', 0.025), ('signed', 0.025), ('harmeling', 0.025), ('worked', 0.025), ('fi', 0.024), ('scales', 0.024), ('det', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="30-tfidf-1" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>Author: Motoaki Kawanabe, Klaus-Robert Müller</p><p>Abstract: A blind separation problem where the sources are not independent, but have variance dependencies is discussed. For this scenario Hyv¨ rinen and Hurri (2004) proposed an algorithm which requires a no assumption on distributions of sources and no parametric model of dependencies between components. In this paper, we extend the semiparametric approach of Amari and Cardoso (1997) to variance dependencies and study estimating functions for blind separation of such dependent sources. In particular, we show that many ICA algorithms are applicable to the variance-dependent model as well under mild conditions, although they should in principle not. Our results indicate that separation can be done based only on normalized sources which are adjusted to have stationary variances and is not affected by the dependent activity levels. We also study the asymptotic distribution of the quasi maximum likelihood method and the stability of the natural gradient learning in detail. Simulation results of artiﬁcial and realistic examples match well with our theoretical ﬁndings. Keywords: blind source separation, variance dependencies, independent component analysis, semiparametric statistical models, estimating functions</p><p>2 0.21109863 <a title="30-tfidf-2" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>Author: Luís B. Almeida</p><p>Abstract: When acquiring an image of a paper document, the image printed on the back page sometimes shows through. The mixture of the front- and back-page images thus obtained is markedly nonlinear, and thus constitutes a good real-life test case for nonlinear blind source separation. This paper addresses a difﬁcult version of this problem, corresponding to the use of “onion skin” paper, which results in a relatively strong nonlinearity of the mixture, which becomes close to singular in the lighter regions of the images. The separation is achieved through the MISEP technique, which is an extension of the well known INFOMAX method. The separation results are assessed with objective quality measures. They show an improvement over the results obtained with linear separation, but have room for further improvement. Keywords: ICA, blind source separation, nonlinear mixtures, nonlinear separation, image mixture, image separation</p><p>3 0.17053783 <a title="30-tfidf-3" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>Author: Jaakko Särelä, Harri Valpola</p><p>Abstract: A new algorithmic framework called denoising source separation (DSS) is introduced. The main beneﬁt of this framework is that it allows for the easy development of new source separation algorithms which can be optimised for speciﬁc problems. In this framework, source separation algorithms are constructed around denoising procedures. The resulting algorithms can range from almost blind to highly specialised source separation algorithms. Both simple linear and more complex nonlinear or adaptive denoising schemes are considered. Some existing independent component analysis algorithms are reinterpreted within the DSS framework and new, robust blind source separation algorithms are suggested. The framework is derived as a one-unit equivalent to an EM algorithm for source separation. However, in the DSS framework it is easy to utilise various kinds of denoising procedures which need not be based on generative models. In the experimental section, various DSS schemes are applied extensively to artiﬁcial data, to real magnetoencephalograms and to simulated CDMA mobile network signals. Finally, various extensions to the proposed DSS algorithms are considered. These include nonlinear observation mappings, hierarchical models and over-complete, nonorthogonal feature spaces. With these extensions, DSS appears to have relevance to many existing models of neural information processing. Keywords: blind source separation, BSS, prior information, denoising, denoising source separation, DSS, independent component analysis, ICA, magnetoencephalograms, MEG, CDMA</p><p>4 0.13119859 <a title="30-tfidf-4" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>Author: Aapo Hyvärinen</p><p>Abstract: One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data. Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence</p><p>5 0.10871425 <a title="30-tfidf-5" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>Author: Arthur Gretton, Ralf Herbrich, Alexander Smola, Olivier Bousquet, Bernhard Schölkopf</p><p>Abstract: We introduce two new functionals, the constrained covariance and the kernel mutual information, to measure the degree of independence of random variables. These quantities are both based on the covariance between functions of the random variables in reproducing kernel Hilbert spaces (RKHSs). We prove that when the RKHSs are universal, both functionals are zero if and only if the random variables are pairwise independent. We also show that the kernel mutual information is an upper bound near independence on the Parzen window estimate of the mutual information. Analogous results apply for two correlation-based dependence functionals introduced earlier: we show the kernel canonical correlation and the kernel generalised variance to be independence measures for universal kernels, and prove the latter to be an upper bound on the mutual information near independence. The performance of the kernel dependence functionals in measuring independence is veriﬁed in the context of independent component analysis. Keywords: independence, covariance operator, mutual information, kernel, Parzen window estimate, independent component analysis c 2005 Arthur Gretton, Ralf Herbrich, Alexander Smola, Olivier Bousquet and Bernhard Schölkopf . G RETTON , H ERBRICH , S MOLA , B OUSQUET AND S CHÖLKOPF</p><p>6 0.10214231 <a title="30-tfidf-6" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>7 0.04241588 <a title="30-tfidf-7" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>8 0.04007059 <a title="30-tfidf-8" href="./jmlr-2005-Learning_with_Decision_Lists_of_Data-Dependent_Features.html">50 jmlr-2005-Learning with Decision Lists of Data-Dependent Features</a></p>
<p>9 0.038630851 <a title="30-tfidf-9" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>10 0.037411004 <a title="30-tfidf-10" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>11 0.036710184 <a title="30-tfidf-11" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>12 0.031234957 <a title="30-tfidf-12" href="./jmlr-2005-Stability_of_Randomized_Learning_Algorithms.html">67 jmlr-2005-Stability of Randomized Learning Algorithms</a></p>
<p>13 0.03102156 <a title="30-tfidf-13" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>14 0.029425606 <a title="30-tfidf-14" href="./jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error.html">13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</a></p>
<p>15 0.028333535 <a title="30-tfidf-15" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>16 0.026049038 <a title="30-tfidf-16" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>17 0.024316281 <a title="30-tfidf-17" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>18 0.023918172 <a title="30-tfidf-18" href="./jmlr-2005-Frames%2C_Reproducing_Kernels%2C_Regularization_and_Learning.html">35 jmlr-2005-Frames, Reproducing Kernels, Regularization and Learning</a></p>
<p>19 0.023590151 <a title="30-tfidf-19" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>20 0.022904582 <a title="30-tfidf-20" href="./jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2005_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.205), (1, 0.351), (2, -0.434), (3, 0.032), (4, -0.136), (5, -0.121), (6, 0.145), (7, 0.061), (8, 0.033), (9, -0.065), (10, 0.116), (11, 0.185), (12, -0.061), (13, -0.039), (14, 0.01), (15, -0.013), (16, -0.031), (17, 0.043), (18, -0.009), (19, 0.035), (20, 0.015), (21, 0.074), (22, 0.032), (23, -0.043), (24, 0.067), (25, -0.016), (26, -0.023), (27, 0.032), (28, -0.049), (29, 0.028), (30, 0.002), (31, -0.001), (32, 0.004), (33, -0.045), (34, -0.022), (35, 0.043), (36, -0.029), (37, 0.005), (38, -0.041), (39, -0.001), (40, -0.036), (41, 0.02), (42, 0.009), (43, 0.008), (44, 0.012), (45, -0.001), (46, 0.031), (47, 0.065), (48, -0.024), (49, -0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97343957 <a title="30-lsi-1" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>Author: Motoaki Kawanabe, Klaus-Robert Müller</p><p>Abstract: A blind separation problem where the sources are not independent, but have variance dependencies is discussed. For this scenario Hyv¨ rinen and Hurri (2004) proposed an algorithm which requires a no assumption on distributions of sources and no parametric model of dependencies between components. In this paper, we extend the semiparametric approach of Amari and Cardoso (1997) to variance dependencies and study estimating functions for blind separation of such dependent sources. In particular, we show that many ICA algorithms are applicable to the variance-dependent model as well under mild conditions, although they should in principle not. Our results indicate that separation can be done based only on normalized sources which are adjusted to have stationary variances and is not affected by the dependent activity levels. We also study the asymptotic distribution of the quasi maximum likelihood method and the stability of the natural gradient learning in detail. Simulation results of artiﬁcial and realistic examples match well with our theoretical ﬁndings. Keywords: blind source separation, variance dependencies, independent component analysis, semiparametric statistical models, estimating functions</p><p>2 0.80432427 <a title="30-lsi-2" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>Author: Jaakko Särelä, Harri Valpola</p><p>Abstract: A new algorithmic framework called denoising source separation (DSS) is introduced. The main beneﬁt of this framework is that it allows for the easy development of new source separation algorithms which can be optimised for speciﬁc problems. In this framework, source separation algorithms are constructed around denoising procedures. The resulting algorithms can range from almost blind to highly specialised source separation algorithms. Both simple linear and more complex nonlinear or adaptive denoising schemes are considered. Some existing independent component analysis algorithms are reinterpreted within the DSS framework and new, robust blind source separation algorithms are suggested. The framework is derived as a one-unit equivalent to an EM algorithm for source separation. However, in the DSS framework it is easy to utilise various kinds of denoising procedures which need not be based on generative models. In the experimental section, various DSS schemes are applied extensively to artiﬁcial data, to real magnetoencephalograms and to simulated CDMA mobile network signals. Finally, various extensions to the proposed DSS algorithms are considered. These include nonlinear observation mappings, hierarchical models and over-complete, nonorthogonal feature spaces. With these extensions, DSS appears to have relevance to many existing models of neural information processing. Keywords: blind source separation, BSS, prior information, denoising, denoising source separation, DSS, independent component analysis, ICA, magnetoencephalograms, MEG, CDMA</p><p>3 0.78800315 <a title="30-lsi-3" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>Author: Luís B. Almeida</p><p>Abstract: When acquiring an image of a paper document, the image printed on the back page sometimes shows through. The mixture of the front- and back-page images thus obtained is markedly nonlinear, and thus constitutes a good real-life test case for nonlinear blind source separation. This paper addresses a difﬁcult version of this problem, corresponding to the use of “onion skin” paper, which results in a relatively strong nonlinearity of the mixture, which becomes close to singular in the lighter regions of the images. The separation is achieved through the MISEP technique, which is an extension of the well known INFOMAX method. The separation results are assessed with objective quality measures. They show an improvement over the results obtained with linear separation, but have room for further improvement. Keywords: ICA, blind source separation, nonlinear mixtures, nonlinear separation, image mixture, image separation</p><p>4 0.3959417 <a title="30-lsi-4" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>Author: Arthur Gretton, Ralf Herbrich, Alexander Smola, Olivier Bousquet, Bernhard Schölkopf</p><p>Abstract: We introduce two new functionals, the constrained covariance and the kernel mutual information, to measure the degree of independence of random variables. These quantities are both based on the covariance between functions of the random variables in reproducing kernel Hilbert spaces (RKHSs). We prove that when the RKHSs are universal, both functionals are zero if and only if the random variables are pairwise independent. We also show that the kernel mutual information is an upper bound near independence on the Parzen window estimate of the mutual information. Analogous results apply for two correlation-based dependence functionals introduced earlier: we show the kernel canonical correlation and the kernel generalised variance to be independence measures for universal kernels, and prove the latter to be an upper bound on the mutual information near independence. The performance of the kernel dependence functionals in measuring independence is veriﬁed in the context of independent component analysis. Keywords: independence, covariance operator, mutual information, kernel, Parzen window estimate, independent component analysis c 2005 Arthur Gretton, Ralf Herbrich, Alexander Smola, Olivier Bousquet and Bernhard Schölkopf . G RETTON , H ERBRICH , S MOLA , B OUSQUET AND S CHÖLKOPF</p><p>5 0.39273372 <a title="30-lsi-5" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>Author: Aapo Hyvärinen</p><p>Abstract: One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data. Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence</p><p>6 0.35716137 <a title="30-lsi-6" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>7 0.15764308 <a title="30-lsi-7" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>8 0.14997822 <a title="30-lsi-8" href="./jmlr-2005-Learning_with_Decision_Lists_of_Data-Dependent_Features.html">50 jmlr-2005-Learning with Decision Lists of Data-Dependent Features</a></p>
<p>9 0.14739604 <a title="30-lsi-9" href="./jmlr-2005-Asymptotic_Model_Selection_for_Naive_Bayesian_Networks.html">15 jmlr-2005-Asymptotic Model Selection for Naive Bayesian Networks</a></p>
<p>10 0.14591539 <a title="30-lsi-10" href="./jmlr-2005-Stability_of_Randomized_Learning_Algorithms.html">67 jmlr-2005-Stability of Randomized Learning Algorithms</a></p>
<p>11 0.13412131 <a title="30-lsi-11" href="./jmlr-2005-A_Modified_Finite_Newton_Method_for_Fast_Solution_of_Large_Scale_Linear_SVMs.html">6 jmlr-2005-A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs</a></p>
<p>12 0.13180043 <a title="30-lsi-12" href="./jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error.html">13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</a></p>
<p>13 0.11777526 <a title="30-lsi-13" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>14 0.11701227 <a title="30-lsi-14" href="./jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</a></p>
<p>15 0.11596604 <a title="30-lsi-15" href="./jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</a></p>
<p>16 0.111631 <a title="30-lsi-16" href="./jmlr-2005-Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification.html">14 jmlr-2005-Assessing Approximate Inference for Binary Gaussian Process Classification</a></p>
<p>17 0.10698949 <a title="30-lsi-17" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>18 0.10335971 <a title="30-lsi-18" href="./jmlr-2005-Frames%2C_Reproducing_Kernels%2C_Regularization_and_Learning.html">35 jmlr-2005-Frames, Reproducing Kernels, Regularization and Learning</a></p>
<p>19 0.10281343 <a title="30-lsi-19" href="./jmlr-2005-Change_Point_Problems_in_Linear_Dynamical_Systems.html">17 jmlr-2005-Change Point Problems in Linear Dynamical Systems</a></p>
<p>20 0.10187912 <a title="30-lsi-20" href="./jmlr-2005-What%27s_Strange_About_Recent_Events_%28WSARE%29%3A_An_Algorithm_for_the_Early_Detection_of_Disease_Outbreaks.html">72 jmlr-2005-What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2005_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.515), (13, 0.017), (17, 0.019), (19, 0.021), (36, 0.027), (37, 0.028), (43, 0.023), (47, 0.02), (49, 0.012), (52, 0.064), (59, 0.014), (70, 0.025), (80, 0.028), (88, 0.066), (90, 0.022), (94, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7923879 <a title="30-lda-1" href="./jmlr-2005-Estimating_Functions_for_Blind_Separation_When_Sources_Have_Variance_Dependencies.html">30 jmlr-2005-Estimating Functions for Blind Separation When Sources Have Variance Dependencies</a></p>
<p>Author: Motoaki Kawanabe, Klaus-Robert Müller</p><p>Abstract: A blind separation problem where the sources are not independent, but have variance dependencies is discussed. For this scenario Hyv¨ rinen and Hurri (2004) proposed an algorithm which requires a no assumption on distributions of sources and no parametric model of dependencies between components. In this paper, we extend the semiparametric approach of Amari and Cardoso (1997) to variance dependencies and study estimating functions for blind separation of such dependent sources. In particular, we show that many ICA algorithms are applicable to the variance-dependent model as well under mild conditions, although they should in principle not. Our results indicate that separation can be done based only on normalized sources which are adjusted to have stationary variances and is not affected by the dependent activity levels. We also study the asymptotic distribution of the quasi maximum likelihood method and the stability of the natural gradient learning in detail. Simulation results of artiﬁcial and realistic examples match well with our theoretical ﬁndings. Keywords: blind source separation, variance dependencies, independent component analysis, semiparametric statistical models, estimating functions</p><p>2 0.32050067 <a title="30-lda-2" href="./jmlr-2005-Denoising_Source_Separation.html">25 jmlr-2005-Denoising Source Separation</a></p>
<p>Author: Jaakko Särelä, Harri Valpola</p><p>Abstract: A new algorithmic framework called denoising source separation (DSS) is introduced. The main beneﬁt of this framework is that it allows for the easy development of new source separation algorithms which can be optimised for speciﬁc problems. In this framework, source separation algorithms are constructed around denoising procedures. The resulting algorithms can range from almost blind to highly specialised source separation algorithms. Both simple linear and more complex nonlinear or adaptive denoising schemes are considered. Some existing independent component analysis algorithms are reinterpreted within the DSS framework and new, robust blind source separation algorithms are suggested. The framework is derived as a one-unit equivalent to an EM algorithm for source separation. However, in the DSS framework it is easy to utilise various kinds of denoising procedures which need not be based on generative models. In the experimental section, various DSS schemes are applied extensively to artiﬁcial data, to real magnetoencephalograms and to simulated CDMA mobile network signals. Finally, various extensions to the proposed DSS algorithms are considered. These include nonlinear observation mappings, hierarchical models and over-complete, nonorthogonal feature spaces. With these extensions, DSS appears to have relevance to many existing models of neural information processing. Keywords: blind source separation, BSS, prior information, denoising, denoising source separation, DSS, independent component analysis, ICA, magnetoencephalograms, MEG, CDMA</p><p>3 0.27043083 <a title="30-lda-3" href="./jmlr-2005-Separating_a_Real-Life_Nonlinear_Image_Mixture.html">65 jmlr-2005-Separating a Real-Life Nonlinear Image Mixture</a></p>
<p>Author: Luís B. Almeida</p><p>Abstract: When acquiring an image of a paper document, the image printed on the back page sometimes shows through. The mixture of the front- and back-page images thus obtained is markedly nonlinear, and thus constitutes a good real-life test case for nonlinear blind source separation. This paper addresses a difﬁcult version of this problem, corresponding to the use of “onion skin” paper, which results in a relatively strong nonlinearity of the mixture, which becomes close to singular in the lighter regions of the images. The separation is achieved through the MISEP technique, which is an extension of the well known INFOMAX method. The separation results are assessed with objective quality measures. They show an improvement over the results obtained with linear separation, but have room for further improvement. Keywords: ICA, blind source separation, nonlinear mixtures, nonlinear separation, image mixture, image separation</p><p>4 0.25473166 <a title="30-lda-4" href="./jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</a></p>
<p>Author: Aapo Hyvärinen</p><p>Abstract: One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data. Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence</p><p>5 0.24456449 <a title="30-lda-5" href="./jmlr-2005-Kernel_Methods_for_Measuring_Independence.html">41 jmlr-2005-Kernel Methods for Measuring Independence</a></p>
<p>Author: Arthur Gretton, Ralf Herbrich, Alexander Smola, Olivier Bousquet, Bernhard Schölkopf</p><p>Abstract: We introduce two new functionals, the constrained covariance and the kernel mutual information, to measure the degree of independence of random variables. These quantities are both based on the covariance between functions of the random variables in reproducing kernel Hilbert spaces (RKHSs). We prove that when the RKHSs are universal, both functionals are zero if and only if the random variables are pairwise independent. We also show that the kernel mutual information is an upper bound near independence on the Parzen window estimate of the mutual information. Analogous results apply for two correlation-based dependence functionals introduced earlier: we show the kernel canonical correlation and the kernel generalised variance to be independence measures for universal kernels, and prove the latter to be an upper bound on the mutual information near independence. The performance of the kernel dependence functionals in measuring independence is veriﬁed in the context of independent component analysis. Keywords: independence, covariance operator, mutual information, kernel, Parzen window estimate, independent component analysis c 2005 Arthur Gretton, Ralf Herbrich, Alexander Smola, Olivier Bousquet and Bernhard Schölkopf . G RETTON , H ERBRICH , S MOLA , B OUSQUET AND S CHÖLKOPF</p><p>6 0.22121902 <a title="30-lda-6" href="./jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</a></p>
<p>7 0.20405903 <a title="30-lda-7" href="./jmlr-2005-Fast_Kernel_Classifiers_with_Online_and_Active_Learning.html">33 jmlr-2005-Fast Kernel Classifiers with Online and Active Learning</a></p>
<p>8 0.20194535 <a title="30-lda-8" href="./jmlr-2005-Learning_the_Kernel_with_Hyperkernels_%C2%A0%C2%A0%C2%A0%C2%A0%28Kernel_Machines_Section%29.html">49 jmlr-2005-Learning the Kernel with Hyperkernels     (Kernel Machines Section)</a></p>
<p>9 0.20079044 <a title="30-lda-9" href="./jmlr-2005-Variational_Message_Passing.html">71 jmlr-2005-Variational Message Passing</a></p>
<p>10 0.2004187 <a title="30-lda-10" href="./jmlr-2005-Expectation_Consistent_Approximate_Inference.html">32 jmlr-2005-Expectation Consistent Approximate Inference</a></p>
<p>11 0.20035328 <a title="30-lda-11" href="./jmlr-2005-Gaussian_Processes_for_Ordinal_Regression.html">36 jmlr-2005-Gaussian Processes for Ordinal Regression</a></p>
<p>12 0.19843031 <a title="30-lda-12" href="./jmlr-2005-Semigroup_Kernels_on_Measures.html">64 jmlr-2005-Semigroup Kernels on Measures</a></p>
<p>13 0.19757433 <a title="30-lda-13" href="./jmlr-2005-Information_Bottleneck_for_Gaussian_Variables.html">39 jmlr-2005-Information Bottleneck for Gaussian Variables</a></p>
<p>14 0.19611937 <a title="30-lda-14" href="./jmlr-2005-Learning_a_Mahalanobis_Metric_from_Equivalence_Constraints.html">46 jmlr-2005-Learning a Mahalanobis Metric from Equivalence Constraints</a></p>
<p>15 0.19574215 <a title="30-lda-15" href="./jmlr-2005-A_Classification_Framework_for_Anomaly_Detection.html">3 jmlr-2005-A Classification Framework for Anomaly Detection</a></p>
<p>16 0.19453083 <a title="30-lda-16" href="./jmlr-2005-Learning_Module_Networks.html">44 jmlr-2005-Learning Module Networks</a></p>
<p>17 0.19428803 <a title="30-lda-17" href="./jmlr-2005-Maximum_Margin_Algorithms_with_Boolean_Kernels.html">56 jmlr-2005-Maximum Margin Algorithms with Boolean Kernels</a></p>
<p>18 0.192766 <a title="30-lda-18" href="./jmlr-2005-Algorithmic_Stability_and_Meta-Learning.html">11 jmlr-2005-Algorithmic Stability and Meta-Learning</a></p>
<p>19 0.19267331 <a title="30-lda-19" href="./jmlr-2005-Clustering_on_the_Unit_Hypersphere_using_von_Mises-Fisher__Distributions.html">19 jmlr-2005-Clustering on the Unit Hypersphere using von Mises-Fisher  Distributions</a></p>
<p>20 0.19138713 <a title="30-lda-20" href="./jmlr-2005-Clustering_with_Bregman_Divergences.html">20 jmlr-2005-Clustering with Bregman Divergences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
