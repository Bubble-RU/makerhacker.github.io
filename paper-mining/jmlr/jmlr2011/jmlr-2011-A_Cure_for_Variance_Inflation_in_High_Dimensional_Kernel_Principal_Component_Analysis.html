<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 jmlr-2011-A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-3" href="#">jmlr2011-3</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3 jmlr-2011-A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis</h1>
<br/><p>Source: <a title="jmlr-2011-3-pdf" href="http://jmlr.org/papers/volume12/abrahamsen11a/abrahamsen11a.pdf">pdf</a></p><p>Author: Trine Julie Abrahamsen, Lars Kai Hansen</p><p>Abstract: Small sample high-dimensional principal component analysis (PCA) suffers from variance inﬂation and lack of generalizability. It has earlier been pointed out that a simple leave-one-out variance renormalization scheme can cure the problem. In this paper we generalize the cure in two directions: First, we propose a computationally less intensive approximate leave-one-out estimator, secondly, we show that variance inﬂation is also present in kernel principal component analysis (kPCA) and we provide a non-parametric renormalization scheme which can quite efﬁciently restore generalizability in kPCA. As for PCA our analysis also suggests a simpliﬁed approximate expression. Keywords: PCA, kernel PCA, generalizability, variance renormalization</p><p>Reference: <a title="jmlr-2011-3-reference" href="../jmlr2011_reference/jmlr-2011-A_Cure_for_Variance_Inflation_in_High_Dimensional_Kernel_Principal_Component_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 DK  DTU Informatics Technical University of Denmark Richard Petersens Plads, 2800 Lyngby, Denmark  Editor: Manfred Opper  Abstract Small sample high-dimensional principal component analysis (PCA) suffers from variance inﬂation and lack of generalizability. [sent-5, score-0.219]
</p><p>2 It has earlier been pointed out that a simple leave-one-out variance renormalization scheme can cure the problem. [sent-6, score-0.659]
</p><p>3 Keywords: PCA, kernel PCA, generalizability, variance renormalization  1. [sent-9, score-0.6]
</p><p>4 Introduction While linear dimensionality reduction by principal component analysis (PCA) is a trusted machine learning workhorse, kernel based methods for non-linear dimensionality reduction are only starting to ﬁnd application. [sent-10, score-0.199]
</p><p>5 We expect the use of non-linear dimensionality reduction to expand in many applications as recent research has shown that kernel principal component analysis (kPCA) can be expected to work well as a pre-processing device for pattern recognition (Braun et al. [sent-11, score-0.199]
</p><p>6 In the following we consider non-linear signal detection by kernel PCA followed by a linear discriminant classiﬁer. [sent-13, score-0.165]
</p><p>7 In PCA there is a sharp transition as function of sample size from no learning at all to a regime where the projections become more and more accurate. [sent-19, score-0.218]
</p><p>8 (2001) it was shown that this can be interpreted as a case of over-ﬁtting and leads to pronounced variance inﬂation in the training set projections and results in lack of generalization to test data as illustrated in Figure 1. [sent-22, score-0.379]
</p><p>9 When the data analytic pipeline is applied to test data the reduced variance of the PCA text projections can lead to signiﬁcantly reduced performance. [sent-24, score-0.324]
</p><p>10 This leads to lack of generalizability if the training data is used to train a classiﬁer, say a linear discriminant (D). [sent-31, score-0.195]
</p><p>11 (2001) this problem was noted and it was shown that the necessary renormalization can be estimated in a leave-one-out procedure  be reduced effectively by a leave-one-out (LOO) scale renormalization of the PCA test projections to restore generalizability (Kjems et al. [sent-33, score-1.286]
</p><p>12 We then provide an extension to the LOO procedure for kPCA which can cope with potential non-Gaussian distributions of the kPCA projections, and ﬁnally we propose a simpliﬁed approximate renormalization scheme. [sent-40, score-0.454]
</p><p>13 The stabilization of a given principal component happens at a given sample size and takes the form of a phase transition. [sent-45, score-0.169]
</p><p>14 For small sample sizes -below the phase transition point - the training set principal component eigenvectors are in completely random directions in space and there is no learning at all. [sent-46, score-0.305]
</p><p>15 In this simulated data set we show the phase transition like behavior of the overlap (the mean square of the projection) of the ﬁrst PCA eigenvector and the signal direction u. [sent-63, score-0.181]
</p><p>16 phase transition depends on the signal variance to noise variance ratio (SNR). [sent-69, score-0.337]
</p><p>17 The PCA projections will be offset by different angles depending on how severe the given component is affected by the noise. [sent-72, score-0.22]
</p><p>18 Because of the bias the test projections will follow different probability laws than the training data, typically with much lower variance. [sent-73, score-0.296]
</p><p>19 Hence, if we train a classiﬁer on the training projections the classiﬁer will make additional errors on the test set as visualized in Figure 1. [sent-74, score-0.296]
</p><p>20 In the case of PCA the subspace projections are uncorrelated, hence, it is meaningful to renormalize them independently. [sent-75, score-0.266]
</p><p>21 The scale factor is simply the ratio of the standard deviations of the training and test projections and can be estimated by a leave-one-out procedure (Kjems et al. [sent-77, score-0.296]
</p><p>22 In the four panels we show the training set projections (red crosses), the projections corrected for the theoretical mean overlap (Hoyle and Rattray, 2007) (yellow squares) and the geometric approximation in Equation (1) (green dots) versus the exact LOO projections (black line). [sent-99, score-0.613]
</p><p>23 Deﬁne the orthogonal and parallel components of the test point, xN = x⊥ + xN , relative to the subspace spanned by the training data. [sent-108, score-0.215]
</p><p>24 As the N PCA eigenvectors with non-zero variance are all in the span of the training data we obtain T uT N−1,k · xN = uN−1,k · xN ,  where uN−1,k is the k’th eigenvector of the LOO training set. [sent-109, score-0.314]
</p><p>25 Assuming that the changes in the PCA eigenvectors going from sample size N to N − 1 are small, we can approximate the test projections as T T uT N−1,k · xN = uN−1,k · xN ≈ uN,k · xN ,  (1)  where uN,k is the k’th eigenvector on the full sample. [sent-110, score-0.362]
</p><p>26 N N N  (2)  The projection of a ϕ-mapped test point onto the i’th component is given by ˜ βi = ϕ(x)T vi =  N  ˜ ˜ ∑ αin ϕ(x)T ϕ(xn ) =  n=1  N  ˜ ∑ αin k(x, xn ) ,  (3)  n=1  where vi is the i’th eigenvector of the feature space covariance matrix and the αi ’s have been nor1 1 ˜ malized. [sent-136, score-0.292]
</p><p>27 Consider the squared distance ||xn − xN ||2 in the exponent in the Gaussian kernel for some training set point xn and a test point xN . [sent-147, score-0.257]
</p><p>28 If we split the test point in the orthogonal components as above with respect to the subspace spanned by the training set we obtain, ||  ||xn − xN ||2 = ||xn − xN ||2 + ||x⊥ ||2 . [sent-148, score-0.215]
</p><p>29 In the four panels we show the four kPCA component’s training set projections (red crosses), and the result of applying the point wise correction factor exp( 1 ||x⊥ ||2 ) for the lost orthogonal projection N c (green dots) versus the exact LOO kPCA test projections (black). [sent-192, score-0.563]
</p><p>30 For a coordinate-wise LOO renormalization procedure we thus propose to compute N test projections by repeated kPCA on the N − 1 sized sub training sets. [sent-193, score-0.75]
</p><p>31 If the null hypothesis is rejected for a given set of components we cannot expect coordinate-wise renormalization to be effective. [sent-198, score-0.563]
</p><p>32 If, on the other hand, the kernel PCA projections pass the independence test we can proceed to renormalize the components individually. [sent-199, score-0.365]
</p><p>33 Equalizing two equal sized samples, simply involves sorting both and assigning the sorted test projections the sorted values of the training projections, this procedure is easily seen to equalize the histograms without changing the level sets (relative ordering) of the LOO test projections. [sent-207, score-0.379]
</p><p>34 However, due to variance inﬂation (induced by, for example, kernel PCA) the test set does not follow the same 2032  A C URE FOR VARIANCE I NFLATION IN H IGH D IMENSIONAL K ERNEL PCA  600  Sample No. [sent-210, score-0.201]
</p><p>35 600  400  200  400  200  0  0 0  5  10  15  20  0  5  Histograms  10  15  20  Data 150  150 100  Renormalize  100  50  50 0  0 0  5  10  15  20  0  x  5  10  15  20  x  Figure 5: Illustration of renormalization by histogram equalization. [sent-212, score-0.494]
</p><p>36 The histograms are then equalized as seen in the right panel, where the green dots are the renormalized test data. [sent-214, score-0.192]
</p><p>37 The renormalization clearly restores the variation of the test set. [sent-215, score-0.537]
</p><p>38 Let the test set projections on the same component for Ntest samples take values g(m). [sent-220, score-0.275]
</p><p>39 Then the renormalized value of the test projection m is g(m) = H −1 (I(m)/Ntest ) . [sent-222, score-0.239]
</p><p>40 The test set projections can be obtained by the simple relation g(m) = fsort (I(m)) ,  (4)  where fsort is the sorted list of training set projections. [sent-223, score-0.462]
</p><p>41 The algorithm for approximate renormalization is summarized in Algorithm 1. [sent-224, score-0.454]
</p><p>42 We thank the reviewers for pointing out that while non-normality is expected in the case of kPCA, non-normality may also appear in PCA calling for application of the proposed non-parametric renormalization scheme in this case. [sent-228, score-0.48]
</p><p>43 2033  A BRAHAMSEN AND H ANSEN  Algorithm 1 Approximate renormalization in kernel PCA Require: Xtr and Xte to be Ntr × D and Nte × D respectively Compute Ktr using Equation (2) and ﬁnd the eigenvectors, α1 , . [sent-229, score-0.517]
</p><p>44 In Figure 6 we show in the left panel a linear discriminant trained on the training set projections in a data set of N = 500 in D = 1000 dimensions. [sent-244, score-0.342]
</p><p>45 Finally, Figure 8 shows how renormalization improves the learning curve for the same problem. [sent-247, score-0.454]
</p><p>46 2 USPS Handwritten Digit Data The USPS handwritten digit benchmark data set is often used to illustrate unsupervised and supervised kernel methods. [sent-249, score-0.166]
</p><p>47 15  KPCA 1  Figure 6: An unbalanced two cluster data set showing a pronounced variance inﬂation problem in the projections of the test data in the middle panel. [sent-283, score-0.324]
</p><p>48 In the right panel we have applied the cure based on non-parametric renormalization to equalize training and test projections using histogram equalization. [sent-284, score-0.957]
</p><p>49 and the number of principal components was chosen so 85% of the variance was contained in the principal subspace leading to around q = 57 PCs to be included. [sent-292, score-0.358]
</p><p>50 For every pair of principal components a permutation test with 1000 permutations was performed in order to test the null hypothesis of the two given components being independent. [sent-294, score-0.32]
</p><p>51 05 signiﬁcance level, we ﬁnd that the null hypothesis can only be rejected for approximately 2% of the principal component pairs when not using Bonferroni correction. [sent-296, score-0.219]
</p><p>52 The combinations for which the null hypothesis can be rejected are equally distributed across the principal components. [sent-297, score-0.185]
</p><p>53 Since the expected number of rejected tests at the given conﬁdence level is 5%, hence, we can safely proceed with the coordinate-wise renormalization process. [sent-298, score-0.507]
</p><p>54 In the q dimensional principal subspace the projections of the test set are renormalized to follow the training set histogram. [sent-299, score-0.58]
</p><p>55 A linear discriminant classiﬁer was trained on the kernel PCA projections of the training set, and the classiﬁcation error was found using both the conventional kernel PCA projections of the test set and their renormalized counterparts. [sent-301, score-0.861]
</p><p>56 While classiﬁcation based on the conventional projections resulted in a mean classiﬁcation error rate (± 1 std) of 0. [sent-303, score-0.275]
</p><p>57 01, using the renormalized projections lowered the error rate to 0. [sent-305, score-0.354]
</p><p>58 Figure 9 shows an example of the projections before and after renormalization. [sent-310, score-0.186]
</p><p>59 The linear discriminant performs close to the optimal Bayes rate after the renormalization operation in all cases, while the un-renormalized systems suffers from poor generalizability. [sent-325, score-0.543]
</p><p>60 The bottom row illustrates how renormalization overcomes the distortions induced by the variance inﬂation. [sent-328, score-0.537]
</p><p>61 To gain a better understanding of how the variance inﬂation and quality of the renormalization are effected by noise, we added Gaussian noise (N (0, σ2 )) with σe ∈ [0, 5]. [sent-330, score-0.599]
</p><p>62 For every noise level, e 300 random training and test sets where drawn as explained above and kPCA was performed. [sent-331, score-0.172]
</p><p>63 Once again our goal was to classify digit 8 versus the rest by a linear classiﬁer in the principal subspace. [sent-332, score-0.158]
</p><p>64 The results are summarized in Figure 10 where we show the error rate before and after renormalization as well as the result based on renormalizing according to the leave-one-out error. [sent-333, score-0.513]
</p><p>65 In the last case, the N projections determined from leave-one-out cross validation (LOOCV) are renormalized to follow the entire training set histogram. [sent-334, score-0.378]
</p><p>66 Renormalization is then only applied to the test set when this renormalized LOOCV error is less than the estimated baseline error. [sent-335, score-0.192]
</p><p>67 In the right panel of Figure 10 it is seen how renormalizing the projections leads to a much improved classiﬁer as long as the SNR is ‘reasonable’. [sent-336, score-0.257]
</p><p>68 Even when σe = 0 there is some inherent noise in the data, which explains why renormalization still improves the classiﬁcation. [sent-337, score-0.516]
</p><p>69 Basically, increasing the noise result in a more skewed test set subspace in relation to the subspace spanned by the training set (see Figure 1). [sent-341, score-0.262]
</p><p>70 At a given threshold this causes all the projections to lie on the same side of the discrimination function due to the imbalanced composition, leading to a misclassiﬁcations rate of 1/10. [sent-342, score-0.217]
</p><p>71 As the idea of renormalization by histogram equalization is to restore the variation in the test set, this be2036  A C URE FOR VARIANCE I NFLATION IN H IGH D IMENSIONAL K ERNEL PCA  0. [sent-343, score-0.639]
</p><p>72 The linear discriminant performs close to the optimal Bayes rate after the renormalization operation in all cases, while the conventional system suffers from poor generalizability, and requires about ten times as many examples to reach the same error level as the renormalized classiﬁer. [sent-357, score-0.738]
</p><p>73 Instead, as the SNR decreases, renormalization increases the error rate, as the test set observations are forced to be distributed on both sides of the discrimination line - which leads to many misclassiﬁcations when the signal is suppressed by the noise. [sent-360, score-0.553]
</p><p>74 However, using LOOCV based renormalization prevents the error rate from blowing up while at the same time improving the classiﬁcation in the more sensible SNR regime as compared to conventional kPCA. [sent-361, score-0.543]
</p><p>75 The top row shows the conventional projections, while the bottom row shows the projections after renormalization. [sent-448, score-0.244]
</p><p>76 As the test and training data are independent, the test error estimate is an unbiased estimator of performance. [sent-455, score-0.165]
</p><p>77 The scale of the Gaussian kernel was chosen as the 5th percentile of the mutual distances leading to c ≈ 15000, while the dimension of the principal subspace is chosen as q = 20. [sent-456, score-0.287]
</p><p>78 Again the principal components are tested for independence by a mutual information permutation test. [sent-457, score-0.196]
</p><p>79 05 signiﬁcance level, we ﬁnd that the null hypothesis is rejected for approximately 1% of the principal component pairs. [sent-459, score-0.219]
</p><p>80 Similar to the handwritten digit data we perform linear classiﬁcation in the kernel principal subspace. [sent-460, score-0.268]
</p><p>81 Again renormalization is seen to decrease the error rate signiﬁcantly, while the LOOCV based scheme furthermore prevents the increase in error rate for high noise levels (low SNR). [sent-463, score-0.604]
</p><p>82 The test error based on conventional kernel PCA projections, renormalized projections, and a LOOCV scheme is shown. [sent-493, score-0.339]
</p><p>83 Renormalization is seen to improve the performance, while LOOCV based renormalization prevents the classiﬁcation error to blow up in the very low SNR regime. [sent-494, score-0.454]
</p><p>84 For kPCA we showed that the effects can be even more dramatic than in PCA, and we proposed a scheme for exact LOO renormalization of the embedding, and an approximate expression at lower cost. [sent-499, score-0.48]
</p><p>85 The test error based on conventional kernel PCA projections, renormalized projections, and a LOOCV scheme is shown. [sent-513, score-0.339]
</p><p>86 Let uN,k be the k’th eigenvector of the covariance matrix on the full sample ΣN and uN−1,k be the corresponding eigenvector of LOO training set covariance matrix ΣN−1 . [sent-516, score-0.199]
</p><p>87 In the following we use ﬁrst order perturbation theory to show that T uT N−1,k · xN ≈ uN,k · xN ,  where the data vector x has been split in its orthogonal and parallel components, xN = x⊥ + xN , N relative to the subspace spanned by the training data. [sent-517, score-0.164]
</p><p>88 We now look at the k’th eigenvector of A and B:  Buk = λk uk ,  (6)  Avk = νk vk . [sent-522, score-0.188]
</p><p>89 1  kPC 6  Figure 12: Test set projections of the fMRI data with Gaussian noise added as marked on Figure 11 (εi = N (0, 3. [sent-593, score-0.248]
</p><p>90 The top row shows the conventional projections, while the bottom row shows the projections after renormalization. [sent-595, score-0.244]
</p><p>91 ,  ||vk ||2 = ||uk + δwk ||2 = ||uk ||2 + δ2 ||wk ||2 + 2δuT wk = 1 k =1  ≈0  δuT wk = 0 . [sent-603, score-0.294]
</p><p>92 (10)  We now look for an estimate of ξk by left multiplying with uT k uT Cuk + uT Bwk = λk uT wk + ξk uT uk , k k k k using ||uk ||2 = 1 and uk ⊥wk gives uT Cuk + uT Bwk = ξk , k k since B is symmetric, uk is both a left and right singular vector. [sent-605, score-0.381]
</p><p>93 j uT Cuk + uT Bwk = λk uT wk + ξk uT uk , j j j j again we exploit the fact that B is symmetric and that u j is orthogonal to uk , which gives uT Cuk + λ j uT wk = λk uT wk . [sent-609, score-0.631]
</p><p>94 , vD }, that is, the v-basis is a rotation of the u-basis, which implies that wk can be represented as a linear combination of the u-vectors (or v-vectors), leads to D  wk =  ∑ hkm um . [sent-616, score-0.385]
</p><p>95 m=1  Due to orthonormality of the eigenvectors, we now realize that hkk = 0 and uT wk = uT ∑D hkm um j m=1 j will only be non-zero for m = j. [sent-617, score-0.266]
</p><p>96 Thus, wk can be expressed as N uT Cuk m um , (13) wk = ∑ λk − λm m=1=k 2042  A C URE FOR VARIANCE I NFLATION IN H IGH D IMENSIONAL K ERNEL PCA  where we used that Cuk is only non-zero for k ≤ N. [sent-620, score-0.357]
</p><p>97 We are now ready to return to Equation (8) and (9) inserting the expressions derived for ξk and wk in Equation (11) and (13) respectively: νk = λk + δuT Cuk k vk = uk + δ  N  ∑  (14)  (uT (xN m  − µN−1 ))(uT (xN k  − µN−1 ))  λk − λm  m=1=k  um . [sent-621, score-0.326]
</p><p>98 Limiting form of the sample covariance eigenspectrum in pca and kernel pca. [sent-646, score-0.322]
</p><p>99 The stability of kernel principal components analysis and its relation to the process eigenspectrum. [sent-691, score-0.191]
</p><p>100 On the convergence of eigenspaces in kernel principal component analysis. [sent-700, score-0.199]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('renormalization', 0.454), ('pca', 0.259), ('kpca', 0.242), ('loo', 0.242), ('kpc', 0.234), ('ut', 0.201), ('ation', 0.189), ('projections', 0.186), ('cuk', 0.165), ('hoyle', 0.151), ('wk', 0.147), ('renormalized', 0.137), ('ansen', 0.124), ('brahamsen', 0.124), ('loocv', 0.124), ('nflation', 0.11), ('ure', 0.11), ('kjems', 0.105), ('snr', 0.105), ('principal', 0.102), ('cure', 0.096), ('nte', 0.096), ('ntr', 0.096), ('xn', 0.084), ('variance', 0.083), ('bwk', 0.083), ('fsort', 0.083), ('generalizability', 0.082), ('rattray', 0.082), ('uk', 0.078), ('eigenvector', 0.072), ('igh', 0.072), ('imensional', 0.072), ('um', 0.063), ('kernel', 0.063), ('noise', 0.062), ('discriminant', 0.058), ('conventional', 0.058), ('th', 0.057), ('digit', 0.056), ('ernel', 0.056), ('etest', 0.055), ('etestrenorm', 0.055), ('restore', 0.055), ('training', 0.055), ('test', 0.055), ('rejected', 0.053), ('usps', 0.051), ('eigenvectors', 0.049), ('projection', 0.047), ('magnus', 0.047), ('handwritten', 0.047), ('subspace', 0.045), ('fmri', 0.045), ('signal', 0.044), ('panel', 0.043), ('mutual', 0.042), ('histogram', 0.04), ('vk', 0.038), ('kx', 0.036), ('percentile', 0.035), ('equalization', 0.035), ('renormalize', 0.035), ('orthogonal', 0.034), ('component', 0.034), ('phase', 0.033), ('transition', 0.032), ('sec', 0.032), ('dtu', 0.032), ('rate', 0.031), ('acquired', 0.03), ('perturbation', 0.03), ('null', 0.03), ('eigenvalue', 0.029), ('abrahamsen', 0.028), ('avk', 0.028), ('biehl', 0.028), ('buk', 0.028), ('darkness', 0.028), ('equalize', 0.028), ('etrain', 0.028), ('fte', 0.028), ('ftr', 0.028), ('gonzalez', 0.028), ('hkk', 0.028), ('hkm', 0.028), ('mosci', 0.028), ('reimann', 0.028), ('renormalizing', 0.028), ('restores', 0.028), ('silverstein', 0.028), ('trine', 0.028), ('zwald', 0.028), ('blanchard', 0.027), ('un', 0.027), ('physics', 0.027), ('scheme', 0.026), ('components', 0.026), ('pc', 0.026), ('permutation', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="3-tfidf-1" href="./jmlr-2011-A_Cure_for_Variance_Inflation_in_High_Dimensional_Kernel_Principal_Component_Analysis.html">3 jmlr-2011-A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis</a></p>
<p>Author: Trine Julie Abrahamsen, Lars Kai Hansen</p><p>Abstract: Small sample high-dimensional principal component analysis (PCA) suffers from variance inﬂation and lack of generalizability. It has earlier been pointed out that a simple leave-one-out variance renormalization scheme can cure the problem. In this paper we generalize the cure in two directions: First, we propose a computationally less intensive approximate leave-one-out estimator, secondly, we show that variance inﬂation is also present in kernel principal component analysis (kPCA) and we provide a non-parametric renormalization scheme which can quite efﬁciently restore generalizability in kPCA. As for PCA our analysis also suggests a simpliﬁed approximate expression. Keywords: PCA, kernel PCA, generalizability, variance renormalization</p><p>2 0.12754221 <a title="3-tfidf-2" href="./jmlr-2011-Locally_Defined_Principal_Curves_and_Surfaces.html">60 jmlr-2011-Locally Defined Principal Curves and Surfaces</a></p>
<p>Author: Umut Ozertem, Deniz Erdogmus</p><p>Abstract: Principal curves are deﬁned as self-consistent smooth curves passing through the middle of the data, and they have been used in many applications of machine learning as a generalization, dimensionality reduction and a feature extraction tool. We redeﬁne principal curves and surfaces in terms of the gradient and the Hessian of the probability density estimate. This provides a geometric understanding of the principal curves and surfaces, as well as a unifying view for clustering, principal curve ﬁtting and manifold learning by regarding those as principal manifolds of different intrinsic dimensionalities. The theory does not impose any particular density estimation method can be used with any density estimator that gives continuous ﬁrst and second derivatives. Therefore, we ﬁrst present our principal curve/surface deﬁnition without assuming any particular density estimation method. Afterwards, we develop practical algorithms for the commonly used kernel density estimation (KDE) and Gaussian mixture models (GMM). Results of these algorithms are presented in notional data sets as well as real applications with comparisons to other approaches in the principal curve literature. All in all, we present a novel theoretical understanding of principal curves and surfaces, practical algorithms as general purpose machine learning tools, and applications of these algorithms to several practical problems. Keywords: unsupervised learning, dimensionality reduction, principal curves, principal surfaces, subspace constrained mean-shift</p><p>3 0.068136215 <a title="3-tfidf-3" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>Author: Grégoire Montavon, Mikio L. Braun, Klaus-Robert Müller</p><p>Abstract: When training deep networks it is common knowledge that an efﬁcient and well generalizing representation of the problem is formed. In this paper we aim to elucidate what makes the emerging representation successful. We analyze the layer-wise evolution of the representation in a deep network by building a sequence of deeper and deeper kernels that subsume the mapping performed by more and more layers of the deep network and measuring how these increasingly complex kernels ﬁt the learning problem. We observe that deep networks create increasingly better representations of the learning problem and that the structure of the deep network controls how fast the representation of the task is formed layer after layer. Keywords: deep networks, kernel principal component analysis, representations</p><p>4 0.067534208 <a title="3-tfidf-4" href="./jmlr-2011-Hyper-Sparse_Optimal_Aggregation.html">40 jmlr-2011-Hyper-Sparse Optimal Aggregation</a></p>
<p>Author: Stéphane Gaïffas, Guillaume Lecué</p><p>Abstract: Given a ﬁnite set F of functions and a learning sample, the aim of an aggregation procedure is to have a risk as close as possible to risk of the best function in F. Up to now, optimal aggregation procedures are convex combinations of every elements of F. In this paper, we prove that optimal aggregation procedures combining only two functions in F exist. Such algorithms are of particular interest when F contains many irrelevant functions that should not appear in the aggregation procedure. Since selectors are suboptimal aggregation procedures, this proves that two is the minimal number of elements of F required for the construction of an optimal aggregation procedure in every situations. Then, we perform a numerical study for the problem of selection of the regularization parameters of the Lasso and the Elastic-net estimators. We compare on simulated examples our aggregation algorithms to aggregation with exponential weights, to Mallow’s Cp and to cross-validation selection procedures. Keywords: aggregation, exact oracle inequality, empirical risk minimization, empirical process theory, sparsity, Lasso, Lars</p><p>5 0.065684125 <a title="3-tfidf-5" href="./jmlr-2011-Regression_on_Fixed-Rank_Positive_Semidefinite_Matrices%3A_A_Riemannian_Approach.html">80 jmlr-2011-Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach</a></p>
<p>Author: Gilles Meyer, Silvère Bonnabel, Rodolphe Sepulchre</p><p>Abstract: The paper addresses the problem of learning a regression model parameterized by a ﬁxed-rank positive semideﬁnite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of ﬁxedrank positive semideﬁnite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semideﬁnite matrix. Good performance is observed on classical benchmarks. Keywords: linear regression, positive semideﬁnite matrices, low-rank approximation, Riemannian geometry, gradient-based learning</p><p>6 0.059025224 <a title="3-tfidf-6" href="./jmlr-2011-Unsupervised_Supervised_Learning_II%3A_Margin-Based_Classification_Without_Labels.html">100 jmlr-2011-Unsupervised Supervised Learning II: Margin-Based Classification Without Labels</a></p>
<p>7 0.057927124 <a title="3-tfidf-7" href="./jmlr-2011-Group_Lasso_Estimation_of_High-dimensional_Covariance_Matrices.html">37 jmlr-2011-Group Lasso Estimation of High-dimensional Covariance Matrices</a></p>
<p>8 0.046113495 <a title="3-tfidf-8" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>9 0.04383488 <a title="3-tfidf-9" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>10 0.040817387 <a title="3-tfidf-10" href="./jmlr-2011-Discriminative_Learning_of_Bayesian_Networks_via_Factorized_Conditional_Log-Likelihood.html">25 jmlr-2011-Discriminative Learning of Bayesian Networks via Factorized Conditional Log-Likelihood</a></p>
<p>11 0.040362019 <a title="3-tfidf-11" href="./jmlr-2011-Anechoic_Blind_Source_Separation_Using_Wigner_Marginals.html">10 jmlr-2011-Anechoic Blind Source Separation Using Wigner Marginals</a></p>
<p>12 0.037488468 <a title="3-tfidf-12" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<p>13 0.033672538 <a title="3-tfidf-13" href="./jmlr-2011-Proximal_Methods_for_Hierarchical_Sparse_Coding.html">79 jmlr-2011-Proximal Methods for Hierarchical Sparse Coding</a></p>
<p>14 0.03313458 <a title="3-tfidf-14" href="./jmlr-2011-Efficient_and_Effective_Visual_Codebook_Generation_Using_Additive_Kernels.html">31 jmlr-2011-Efficient and Effective Visual Codebook Generation Using Additive Kernels</a></p>
<p>15 0.029040562 <a title="3-tfidf-15" href="./jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</a></p>
<p>16 0.028544286 <a title="3-tfidf-16" href="./jmlr-2011-Domain_Decomposition_Approach_for_Fast_Gaussian_Process_Regression_of_Large_Spatial_Data_Sets.html">27 jmlr-2011-Domain Decomposition Approach for Fast Gaussian Process Regression of Large Spatial Data Sets</a></p>
<p>17 0.027409976 <a title="3-tfidf-17" href="./jmlr-2011-Learning_a_Robust_Relevance_Model_for_Search_Using_Kernel_Methods.html">57 jmlr-2011-Learning a Robust Relevance Model for Search Using Kernel Methods</a></p>
<p>18 0.02704097 <a title="3-tfidf-18" href="./jmlr-2011-lp-Norm_Multiple_Kernel_Learning.html">105 jmlr-2011-lp-Norm Multiple Kernel Learning</a></p>
<p>19 0.026377188 <a title="3-tfidf-19" href="./jmlr-2011-Bayesian_Generalized_Kernel_Mixed_Models.html">13 jmlr-2011-Bayesian Generalized Kernel Mixed Models</a></p>
<p>20 0.02493092 <a title="3-tfidf-20" href="./jmlr-2011-A_Simpler_Approach_to_Matrix_Completion.html">6 jmlr-2011-A Simpler Approach to Matrix Completion</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.157), (1, -0.04), (2, 0.047), (3, -0.022), (4, -0.028), (5, -0.019), (6, -0.018), (7, -0.133), (8, -0.129), (9, -0.03), (10, 0.045), (11, 0.052), (12, 0.107), (13, 0.017), (14, -0.178), (15, 0.127), (16, 0.269), (17, 0.078), (18, -0.142), (19, -0.22), (20, 0.111), (21, 0.192), (22, 0.178), (23, 0.025), (24, 0.108), (25, 0.213), (26, -0.137), (27, 0.008), (28, -0.031), (29, -0.057), (30, -0.071), (31, -0.04), (32, 0.112), (33, 0.004), (34, -0.029), (35, 0.073), (36, 0.072), (37, -0.049), (38, 0.048), (39, 0.113), (40, 0.069), (41, -0.017), (42, 0.164), (43, -0.066), (44, -0.034), (45, 0.016), (46, 0.003), (47, -0.014), (48, 0.02), (49, -0.121)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95954478 <a title="3-lsi-1" href="./jmlr-2011-A_Cure_for_Variance_Inflation_in_High_Dimensional_Kernel_Principal_Component_Analysis.html">3 jmlr-2011-A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis</a></p>
<p>Author: Trine Julie Abrahamsen, Lars Kai Hansen</p><p>Abstract: Small sample high-dimensional principal component analysis (PCA) suffers from variance inﬂation and lack of generalizability. It has earlier been pointed out that a simple leave-one-out variance renormalization scheme can cure the problem. In this paper we generalize the cure in two directions: First, we propose a computationally less intensive approximate leave-one-out estimator, secondly, we show that variance inﬂation is also present in kernel principal component analysis (kPCA) and we provide a non-parametric renormalization scheme which can quite efﬁciently restore generalizability in kPCA. As for PCA our analysis also suggests a simpliﬁed approximate expression. Keywords: PCA, kernel PCA, generalizability, variance renormalization</p><p>2 0.76369673 <a title="3-lsi-2" href="./jmlr-2011-Locally_Defined_Principal_Curves_and_Surfaces.html">60 jmlr-2011-Locally Defined Principal Curves and Surfaces</a></p>
<p>Author: Umut Ozertem, Deniz Erdogmus</p><p>Abstract: Principal curves are deﬁned as self-consistent smooth curves passing through the middle of the data, and they have been used in many applications of machine learning as a generalization, dimensionality reduction and a feature extraction tool. We redeﬁne principal curves and surfaces in terms of the gradient and the Hessian of the probability density estimate. This provides a geometric understanding of the principal curves and surfaces, as well as a unifying view for clustering, principal curve ﬁtting and manifold learning by regarding those as principal manifolds of different intrinsic dimensionalities. The theory does not impose any particular density estimation method can be used with any density estimator that gives continuous ﬁrst and second derivatives. Therefore, we ﬁrst present our principal curve/surface deﬁnition without assuming any particular density estimation method. Afterwards, we develop practical algorithms for the commonly used kernel density estimation (KDE) and Gaussian mixture models (GMM). Results of these algorithms are presented in notional data sets as well as real applications with comparisons to other approaches in the principal curve literature. All in all, we present a novel theoretical understanding of principal curves and surfaces, practical algorithms as general purpose machine learning tools, and applications of these algorithms to several practical problems. Keywords: unsupervised learning, dimensionality reduction, principal curves, principal surfaces, subspace constrained mean-shift</p><p>3 0.39180055 <a title="3-lsi-3" href="./jmlr-2011-Regression_on_Fixed-Rank_Positive_Semidefinite_Matrices%3A_A_Riemannian_Approach.html">80 jmlr-2011-Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach</a></p>
<p>Author: Gilles Meyer, Silvère Bonnabel, Rodolphe Sepulchre</p><p>Abstract: The paper addresses the problem of learning a regression model parameterized by a ﬁxed-rank positive semideﬁnite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of ﬁxedrank positive semideﬁnite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semideﬁnite matrix. Good performance is observed on classical benchmarks. Keywords: linear regression, positive semideﬁnite matrices, low-rank approximation, Riemannian geometry, gradient-based learning</p><p>4 0.34429392 <a title="3-lsi-4" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>Author: Grégoire Montavon, Mikio L. Braun, Klaus-Robert Müller</p><p>Abstract: When training deep networks it is common knowledge that an efﬁcient and well generalizing representation of the problem is formed. In this paper we aim to elucidate what makes the emerging representation successful. We analyze the layer-wise evolution of the representation in a deep network by building a sequence of deeper and deeper kernels that subsume the mapping performed by more and more layers of the deep network and measuring how these increasingly complex kernels ﬁt the learning problem. We observe that deep networks create increasingly better representations of the learning problem and that the structure of the deep network controls how fast the representation of the task is formed layer after layer. Keywords: deep networks, kernel principal component analysis, representations</p><p>5 0.28251252 <a title="3-lsi-5" href="./jmlr-2011-Efficient_and_Effective_Visual_Codebook_Generation_Using_Additive_Kernels.html">31 jmlr-2011-Efficient and Effective Visual Codebook Generation Using Additive Kernels</a></p>
<p>Author: Jianxin Wu, Wei-Chian Tan, James M. Rehg</p><p>Abstract: Common visual codebook generation methods used in a bag of visual words model, for example, k-means or Gaussian Mixture Model, use the Euclidean distance to cluster features into visual code words. However, most popular visual descriptors are histograms of image measurements. It has been shown that with histogram features, the Histogram Intersection Kernel (HIK) is more effective than the Euclidean distance in supervised learning tasks. In this paper, we demonstrate that HIK can be used in an unsupervised manner to signiﬁcantly improve the generation of visual codebooks. We propose a histogram kernel k-means algorithm which is easy to implement and runs almost as fast as the standard k-means. The HIK codebooks have consistently higher recognition accuracy over k-means codebooks by 2–4% in several benchmark object and scene recognition data sets. The algorithm is also generalized to arbitrary additive kernels. Its speed is thousands of times faster than a naive implementation of the kernel k-means algorithm. In addition, we propose a one-class SVM formulation to create more effective visual code words. Finally, we show that the standard kmedian clustering method can be used for visual codebook generation and can act as a compromise between the HIK / additive kernel and the k-means approaches. Keywords: visual codebook, additive kernel, histogram intersection kernel</p><p>6 0.26149917 <a title="3-lsi-6" href="./jmlr-2011-Unsupervised_Supervised_Learning_II%3A_Margin-Based_Classification_Without_Labels.html">100 jmlr-2011-Unsupervised Supervised Learning II: Margin-Based Classification Without Labels</a></p>
<p>7 0.254058 <a title="3-lsi-7" href="./jmlr-2011-Anechoic_Blind_Source_Separation_Using_Wigner_Marginals.html">10 jmlr-2011-Anechoic Blind Source Separation Using Wigner Marginals</a></p>
<p>8 0.24371126 <a title="3-lsi-8" href="./jmlr-2011-Group_Lasso_Estimation_of_High-dimensional_Covariance_Matrices.html">37 jmlr-2011-Group Lasso Estimation of High-dimensional Covariance Matrices</a></p>
<p>9 0.23765421 <a title="3-lsi-9" href="./jmlr-2011-Hyper-Sparse_Optimal_Aggregation.html">40 jmlr-2011-Hyper-Sparse Optimal Aggregation</a></p>
<p>10 0.2276857 <a title="3-lsi-10" href="./jmlr-2011-Waffles%3A_A_Machine_Learning_Toolkit.html">102 jmlr-2011-Waffles: A Machine Learning Toolkit</a></p>
<p>11 0.20045862 <a title="3-lsi-11" href="./jmlr-2011-Scikit-learn%3A_Machine_Learning_in_Python.html">83 jmlr-2011-Scikit-learn: Machine Learning in Python</a></p>
<p>12 0.19750959 <a title="3-lsi-12" href="./jmlr-2011-Discriminative_Learning_of_Bayesian_Networks_via_Factorized_Conditional_Log-Likelihood.html">25 jmlr-2011-Discriminative Learning of Bayesian Networks via Factorized Conditional Log-Likelihood</a></p>
<p>13 0.19382925 <a title="3-lsi-13" href="./jmlr-2011-Domain_Decomposition_Approach_for_Fast_Gaussian_Process_Regression_of_Large_Spatial_Data_Sets.html">27 jmlr-2011-Domain Decomposition Approach for Fast Gaussian Process Regression of Large Spatial Data Sets</a></p>
<p>14 0.18059681 <a title="3-lsi-14" href="./jmlr-2011-Bayesian_Generalized_Kernel_Mixed_Models.html">13 jmlr-2011-Bayesian Generalized Kernel Mixed Models</a></p>
<p>15 0.17957325 <a title="3-lsi-15" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>16 0.16395012 <a title="3-lsi-16" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>17 0.15821986 <a title="3-lsi-17" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>18 0.15534341 <a title="3-lsi-18" href="./jmlr-2011-Hierarchical_Knowledge_Gradient_for_Sequential_Sampling.html">38 jmlr-2011-Hierarchical Knowledge Gradient for Sequential Sampling</a></p>
<p>19 0.14821179 <a title="3-lsi-19" href="./jmlr-2011-Multiple_Kernel_Learning_Algorithms.html">66 jmlr-2011-Multiple Kernel Learning Algorithms</a></p>
<p>20 0.14367506 <a title="3-lsi-20" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.056), (9, 0.027), (10, 0.032), (24, 0.03), (31, 0.095), (32, 0.018), (41, 0.024), (60, 0.012), (65, 0.021), (71, 0.015), (73, 0.041), (78, 0.048), (86, 0.019), (87, 0.466), (90, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80892628 <a title="3-lda-1" href="./jmlr-2011-CARP%3A_Software_for_Fishing_Out_Good_Clustering_Algorithms.html">15 jmlr-2011-CARP: Software for Fishing Out Good Clustering Algorithms</a></p>
<p>Author: Volodymyr Melnykov, Ranjan Maitra</p><p>Abstract: This paper presents the C LUSTERING A LGORITHMS ’ R EFEREE PACKAGE or CARP, an open source GNU GPL-licensed C package for evaluating clustering algorithms. Calibrating performance of such algorithms is important and CARP addresses this need by generating datasets of different clustering complexity and by assessing the performance of the concerned algorithm in terms of its ability to classify each dataset relative to the true grouping. This paper brieﬂy describes the software and its capabilities. Keywords: CARP, M IX S IM, clustering algorithm, Gaussian mixture, overlap</p><p>same-paper 2 0.75656712 <a title="3-lda-2" href="./jmlr-2011-A_Cure_for_Variance_Inflation_in_High_Dimensional_Kernel_Principal_Component_Analysis.html">3 jmlr-2011-A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis</a></p>
<p>Author: Trine Julie Abrahamsen, Lars Kai Hansen</p><p>Abstract: Small sample high-dimensional principal component analysis (PCA) suffers from variance inﬂation and lack of generalizability. It has earlier been pointed out that a simple leave-one-out variance renormalization scheme can cure the problem. In this paper we generalize the cure in two directions: First, we propose a computationally less intensive approximate leave-one-out estimator, secondly, we show that variance inﬂation is also present in kernel principal component analysis (kPCA) and we provide a non-parametric renormalization scheme which can quite efﬁciently restore generalizability in kPCA. As for PCA our analysis also suggests a simpliﬁed approximate expression. Keywords: PCA, kernel PCA, generalizability, variance renormalization</p><p>3 0.26684594 <a title="3-lda-3" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<p>Author: Graham W. Taylor, Geoffrey E. Hinton, Sam T. Roweis</p><p>Abstract: In this paper we develop a class of nonlinear generative models for high-dimensional time series. We ﬁrst propose a model based on the restricted Boltzmann machine (RBM) that uses an undirected model with binary latent variables and real-valued “visible” variables. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. This “conditional” RBM (CRBM) makes on-line inference efﬁcient and allows us to use a simple approximate learning procedure. We demonstrate the power of our approach by synthesizing various sequences from a model trained on motion capture data and by performing on-line ﬁlling in of data lost during capture. We extend the CRBM in a way that preserves its most important computational properties and introduces multiplicative three-way interactions that allow the effective interaction weight between two variables to be modulated by the dynamic state of a third variable. We introduce a factoring of the implied three-way weight tensor to permit a more compact parameterization. The resulting model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve its ability to blend motion styles or to transition smoothly among them. Videos and source code can be found at http://www.cs.nyu.edu/˜gwtaylor/publications/ jmlr2011. Keywords: unsupervised learning, restricted Boltzmann machines, time series, generative models, motion capture</p><p>4 0.264103 <a title="3-lda-4" href="./jmlr-2011-Multiple_Kernel_Learning_Algorithms.html">66 jmlr-2011-Multiple Kernel Learning Algorithms</a></p>
<p>Author: Mehmet Gönen, Ethem Alpaydın</p><p>Abstract: In recent years, several methods have been proposed to combine multiple kernels instead of using a single one. These different kernels may correspond to using different notions of similarity or may be using information coming from multiple sources (different representations or different feature subsets). In trying to organize and highlight the similarities and differences between them, we give a taxonomy of and review several multiple kernel learning algorithms. We perform experiments on real data sets for better illustration and comparison of existing algorithms. We see that though there may not be large differences in terms of accuracy, there is difference between them in complexity as given by the number of stored support vectors, the sparsity of the solution as given by the number of used kernels, and training time complexity. We see that overall, using multiple kernels instead of a single one is useful and believe that combining kernels in a nonlinear or data-dependent way seems more promising than linear combination in fusing information provided by simple linear kernels, whereas linear methods are more reasonable when combining complex Gaussian kernels. Keywords: support vector machines, kernel machines, multiple kernel learning</p><p>5 0.26331398 <a title="3-lda-5" href="./jmlr-2011-Operator_Norm_Convergence_of_Spectral_Clustering_on_Level_Sets.html">74 jmlr-2011-Operator Norm Convergence of Spectral Clustering on Level Sets</a></p>
<p>Author: Bruno Pelletier, Pierre Pudlo</p><p>Abstract: Following Hartigan (1975), a cluster is deﬁned as a connected component of the t-level set of the underlying density, that is, the set of points for which the density is greater than t. A clustering algorithm which combines a density estimate with spectral clustering techniques is proposed. Our algorithm is composed of two steps. First, a nonparametric density estimate is used to extract the data points for which the estimated density takes a value greater than t. Next, the extracted points are clustered based on the eigenvectors of a graph Laplacian matrix. Under mild assumptions, we prove the almost sure convergence in operator norm of the empirical graph Laplacian operator associated with the algorithm. Furthermore, we give the typical behavior of the representation of the data set into the feature space, which establishes the strong consistency of our proposed algorithm. Keywords: spectral clustering, graph, unsupervised classiﬁcation, level sets, connected components</p><p>6 0.26283753 <a title="3-lda-6" href="./jmlr-2011-A_Family_of_Simple_Non-Parametric_Kernel_Learning_Algorithms.html">4 jmlr-2011-A Family of Simple Non-Parametric Kernel Learning Algorithms</a></p>
<p>7 0.26221657 <a title="3-lda-7" href="./jmlr-2011-The_Sample_Complexity_of_Dictionary_Learning.html">91 jmlr-2011-The Sample Complexity of Dictionary Learning</a></p>
<p>8 0.26152065 <a title="3-lda-8" href="./jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</a></p>
<p>9 0.26077583 <a title="3-lda-9" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>10 0.25971332 <a title="3-lda-10" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>11 0.2584219 <a title="3-lda-11" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>12 0.25641358 <a title="3-lda-12" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<p>13 0.25596383 <a title="3-lda-13" href="./jmlr-2011-Learning_with_Structured_Sparsity.html">59 jmlr-2011-Learning with Structured Sparsity</a></p>
<p>14 0.25551224 <a title="3-lda-14" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>15 0.25545177 <a title="3-lda-15" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>16 0.25541592 <a title="3-lda-16" href="./jmlr-2011-Neyman-Pearson_Classification%2C_Convexity_and_Stochastic_Constraints.html">69 jmlr-2011-Neyman-Pearson Classification, Convexity and Stochastic Constraints</a></p>
<p>17 0.25527528 <a title="3-lda-17" href="./jmlr-2011-Locally_Defined_Principal_Curves_and_Surfaces.html">60 jmlr-2011-Locally Defined Principal Curves and Surfaces</a></p>
<p>18 0.25427228 <a title="3-lda-18" href="./jmlr-2011-Computationally_Efficient_Convolved_Multiple_Output_Gaussian_Processes.html">17 jmlr-2011-Computationally Efficient Convolved Multiple Output Gaussian Processes</a></p>
<p>19 0.254262 <a title="3-lda-19" href="./jmlr-2011-Variable_Sparsity_Kernel_Learning.html">101 jmlr-2011-Variable Sparsity Kernel Learning</a></p>
<p>20 0.25370723 <a title="3-lda-20" href="./jmlr-2011-Training_SVMs_Without_Offset.html">95 jmlr-2011-Training SVMs Without Offset</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
