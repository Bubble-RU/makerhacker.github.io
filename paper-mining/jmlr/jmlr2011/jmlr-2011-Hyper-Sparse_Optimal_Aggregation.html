<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 jmlr-2011-Hyper-Sparse Optimal Aggregation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-40" href="#">jmlr2011-40</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>40 jmlr-2011-Hyper-Sparse Optimal Aggregation</h1>
<br/><p>Source: <a title="jmlr-2011-40-pdf" href="http://jmlr.org/papers/volume12/gaiffas11a/gaiffas11a.pdf">pdf</a></p><p>Author: Stéphane Gaïffas, Guillaume Lecué</p><p>Abstract: Given a ﬁnite set F of functions and a learning sample, the aim of an aggregation procedure is to have a risk as close as possible to risk of the best function in F. Up to now, optimal aggregation procedures are convex combinations of every elements of F. In this paper, we prove that optimal aggregation procedures combining only two functions in F exist. Such algorithms are of particular interest when F contains many irrelevant functions that should not appear in the aggregation procedure. Since selectors are suboptimal aggregation procedures, this proves that two is the minimal number of elements of F required for the construction of an optimal aggregation procedure in every situations. Then, we perform a numerical study for the problem of selection of the regularization parameters of the Lasso and the Elastic-net estimators. We compare on simulated examples our aggregation algorithms to aggregation with exponential weights, to Mallow’s Cp and to cross-validation selection procedures. Keywords: aggregation, exact oracle inequality, empirical risk minimization, empirical process theory, sparsity, Lasso, Lars</p><p>Reference: <a title="jmlr-2011-40-reference" href="../jmlr2011_reference/jmlr-2011-Hyper-Sparse_Optimal_Aggregation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Up to now, optimal aggregation procedures are convex combinations of every elements of F. [sent-6, score-0.52]
</p><p>2 In this paper, we prove that optimal aggregation procedures combining only two functions in F exist. [sent-7, score-0.52]
</p><p>3 Such algorithms are of particular interest when F contains many irrelevant functions that should not appear in the aggregation procedure. [sent-8, score-0.462]
</p><p>4 Since selectors are suboptimal aggregation procedures, this proves that two is the minimal number of elements of F required for the construction of an optimal aggregation procedure in every situations. [sent-9, score-0.947]
</p><p>5 We compare on simulated examples our aggregation algorithms to aggregation with exponential weights, to Mallow’s Cp and to cross-validation selection procedures. [sent-11, score-0.946]
</p><p>6 Keywords: aggregation, exact oracle inequality, empirical risk minimization, empirical process theory, sparsity, Lasso, Lars  1. [sent-12, score-0.187]
</p><p>7 This set of functions is often a set of estimators computed on a training sample, which is independent of the sample Dn (learning sample). [sent-18, score-0.06]
</p><p>8 The aim of the problem of aggregation is to construct a procedure f˜ (called an aggregate) using Dn and F with a risk which is very close to the smallest risk over F. [sent-22, score-0.562]
</p><p>9 Inequalities of the form (1) are called exact oracle inequalities and r(F, n) is called the residue. [sent-25, score-0.137]
</p><p>10 , 2008) says that aggregates with values in F cannot satisfy an inequality like (1) with a residue smaller than ((log M)/n)1/2 for every F. [sent-27, score-0.143]
</p><p>11 Nevertheless, it is possible to mimic the oracle (an oracle is a element in F achieving the minimal risk over F) up to the residue (log M)/n (see Juditsky et al. [sent-28, score-0.349]
</p><p>12 , 2008 and Lecu´ and Mendele son, 2009, among others) using an aggregate f˜ that combines all the elements of F. [sent-29, score-0.212]
</p><p>13 In this case, we say that f˜ is an optimal aggregation procedure. [sent-30, score-0.462]
</p><p>14 e Given the set of functions F, a natural way to predict Y is to compute the empirical risk minimization procedure (ERM), the one that minimizes the empirical risk Rn ( f ) :=  1 n ∑ (Yi − f (Xi ))2 n i=1  over F. [sent-32, score-0.1]
</p><p>15 This very basic principle is at the core of aggregation procedures (for regression with squared loss). [sent-33, score-0.52]
</p><p>16 An aggregate is typically represented as a convex combination of the elements of F. [sent-34, score-0.212]
</p><p>17 Up to now, most of j=1 the optimal aggregation procedures are based on exponential weights: aggregation with cumulated exponential weights (ACEW), see Catoni (2001), Yang (2004), Yang (2000), Juditsky et al. [sent-36, score-1.011]
</p><p>18 (2005), Audibert (2009) and aggregation with exponential weights (AEW), see Leung and Barron (2006) and Dalalyan and Tsybakov (2007), among others. [sent-38, score-0.491]
</p><p>19 This can be a problem when one wants to use aggregation to construct adaptive procedures. [sent-47, score-0.481]
</p><p>20 Indeed, one could imagine large dictionaries 1814  H YPER -S PARSE O PTIMAL AGGREGATION  containing many different types of estimators (kernel estimators, projection estimators, etc. [sent-48, score-0.096]
</p><p>21 Some of the estimators are likely to be more adapted than the others, depending on the kind of models that ﬁts well the data, and, there may be only few of them among a large dictionary. [sent-51, score-0.06]
</p><p>22 An aggregate that combines only the most adapted estimators from the dictionary and that removes the irrelevant ones is suitable in this case. [sent-52, score-0.482]
</p><p>23 An improvement going in this direction has been made using a preselection step in Lecu´ and Mendele son (2009). [sent-54, score-0.158]
</p><p>24 This preselection step allows to remove all the estimators in F which performs badly on a learning subsample. [sent-55, score-0.193]
</p><p>25 In this paper, we want to go a step further: we look for an aggregation algorithm that shares the same property of optimality, but with as few non-zero coefﬁcients θ j as possible, hence the name hyper-sparse aggregate. [sent-56, score-0.462]
</p><p>26 This leads to the following question: Question 1 What is the minimal number of non-zero coefﬁcients θ j such that an aggregation procedure f˜ = ∑M θ j f j is optimal? [sent-57, score-0.462]
</p><p>27 Indeed, if every coefﬁcient is zero, excepted for one, the aggregate coincides with an element of F, and as we mentioned before, such a procedure can only achieve the rate ((log M)/n)1/2 (unless extra properties are satisﬁed by F and ν). [sent-59, score-0.212]
</p><p>28 We prove in Theorem 2 below that these procedures are optimal, since they achieve the rate (log M)/n. [sent-61, score-0.058]
</p><p>29 • (Bounded setup) There is a constant b > 0 such that: max  Y  ∞ , sup f ∈F  f (X)  L∞  ≤ b. [sent-71, score-0.101]
</p><p>30 (2)  • (Sub-exponential setup) There is a constant b > 0 such that: max  ε  ψ1 , sup f ∈F  f (X) − f0 (X)  L∞  ≤ b. [sent-72, score-0.101]
</p><p>31 The results given below differ a bit depending on the considered assumption (there is an extra log n term in the sub-exponential case). [sent-74, score-0.053]
</p><p>32 Preselection) Use Dn,1 to deﬁne a random subset of F : F1 = where f  2 n,1  f ∈ F : Rn,1 ( f ) ≤ Rn,1 ( fn,1 ) + c max φ fn,1 − f  n,1 , φ  2  ,  (4)  = n−1 ∑n f (Xi )2 , Rn,1 ( f ) = n−1 ∑n ( f (Xi ) −Yi )2 , fn,1 ∈ argmin f ∈F Rn,1 ( f ). [sent-84, score-0.059]
</p><p>33 In Figure 1 we summarize the aggregation steps in the three cases. [sent-88, score-0.462]
</p><p>34 In Figure 2 we give a simulated illustration of the preselection step, and we show the value of the weights of the AEW for a comparison. [sent-89, score-0.162]
</p><p>35 As mentioned above, the Step 3 of the algorithm returns, when F is given by (6) or (7), an aggregate which is a convex combination of only two functions in F, among the ones remaining after the preselection step. [sent-90, score-0.345]
</p><p>36 The preselection step was introduced in Lecu´ and Mendelson (2009), with the use of (5) only for the aggregation e step. [sent-91, score-0.595]
</p><p>37 Theorem 2 Let x > 0 be a conﬁdence level, F be a dictionary with cardinality M and f˜ be one of the aggregation procedure given in Deﬁnition 1. [sent-94, score-0.672]
</p><p>38 If (3) holds, we have, with ν2n -probability at least 1 − 4e−x : R( f˜) ≤ min R( f ) + cσε ,b f ∈F  (1 + x) log M log n . [sent-97, score-0.106]
</p><p>39 Now, we give details for the computation of the star-shaped aggregate, namely the aggregate f˜ given by Deﬁnition 1 when F is (7). [sent-102, score-0.212]
</p><p>40 Only the elements of F with an empirical risk smaller than the threshold are kept from the dictionary for the construction of the aggregates of Deﬁnition (1). [sent-108, score-0.402]
</p><p>41 The ﬁrst and third examples correspond to a case where an aggregate with preselection step improves upon AEW, while in the second example, both procedures behaves similarly. [sent-109, score-0.403]
</p><p>42 Since aggregation procedures are known (see references above) to outperform selectors in terms of prediction error, it is tempting to use aggregation for the choice of the tuning parameters. [sent-117, score-1.055]
</p><p>43 Unfortunately, as we mentioned before, most aggregation procedures provide non-zero weights to many non relevant element in a dictionary: this is a problem for variable selection. [sent-118, score-0.549]
</p><p>44 Indeed, if we use, for instance, the AEW on a dictionary consisting of the full path of Lasso estimators (provided by the Lars algorithm, see Efron et al. [sent-119, score-0.27]
</p><p>45 , 2004), then the resulting aggregate is likely to select all the variables since the Lasso with a small regularization parameter is very close (and equal if it is zero) to ordinary least-squares (which does not perform any variables selection). [sent-120, score-0.212]
</p><p>46 So, in this context, the hyper-sparse aggregate of Section 2 is of particular interest. [sent-121, score-0.212]
</p><p>47 In this section, we compare the prediction error and the accuracy of variable selection of our star-shaped aggregation algorithm to Mallow’s Cp heuristic, leave-one-out crossvalidation and 10-fold cross-validation. [sent-122, score-0.516]
</p><p>48 2 we consider a dictionary consisting of the 1818  H YPER -S PARSE O PTIMAL AGGREGATION  Algorithm 1: Computation of the star-shaped aggregate. [sent-124, score-0.21]
</p><p>49 Input: dictionary F, data (Xi ,Yi )2n , and a conﬁdence level x > 0 i=1 Output: star-shaped aggregate f˜ Split D2n into two samples Dn,1 and Dn,2 foreach j ∈ {1, . [sent-125, score-0.466]
</p><p>50 , M} do Compute Rn,1 ( f j ) and Rn,2 ( f j ), and use this loop to ﬁnd fn,1 ∈ argmin f ∈F Rn,1 ( f ) end foreach j ∈ {1, . [sent-128, score-0.073]
</p><p>51 Indeed, let us recall that here, the focus is on the comparison of selection and aggregation procedures for the choice of tuning parameters, and not on the comparison of the procedures inside the dictionary themselves. [sent-135, score-0.828]
</p><p>52 The noise εi is N(0, σ2 ) with σ equal to 1 or 3. [sent-147, score-0.054]
</p><p>53 The noise εi is N(0, σ2 ) with σ equal to 15 or 7. [sent-158, score-0.054]
</p><p>54 2 Procedures We consider a dictionary consisting of the entire sequence of Lasso estimators and a dictionary with several sequences of elastic-net estimators, corresponding to ridge parameters in the set of values {0, 0. [sent-180, score-0.501]
</p><p>55 1, 1, 5, 10, 20, 50, 100} (these dictionaries are computed with the lars and enet routines from R). [sent-182, score-0.093]
</p><p>56 1 For each dictionary, we compute the prediction errors |X(β − β)|2 (where X is the matrix ⊤ ⊤ with rows X1 , . [sent-183, score-0.065]
</p><p>57 For both aggregates we use jackknife: we compute the mean of 100 aggregates obtained with several splits chosen at random. [sent-188, score-0.236]
</p><p>58 As a matter of fact, we observed in our numerical studies that Star-shaped aggregation with the preselection step and without it (see Deﬁnition 1) provides close estimators. [sent-194, score-0.595]
</p><p>59 So, in order to improve the computational burden, the numerical results of the Star-shaped aggregate reproduced here are the ones obtained without the preselection step. [sent-195, score-0.345]
</p><p>60 We need to explain how variable selection is performed based on J star-shaped aggregates coming from J random splits (here we take J = 100). [sent-196, score-0.14]
</p><p>61 This procedure is close in spirit to the stability selection procedure described in Meinshausen and B¨ hlmann (2010), since each aggregate u is related to a subsample. [sent-205, score-0.234]
</p><p>62 Finally, the selected covariates are the one in S = k ∈ {1, . [sent-206, score-0.064]
</p><p>63 u For each of the Models 1-6, the boxplots of the 200 prediction errors are given in Figures 3 and 4. [sent-212, score-0.065]
</p><p>64 Note that in a high dimensional setting (p > n), we don’t reproduce the Cp ’s prediction errors, since in this case the lars package does not give it correctly. [sent-213, score-0.115]
</p><p>65 The results concerning variables selection for the Lars and the Elastic-Net dictionaries are given in Tables 1 and 2. [sent-215, score-0.058]
</p><p>66 In these tables we reproduce the number of selected variables by each procedure, and the number of noise variables (the selected variables which are not active in the true model). [sent-216, score-0.142]
</p><p>67 3 Conclusion In most cases, the Star-Shaped aggregate improves upon the AEW and the considered selection procedures both in terms of prediction error and variable selection. [sent-218, score-0.324]
</p><p>68 The proposed variable selection algorithm based on star-shaped aggregation and stability selection tends to select smaller models than the Cp and cross-validation methods (see Table 1, Models 1-4) leading to less noise variables. [sent-219, score-0.56]
</p><p>69 In particular, in high-dimensional cases (p > n), it is much more stable regarding the sample size and noise level, and provides better results most of the time (see Table 1, Models 5-6). [sent-220, score-0.054]
</p><p>70 We can say that, roughly, the Cp and the cross-validations are better than the Star-Shaped aggregate only for non-sparse vectors (since these selection procedures tend to select larger models), in particular when n is small and σ is large. [sent-222, score-0.292]
</p><p>71 We can conclude by saying that, in the worst cases, the Star-shaped algorithm has prediction and selection performances which are comparable to cross-validations and Cp heuristic, but, on the other hand, it can improve them a lot (in particular for sparse vectors). [sent-223, score-0.054]
</p><p>72 One can think of the Star-Shaped aggregation algorithm as an alternative to cross-validation and Cp . [sent-224, score-0.462]
</p><p>73 , Xn are independent random variables and F is a countable set of functions such that E f (X) = 0, ∀ f ∈ F and sup f ∈F f (X) ψ1 < +∞. [sent-397, score-0.071]
</p><p>74 Deﬁne 1 n Z := sup ∑ f (Xi ) f ∈F n i=1 1827  ´ G A¨FFAS AND L ECU E I  and σ2 = sup E f (X)2 and b := f ∈F  max sup | f (Xi )|  i=1,. [sent-398, score-0.243]
</p><p>75 Then, for any η ∈ (0, 1) and δ > 0, there is c = cη,δ such that for any x > 0: x P Z ≥ (1 + η)EZ + σ 2(1 + δ) + cb n x P Z ≤ (1 − η)EZ − σ 2(1 + δ) − cb n  x n x n  ≤ 4e−x ≤ 4e−x . [sent-402, score-0.218]
</p><p>76 For any function f deﬁne (P − Pn )( f ) := i=1 n−1 ∑n f (Zi ) − E f (Z) and for a class of functions F, deﬁne P − Pn F := sup f ∈F |(P − Pn )( f )|. [sent-405, score-0.071]
</p><p>77 Lemma 7 Deﬁne σ2 (F) = sup E[ f (X)2 ],  d(F) := diam(F, L2 (µ)),  C = conv(F),  f ∈F  and LC (C ) = {(Y − f (X))2 − (Y − f C (X))2 : f ∈ C }, where f C ∈ argming∈C R(g). [sent-408, score-0.071]
</p><p>78 If (2) holds, we have 1 n 2 b2 log M f (Xi ) ≤ c max σ2 (F), , and ∑ n f ∈F n i=1  E sup  E Pn − P LC (C ) ≤ cb  log M max b n  log M , d(F) . [sent-409, score-0.399]
</p><p>79 n  If (3) holds, we have b2 log M 1 n 2 ∑ f (Xi ) ≤ c max σ2 (F), n , and f ∈F n i=1  E sup  log M log n max b n  E Pn − P LC (C ) ≤ cb  log M log n , d(F) . [sent-410, score-0.505]
</p><p>80 Deﬁne 1 n ∑ f (Xi )2 , f ∈F n i=1  r2 = sup  e and note that EX (r2 ) ≤ EX P − Pn F 2 + σ(F)2 , where F := { f 2 : f ∈ F}. [sent-412, score-0.071]
</p><p>81 Using the Gin´ -Zinn symmetrization argument, see Gin´ and Zinn (1984), we have e EX P − Pn  F2  c ≤ EX Eg sup n f ∈F 1828  n  ∑ gi f 2 (Xi )  i=1  ,  H YPER -S PARSE O PTIMAL AGGREGATION  where (gi ) are i. [sent-413, score-0.108]
</p><p>82 Using (3) we have dn,∞ ( f , f ′ ) ≤ 2b for any f , f ′ ∈ F, so using Dudley’s entropy integral, we have Eg P − Pn  F2  c ≤√ n  2b  log N(F, dn,∞ ,t)dt ≤ cr  0  log M . [sent-421, score-0.106]
</p><p>83 n  So, we get EX P − Pn  F2  log M EX [r] ≤ cb n  ≤ cb  log M n  EX [r2 ],  which entails that  b2 log M + σ(F)2 . [sent-422, score-0.377]
</p><p>84 Using the same argument as before we have EX (r2 ) ≤ c max  c E P − Pn LC (C ) ≤ E(X,Y ) Eg sup n f ∈C  n  ∑ gi L f (Xi ,Yi )  . [sent-425, score-0.138]
</p><p>85 Therefore, by Slepian’s Lemma, we have for every (Xi ,Yi )n : i=1 i=1 Eg sup Z f ≤ max sup |2Yi − f (Xi ) − f ′ (Xi )| × Eg sup Z ′f , f ∈C  i=1,. [sent-434, score-0.243]
</p><p>86 , M and ∑ α j = 1, Z ′f = ∑M α j Z f j , j=1 j=1 we have Eg sup Z ′f ≤ Eg sup Z ′f . [sent-440, score-0.142]
</p><p>87 f ∈F  f ∈C  Moreover, we have, using Dudley’s entropy integral argument, 1 c Eg sup Z ′f ≤ √ n n f ∈F  ∆n (F ′ ) 0  N(F, · 1829  n ,t)dt  ≤c  log M ′ r, n  ´ G A¨FFAS AND L ECU E I  where F ′ := { f − f C : f ∈ F} and ∆n (F ′ ) := diam(F ′ , ·  n)  and  1 n ∑ f (Xi )2 . [sent-441, score-0.124]
</p><p>88 ′ n f ∈F i=1  r′2 := sup Hence, we proved that E P − Pn LC (C ) ≤ c  log M n  E max |2Yi − f (Xi ) − f ′ (Xi )|2 i=1,. [sent-442, score-0.154]
</p><p>89 Using Pisier’s inequality for ψ1 random variables and the fact that E(U 2 ) ≤ 4 U random variable U, together with (3), we obtain that  ψ1  for any ψ1 -  E max sup |2Yi − f (Xi ) − f ′ (Xi )|2 ≤ cb2 log(n). [sent-446, score-0.101]
</p><p>90 ,n f , f ′ ∈C  (8)  So, we ﬁnally obtain log n log M n  E P − Pn LC (C ) ≤ c  E(r′2 ),  and the conclusion follows from the ﬁrst part of the Lemma, since σ(F ′ ) ≤ d(F). [sent-450, score-0.106]
</p><p>91 If (3) holds, we have, with probability larger than 1 − 4e−x , that for every f ∈ C : 1 n ∑ L f (Xi ,Yi ) − EL f (X,Y ) n i=1 ≤ c(σε + b)  (log M + x) log n max b n  (log M + x) log n , d(F) . [sent-454, score-0.136]
</p><p>92 n  H YPER -S PARSE O PTIMAL AGGREGATION  where σ(C )2 = sup E[L f (X,Y )2 ], and f ∈C  bn (C ) =  max sup |L f (Xi ,Yi ) − E[L f (X,Y )]|  i=1,. [sent-456, score-0.198]
</p><p>93 ε  ψ1 ,  we have bn (C ) ≤ 2 log(n + 1) sup f ∈C |L f (X,Y )|  ψ1 . [sent-461, score-0.097]
</p><p>94 Putting all this together, and using Lemma 7, we arrive at Z ≤ c(σε + b)  (log M + x) log n max b n  (log M + x) log n , d(F) , n  with probability larger than 1 − 4e−x for any x > 0. [sent-463, score-0.136]
</p><p>95 If (3) holds, we have with probability larger than 1 − 4e−x , that for every f ∈ F: 1 n ∑ L f (Xi ,Yi ) − EL f (X,Y ) n i=1  (log M + x) log n max b n  ≤ c(σε + b)  (log M + x) log n , f − fF n  . [sent-466, score-0.136]
</p><p>96 Also, with probability at least 1 − 4e−x , we have for every f , g ∈ F: f −g  2 n−  f −g  ≤ cb  2  (log M + x) log n max b n  (log M + x) log n , f −g n  . [sent-467, score-0.245]
</p><p>97 If (3) holds, we have with probability at least 1 − 4 exp(−x) that f F ∈ F1 , and any function f ∈ F1 satisﬁes R( f ) ≤ R( f F ) + c(σε + b)  (log M + x) log n max b n  (log M + x) log n , d(F1 ) . [sent-471, score-0.136]
</p><p>98 n  If (2) holds, we have with probability at least 1 − 2 exp(−x) that f F ∈ F1 , and any function f ∈ F1 satisﬁes R( f ) ≤ R( f F ) + cb  log M + x max b n  log M + x , d(F1 ) . [sent-472, score-0.245]
</p><p>99 Recursive aggregation of estimators by the mirror descent method with averaging. [sent-517, score-0.541]
</p><p>100 On the optimality of the aggregate with exponential e weights for small temperatures. [sent-545, score-0.241]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('aggregation', 0.462), ('aew', 0.427), ('loo', 0.236), ('star', 0.22), ('cp', 0.214), ('aggregate', 0.212), ('dictionary', 0.21), ('ffas', 0.177), ('lecu', 0.162), ('ecu', 0.147), ('yper', 0.147), ('oracle', 0.137), ('preselection', 0.133), ('aggregates', 0.118), ('ptimal', 0.113), ('cb', 0.109), ('lc', 0.1), ('eg', 0.094), ('pn', 0.092), ('parse', 0.086), ('dn', 0.086), ('juditsky', 0.075), ('erm', 0.072), ('sup', 0.071), ('conv', 0.068), ('issn', 0.063), ('lasso', 0.063), ('ff', 0.063), ('lf', 0.062), ('estimators', 0.06), ('acew', 0.059), ('dalalyan', 0.059), ('procedures', 0.058), ('xi', 0.058), ('lars', 0.057), ('noise', 0.054), ('log', 0.053), ('diam', 0.052), ('tsybakov', 0.051), ('ex', 0.051), ('risk', 0.05), ('mendelson', 0.048), ('guillaume', 0.045), ('ferm', 0.044), ('foreach', 0.044), ('seg', 0.044), ('talagrand', 0.044), ('el', 0.04), ('mallow', 0.038), ('gi', 0.037), ('dictionaries', 0.036), ('doi', 0.034), ('gin', 0.034), ('errors', 0.033), ('covariates', 0.033), ('prediction', 0.032), ('selected', 0.031), ('max', 0.03), ('weights', 0.029), ('anatoli', 0.029), ('appliqu', 0.029), ('argming', 0.029), ('cances', 0.029), ('fother', 0.029), ('mendele', 0.029), ('yuhong', 0.029), ('argmin', 0.029), ('alexandre', 0.027), ('zi', 0.027), ('bn', 0.026), ('reproduce', 0.026), ('meinshausen', 0.026), ('leung', 0.025), ('temperatures', 0.025), ('son', 0.025), ('residue', 0.025), ('threshold', 0.024), ('split', 0.023), ('shahar', 0.023), ('selectors', 0.023), ('temperature', 0.023), ('ez', 0.022), ('url', 0.022), ('selection', 0.022), ('truth', 0.022), ('ridge', 0.021), ('dudley', 0.021), ('segments', 0.021), ('efron', 0.021), ('zou', 0.021), ('lemma', 0.02), ('phane', 0.019), ('mirror', 0.019), ('laboratoire', 0.019), ('excess', 0.019), ('wants', 0.019), ('anr', 0.019), ('unbounded', 0.018), ('tuning', 0.018), ('hui', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="40-tfidf-1" href="./jmlr-2011-Hyper-Sparse_Optimal_Aggregation.html">40 jmlr-2011-Hyper-Sparse Optimal Aggregation</a></p>
<p>Author: Stéphane Gaïffas, Guillaume Lecué</p><p>Abstract: Given a ﬁnite set F of functions and a learning sample, the aim of an aggregation procedure is to have a risk as close as possible to risk of the best function in F. Up to now, optimal aggregation procedures are convex combinations of every elements of F. In this paper, we prove that optimal aggregation procedures combining only two functions in F exist. Such algorithms are of particular interest when F contains many irrelevant functions that should not appear in the aggregation procedure. Since selectors are suboptimal aggregation procedures, this proves that two is the minimal number of elements of F required for the construction of an optimal aggregation procedure in every situations. Then, we perform a numerical study for the problem of selection of the regularization parameters of the Lasso and the Elastic-net estimators. We compare on simulated examples our aggregation algorithms to aggregation with exponential weights, to Mallow’s Cp and to cross-validation selection procedures. Keywords: aggregation, exact oracle inequality, empirical risk minimization, empirical process theory, sparsity, Lasso, Lars</p><p>2 0.19650573 <a title="40-tfidf-2" href="./jmlr-2011-Hierarchical_Knowledge_Gradient_for_Sequential_Sampling.html">38 jmlr-2011-Hierarchical Knowledge Gradient for Sequential Sampling</a></p>
<p>Author: Martijn R.K. Mes, Warren B. Powell, Peter I. Frazier</p><p>Abstract: We propose a sequential sampling policy for noisy discrete global optimization and ranking and selection, in which we aim to efﬁciently explore a ﬁnite set of alternatives before selecting an alternative as best when exploration stops. Each alternative may be characterized by a multidimensional vector of categorical and numerical attributes and has independent normal rewards. We use a Bayesian probability model for the unknown reward of each alternative and follow a fully sequential sampling policy called the knowledge-gradient policy. This policy myopically optimizes the expected increment in the value of sampling information in each time period. We propose a hierarchical aggregation technique that uses the common features shared by alternatives to learn about many alternatives from even a single measurement. This approach greatly reduces the measurement effort required, but it requires some prior knowledge on the smoothness of the function in the form of an aggregation function and computational issues limit the number of alternatives that can be easily considered to the thousands. We prove that our policy is consistent, ﬁnding a globally optimal alternative when given enough measurements, and show through simulations that it performs competitively with or signiﬁcantly better than other policies. Keywords: sequential experimental design, ranking and selection, adaptive learning, hierarchical statistics, Bayesian statistics</p><p>3 0.14458111 <a title="40-tfidf-3" href="./jmlr-2011-The_Sample_Complexity_of_Dictionary_Learning.html">91 jmlr-2011-The Sample Complexity of Dictionary Learning</a></p>
<p>Author: Daniel Vainsencher, Shie Mannor, Alfred M. Bruckstein</p><p>Abstract: A large set of signals can sometimes be described sparsely using a dictionary, that is, every element can be represented as a linear combination of few elements from the dictionary. Algorithms for various signal processing applications, including classiﬁcation, denoising and signal separation, learn a dictionary from a given set of signals to be represented. Can we expect that the error in representing by such a dictionary a previously unseen signal from the same source will be of similar magnitude as those for the given examples? We assume signals are generated from a ﬁxed distribution, and study these questions from a statistical learning theory perspective. We develop generalization bounds on the quality of the learned dictionary for two types of constraints on the coefﬁcient selection, as measured by the expected L2 error in representation when the dictionary is used. For the case of l1 regularized coefﬁcient selection we provide a generalnp ln(mλ)/m , where n is the dimension, p is the number of ization bound of the order of O elements in the dictionary, λ is a bound on the l1 norm of the coefﬁcient vector and m is the number of samples, which complements existing results. For the case of representing a new signal as a combination of at most k dictionary elements, we provide a bound of the order O( np ln(mk)/m) under an assumption on the closeness to orthogonality of the dictionary (low Babel function). We further show that this assumption holds for most dictionaries in high dimensions in a strong probabilistic sense. Our results also include bounds that converge as 1/m, not previously known for this problem. We provide similar results in a general setting using kernels with weak smoothness requirements. Keywords: dictionary learning, generalization bound, sparse representation</p><p>4 0.11665761 <a title="40-tfidf-4" href="./jmlr-2011-Proximal_Methods_for_Hierarchical_Sparse_Coding.html">79 jmlr-2011-Proximal Methods for Hierarchical Sparse Coding</a></p>
<p>Author: Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, Francis Bach</p><p>Abstract: Sparse coding consists in representing signals as sparse linear combinations of atoms selected from a dictionary. We consider an extension of this framework where the atoms are further assumed to be embedded in a tree. This is achieved using a recently introduced tree-structured sparse regularization norm, which has proven useful in several applications. This norm leads to regularized problems that are difﬁcult to optimize, and in this paper, we propose efﬁcient algorithms for solving them. More precisely, we show that the proximal operator associated with this norm is computable exactly via a dual approach that can be viewed as the composition of elementary proximal operators. Our procedure has a complexity linear, or close to linear, in the number of atoms, and allows the use of accelerated gradient techniques to solve the tree-structured sparse approximation problem at the same computational cost as traditional ones using the ℓ1 -norm. Our method is efﬁcient and scales gracefully to millions of variables, which we illustrate in two types of applications: ﬁrst, we consider ﬁxed hierarchical dictionaries of wavelets to denoise natural images. Then, we apply our optimization tools in the context of dictionary learning, where learned dictionary elements naturally self-organize in a prespeciﬁed arborescent structure, leading to better performance in reconstruction of natural image patches. When applied to text documents, our method learns hierarchies of topics, thus providing a competitive alternative to probabilistic topic models. Keywords: Proximal methods, dictionary learning, structured sparsity, matrix factorization</p><p>5 0.067804962 <a title="40-tfidf-5" href="./jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</a></p>
<p>Author: Vincent Y.F. Tan, Animashree Anandkumar, Alan S. Willsky</p><p>Abstract: The problem of learning forest-structured discrete graphical models from i.i.d. samples is considered. An algorithm based on pruning of the Chow-Liu tree through adaptive thresholding is proposed. It is shown that this algorithm is both structurally consistent and risk consistent and the error probability of structure learning decays faster than any polynomial in the number of samples under ﬁxed model size. For the high-dimensional scenario where the size of the model d and the number of edges k scale with the number of samples n, sufﬁcient conditions on (n, d, k) are given for the algorithm to satisfy structural and risk consistencies. In addition, the extremal structures for learning are identiﬁed; we prove that the independent (resp., tree) model is the hardest (resp., easiest) to learn using the proposed algorithm in terms of error rates for structure learning. Keywords: graphical models, forest distributions, structural consistency, risk consistency, method of types</p><p>6 0.067534208 <a title="40-tfidf-6" href="./jmlr-2011-A_Cure_for_Variance_Inflation_in_High_Dimensional_Kernel_Principal_Component_Analysis.html">3 jmlr-2011-A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis</a></p>
<p>7 0.066242509 <a title="40-tfidf-7" href="./jmlr-2011-Union_Support_Recovery_in_Multi-task_Learning.html">97 jmlr-2011-Union Support Recovery in Multi-task Learning</a></p>
<p>8 0.062357657 <a title="40-tfidf-8" href="./jmlr-2011-Neyman-Pearson_Classification%2C_Convexity_and_Stochastic_Constraints.html">69 jmlr-2011-Neyman-Pearson Classification, Convexity and Stochastic Constraints</a></p>
<p>9 0.060105175 <a title="40-tfidf-9" href="./jmlr-2011-Group_Lasso_Estimation_of_High-dimensional_Covariance_Matrices.html">37 jmlr-2011-Group Lasso Estimation of High-dimensional Covariance Matrices</a></p>
<p>10 0.051543977 <a title="40-tfidf-10" href="./jmlr-2011-Information_Rates_of_Nonparametric_Gaussian_Process_Methods.html">44 jmlr-2011-Information Rates of Nonparametric Gaussian Process Methods</a></p>
<p>11 0.050112717 <a title="40-tfidf-11" href="./jmlr-2011-Information%2C_Divergence_and_Risk_for_Binary_Experiments.html">43 jmlr-2011-Information, Divergence and Risk for Binary Experiments</a></p>
<p>12 0.04095193 <a title="40-tfidf-12" href="./jmlr-2011-On_the_Relation_between_Realizable_and_Nonrealizable_Cases_of_the_Sequence_Prediction_Problem.html">72 jmlr-2011-On the Relation between Realizable and Nonrealizable Cases of the Sequence Prediction Problem</a></p>
<p>13 0.038450576 <a title="40-tfidf-13" href="./jmlr-2011-High-dimensional_Covariance_Estimation_Based_On_Gaussian_Graphical_Models.html">39 jmlr-2011-High-dimensional Covariance Estimation Based On Gaussian Graphical Models</a></p>
<p>14 0.035519138 <a title="40-tfidf-14" href="./jmlr-2011-Forest_Density_Estimation.html">35 jmlr-2011-Forest Density Estimation</a></p>
<p>15 0.03428679 <a title="40-tfidf-15" href="./jmlr-2011-Smoothness%2C_Disagreement_Coefficient%2C_and_the_Label_Complexity_of_Agnostic_Active_Learning.html">85 jmlr-2011-Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning</a></p>
<p>16 0.033961389 <a title="40-tfidf-16" href="./jmlr-2011-Convex_and_Network_Flow_Optimization_for_Structured_Sparsity.html">20 jmlr-2011-Convex and Network Flow Optimization for Structured Sparsity</a></p>
<p>17 0.030596623 <a title="40-tfidf-17" href="./jmlr-2011-Natural_Language_Processing_%28Almost%29_from_Scratch.html">68 jmlr-2011-Natural Language Processing (Almost) from Scratch</a></p>
<p>18 0.029496506 <a title="40-tfidf-18" href="./jmlr-2011-Dirichlet_Process_Mixtures_of_Generalized_Linear_Models.html">24 jmlr-2011-Dirichlet Process Mixtures of Generalized Linear Models</a></p>
<p>19 0.027342604 <a title="40-tfidf-19" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>20 0.02567298 <a title="40-tfidf-20" href="./jmlr-2011-Structured_Variable_Selection_with_Sparsity-Inducing_Norms.html">88 jmlr-2011-Structured Variable Selection with Sparsity-Inducing Norms</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.179), (1, -0.019), (2, 0.132), (3, 0.253), (4, 0.008), (5, 0.055), (6, -0.087), (7, -0.069), (8, -0.093), (9, -0.041), (10, -0.098), (11, 0.076), (12, -0.181), (13, 0.202), (14, -0.117), (15, 0.272), (16, 0.027), (17, 0.308), (18, -0.205), (19, 0.141), (20, 0.069), (21, 0.006), (22, -0.021), (23, -0.059), (24, 0.039), (25, -0.092), (26, -0.079), (27, -0.19), (28, 0.026), (29, -0.09), (30, 0.069), (31, -0.064), (32, 0.045), (33, -0.236), (34, -0.078), (35, 0.087), (36, 0.057), (37, 0.068), (38, 0.059), (39, 0.011), (40, 0.087), (41, -0.004), (42, -0.066), (43, 0.023), (44, -0.061), (45, -0.04), (46, 0.07), (47, -0.037), (48, 0.034), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94087893 <a title="40-lsi-1" href="./jmlr-2011-Hyper-Sparse_Optimal_Aggregation.html">40 jmlr-2011-Hyper-Sparse Optimal Aggregation</a></p>
<p>Author: Stéphane Gaïffas, Guillaume Lecué</p><p>Abstract: Given a ﬁnite set F of functions and a learning sample, the aim of an aggregation procedure is to have a risk as close as possible to risk of the best function in F. Up to now, optimal aggregation procedures are convex combinations of every elements of F. In this paper, we prove that optimal aggregation procedures combining only two functions in F exist. Such algorithms are of particular interest when F contains many irrelevant functions that should not appear in the aggregation procedure. Since selectors are suboptimal aggregation procedures, this proves that two is the minimal number of elements of F required for the construction of an optimal aggregation procedure in every situations. Then, we perform a numerical study for the problem of selection of the regularization parameters of the Lasso and the Elastic-net estimators. We compare on simulated examples our aggregation algorithms to aggregation with exponential weights, to Mallow’s Cp and to cross-validation selection procedures. Keywords: aggregation, exact oracle inequality, empirical risk minimization, empirical process theory, sparsity, Lasso, Lars</p><p>2 0.66516477 <a title="40-lsi-2" href="./jmlr-2011-Hierarchical_Knowledge_Gradient_for_Sequential_Sampling.html">38 jmlr-2011-Hierarchical Knowledge Gradient for Sequential Sampling</a></p>
<p>Author: Martijn R.K. Mes, Warren B. Powell, Peter I. Frazier</p><p>Abstract: We propose a sequential sampling policy for noisy discrete global optimization and ranking and selection, in which we aim to efﬁciently explore a ﬁnite set of alternatives before selecting an alternative as best when exploration stops. Each alternative may be characterized by a multidimensional vector of categorical and numerical attributes and has independent normal rewards. We use a Bayesian probability model for the unknown reward of each alternative and follow a fully sequential sampling policy called the knowledge-gradient policy. This policy myopically optimizes the expected increment in the value of sampling information in each time period. We propose a hierarchical aggregation technique that uses the common features shared by alternatives to learn about many alternatives from even a single measurement. This approach greatly reduces the measurement effort required, but it requires some prior knowledge on the smoothness of the function in the form of an aggregation function and computational issues limit the number of alternatives that can be easily considered to the thousands. We prove that our policy is consistent, ﬁnding a globally optimal alternative when given enough measurements, and show through simulations that it performs competitively with or signiﬁcantly better than other policies. Keywords: sequential experimental design, ranking and selection, adaptive learning, hierarchical statistics, Bayesian statistics</p><p>3 0.44003224 <a title="40-lsi-3" href="./jmlr-2011-The_Sample_Complexity_of_Dictionary_Learning.html">91 jmlr-2011-The Sample Complexity of Dictionary Learning</a></p>
<p>Author: Daniel Vainsencher, Shie Mannor, Alfred M. Bruckstein</p><p>Abstract: A large set of signals can sometimes be described sparsely using a dictionary, that is, every element can be represented as a linear combination of few elements from the dictionary. Algorithms for various signal processing applications, including classiﬁcation, denoising and signal separation, learn a dictionary from a given set of signals to be represented. Can we expect that the error in representing by such a dictionary a previously unseen signal from the same source will be of similar magnitude as those for the given examples? We assume signals are generated from a ﬁxed distribution, and study these questions from a statistical learning theory perspective. We develop generalization bounds on the quality of the learned dictionary for two types of constraints on the coefﬁcient selection, as measured by the expected L2 error in representation when the dictionary is used. For the case of l1 regularized coefﬁcient selection we provide a generalnp ln(mλ)/m , where n is the dimension, p is the number of ization bound of the order of O elements in the dictionary, λ is a bound on the l1 norm of the coefﬁcient vector and m is the number of samples, which complements existing results. For the case of representing a new signal as a combination of at most k dictionary elements, we provide a bound of the order O( np ln(mk)/m) under an assumption on the closeness to orthogonality of the dictionary (low Babel function). We further show that this assumption holds for most dictionaries in high dimensions in a strong probabilistic sense. Our results also include bounds that converge as 1/m, not previously known for this problem. We provide similar results in a general setting using kernels with weak smoothness requirements. Keywords: dictionary learning, generalization bound, sparse representation</p><p>4 0.26302367 <a title="40-lsi-4" href="./jmlr-2011-Proximal_Methods_for_Hierarchical_Sparse_Coding.html">79 jmlr-2011-Proximal Methods for Hierarchical Sparse Coding</a></p>
<p>Author: Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, Francis Bach</p><p>Abstract: Sparse coding consists in representing signals as sparse linear combinations of atoms selected from a dictionary. We consider an extension of this framework where the atoms are further assumed to be embedded in a tree. This is achieved using a recently introduced tree-structured sparse regularization norm, which has proven useful in several applications. This norm leads to regularized problems that are difﬁcult to optimize, and in this paper, we propose efﬁcient algorithms for solving them. More precisely, we show that the proximal operator associated with this norm is computable exactly via a dual approach that can be viewed as the composition of elementary proximal operators. Our procedure has a complexity linear, or close to linear, in the number of atoms, and allows the use of accelerated gradient techniques to solve the tree-structured sparse approximation problem at the same computational cost as traditional ones using the ℓ1 -norm. Our method is efﬁcient and scales gracefully to millions of variables, which we illustrate in two types of applications: ﬁrst, we consider ﬁxed hierarchical dictionaries of wavelets to denoise natural images. Then, we apply our optimization tools in the context of dictionary learning, where learned dictionary elements naturally self-organize in a prespeciﬁed arborescent structure, leading to better performance in reconstruction of natural image patches. When applied to text documents, our method learns hierarchies of topics, thus providing a competitive alternative to probabilistic topic models. Keywords: Proximal methods, dictionary learning, structured sparsity, matrix factorization</p><p>5 0.24151352 <a title="40-lsi-5" href="./jmlr-2011-Neyman-Pearson_Classification%2C_Convexity_and_Stochastic_Constraints.html">69 jmlr-2011-Neyman-Pearson Classification, Convexity and Stochastic Constraints</a></p>
<p>Author: Philippe Rigollet, Xin Tong</p><p>Abstract: Motivated by problems of anomaly detection, this paper implements the Neyman-Pearson paradigm to deal with asymmetric errors in binary classiﬁcation with a convex loss ϕ. Given a ﬁnite collection of classiﬁers, we combine them and obtain a new classiﬁer that satisﬁes simultaneously the two following properties with high probability: (i) its ϕ-type I error is below a pre-speciﬁed level and (ii), it has ϕ-type II error close to the minimum possible. The proposed classiﬁer is obtained by minimizing an empirical convex objective with an empirical convex constraint. The novelty of the method is that the classiﬁer output by this computationally feasible program is shown to satisfy the original constraint on type I error. New techniques to handle such problems are developed and they have consequences on chance constrained programming. We also evaluate the price to pay in terms of type II error for being conservative on type I error. Keywords: binary classiﬁcation, Neyman-Pearson paradigm, anomaly detection, empirical constraint, empirical risk minimization, chance constrained optimization</p><p>6 0.22574356 <a title="40-lsi-6" href="./jmlr-2011-Group_Lasso_Estimation_of_High-dimensional_Covariance_Matrices.html">37 jmlr-2011-Group Lasso Estimation of High-dimensional Covariance Matrices</a></p>
<p>7 0.22466259 <a title="40-lsi-7" href="./jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</a></p>
<p>8 0.21730433 <a title="40-lsi-8" href="./jmlr-2011-A_Cure_for_Variance_Inflation_in_High_Dimensional_Kernel_Principal_Component_Analysis.html">3 jmlr-2011-A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis</a></p>
<p>9 0.20332491 <a title="40-lsi-9" href="./jmlr-2011-On_the_Relation_between_Realizable_and_Nonrealizable_Cases_of_the_Sequence_Prediction_Problem.html">72 jmlr-2011-On the Relation between Realizable and Nonrealizable Cases of the Sequence Prediction Problem</a></p>
<p>10 0.19559456 <a title="40-lsi-10" href="./jmlr-2011-Information%2C_Divergence_and_Risk_for_Binary_Experiments.html">43 jmlr-2011-Information, Divergence and Risk for Binary Experiments</a></p>
<p>11 0.1739879 <a title="40-lsi-11" href="./jmlr-2011-Union_Support_Recovery_in_Multi-task_Learning.html">97 jmlr-2011-Union Support Recovery in Multi-task Learning</a></p>
<p>12 0.15888052 <a title="40-lsi-12" href="./jmlr-2011-High-dimensional_Covariance_Estimation_Based_On_Gaussian_Graphical_Models.html">39 jmlr-2011-High-dimensional Covariance Estimation Based On Gaussian Graphical Models</a></p>
<p>13 0.15061718 <a title="40-lsi-13" href="./jmlr-2011-Forest_Density_Estimation.html">35 jmlr-2011-Forest Density Estimation</a></p>
<p>14 0.13976128 <a title="40-lsi-14" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>15 0.13571359 <a title="40-lsi-15" href="./jmlr-2011-Information_Rates_of_Nonparametric_Gaussian_Process_Methods.html">44 jmlr-2011-Information Rates of Nonparametric Gaussian Process Methods</a></p>
<p>16 0.12985802 <a title="40-lsi-16" href="./jmlr-2011-Smoothness%2C_Disagreement_Coefficient%2C_and_the_Label_Complexity_of_Agnostic_Active_Learning.html">85 jmlr-2011-Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning</a></p>
<p>17 0.12536663 <a title="40-lsi-17" href="./jmlr-2011-Dirichlet_Process_Mixtures_of_Generalized_Linear_Models.html">24 jmlr-2011-Dirichlet Process Mixtures of Generalized Linear Models</a></p>
<p>18 0.12297376 <a title="40-lsi-18" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<p>19 0.11908793 <a title="40-lsi-19" href="./jmlr-2011-Differentially_Private_Empirical_Risk_Minimization.html">22 jmlr-2011-Differentially Private Empirical Risk Minimization</a></p>
<p>20 0.11758464 <a title="40-lsi-20" href="./jmlr-2011-An_Asymptotic_Behaviour_of_the_Marginal_Likelihood_for_General_Markov_Models.html">9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.057), (6, 0.01), (9, 0.064), (10, 0.023), (24, 0.035), (31, 0.043), (32, 0.03), (41, 0.029), (60, 0.025), (64, 0.013), (73, 0.03), (78, 0.108), (94, 0.417)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.68723166 <a title="40-lda-1" href="./jmlr-2011-Hyper-Sparse_Optimal_Aggregation.html">40 jmlr-2011-Hyper-Sparse Optimal Aggregation</a></p>
<p>Author: Stéphane Gaïffas, Guillaume Lecué</p><p>Abstract: Given a ﬁnite set F of functions and a learning sample, the aim of an aggregation procedure is to have a risk as close as possible to risk of the best function in F. Up to now, optimal aggregation procedures are convex combinations of every elements of F. In this paper, we prove that optimal aggregation procedures combining only two functions in F exist. Such algorithms are of particular interest when F contains many irrelevant functions that should not appear in the aggregation procedure. Since selectors are suboptimal aggregation procedures, this proves that two is the minimal number of elements of F required for the construction of an optimal aggregation procedure in every situations. Then, we perform a numerical study for the problem of selection of the regularization parameters of the Lasso and the Elastic-net estimators. We compare on simulated examples our aggregation algorithms to aggregation with exponential weights, to Mallow’s Cp and to cross-validation selection procedures. Keywords: aggregation, exact oracle inequality, empirical risk minimization, empirical process theory, sparsity, Lasso, Lars</p><p>2 0.63390112 <a title="40-lda-2" href="./jmlr-2011-An_Asymptotic_Behaviour_of_the_Marginal_Likelihood_for_General_Markov_Models.html">9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</a></p>
<p>Author: Piotr Zwiernik</p><p>Abstract: The standard Bayesian Information Criterion (BIC) is derived under regularity conditions which are not always satisÄ?Ĺš ed in the case of graphical models with hidden variables. In this paper we derive the BIC for the binary graphical tree models where all the inner nodes of a tree represent binary hidden variables. This provides an extension of a similar formula given by Rusakov and Geiger for naive Bayes models. The main tool used in this paper is the connection between the growth behavior of marginal likelihood integrals and the real log-canonical threshold. Keywords: BIC, marginal likelihood, singular models, tree models, Bayesian networks, real logcanonical threshold</p><p>3 0.32789421 <a title="40-lda-3" href="./jmlr-2011-The_Sample_Complexity_of_Dictionary_Learning.html">91 jmlr-2011-The Sample Complexity of Dictionary Learning</a></p>
<p>Author: Daniel Vainsencher, Shie Mannor, Alfred M. Bruckstein</p><p>Abstract: A large set of signals can sometimes be described sparsely using a dictionary, that is, every element can be represented as a linear combination of few elements from the dictionary. Algorithms for various signal processing applications, including classiﬁcation, denoising and signal separation, learn a dictionary from a given set of signals to be represented. Can we expect that the error in representing by such a dictionary a previously unseen signal from the same source will be of similar magnitude as those for the given examples? We assume signals are generated from a ﬁxed distribution, and study these questions from a statistical learning theory perspective. We develop generalization bounds on the quality of the learned dictionary for two types of constraints on the coefﬁcient selection, as measured by the expected L2 error in representation when the dictionary is used. For the case of l1 regularized coefﬁcient selection we provide a generalnp ln(mλ)/m , where n is the dimension, p is the number of ization bound of the order of O elements in the dictionary, λ is a bound on the l1 norm of the coefﬁcient vector and m is the number of samples, which complements existing results. For the case of representing a new signal as a combination of at most k dictionary elements, we provide a bound of the order O( np ln(mk)/m) under an assumption on the closeness to orthogonality of the dictionary (low Babel function). We further show that this assumption holds for most dictionaries in high dimensions in a strong probabilistic sense. Our results also include bounds that converge as 1/m, not previously known for this problem. We provide similar results in a general setting using kernels with weak smoothness requirements. Keywords: dictionary learning, generalization bound, sparse representation</p><p>4 0.32655108 <a title="40-lda-4" href="./jmlr-2011-Group_Lasso_Estimation_of_High-dimensional_Covariance_Matrices.html">37 jmlr-2011-Group Lasso Estimation of High-dimensional Covariance Matrices</a></p>
<p>Author: Jérémie Bigot, Rolando J. Biscay, Jean-Michel Loubes, Lillian Muñiz-Alvarez</p><p>Abstract: In this paper, we consider the Group Lasso estimator of the covariance matrix of a stochastic process corrupted by an additive noise. We propose to estimate the covariance matrix in a highdimensional setting under the assumption that the process has a sparse representation in a large dictionary of basis functions. Using a matrix regression model, we propose a new methodology for high-dimensional covariance matrix estimation based on empirical contrast regularization by a group Lasso penalty. Using such a penalty, the method selects a sparse set of basis functions in the dictionary used to approximate the process, leading to an approximation of the covariance matrix into a low dimensional space. Consistency of the estimator is studied in Frobenius and operator norms and an application to sparse PCA is proposed. Keywords: group Lasso, ℓ1 penalty, high-dimensional covariance estimation, basis expansion, sparsity, oracle inequality, sparse PCA</p><p>5 0.31627345 <a title="40-lda-5" href="./jmlr-2011-Super-Linear_Convergence_of_Dual_Augmented_Lagrangian_Algorithm_for_Sparsity_Regularized_Estimation.html">89 jmlr-2011-Super-Linear Convergence of Dual Augmented Lagrangian Algorithm for Sparsity Regularized Estimation</a></p>
<p>Author: Ryota Tomioka, Taiji Suzuki, Masashi Sugiyama</p><p>Abstract: We analyze the convergence behaviour of a recently proposed algorithm for regularized estimation called Dual Augmented Lagrangian (DAL). Our analysis is based on a new interpretation of DAL as a proximal minimization algorithm. We theoretically show under some conditions that DAL converges super-linearly in a non-asymptotic and global sense. Due to a special modelling of sparse estimation problems in the context of machine learning, the assumptions we make are milder and more natural than those made in conventional analysis of augmented Lagrangian algorithms. In addition, the new interpretation enables us to generalize DAL to wide varieties of sparse estimation problems. We experimentally conﬁrm our analysis in a large scale ℓ1 -regularized logistic regression problem and extensively compare the efﬁciency of DAL algorithm to previously proposed algorithms on both synthetic and benchmark data sets. Keywords: dual augmented Lagrangian, proximal minimization, global convergence, sparse estimation, convex optimization</p><p>6 0.31314892 <a title="40-lda-6" href="./jmlr-2011-Learning_with_Structured_Sparsity.html">59 jmlr-2011-Learning with Structured Sparsity</a></p>
<p>7 0.31277633 <a title="40-lda-7" href="./jmlr-2011-Structured_Variable_Selection_with_Sparsity-Inducing_Norms.html">88 jmlr-2011-Structured Variable Selection with Sparsity-Inducing Norms</a></p>
<p>8 0.31212577 <a title="40-lda-8" href="./jmlr-2011-Neyman-Pearson_Classification%2C_Convexity_and_Stochastic_Constraints.html">69 jmlr-2011-Neyman-Pearson Classification, Convexity and Stochastic Constraints</a></p>
<p>9 0.30880547 <a title="40-lda-9" href="./jmlr-2011-Convex_and_Network_Flow_Optimization_for_Structured_Sparsity.html">20 jmlr-2011-Convex and Network Flow Optimization for Structured Sparsity</a></p>
<p>10 0.30833462 <a title="40-lda-10" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<p>11 0.30797499 <a title="40-lda-11" href="./jmlr-2011-Differentially_Private_Empirical_Risk_Minimization.html">22 jmlr-2011-Differentially Private Empirical Risk Minimization</a></p>
<p>12 0.30771893 <a title="40-lda-12" href="./jmlr-2011-Efficient_Learning_with_Partially_Observed_Attributes.html">29 jmlr-2011-Efficient Learning with Partially Observed Attributes</a></p>
<p>13 0.3065061 <a title="40-lda-13" href="./jmlr-2011-Robust_Approximate_Bilinear_Programming_for_Value_Function_Approximation.html">81 jmlr-2011-Robust Approximate Bilinear Programming for Value Function Approximation</a></p>
<p>14 0.30643708 <a title="40-lda-14" href="./jmlr-2011-lp-Norm_Multiple_Kernel_Learning.html">105 jmlr-2011-lp-Norm Multiple Kernel Learning</a></p>
<p>15 0.30630451 <a title="40-lda-15" href="./jmlr-2011-Operator_Norm_Convergence_of_Spectral_Clustering_on_Level_Sets.html">74 jmlr-2011-Operator Norm Convergence of Spectral Clustering on Level Sets</a></p>
<p>16 0.30590358 <a title="40-lda-16" href="./jmlr-2011-X-Armed_Bandits.html">104 jmlr-2011-X-Armed Bandits</a></p>
<p>17 0.30564675 <a title="40-lda-17" href="./jmlr-2011-Stochastic_Methods_forl1-regularized_Loss_Minimization.html">87 jmlr-2011-Stochastic Methods forl1-regularized Loss Minimization</a></p>
<p>18 0.30336189 <a title="40-lda-18" href="./jmlr-2011-On_Equivalence_Relationships_Between_Classification_and_Ranking_Algorithms.html">71 jmlr-2011-On Equivalence Relationships Between Classification and Ranking Algorithms</a></p>
<p>19 0.30017322 <a title="40-lda-19" href="./jmlr-2011-Smoothness%2C_Disagreement_Coefficient%2C_and_the_Label_Complexity_of_Agnostic_Active_Learning.html">85 jmlr-2011-Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning</a></p>
<p>20 0.29980111 <a title="40-lda-20" href="./jmlr-2011-Convergence_of_Distributed_Asynchronous_Learning_Vector_Quantization_Algorithms.html">19 jmlr-2011-Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
