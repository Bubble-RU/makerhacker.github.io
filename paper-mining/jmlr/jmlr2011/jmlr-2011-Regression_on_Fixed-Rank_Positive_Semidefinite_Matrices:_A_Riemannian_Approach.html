<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>80 jmlr-2011-Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-80" href="#">jmlr2011-80</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>80 jmlr-2011-Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach</h1>
<br/><p>Source: <a title="jmlr-2011-80-pdf" href="http://jmlr.org/papers/volume12/meyer11a/meyer11a.pdf">pdf</a></p><p>Author: Gilles Meyer, Silvère Bonnabel, Rodolphe Sepulchre</p><p>Abstract: The paper addresses the problem of learning a regression model parameterized by a ﬁxed-rank positive semideﬁnite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of ﬁxedrank positive semideﬁnite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semideﬁnite matrix. Good performance is observed on classical benchmarks. Keywords: linear regression, positive semideﬁnite matrices, low-rank approximation, Riemannian geometry, gradient-based learning</p><p>Reference: <a title="jmlr-2011-80-reference" href="../jmlr2011_reference/jmlr-2011-Regression_on_Fixed-Rank_Positive_Semidefinite_Matrices%3A_A_Riemannian_Approach_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of ﬁxedrank positive semideﬁnite matrices. [sent-11, score-0.278]
</p><p>2 Learning a parametric model in S+ (r, d) amounts to jointly learn a r-dimensional subspace and a quadratic distance in this subspace. [sent-24, score-0.211]
</p><p>3 Our approach makes use of two quotient geometries of the set S+ (r, d) that have been recently studied by Journée et al. [sent-29, score-0.236]
</p><p>4 ˆ ˆ As it is generally not possible to compute F(W) explicitly, batch learning algorithms minimize instead the empirical cost 1 n fn (W) = ˆ (1) ∑ (yi − yi )2 , 2n i=1 which is the average loss computed over a ﬁnite number of samples {(Xi , yi )}n . [sent-68, score-0.327]
</p><p>5 At time t, the online learning algorithm minimizes the instantaneous cost 1 ft (W) = (yt − yt )2 . [sent-70, score-0.24]
</p><p>6 The single necessary change to convert an online algorithm into its batch counterpart is to perform, at each iteration, the minimization of the empirical cost fn instead of the minimization of the instantaneous cost ft . [sent-72, score-0.327]
</p><p>7 (2008), an abstract gradient descent algorithm can then be derived based on the update formula Wt+1 = RWt (−st grad f (Wt )). [sent-77, score-0.498]
</p><p>8 595  (2)  M EYER , B ONNABEL AND S EPULCHRE  The gradient grad f (Wt ) is an element of the tangent space TWt W . [sent-78, score-0.538]
</p><p>9 The retraction RWt is a mapping from the tangent space TWt W to the Riemannian manifold. [sent-80, score-0.278]
</p><p>10 Under mild conditions on the retraction R, the classical convergence theory of line-search algorithms in linear spaces generalizes to Riemannian manifolds (see Absil et al. [sent-81, score-0.218]
</p><p>11 Observe that the standard (online) learning algorithm for linear regression in Rd , wt+1 = wt − st (wtT xt − yt )xt ,  (3)  can be interpreted as a particular case of (2) for the linear model y = wT x in the linear search space ˆ d . [sent-83, score-0.533]
</p><p>12 This example illustrates that the main ingredients to obtain a concrete algorithm are convenient formulas for the gradient and for the retraction mapping. [sent-87, score-0.259]
</p><p>13 Each of those sets will be equipped with quotient Riemannian geometries that provide convenient formulas for the gradient and for the retractions. [sent-89, score-0.331]
</p><p>14 However, when the search space is endowed with a particular manifold structure, it is possible to design an exploration strategy that is consistent with the geometry of the problem and that appropriately turns the problem into an unconstrained optimization problem. [sent-97, score-0.243]
</p><p>15 One usually makes the distinction between embedded submanifolds (subsets of larger manifolds) and quotient manifolds (manifolds described by a set of equivalence classes). [sent-100, score-0.292]
</p><p>16 A typical example of quotient manifold is the set of rdimensional subspaces in Rd , viewed as a collection of r-dimensional orthogonal frames that cannot be superposed by a rotation. [sent-102, score-0.268]
</p><p>17 The rotational variants of a given frame thus deﬁne an equivalence class (denoted using square brackets [·]), which is identiﬁed as a single point on the quotient manifold. [sent-103, score-0.238]
</p><p>18 The updated point Wt+1 automatically remains inside the manifold thanks to the retraction mapping. [sent-107, score-0.231]
</p><p>19 For quotient manifolds W = W / ∼, where W is the total space and ∼ is the equivalence relation that deﬁnes the quotient, the tangent space T[W] W at [W] is sufﬁciently described by the directions that do not induce any displacement in the set of equivalence classes [W]. [sent-111, score-0.443]
</p><p>20 Provided that the metric gW in the total space is invariant along the ¯ equivalence classes, it deﬁnes a metric in the quotient space g[W] (ξ[W] , ζ[W] )  gW (ξW , ζW ). [sent-113, score-0.448]
</p><p>21 ¯ ¯ ¯  The horizontal gradient grad f (W) is obtained by projecting the gradient grad f (W) in the total ¯ space onto the set of horizontal vectors ξW at W. [sent-114, score-0.934]
</p><p>22 Natural displacements at W in a direction ξW on the manifold are performed by following geodesics (paths of shortest length on the manifold) starting from W and tangent to ξW . [sent-115, score-0.222]
</p><p>23 It is a quotient representation of the set of r-dimensional subspaces in Rd , that is, the Grassmann manifold Gr(r, d). [sent-132, score-0.268]
</p><p>24 The quotient geometries of Gr(r, d) have been well studied (Edelman et al. [sent-133, score-0.236]
</p><p>25 The metric g[U] (ξ[U] , ζ[U] )  gU (ξU , ζU ), ¯ ¯ ¯  is induced by the standard metric in Rd×r , ∆1 gU (∆ 1 , ∆2 ) = Tr(∆ T ∆2 ), ¯ ∆ which is invariant along the ﬁbers, that is, equivalence classes. [sent-136, score-0.247]
</p><p>26 Therefore, the gradient admits the simple horizontal representation grad f (U) = ΠU grad f (U), 598  (5)  R EGRESSION ON F IXED -R ANK PSD M ATRICES : A R IEMANNIAN A PPROACH  where grad f (U) is deﬁned by the identity ∆ D f (U)[∆ ] = gU (∆ , grad f (U)). [sent-138, score-1.547]
</p><p>27 A possible advantage of the retraction (7) over the retraction (6) is that, in contrast to the SVD computation, the QR decomposition is computed in a ﬁxed number O(dr2 ) of arithmetic operations. [sent-143, score-0.328]
</p><p>28 With the formulas (5) and (7) applied to the cost function (4), the abstract update (2) becomes Ut+1 = qf(Ut + st (I − Ut UtT )xt xtT Ut ), which is Oja’s update for subspace tracking (Oja, 1992). [sent-144, score-0.372]
</p><p>29 ˆ 2 The quotient geometries of S+ (d) are rooted in the matrix factorization W = GGT ,  G ∈ GL(d),  where GL(d) is the set of all invertible d × d matrices. [sent-153, score-0.236]
</p><p>30 Because the factorization is invariant by rotation, G → GO, O ∈ O (d), the search space is once again identiﬁed to the quotient S+ (d) ≃ GL(d)/O (d), which represents the set of equivalence classes [G] = {GO s. [sent-154, score-0.27]
</p><p>31 1 A Flat Metric on S+ (d) The metric on the quotient GL(d)/O (d): g[G] (ξ[G] , ζ[G] )  gG (ξG , ζG ), ¯ ¯ ¯  is induced by the standard metric in Rd×d , ∆1 gG (∆ 1 , ∆ 2 ) = Tr(∆ T ∆ 2 ), ¯ ∆ which is invariant by rotation along the set of equivalence classes. [sent-159, score-0.448]
</p><p>32 With this geometry, a tangent vector ξ[G] at [G] is represented by a horizontal ¯ tangent vector ξG at G by ¯ ∆ ξG = Sym(∆ )G, ∆ ∈ Rd×d . [sent-161, score-0.271]
</p><p>33 The horizontal gradient of 1 f (G) = ℓ(y, y) = (Tr(GGT Sym(X)) − y)2 , ˆ 2  (8)  is the unique horizontal vector grad f (G) that satisﬁes ∆ D f (G)[∆ ] = gG (∆ , grad f (G)). [sent-162, score-0.87]
</p><p>34 ¯ ∆ Elementary computations yield grad f (G) = 2(y − y)Sym(X)G. [sent-163, score-0.36]
</p><p>35 Those formulas applied to the cost (8) turns the abstract update (2) into the simple formula Gt+1 = Gt − 2st (yt − yt )Sym(Xt )Gt , ˆ  (9)  for an online gradient algorithm and Gt+1 = Gt − 2st  1 n ˆ ∑ (yi − yi )Sym(Xi )Gt , n i=1  (10)  for a batch gradient algorithm. [sent-165, score-0.656]
</p><p>36 (11)  The afﬁne-invariant geometry of S+ (d) has been well studied, in particular in the context of information geometry (Smith, 2005). [sent-172, score-0.352]
</p><p>37 The gradient grad f (W) is given by ∆ ∆ D f (W)[∆ ] = gW (∆ , grad f (W)). [sent-178, score-0.784]
</p><p>38 Applying this formula to (5) yields grad f (W) = (y − y)WSym(X)W. [sent-179, score-0.36]
</p><p>39 (14)  The formulas (12) and (13) applied to the cost (5) turn the abstract update (2) into 1  1  1  1  Wt+1 = Wt2 exp(−st (yt − yt )Wt2 Sym(Xt )Wt2 )Wt2 . [sent-182, score-0.224]
</p><p>40 ˆ With the alternative retraction (14), the update becomes Wt+1 = Wt − st (yt − yt )Wt Sym(Xt )Wt , ˆ which is the update of Davis et al. [sent-183, score-0.472]
</p><p>41 The gradient of this cost function is given by grad f (S) = (yt − yt )Sym(Xt ), ˆ and the retraction is RS (sξS ) = exp(log W + sξS ). [sent-192, score-0.745]
</p><p>42 The corresponding gradient descent update is Wt+1 = exp(log Wt − st (yt − yt )Sym(Xt )), ˆ which is the update of Tsuda et al. [sent-193, score-0.41]
</p><p>43 Indeed, the ﬂat quotient geometry of the manifold ∗ S+ (d) ≃ GL(d)/O (d) is generalized to the quotient geometry of S+ (r, d) ≃ Rd×r /O (r) by a mere ∗ adaptation of matrix dimension, leading to the updates (9) and (10) for matrices Gt ∈ Rd×r . [sent-202, score-0.863]
</p><p>44 (2010), where the quotient geometry of S+ (r, d) ≃ Rd×r /O (r) is studied in ∗ details. [sent-204, score-0.377]
</p><p>45 In the next section, we propose an alternative geometry that jointly learns a r-dimensional subspace and a full-rank quadratic model in this subspace. [sent-205, score-0.302]
</p><p>46 However, a generalization ∗ ∗ is possible by considering the polar matrix factorization G = UR,  U ∈ St(r, d), R ∈ S+ (r). [sent-208, score-0.257]
</p><p>47 This gives a polar parameterization of S+ (r, d) W = UR2 UT . [sent-210, score-0.257]
</p><p>48 This development leads to the quotient representation S+ (r, d) ≃ (St(r, d) × S+ (r))/O (r),  (15)  based on the invariance of W to the transformation (U, R2 ) → (UO, OT R2 O), O ∈ O (r). [sent-211, score-0.237]
</p><p>49 A ¯ tangent vector ξ[W] = (ξU , ξR2 )[U,R2 ] at [(U, R2 )] is described by a horizontal tangent vector ξW = ¯ ¯ (ξU , ξR2 )(U,R2 ) at (U, R2 ) by ¯ ξU = ΠU ∆ , ∆ ∈ Rd×r ,  ¯ Ψ ξR2 = RSym(Ψ )R, Ψ ∈ Rr×r . [sent-217, score-0.271]
</p><p>50 ¯ Ψ  The proposed metric is invariant along the set of equivalence classes and thus induces a quotient structure on S+ (r, d). [sent-219, score-0.359]
</p><p>51 Combining the gradient of (16) with the retractions (18) and (19) gives Ut+1 = qf Ut − 2λst (yt − yt )(I − Ut UtT )Sym(Xt )Ut Rt2 , ˆ  2 Rt+1 = Rt exp −(1 − λ)st (yt − yt )Rt UtT Sym(Xt )Ut Rt Rt . [sent-228, score-0.406]
</p><p>52 A proper tuning of this parameter allows us to place more emphasis either on the learning of the subspace U or on the distance in that subspace R2 . [sent-238, score-0.337]
</p><p>53 Intermediate values of λ continuously interpolate between the subspace learning problem and the distance learning problem at ﬁxed range space. [sent-242, score-0.211]
</p><p>54 A proper tuning of λ is of interest when a good estimate of the subspace is available (for instance a subspace given by a proper dimension reduction technique) or when too few observations are available to jointly estimate the subspace and the distance within that subspace. [sent-243, score-0.463]
</p><p>55 In the latter case, one has the choice to favor either subspace or distance learning. [sent-244, score-0.211]
</p><p>56 4 Strategies for Choosing the Step Size We here present strategies for choosing the step size in both the batch and online cases. [sent-263, score-0.263]
</p><p>57 In this paper, we use the Armijo step sA deﬁned at each iteration by the condition f (RWt (−sA grad f (Wt ))) ≤ f (Wt ) + c grad f (Wt )  2 Wt ,  (22)  where Wt ∈ S+ (r, d) is the current iterate, c ∈ (0, 1), f is the empirical cost (1) and RW is the chosen retraction. [sent-267, score-0.752]
</p><p>58 In order to reduce the dependence on smax in a particular problem, it is chosen inversely proportional to the norm of the gradient at each iteration, smax =  s0 grad f (Wt )  . [sent-270, score-0.486]
</p><p>59 In this paper, the step size schedule st is chosen as s nt0 st = × , (23) µgrad nt0 + t ˆ where s > 0, n is the number of considered learning samples, µgrad is an estimate of the average ˆ gradient norm grad f (W0 ) W0 , and t0 > 0 controls the annealing rate of st . [sent-275, score-0.757]
</p><p>60 For batch algorithms, the local convergence results follow from the convergence theory of linesearch algorithms on Riemannian manifolds (see, for example, Absil et al. [sent-295, score-0.234]
</p><p>61 For online algorithms, one can prove that the algorithm based on the ﬂat geometry enjoys almost sure asymptotic convergence to a local minimum of the expected cost. [sent-297, score-0.259]
</p><p>62 In contrast, when the polar parameterization is used, the convergence results presented by Bottou (1998) do not apply directly because of the quotient nature of the search space. [sent-299, score-0.458]
</p><p>63 Because the extension would require technical arguments beyond the scope of the present paper, we refrain from stating a formal convergence result for the online algorithm based on the polar geometry, even though the result is quite plausible. [sent-300, score-0.34]
</p><p>64 One solves (25) by solving the algebraic equation ˆ grad D(W, Wt ) = −st grad ℓ(yt+1 , yt ), ˆ 607  (26)  M EYER , B ONNABEL AND S EPULCHRE  which is a ﬁrst-order (necessary) optimality condition. [sent-313, score-0.845]
</p><p>65 If the search space W is a Riemannian manifold and if the closeness measure D(W, Wt ) is the Riemannian distance, the solution of (26) is Wt+1 = ExpWt (−st grad ℓ(yt+1 , yt )). [sent-314, score-0.592]
</p><p>66 However, yt+1 is generally ˆ ˆ replaced by yt (which is equal to yt+1 up to ﬁrst order terms in st ), which gives the update (2) where ˆ ˆ the exponential mapping is chosen as a retraction. [sent-316, score-0.272]
</p><p>67 We have shown in Section 5 that the resulting updates can be interpreted as line-search updates for the log-Euclidean metric and the afﬁne-invariant metric of S+ (d) and for speciﬁc choices of the retraction mapping. [sent-319, score-0.342]
</p><p>68 where the θi ’s are the principal angles between the subspaces spanned by W and Wt (Golub and Van Loan, 1996), and the second term is the afﬁne-invariant distance of S+ (d) between matrices R2 and Rt2 involved in the polar representation of W and Wt . [sent-322, score-0.384]
</p><p>69 A potential area of future work is the application of the proposed online algorithm (9) for adapting a batch solution to slight modiﬁcations of the dissimilarities over time. [sent-346, score-0.263]
</p><p>70 Overall, the experiments support that a joint estimation of a subspace and low-dimensional distance in that subspace is a major advantage of the proposed algorithms over methods that estimate the matrix for a subspace that is ﬁxed beforehand. [sent-413, score-0.463]
</p><p>71 Recent methods compute that subspace of reduced dimension using principal component analysis (Davis and Dhillon, 2008; Weinberger and Saul, 2009), that is, a subspace that captures a maximal amount of variance in the data. [sent-445, score-0.252]
</p><p>72 However, in general, there is no reason why the subspace spanned by the top principal components should coincide with the subspace that is deﬁned by the target model. [sent-446, score-0.252]
</p><p>73 Therefore, a more appropriate approach consists in learning jointly the subspace and a distance in that subspace that best ﬁts the data to observations within that subspace. [sent-447, score-0.337]
</p><p>74 The top plot shows that the learned subspace (which identiﬁes with the target subspace) is indeed very different from the subspace spanned by the top two principal components. [sent-453, score-0.252]
</p><p>75 612  R EGRESSION ON F IXED -R ANK PSD M ATRICES : A R IEMANNIAN A PPROACH  Data Target model subspace PCA subspace Learned subspace  10  x3  5 0 −5 −10 5  5 0 x2  0 −5 −5  1  x1 0. [sent-482, score-0.378]
</p><p>76 Top: the learned subspace is very different from the subspace computed from a classical heuristic. [sent-502, score-0.252]
</p><p>77 subspace and the distance in that subspace are learned jointly. [sent-505, score-0.337]
</p><p>78 BATCH This experiment shows that when a large amount of sample is available (80, 000 training samples and 20, 000 test samples for learning a parameter W∗ in S+ (10, 50)), online algorithms minimize the test error more rapidly than batch ones. [sent-545, score-0.329]
</p><p>79 Figure 5 report the test error as a function of the learning time, that is, the time after each iteration for batch algorithm and the time after each epoch for online algorithms. [sent-548, score-0.263]
</p><p>80 For the algorithm based on the polar geometry, the mini-batch extension is strongly recommended to amortize the larger cost of each update. [sent-549, score-0.289]
</p><p>81 For a large number of samples, online algorithms reduce the test error much more rapidly than batch ones. [sent-553, score-0.263]
</p><p>82 Distance constraints are generated as yi j ≤ yi j (1 − α) for identically labeled samples and yi j ≥ ˆ ˆ yi j (1 + α) for differentially labeled samples, where α ≥ 0 is a scaling factor, yi j = φ(xi ) − φ(x j ) 2 and yi j = Tr(W(ei − e j )(ei − e j )T ) = (ei − e j )T W(ei − e j ). [sent-562, score-0.324]
</p><p>83 1  90  Clustering Score (NMI)  Classification accuracy (%)  100 80 70 60 50 40 30 20 10  Batch flat geometry Batch polar geometry LogDet−KL Original kernel  0 0 100 200 300 400 500 600 700 800 9001000 Number of constraints  0. [sent-589, score-0.808]
</p><p>84 6 Batch flat geometry Batch polar geometry (λ = 0. [sent-591, score-0.696]
</p><p>85 5) Batch polar geometry (λ = 0) LogDet−KL KSR Original Kernel  0. [sent-592, score-0.433]
</p><p>86 The algorithm based on the polar geometry competes with LogDet-KL. [sent-595, score-0.433]
</p><p>87 We compare the proposed batch methods with the LogDet-KL algorithm, the only competing algorithm that also learns directly from distance constraints. [sent-606, score-0.265]
</p><p>88 In this full-rank learning setting, the algorithm based on the polar geometry compete with the LogDet-KL algorithm. [sent-611, score-0.433]
</p><p>89 The convergence time of the algorithm based on the polar geometry is however much faster (0. [sent-612, score-0.433]
</p><p>90 The algorithm based on the ﬂat geometry has inferior performance when too few constraints are provided. [sent-614, score-0.221]
</p><p>91 The ﬁgure shows that KSR, LogDet-KL and the algorithm based on the polar geometry with λ = 0 perform similarly. [sent-626, score-0.433]
</p><p>92 These methods are however outperformed by the proposed algorithms (ﬂat geometry and polar geometry with λ = 0. [sent-627, score-0.609]
</p><p>93 This experiment also enlightens the ﬂexibility of the polar geometry, which allows us to ﬁx the subspace in situations where too few constraints are available. [sent-629, score-0.428]
</p><p>94 4  Batch flat geometry Batch polar geometry LogDet−KL KSR Original kernel  0. [sent-634, score-0.763]
</p><p>95 6 Batch flat geometry Batch polar geometry LogDet−KL KSR MVU Kernel PCA  0. [sent-636, score-0.696]
</p><p>96 30 seconds for the algorithm based on the polar geometry. [sent-657, score-0.257]
</p><p>97 3 R ESULTS 35  Batch flat geometry Batch polar geometry ITML LEGO POLA LMNN Euclidean baseline  Classification Error (%)  30 25 20 15 10 5 0  Wine  Ionosphere Bal. [sent-700, score-0.696]
</p><p>98 (2009), we demonstrate that the proposed batch algorithms compete with state-of-the-art full-rank Mahalanobis distance learning algorithms on several UCI data sets (Figure 8). [sent-704, score-0.265]
</p><p>99 We have not included the online versions of our algorithms in this comparison because we consider that the batch approaches are more relevant on such small data sets. [sent-705, score-0.263]
</p><p>100 The rich Riemannian geometry of the set of ﬁxed-rank PSD matrices is exploited through a geometric optimization approach. [sent-723, score-0.218]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('grad', 0.36), ('polar', 0.257), ('wt', 0.211), ('sym', 0.206), ('quotient', 0.201), ('batch', 0.18), ('geometry', 0.176), ('riemannian', 0.173), ('retraction', 0.164), ('epulchre', 0.144), ('eyer', 0.144), ('iemannian', 0.144), ('onnabel', 0.144), ('mahalanobis', 0.141), ('subspace', 0.126), ('yt', 0.125), ('tol', 0.123), ('ank', 0.122), ('atrices', 0.122), ('tangent', 0.114), ('gw', 0.113), ('st', 0.111), ('ixed', 0.11), ('psd', 0.108), ('ut', 0.103), ('ksr', 0.103), ('rt', 0.096), ('kulis', 0.094), ('pproach', 0.094), ('gt', 0.092), ('metric', 0.089), ('flat', 0.087), ('distance', 0.085), ('rd', 0.085), ('online', 0.083), ('absil', 0.082), ('egression', 0.076), ('itml', 0.072), ('lego', 0.072), ('lmnn', 0.072), ('logdet', 0.072), ('utt', 0.072), ('tr', 0.069), ('kernel', 0.067), ('manifold', 0.067), ('gradient', 0.064), ('bonnabel', 0.062), ('rwt', 0.062), ('qf', 0.061), ('gl', 0.061), ('semide', 0.06), ('davis', 0.057), ('pca', 0.056), ('xt', 0.054), ('manifolds', 0.054), ('weinberger', 0.054), ('nmi', 0.051), ('pola', 0.051), ('sepulchre', 0.051), ('constraints', 0.045), ('horizontal', 0.043), ('matrices', 0.042), ('tsuda', 0.042), ('yi', 0.041), ('geodesics', 0.041), ('closeness', 0.04), ('isolet', 0.039), ('bregman', 0.038), ('descent', 0.038), ('rank', 0.038), ('equivalence', 0.037), ('asuncion', 0.036), ('newman', 0.036), ('invariance', 0.036), ('tw', 0.036), ('update', 0.036), ('ggt', 0.035), ('geometries', 0.035), ('uo', 0.035), ('multidimensional', 0.034), ('divergences', 0.034), ('samples', 0.033), ('regression', 0.032), ('clustering', 0.032), ('invariant', 0.032), ('jain', 0.032), ('cost', 0.032), ('mvu', 0.031), ('meyer', 0.031), ('classification', 0.031), ('formulas', 0.031), ('af', 0.031), ('xi', 0.031), ('gyrb', 0.031), ('retractions', 0.031), ('smax', 0.031), ('twt', 0.031), ('wsym', 0.031), ('xxt', 0.031), ('gu', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="80-tfidf-1" href="./jmlr-2011-Regression_on_Fixed-Rank_Positive_Semidefinite_Matrices%3A_A_Riemannian_Approach.html">80 jmlr-2011-Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach</a></p>
<p>Author: Gilles Meyer, Silvère Bonnabel, Rodolphe Sepulchre</p><p>Abstract: The paper addresses the problem of learning a regression model parameterized by a ﬁxed-rank positive semideﬁnite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of ﬁxedrank positive semideﬁnite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semideﬁnite matrix. Good performance is observed on classical benchmarks. Keywords: linear regression, positive semideﬁnite matrices, low-rank approximation, Riemannian geometry, gradient-based learning</p><p>2 0.18631899 <a title="80-tfidf-2" href="./jmlr-2011-Stochastic_Methods_forl1-regularized_Loss_Minimization.html">87 jmlr-2011-Stochastic Methods forl1-regularized Loss Minimization</a></p>
<p>Author: Shai Shalev-Shwartz, Ambuj Tewari</p><p>Abstract: We describe and analyze two stochastic methods for ℓ1 regularized loss minimization problems, such as the Lasso. The ﬁrst method updates the weight of a single feature at each iteration while the second method updates the entire weight vector but only uses a single training example at each iteration. In both methods, the choice of feature or example is uniformly at random. Our theoretical runtime analysis suggests that the stochastic methods should outperform state-of-the-art deterministic approaches, including their deterministic counterparts, when the size of the problem is large. We demonstrate the advantage of stochastic methods by experimenting with synthetic and natural data sets.1 Keywords: L1 regularization, optimization, coordinate descent, mirror descent, sparsity</p><p>3 0.14826575 <a title="80-tfidf-3" href="./jmlr-2011-Generalized_TD_Learning.html">36 jmlr-2011-Generalized TD Learning</a></p>
<p>Author: Tsuyoshi Ueno, Shin-ichi Maeda, Motoaki Kawanabe, Shin Ishii</p><p>Abstract: Since the invention of temporal difference (TD) learning (Sutton, 1988), many new algorithms for model-free policy evaluation have been proposed. Although they have brought much progress in practical applications of reinforcement learning (RL), there still remain fundamental problems concerning statistical properties of the value function estimation. To solve these problems, we introduce a new framework, semiparametric statistical inference, to model-free policy evaluation. This framework generalizes TD learning and its extensions, and allows us to investigate statistical properties of both of batch and online learning procedures for the value function estimation in a uniﬁed way in terms of estimating functions. Furthermore, based on this framework, we derive an optimal estimating function with the minimum asymptotic variance and propose batch and online learning algorithms which achieve the optimality. Keywords: reinforcement learning, model-free policy evaluation, TD learning, semiparametirc model, estimating function</p><p>4 0.1390972 <a title="80-tfidf-4" href="./jmlr-2011-Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization.html">8 jmlr-2011-Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></p>
<p>Author: John Duchi, Elad Hazan, Yoram Singer</p><p>Abstract: We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to ﬁnd needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which signiﬁcantly simpliﬁes setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efﬁcient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms. Keywords: subgradient methods, adaptivity, online learning, stochastic convex optimization</p><p>5 0.13232724 <a title="80-tfidf-5" href="./jmlr-2011-Super-Linear_Convergence_of_Dual_Augmented_Lagrangian_Algorithm_for_Sparsity_Regularized_Estimation.html">89 jmlr-2011-Super-Linear Convergence of Dual Augmented Lagrangian Algorithm for Sparsity Regularized Estimation</a></p>
<p>Author: Ryota Tomioka, Taiji Suzuki, Masashi Sugiyama</p><p>Abstract: We analyze the convergence behaviour of a recently proposed algorithm for regularized estimation called Dual Augmented Lagrangian (DAL). Our analysis is based on a new interpretation of DAL as a proximal minimization algorithm. We theoretically show under some conditions that DAL converges super-linearly in a non-asymptotic and global sense. Due to a special modelling of sparse estimation problems in the context of machine learning, the assumptions we make are milder and more natural than those made in conventional analysis of augmented Lagrangian algorithms. In addition, the new interpretation enables us to generalize DAL to wide varieties of sparse estimation problems. We experimentally conﬁrm our analysis in a large scale ℓ1 -regularized logistic regression problem and extensively compare the efﬁciency of DAL algorithm to previously proposed algorithms on both synthetic and benchmark data sets. Keywords: dual augmented Lagrangian, proximal minimization, global convergence, sparse estimation, convex optimization</p><p>6 0.089073956 <a title="80-tfidf-6" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>7 0.076378822 <a title="80-tfidf-7" href="./jmlr-2011-Efficient_Learning_with_Partially_Observed_Attributes.html">29 jmlr-2011-Efficient Learning with Partially Observed Attributes</a></p>
<p>8 0.068967715 <a title="80-tfidf-8" href="./jmlr-2011-Better_Algorithms_for_Benign_Bandits.html">14 jmlr-2011-Better Algorithms for Benign Bandits</a></p>
<p>9 0.065684125 <a title="80-tfidf-9" href="./jmlr-2011-A_Cure_for_Variance_Inflation_in_High_Dimensional_Kernel_Principal_Component_Analysis.html">3 jmlr-2011-A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis</a></p>
<p>10 0.063851871 <a title="80-tfidf-10" href="./jmlr-2011-Locally_Defined_Principal_Curves_and_Surfaces.html">60 jmlr-2011-Locally Defined Principal Curves and Surfaces</a></p>
<p>11 0.059801783 <a title="80-tfidf-11" href="./jmlr-2011-Double_Updating_Online_Learning.html">28 jmlr-2011-Double Updating Online Learning</a></p>
<p>12 0.051474869 <a title="80-tfidf-12" href="./jmlr-2011-A_Family_of_Simple_Non-Parametric_Kernel_Learning_Algorithms.html">4 jmlr-2011-A Family of Simple Non-Parametric Kernel Learning Algorithms</a></p>
<p>13 0.049506269 <a title="80-tfidf-13" href="./jmlr-2011-Learning_Transformation_Models_for_Ranking_and_Survival_Analysis.html">56 jmlr-2011-Learning Transformation Models for Ranking and Survival Analysis</a></p>
<p>14 0.049182698 <a title="80-tfidf-14" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>15 0.044235874 <a title="80-tfidf-15" href="./jmlr-2011-Convergence_of_Distributed_Asynchronous_Learning_Vector_Quantization_Algorithms.html">19 jmlr-2011-Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms</a></p>
<p>16 0.042930093 <a title="80-tfidf-16" href="./jmlr-2011-Information%2C_Divergence_and_Risk_for_Binary_Experiments.html">43 jmlr-2011-Information, Divergence and Risk for Binary Experiments</a></p>
<p>17 0.041274782 <a title="80-tfidf-17" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<p>18 0.040951792 <a title="80-tfidf-18" href="./jmlr-2011-Online_Learning_in_Case_of_Unbounded_Losses_Using_Follow_the_Perturbed_Leader_Algorithm.html">73 jmlr-2011-Online Learning in Case of Unbounded Losses Using Follow the Perturbed Leader Algorithm</a></p>
<p>19 0.037315492 <a title="80-tfidf-19" href="./jmlr-2011-A_Bayesian_Approach_for_Learning_and_Planning_in_Partially_Observable_Markov_Decision_Processes.html">1 jmlr-2011-A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes</a></p>
<p>20 0.036439948 <a title="80-tfidf-20" href="./jmlr-2011-A_Simpler_Approach_to_Matrix_Completion.html">6 jmlr-2011-A Simpler Approach to Matrix Completion</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.242), (1, 0.334), (2, -0.034), (3, -0.152), (4, -0.091), (5, -0.142), (6, -0.023), (7, -0.021), (8, -0.017), (9, -0.004), (10, 0.04), (11, 0.051), (12, 0.019), (13, -0.016), (14, -0.077), (15, -0.009), (16, 0.199), (17, 0.019), (18, -0.066), (19, -0.126), (20, 0.049), (21, 0.065), (22, -0.005), (23, -0.005), (24, 0.027), (25, 0.027), (26, -0.011), (27, -0.056), (28, -0.031), (29, 0.0), (30, 0.099), (31, 0.031), (32, -0.059), (33, -0.056), (34, -0.068), (35, -0.015), (36, 0.045), (37, -0.003), (38, -0.012), (39, -0.034), (40, 0.03), (41, -0.047), (42, 0.033), (43, 0.071), (44, -0.044), (45, 0.001), (46, -0.022), (47, -0.006), (48, 0.023), (49, -0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94248533 <a title="80-lsi-1" href="./jmlr-2011-Regression_on_Fixed-Rank_Positive_Semidefinite_Matrices%3A_A_Riemannian_Approach.html">80 jmlr-2011-Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach</a></p>
<p>Author: Gilles Meyer, Silvère Bonnabel, Rodolphe Sepulchre</p><p>Abstract: The paper addresses the problem of learning a regression model parameterized by a ﬁxed-rank positive semideﬁnite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of ﬁxedrank positive semideﬁnite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semideﬁnite matrix. Good performance is observed on classical benchmarks. Keywords: linear regression, positive semideﬁnite matrices, low-rank approximation, Riemannian geometry, gradient-based learning</p><p>2 0.70279527 <a title="80-lsi-2" href="./jmlr-2011-Generalized_TD_Learning.html">36 jmlr-2011-Generalized TD Learning</a></p>
<p>Author: Tsuyoshi Ueno, Shin-ichi Maeda, Motoaki Kawanabe, Shin Ishii</p><p>Abstract: Since the invention of temporal difference (TD) learning (Sutton, 1988), many new algorithms for model-free policy evaluation have been proposed. Although they have brought much progress in practical applications of reinforcement learning (RL), there still remain fundamental problems concerning statistical properties of the value function estimation. To solve these problems, we introduce a new framework, semiparametric statistical inference, to model-free policy evaluation. This framework generalizes TD learning and its extensions, and allows us to investigate statistical properties of both of batch and online learning procedures for the value function estimation in a uniﬁed way in terms of estimating functions. Furthermore, based on this framework, we derive an optimal estimating function with the minimum asymptotic variance and propose batch and online learning algorithms which achieve the optimality. Keywords: reinforcement learning, model-free policy evaluation, TD learning, semiparametirc model, estimating function</p><p>3 0.66833538 <a title="80-lsi-3" href="./jmlr-2011-Stochastic_Methods_forl1-regularized_Loss_Minimization.html">87 jmlr-2011-Stochastic Methods forl1-regularized Loss Minimization</a></p>
<p>Author: Shai Shalev-Shwartz, Ambuj Tewari</p><p>Abstract: We describe and analyze two stochastic methods for ℓ1 regularized loss minimization problems, such as the Lasso. The ﬁrst method updates the weight of a single feature at each iteration while the second method updates the entire weight vector but only uses a single training example at each iteration. In both methods, the choice of feature or example is uniformly at random. Our theoretical runtime analysis suggests that the stochastic methods should outperform state-of-the-art deterministic approaches, including their deterministic counterparts, when the size of the problem is large. We demonstrate the advantage of stochastic methods by experimenting with synthetic and natural data sets.1 Keywords: L1 regularization, optimization, coordinate descent, mirror descent, sparsity</p><p>4 0.62459671 <a title="80-lsi-4" href="./jmlr-2011-Super-Linear_Convergence_of_Dual_Augmented_Lagrangian_Algorithm_for_Sparsity_Regularized_Estimation.html">89 jmlr-2011-Super-Linear Convergence of Dual Augmented Lagrangian Algorithm for Sparsity Regularized Estimation</a></p>
<p>Author: Ryota Tomioka, Taiji Suzuki, Masashi Sugiyama</p><p>Abstract: We analyze the convergence behaviour of a recently proposed algorithm for regularized estimation called Dual Augmented Lagrangian (DAL). Our analysis is based on a new interpretation of DAL as a proximal minimization algorithm. We theoretically show under some conditions that DAL converges super-linearly in a non-asymptotic and global sense. Due to a special modelling of sparse estimation problems in the context of machine learning, the assumptions we make are milder and more natural than those made in conventional analysis of augmented Lagrangian algorithms. In addition, the new interpretation enables us to generalize DAL to wide varieties of sparse estimation problems. We experimentally conﬁrm our analysis in a large scale ℓ1 -regularized logistic regression problem and extensively compare the efﬁciency of DAL algorithm to previously proposed algorithms on both synthetic and benchmark data sets. Keywords: dual augmented Lagrangian, proximal minimization, global convergence, sparse estimation, convex optimization</p><p>5 0.50603348 <a title="80-lsi-5" href="./jmlr-2011-A_Family_of_Simple_Non-Parametric_Kernel_Learning_Algorithms.html">4 jmlr-2011-A Family of Simple Non-Parametric Kernel Learning Algorithms</a></p>
<p>Author: Jinfeng Zhuang, Ivor W. Tsang, Steven C.H. Hoi</p><p>Abstract: Previous studies of Non-Parametric Kernel Learning (NPKL) usually formulate the learning task as a Semi-Deﬁnite Programming (SDP) problem that is often solved by some general purpose SDP solvers. However, for N data examples, the time complexity of NPKL using a standard interiorpoint SDP solver could be as high as O(N 6.5 ), which prohibits NPKL methods applicable to real applications, even for data sets of moderate size. In this paper, we present a family of efﬁcient NPKL algorithms, termed “SimpleNPKL”, which can learn non-parametric kernels from a large set of pairwise constraints efﬁciently. In particular, we propose two efﬁcient SimpleNPKL algorithms. One is SimpleNPKL algorithm with linear loss, which enjoys a closed-form solution that can be efﬁciently computed by the Lanczos sparse eigen decomposition technique. Another one is SimpleNPKL algorithm with other loss functions (including square hinge loss, hinge loss, square loss) that can be re-formulated as a saddle-point optimization problem, which can be further resolved by a fast iterative algorithm. In contrast to the previous NPKL approaches, our empirical results show that the proposed new technique, maintaining the same accuracy, is signiﬁcantly more efﬁcient and scalable. Finally, we also demonstrate that the proposed new technique is also applicable to speed up many kernel learning tasks, including colored maximum variance unfolding, minimum volume embedding, and structure preserving embedding. Keywords: non-parametric kernel learning, semi-deﬁnite programming, semi-supervised learning, side information, pairwise constraints</p><p>6 0.46392581 <a title="80-lsi-6" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>7 0.44927028 <a title="80-lsi-7" href="./jmlr-2011-A_Cure_for_Variance_Inflation_in_High_Dimensional_Kernel_Principal_Component_Analysis.html">3 jmlr-2011-A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis</a></p>
<p>8 0.43349341 <a title="80-lsi-8" href="./jmlr-2011-Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization.html">8 jmlr-2011-Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></p>
<p>9 0.38907802 <a title="80-lsi-9" href="./jmlr-2011-Locally_Defined_Principal_Curves_and_Surfaces.html">60 jmlr-2011-Locally Defined Principal Curves and Surfaces</a></p>
<p>10 0.37155202 <a title="80-lsi-10" href="./jmlr-2011-Efficient_Learning_with_Partially_Observed_Attributes.html">29 jmlr-2011-Efficient Learning with Partially Observed Attributes</a></p>
<p>11 0.34793514 <a title="80-lsi-11" href="./jmlr-2011-Double_Updating_Online_Learning.html">28 jmlr-2011-Double Updating Online Learning</a></p>
<p>12 0.31094787 <a title="80-lsi-12" href="./jmlr-2011-Learning_Transformation_Models_for_Ranking_and_Survival_Analysis.html">56 jmlr-2011-Learning Transformation Models for Ranking and Survival Analysis</a></p>
<p>13 0.29275802 <a title="80-lsi-13" href="./jmlr-2011-Better_Algorithms_for_Benign_Bandits.html">14 jmlr-2011-Better Algorithms for Benign Bandits</a></p>
<p>14 0.2776866 <a title="80-lsi-14" href="./jmlr-2011-Online_Learning_in_Case_of_Unbounded_Losses_Using_Follow_the_Perturbed_Leader_Algorithm.html">73 jmlr-2011-Online Learning in Case of Unbounded Losses Using Follow the Perturbed Leader Algorithm</a></p>
<p>15 0.2661199 <a title="80-lsi-15" href="./jmlr-2011-Convergence_of_Distributed_Asynchronous_Learning_Vector_Quantization_Algorithms.html">19 jmlr-2011-Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms</a></p>
<p>16 0.26493254 <a title="80-lsi-16" href="./jmlr-2011-A_Simpler_Approach_to_Matrix_Completion.html">6 jmlr-2011-A Simpler Approach to Matrix Completion</a></p>
<p>17 0.23979324 <a title="80-lsi-17" href="./jmlr-2011-Discriminative_Learning_of_Bayesian_Networks_via_Factorized_Conditional_Log-Likelihood.html">25 jmlr-2011-Discriminative Learning of Bayesian Networks via Factorized Conditional Log-Likelihood</a></p>
<p>18 0.23725623 <a title="80-lsi-18" href="./jmlr-2011-Operator_Norm_Convergence_of_Spectral_Clustering_on_Level_Sets.html">74 jmlr-2011-Operator Norm Convergence of Spectral Clustering on Level Sets</a></p>
<p>19 0.2307947 <a title="80-lsi-19" href="./jmlr-2011-Bayesian_Generalized_Kernel_Mixed_Models.html">13 jmlr-2011-Bayesian Generalized Kernel Mixed Models</a></p>
<p>20 0.22734815 <a title="80-lsi-20" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.036), (9, 0.03), (10, 0.023), (11, 0.449), (24, 0.044), (31, 0.082), (32, 0.03), (41, 0.019), (60, 0.015), (65, 0.011), (70, 0.022), (73, 0.057), (78, 0.079), (90, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86202431 <a title="80-lda-1" href="./jmlr-2011-Improved_Moves_for_Truncated_Convex_Models.html">41 jmlr-2011-Improved Moves for Truncated Convex Models</a></p>
<p>Author: M. Pawan Kumar, Olga Veksler, Philip H.S. Torr</p><p>Abstract: We consider the problem of obtaining an approximate maximum a posteriori estimate of a discrete random ﬁeld characterized by pairwise potentials that form a truncated convex model. For this problem, we propose two st-MINCUT based move making algorithms that we call Range Swap and Range Expansion. Our algorithms can be thought of as extensions of αβ-Swap and α-Expansion respectively that fully exploit the form of the pairwise potentials. Speciﬁcally, instead of dealing with one or two labels at each iteration, our methods explore a large search space by considering a range of labels (that is, an interval of consecutive labels). Furthermore, we show that Range Expansion provides the same multiplicative bounds as the standard linear programming (LP) relaxation in polynomial time. Compared to previous approaches based on the LP relaxation, for example interior-point algorithms or tree-reweighted message passing (TRW), our methods are faster as they use only the efﬁcient st-MINCUT algorithm in their design. We demonstrate the usefulness of the proposed approaches on both synthetic and standard real data problems. Keywords: truncated convex models, move making algorithms, range moves, multiplicative bounds, linear programming relaxation</p><p>same-paper 2 0.74686688 <a title="80-lda-2" href="./jmlr-2011-Regression_on_Fixed-Rank_Positive_Semidefinite_Matrices%3A_A_Riemannian_Approach.html">80 jmlr-2011-Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach</a></p>
<p>Author: Gilles Meyer, Silvère Bonnabel, Rodolphe Sepulchre</p><p>Abstract: The paper addresses the problem of learning a regression model parameterized by a ﬁxed-rank positive semideﬁnite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of ﬁxedrank positive semideﬁnite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semideﬁnite matrix. Good performance is observed on classical benchmarks. Keywords: linear regression, positive semideﬁnite matrices, low-rank approximation, Riemannian geometry, gradient-based learning</p><p>3 0.3231225 <a title="80-lda-3" href="./jmlr-2011-A_Family_of_Simple_Non-Parametric_Kernel_Learning_Algorithms.html">4 jmlr-2011-A Family of Simple Non-Parametric Kernel Learning Algorithms</a></p>
<p>Author: Jinfeng Zhuang, Ivor W. Tsang, Steven C.H. Hoi</p><p>Abstract: Previous studies of Non-Parametric Kernel Learning (NPKL) usually formulate the learning task as a Semi-Deﬁnite Programming (SDP) problem that is often solved by some general purpose SDP solvers. However, for N data examples, the time complexity of NPKL using a standard interiorpoint SDP solver could be as high as O(N 6.5 ), which prohibits NPKL methods applicable to real applications, even for data sets of moderate size. In this paper, we present a family of efﬁcient NPKL algorithms, termed “SimpleNPKL”, which can learn non-parametric kernels from a large set of pairwise constraints efﬁciently. In particular, we propose two efﬁcient SimpleNPKL algorithms. One is SimpleNPKL algorithm with linear loss, which enjoys a closed-form solution that can be efﬁciently computed by the Lanczos sparse eigen decomposition technique. Another one is SimpleNPKL algorithm with other loss functions (including square hinge loss, hinge loss, square loss) that can be re-formulated as a saddle-point optimization problem, which can be further resolved by a fast iterative algorithm. In contrast to the previous NPKL approaches, our empirical results show that the proposed new technique, maintaining the same accuracy, is signiﬁcantly more efﬁcient and scalable. Finally, we also demonstrate that the proposed new technique is also applicable to speed up many kernel learning tasks, including colored maximum variance unfolding, minimum volume embedding, and structure preserving embedding. Keywords: non-parametric kernel learning, semi-deﬁnite programming, semi-supervised learning, side information, pairwise constraints</p><p>4 0.29321888 <a title="80-lda-4" href="./jmlr-2011-The_Sample_Complexity_of_Dictionary_Learning.html">91 jmlr-2011-The Sample Complexity of Dictionary Learning</a></p>
<p>Author: Daniel Vainsencher, Shie Mannor, Alfred M. Bruckstein</p><p>Abstract: A large set of signals can sometimes be described sparsely using a dictionary, that is, every element can be represented as a linear combination of few elements from the dictionary. Algorithms for various signal processing applications, including classiﬁcation, denoising and signal separation, learn a dictionary from a given set of signals to be represented. Can we expect that the error in representing by such a dictionary a previously unseen signal from the same source will be of similar magnitude as those for the given examples? We assume signals are generated from a ﬁxed distribution, and study these questions from a statistical learning theory perspective. We develop generalization bounds on the quality of the learned dictionary for two types of constraints on the coefﬁcient selection, as measured by the expected L2 error in representation when the dictionary is used. For the case of l1 regularized coefﬁcient selection we provide a generalnp ln(mλ)/m , where n is the dimension, p is the number of ization bound of the order of O elements in the dictionary, λ is a bound on the l1 norm of the coefﬁcient vector and m is the number of samples, which complements existing results. For the case of representing a new signal as a combination of at most k dictionary elements, we provide a bound of the order O( np ln(mk)/m) under an assumption on the closeness to orthogonality of the dictionary (low Babel function). We further show that this assumption holds for most dictionaries in high dimensions in a strong probabilistic sense. Our results also include bounds that converge as 1/m, not previously known for this problem. We provide similar results in a general setting using kernels with weak smoothness requirements. Keywords: dictionary learning, generalization bound, sparse representation</p><p>5 0.29156131 <a title="80-lda-5" href="./jmlr-2011-Computationally_Efficient_Convolved_Multiple_Output_Gaussian_Processes.html">17 jmlr-2011-Computationally Efficient Convolved Multiple Output Gaussian Processes</a></p>
<p>Author: Mauricio A. Álvarez, Neil D. Lawrence</p><p>Abstract: Recently there has been an increasing interest in regression methods that deal with multiple outputs. This has been motivated partly by frameworks like multitask learning, multisensor networks or structured output data. From a Gaussian processes perspective, the problem reduces to specifying an appropriate covariance function that, whilst being positive semi-deﬁnite, captures the dependencies between all the data points and across all the outputs. One approach to account for non-trivial correlations between outputs employs convolution processes. Under a latent function interpretation of the convolution transform we establish dependencies between output variables. The main drawbacks of this approach are the associated computational and storage demands. In this paper we address these issues. We present different efﬁcient approximations for dependent output Gaussian processes constructed through the convolution formalism. We exploit the conditional independencies present naturally in the model. This leads to a form of the covariance similar in spirit to the so called PITC and FITC approximations for a single output. We show experimental results with synthetic and real data, in particular, we show results in school exams score prediction, pollution prediction and gene expression data. Keywords: Gaussian processes, convolution processes, efﬁcient approximations, multitask learning, structured outputs, multivariate processes</p><p>6 0.29154843 <a title="80-lda-6" href="./jmlr-2011-Operator_Norm_Convergence_of_Spectral_Clustering_on_Level_Sets.html">74 jmlr-2011-Operator Norm Convergence of Spectral Clustering on Level Sets</a></p>
<p>7 0.29023892 <a title="80-lda-7" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>8 0.28992495 <a title="80-lda-8" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<p>9 0.2887527 <a title="80-lda-9" href="./jmlr-2011-Forest_Density_Estimation.html">35 jmlr-2011-Forest Density Estimation</a></p>
<p>10 0.28780675 <a title="80-lda-10" href="./jmlr-2011-Robust_Gaussian_Process_Regression_with_a_Student-tLikelihood.html">82 jmlr-2011-Robust Gaussian Process Regression with a Student-tLikelihood</a></p>
<p>11 0.28641391 <a title="80-lda-11" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>12 0.28460467 <a title="80-lda-12" href="./jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</a></p>
<p>13 0.28245607 <a title="80-lda-13" href="./jmlr-2011-Efficient_Learning_with_Partially_Observed_Attributes.html">29 jmlr-2011-Efficient Learning with Partially Observed Attributes</a></p>
<p>14 0.28204697 <a title="80-lda-14" href="./jmlr-2011-Bayesian_Generalized_Kernel_Mixed_Models.html">13 jmlr-2011-Bayesian Generalized Kernel Mixed Models</a></p>
<p>15 0.28193218 <a title="80-lda-15" href="./jmlr-2011-Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization.html">8 jmlr-2011-Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></p>
<p>16 0.28138229 <a title="80-lda-16" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>17 0.28082141 <a title="80-lda-17" href="./jmlr-2011-Information%2C_Divergence_and_Risk_for_Binary_Experiments.html">43 jmlr-2011-Information, Divergence and Risk for Binary Experiments</a></p>
<p>18 0.28067055 <a title="80-lda-18" href="./jmlr-2011-An_Asymptotic_Behaviour_of_the_Marginal_Likelihood_for_General_Markov_Models.html">9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</a></p>
<p>19 0.28064641 <a title="80-lda-19" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>20 0.2805073 <a title="80-lda-20" href="./jmlr-2011-Convex_and_Network_Flow_Optimization_for_Structured_Sparsity.html">20 jmlr-2011-Convex and Network Flow Optimization for Structured Sparsity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
