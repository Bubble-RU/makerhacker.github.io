<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 jmlr-2011-Faster Algorithms for Max-Product Message-Passing</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-34" href="#">jmlr2011-34</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>34 jmlr-2011-Faster Algorithms for Max-Product Message-Passing</h1>
<br/><p>Source: <a title="jmlr-2011-34-pdf" href="http://jmlr.org/papers/volume12/mcauley11a/mcauley11a.pdf">pdf</a></p><p>Author: Julian J. McAuley, TibĂŠrio S. Caetano</p><p>Abstract: Maximum A Posteriori inference in graphical models is often solved via message-passing algorithms, such as the junction-tree algorithm or loopy belief-propagation. The exact solution to this problem is well-known to be exponential in the size of the maximal cliques of the triangulated model, while approximate inference is typically exponential in the size of the modelĂ˘&euro;&trade;s factors. In this paper, we take advantage of the fact that many models have maximal cliques that are larger than their constituent factors, and also of the fact that many factors consist only of latent variables (i.e., they do not depend on an observation). This is a common case in a wide variety of applications that deal with grid-, tree-, and ring-structured models. In such cases, we are able to decrease the exponent of complexity for message-passing by 0.5 for both exact and approximate inference. We demonstrate that message-passing operations in such models are equivalent to some variant of matrix multiplication in the tropical semiring, for which we offer an O(N 2.5 ) expected-case solution. Keywords: graphical models, belief-propagation, tropical matrix multiplication</p><p>Reference: <a title="jmlr-2011-34-reference" href="../jmlr2011_reference/jmlr-2011-Faster_Algorithms_for_Max-Product_Message-Passing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The exact solution to this problem is well-known to be exponential in the size of the maximal cliques of the triangulated model, while approximate inference is typically exponential in the size of the modelĂ˘&euro;&trade;s factors. [sent-10, score-0.508]
</p><p>2 In this paper, we take advantage of the fact that many models have maximal cliques that are larger than their constituent factors, and also of the fact that many factors consist only of latent variables (i. [sent-11, score-0.559]
</p><p>3 Keywords: graphical models, belief-propagation, tropical matrix multiplication  1. [sent-19, score-0.22]
</p><p>4 Ĺš ciency is determined by the size of the maximal cliques after triangulation, a quantity related to the tree-width of the graph. [sent-24, score-0.4]
</p><p>5 Despite the fact that the new models have larger maximal cliques, the corresponding potentials are still factored over pairs of nodes only. [sent-37, score-0.356]
</p><p>6 (a)  (b)  (c)  (d)  Figure 2: Some graphical models to which our results apply: factors conditioned upon observations have fewer latent variables than purely latent factors. [sent-39, score-0.318]
</p><p>7 In other words, factors containing a gray node encode the data likelihood, whereas factors containing only white nodes encode priors. [sent-41, score-0.301]
</p><p>8 Hence approximate solutions in the original graph (such as loopy belief-propagation, or inference in a loopy factor-graph) are often preferred over an exact solution via the junction-tree algorithm. [sent-44, score-0.289]
</p><p>9 Even when the modelĂ˘&euro;&trade;s factors are the same size as its maximal cliques, neither exact nor approximate inference algorithms take advantage of the fact that many factors consist only of latent variables. [sent-45, score-0.351]
</p><p>10 In many models, those factors that are conditioned upon the observation contain fewer latent variables than the purely latent factors. [sent-46, score-0.28]
</p><p>11 In this paper, we exploit the fact that the maximal cliques (after triangulation) often have potentials that factor over subcliques, as illustrated in Figure 1. [sent-50, score-0.663]
</p><p>12 We will show that whenever this is the case, the expected computational complexity of message-passing between such cliques can be improved (both the asymptotic upper-bound and the actual runtime). [sent-51, score-0.358]
</p><p>13 A core operation encountered in the junction-tree algorithm is that of computing the innerproduct of two vectors va and vb . [sent-56, score-0.588]
</p><p>14 In the max-product semiring (used for MAP inference), the Ă˘&euro;&tilde;inner-productĂ˘&euro;&trade; becomes max {va [i] Ä&sbquo;&mdash; vb [i]} . [sent-57, score-0.335]
</p><p>15 N}  Our results stem from theĂ˘&circ;&scaron; realization that while (Equation 1) appears to be a linear time operation, it can be decreased to O( N) (in the expected case) if we know the permutations that sort va and vb (i. [sent-61, score-0.688]
</p><p>16 Ă˘&euro;Ë&tilde; We are able to lower the asymptotic expected running time of max-product message-passing for any discrete graphical model whose cliques factorize into lower-order terms. [sent-70, score-0.551]
</p><p>17 Ă˘&euro;Ë&tilde; Our algorithm also applies whenever factors that are conditioned upon an observation contain fewer latent variables than those factors that are not conditioned upon an observation, as in Figure 2 (in which case certain computations can be taken ofÄ? [sent-72, score-0.325]
</p><p>18 For example, in models with third-order cliques containing pairwise terms, message-passing is reduced Ă˘&circ;&scaron; from Ă&#x17D;&tilde;(N 3 ) to O(N 2 N), as in Figure 1(d). [sent-75, score-0.421]
</p><p>19 1  1 Ă˘&euro;Ë&tilde; For cliques composed of K-ary factors, the expected speed-up generalizes to at least Ă˘&bdquo;Ĺ&scaron;( K N K ), though it is never asymptotically slower than the original solution. [sent-77, score-0.358]
</p><p>20 We shall initially present our algorithm in terms of pairwise graphical models such as those shown in Figure 2. [sent-87, score-0.22]
</p><p>21 Finally we shall explore other applications besides message-passing that make use of tropical matrix multiplication as a subroutine, such all-pairs shortest-path problems. [sent-90, score-0.301]
</p><p>22 (2009) study the case where different cliques share the same potential function. [sent-94, score-0.358]
</p><p>23 In Kumar and Torr (2006), the authors provide faster algorithms for the case in which the potentials are truncated, whereas in Petersen et al. [sent-96, score-0.308]
</p><p>24 KjÄ&sbquo;Ĺ&scaron;rulff (1998) also exploits factorization within cliques of junction-trees, albeit a different type of factorization than that studied here. [sent-105, score-0.358]
</p><p>25 Ă&#x17D;Ĺ&scaron;C (xC ), x  CĂ˘&circ;&circ;C  where C is the set of maximal cliques in G . [sent-118, score-0.4]
</p><p>26 In each case, the triangulated model has third-order cliques, but the potentials are only pairwise. [sent-124, score-0.327]
</p><p>27 Ă&#x17D;Ĺ&scaron;Q (xQ |y)  FĂ˘&Scaron;&dagger;C  ,  QĂ˘&Scaron;&dagger;C  data-independent  data-dependent  We shall say that those factors that are not conditioned on the observation are Ă˘&euro;&tilde;data-independentĂ˘&euro;&trade;. [sent-132, score-0.225]
</p><p>28 Our results shall apply to message-passing equations in those cliques C where for each dataindependent factor F we have F Ă˘&Scaron;&sbquo; C, or for each data-dependent factor Q we have Q Ă˘&Scaron;&sbquo; C, that is, when all F or all Q in C are proper subsets of C. [sent-133, score-0.477]
</p><p>29 The message from a clique X to an intersecting clique Y (both sets of latent variables) is deÄ? [sent-137, score-0.249]
</p><p>30 mZĂ˘&dagger;&rsquo;X (xXĂ˘&circ;Ĺ Z )  (4)  ZĂ˘&circ;&circ;Ă&#x17D;&ldquo;(X)\Y  (where Ă&#x17D;&ldquo;(X) is the set of neighbors of the clique X, that is, the set of cliques that intersect with X). [sent-139, score-0.456]
</p><p>31 After all messages have been passed, the MAP-state for a set of latent variables M (assumed to be a subset of a single clique X) is computed using mM (xM ) = max Ă&#x17D;Ĺ&scaron;X (xX ) xX\M  Ă˘&circ;? [sent-145, score-0.204]
</p><p>32 (5)  ZĂ˘&circ;&circ;Ă&#x17D;&ldquo;(X)  For cliques that are factorizable (according to our previous deÄ? [sent-147, score-0.522]
</p><p>33 Although the (triangulated) models have cliques of size three, their potentials factorize into pairwise terms. [sent-152, score-0.729]
</p><p>34 Ĺš rst, in Section 3, we shall consider cliques X whose messages take the form mM (xM ) = max Ă&#x17D;Ĺ&scaron;X (xX ) xX\M  Ă˘&circ;? [sent-158, score-0.53]
</p><p>35 QĂ˘&Scaron;&sbquo;X  We say that such cliques are conditionally factorizable (since all conditional terms factorize); examples are shown in Figure 2. [sent-160, score-0.551]
</p><p>36 Next, in Section 4, we consider cliques whose messages take the form mM (xM ) = max Ă˘&circ;? [sent-161, score-0.411]
</p><p>37 xX\M  FĂ˘&Scaron;&sbquo;X  We say that such cliques are latently factorizable (since terms containing only latent variables factorize); examples are shown in Figure 1. [sent-163, score-0.648]
</p><p>38 Ĺš cient version of Algorithm 1, we begin by considering the simplest nontrivial conditionally factorizable model: a pairwise model in which each latent variable depends upon the observation, that is, Ă&lsaquo;&dagger; x(y) = argmax Ă˘&circ;? [sent-166, score-0.339]
</p><p>39 Ĺš nitions, we say that the node potentials are Ă˘&euro;&tilde;data-dependentĂ˘&euro;&trade;, whereas the edge potentials are Ă˘&euro;&tilde;data-independentĂ˘&euro;&trade;. [sent-171, score-0.564]
</p><p>40 In many models, such as grids and rings, (Equation 7) shall be solved approximately by means of either loopy belief-propagation, or inference in a factor-graph, which consists of solving (Equation 8) according to protocols other than the optimal junction-tree protocol. [sent-184, score-0.268]
</p><p>41 Ĺš ciently if we know the order statistics of va and vb , that is, if we know the permutations that sort Ă&#x17D;Ĺ&scaron; j and every row of Ă&#x17D;Ĺ&scaron;i, j in (Equation 8). [sent-188, score-0.688]
</p><p>42 N} {va [i] Ä&sbquo;&mdash; vb [i]} must have va [p] Ă˘&permil;Ä˝ va [q] or vb [p] Ă˘&permil;Ä˝ vb [q]. [sent-196, score-1.429]
</p><p>43 Therefore, having computed va [q] Ä&sbquo;&mdash; vb [q], we can Ä? [sent-197, score-0.588]
</p><p>44 Ĺš nd Ă˘&euro;&tilde;pĂ˘&euro;&trade; by computing only those products va [i] Ä&sbquo;&mdash; vb [i] where either va [i] > va [q] or vb [i] > vb [q]. [sent-198, score-1.764]
</p><p>45 Here we iterate through the indices starting from the largest values of va and vb , stopping once both indices are Ă˘&euro;&tilde;behindĂ˘&euro;&trade; the maximum value found so far (which we then know is the maximum). [sent-200, score-0.588]
</p><p>46 Note that Lemma 1 only depends upon the relative values of elements in va and vb , meaning that the number of computations that must be performed is purely a function of their order statistics (i. [sent-202, score-0.699]
</p><p>47 , it does not depend on the actual values of va or vb ). [sent-204, score-0.588]
</p><p>48 At this stage we shall state an upper-bound on the true complexity in the following theorem: Ă˘&circ;&scaron; Theorem 2 The expected running time of Algorithm 2 is O( N), yielding a speed-up of at least Ă˘&circ;&scaron; Ă˘&bdquo;Ĺ&scaron;( N) in cliques containing pairwise factors. [sent-207, score-0.65]
</p><p>49 This expectation is derived under the assumption that va and vb have independent order statistics. [sent-208, score-0.588]
</p><p>50 The arrows begin at pa [start] and pb [start]; the red dashed line connects enda and endb , behind which we need not search; a dashed arrow is used when a new maximum is found. [sent-290, score-0.206]
</p><p>51 Note that in the event that va and vb contain repeated elements, they can be sorted arbitrarily. [sent-291, score-0.645]
</p><p>52 1: compute the permutation function pa by sorting Ă&#x17D;Â¨ j {takes Ă&#x17D;&tilde;(N log N)} 2: for q Ă˘&circ;&circ; {1 . [sent-295, score-0.254]
</p><p>53 Latently Factorizable Models Just as we considered the simplest conditionally factorizable model in Section 3, we now consider the simplest nontrivial latently factorizable model: a clique of size three containing pairwise factors. [sent-304, score-0.591]
</p><p>54 xk  For a particular value of (xi , x j ) = (a, b), we must solve mi, j (a, b) = Ă&#x17D;Ĺ&scaron;i, j (a, b) Ä&sbquo;&mdash; max Ă&#x17D;Ĺ&scaron;i,k (a, xk ) Ä&sbquo;&mdash; Ă&#x17D;Ĺ&scaron; j,k (b, xk ), xk  va  (11)  vb  which again is in precisely the form shown in (Equation 1). [sent-306, score-0.824]
</p><p>55 Extensions So far we have only considered the case of pairwise graphical models, though as mentioned our results can in principle be applied to any conditionally or latently factorizable models, no matter the size of the factors. [sent-323, score-0.367]
</p><p>56 Ĺš rst treat latently factorizable models, after which the same ideas can be applied to conditionally factorizable models. [sent-326, score-0.43]
</p><p>57 be adapted to solve mi, j (xi , x j ) = max Ă&#x17D;Ĺ&scaron;i, j (xi , x j ) Ä&sbquo;&mdash; Ă&#x17D;Ĺ&scaron;i,k,m (xi , xk , xm ) Ä&sbquo;&mdash; Ă&#x17D;Ĺ&scaron; j,k,m (x j , xk , xm ), xk ,xm  (12)  and similar variants containing three factors. [sent-345, score-0.281]
</p><p>58 Ĺš xed value of xi , we are now sorting an array rather than a vector (Algorithm 4, lines 2 and 3); in this case, the permutation functions pa and pb in Algorithm 2 simply return pairs of indices. [sent-348, score-0.4]
</p><p>59 For example, consider the clique in Figure 6(a), which we shall call G (the entire graph is a clique, but for clarity we only draw an edge when the corresponding nodes belong to a common factor). [sent-356, score-0.303]
</p><p>60 Each of the factors in this graph have been labeled using either differently colored edges (for factors of size larger than two) or dotted edges (for factors of size two), and the max-marginal we wish to compute has been labeled using colored nodes. [sent-357, score-0.458]
</p><p>61 In order to simplify the analysis of this algorithm, we shall express the running time in terms of the size of the largest group, S = max(|X|, |Y |, |Z|), and the largest difference, S\ = max(|Y \ X|, |Z \ X|). [sent-363, score-0.229]
</p><p>62 The running times shown in Algorithm 5 are loose upper-bounds, given for the sake of expressing the running time in simple terms. [sent-365, score-0.22]
</p><p>63 Again, we shall discuss the running time of this extension in Appendix A. [sent-439, score-0.229]
</p><p>64 For the moment, we state the following theorem: KĂ˘&circ;&rsquo;1  Theorem 3 Algorithm 6 generalizes Algorithm 2 to K lists with an expected running time of O(KN K ), 1 1 yielding a speed-up of at least Ă˘&bdquo;Ĺ&scaron;( K N K ) in cliques containing K-ary factors. [sent-440, score-0.551]
</p><p>65 2, we can extend Algorithm 3 to factors of any size, so long as the purely latent cliques contain more latent variables than those cliques that depend upon the observation. [sent-485, score-0.996]
</p><p>66 As we have mentioned, our results apply to any model whose cliques decompose into lower-order terms. [sent-497, score-0.358]
</p><p>67 In each case, third-order cliques factorize Ă˘&circ;&scaron; into second-order terms; hence we can apply Algorithm 4 to achieve a speed-up of Ă˘&bdquo;Ĺ&scaron;( N). [sent-507, score-0.403]
</p><p>68 Maximal cliques grow with the 1367  M C AULEY AND C AETANO  Reference McAuley et al. [sent-522, score-0.358]
</p><p>69 K is the number of lists in (Equation 14); we can observe this number of lists only if we are working in cliques of size K + 1, and then only if the factors are of size K (e. [sent-556, score-0.63]
</p><p>70 The performance reported is simply the number of elements read from the lists (which is at most K Ä&sbquo;&mdash; start). [sent-567, score-0.225]
</p><p>71 Figure 11 shows how the order statistics of va and vb can affect the performance of our algorithm. [sent-578, score-0.588]
</p><p>72 , for Algorithm 2), where each (va [i], vb [i]) is an independent sample from a 2-dimensional Gaussian with covariance matrix Ă&#x17D;Ĺ =  1 c c 1 1369  ,  M C AULEY AND C AETANO  Ă˘&dagger;? [sent-583, score-0.253]
</p><p>73 Each permutation matrix transforms the sorted values of one list into the sorted values of the other, that is, it transforms va as sorted by pa into vb as sorted by pb . [sent-585, score-1.083]
</p><p>74 3 Message-Passing in Latently Factorizable Models In this section we present experiments in models whose cliques factorize into smaller terms, as discussed in Section 4. [sent-594, score-0.403]
</p><p>75 (2008), which performs 2-dimensional graph-matching, using a loopy graph with cliques of size three, containing only second-order potentials (as described in Section 6); the Ă&#x17D;&tilde;(NM 3 ) performance of McAuley et al. [sent-601, score-0.761]
</p><p>76 For instance 1371  M C AULEY AND C AETANO  Random potentials (5 iterations)  450  naĂ&sbquo;Â¨ve method Ă&bdquo;Ä&hellip;  400  naĂ&sbquo;Â¨ve method Ă&bdquo;Ä&hellip;  0. [sent-643, score-0.263]
</p><p>77 Ĺš ning the node potentials Ă&#x17D;Ĺ&scaron;i (xi |yi ), the edge potentials Ă&#x17D;Ĺ&scaron;i, j (xi , x j ), and the topology (N , E ) of the graph. [sent-691, score-0.564]
</p><p>78 Furthermore we assume that the edge potentials are homogeneous, that is, that the potential for each edge is the same, or rather that they have the same order statistics (for example, they may differ by a multiplicative constant). [sent-692, score-0.263]
</p><p>79 When subject to heterogeneous potentials we need merely sort them ofÄ? [sent-694, score-0.297]
</p><p>80 1374  FASTER A LGORITHMS FOR M AX -P RODUCT M ESSAGE -PASSING  Random potentials (2500 node chain)  4. [sent-749, score-0.301]
</p><p>81 00 75  90 105 120 135 150  Korean  10 0  100  200 300 N (number of states)  400  500  0  0  500  1000 1500 N (alphabet size)  2000  Figure 17: Running time of inference in chain-structured models: random potentials (left), and text denoising (right). [sent-773, score-0.38]
</p><p>82 Figure 18 (left) shows the performance of our method on a grid with random potentials (similar to the experiment in Section 7. [sent-778, score-0.263]
</p><p>83 The pairwise potentials simply encode the Euclidean distance between two Ä? [sent-790, score-0.326]
</p><p>84 Ĺš&sbquo;ow) are studied in Felzenszwalb and Huttenlocher (2006), where the highly structured nature of the potentials in question often allows for efÄ? [sent-793, score-0.263]
</p><p>85 1375  M C AULEY AND C AETANO  Random potentials (50 Ä&sbquo;&mdash; 50 grid, 5 iterations)  90  naĂ&sbquo;Â¨ve method Ă&bdquo;Ä&hellip;  80  naĂ&sbquo;Â¨ve method Ă&bdquo;Ä&hellip;  0. [sent-806, score-0.263]
</p><p>86 76) 60  40  20  10 0  0  100  200 300 N (number of states)  400  500  0  0  100  200 300 N (number of states)  400  500  Figure 18: Running time of inference in grid-structured models: random potentials (left), and optical Ä? [sent-817, score-0.347]
</p><p>87 This behavior is observed for certain types of concave potentials (or convex potentials in a min-sum formulation). [sent-820, score-0.526]
</p><p>88 In these applications, the permutation matrices that transform the sorted values of va to the sorted values of vb are block-off-diagonal (see the sixth permutation in Figure 11). [sent-823, score-0.898]
</p><p>89 We similarly perform an experiment on image denoising, where the unary potentials are again convex functions of the input (see Geman and Geman, 1984; Lan et al. [sent-828, score-0.263]
</p><p>90 Instead of using a pairwise potential that merely encodes smoothness, we extract the pairwise statistics from image data (similar to our experiment on text denoising); thus the potentials are no longer concave. [sent-830, score-0.419]
</p><p>91 63) our method  80  0  100  200 300 N (number of states)  400  500  0  0  100  200 300 N (number of states)  400  500  Figure 19: Two experiments whose potentials and messages have highly dependent order statistics: stereo disparity (left), and image denoising (right). [sent-844, score-0.449]
</p><p>92 Namely, we terminate the algorithm if va [pa [start]] Ä&sbquo;&mdash; vb [pb [start]] < max. [sent-878, score-0.588]
</p><p>93 Ĺš cant variation in the variable size (note that N only shows the average variable size), but it may also suggest that there is a complicated structure in the potentials which violates our assumption of independent order statistics. [sent-888, score-0.263]
</p><p>94 Given the strong relationship between the two problems, it remains a promising open problem to assess whether the analysis from these solutions to all-pairs shortest-path can be applied to produce max-product matrix multiplication algorithms with similar asymptotic running times. [sent-943, score-0.241]
</p><p>95 3 LĂ˘&circ;&#x17E; D ISTANCES The problem of computing an inner product in the max-sum semiring is closely related to computing the LĂ˘&circ;&#x17E; distance between two vectors ||va Ă˘&circ;&rsquo; vb ||Ă˘&circ;&#x17E; = max va [i] Ă˘&circ;&rsquo; vb [i] . [sent-955, score-0.923]
</p><p>96 Ĺš rst considering the largest values of va and Ă˘&circ;&rsquo;vb , before re-running the algorithm starting from the smallest values. [sent-961, score-0.335]
</p><p>97 A similar trick can be applied to compute message in the max-product semiring even for potentials that contain negative values, though this may require up to four executions of Algorithm 2, so it is unlikely to be practical. [sent-969, score-0.345]
</p><p>98 In factors with a high dynamic range, or when different factors have different scales, it may be possible to identify the maximum value very quickly, as we attempted to do in Section 7. [sent-1015, score-0.212]
</p><p>99 Asymptotic Performance of Algorithm 2 and Extensions In this section we shall determine the expected-case running times of Algorithm 2 and Algorithm 6. [sent-1028, score-0.229]
</p><p>100 Algorithm 2 traverses va and vb until it reaches the smallest value of m for which there is some j Ă˘&permil;Â¤ m for which m Ă˘&permil;Ä˝ pĂ˘&circ;&rsquo;1 [pa [ j]]. [sent-1029, score-0.588]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cliques', 0.358), ('va', 0.335), ('potentials', 0.263), ('vb', 0.253), ('aetano', 0.183), ('auley', 0.183), ('roduct', 0.183), ('essage', 0.174), ('factorizable', 0.164), ('read', 0.142), ('mcauley', 0.137), ('na', 0.134), ('multiplication', 0.131), ('shall', 0.119), ('running', 0.11), ('mn', 0.107), ('factors', 0.106), ('loopy', 0.105), ('pb', 0.102), ('permutation', 0.098), ('clique', 0.098), ('karger', 0.091), ('xx', 0.091), ('sorting', 0.089), ('ax', 0.085), ('dom', 0.084), ('lists', 0.083), ('semiring', 0.082), ('wall', 0.08), ('sontag', 0.077), ('deformable', 0.073), ('latently', 0.073), ('lgorithms', 0.073), ('pa', 0.067), ('ve', 0.066), ('permutations', 0.066), ('subcubic', 0.064), ('triangulated', 0.064), ('pairwise', 0.063), ('stereo', 0.062), ('xk', 0.059), ('sorted', 0.057), ('felzenszwalb', 0.056), ('tib', 0.055), ('julian', 0.054), ('messages', 0.053), ('latent', 0.053), ('xm', 0.052), ('ow', 0.051), ('tropical', 0.051), ('nodes', 0.051), ('xe', 0.048), ('loop', 0.047), ('aho', 0.047), ('rio', 0.047), ('coughlan', 0.046), ('sigal', 0.046), ('xy', 0.045), ('faster', 0.045), ('factorize', 0.045), ('array', 0.044), ('tted', 0.044), ('inference', 0.044), ('meaning', 0.043), ('denoising', 0.043), ('caetano', 0.042), ('maximal', 0.042), ('mm', 0.04), ('optical', 0.04), ('xz', 0.039), ('xf', 0.039), ('ine', 0.039), ('colored', 0.038), ('start', 0.038), ('purely', 0.038), ('node', 0.038), ('graphical', 0.038), ('aji', 0.037), ('endb', 0.037), ('ferreira', 0.037), ('galley', 0.037), ('equation', 0.036), ('pk', 0.035), ('vk', 0.035), ('marginalization', 0.035), ('graph', 0.035), ('seconds', 0.035), ('huttenlocher', 0.035), ('takes', 0.035), ('groups', 0.035), ('sort', 0.034), ('alon', 0.031), ('fitted', 0.031), ('nicta', 0.031), ('text', 0.03), ('upon', 0.03), ('wish', 0.029), ('conditionally', 0.029), ('xc', 0.029), ('disparity', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="34-tfidf-1" href="./jmlr-2011-Faster_Algorithms_for_Max-Product_Message-Passing.html">34 jmlr-2011-Faster Algorithms for Max-Product Message-Passing</a></p>
<p>Author: Julian J. McAuley, TibĂŠrio S. Caetano</p><p>Abstract: Maximum A Posteriori inference in graphical models is often solved via message-passing algorithms, such as the junction-tree algorithm or loopy belief-propagation. The exact solution to this problem is well-known to be exponential in the size of the maximal cliques of the triangulated model, while approximate inference is typically exponential in the size of the modelĂ˘&euro;&trade;s factors. In this paper, we take advantage of the fact that many models have maximal cliques that are larger than their constituent factors, and also of the fact that many factors consist only of latent variables (i.e., they do not depend on an observation). This is a common case in a wide variety of applications that deal with grid-, tree-, and ring-structured models. In such cases, we are able to decrease the exponent of complexity for message-passing by 0.5 for both exact and approximate inference. We demonstrate that message-passing operations in such models are equivalent to some variant of matrix multiplication in the tropical semiring, for which we offer an O(N 2.5 ) expected-case solution. Keywords: graphical models, belief-propagation, tropical matrix multiplication</p><p>2 0.14494702 <a title="34-tfidf-2" href="./jmlr-2011-Improved_Moves_for_Truncated_Convex_Models.html">41 jmlr-2011-Improved Moves for Truncated Convex Models</a></p>
<p>Author: M. Pawan Kumar, Olga Veksler, Philip H.S. Torr</p><p>Abstract: We consider the problem of obtaining an approximate maximum a posteriori estimate of a discrete random ﬁeld characterized by pairwise potentials that form a truncated convex model. For this problem, we propose two st-MINCUT based move making algorithms that we call Range Swap and Range Expansion. Our algorithms can be thought of as extensions of αβ-Swap and α-Expansion respectively that fully exploit the form of the pairwise potentials. Speciﬁcally, instead of dealing with one or two labels at each iteration, our methods explore a large search space by considering a range of labels (that is, an interval of consecutive labels). Furthermore, we show that Range Expansion provides the same multiplicative bounds as the standard linear programming (LP) relaxation in polynomial time. Compared to previous approaches based on the LP relaxation, for example interior-point algorithms or tree-reweighted message passing (TRW), our methods are faster as they use only the efﬁcient st-MINCUT algorithm in their design. We demonstrate the usefulness of the proposed approaches on both synthetic and standard real data problems. Keywords: truncated convex models, move making algorithms, range moves, multiplicative bounds, linear programming relaxation</p><p>3 0.072354361 <a title="34-tfidf-3" href="./jmlr-2011-Adaptive_Exact_Inference_in_Graphical_Models.html">7 jmlr-2011-Adaptive Exact Inference in Graphical Models</a></p>
<p>Author: Özgür Sümer, Umut A. Acar, Alexander T. Ihler, Ramgopal R. Mettu</p><p>Abstract: Many algorithms and applications involve repeatedly solving variations of the same inference problem, for example to introduce new evidence to the model or to change conditional dependencies. As the model is updated, the goal of adaptive inference is to take advantage of previously computed quantities to perform inference more rapidly than from scratch. In this paper, we present algorithms for adaptive exact inference on general graphs that can be used to efﬁciently compute marginals and update MAP conﬁgurations under arbitrary changes to the input factor graph and its associated elimination tree. After a linear time preprocessing step, our approach enables updates to the model and the computation of any marginal in time that is logarithmic in the size of the input model. Moreover, in contrast to max-product our approach can also be used to update MAP conﬁgurations in time that is roughly proportional to the number of updated entries, rather than the size of the input model. To evaluate the practical effectiveness of our algorithms, we implement and test them using synthetic data as well as for two real-world computational biology applications. Our experiments show that adaptive inference can achieve substantial speedups over performing complete inference as the model undergoes small changes over time. Keywords: exact inference, factor graphs, factor elimination, marginalization, dynamic programming, MAP computation, model updates, parallel tree contraction ¨ u u c 2011 Ozg¨ r S¨ mer, Umut A. Acar, Alexander T. Ihler and Ramgopal R. Mettu. ¨ S UMER , ACAR , I HLER AND M ETTU</p><p>4 0.069298014 <a title="34-tfidf-4" href="./jmlr-2011-Logistic_Stick-Breaking_Process.html">61 jmlr-2011-Logistic Stick-Breaking Process</a></p>
<p>Author: Lu Ren, Lan Du, Lawrence Carin, David Dunson</p><p>Abstract: A logistic stick-breaking process (LSBP) is proposed for non-parametric clustering of general spatially- or temporally-dependent data, imposing the belief that proximate data are more likely to be clustered together. The sticks in the LSBP are realized via multiple logistic regression functions, with shrinkage priors employed to favor contiguous and spatially localized segments. The LSBP is also extended for the simultaneous processing of multiple data sets, yielding a hierarchical logistic stick-breaking process (H-LSBP). The model parameters (atoms) within the H-LSBP are shared across the multiple learning tasks. Efﬁcient variational Bayesian inference is derived, and comparisons are made to related techniques in the literature. Experimental analysis is performed for audio waveforms and images, and it is demonstrated that for segmentation applications the LSBP yields generally homogeneous segments with sharp boundaries. Keywords: Bayesian, nonparametric, dependent, hierarchical models, segmentation</p><p>5 0.055166766 <a title="34-tfidf-5" href="./jmlr-2011-Weisfeiler-Lehman_Graph_Kernels.html">103 jmlr-2011-Weisfeiler-Lehman Graph Kernels</a></p>
<p>Author: Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, Karsten M. Borgwardt</p><p>Abstract: In this article, we propose a family of efﬁcient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be deﬁned based on this Weisfeiler-Lehman sequence of graphs, including a highly efﬁcient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classiﬁcation benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis. Keywords: graph kernels, graph classiﬁcation, similarity measures for graphs, Weisfeiler-Lehman algorithm c 2011 Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn and Karsten M. Borgwardt. S HERVASHIDZE , S CHWEITZER , VAN L EEUWEN , M EHLHORN AND B ORGWARDT</p><p>6 0.053476024 <a title="34-tfidf-6" href="./jmlr-2011-Learning_Latent_Tree_Graphical_Models.html">54 jmlr-2011-Learning Latent Tree Graphical Models</a></p>
<p>7 0.052506309 <a title="34-tfidf-7" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>8 0.051279098 <a title="34-tfidf-8" href="./jmlr-2011-Cumulative_Distribution_Networks_and_the_Derivative-sum-product_Algorithm%3A_Models_and_Inference_for_Cumulative_Distribution_Functions_on_Graphs.html">21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</a></p>
<p>9 0.051169764 <a title="34-tfidf-9" href="./jmlr-2011-Theoretical_Analysis_of_Bayesian_Matrix_Factorization.html">94 jmlr-2011-Theoretical Analysis of Bayesian Matrix Factorization</a></p>
<p>10 0.045623854 <a title="34-tfidf-10" href="./jmlr-2011-Smoothness%2C_Disagreement_Coefficient%2C_and_the_Label_Complexity_of_Agnostic_Active_Learning.html">85 jmlr-2011-Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning</a></p>
<p>11 0.045461643 <a title="34-tfidf-11" href="./jmlr-2011-Robust_Gaussian_Process_Regression_with_a_Student-tLikelihood.html">82 jmlr-2011-Robust Gaussian Process Regression with a Student-tLikelihood</a></p>
<p>12 0.044859927 <a title="34-tfidf-12" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>13 0.041656815 <a title="34-tfidf-13" href="./jmlr-2011-Convex_and_Network_Flow_Optimization_for_Structured_Sparsity.html">20 jmlr-2011-Convex and Network Flow Optimization for Structured Sparsity</a></p>
<p>14 0.040901881 <a title="34-tfidf-14" href="./jmlr-2011-Parallel_Algorithm_for_Learning_Optimal_Bayesian_Network_Structure.html">75 jmlr-2011-Parallel Algorithm for Learning Optimal Bayesian Network Structure</a></p>
<p>15 0.037747644 <a title="34-tfidf-15" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>16 0.036855329 <a title="34-tfidf-16" href="./jmlr-2011-The_Indian_Buffet_Process%3A_An_Introduction_and_Review.html">90 jmlr-2011-The Indian Buffet Process: An Introduction and Review</a></p>
<p>17 0.035050552 <a title="34-tfidf-17" href="./jmlr-2011-Kernel_Regression_in_the_Presence_of_Correlated_Errors.html">49 jmlr-2011-Kernel Regression in the Presence of Correlated Errors</a></p>
<p>18 0.034553315 <a title="34-tfidf-18" href="./jmlr-2011-Proximal_Methods_for_Hierarchical_Sparse_Coding.html">79 jmlr-2011-Proximal Methods for Hierarchical Sparse Coding</a></p>
<p>19 0.03355867 <a title="34-tfidf-19" href="./jmlr-2011-Forest_Density_Estimation.html">35 jmlr-2011-Forest Density Estimation</a></p>
<p>20 0.032031573 <a title="34-tfidf-20" href="./jmlr-2011-Approximate_Marginals_in_Latent_Gaussian_Models.html">11 jmlr-2011-Approximate Marginals in Latent Gaussian Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.184), (1, -0.076), (2, -0.037), (3, 0.014), (4, 0.014), (5, 0.015), (6, 0.003), (7, 0.056), (8, 0.097), (9, 0.069), (10, -0.177), (11, 0.15), (12, 0.268), (13, -0.079), (14, 0.245), (15, -0.142), (16, 0.062), (17, 0.113), (18, -0.226), (19, 0.063), (20, 0.068), (21, -0.091), (22, 0.027), (23, 0.132), (24, 0.088), (25, 0.044), (26, 0.152), (27, 0.05), (28, 0.02), (29, 0.054), (30, -0.093), (31, -0.001), (32, 0.111), (33, -0.016), (34, 0.064), (35, 0.069), (36, -0.006), (37, 0.068), (38, -0.044), (39, 0.011), (40, 0.003), (41, 0.03), (42, -0.084), (43, -0.038), (44, 0.085), (45, 0.004), (46, 0.043), (47, -0.046), (48, 0.076), (49, -0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93584394 <a title="34-lsi-1" href="./jmlr-2011-Faster_Algorithms_for_Max-Product_Message-Passing.html">34 jmlr-2011-Faster Algorithms for Max-Product Message-Passing</a></p>
<p>Author: Julian J. McAuley, TibĂŠrio S. Caetano</p><p>Abstract: Maximum A Posteriori inference in graphical models is often solved via message-passing algorithms, such as the junction-tree algorithm or loopy belief-propagation. The exact solution to this problem is well-known to be exponential in the size of the maximal cliques of the triangulated model, while approximate inference is typically exponential in the size of the modelĂ˘&euro;&trade;s factors. In this paper, we take advantage of the fact that many models have maximal cliques that are larger than their constituent factors, and also of the fact that many factors consist only of latent variables (i.e., they do not depend on an observation). This is a common case in a wide variety of applications that deal with grid-, tree-, and ring-structured models. In such cases, we are able to decrease the exponent of complexity for message-passing by 0.5 for both exact and approximate inference. We demonstrate that message-passing operations in such models are equivalent to some variant of matrix multiplication in the tropical semiring, for which we offer an O(N 2.5 ) expected-case solution. Keywords: graphical models, belief-propagation, tropical matrix multiplication</p><p>2 0.67801344 <a title="34-lsi-2" href="./jmlr-2011-Improved_Moves_for_Truncated_Convex_Models.html">41 jmlr-2011-Improved Moves for Truncated Convex Models</a></p>
<p>Author: M. Pawan Kumar, Olga Veksler, Philip H.S. Torr</p><p>Abstract: We consider the problem of obtaining an approximate maximum a posteriori estimate of a discrete random ﬁeld characterized by pairwise potentials that form a truncated convex model. For this problem, we propose two st-MINCUT based move making algorithms that we call Range Swap and Range Expansion. Our algorithms can be thought of as extensions of αβ-Swap and α-Expansion respectively that fully exploit the form of the pairwise potentials. Speciﬁcally, instead of dealing with one or two labels at each iteration, our methods explore a large search space by considering a range of labels (that is, an interval of consecutive labels). Furthermore, we show that Range Expansion provides the same multiplicative bounds as the standard linear programming (LP) relaxation in polynomial time. Compared to previous approaches based on the LP relaxation, for example interior-point algorithms or tree-reweighted message passing (TRW), our methods are faster as they use only the efﬁcient st-MINCUT algorithm in their design. We demonstrate the usefulness of the proposed approaches on both synthetic and standard real data problems. Keywords: truncated convex models, move making algorithms, range moves, multiplicative bounds, linear programming relaxation</p><p>3 0.44732824 <a title="34-lsi-3" href="./jmlr-2011-Theoretical_Analysis_of_Bayesian_Matrix_Factorization.html">94 jmlr-2011-Theoretical Analysis of Bayesian Matrix Factorization</a></p>
<p>Author: Shinichi Nakajima, Masashi Sugiyama</p><p>Abstract: Recently, variational Bayesian (VB) techniques have been applied to probabilistic matrix factorization and shown to perform very well in experiments. In this paper, we theoretically elucidate properties of the VB matrix factorization (VBMF) method. Through ﬁnite-sample analysis of the VBMF estimator, we show that two types of shrinkage factors exist in the VBMF estimator: the positive-part James-Stein (PJS) shrinkage and the trace-norm shrinkage, both acting on each singular component separately for producing low-rank solutions. The trace-norm shrinkage is simply induced by non-ﬂat prior information, similarly to the maximum a posteriori (MAP) approach. Thus, no trace-norm shrinkage remains when priors are non-informative. On the other hand, we show a counter-intuitive fact that the PJS shrinkage factor is kept activated even with ﬂat priors. This is shown to be induced by the non-identiﬁability of the matrix factorization model, that is, the mapping between the target matrix and factorized matrices is not one-to-one. We call this model-induced regularization. We further extend our analysis to empirical Bayes scenarios where hyperparameters are also learned based on the VB free energy. Throughout the paper, we assume no missing entry in the observed matrix, and therefore collaborative ﬁltering is out of scope. Keywords: matrix factorization, variational Bayes, empirical Bayes, positive-part James-Stein shrinkage, non-identiﬁable model, model-induced regularization</p><p>4 0.40583947 <a title="34-lsi-4" href="./jmlr-2011-Logistic_Stick-Breaking_Process.html">61 jmlr-2011-Logistic Stick-Breaking Process</a></p>
<p>Author: Lu Ren, Lan Du, Lawrence Carin, David Dunson</p><p>Abstract: A logistic stick-breaking process (LSBP) is proposed for non-parametric clustering of general spatially- or temporally-dependent data, imposing the belief that proximate data are more likely to be clustered together. The sticks in the LSBP are realized via multiple logistic regression functions, with shrinkage priors employed to favor contiguous and spatially localized segments. The LSBP is also extended for the simultaneous processing of multiple data sets, yielding a hierarchical logistic stick-breaking process (H-LSBP). The model parameters (atoms) within the H-LSBP are shared across the multiple learning tasks. Efﬁcient variational Bayesian inference is derived, and comparisons are made to related techniques in the literature. Experimental analysis is performed for audio waveforms and images, and it is demonstrated that for segmentation applications the LSBP yields generally homogeneous segments with sharp boundaries. Keywords: Bayesian, nonparametric, dependent, hierarchical models, segmentation</p><p>5 0.39646801 <a title="34-lsi-5" href="./jmlr-2011-Adaptive_Exact_Inference_in_Graphical_Models.html">7 jmlr-2011-Adaptive Exact Inference in Graphical Models</a></p>
<p>Author: Özgür Sümer, Umut A. Acar, Alexander T. Ihler, Ramgopal R. Mettu</p><p>Abstract: Many algorithms and applications involve repeatedly solving variations of the same inference problem, for example to introduce new evidence to the model or to change conditional dependencies. As the model is updated, the goal of adaptive inference is to take advantage of previously computed quantities to perform inference more rapidly than from scratch. In this paper, we present algorithms for adaptive exact inference on general graphs that can be used to efﬁciently compute marginals and update MAP conﬁgurations under arbitrary changes to the input factor graph and its associated elimination tree. After a linear time preprocessing step, our approach enables updates to the model and the computation of any marginal in time that is logarithmic in the size of the input model. Moreover, in contrast to max-product our approach can also be used to update MAP conﬁgurations in time that is roughly proportional to the number of updated entries, rather than the size of the input model. To evaluate the practical effectiveness of our algorithms, we implement and test them using synthetic data as well as for two real-world computational biology applications. Our experiments show that adaptive inference can achieve substantial speedups over performing complete inference as the model undergoes small changes over time. Keywords: exact inference, factor graphs, factor elimination, marginalization, dynamic programming, MAP computation, model updates, parallel tree contraction ¨ u u c 2011 Ozg¨ r S¨ mer, Umut A. Acar, Alexander T. Ihler and Ramgopal R. Mettu. ¨ S UMER , ACAR , I HLER AND M ETTU</p><p>6 0.29290763 <a title="34-lsi-6" href="./jmlr-2011-Weisfeiler-Lehman_Graph_Kernels.html">103 jmlr-2011-Weisfeiler-Lehman Graph Kernels</a></p>
<p>7 0.28681135 <a title="34-lsi-7" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>8 0.2637057 <a title="34-lsi-8" href="./jmlr-2011-Learning_Latent_Tree_Graphical_Models.html">54 jmlr-2011-Learning Latent Tree Graphical Models</a></p>
<p>9 0.24639656 <a title="34-lsi-9" href="./jmlr-2011-Kernel_Regression_in_the_Presence_of_Correlated_Errors.html">49 jmlr-2011-Kernel Regression in the Presence of Correlated Errors</a></p>
<p>10 0.24420524 <a title="34-lsi-10" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>11 0.24368177 <a title="34-lsi-11" href="./jmlr-2011-Parallel_Algorithm_for_Learning_Optimal_Bayesian_Network_Structure.html">75 jmlr-2011-Parallel Algorithm for Learning Optimal Bayesian Network Structure</a></p>
<p>12 0.22856048 <a title="34-lsi-12" href="./jmlr-2011-Cumulative_Distribution_Networks_and_the_Derivative-sum-product_Algorithm%3A_Models_and_Inference_for_Cumulative_Distribution_Functions_on_Graphs.html">21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</a></p>
<p>13 0.21490106 <a title="34-lsi-13" href="./jmlr-2011-Convex_and_Network_Flow_Optimization_for_Structured_Sparsity.html">20 jmlr-2011-Convex and Network Flow Optimization for Structured Sparsity</a></p>
<p>14 0.21126163 <a title="34-lsi-14" href="./jmlr-2011-Smoothness%2C_Disagreement_Coefficient%2C_and_the_Label_Complexity_of_Agnostic_Active_Learning.html">85 jmlr-2011-Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning</a></p>
<p>15 0.20542464 <a title="34-lsi-15" href="./jmlr-2011-The_Indian_Buffet_Process%3A_An_Introduction_and_Review.html">90 jmlr-2011-The Indian Buffet Process: An Introduction and Review</a></p>
<p>16 0.20504248 <a title="34-lsi-16" href="./jmlr-2011-On_Equivalence_Relationships_Between_Classification_and_Ranking_Algorithms.html">71 jmlr-2011-On Equivalence Relationships Between Classification and Ranking Algorithms</a></p>
<p>17 0.20197177 <a title="34-lsi-17" href="./jmlr-2011-X-Armed_Bandits.html">104 jmlr-2011-X-Armed Bandits</a></p>
<p>18 0.20152451 <a title="34-lsi-18" href="./jmlr-2011-Parameter_Screening_and_Optimisation_for_ILP_using_Designed_Experiments.html">76 jmlr-2011-Parameter Screening and Optimisation for ILP using Designed Experiments</a></p>
<p>19 0.18764409 <a title="34-lsi-19" href="./jmlr-2011-Semi-Supervised_Learning_with_Measure_Propagation.html">84 jmlr-2011-Semi-Supervised Learning with Measure Propagation</a></p>
<p>20 0.1874246 <a title="34-lsi-20" href="./jmlr-2011-Structured_Variable_Selection_with_Sparsity-Inducing_Norms.html">88 jmlr-2011-Structured Variable Selection with Sparsity-Inducing Norms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.042), (9, 0.026), (10, 0.587), (24, 0.033), (31, 0.052), (32, 0.02), (41, 0.015), (60, 0.017), (71, 0.016), (73, 0.029), (78, 0.046), (90, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98038697 <a title="34-lda-1" href="./jmlr-2011-Parallel_Algorithm_for_Learning_Optimal_Bayesian_Network_Structure.html">75 jmlr-2011-Parallel Algorithm for Learning Optimal Bayesian Network Structure</a></p>
<p>Author: Yoshinori Tamada, Seiya Imoto, Satoru Miyano</p><p>Abstract: We present a parallel algorithm for the score-based optimal structure search of Bayesian networks. This algorithm is based on a dynamic programming (DP) algorithm having O(n · 2n ) time and space complexity, which is known to be the fastest algorithm for the optimal structure search of networks with n nodes. The bottleneck of the problem is the memory requirement, and therefore, the algorithm is currently applicable for up to a few tens of nodes. While the recently proposed algorithm overcomes this limitation by a space-time trade-off, our proposed algorithm realizes direct parallelization of the original DP algorithm with O(nσ ) time and space overhead calculations, where σ > 0 controls the communication-space trade-off. The overall time and space complexity is O(nσ+1 2n ). This algorithm splits the search space so that the required communication between independent calculations is minimal. Because of this advantage, our algorithm can run on distributed memory supercomputers. Through computational experiments, we conﬁrmed that our algorithm can run in parallel using up to 256 processors with a parallelization efﬁciency of 0.74, compared to the original DP algorithm with a single processor. We also demonstrate optimal structure search for a 32-node network without any constraints, which is the largest network search presented in literature. Keywords: optimal Bayesian network structure, parallel algorithm</p><p>same-paper 2 0.94006383 <a title="34-lda-2" href="./jmlr-2011-Faster_Algorithms_for_Max-Product_Message-Passing.html">34 jmlr-2011-Faster Algorithms for Max-Product Message-Passing</a></p>
<p>Author: Julian J. McAuley, TibĂŠrio S. Caetano</p><p>Abstract: Maximum A Posteriori inference in graphical models is often solved via message-passing algorithms, such as the junction-tree algorithm or loopy belief-propagation. The exact solution to this problem is well-known to be exponential in the size of the maximal cliques of the triangulated model, while approximate inference is typically exponential in the size of the modelĂ˘&euro;&trade;s factors. In this paper, we take advantage of the fact that many models have maximal cliques that are larger than their constituent factors, and also of the fact that many factors consist only of latent variables (i.e., they do not depend on an observation). This is a common case in a wide variety of applications that deal with grid-, tree-, and ring-structured models. In such cases, we are able to decrease the exponent of complexity for message-passing by 0.5 for both exact and approximate inference. We demonstrate that message-passing operations in such models are equivalent to some variant of matrix multiplication in the tropical semiring, for which we offer an O(N 2.5 ) expected-case solution. Keywords: graphical models, belief-propagation, tropical matrix multiplication</p><p>3 0.49811628 <a title="34-lda-3" href="./jmlr-2011-Adaptive_Exact_Inference_in_Graphical_Models.html">7 jmlr-2011-Adaptive Exact Inference in Graphical Models</a></p>
<p>Author: Özgür Sümer, Umut A. Acar, Alexander T. Ihler, Ramgopal R. Mettu</p><p>Abstract: Many algorithms and applications involve repeatedly solving variations of the same inference problem, for example to introduce new evidence to the model or to change conditional dependencies. As the model is updated, the goal of adaptive inference is to take advantage of previously computed quantities to perform inference more rapidly than from scratch. In this paper, we present algorithms for adaptive exact inference on general graphs that can be used to efﬁciently compute marginals and update MAP conﬁgurations under arbitrary changes to the input factor graph and its associated elimination tree. After a linear time preprocessing step, our approach enables updates to the model and the computation of any marginal in time that is logarithmic in the size of the input model. Moreover, in contrast to max-product our approach can also be used to update MAP conﬁgurations in time that is roughly proportional to the number of updated entries, rather than the size of the input model. To evaluate the practical effectiveness of our algorithms, we implement and test them using synthetic data as well as for two real-world computational biology applications. Our experiments show that adaptive inference can achieve substantial speedups over performing complete inference as the model undergoes small changes over time. Keywords: exact inference, factor graphs, factor elimination, marginalization, dynamic programming, MAP computation, model updates, parallel tree contraction ¨ u u c 2011 Ozg¨ r S¨ mer, Umut A. Acar, Alexander T. Ihler and Ramgopal R. Mettu. ¨ S UMER , ACAR , I HLER AND M ETTU</p><p>4 0.49511245 <a title="34-lda-4" href="./jmlr-2011-Efficient_Structure_Learning_of_Bayesian_Networks_using_Constraints.html">30 jmlr-2011-Efficient Structure Learning of Bayesian Networks using Constraints</a></p>
<p>Author: Cassio P. de Campos, Qiang Ji</p><p>Abstract: This paper addresses the problem of learning Bayesian network structures from data based on score functions that are decomposable. It describes properties that strongly reduce the time and memory costs of many known methods without losing global optimality guarantees. These properties are derived for different score criteria such as Minimum Description Length (or Bayesian Information Criterion), Akaike Information Criterion and Bayesian Dirichlet Criterion. Then a branch-andbound algorithm is presented that integrates structural constraints with data in a way to guarantee global optimality. As an example, structural constraints are used to map the problem of structure learning in Dynamic Bayesian networks into a corresponding augmented Bayesian network. Finally, we show empirically the beneﬁts of using the properties with state-of-the-art methods and with the new algorithm, which is able to handle larger data sets than before. Keywords: Bayesian networks, structure learning, properties of decomposable scores, structural constraints, branch-and-bound technique</p><p>5 0.40764236 <a title="34-lda-5" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>Author: Antti Ukkonen</p><p>Abstract: We consider the problem of clustering a set of chains to k clusters. A chain is a totally ordered subset of a ﬁnite set of items. Chains are an intuitive way to express preferences over a set of alternatives, as well as a useful representation of ratings in situations where the item-speciﬁc scores are either difﬁcult to obtain, too noisy due to measurement error, or simply not as relevant as the order that they induce over the items. First we adapt the classical k-means for chains by proposing a suitable distance function and a centroid structure. We also present two different approaches for mapping chains to a vector space. The ﬁrst one is related to the planted partition model, while the second one has an intuitive geometrical interpretation. Finally we discuss a randomization test for assessing the signiﬁcance of a clustering. To this end we present an MCMC algorithm for sampling random sets of chains that share certain properties with the original data. The methods are studied in a series of experiments using real and artiﬁcial data. Results indicate that the methods produce interesting clusterings, and for certain types of inputs improve upon previous work on clustering algorithms for orders. Keywords: Lloyd’s algorithm, orders, preference statements, planted partition model, randomization testing</p><p>6 0.38851216 <a title="34-lda-6" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>7 0.38661271 <a title="34-lda-7" href="./jmlr-2011-Domain_Decomposition_Approach_for_Fast_Gaussian_Process_Regression_of_Large_Spatial_Data_Sets.html">27 jmlr-2011-Domain Decomposition Approach for Fast Gaussian Process Regression of Large Spatial Data Sets</a></p>
<p>8 0.38028088 <a title="34-lda-8" href="./jmlr-2011-Learning_with_Structured_Sparsity.html">59 jmlr-2011-Learning with Structured Sparsity</a></p>
<p>9 0.37915057 <a title="34-lda-9" href="./jmlr-2011-Logistic_Stick-Breaking_Process.html">61 jmlr-2011-Logistic Stick-Breaking Process</a></p>
<p>10 0.36725602 <a title="34-lda-10" href="./jmlr-2011-A_Family_of_Simple_Non-Parametric_Kernel_Learning_Algorithms.html">4 jmlr-2011-A Family of Simple Non-Parametric Kernel Learning Algorithms</a></p>
<p>11 0.36669222 <a title="34-lda-11" href="./jmlr-2011-Discriminative_Learning_of_Bayesian_Networks_via_Factorized_Conditional_Log-Likelihood.html">25 jmlr-2011-Discriminative Learning of Bayesian Networks via Factorized Conditional Log-Likelihood</a></p>
<p>12 0.36266088 <a title="34-lda-12" href="./jmlr-2011-Inverse_Reinforcement_Learning_in_Partially_Observable_Environments.html">47 jmlr-2011-Inverse Reinforcement Learning in Partially Observable Environments</a></p>
<p>13 0.36254823 <a title="34-lda-13" href="./jmlr-2011-Parameter_Screening_and_Optimisation_for_ILP_using_Designed_Experiments.html">76 jmlr-2011-Parameter Screening and Optimisation for ILP using Designed Experiments</a></p>
<p>14 0.35746863 <a title="34-lda-14" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>15 0.35724047 <a title="34-lda-15" href="./jmlr-2011-Bayesian_Generalized_Kernel_Mixed_Models.html">13 jmlr-2011-Bayesian Generalized Kernel Mixed Models</a></p>
<p>16 0.35386023 <a title="34-lda-16" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>17 0.35326281 <a title="34-lda-17" href="./jmlr-2011-Convergence_of_Distributed_Asynchronous_Learning_Vector_Quantization_Algorithms.html">19 jmlr-2011-Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms</a></p>
<p>18 0.35031343 <a title="34-lda-18" href="./jmlr-2011-Super-Linear_Convergence_of_Dual_Augmented_Lagrangian_Algorithm_for_Sparsity_Regularized_Estimation.html">89 jmlr-2011-Super-Linear Convergence of Dual Augmented Lagrangian Algorithm for Sparsity Regularized Estimation</a></p>
<p>19 0.3474986 <a title="34-lda-19" href="./jmlr-2011-Cumulative_Distribution_Networks_and_the_Derivative-sum-product_Algorithm%3A_Models_and_Inference_for_Cumulative_Distribution_Functions_on_Graphs.html">21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</a></p>
<p>20 0.34387711 <a title="34-lda-20" href="./jmlr-2011-Convex_and_Network_Flow_Optimization_for_Structured_Sparsity.html">20 jmlr-2011-Convex and Network Flow Optimization for Structured Sparsity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
