<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 jmlr-2011-A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-1" href="#">jmlr2011-1</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 jmlr-2011-A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes</h1>
<br/><p>Source: <a title="jmlr-2011-1-pdf" href="http://jmlr.org/papers/volume12/ross11a/ross11a.pdf">pdf</a></p><p>Author: Stéphane Ross, Joelle Pineau, Brahim Chaib-draa, Pierre Kreitmann</p><p>Abstract: Bayesian learning methods have recently been shown to provide an elegant solution to the explorationexploitation trade-off in reinforcement learning. However most investigations of Bayesian reinforcement learning to date focus on the standard Markov Decision Processes (MDPs). The primary focus of this paper is to extend these ideas to the case of partially observable domains, by introducing the Bayes-Adaptive Partially Observable Markov Decision Processes. This new framework can be used to simultaneously (1) learn a model of the POMDP domain through interaction with the environment, (2) track the state of the system under partial observability, and (3) plan (near-)optimal sequences of actions. An important contribution of this paper is to provide theoretical results showing how the model can be ﬁnitely approximated while preserving good learning performance. We present approximate algorithms for belief tracking and planning in this model, as well as empirical results that illustrate how the model estimate and agent’s return improve as a function of experience. Keywords: processes reinforcement learning, Bayesian inference, partially observable Markov decision</p><p>Reference: <a title="jmlr-2011-1-reference" href="../jmlr2011_reference/jmlr-2011-A_Bayesian_Approach_for_Learning_and_Planning_in_Partially_Observable_Markov_Decision_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We present approximate algorithms for belief tracking and planning in this model, as well as empirical results that illustrate how the model estimate and agent’s return improve as a function of experience. [sent-13, score-0.37]
</p><p>2 A fundamental problem in RL is that of exploration-exploitation: namely, how should the agent chooses actions during the learning phase, in order to both maximize its knowledge of the model as needed to better achieve the objective later (i. [sent-28, score-0.26]
</p><p>3 Thus the exploration-exploitation problem can be handled as an explicit sequential decision problem, where the agent seeks to maximize future expected return with respect to its current uncertainty on the model. [sent-37, score-0.259]
</p><p>4 2 We derive optimal algorithms for belief tracking and ﬁnite-horizon planning in this model. [sent-50, score-0.332]
</p><p>5 These results are leveraged to derive a novel belief monitoring algorithm, which is used to maintain a posterior over both model parameters, and state of the system. [sent-54, score-0.281]
</p><p>6 Both the belief tracking and planning algorithms are parameterized so as to allow a trade-off between computational time and accuracy, such that the algorithms can be applied in real-time settings. [sent-56, score-0.332]
</p><p>7 Actions: A is a ﬁnite set of actions the agent can make in the system. [sent-82, score-0.26]
</p><p>8 Given the current state s, and an action a exe′ cuted by the agent, T sas speciﬁes the probability Pr(s′ |s, a) of moving to state s′ . [sent-86, score-0.324]
</p><p>9 For a ﬁxed current state s and action a, T sa· deﬁnes a probability distribution over the next state s′ , such ′ that ∑s′ ∈S T sas = 1, for all (s, a). [sent-87, score-0.324]
</p><p>10 The deﬁnition of T is based on the Markov assumption, which states that the transition probabilities only depend on the current state and action, that is, Pr(st+1 = s′ |at , st , . [sent-88, score-0.302]
</p><p>11 , a0 , s0 ) = Pr(st+1 = s′ |at , st ), where at and st denote respectively the action and state at time t. [sent-91, score-0.376]
</p><p>12 Rewards: R : S × A → R is the function which speciﬁes the reward R(s, a) obtained by the agent for doing a particular action a in current state s. [sent-93, score-0.389]
</p><p>13 Then at any time t, the agent chooses an action at ∈ A, performs it in the current state st , receives the reward R(st , at ) and moves to the next state st+1 with probability T st at st+1 . [sent-99, score-0.688]
</p><p>14 This corresponds to the question of determining how the agent should choose actions while learning about the task. [sent-109, score-0.26]
</p><p>15 Starting from a prior distribution over the unknown model parameters, the agent updates a posterior distribution over these parameters as it performs actions and gets observations from the environment. [sent-212, score-0.349]
</p><p>16 We deﬁne the new set of states ′ = S × T , where T = {φ ∈ N|S|2 |A| |∀(s, a) ∈ S × A, S ∑s′ ∈S φa ′ > 0}, and A is the original action ss space. [sent-273, score-0.464]
</p><p>17 Also note that the next information state φ′ only depends on the previous information state φ and the transition (s, a, s′ ) that occurred in the physical system, so that transitions between hyperstates also exhibit the Markov property. [sent-276, score-0.266]
</p><p>18 Since we want the agent to maximize the rewards it obtains in the physical system, the reward function R′ should return the same reward as in the physical system, as deﬁned in R. [sent-277, score-0.398]
</p><p>19 By the law of total probability, Pr(s′ |s, a, φ) = ′ Pr(s′ |s, a, T, φ) f (T |φ)dT = T sas f (T |φ)dT , where the integral is carried over transition function T , and f (T |φ) is the probability density of transition function T under the posterior deﬁned by 1736  BAYES -A DAPTIVE POMDP S  φ. [sent-284, score-0.314]
</p><p>20 The term  ′  ′  T sas f (T |φ)dT is the expectation of T sas for the Dirichlet posterior deﬁned by the  parameters φa 1 , . [sent-285, score-0.291]
</p><p>21 The new reward function R′ then becomes the expected reward under the current posterior over R, and the transition function T ′ would also model how to update the posterior over R, upon observation of any reward r. [sent-295, score-0.392]
</p><p>22 An equivalent continuous POMDP (over the space of states and transition functions) is solved instead of the BAMDP (assuming (s, φ) is a belief state in that POMDP). [sent-343, score-0.314]
</p><p>23 2, this can be achieved by including the Dirichlet parameters in the state space, and maintaining a belief state over these parameters. [sent-359, score-0.283]
</p><p>24 The POMDP is also deﬁned by transition probabilities {T sas }s,s′ ∈S,a∈A , where T sas = Pr(st+1 = s′ |st = s, at = a), as well as observation probabilities {Osaz }s∈S,a∈A,z∈Z where Osaz = Pr(zt = z|st = s, at−1 = a). [sent-375, score-0.359]
</p><p>25 Since the state is not directly observed, the agent must rely on the observation and action at each time step to maintain a belief state b ∈ ∆S, where ∆S is the space of probability distributions over S. [sent-377, score-0.571]
</p><p>26 The belief state speciﬁes the probability of being in each state given the history of action and observation experienced so far, starting from an initial belief b0 . [sent-378, score-0.554]
</p><p>27 ′′ ′′ ∑s′′ ∈s Os at zt+1 ∑s∈S T sat s bt (s) ′  A policy π : ∆S → A indicates how the agent should select actions as a function of the current belief. [sent-380, score-0.346]
</p><p>28 The return obtained by following π∗ from a belief b is deﬁned by Bellman’s equation: V ∗ (b) = max a∈A  ∑ b(s)R(s, a) + γ ∑ Pr(z|b, a)V ∗ (τ(b, a, z))  ,  z∈Z  s∈S  where τ(b, a, z) is the new belief after performing action a and observation z,and γ ∈ [0, 1) is the discount factor. [sent-382, score-0.436]
</p><p>29 The value of a belief state is the maximum value returned by one of the α-vectors for this belief state: Vt (b) = max ∑ α(s)b(s). [sent-390, score-0.365]
</p><p>30 The idea is that any t-step contingency plan can be expressed by an immediate action and a mapping associating a (t-1)-step contingency plan to every observation the agent could get after this immediate action. [sent-393, score-0.334]
</p><p>31 Consider an agent in a POMDP (S, A, Z, T, O, R, γ), where the transition function T and observation function O are the only unknown components of the POMDP model. [sent-411, score-0.279]
</p><p>32 , zt ) be ¯ the history of observations of the agent up to time t. [sent-415, score-0.262]
</p><p>33 Since the current state ¯ ¯ st of the agent at time t is unknown in the POMDP, we consider a joint posterior g(st , T, O|at−1 , zt ) ¯ ¯ over st , T , and O. [sent-424, score-0.604]
</p><p>34 In order to update the posterior online, each time the agent performs an action and gets an observation, it is more useful to express the posterior in recursive form: g(st , T, O|at−1 , zt ) ∝ ¯ ¯  ∑  T st−1 at−1 st Ost at−1 zt g(st−1 , T, O|at−2 , zt−1 ). [sent-430, score-0.615]
</p><p>35 This last equation gives us an online algorithm to maintain the posterior over (s, T, O), and thus allows the agent to learn about the unknown T and O via Bayesian inference. [sent-432, score-0.281]
</p><p>36 First, notice that the posterior g(st , T, O|at−1 , zt ) can be seen as a probability distribution (belief) ¯ ¯ b over tuples (s, φ, ψ), where each tuple represents a particular joint Dirichlet component parameterized by the counts (φ, ψ) for a state sequence ending in state s (i. [sent-436, score-0.293]
</p><p>37 Now we would like to ﬁnd a policy π for the agent which maps such beliefs over (s, φ, ψ) to actions a ∈ A. [sent-439, score-0.343]
</p><p>38 Consider a new POMDP (S′ , A, Z, P′ , R′ , γ), where the set of states (hyperstates) is formally de2 ﬁned as S′ = S × T × O , with T = {φ ∈ N|S| |A| |∀(s, a) ∈ S × A, ∑s′ ∈S φa ′ > 0} and O = {ψ ∈ ss N|S||A||Z| |∀(s, a) ∈ S × A, ∑z∈Z ψa > 0}. [sent-441, score-0.387]
</p><p>39 Similarly, Pr(z|a, s′ , ψ) = ′  s ∈S ss′′  Os az f (O|ψ)dO, which is the expectation of the Dirichlet posterior for Pr(z|s′ , a), and Pr(ψ′ |ψ, a, s′ , z), is either 0 or 1, depending on whether ψ′ corresponds to the posterior after observing observation (s′ , a, z) from prior ψ. [sent-449, score-0.299]
</p><p>40 Thus there exists a unique (φ,ψ) reﬂecting the correct posterior counts according to the state sequence that occurred, but these correct posterior counts are only partially observable through the observations z ∈ Z obtained by the agent. [sent-455, score-0.382]
</p><p>41 Thus (φ, ψ) can simply be thought of as other hidden state variables that the agent tracks via the belief state, based on its observations. [sent-456, score-0.404]
</p><p>42 The belief state in the BAPOMDP corresponds exactly to the posterior deﬁned in the previous section (Equation 2). [sent-458, score-0.281]
</p><p>43 By maintaining this belief, the agent maintains its uncertainty on the POMDP model and learns about the unknown transition and observations functions. [sent-459, score-0.289]
</p><p>44 ′ The proof of Theorem 1 suggests that it is sufﬁcient to iterate over S and Sb′ in order to t−1 compute the belief state bt′ when an action and observation are taken in the environment. [sent-471, score-0.316]
</p><p>45 Approximating the BAPOMDP by a Finite POMDP Solving the BAPOMDP exactly for all belief states is often impossible due to the dimensionality of the state space, in particular because the count vectors can grow unbounded. [sent-485, score-0.284]
</p><p>46 The main intuition behind the compression of the state space presented here is that, as the Dirichlet counts grow larger and larger, the transition and observation probabilities deﬁned by these counts do not change much when the counts are incremented by one. [sent-489, score-0.305]
</p><p>47 This bound ′ ′ uses the following deﬁnitions: given φ, φ′ ∈ T , and ψ, ψ′ ∈ O , deﬁne Dsa (φ, φ′ ) = ∑s′ ∈S Tφsas − Tφsas , ′ S  sa Dsa (ψ, ψ′ ) = ∑z∈Z Osaz − Osaz , Nφsa = ∑s′ ∈S φa ′ , and Nψ = ∑z∈Z ψa . [sent-493, score-0.46]
</p><p>48 Theorem 4 suggests that if we want a precision of ε on the value function, we just need to restrict 2 ˜ the space of Dirichlet parameters to count vectors φ ∈ Tε = {φ ∈ N|S| |A| |∀a ∈ A, s ∈ S, 0 < Nφsa ≤ ε ε sa ˜ ˜ ˜ NS }, and ψ ∈ Oε = {ψ ∈ N|S||A||Z| |∀a ∈ A, s ∈ S, 0 < Nψ ≤ NZ }. [sent-499, score-0.498]
</p><p>49 The next section proposes several practical efﬁcient approximations for both belief updating and online planning in the BAPOMDP. [sent-527, score-0.311]
</p><p>50 Towards a Tractable Approach to BAPOMDPs Having fully speciﬁed the BAPOMDP framework and its ﬁnite approximation, we now turn our attention to the problem of scalable belief tracking and planning in this framework. [sent-529, score-0.332]
</p><p>51 1 Approximate Belief Monitoring As shown in Theorem 1, the number of states with non-zero probability grows exponentially in the planning horizon, thus exact belief monitoring can quickly become intractable. [sent-533, score-0.313]
</p><p>52 Given a prior belief b, followed by action a and observation z, the new belief b′ is obtained by ﬁrst sampling K states from the distribution b, then for each sampled ′ s a new state s′ is sampled from T sa· . [sent-539, score-0.519]
</p><p>53 Finally, the probability Os az is added to b′ (s′ ) and the belief b′ is re-normalized. [sent-540, score-0.271]
</p><p>54 Most Probable: Another possibility is to perform the exact belief update at a given time step, but then only keep the K most probable states in the new belief b′ , and re-normalize b′ . [sent-546, score-0.397]
</p><p>55 This minimizes the L1 distance between the exact belief b′ and the approximate belief maintained with K particles. [sent-547, score-0.319]
</p><p>56 8 While keeping only the K most probable particles biases the belief of the agent, this can still be a good approach in practice, as minimizing the L1 distance bounds the difference between the values of the exact and approximate belief: that is, |V ∗ (b) −V ∗ (b′ )| ≤ ||R||∞ ||b − b′ ||1 . [sent-548, score-0.269]
</p><p>57 1−γ Weighted Distance Minimization: Finally, we consider an belief approximation technique which aims to directly minimize the difference in value function between the approximate and exact belief state by exploiting the upper bound on the value difference deﬁned in Section 4. [sent-549, score-0.365]
</p><p>58 Another advantage of the online approach is that by planning from the current belief, for any ﬁnite planning horizon t, one can compute exactly the optimal value function, as only a ﬁnite number of beliefs can be reached over that ﬁnite planning horizon. [sent-559, score-0.496]
</p><p>59 The action with highest return over that ﬁnite horizon is executed and then planning is conducted again on the next belief. [sent-566, score-0.294]
</p><p>60 To further limit the complexity of the online planning algorithm, we used the approximate belief monitoring methods detailed above. [sent-567, score-0.311]
</p><p>61 This algorithm takes as input: b is the current belief of the agent, D the desired depth of the search, and K the number of particles to use to compute the next belief states. [sent-569, score-0.328]
</p><p>62 At the end of this procedure, the agent executes action bestA in the environment and restarts this procedure with its next belief. [sent-570, score-0.265]
</p><p>63 , 1998), a new domain called Follow which simulates simple human-robot interactions and ﬁnally a standard robot planning domain known as RockSample (Smith and Simmons, 2004). [sent-585, score-0.266]
</p><p>64 Episodes terminate when the agent opens a door, at which point the POMDP state (i. [sent-596, score-0.255]
</p><p>65 Figure 1 shows how the average return and model accuracy evolve over the 100 episodes (results are averaged over 1000 simulations), using an online 3-step lookahead search with varying belief approximations and parameters. [sent-599, score-0.289]
</p><p>66 2 0 0  100  20  40 60 Episode  80  100  Planning Time/Action (ms)  20  15  10  5  0  MP (2)  MC (64)  WD (2)  Figure 1: Tiger: Empirical return (top left), belief estimation error (top right), and planning time (bottom), for different belief tracking approximation. [sent-618, score-0.519]
</p><p>67 The reward R = +1 if the robot and person are at the same position (central grid cell), R = 0 if the person is one cell away from the robot, and R = −1 if the person is two cells away. [sent-627, score-0.315]
</p><p>68 The robot can choose between 4 (deterministic) motion actions to move to neighboring cells, the Sample action, and a Sensor action for each rock, so there are k + 5 actions in general. [sent-667, score-0.353]
</p><p>69 The sensor can be used when the robot is away from the rock, but the accuracy depends on the distance d between the robot and the rock. [sent-670, score-0.325]
</p><p>70 1 I NFLUENCE OF L ARGE N UMBER OF S TATES We consider the case where transition probabilities are known, and the agent must learn its observation function. [sent-674, score-0.3]
</p><p>71 Figure 3(right) sheds further light on this issue, by showing, for ˆ each episode, the maximum L1 distance between the estimated belief b(s) = ∑ψ,φ b(s, φ, ψ), and the correct belief b(s) (assuming the model is known a priori). [sent-682, score-0.319]
</p><p>72 This suggests that the robot has reached a point where it knows its model well enough to have the same belief over the physical states as a robot who would know the true model. [sent-684, score-0.443]
</p><p>73 1 0 90  100  0  10  20  30  40  50 60 Episodes  70  80  90  100  Figure 3: RockSample: Empirical return (left) and belief estimation error (right) for different belief tracking approximation. [sent-697, score-0.385]
</p><p>74 The right-side graph clearly shows how the magnitude of the initial k impacts the error in the belief over physical states (indicating that the robot doesn’t know the quality of the rocks as well as if it knew the correct model). [sent-774, score-0.332]
</p><p>75 belief state tracking is signiﬁcantly reduced after about 80 iterations, conﬁrming that our algorithm is able to overcome poor priors, even those with high initial conﬁdence. [sent-788, score-0.265]
</p><p>76 Finally, Doshi-Velez (2010) proposed a Bayesian learning framework for the case of POMDPs where the number of states is not known a priori, thus allowing the number of states to grow gradually as the agent explores the world, while simultaneously updating a posterior over the parameters. [sent-836, score-0.313]
</p><p>77 Hence, a s ss ′ particular (s, φ, ψ) such that bt−1 (s, φ, ψ) > 0 yields non-zero probabilities to at most |S| different ′ states in bt′ . [sent-902, score-0.408]
</p><p>78 For some of the following theorems, lemmas and proofs, we will sometime denote the Dirichlet count update operator U , as deﬁned for the BAPOMDP, as a vector addition: φ′ = φ + δa ′ = ss U (φ, s, a, s′ ), that is, δa ′ is a vector full of zeros, with a 1 for the element φa ′ . [sent-912, score-0.395]
</p><p>79 ss ss ′  Lemma 1 For any t ≥ 2, any α-vector αt ∈ Γt can be expressed as αta,α (s, φ, ψ) = R(s, a) + ′ s′ γ ∑z∈Z ∑s∈S′ Tφsas Oψaz α′ (z)(s′ , φ+δa ′ , ψ+δa′ z ) for some a ∈ A, and α′ deﬁning a mapping Z → Γt−1 . [sent-913, score-0.714]
</p><p>80 Lemma 3 Given any φ, φ′ ∈ T , ψ, ψ′ ∈ O , then for all s ∈ S, a ∈ A, we have that φ′a′ ψ′a ′  φa ′ ψa′  ′  s ∑s′ ∈S ∑z∈Z N ss Nssz′ a − N ss Nssz′ a ≤ Dsa (φ′ , φ) + sups′ ∈S DZ a (ψ′ , ψ). [sent-917, score-0.714]
</p><p>81 S  Lemma 4 Given any φ, φ′ , ∆ ∈ T , then for all s ∈ S, a ∈ A, 2N sa ∑ ′  |φa −φ′a |  ′ ∈S ss Dsa (φ + ∆, φ′ + ∆) ≤ Dsa (φ, φ′ ) + (N sa∆+Nssa )(Nsssa +N ′sa ) . [sent-919, score-0.817]
</p><p>82 φ′  ∆  ′  s Nψ a  ψa′  ROSS , P INEAU , C HAIB - DRAA AND K REITMANN  Lemma 5 Given any ψ, ψ′ , ∆ ∈ O , then for all s ∈ S, a ∈ A, 2N sa ∑ |ψa −ψ′a | sz Dsa (ψ + ∆, ψ′ + ∆) ≤ Dsa (ψ, ψ′ ) + (N sa∆ N z∈Z Nsz +N sa ) . [sent-921, score-0.959]
</p><p>83 Z Z + sa )( sa ψ  ∆  ∆  ψ′  Proof Same proof as for lemma 4, except that we sum over z ∈ Z in this case. [sent-922, score-0.92]
</p><p>84 1 1 Theorem 3 Given any φ, φ′ ∈ T , ψ, ψ′ ∈ O and γ ∈ (0, 1), then ∀t: sup |αt (s, φ, ψ) − αt (s, φ′ , ψ′ )| ≤  αt ∈Γt ,s∈S 4 ln(γ−e )  ∑s′′ ∈S |φa ′′ −φ′a′′ | ss ss (Nφsa +1)(Nφsa +1) ′  +  2γ||R||∞ (1−γ)2  ∑z∈Z |ψa′ z −ψ′az | s s′ ′  ′  s s (Nψ a +1)(Nψ′ a +1)  1762  sup s,s′ ∈S,a∈A  . [sent-932, score-0.782]
</p><p>85 Hence by taking the sup we get:  supαt ∈Γt ,s∈S |αt (s, φ, ψ) − αt (s, φ′ , ψ′ )| ≤ γ sup αt−1 (s′ , φ + δa ′ , ψ + δa′ z ) − αt−1 (s′ , φ′ + δa ′ , ψ′ + δa′ z ) s ss s ss s,s′ ∈S,a∈A,z∈Z,αt−1 ∈Γt−1 s′ + γ||R||∞ sup Dsa (φ′ , φ) + DZ a (ψ′ , ψ) S 1−γ s,s′ ∈S,a∈A  . [sent-935, score-0.816]
</p><p>86 BAYES -A DAPTIVE POMDP S  Proof  ∑s′ ∈S |φa ′ −(φa ′ +∆a ′ )| ss ss ss sa (Nφsa +1)(Nφsa +N∆ +1) ∑s′ ∈S ∆a ss = (N sa +1)(N sa +′N sa +1) φ φ ∆  =  1  Nφsa +1  N∆sa sa +N sa +1 N∆ φ  . [sent-942, score-4.188]
</p><p>87 N sa  sa ∆ The term N sa +N sa +1 is monotonically increasing and converge to 1 as N∆ → ∞. [sent-943, score-1.84]
</p><p>88 Corollary 1 Given ε > 0, φ ∈ T , s ∈ S, a ∈ A, if Nφsa > ∑s′ ∈S |φa ′ −(φa ′ +∆a ′ )| ss ss ss sa (Nφsa +1)(Nφsa +N∆ +1)  1 ε  < ε. [sent-945, score-1.531]
</p><p>89 − 1 then for all ∆ ∈ T we have that ∑′  |φa −(φa +∆a )|  ∈S ss 1 ss Proof According to lemma 8, we know that for all ∆ ∈ T , we have that (Ns sa +1)(′N sa +′N sass′ ≤ N sa +1 . [sent-946, score-2.094]
</p><p>90 φ φ φ ∆ +1) 1 Hence if Nφsa > 1 − 1, then N sa +1 < ε. [sent-947, score-0.46]
</p><p>91 ε φ  ∑ 1 sz Lemma 9 Given ψ ∈ O , s ∈ S, a ∈ A, then for all ∆ ∈ O , (N z∈Z szN sa +N sasz ≤ N sa +1 . [sent-948, score-0.959]
</p><p>92 sa ψ +1)( ψ ψ ∆ +1) |ψa −(ψa +∆a )|  Proof Same proof as lemma 8. [sent-949, score-0.46]
</p><p>93 sa Corollary 2 Given ε > 0, ψ ∈ O , s ∈ S, a ∈ A, if Nψ > ∑z∈Z |ψa −(ψa +∆a )| sz sz sz sa sa sa (Nψ +1)(Nψ +N∆ +1)  1 ε  <ε  − 1 then for all ∆ ∈ O we have that  Proof Same proof as corollary 1, but using lemma 9 instead. [sent-950, score-1.957]
</p><p>94 According to corollary 1, we have that for any φ ∈ T such that N sa > 1 − 1, ε φ ε′′ 32γ||R||∞ a ′a ′ ∈ T such that there exists a ∆ ∈ T where φ′ = φ + ∆, then ∑s′′ ∈S |φss′′ −φss′′ | < ε′′ . [sent-954, score-0.46]
</p><p>95 Hence then for all φ (Nφsa +1)(Nφsa +1) ′ sa > N, there exists a φ′ ∈ T such that N sa ≤ N, we want to ﬁnd an N such that given φ ∈ T with Nφ φ′ sa (φ, φ′ ) < ε′ and exists a ∆ ∈ T such that φ = φ′ + ∆. [sent-955, score-1.38]
</p><p>96 Let’s consider an arbitrary φ such that DS Nφa ss Nφsa > N. [sent-956, score-0.357]
</p><p>97 We can construct a new vector φ′ as follows, for all s′ deﬁne φ′a′ = N sa′ and for all ss φ  higher counts is within ε distance of another vector with lower counts. [sent-957, score-0.42]
</p><p>98 NS = NS , as deﬁned in Section  4, will be our bound on Nφsa such that, as we have just showed, for any φ ∈ T such that Nφsa > NS , ∑ ′′  |φa −φ′a |  ∈S ss ss′′ we can ﬁnd a φ′ ∈ T such that Nφsa ≤ NS , Dsa (φ, φ′ ) < ε′ and (Ns sa +1)(′′ sa +1) < ε′′ . [sent-965, score-1.277]
</p><p>99 By Theorem 3, for any t, supαt ∈Γt ,s∈S |αt (s, φ, ψ) − αt (s, φ, ψ)| ≤  2γ||R||∞ (1−γ)2  sup s,s′ ∈S,a∈A  ′  ˜ ∑s′′ ∈S |φa ′′ −φa ′′ | ss ss sa +1)(N sa +1) ( Nφ ˜ φ  +  ˜s ∑z∈Z |ψa′ z −ψa′ z | s ′  ′  s s (Nψ a +1)(Nψ a +1) ˜  <  4 ε′ + ε′ + ln(γ−e ) (ε′′ + ε′′ ) = ε. [sent-976, score-1.668]
</p><p>100 i=0 ˜ Theorem 6 Given any ε > 0, and any horizon t, let πt be the optimal t-step policy computed from ˜ ˜ ˜ ˜ the ﬁnite POMDP (Sε , A, Z, Tε , Oε , Rε , γ), then for any initial belief b the value of executing policy ε ˜ πt in the BAPOMDP Vπt (b) ≥ V ∗ (b) − 2 1−γ . [sent-985, score-0.318]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sa', 0.46), ('pomdp', 0.383), ('ss', 0.357), ('bapomdp', 0.282), ('agent', 0.188), ('ross', 0.173), ('dsa', 0.168), ('belief', 0.149), ('planning', 0.134), ('robot', 0.132), ('az', 0.122), ('st', 0.116), ('sas', 0.113), ('draa', 0.104), ('haib', 0.104), ('ineau', 0.104), ('nsssa', 0.104), ('reitmann', 0.104), ('pr', 0.081), ('daptive', 0.079), ('observable', 0.079), ('action', 0.077), ('ns', 0.074), ('actions', 0.072), ('reinforcement', 0.072), ('probable', 0.069), ('mdp', 0.068), ('transition', 0.068), ('state', 0.067), ('dirichlet', 0.067), ('posterior', 0.065), ('hyperstates', 0.064), ('policy', 0.062), ('nz', 0.06), ('rewards', 0.058), ('reward', 0.057), ('rocksample', 0.054), ('zt', 0.052), ('tracking', 0.049), ('episodes', 0.049), ('horizon', 0.045), ('bamdp', 0.045), ('person', 0.042), ('counts', 0.042), ('sb', 0.042), ('dz', 0.041), ('rl', 0.04), ('observability', 0.04), ('sensor', 0.04), ('sz', 0.039), ('bayesian', 0.038), ('ds', 0.038), ('count', 0.038), ('pineau', 0.038), ('rock', 0.038), ('return', 0.038), ('duff', 0.035), ('sup', 0.034), ('pomdps', 0.034), ('uncertainty', 0.033), ('maxa', 0.031), ('ln', 0.031), ('particles', 0.03), ('za', 0.03), ('os', 0.03), ('states', 0.03), ('brl', 0.03), ('dearden', 0.03), ('engel', 0.03), ('gsa', 0.03), ('nss', 0.03), ('osaz', 0.03), ('poupart', 0.029), ('bayes', 0.029), ('online', 0.028), ('exploration', 0.028), ('tiger', 0.026), ('vlassis', 0.026), ('domains', 0.026), ('ghavamzadeh', 0.025), ('lookahead', 0.025), ('sups', 0.025), ('prior', 0.024), ('episode', 0.024), ('bt', 0.024), ('ine', 0.024), ('plan', 0.023), ('fx', 0.023), ('priors', 0.023), ('observation', 0.023), ('history', 0.022), ('partially', 0.022), ('distance', 0.021), ('beliefs', 0.021), ('precup', 0.021), ('rocks', 0.021), ('spaan', 0.021), ('probabilities', 0.021), ('aixi', 0.02), ('autonomous', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="1-tfidf-1" href="./jmlr-2011-A_Bayesian_Approach_for_Learning_and_Planning_in_Partially_Observable_Markov_Decision_Processes.html">1 jmlr-2011-A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes</a></p>
<p>Author: Stéphane Ross, Joelle Pineau, Brahim Chaib-draa, Pierre Kreitmann</p><p>Abstract: Bayesian learning methods have recently been shown to provide an elegant solution to the explorationexploitation trade-off in reinforcement learning. However most investigations of Bayesian reinforcement learning to date focus on the standard Markov Decision Processes (MDPs). The primary focus of this paper is to extend these ideas to the case of partially observable domains, by introducing the Bayes-Adaptive Partially Observable Markov Decision Processes. This new framework can be used to simultaneously (1) learn a model of the POMDP domain through interaction with the environment, (2) track the state of the system under partial observability, and (3) plan (near-)optimal sequences of actions. An important contribution of this paper is to provide theoretical results showing how the model can be ﬁnitely approximated while preserving good learning performance. We present approximate algorithms for belief tracking and planning in this model, as well as empirical results that illustrate how the model estimate and agent’s return improve as a function of experience. Keywords: processes reinforcement learning, Bayesian inference, partially observable Markov decision</p><p>2 0.29820392 <a title="1-tfidf-2" href="./jmlr-2011-Exploiting_Best-Match_Equations_for_Efficient_Reinforcement_Learning.html">33 jmlr-2011-Exploiting Best-Match Equations for Efficient Reinforcement Learning</a></p>
<p>Author: Harm van Seijen, Shimon Whiteson, Hado van Hasselt, Marco Wiering</p><p>Abstract: This article presents and evaluates best-match learning, a new approach to reinforcement learning that trades off the sample efﬁciency of model-based methods with the space efﬁciency of modelfree methods. Best-match learning works by approximating the solution to a set of best-match equations, which combine a sparse model with a model-free Q-value function constructed from samples not used by the model. We prove that, unlike regular sparse model-based methods, bestmatch learning is guaranteed to converge to the optimal Q-values in the tabular case. Empirical results demonstrate that best-match learning can substantially outperform regular sparse modelbased methods, as well as several model-free methods that strive to improve the sample efﬁciency of temporal-difference methods. In addition, we demonstrate that best-match learning can be successfully combined with function approximation. Keywords: reinforcement learning, on-line learning, temporal-difference methods, function approximation, data reuse</p><p>3 0.25143251 <a title="1-tfidf-3" href="./jmlr-2011-Inverse_Reinforcement_Learning_in_Partially_Observable_Environments.html">47 jmlr-2011-Inverse Reinforcement Learning in Partially Observable Environments</a></p>
<p>Author: Jaedeug Choi, Kee-Eung Kim</p><p>Abstract: Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behavior of an expert. Most of the existing IRL algorithms assume that the environment is modeled as a Markov decision process (MDP), although it is desirable to handle partially observable settings in order to handle more realistic scenarios. In this paper, we present IRL algorithms for partially observable environments that can be modeled as a partially observable Markov decision process (POMDP). We deal with two cases according to the representation of the given expert’s behavior, namely the case in which the expert’s policy is explicitly given, and the case in which the expert’s trajectories are available instead. The IRL in POMDPs poses a greater challenge than in MDPs since it is not only ill-posed due to the nature of IRL, but also computationally intractable due to the hardness in solving POMDPs. To overcome these obstacles, we present algorithms that exploit some of the classical results from the POMDP literature. Experimental results on several benchmark POMDP domains show that our work is useful for partially observable settings. Keywords: inverse reinforcement learning, partially observable Markov decision process, inverse optimization, linear programming, quadratically constrained programming</p><p>4 0.089434952 <a title="1-tfidf-4" href="./jmlr-2011-Generalized_TD_Learning.html">36 jmlr-2011-Generalized TD Learning</a></p>
<p>Author: Tsuyoshi Ueno, Shin-ichi Maeda, Motoaki Kawanabe, Shin Ishii</p><p>Abstract: Since the invention of temporal difference (TD) learning (Sutton, 1988), many new algorithms for model-free policy evaluation have been proposed. Although they have brought much progress in practical applications of reinforcement learning (RL), there still remain fundamental problems concerning statistical properties of the value function estimation. To solve these problems, we introduce a new framework, semiparametric statistical inference, to model-free policy evaluation. This framework generalizes TD learning and its extensions, and allows us to investigate statistical properties of both of batch and online learning procedures for the value function estimation in a uniﬁed way in terms of estimating functions. Furthermore, based on this framework, we derive an optimal estimating function with the minimum asymptotic variance and propose batch and online learning algorithms which achieve the optimality. Keywords: reinforcement learning, model-free policy evaluation, TD learning, semiparametirc model, estimating function</p><p>5 0.063276261 <a title="1-tfidf-5" href="./jmlr-2011-Robust_Approximate_Bilinear_Programming_for_Value_Function_Approximation.html">81 jmlr-2011-Robust Approximate Bilinear Programming for Value Function Approximation</a></p>
<p>Author: Marek Petrik, Shlomo Zilberstein</p><p>Abstract: Value function approximation methods have been successfully used in many applications, but the prevailing techniques often lack useful a priori error bounds. We propose a new approximate bilinear programming formulation of value function approximation, which employs global optimization. The formulation provides strong a priori guarantees on both robust and expected policy loss by minimizing speciﬁc norms of the Bellman residual. Solving a bilinear program optimally is NP-hard, but this worst-case complexity is unavoidable because the Bellman-residual minimization itself is NP-hard. We describe and analyze the formulation as well as a simple approximate algorithm for solving bilinear programs. The analysis shows that this algorithm offers a convergent generalization of approximate policy iteration. We also brieﬂy analyze the behavior of bilinear programming algorithms under incomplete samples. Finally, we demonstrate that the proposed approach can consistently minimize the Bellman residual on simple benchmark problems. Keywords: value function approximation, approximate dynamic programming, Markov decision processes</p><p>6 0.050452419 <a title="1-tfidf-6" href="./jmlr-2011-Double_Updating_Online_Learning.html">28 jmlr-2011-Double Updating Online Learning</a></p>
<p>7 0.049509998 <a title="1-tfidf-7" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>8 0.04037042 <a title="1-tfidf-8" href="./jmlr-2011-Online_Learning_in_Case_of_Unbounded_Losses_Using_Follow_the_Perturbed_Leader_Algorithm.html">73 jmlr-2011-Online Learning in Case of Unbounded Losses Using Follow the Perturbed Leader Algorithm</a></p>
<p>9 0.040199153 <a title="1-tfidf-9" href="./jmlr-2011-Dirichlet_Process_Mixtures_of_Generalized_Linear_Models.html">24 jmlr-2011-Dirichlet Process Mixtures of Generalized Linear Models</a></p>
<p>10 0.039081916 <a title="1-tfidf-10" href="./jmlr-2011-The_Indian_Buffet_Process%3A_An_Introduction_and_Review.html">90 jmlr-2011-The Indian Buffet Process: An Introduction and Review</a></p>
<p>11 0.037315492 <a title="1-tfidf-11" href="./jmlr-2011-Regression_on_Fixed-Rank_Positive_Semidefinite_Matrices%3A_A_Riemannian_Approach.html">80 jmlr-2011-Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach</a></p>
<p>12 0.036580723 <a title="1-tfidf-12" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>13 0.034316536 <a title="1-tfidf-13" href="./jmlr-2011-Hierarchical_Knowledge_Gradient_for_Sequential_Sampling.html">38 jmlr-2011-Hierarchical Knowledge Gradient for Sequential Sampling</a></p>
<p>14 0.033271533 <a title="1-tfidf-14" href="./jmlr-2011-Convergence_of_Distributed_Asynchronous_Learning_Vector_Quantization_Algorithms.html">19 jmlr-2011-Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms</a></p>
<p>15 0.0328505 <a title="1-tfidf-15" href="./jmlr-2011-X-Armed_Bandits.html">104 jmlr-2011-X-Armed Bandits</a></p>
<p>16 0.032163668 <a title="1-tfidf-16" href="./jmlr-2011-Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization.html">8 jmlr-2011-Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></p>
<p>17 0.030360471 <a title="1-tfidf-17" href="./jmlr-2011-In_All_Likelihood%2C_Deep_Belief_Is_Not_Enough.html">42 jmlr-2011-In All Likelihood, Deep Belief Is Not Enough</a></p>
<p>18 0.029953837 <a title="1-tfidf-18" href="./jmlr-2011-Internal_Regret_with_Partial_Monitoring%3A_Calibration-Based_Optimal_Algorithms.html">45 jmlr-2011-Internal Regret with Partial Monitoring: Calibration-Based Optimal Algorithms</a></p>
<p>19 0.028588099 <a title="1-tfidf-19" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>20 0.025082424 <a title="1-tfidf-20" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.16), (1, 0.116), (2, -0.287), (3, 0.056), (4, -0.132), (5, 0.43), (6, -0.313), (7, -0.067), (8, 0.082), (9, -0.072), (10, 0.044), (11, -0.006), (12, 0.016), (13, -0.118), (14, 0.047), (15, -0.068), (16, 0.008), (17, -0.214), (18, 0.097), (19, -0.048), (20, 0.038), (21, 0.022), (22, 0.049), (23, -0.167), (24, 0.109), (25, 0.028), (26, -0.084), (27, -0.112), (28, 0.054), (29, 0.005), (30, 0.074), (31, -0.009), (32, -0.013), (33, -0.113), (34, 0.078), (35, 0.066), (36, 0.005), (37, 0.033), (38, 0.029), (39, 0.022), (40, 0.062), (41, -0.008), (42, -0.028), (43, -0.013), (44, -0.048), (45, -0.008), (46, 0.048), (47, 0.021), (48, 0.016), (49, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96913773 <a title="1-lsi-1" href="./jmlr-2011-Exploiting_Best-Match_Equations_for_Efficient_Reinforcement_Learning.html">33 jmlr-2011-Exploiting Best-Match Equations for Efficient Reinforcement Learning</a></p>
<p>Author: Harm van Seijen, Shimon Whiteson, Hado van Hasselt, Marco Wiering</p><p>Abstract: This article presents and evaluates best-match learning, a new approach to reinforcement learning that trades off the sample efﬁciency of model-based methods with the space efﬁciency of modelfree methods. Best-match learning works by approximating the solution to a set of best-match equations, which combine a sparse model with a model-free Q-value function constructed from samples not used by the model. We prove that, unlike regular sparse model-based methods, bestmatch learning is guaranteed to converge to the optimal Q-values in the tabular case. Empirical results demonstrate that best-match learning can substantially outperform regular sparse modelbased methods, as well as several model-free methods that strive to improve the sample efﬁciency of temporal-difference methods. In addition, we demonstrate that best-match learning can be successfully combined with function approximation. Keywords: reinforcement learning, on-line learning, temporal-difference methods, function approximation, data reuse</p><p>same-paper 2 0.96850586 <a title="1-lsi-2" href="./jmlr-2011-A_Bayesian_Approach_for_Learning_and_Planning_in_Partially_Observable_Markov_Decision_Processes.html">1 jmlr-2011-A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes</a></p>
<p>Author: Stéphane Ross, Joelle Pineau, Brahim Chaib-draa, Pierre Kreitmann</p><p>Abstract: Bayesian learning methods have recently been shown to provide an elegant solution to the explorationexploitation trade-off in reinforcement learning. However most investigations of Bayesian reinforcement learning to date focus on the standard Markov Decision Processes (MDPs). The primary focus of this paper is to extend these ideas to the case of partially observable domains, by introducing the Bayes-Adaptive Partially Observable Markov Decision Processes. This new framework can be used to simultaneously (1) learn a model of the POMDP domain through interaction with the environment, (2) track the state of the system under partial observability, and (3) plan (near-)optimal sequences of actions. An important contribution of this paper is to provide theoretical results showing how the model can be ﬁnitely approximated while preserving good learning performance. We present approximate algorithms for belief tracking and planning in this model, as well as empirical results that illustrate how the model estimate and agent’s return improve as a function of experience. Keywords: processes reinforcement learning, Bayesian inference, partially observable Markov decision</p><p>3 0.5676474 <a title="1-lsi-3" href="./jmlr-2011-Inverse_Reinforcement_Learning_in_Partially_Observable_Environments.html">47 jmlr-2011-Inverse Reinforcement Learning in Partially Observable Environments</a></p>
<p>Author: Jaedeug Choi, Kee-Eung Kim</p><p>Abstract: Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behavior of an expert. Most of the existing IRL algorithms assume that the environment is modeled as a Markov decision process (MDP), although it is desirable to handle partially observable settings in order to handle more realistic scenarios. In this paper, we present IRL algorithms for partially observable environments that can be modeled as a partially observable Markov decision process (POMDP). We deal with two cases according to the representation of the given expert’s behavior, namely the case in which the expert’s policy is explicitly given, and the case in which the expert’s trajectories are available instead. The IRL in POMDPs poses a greater challenge than in MDPs since it is not only ill-posed due to the nature of IRL, but also computationally intractable due to the hardness in solving POMDPs. To overcome these obstacles, we present algorithms that exploit some of the classical results from the POMDP literature. Experimental results on several benchmark POMDP domains show that our work is useful for partially observable settings. Keywords: inverse reinforcement learning, partially observable Markov decision process, inverse optimization, linear programming, quadratically constrained programming</p><p>4 0.27554137 <a title="1-lsi-4" href="./jmlr-2011-Generalized_TD_Learning.html">36 jmlr-2011-Generalized TD Learning</a></p>
<p>Author: Tsuyoshi Ueno, Shin-ichi Maeda, Motoaki Kawanabe, Shin Ishii</p><p>Abstract: Since the invention of temporal difference (TD) learning (Sutton, 1988), many new algorithms for model-free policy evaluation have been proposed. Although they have brought much progress in practical applications of reinforcement learning (RL), there still remain fundamental problems concerning statistical properties of the value function estimation. To solve these problems, we introduce a new framework, semiparametric statistical inference, to model-free policy evaluation. This framework generalizes TD learning and its extensions, and allows us to investigate statistical properties of both of batch and online learning procedures for the value function estimation in a uniﬁed way in terms of estimating functions. Furthermore, based on this framework, we derive an optimal estimating function with the minimum asymptotic variance and propose batch and online learning algorithms which achieve the optimality. Keywords: reinforcement learning, model-free policy evaluation, TD learning, semiparametirc model, estimating function</p><p>5 0.23447293 <a title="1-lsi-5" href="./jmlr-2011-Robust_Approximate_Bilinear_Programming_for_Value_Function_Approximation.html">81 jmlr-2011-Robust Approximate Bilinear Programming for Value Function Approximation</a></p>
<p>Author: Marek Petrik, Shlomo Zilberstein</p><p>Abstract: Value function approximation methods have been successfully used in many applications, but the prevailing techniques often lack useful a priori error bounds. We propose a new approximate bilinear programming formulation of value function approximation, which employs global optimization. The formulation provides strong a priori guarantees on both robust and expected policy loss by minimizing speciﬁc norms of the Bellman residual. Solving a bilinear program optimally is NP-hard, but this worst-case complexity is unavoidable because the Bellman-residual minimization itself is NP-hard. We describe and analyze the formulation as well as a simple approximate algorithm for solving bilinear programs. The analysis shows that this algorithm offers a convergent generalization of approximate policy iteration. We also brieﬂy analyze the behavior of bilinear programming algorithms under incomplete samples. Finally, we demonstrate that the proposed approach can consistently minimize the Bellman residual on simple benchmark problems. Keywords: value function approximation, approximate dynamic programming, Markov decision processes</p><p>6 0.20218675 <a title="1-lsi-6" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>7 0.20025128 <a title="1-lsi-7" href="./jmlr-2011-Double_Updating_Online_Learning.html">28 jmlr-2011-Double Updating Online Learning</a></p>
<p>8 0.16752627 <a title="1-lsi-8" href="./jmlr-2011-Online_Learning_in_Case_of_Unbounded_Losses_Using_Follow_the_Perturbed_Leader_Algorithm.html">73 jmlr-2011-Online Learning in Case of Unbounded Losses Using Follow the Perturbed Leader Algorithm</a></p>
<p>9 0.16132897 <a title="1-lsi-9" href="./jmlr-2011-Regression_on_Fixed-Rank_Positive_Semidefinite_Matrices%3A_A_Riemannian_Approach.html">80 jmlr-2011-Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach</a></p>
<p>10 0.14879967 <a title="1-lsi-10" href="./jmlr-2011-The_Indian_Buffet_Process%3A_An_Introduction_and_Review.html">90 jmlr-2011-The Indian Buffet Process: An Introduction and Review</a></p>
<p>11 0.1347505 <a title="1-lsi-11" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>12 0.13430057 <a title="1-lsi-12" href="./jmlr-2011-Dirichlet_Process_Mixtures_of_Generalized_Linear_Models.html">24 jmlr-2011-Dirichlet Process Mixtures of Generalized Linear Models</a></p>
<p>13 0.12588215 <a title="1-lsi-13" href="./jmlr-2011-Convergence_of_Distributed_Asynchronous_Learning_Vector_Quantization_Algorithms.html">19 jmlr-2011-Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms</a></p>
<p>14 0.12400697 <a title="1-lsi-14" href="./jmlr-2011-Adaptive_Exact_Inference_in_Graphical_Models.html">7 jmlr-2011-Adaptive Exact Inference in Graphical Models</a></p>
<p>15 0.11728248 <a title="1-lsi-15" href="./jmlr-2011-The_Sample_Complexity_of_Dictionary_Learning.html">91 jmlr-2011-The Sample Complexity of Dictionary Learning</a></p>
<p>16 0.11499573 <a title="1-lsi-16" href="./jmlr-2011-In_All_Likelihood%2C_Deep_Belief_Is_Not_Enough.html">42 jmlr-2011-In All Likelihood, Deep Belief Is Not Enough</a></p>
<p>17 0.10724777 <a title="1-lsi-17" href="./jmlr-2011-Logistic_Stick-Breaking_Process.html">61 jmlr-2011-Logistic Stick-Breaking Process</a></p>
<p>18 0.10678657 <a title="1-lsi-18" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<p>19 0.10609585 <a title="1-lsi-19" href="./jmlr-2011-Bayesian_Generalized_Kernel_Mixed_Models.html">13 jmlr-2011-Bayesian Generalized Kernel Mixed Models</a></p>
<p>20 0.10592863 <a title="1-lsi-20" href="./jmlr-2011-Information_Rates_of_Nonparametric_Gaussian_Process_Methods.html">44 jmlr-2011-Information Rates of Nonparametric Gaussian Process Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.031), (9, 0.017), (10, 0.029), (24, 0.065), (31, 0.062), (32, 0.027), (41, 0.035), (60, 0.012), (65, 0.013), (73, 0.027), (76, 0.379), (78, 0.139), (85, 0.035), (90, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75891525 <a title="1-lda-1" href="./jmlr-2011-A_Bayesian_Approach_for_Learning_and_Planning_in_Partially_Observable_Markov_Decision_Processes.html">1 jmlr-2011-A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes</a></p>
<p>Author: Stéphane Ross, Joelle Pineau, Brahim Chaib-draa, Pierre Kreitmann</p><p>Abstract: Bayesian learning methods have recently been shown to provide an elegant solution to the explorationexploitation trade-off in reinforcement learning. However most investigations of Bayesian reinforcement learning to date focus on the standard Markov Decision Processes (MDPs). The primary focus of this paper is to extend these ideas to the case of partially observable domains, by introducing the Bayes-Adaptive Partially Observable Markov Decision Processes. This new framework can be used to simultaneously (1) learn a model of the POMDP domain through interaction with the environment, (2) track the state of the system under partial observability, and (3) plan (near-)optimal sequences of actions. An important contribution of this paper is to provide theoretical results showing how the model can be ﬁnitely approximated while preserving good learning performance. We present approximate algorithms for belief tracking and planning in this model, as well as empirical results that illustrate how the model estimate and agent’s return improve as a function of experience. Keywords: processes reinforcement learning, Bayesian inference, partially observable Markov decision</p><p>2 0.4065389 <a title="1-lda-2" href="./jmlr-2011-Inverse_Reinforcement_Learning_in_Partially_Observable_Environments.html">47 jmlr-2011-Inverse Reinforcement Learning in Partially Observable Environments</a></p>
<p>Author: Jaedeug Choi, Kee-Eung Kim</p><p>Abstract: Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behavior of an expert. Most of the existing IRL algorithms assume that the environment is modeled as a Markov decision process (MDP), although it is desirable to handle partially observable settings in order to handle more realistic scenarios. In this paper, we present IRL algorithms for partially observable environments that can be modeled as a partially observable Markov decision process (POMDP). We deal with two cases according to the representation of the given expert’s behavior, namely the case in which the expert’s policy is explicitly given, and the case in which the expert’s trajectories are available instead. The IRL in POMDPs poses a greater challenge than in MDPs since it is not only ill-posed due to the nature of IRL, but also computationally intractable due to the hardness in solving POMDPs. To overcome these obstacles, we present algorithms that exploit some of the classical results from the POMDP literature. Experimental results on several benchmark POMDP domains show that our work is useful for partially observable settings. Keywords: inverse reinforcement learning, partially observable Markov decision process, inverse optimization, linear programming, quadratically constrained programming</p><p>3 0.40040669 <a title="1-lda-3" href="./jmlr-2011-Differentially_Private_Empirical_Risk_Minimization.html">22 jmlr-2011-Differentially Private Empirical Risk Minimization</a></p>
<p>Author: Kamalika Chaudhuri, Claire Monteleoni, Anand D. Sarwate</p><p>Abstract: Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or ﬁnancial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classiﬁers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the ε-differential privacy deﬁnition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classiﬁcation. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classiﬁers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacypreserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance. Keywords: privacy, classiﬁcation, optimization, empirical risk minimization, support vector machines, logistic regression</p><p>4 0.39398396 <a title="1-lda-4" href="./jmlr-2011-Stochastic_Methods_forl1-regularized_Loss_Minimization.html">87 jmlr-2011-Stochastic Methods forl1-regularized Loss Minimization</a></p>
<p>Author: Shai Shalev-Shwartz, Ambuj Tewari</p><p>Abstract: We describe and analyze two stochastic methods for ℓ1 regularized loss minimization problems, such as the Lasso. The ﬁrst method updates the weight of a single feature at each iteration while the second method updates the entire weight vector but only uses a single training example at each iteration. In both methods, the choice of feature or example is uniformly at random. Our theoretical runtime analysis suggests that the stochastic methods should outperform state-of-the-art deterministic approaches, including their deterministic counterparts, when the size of the problem is large. We demonstrate the advantage of stochastic methods by experimenting with synthetic and natural data sets.1 Keywords: L1 regularization, optimization, coordinate descent, mirror descent, sparsity</p><p>5 0.39116713 <a title="1-lda-5" href="./jmlr-2011-Efficient_Learning_with_Partially_Observed_Attributes.html">29 jmlr-2011-Efficient Learning with Partially Observed Attributes</a></p>
<p>Author: Nicolò Cesa-Bianchi, Shai Shalev-Shwartz, Ohad Shamir</p><p>Abstract: We investigate three variants of budgeted learning, a setting in which the learner is allowed to access a limited number of attributes from training or test examples. In the “local budget” setting, where a constraint is imposed on the number of available attributes per training example, we design and analyze an efﬁcient algorithm for learning linear predictors that actively samples the attributes of each training instance. Our analysis bounds the number of additional examples sufﬁcient to compensate for the lack of full information on the training set. This result is complemented by a general lower bound for the easier “global budget” setting, where it is only the overall number of accessible training attributes that is being constrained. In the third, “prediction on a budget” setting, when the constraint is on the number of available attributes per test example, we show that there are cases in which there exists a linear predictor with zero error but it is statistically impossible to achieve arbitrary accuracy without full information on test examples. Finally, we run simple experiments on a digit recognition problem that reveal that our algorithm has a good performance against both partial information and full information baselines. Keywords: budgeted learning, statistical learning, linear predictors, learning with partial information, learning theory</p><p>6 0.3910836 <a title="1-lda-6" href="./jmlr-2011-On_Equivalence_Relationships_Between_Classification_and_Ranking_Algorithms.html">71 jmlr-2011-On Equivalence Relationships Between Classification and Ranking Algorithms</a></p>
<p>7 0.38626528 <a title="1-lda-7" href="./jmlr-2011-Group_Lasso_Estimation_of_High-dimensional_Covariance_Matrices.html">37 jmlr-2011-Group Lasso Estimation of High-dimensional Covariance Matrices</a></p>
<p>8 0.38615149 <a title="1-lda-8" href="./jmlr-2011-Exploiting_Best-Match_Equations_for_Efficient_Reinforcement_Learning.html">33 jmlr-2011-Exploiting Best-Match Equations for Efficient Reinforcement Learning</a></p>
<p>9 0.38483366 <a title="1-lda-9" href="./jmlr-2011-Robust_Approximate_Bilinear_Programming_for_Value_Function_Approximation.html">81 jmlr-2011-Robust Approximate Bilinear Programming for Value Function Approximation</a></p>
<p>10 0.38476512 <a title="1-lda-10" href="./jmlr-2011-High-dimensional_Covariance_Estimation_Based_On_Gaussian_Graphical_Models.html">39 jmlr-2011-High-dimensional Covariance Estimation Based On Gaussian Graphical Models</a></p>
<p>11 0.38075718 <a title="1-lda-11" href="./jmlr-2011-Hyper-Sparse_Optimal_Aggregation.html">40 jmlr-2011-Hyper-Sparse Optimal Aggregation</a></p>
<p>12 0.3783493 <a title="1-lda-12" href="./jmlr-2011-The_Sample_Complexity_of_Dictionary_Learning.html">91 jmlr-2011-The Sample Complexity of Dictionary Learning</a></p>
<p>13 0.37822634 <a title="1-lda-13" href="./jmlr-2011-An_Asymptotic_Behaviour_of_the_Marginal_Likelihood_for_General_Markov_Models.html">9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</a></p>
<p>14 0.37773114 <a title="1-lda-14" href="./jmlr-2011-Operator_Norm_Convergence_of_Spectral_Clustering_on_Level_Sets.html">74 jmlr-2011-Operator Norm Convergence of Spectral Clustering on Level Sets</a></p>
<p>15 0.3770918 <a title="1-lda-15" href="./jmlr-2011-Convergence_of_Distributed_Asynchronous_Learning_Vector_Quantization_Algorithms.html">19 jmlr-2011-Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms</a></p>
<p>16 0.37702617 <a title="1-lda-16" href="./jmlr-2011-Super-Linear_Convergence_of_Dual_Augmented_Lagrangian_Algorithm_for_Sparsity_Regularized_Estimation.html">89 jmlr-2011-Super-Linear Convergence of Dual Augmented Lagrangian Algorithm for Sparsity Regularized Estimation</a></p>
<p>17 0.37536302 <a title="1-lda-17" href="./jmlr-2011-X-Armed_Bandits.html">104 jmlr-2011-X-Armed Bandits</a></p>
<p>18 0.37331751 <a title="1-lda-18" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>19 0.37306944 <a title="1-lda-19" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<p>20 0.37068719 <a title="1-lda-20" href="./jmlr-2011-Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization.html">8 jmlr-2011-Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
