<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>32 jmlr-2011-Exploitation of Machine Learning Techniques in Modelling Phrase Movements for Machine Translation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-32" href="#">jmlr2011-32</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>32 jmlr-2011-Exploitation of Machine Learning Techniques in Modelling Phrase Movements for Machine Translation</h1>
<br/><p>Source: <a title="jmlr-2011-32-pdf" href="http://jmlr.org/papers/volume12/ni11a/ni11a.pdf">pdf</a></p><p>Author: Yizhao Ni, Craig Saunders, Sandor Szedmak, Mahesan Niranjan</p><p>Abstract: We propose a distance phrase reordering model (DPR) for statistical machine translation (SMT), where the aim is to learn the grammatical rules and context dependent changes using a phrase reordering classiﬁcation framework. We consider a variety of machine learning techniques, including state-of-the-art structured prediction methods. Techniques are compared and evaluated on a Chinese-English corpus, a language pair known for the high reordering characteristics which cannot be adequately captured with current models. In the reordering classiﬁcation task, the method signiﬁcantly outperforms the baseline against which it was tested, and further, when integrated as a component of the state-of-the-art machine translation system, MOSES, it achieves improvement in translation results. Keywords: statistical machine translation (SMT), phrase reordering, lexicalized reordering (LR), maximum entropy (ME), support vector machine (SVM), maximum margin regression (MMR) , max-margin structure learning (MMS)</p><p>Reference: <a title="jmlr-2011-32-reference" href="../jmlr2011_reference/jmlr-2011-Exploitation_of_Machine_Learning_Techniques_in_Modelling_Phrase_Movements_for_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In the reordering classiﬁcation task, the method signiﬁcantly outperforms the baseline against which it was tested, and further, when integrated as a component of the state-of-the-art machine translation system, MOSES, it achieves improvement in translation results. [sent-15, score-0.546]
</p><p>2 Keywords: statistical machine translation (SMT), phrase reordering, lexicalized reordering (LR), maximum entropy (ME), support vector machine (SVM), maximum margin regression (MMR) , max-margin structure learning (MMS)  1. [sent-16, score-1.027]
</p><p>3 , f jr ]  the source phrase sequence  and j denotes that f¯j is the j-th phrase in ¯I f ei ¯  the target phrase where e denotes the sequence of words [eil , . [sent-25, score-1.858]
</p><p>4 This view enables a probabilistic formulation in which the task becomes the maximisation of the posterior probability over all the phrase sequences in the target language. [sent-30, score-0.6]
</p><p>5 As the denominator only depends on the source phrase sequence ¯I , it is usually discarded and the solution is also represented as f I = arg max ¯I ˆ I ¯ e ˆ eI ∈E exp ∑m λm hm (f , e ) . [sent-36, score-0.619]
</p><p>6 A combination of several sub-models {hm } (see Figure 1), including a phrase translation probability model, a language model and a phrase reordering model are commonly used. [sent-38, score-1.579]
</p><p>7 1 Modelling Phrase Movements In this paper, we focus on developing a crucial component in statistical machine translation—the phrase reordering model. [sent-43, score-0.842]
</p><p>8 Word or phrase reordering is a common problem in bilingual translations arising from different grammatical structures. [sent-44, score-0.926]
</p><p>9 However, due to different linguistic environment it may come from, this Chinese possessive structure can express more sophisticated relationships which are inappropriate for the “NP1 ’s NP2 ” expression, for example, the “NP2 of NP1 ” sequence which requires phrase swapping (see Figure 2 (b)). [sent-46, score-0.606]
</p><p>10 In general, if the decoder “knows” the orders of phrase translations in the target 3  N I , S AUNDERS , S ZEDMAK AND N IRANJAN  Figure 2: Example: the distance phrase reordering in Chinese-to-English bilingual translation. [sent-47, score-1.56]
</p><p>11 This motivates investigations into, and development of models for, phrase reordering. [sent-50, score-0.568]
</p><p>12 Therefore, an ideal phrase reordering model should be able to handle arbitrary distance phrase movements. [sent-54, score-1.44]
</p><p>13 Within recently developed SMT systems, a simple phrase reordering model, named word distance-based reordering model (WDR), is commonly used (Och et al. [sent-56, score-1.162]
</p><p>14 This model deﬁnes a reordering distance for the j-th source phrase f¯j as (see Figure 3 for an illustration of this. [sent-59, score-0.9]
</p><p>15 , 2005), which splits the distance space into several segments, each of which represents a phrase reordering orientation o (see Figure 4). [sent-63, score-1.002]
</p><p>16 Then the phrase reordering probability for a phrase pair ( f¯j , ei ) is ¯ 4  E XPLOITATION OF ML T ECHNIQUES IN M ODELLING P HRASE M OVEMENTS FOR MT  Figure 4: The phrase reordering orientations: the three-class setup (top) and the ﬁve-class setup (bottom). [sent-64, score-2.39]
</p><p>17 predicted using maximum likelihood estimation (MLE) p o|( f¯j , ei ) = ¯  count(o, ( f¯j , ei )) ¯ , ′ , ( f , e )) ∑ count(o ¯j ¯i o′  where hd (¯I , eI ) = f ¯  ∑  p(o|( f¯j , ei )) is used to represent the cumulative cost of phrase move¯  ( f¯j ,ei )∈(¯I ,¯ I ) ¯ f e  ments. [sent-65, score-0.76]
</p><p>18 Adopting the idea of predicting phrase reordering orientations, researchers started exploiting context or grammatical content which may relate to phrase movements (Tillmann and Zhang, 2005; Xiong et al. [sent-67, score-1.542]
</p><p>19 In general, the distribution of phrase reorderings is expressed with a log-linear form p(o|( f¯j , ei ), wo ) = ¯  h(wT φ( f¯j , ei )) ¯ o Z( f¯j , ei ) ¯  (2)  ¯ with the normalisation term Z( f¯j , ei ) = ∑ h(wT φ( f¯j , ei )). [sent-69, score-1.091]
</p><p>20 To characterise phrase movements, a variety of linguistic features are proposed • Context features – word sequence (n-gram) features in (or around) the phrases. [sent-71, score-0.721]
</p><p>21 These indicator features are also used in the models above, and also in the context-aware phrase selection model of Gim´ nez and M` rquez (2007). [sent-75, score-0.607]
</p><p>22 e a • Statistical features – features such as the lexicalized reordering probability (Koehn et al. [sent-76, score-0.344]
</p><p>23 These real-value features are introduced by Tillmann and Zhang (2005) and are shown to be beneﬁcial in capturing the local phrase reordering information. [sent-78, score-0.865]
</p><p>24 We propose a distance phrase reordering model (DPR) that is also inspired by the orientation prediction framework (Koehn et al. [sent-81, score-1.002]
</p><p>25 (2006) and Zens and Ney (2006) we regard phrase movements as a classiﬁcation problem and use three multi-class learning agents—support vector machine (SVM), maximum margin regression (MMR) and max-margin structure learning (MMS) to perform the classiﬁcation. [sent-84, score-0.692]
</p><p>26 The ﬁrst is a comparison, in terms of classiﬁcation accuracy and computational efﬁciency, between different machine learning techniques for distance phrase movements in machine translation. [sent-89, score-0.697]
</p><p>27 Our second contribution is the demonstration that this paradigm is effective in the task of phrase movements, which is acknowledged as a challenging task in machine translation. [sent-91, score-0.568]
</p><p>28 2 The remainder of the paper is organised as follows: a general framework of the DPR model is given in Section 2, which speciﬁes the modelling of phrase movements and describes the motivations of using the three learning agents. [sent-95, score-0.667]
</p><p>29 Section 4 evaluates the performance of the DPR model with both phrase reordering classiﬁcation and machine translation experiments. [sent-97, score-0.978]
</p><p>30 1 Orientation Class Deﬁnition Following Koehn’s lexicalized reordering model, we use the phrase reordering distance d in (1) to measure phrase movements. [sent-116, score-1.738]
</p><p>31 , CO classes) and the possible start positions of phrases are grouped to make up a phrase orientation set O . [sent-119, score-0.742]
</p><p>32 2 Reordering Probability Model and Learning Agents ¯ ¯i Given a (source, target) phrase pair ( f¯jn , en ) ∈ ϒ with f¯j = [ f jl , . [sent-123, score-0.736]
</p><p>33 , eir ], the distance phrase reordering probability has the form p(o|( f¯jn , en ), {wo }) := ¯i  h wT φ( f¯jn , en ) ¯i o , T φ( f n , en ) ∑ h w′ ¯ ¯  o′ ∈O  o  j  (3)  i  where wo = [wo,0 , . [sent-129, score-1.476]
</p><p>34 In contrast to learning {wo }o∈O by maximising the entropy over all phrase pairs’ orientations max  {wo ∈O }  −  ¯i ¯i ∑ ∑ p(o| f¯jn , en , {wo }) log p(o| f¯jn , en , {wo })  ,  n ¯ ( f¯j ,en )∈ϒ o∈O i  we propose using maximum-margin based approaches to learn {wo }o∈O . [sent-134, score-0.916]
</p><p>35 Under this framework, three discriminative models are introduced, for different purposes of capturing phrase movements. [sent-135, score-0.601]
</p><p>36 1 T 2 wo wo +C  ∑  ξ( f¯n , en ) ¯  ( f¯n ,en )∈ϒ ¯ ¯ ¯ ϕ(on , o) wT φ( f¯n , en ) ≥ 1 − ξ( f¯n , en ) o ¯n , en ) ≥ 0, ∀( f¯n , en ) ∈ ϒ ξ( f ¯ ¯  ,  where ϕ(on , o) is an embedding function for the phrase orientation on , which is assigned 1 if on = o and −1 otherwise. [sent-146, score-1.712]
</p><p>37 The dependence on CO may cause computational problems, especially when the number of phrase orientations increase. [sent-149, score-0.644]
</p><p>38 2 M AXIMUM M ARGIN R EGRESSION (MMR) L EARNING A good agent for learning {wo }o∈O should adapt to the number of phrase orientations CO , enabling Equation (3) to extend to more classes in the future. [sent-152, score-0.682]
</p><p>39 ∑  o∈O  ∑ wT wo +C o  ∑  ξ( f¯n , en ) ¯  o∈O ( f¯n ,en )∈ϒ ¯ T φ( f n , en ) ≥ 1 − ξ( f n , en ) ¯ ¯ ¯ ¯ ϕ(on , o)wo  ,  ξ( f¯n , en ) ≥ 0, ∀( f¯n , en ) ∈ ϒ ¯ ¯  where ϕ(on , o) is an indicator function, which is assigned 1 if the phrase reordering orientation satisﬁes on = o and 0 otherwise. [sent-156, score-1.819]
</p><p>40 Furthermore, it allows the use of non-linear functions, going beyond the approach presented in Zens and Ney (2006), and is expected to provide more ﬂexibility in the expression of phrase features. [sent-162, score-0.568]
</p><p>41 However, as the phrase reordering orientations tend to be interdependent, introducing ﬂexible margins to separate different orientations sounds more reasonable. [sent-167, score-0.994]
</p><p>42 , △(on , o′ ) = 1, ∀o′ ), and it has been applied successfully to phrase translation tasks (Ni et al. [sent-179, score-0.704]
</p><p>43 As an alternative, the risk function (4) can be reformulated as a joint convex optimisation problem4 min max L({wo }o∈O , {zo }o∈O ) (6) { wo ≤R} {zo ∈Z }  with N  ¯ ∑ max 0, ∑ zn △(on , o) + wT φ( f¯n , en ) o o n=1 o∈O  n o = on  zo = −1  n o = on n = 1, . [sent-192, score-0.452]
</p><p>44 , 2005; Zens and Ney, 2006), we consider different kinds of information extracted from the phrase environment (see Table 3). [sent-204, score-0.59]
</p><p>45 , eir ] Word-class n-grams (subphrases) of the target phrase [eil , . [sent-218, score-0.629]
</p><p>46 , eir ]  Table 3: Features extracted from the phrase environment. [sent-221, score-0.619]
</p><p>47 ¯i Figure 6: Illustration of the phrase pair ( f¯jn , en ) (the word alignments are in black rectangle). [sent-223, score-0.768]
</p><p>48 The linguistic features are extracted from the target phrase and a window environment (blue shadow boxes) around the source phrase. [sent-224, score-0.711]
</p><p>49 In this way, the phrase features are distinguished by both the content u and its start position p. [sent-235, score-0.591]
</p><p>50 Figure 7 shows the context feature space created for all ﬁve phrase pairs in Figure 3 and the non-zero features for the phrase pair (“Xiang gang”, “Hong Kong”). [sent-237, score-1.159]
</p><p>51 The whole feature space contains 180 features and only 9 11  N I , S AUNDERS , S ZEDMAK AND N IRANJAN  features are non-zero for this phrase pair. [sent-238, score-0.614]
</p><p>52 The advantage of this feature expression is the collection of comprehensive linguistic information which may relate to phrase movements. [sent-239, score-0.606]
</p><p>53 2 Training and Application The training samples {on , ( f¯n , en )}N (phrase pairs up to length 8) for the DPR model are derived ¯ n=1 from a general phrase pair extraction procedure described in Koehn et al. [sent-242, score-0.75]
</p><p>54 A sub-model using source phrase f the above learning agents is then trained for each cluster. [sent-246, score-0.618]
</p><p>55 During the decoding, the DPR model ﬁnds the corresponding sub-model for a source phrase f¯j and generates the phrase reordering probability for each orientation class with Equation (3). [sent-250, score-1.568]
</p><p>56 However, due to many differences in word order (grammar) occurring for Chinese-English, this corpus contains many long distance phrase movements (see Figure 9). [sent-257, score-0.792]
</p><p>57 In this case, the phrase reordering model is expected to have more inﬂuence on the translation results, which makes this a suitable data set to analyse and demonstrate the effectiveness of our proposed DPR model. [sent-258, score-0.978]
</p><p>58 12  E XPLOITATION OF ML T ECHNIQUES IN M ODELLING P HRASE M OVEMENTS FOR MT  Figure 7: An example of the linguistic feature space created for all phrases in Figure 3 and the nonzero features for the phrase pair (“Xiang gang”, “Hong Kong”). [sent-275, score-0.673]
</p><p>59 13  N I , S AUNDERS , S ZEDMAK AND N IRANJAN  Figure 8: (a) A cluster for the source phrase “an quan” and its training samples (phrase pairs). [sent-279, score-0.626]
</p><p>60 Figure 9: The data statistics for the parallel texts of Hong Kong laws corpus (left) and the statistics of phrase reordering distance d for all consistent phrase pairs (up to length 8) extracted from the corpus (right). [sent-283, score-1.56]
</p><p>61 The right ﬁgure shows that short distance phrase movements (i. [sent-285, score-0.697]
</p><p>62 , d < 4) only take up 62% of the whole phrase movements. [sent-287, score-0.568]
</p><p>63 Since constructing a kernel using the sparse feature expression usually results in a very sparse kernel matrix where little similarity between samples is presented, SVM and MMR might not extract adequate information for modelling phrase movements. [sent-359, score-0.568]
</p><p>64 Meanwhile, we observed that results for forward phrase movements (i. [sent-538, score-0.667]
</p><p>65 We postulate this is because the reordering patterns for backward reorderings also depend on the orientation classes of the phrases nearby. [sent-543, score-0.484]
</p><p>66 For example, in Figure 3, the phrase “on a building” would be in “forward reordering” if it does not meet another “forward” phrase “a ﬁre has taken place”. [sent-544, score-1.136]
</p><p>67 This observation shows that a richer feature set including a potential orientation class of nearby phrases may help the reordering classiﬁcation and will be investigated in our future work. [sent-545, score-0.448]
</p><p>68 In particular, the number of larger circles which imply greater ambiguity in target translations is greater than that of larger rectangles; indicating MMS performs better in these ambiguous clusters, implying that the target translations also contain useful information about phrase movements. [sent-838, score-0.698]
</p><p>69 The ﬁrst (top) example demonstrates the beneﬁt from the target translations as by translating the Chinese source phrase “you guan” into different English words (i. [sent-843, score-0.661]
</p><p>70 , “relating”, “relates” or “relevant”), the phrase pairs usually have different but regular movements. [sent-845, score-0.568]
</p><p>71 Each circle/rectangle/point represents a cluster that contains all phrase pairs with a unique source phrase (e. [sent-847, score-1.164]
</p><p>72 21  N I , S AUNDERS , S ZEDMAK AND N IRANJAN  (middle) example shows a grammatical structure captured by the DPR model: in English the phrase “any of” usually stays in front of the subjects (or objects) it modiﬁes. [sent-854, score-0.619]
</p><p>73 In general, when given enough training samples a discriminative model such as DPR is able to capture various grammatical structures (modelled by phrase movements) better than a generative model. [sent-855, score-0.664]
</p><p>74 The ﬁnal (bottom) example depicts one type of phrase movements caused by the constant expressions in different languages (e. [sent-856, score-0.667]
</p><p>75 Hence, we conclude that the frequent phrase movements, whether caused by different grammatical structures or rule-based expressions, can be captured and the movement information is then passed on to an MT decoder to organise the target sentence structures. [sent-860, score-0.764]
</p><p>76 , 2005) that models phrase movements with the LR models as a baseline system. [sent-873, score-0.667]
</p><p>77 , a phrase translation probability model, a 4-gram language model (Stolcke, 2002) and the beam search decoder). [sent-876, score-0.737]
</p><p>78 In detail, all consistent phrase pairs (up to length 8) were extracted from the training sentence pairs and form the sample pool. [sent-879, score-0.696]
</p><p>79 To make use of the phrase reordering probabilities, two strategies were applied: one is to use the probabilities directly as the reordering cost (dotted line in Figure 1), which is also used in Xiong et al. [sent-881, score-1.116]
</p><p>80 (2006); Zens and Ney (2006); the other is to use them to adjust the word distance-based reordering cost (solid line in Figure 1), where the reordering cost of a sentence is computed as hd (¯I , eI ) = − f ¯  dm βp(o| f¯jm , eim ) ¯ )∈(¯I ,¯ I ) f e  ∑  ¯ ( f¯jm ,eim  (7)  with tuning parameter β. [sent-882, score-0.67]
</p><p>81 , the phrase movements are modelled in a precise way) and the orientation predictions are good enough, it is reasonable to use the reordering probabilities directly. [sent-885, score-1.071]
</p><p>82 But all long distance phrase movements will have the same reordering probabilities, which may mislead the SMT decoder and spoil the translations. [sent-893, score-1.008]
</p><p>83 In this case, the distance-sensitive expression (7) is able to ﬁll the deﬁciency of a small-class setup of DPR by penalising long distance phrase movements. [sent-894, score-0.635]
</p><p>84 Hence in the MT experiments, we used the ﬁve-class phrase reordering probabilities directly while the three-class probabilities were used to adjust the word distance-based reordering cost. [sent-895, score-1.162]
</p><p>85 In effect, WER and NIST weight more on word/phrase translation accuracy; BLEU biases towards translation ﬂuency; and METEOR emphasises translation adequacy (i. [sent-901, score-0.408]
</p><p>86 Since the three-class DPR achieves the same translation quality but it is faster, for the other MT tasks we only used DPR with three-class setup as the phrase reordering model. [sent-951, score-1.015]
</p><p>87 In particular, both systems produce similar predictions in sentence content (represented by similar WERs), but our MT system does better at phrase reordering and produces more ﬂuent translations (represented by better BLEUs). [sent-955, score-0.967]
</p><p>88 , the 20K-sentence task), DPR is unable to collect adequate phrase reordering information. [sent-958, score-0.842]
</p><p>89 Conclusions and Future Work We have proposed a distance phrase reordering (DPR) model using a classiﬁcation scheme and trained and evaluated it in a structured learning framework. [sent-961, score-0.893]
</p><p>90 The phrase reordering classiﬁcation tasks have shown that DPR is better at capturing phrase movements over the LR and ME models. [sent-962, score-1.509]
</p><p>91 For future work, we aim to improve the prediction accuracy of the ﬁve-class setup before applying it to an MT system, as DPR can be more powerful if it is able to provide more precise phrase positions for the decoder. [sent-967, score-0.605]
</p><p>92 We also aim to formulate the phrase reordering problem as an ordinal regression problem rather than a classiﬁcation one proposed in this paper. [sent-968, score-0.842]
</p><p>93 To consider adding a regularisation term, we upper bound the norm of each wo by wo ≤ R. [sent-982, score-0.363]
</p><p>94 ¯ ∑ zo [△(on , o) + wT φ( f¯n , en )] o n  o∈O ∑ zo = 0 n c on = −1 zn zo ≥ 0, o ∈ n  (9)  . [sent-986, score-0.422]
</p><p>95 × ZN , substituting (9) into (8) yields a natural saddle-point form min max L({wo }o∈O , {zo }o∈O )  wo ≤R z∈Z  with N  ¯ ∑ max 0, ∑ zn △(on , o) + wT φ( f¯n , en ) o o n=1 o∈O  n o = on  zo = −1  n o = on n = 1, . [sent-993, score-0.417]
</p><p>96 Ex2 2 2 ploiting this equality the inner product based constraints (10) can be transformed into an equivalent, norm based one ϕ(on )  2+ 2  Wφ( f¯n , en ) ¯  2 − 2 + 2ξ( f n , en ) ¯ ¯ 2  ≥ ϕ(on ) − Wφ( f¯n , en ) ¯  2 2  (11)  . [sent-1005, score-0.408]
</p><p>97 t ∀( f¯n , en ) ∈ ϒ ¯ n , en ) ≤ C, ∀( f n , en ) ∈ ϒ ¯ ¯ s. [sent-1012, score-0.408]
</p><p>98 Context–aware discriminative phrase selection for statistical machine e a translation. [sent-1073, score-0.601]
</p><p>99 Distance phrase reordering for moses - user manual and code guide. [sent-1112, score-0.909]
</p><p>100 Maximum entropy based phrase reordering model for statistical machine translation. [sent-1223, score-0.842]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('phrase', 0.568), ('dpr', 0.403), ('reordering', 0.274), ('mms', 0.251), ('mmr', 0.172), ('wo', 0.167), ('mt', 0.139), ('translation', 0.136), ('en', 0.136), ('orientation', 0.13), ('aunders', 0.108), ('iranjan', 0.108), ('zedmak', 0.108), ('echniques', 0.101), ('hrase', 0.101), ('odelling', 0.101), ('ovements', 0.101), ('xploitation', 0.101), ('movements', 0.099), ('lr', 0.087), ('zens', 0.086), ('zo', 0.086), ('smt', 0.079), ('orientations', 0.076), ('sentence', 0.076), ('koehn', 0.073), ('moses', 0.067), ('ei', 0.064), ('ney', 0.061), ('psl', 0.057), ('southampton', 0.057), ('tillmann', 0.05), ('corpus', 0.049), ('word', 0.046), ('phrases', 0.044), ('szedmak', 0.044), ('wt', 0.042), ('co', 0.04), ('ml', 0.038), ('linguistic', 0.038), ('agent', 0.038), ('svm', 0.037), ('setup', 0.037), ('decoder', 0.037), ('meteor', 0.036), ('reorderings', 0.036), ('linguistics', 0.036), ('optimisation', 0.035), ('jn', 0.035), ('discriminative', 0.033), ('language', 0.033), ('translations', 0.033), ('grammatical', 0.033), ('target', 0.032), ('jl', 0.032), ('jr', 0.03), ('distance', 0.03), ('ni', 0.03), ('training', 0.03), ('bleu', 0.029), ('eil', 0.029), ('eir', 0.029), ('och', 0.029), ('regularisation', 0.029), ('wer', 0.029), ('xiong', 0.029), ('zn', 0.028), ('source', 0.028), ('kong', 0.025), ('margin', 0.025), ('lexicalized', 0.024), ('hong', 0.023), ('hm', 0.023), ('saunders', 0.023), ('features', 0.023), ('agents', 0.022), ('nist', 0.022), ('translated', 0.022), ('extracted', 0.022), ('bristol', 0.022), ('giza', 0.022), ('iwslt', 0.022), ('wdr', 0.022), ('wtn', 0.022), ('structured', 0.021), ('classi', 0.021), ('alignments', 0.018), ('bilingual', 0.018), ('gim', 0.018), ('vickrey', 0.018), ('captured', 0.018), ('june', 0.018), ('dim', 0.018), ('coded', 0.018), ('acl', 0.018), ('system', 0.016), ('craig', 0.016), ('nez', 0.016), ('box', 0.016), ('extraction', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="32-tfidf-1" href="./jmlr-2011-Exploitation_of_Machine_Learning_Techniques_in_Modelling_Phrase_Movements_for_Machine_Translation.html">32 jmlr-2011-Exploitation of Machine Learning Techniques in Modelling Phrase Movements for Machine Translation</a></p>
<p>Author: Yizhao Ni, Craig Saunders, Sandor Szedmak, Mahesan Niranjan</p><p>Abstract: We propose a distance phrase reordering model (DPR) for statistical machine translation (SMT), where the aim is to learn the grammatical rules and context dependent changes using a phrase reordering classiﬁcation framework. We consider a variety of machine learning techniques, including state-of-the-art structured prediction methods. Techniques are compared and evaluated on a Chinese-English corpus, a language pair known for the high reordering characteristics which cannot be adequately captured with current models. In the reordering classiﬁcation task, the method signiﬁcantly outperforms the baseline against which it was tested, and further, when integrated as a component of the state-of-the-art machine translation system, MOSES, it achieves improvement in translation results. Keywords: statistical machine translation (SMT), phrase reordering, lexicalized reordering (LR), maximum entropy (ME), support vector machine (SVM), maximum margin regression (MMR) , max-margin structure learning (MMS)</p><p>2 0.16884427 <a title="32-tfidf-2" href="./jmlr-2011-Introduction_to_the_Special_Topic_on_Grammar_Induction%2C_Representation_of_Language_and_Language_Learning.html">46 jmlr-2011-Introduction to the Special Topic on Grammar Induction, Representation of Language and Language Learning</a></p>
<p>Author: Dorota Głowacka, John Shawe-Taylor, Alex Clark, Colin de la Higuera, Mark Johnson</p><p>Abstract: Grammar induction refers to the process of learning grammars and languages from data; this ﬁnds a variety of applications in syntactic pattern recognition, the modeling of natural language acquisition, data mining and machine translation. This special topic contains several papers presenting some of recent developments in the area of grammar induction and language learning, as applied to various problems in Natural Language Processing, including supervised and unsupervised parsing and statistical machine translation. Keywords: machine translation, Bayesian inference, grammar induction, natural language parsing</p><p>3 0.076910429 <a title="32-tfidf-3" href="./jmlr-2011-Natural_Language_Processing_%28Almost%29_from_Scratch.html">68 jmlr-2011-Natural Language Processing (Almost) from Scratch</a></p>
<p>Author: Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel Kuksa</p><p>Abstract: We propose a uniﬁed neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-speciﬁc engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements. Keywords: natural language processing, neural networks</p><p>4 0.058635529 <a title="32-tfidf-4" href="./jmlr-2011-Producing_Power-Law_Distributions_and_Damping_Word_Frequencies_with_Two-Stage_Language_Models.html">78 jmlr-2011-Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models</a></p>
<p>Author: Sharon Goldwater, Thomas L. Griffiths, Mark Johnson</p><p>Abstract: Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens. We present a framework for developing statistical models that can generically produce power laws, breaking generative models into two stages. The ﬁrst stage, the generator, can be any standard probabilistic model, while the second stage, the adaptor, transforms the word frequencies of this model to provide a closer match to natural language. We show that two commonly used Bayesian models, the Dirichlet-multinomial model and the Dirichlet process, can be viewed as special cases of our framework. We discuss two stochastic processes—the Chinese restaurant process and its two-parameter generalization based on the Pitman-Yor process—that can be used as adaptors in our framework to produce power-law distributions over word frequencies. We show that these adaptors justify common estimation procedures based on logarithmic or inverse-power transformations of empirical frequencies. In addition, taking the Pitman-Yor Chinese restaurant process as an adaptor justiﬁes the appearance of type frequencies in formal analyses of natural language and improves the performance of a model for unsupervised learning of morphology. Keywords: nonparametric Bayes, Pitman-Yor process, language model, unsupervised</p><p>5 0.043002099 <a title="32-tfidf-5" href="./jmlr-2011-Stochastic_Methods_forl1-regularized_Loss_Minimization.html">87 jmlr-2011-Stochastic Methods forl1-regularized Loss Minimization</a></p>
<p>Author: Shai Shalev-Shwartz, Ambuj Tewari</p><p>Abstract: We describe and analyze two stochastic methods for ℓ1 regularized loss minimization problems, such as the Lasso. The ﬁrst method updates the weight of a single feature at each iteration while the second method updates the entire weight vector but only uses a single training example at each iteration. In both methods, the choice of feature or example is uniformly at random. Our theoretical runtime analysis suggests that the stochastic methods should outperform state-of-the-art deterministic approaches, including their deterministic counterparts, when the size of the problem is large. We demonstrate the advantage of stochastic methods by experimenting with synthetic and natural data sets.1 Keywords: L1 regularization, optimization, coordinate descent, mirror descent, sparsity</p><p>6 0.034158695 <a title="32-tfidf-6" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>7 0.032213748 <a title="32-tfidf-7" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<p>8 0.028790424 <a title="32-tfidf-8" href="./jmlr-2011-Super-Linear_Convergence_of_Dual_Augmented_Lagrangian_Algorithm_for_Sparsity_Regularized_Estimation.html">89 jmlr-2011-Super-Linear Convergence of Dual Augmented Lagrangian Algorithm for Sparsity Regularized Estimation</a></p>
<p>9 0.028191043 <a title="32-tfidf-9" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>10 0.026551973 <a title="32-tfidf-10" href="./jmlr-2011-Convergence_of_Distributed_Asynchronous_Learning_Vector_Quantization_Algorithms.html">19 jmlr-2011-Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms</a></p>
<p>11 0.026202828 <a title="32-tfidf-11" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>12 0.025741417 <a title="32-tfidf-12" href="./jmlr-2011-Internal_Regret_with_Partial_Monitoring%3A_Calibration-Based_Optimal_Algorithms.html">45 jmlr-2011-Internal Regret with Partial Monitoring: Calibration-Based Optimal Algorithms</a></p>
<p>13 0.024595398 <a title="32-tfidf-13" href="./jmlr-2011-lp-Norm_Multiple_Kernel_Learning.html">105 jmlr-2011-lp-Norm Multiple Kernel Learning</a></p>
<p>14 0.023061447 <a title="32-tfidf-14" href="./jmlr-2011-Minimum_Description_Length_Penalization_for_Group_and_Multi-Task_Sparse_Learning.html">64 jmlr-2011-Minimum Description Length Penalization for Group and Multi-Task Sparse Learning</a></p>
<p>15 0.022373734 <a title="32-tfidf-15" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<p>16 0.022262573 <a title="32-tfidf-16" href="./jmlr-2011-Regression_on_Fixed-Rank_Positive_Semidefinite_Matrices%3A_A_Riemannian_Approach.html">80 jmlr-2011-Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach</a></p>
<p>17 0.019803025 <a title="32-tfidf-17" href="./jmlr-2011-Unsupervised_Supervised_Learning_II%3A_Margin-Based_Classification_Without_Labels.html">100 jmlr-2011-Unsupervised Supervised Learning II: Margin-Based Classification Without Labels</a></p>
<p>18 0.019443188 <a title="32-tfidf-18" href="./jmlr-2011-Discriminative_Learning_of_Bayesian_Networks_via_Factorized_Conditional_Log-Likelihood.html">25 jmlr-2011-Discriminative Learning of Bayesian Networks via Factorized Conditional Log-Likelihood</a></p>
<p>19 0.019339576 <a title="32-tfidf-19" href="./jmlr-2011-Large_Margin_Hierarchical_Classification_with_Mutually_Exclusive_Class_Membership.html">52 jmlr-2011-Large Margin Hierarchical Classification with Mutually Exclusive Class Membership</a></p>
<p>20 0.018951043 <a title="32-tfidf-20" href="./jmlr-2011-Information%2C_Divergence_and_Risk_for_Binary_Experiments.html">43 jmlr-2011-Information, Divergence and Risk for Binary Experiments</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.118), (1, -0.023), (2, -0.029), (3, -0.056), (4, -0.168), (5, -0.017), (6, 0.026), (7, 0.103), (8, -0.008), (9, -0.07), (10, -0.187), (11, -0.3), (12, -0.132), (13, -0.037), (14, 0.056), (15, -0.027), (16, 0.176), (17, -0.038), (18, 0.029), (19, 0.072), (20, -0.004), (21, -0.003), (22, 0.183), (23, 0.041), (24, -0.138), (25, -0.043), (26, -0.021), (27, 0.191), (28, 0.036), (29, -0.205), (30, -0.156), (31, 0.006), (32, 0.051), (33, -0.057), (34, -0.056), (35, -0.087), (36, 0.036), (37, 0.025), (38, 0.058), (39, 0.063), (40, 0.073), (41, 0.057), (42, -0.162), (43, -0.015), (44, -0.151), (45, -0.091), (46, 0.144), (47, -0.015), (48, -0.03), (49, -0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96892309 <a title="32-lsi-1" href="./jmlr-2011-Exploitation_of_Machine_Learning_Techniques_in_Modelling_Phrase_Movements_for_Machine_Translation.html">32 jmlr-2011-Exploitation of Machine Learning Techniques in Modelling Phrase Movements for Machine Translation</a></p>
<p>Author: Yizhao Ni, Craig Saunders, Sandor Szedmak, Mahesan Niranjan</p><p>Abstract: We propose a distance phrase reordering model (DPR) for statistical machine translation (SMT), where the aim is to learn the grammatical rules and context dependent changes using a phrase reordering classiﬁcation framework. We consider a variety of machine learning techniques, including state-of-the-art structured prediction methods. Techniques are compared and evaluated on a Chinese-English corpus, a language pair known for the high reordering characteristics which cannot be adequately captured with current models. In the reordering classiﬁcation task, the method signiﬁcantly outperforms the baseline against which it was tested, and further, when integrated as a component of the state-of-the-art machine translation system, MOSES, it achieves improvement in translation results. Keywords: statistical machine translation (SMT), phrase reordering, lexicalized reordering (LR), maximum entropy (ME), support vector machine (SVM), maximum margin regression (MMR) , max-margin structure learning (MMS)</p><p>2 0.80387306 <a title="32-lsi-2" href="./jmlr-2011-Introduction_to_the_Special_Topic_on_Grammar_Induction%2C_Representation_of_Language_and_Language_Learning.html">46 jmlr-2011-Introduction to the Special Topic on Grammar Induction, Representation of Language and Language Learning</a></p>
<p>Author: Dorota Głowacka, John Shawe-Taylor, Alex Clark, Colin de la Higuera, Mark Johnson</p><p>Abstract: Grammar induction refers to the process of learning grammars and languages from data; this ﬁnds a variety of applications in syntactic pattern recognition, the modeling of natural language acquisition, data mining and machine translation. This special topic contains several papers presenting some of recent developments in the area of grammar induction and language learning, as applied to various problems in Natural Language Processing, including supervised and unsupervised parsing and statistical machine translation. Keywords: machine translation, Bayesian inference, grammar induction, natural language parsing</p><p>3 0.27687141 <a title="32-lsi-3" href="./jmlr-2011-Producing_Power-Law_Distributions_and_Damping_Word_Frequencies_with_Two-Stage_Language_Models.html">78 jmlr-2011-Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models</a></p>
<p>Author: Sharon Goldwater, Thomas L. Griffiths, Mark Johnson</p><p>Abstract: Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens. We present a framework for developing statistical models that can generically produce power laws, breaking generative models into two stages. The ﬁrst stage, the generator, can be any standard probabilistic model, while the second stage, the adaptor, transforms the word frequencies of this model to provide a closer match to natural language. We show that two commonly used Bayesian models, the Dirichlet-multinomial model and the Dirichlet process, can be viewed as special cases of our framework. We discuss two stochastic processes—the Chinese restaurant process and its two-parameter generalization based on the Pitman-Yor process—that can be used as adaptors in our framework to produce power-law distributions over word frequencies. We show that these adaptors justify common estimation procedures based on logarithmic or inverse-power transformations of empirical frequencies. In addition, taking the Pitman-Yor Chinese restaurant process as an adaptor justiﬁes the appearance of type frequencies in formal analyses of natural language and improves the performance of a model for unsupervised learning of morphology. Keywords: nonparametric Bayes, Pitman-Yor process, language model, unsupervised</p><p>4 0.25544935 <a title="32-lsi-4" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>Author: Jennifer Gillenwater, Kuzman Ganchev, João Graça, Fernando Pereira, Ben Taskar</p><p>Abstract: A strong inductive bias is essential in unsupervised grammar induction. In this paper, we explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. We use part-of-speech (POS) tags to group dependencies by parent-child types and investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In experiments with 12 different languages, we achieve signiﬁcant gains in directed attachment accuracy over the standard expectation maximization (EM) baseline, with an average accuracy improvement of 6.5%, outperforming EM by at least 1% for 9 out of 12 languages. Furthermore, the new method outperforms models based on standard Bayesian sparsity-inducing parameter priors with an average improvement of 5% and positive gains of at least 1% for 9 out of 12 languages. On English text in particular, we show that our approach improves performance over other state-of-the-art techniques.</p><p>5 0.19392963 <a title="32-lsi-5" href="./jmlr-2011-Natural_Language_Processing_%28Almost%29_from_Scratch.html">68 jmlr-2011-Natural Language Processing (Almost) from Scratch</a></p>
<p>Author: Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel Kuksa</p><p>Abstract: We propose a uniﬁed neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-speciﬁc engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements. Keywords: natural language processing, neural networks</p><p>6 0.19034398 <a title="32-lsi-6" href="./jmlr-2011-Convergence_of_Distributed_Asynchronous_Learning_Vector_Quantization_Algorithms.html">19 jmlr-2011-Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms</a></p>
<p>7 0.17859352 <a title="32-lsi-7" href="./jmlr-2011-Universality%2C_Characteristic_Kernels_and_RKHS_Embedding_of_Measures.html">98 jmlr-2011-Universality, Characteristic Kernels and RKHS Embedding of Measures</a></p>
<p>8 0.17349158 <a title="32-lsi-8" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<p>9 0.16521424 <a title="32-lsi-9" href="./jmlr-2011-MSVMpack%3A_A_Multi-Class_Support_Vector_Machine_Package.html">62 jmlr-2011-MSVMpack: A Multi-Class Support Vector Machine Package</a></p>
<p>10 0.15817617 <a title="32-lsi-10" href="./jmlr-2011-Differentially_Private_Empirical_Risk_Minimization.html">22 jmlr-2011-Differentially Private Empirical Risk Minimization</a></p>
<p>11 0.1555834 <a title="32-lsi-11" href="./jmlr-2011-Structured_Variable_Selection_with_Sparsity-Inducing_Norms.html">88 jmlr-2011-Structured Variable Selection with Sparsity-Inducing Norms</a></p>
<p>12 0.14146522 <a title="32-lsi-12" href="./jmlr-2011-Adaptive_Exact_Inference_in_Graphical_Models.html">7 jmlr-2011-Adaptive Exact Inference in Graphical Models</a></p>
<p>13 0.13818508 <a title="32-lsi-13" href="./jmlr-2011-Parameter_Screening_and_Optimisation_for_ILP_using_Designed_Experiments.html">76 jmlr-2011-Parameter Screening and Optimisation for ILP using Designed Experiments</a></p>
<p>14 0.1376821 <a title="32-lsi-14" href="./jmlr-2011-Laplacian_Support_Vector_Machines__Trained_in_the_Primal.html">51 jmlr-2011-Laplacian Support Vector Machines  Trained in the Primal</a></p>
<p>15 0.13048866 <a title="32-lsi-15" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>16 0.12507147 <a title="32-lsi-16" href="./jmlr-2011-On_the_Relation_between_Realizable_and_Nonrealizable_Cases_of_the_Sequence_Prediction_Problem.html">72 jmlr-2011-On the Relation between Realizable and Nonrealizable Cases of the Sequence Prediction Problem</a></p>
<p>17 0.12399158 <a title="32-lsi-17" href="./jmlr-2011-Anechoic_Blind_Source_Separation_Using_Wigner_Marginals.html">10 jmlr-2011-Anechoic Blind Source Separation Using Wigner Marginals</a></p>
<p>18 0.11408709 <a title="32-lsi-18" href="./jmlr-2011-Kernel_Regression_in_the_Presence_of_Correlated_Errors.html">49 jmlr-2011-Kernel Regression in the Presence of Correlated Errors</a></p>
<p>19 0.11224819 <a title="32-lsi-19" href="./jmlr-2011-Large_Margin_Hierarchical_Classification_with_Mutually_Exclusive_Class_Membership.html">52 jmlr-2011-Large Margin Hierarchical Classification with Mutually Exclusive Class Membership</a></p>
<p>20 0.10828009 <a title="32-lsi-20" href="./jmlr-2011-Learning_Transformation_Models_for_Ranking_and_Survival_Analysis.html">56 jmlr-2011-Learning Transformation Models for Ranking and Survival Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.019), (9, 0.02), (10, 0.025), (24, 0.029), (31, 0.058), (32, 0.028), (41, 0.015), (60, 0.013), (65, 0.011), (73, 0.025), (78, 0.059), (82, 0.033), (90, 0.535), (99, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89891797 <a title="32-lda-1" href="./jmlr-2011-Weisfeiler-Lehman_Graph_Kernels.html">103 jmlr-2011-Weisfeiler-Lehman Graph Kernels</a></p>
<p>Author: Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, Karsten M. Borgwardt</p><p>Abstract: In this article, we propose a family of efﬁcient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be deﬁned based on this Weisfeiler-Lehman sequence of graphs, including a highly efﬁcient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classiﬁcation benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis. Keywords: graph kernels, graph classiﬁcation, similarity measures for graphs, Weisfeiler-Lehman algorithm c 2011 Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn and Karsten M. Borgwardt. S HERVASHIDZE , S CHWEITZER , VAN L EEUWEN , M EHLHORN AND B ORGWARDT</p><p>2 0.88792211 <a title="32-lda-2" href="./jmlr-2011-Models_of_Cooperative_Teaching_and_Learning.html">65 jmlr-2011-Models of Cooperative Teaching and Learning</a></p>
<p>Author: Sandra Zilles, Steffen Lange, Robert Holte, Martin Zinkevich</p><p>Abstract: While most supervised machine learning models assume that training examples are sampled at random or adversarially, this article is concerned with models of learning from a cooperative teacher that selects “helpful” training examples. The number of training examples a learner needs for identifying a concept in a given class C of possible target concepts (sample complexity of C) is lower in models assuming such teachers, that is, “helpful” examples can speed up the learning process. The problem of how a teacher and a learner can cooperate in order to reduce the sample complexity, yet without using “coding tricks”, has been widely addressed. Nevertheless, the resulting teaching and learning protocols do not seem to make the teacher select intuitively “helpful” examples. The two models introduced in this paper are built on what we call subset teaching sets and recursive teaching sets. They extend previous models of teaching by letting both the teacher and the learner exploit knowing that the partner is cooperative. For this purpose, we introduce a new notion of “coding trick”/“collusion”. We show how both resulting sample complexity measures (the subset teaching dimension and the recursive teaching dimension) can be arbitrarily lower than the classic teaching dimension and known variants thereof, without using coding tricks. For instance, monomials can be taught with only two examples independent of the number of variables. The subset teaching dimension turns out to be nonmonotonic with respect to subclasses of concept classes. We discuss why this nonmonotonicity might be inherent in many interesting cooperative teaching and learning scenarios. Keywords: teaching dimension, learning Boolean functions, interactive learning, collusion c 2011 Sandra Zilles, Steffen Lange, Robert Holte and Martin Zinkevich. Z ILLES , L ANGE , H OLTE AND Z INKEVICH</p><p>same-paper 3 0.87386739 <a title="32-lda-3" href="./jmlr-2011-Exploitation_of_Machine_Learning_Techniques_in_Modelling_Phrase_Movements_for_Machine_Translation.html">32 jmlr-2011-Exploitation of Machine Learning Techniques in Modelling Phrase Movements for Machine Translation</a></p>
<p>Author: Yizhao Ni, Craig Saunders, Sandor Szedmak, Mahesan Niranjan</p><p>Abstract: We propose a distance phrase reordering model (DPR) for statistical machine translation (SMT), where the aim is to learn the grammatical rules and context dependent changes using a phrase reordering classiﬁcation framework. We consider a variety of machine learning techniques, including state-of-the-art structured prediction methods. Techniques are compared and evaluated on a Chinese-English corpus, a language pair known for the high reordering characteristics which cannot be adequately captured with current models. In the reordering classiﬁcation task, the method signiﬁcantly outperforms the baseline against which it was tested, and further, when integrated as a component of the state-of-the-art machine translation system, MOSES, it achieves improvement in translation results. Keywords: statistical machine translation (SMT), phrase reordering, lexicalized reordering (LR), maximum entropy (ME), support vector machine (SVM), maximum margin regression (MMR) , max-margin structure learning (MMS)</p><p>4 0.85971272 <a title="32-lda-4" href="./jmlr-2011-Non-Parametric_Estimation_of_Topic_Hierarchies_from_Texts_with_Hierarchical_Dirichlet_Processes.html">70 jmlr-2011-Non-Parametric Estimation of Topic Hierarchies from Texts with Hierarchical Dirichlet Processes</a></p>
<p>Author: Elias Zavitsanos, Georgios Paliouras, George A. Vouros</p><p>Abstract: This paper presents hHDP, a hierarchical algorithm for representing a document collection as a hierarchy of latent topics, based on Dirichlet process priors. The hierarchical nature of the algorithm refers to the Bayesian hierarchy that it comprises, as well as to the hierarchy of the latent topics. hHDP relies on nonparametric Bayesian priors and it is able to infer a hierarchy of topics, without making any assumption about the depth of the learned hierarchy and the branching factor at each level. We evaluate the proposed method on real-world data sets in document modeling, as well as in ontology learning, and provide qualitative and quantitative evaluation results, showing that the model is robust, it models accurately the training data set and is able to generalize on held-out data. Keywords: hierarchical Dirichlet processes, probabilistic topic models, topic distributions, ontology learning from text, topic hierarchy</p><p>5 0.35322633 <a title="32-lda-5" href="./jmlr-2011-Natural_Language_Processing_%28Almost%29_from_Scratch.html">68 jmlr-2011-Natural Language Processing (Almost) from Scratch</a></p>
<p>Author: Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel Kuksa</p><p>Abstract: We propose a uniﬁed neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-speciﬁc engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements. Keywords: natural language processing, neural networks</p><p>6 0.34749204 <a title="32-lda-6" href="./jmlr-2011-Introduction_to_the_Special_Topic_on_Grammar_Induction%2C_Representation_of_Language_and_Language_Learning.html">46 jmlr-2011-Introduction to the Special Topic on Grammar Induction, Representation of Language and Language Learning</a></p>
<p>7 0.3284947 <a title="32-lda-7" href="./jmlr-2011-Producing_Power-Law_Distributions_and_Damping_Word_Frequencies_with_Two-Stage_Language_Models.html">78 jmlr-2011-Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models</a></p>
<p>8 0.3084721 <a title="32-lda-8" href="./jmlr-2011-Parameter_Screening_and_Optimisation_for_ILP_using_Designed_Experiments.html">76 jmlr-2011-Parameter Screening and Optimisation for ILP using Designed Experiments</a></p>
<p>9 0.28469422 <a title="32-lda-9" href="./jmlr-2011-Minimum_Description_Length_Penalization_for_Group_and_Multi-Task_Sparse_Learning.html">64 jmlr-2011-Minimum Description Length Penalization for Group and Multi-Task Sparse Learning</a></p>
<p>10 0.28070503 <a title="32-lda-10" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>11 0.2802088 <a title="32-lda-11" href="./jmlr-2011-An_Asymptotic_Behaviour_of_the_Marginal_Likelihood_for_General_Markov_Models.html">9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</a></p>
<p>12 0.27934816 <a title="32-lda-12" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>13 0.27366439 <a title="32-lda-13" href="./jmlr-2011-A_Family_of_Simple_Non-Parametric_Kernel_Learning_Algorithms.html">4 jmlr-2011-A Family of Simple Non-Parametric Kernel Learning Algorithms</a></p>
<p>14 0.27127665 <a title="32-lda-14" href="./jmlr-2011-Semi-Supervised_Learning_with_Measure_Propagation.html">84 jmlr-2011-Semi-Supervised Learning with Measure Propagation</a></p>
<p>15 0.26970011 <a title="32-lda-15" href="./jmlr-2011-Structured_Variable_Selection_with_Sparsity-Inducing_Norms.html">88 jmlr-2011-Structured Variable Selection with Sparsity-Inducing Norms</a></p>
<p>16 0.26803845 <a title="32-lda-16" href="./jmlr-2011-Logistic_Stick-Breaking_Process.html">61 jmlr-2011-Logistic Stick-Breaking Process</a></p>
<p>17 0.26636195 <a title="32-lda-17" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>18 0.2626738 <a title="32-lda-18" href="./jmlr-2011-Adaptive_Exact_Inference_in_Graphical_Models.html">7 jmlr-2011-Adaptive Exact Inference in Graphical Models</a></p>
<p>19 0.26182675 <a title="32-lda-19" href="./jmlr-2011-Operator_Norm_Convergence_of_Spectral_Clustering_on_Level_Sets.html">74 jmlr-2011-Operator Norm Convergence of Spectral Clustering on Level Sets</a></p>
<p>20 0.26086593 <a title="32-lda-20" href="./jmlr-2011-Unsupervised_Similarity-Based_Risk_Stratification_for_Cardiovascular_Events_Using_Long-Term_Time-Series_Data.html">99 jmlr-2011-Unsupervised Similarity-Based Risk Stratification for Cardiovascular Events Using Long-Term Time-Series Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
