<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>49 jmlr-2011-Kernel Regression in the Presence of Correlated Errors</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-49" href="#">jmlr2011-49</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>49 jmlr-2011-Kernel Regression in the Presence of Correlated Errors</h1>
<br/><p>Source: <a title="jmlr-2011-49-pdf" href="http://jmlr.org/papers/volume12/debrabanter11a/debrabanter11a.pdf">pdf</a></p><p>Author: Kris De Brabanter, Jos De Brabanter, Johan A.K. Suykens, Bart De Moor</p><p>Abstract: It is a well-known problem that obtaining a correct bandwidth and/or smoothing parameter in nonparametric regression is difﬁcult in the presence of correlated errors. There exist a wide variety of methods coping with this problem, but they all critically depend on a tuning procedure which requires accurate information about the correlation structure. We propose a bandwidth selection procedure based on bimodal kernels which successfully removes the correlation without requiring any prior knowledge about its structure and its parameters. Further, we show that the form of the kernel is very important when errors are correlated which is in contrast to the independent and identically distributed (i.i.d.) case. Finally, some extensions are proposed to use the proposed criterion in support vector machines and least squares support vector machines for regression. Keywords: nonparametric regression, correlated errors, bandwidth choice, cross-validation, shortrange dependence, bimodal kernel</p><p>Reference: <a title="jmlr-2011-49-reference" href="../jmlr2011_reference/jmlr-2011-Kernel_Regression_in_the_Presence_of_Correlated_Errors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Leuven Kasteelpark Arenberg 10 B-3001 Leuven, Belgium  Editor: Xiaotong Shen  Abstract It is a well-known problem that obtaining a correct bandwidth and/or smoothing parameter in nonparametric regression is difﬁcult in the presence of correlated errors. [sent-22, score-0.584]
</p><p>2 We propose a bandwidth selection procedure based on bimodal kernels which successfully removes the correlation without requiring any prior knowledge about its structure and its parameters. [sent-24, score-0.794]
</p><p>3 Further, we show that the form of the kernel is very important when errors are correlated which is in contrast to the independent and identically distributed (i. [sent-25, score-0.34]
</p><p>4 Keywords: nonparametric regression, correlated errors, bandwidth choice, cross-validation, shortrange dependence, bimodal kernel  1. [sent-30, score-0.927]
</p><p>5 Thus Yi can be considered as the sum of the value of the regression function at xi and some error ei with the expected value zero and the sequence {ei } is a covariance stationary process. [sent-44, score-0.271]
</p><p>6 Many techniques include a smoothing parameter and/or kernel bandwidth which controls the smoothness, bias and variance of the estimate. [sent-46, score-0.442]
</p><p>7 If the errors are positively (negatively) correlated, these methods will produce a small (large) bandwidth which results in a rough (oversmooth) estimate of the regression function. [sent-57, score-0.512]
</p><p>8 The error process is said to be short-range dependent if for some τ > 0, δ > 1 σ2 −iω of the errors satisﬁes and correlation function ρ(·), the spectral density H(ω) = 2π ∑∞ k=−∞ ρ(k)e (Cox, 1984) H(ω) ∼ τω−(1−δ) as ω → 0,  where A ∼ B denotes A is asymptotic equivalent to B. [sent-62, score-0.226]
</p><p>9 (2002) have proven consistency for the data-dependent kernel estimators, that is, correlated errors and/or correlation among the independent variables, there is no need to alter the kernel smoother by adding constraints. [sent-73, score-0.612]
</p><p>10 In fact, we will show in Section 3 that there exists 1956  K ERNEL R EGRESSION IN THE P RESENCE OF C ORRELATED E RRORS  a simple multiplicative relation between the bandwidth under correlation and the bandwidth under the i. [sent-75, score-0.689]
</p><p>11 Problems With Correlation Some quite fundamental problems occur when nonparametric regression is attempted in the presence of correlated errors. [sent-84, score-0.273]
</p><p>12 For all nonparametric regression techniques, the shape and the smoothness of the estimated function depends on a large extent on the speciﬁc value(s) chosen for the kernel bandwidth (and/or regularization parameter). [sent-85, score-0.534]
</p><p>13 Data-driven bandwidth selectors tend to be “fooled” by the correlation, interpreting it as reﬂecting the regression relationship and variance function. [sent-88, score-0.356]
</p><p>14 So, the cyclical pattern in positively correlated errors is viewed as a high frequency regression relationship with small variance, and the bandwidth is set small enough to track the cycles resulting in an undersmoothed ﬁtted regression curve. [sent-89, score-0.679]
</p><p>15 The alternating pattern above and below the true underlying function for negatively correlated errors is interpreted as a high variance, and the bandwidth is set high enough to smooth over the variability, producing an oversmoothed ﬁtted regression curve. [sent-90, score-0.598]
</p><p>16 For 200 equally spaced observations and a polynomial mean function m(x) = 300x3 (1 − x)3 , four progressively more correlated sets of errors were generated from the same vector of independent noise and added to the mean function. [sent-92, score-0.237]
</p><p>17 For each data set, two bandwidth selection methods were used: standard CV and a correlation-corrected CV (CC-CV) which is further discussed in Section 3. [sent-96, score-0.289]
</p><p>18 Table 1 and Figure 1 clearly show that when correlation increases, the bandwidth selected by CV becomes smaller and smaller, and the estimates become more undersmoothed. [sent-98, score-0.431]
</p><p>19 This type of undersmoothing behavior in the presence of positively correlated errors has been observed with most commonly used automated bandwidth selection methods (Altman, 1990; Hart, 1991; Opsomer, Wand & Yang, 2001; Kim et al. [sent-100, score-0.58]
</p><p>20 We make a clear distinction between kernel methods requiring no positive deﬁnite kernel and kernel methods requiring a positive deﬁnite kernel. [sent-104, score-0.393]
</p><p>21 13  Table 1: Summary of bandwidth selection for simulated data in Figure 1  5  4  4  Y , mn (x) ˆ  6  5  Y , mn (x) ˆ  6  3 2 1  3 2 1  0  0  −1  −1  −2 0  0. [sent-116, score-0.645]
</p><p>22 8  1  (b) α = 400  (a) Uncorrelated 6  5  5  4  4  Y , mn (x) ˆ  6  Y , mn (x) ˆ  0. [sent-124, score-0.356]
</p><p>23 6  (d) α = 100  Figure 1: Simulated data with four levels of AR(1) correlation, estimated with local linear regression; full line represents the estimates obtained with bandwidth selected by CV; dashed line represents the estimates obtained with bandwidth selected by our method. [sent-132, score-0.64]
</p><p>24 1 No Positive Deﬁnite Kernel Constraint To estimate the unknown regression function m, consider the Nadaraya-Watson (NW) kernel estimator (Nadaraya, 1964; Watson, 1964) deﬁned as K( x−xi )Yi h  n  mn (x) = ∑ ˆ  ∑n K( j=1  i=1  x−x j , h )  where h is the bandwidth of the kernel K. [sent-140, score-0.82]
</p><p>25 An optimal h can for example be found by minimizing the leave-one-out cross-validation (LCV) score function LCV(h) =  1 n (−i) ˆ ∑ Yi − mn (xi ; h) n i=1  2  ,  (2)  (−i)  where mn (xi ; h) denotes the leave-one-out estimator where point i is left out from the training. [sent-142, score-0.356]
</p><p>26 ˆ For notational ease, the dependence on the bandwidth h will be suppressed. [sent-143, score-0.289]
</p><p>27 Lemma 2 Assume that the errors are zero-mean, then the expected value of the LCV score function (2) is given by E[LCV(h)] =  1 E n  n  ∑  i=1  (−i)  m(xi ) − mn ˆ  (xi )  2  + σ2 −  2 n (−i) ˆ ∑ Cov mn (xi ), ei . [sent-145, score-0.586]
</p><p>28 Hart (1991) shows, if n → ∞, nh → ∞, nh5 → 0 and for positively correlated errors, that E[LCV(h)] ≈ σ2 + c/nh where c < 0 and c does not depend on the bandwidth. [sent-148, score-0.474]
</p><p>29 From this result it is clear that, by taking a kernel satisfying the condition K(0) = 0, the correlation structure is removed without requiring any prior information about its structure and (3) reduces to n 2 1 (−i) E[LCV(h)] = E ∑ m(xi ) − mn (xi ) + σ2 + o(n−1 h−1 ). [sent-161, score-0.42]
</p><p>30 (4) ˆ n i=1 Therefore, it is natural to use a bandwidth selection criterion based on a kernel satisfying K(0) = 0, deﬁned by ˆ hb = arg min LCV(h), h∈Qn  where Qn is a ﬁnite set of parameters. [sent-162, score-0.483]
</p><p>31 A major advantage of using a bandwidth selection criterion based on bimodal kernels is the fact that is more efﬁcient in removing the correlation than leave-(2l + 1)-out CV (Chu & Marron, 1991). [sent-167, score-0.794]
</p><p>32 ˆ Taking a bimodal kernel satisfying K(0) = 0 results in Equation (4) while leave-(2l + 1)-out CV with unimodal kernel K, under the conditions of Theorem 3, yields E[MCV(h)] =  1 E n  n  ∑  i=1  (−i)  m(xi ) − mn ˆ  (xi )  2  + σ2 −  ∞ 4K(0) ∑ γk + o(n−1h−1 ). [sent-169, score-0.806]
</p><p>33 nh − K(0) k=l+1  The formula above clearly shows that leave-(2l + 1)-out CV with unimodal kernel K cannot completely remove the correlation structure. [sent-170, score-0.575]
</p><p>34 1960  K ERNEL R EGRESSION IN THE P RESENCE OF C ORRELATED E RRORS  Another possibility of bandwidth selection under correlation, not based on bimodal kernels, is to estimate the covariance structure γ0 , γ1 , . [sent-172, score-0.649]
</p><p>35 A third approach is to derive an asymptotic bias-variance decomposition under the correlated error assumption of the kernel smoother. [sent-181, score-0.285]
</p><p>36 In this way and under certain conditions on the correlation function, plug-ins can be derived taking the correlation into account, see Hermann, Gasser & Kneip (1992), Opsomer, Wand & Yang (2001), Hall & Van Keilegom (2003), FranciscoFern´ ndez & Opsomer (2004) and Francisco-Fern´ ndez et al. [sent-182, score-0.324]
</p><p>37 a a (2006) proposed to estimate the error correlation nonparametrically without prior knowledge of the correlation structure. [sent-185, score-0.246]
</p><p>38 However, the following theorem reveals why a bimodal ˜ kernel K cannot be directly applied in these methods. [sent-189, score-0.467]
</p><p>39 ˜ ˜ Theorem 6 A bimodal kernel K, satisfying K(0) = 0, is never positive (semi) deﬁnite. [sent-190, score-0.467]
</p><p>40 Consequently, the previous strategy of using bimodal kernels cannot directly be applied to SVM ˆ and LS-SVM. [sent-192, score-0.394]
</p><p>41 A possible way to circumvent this obstacle, is to use the bandwidth hb , obtained from the bimodal kernel, as a pilot bandwidth selector for other data-driven selection procedures such as leave-(2l + 1)-out CV or block bootstrap bandwidth selector (Hall, Lahiri & Polzehl, 1995). [sent-193, score-1.514]
</p><p>42 Since the block bootstrap in Hall, Lahiri & Polzehl (1995) is based on two smoothers, that is, one is used to compute centered residuals and the other generates bootstrap data, the procedure is computationally costly. [sent-194, score-0.221]
</p><p>43 One possible method to select a value for l ˆ ˆ ˜ is to use hb as pilot bandwidth selector. [sent-204, score-0.384]
</p><p>44 Deﬁne a bimodal kernel K and assume hb is available, then 1961  D E B RABANTER , D E B RABANTER , S UYKENS AND D E M OOR  one can calculate ˜ K  n  mn (x) = ∑ ˆ  i=1  x−xi ˆ hb  ˜ ∑n K j=1  Yi x−x j ˆ hb  . [sent-205, score-0.834]
</p><p>45 (6)  From this result, the residuals are obtained by ei = Yi − mn (xi ), for i = 1, . [sent-206, score-0.349]
</p><p>46 (2006) stating that 1 n−q 1 n−q ei ei+q = ˆˆ ∑ ∑ ei ei+q + O(n−4/5 ). [sent-216, score-0.29]
</p><p>47 We then call Correlation-Corrected CV (CC-CV) the combination of ﬁnding l via bimodal kernels and using the obtained l in leave-(2l + 1)-out CV. [sent-218, score-0.394]
</p><p>48 3 Drawback of Using Bimodal Kernels Although bimodal kernels are very effective in removing the correlation structure, they have an inherent drawback. [sent-223, score-0.505]
</p><p>49 When using bimodal kernels to estimate the regression function m, the estimate mn will suffer from increased mean squared error (MSE). [sent-224, score-0.711]
</p><p>50 The following theorem indicates the ˆ asymptotic behavior of the MSE of mn (x) when the errors are covariance stationary. [sent-225, score-0.293]
</p><p>51 ˆ An asymptotic optimal constant or global bandwidth hAMISE , for m′′ (x) = 0, is the minimizer of the Asymptotic MISE (AMISE) AMISE(mn ) = ˆ  µ2 (K)h4 (m′′ (x))2 dx R(K)[σ2 + 2 ∑∞ γk ] k=1 2 + , 4 nh  w. [sent-229, score-0.651]
</p><p>52 (8)  ˆ ˆ We see that hAMISE is at least as big as the bandwidth for i. [sent-233, score-0.289]
</p><p>53 The following corollary shows that there is a simple multiplicative relationship between the asymptotic ˆ ˆ optimal bandwidth for dependent data hAMISE and bandwidth for independent data h0 . [sent-236, score-0.608]
</p><p>54 Thus, if the data are positively autocorrelated (ρ(k) ≥ 0 ∀k), the optimal bandwidth under correlation is larger than that for independent data. [sent-239, score-0.447]
</p><p>55 By taking hAMISE as in Equation (8), the corresponding asymptotic MISE is equal to 2/5  AMISE(mn ) = cDK n−4/5 , ˆ where c depends neither on the bandwidth nor on the kernel K and 2  DK = µ2 (K)R(K)2 =  u2 K(u) du 1963  K 2 (u) du  . [sent-244, score-0.532]
</p><p>56 Since an optimal kernel in this class cannot be found, we have to be satisﬁed with a so-called ε-optimal class of ˜ bimodal kernels Kε (u), with 0 < ε < 1, deﬁned as ˜ Kε (u) =  4 4 − 3ε − ε3  |u| ≥ ε; |u| < ε. [sent-251, score-0.525]
</p><p>57 Table 2 displays several possible bimodal kernel functions with their respective DK value compared to the Epanechnikov kernel. [sent-253, score-0.467]
</p><p>58 An illustration of the ε-optimal class of bimodal kernels is shown in Figure 2b. [sent-255, score-0.394]
</p><p>59 In theory, there is indeed a ˜ difference between kernel K3 and the ε-optimal class of bimodal kernels. [sent-322, score-0.467]
</p><p>60 case where the Epanechnikov kernel is the optimal kernel, but in practice the difference with say a Gaussian kernel is negligible. [sent-327, score-0.262]
</p><p>61 1 in the ﬁrst step and the Gaussian kernel in the second step) to the classical leave-one-out CV (LCV) based on the Epanechnikov (unimodal) kernel in the presence of correlation. [sent-333, score-0.297]
</p><p>62 By looking ˆ error, deﬁned as ASE = n ∑i=1 (m(xi ) − mn (xi )) at the average ASE, it is clear that the tuning parameters obtained by CC-CV result into better 1965  D E B RABANTER , D E B RABANTER , S UYKENS AND D E M OOR  estimates which are not inﬂuenced by the correlation. [sent-358, score-0.247]
</p><p>63 Figure 4 and Figure 5 show the CV surfaces 8  8 7 6 5  4  Y , mn (x) ˆ  Y , mn (x) ˆ  6  2  0  4 3 2 1 0  −2  −1 −4 0  0. [sent-362, score-0.356]
</p><p>64 For each φ, 100 replications of size n were made to report the average regularization parameter, bandwidth and ˜ average ASE for both methods. [sent-407, score-0.289]
</p><p>65 1 in the ﬁrst step and the Gaussian kernel in the second step for CC-CV and the Gaussian kernel for classical leave-one-out CV (LCV). [sent-410, score-0.262]
</p><p>66 On the other hand, for negatively correlated errors (φ < 0), both methods perform equally well. [sent-463, score-0.242]
</p><p>67 The reason why the effects from correlated errors is more outspoken for positive φ than for negative φ might be related to the fact that negatively correlated errors are seemingly hard to differentiate form i. [sent-464, score-0.451]
</p><p>68 γ ˆ 1968  K ERNEL R EGRESSION IN THE P RESENCE OF C ORRELATED E RRORS  5  4  4  Y , mn (x) ˆ  Y , mn (x) ˆ  5  3  3  2  2  1  1  0 0  0  0. [sent-639, score-0.356]
</p><p>69 8  1  (b)  ˜ Figure 6: Difference in the regression estimate (Nadaraya-Watson) (a) based on kernel K1 (full ˜ ˜ line) and K3 (dashed line). [sent-647, score-0.222]
</p><p>70 Due to the larger DK value of K1 , the estimate tends to be ˜ ˜ more wiggly compared to K3 ; (b) based on kernel K3 (full line) and ε-optimal kernel with ε = 0. [sent-648, score-0.324]
</p><p>71 01  ˜ K1  ˜ K3  ˜ Kǫ  ˜ K2  Figure 7: Boxplot of the asymptotic squared errors for the regression estimates based on bimodal ˜ ˜ ˜ ˜ kernels K1 , K2 , K3 and Kε with ε = 0. [sent-656, score-0.631]
</p><p>72 Conclusion We have introduced a new type of cross-validation procedure, based on bimodal kernels, in order to automatically remove the error correlation without requiring any prior knowledge about its structure. [sent-659, score-0.447]
</p><p>73 We have shown that the form of the kernel is very important when errors are correlated. [sent-660, score-0.216]
</p><p>74 As a consequence of the bimodal kernel choice the estimate suffers from increased mean squared error. [sent-665, score-0.515]
</p><p>75 Since an optimal bimodal kernel (in mean squared error sense) cannot be found we have proposed a ε-optimal class of bimodal kernels. [sent-666, score-0.827]
</p><p>76 Further, we have used the bandwidth of the bimodal kernel as pilot bandwidth selector for leave-(2l + 1)-out cross-validation. [sent-667, score-1.126]
</p><p>77 By taking this extra step, methods that require a positive deﬁnite kernel (SVM and LS-SVM) can be equipped with this technique of handling data in the presence of correlated errors since they require a positive deﬁnite kernel. [sent-668, score-0.375]
</p><p>78 Since Yi = m(xi ) + ei LCV(h) =  1 n ˆ ∑ Yi − m(−i) (xi ) n i=1  2  =  1 n (−i) (−i) ˆ ˆ ∑ m2 (xi ) + 2m(xi )ei + e2 − 2Yi mn (xi ) + mn (xi ) i n i=1  =  1 n (−i) ˆ ∑ m(xi ) − mn (xi ) n i=1 +  2  +  1 n 2 ∑e n i=1 i  2 n (−i) ˆ ∑ m(xi ) − mn (xi ) ei . [sent-697, score-1.002]
</p><p>79 n i=1 1970  2  K ERNEL R EGRESSION IN THE P RESENCE OF C ORRELATED E RRORS  Taking expectations, yields 1 E[LCV(h)] = E n  n  ∑  i=1  (−i)  m(xi ) − mn ˆ  (xi )  2  + σ2 −  2 n (−i) ˆ ∑ Cov mn (xi ), ei . [sent-698, score-0.501]
</p><p>80 Proof of Theorem 3 Consider only the last term of the expected LCV (Lemma 2), that is, A(h) = −  2 n (−i) ˆ ∑ Cov mn (xi ), ei . [sent-700, score-0.323]
</p><p>81 n i=1 (−i)  Plugging in the Nadaraya-Watson kernel smoother for mn (xi ) in the term above yields ˆ   xi −x j n K Yj h 2 n  A(h) = − ∑ Cov  ∑ n xi −xl , ei . [sent-701, score-0.602]
</p><p>82 n i=1 j=i ∑ j=i K xi −xl h  By slightly rewriting the denominator and using the covariance stationary property of the errors (Deﬁnition 1), the above equation can be written as K 2 n n A(h) = − ∑ ∑ n n i=1 j=i ∑ j=1 K  xi −x j h xi −xl h  − K(0)  γ|i− j| . [sent-703, score-0.262]
</p><p>83 The ﬁrst term of the denominator can be written as n  ∑K  j=1  xi − xl h  = nh fˆ(xi ) = nh f (xi ) + nh( fˆ(xi ) − f (xi )). [sent-705, score-0.728]
</p><p>84 Hence, for n → ∞, the following approximation is valid nh fˆ(xi ) ≈ nh f (xi ). [sent-708, score-0.606]
</p><p>85 nh  γ|i− j|  n−1 k Next, we show that ∑k=1 n−k K nh γk = K(0) ∑∞ γk + o(n−1 h−1 ) for n → ∞. [sent-710, score-0.606]
</p><p>86 Then, for K(0) ≥ 0 and C1 > C2 , we establish the following upperbound n−1  ∑  k=1  n−k K n  k γk ≤ nh ≤  n−1  ∑  k=1 n−1  1−  k n  K(0) +C1 n−1  k γk nh  k  ∑ K(0)γk + ∑ C1 nh γk . [sent-712, score-0.909]
</p><p>87 nh nh k=1 k=1  C1 ∑ Hence, n−1  ∑  k=1  n−k K n  ∞ k γk ≤ K(0) ∑ γk + o(n−1 h−1 ). [sent-714, score-0.606]
</p><p>88 nh k=1  For the construction of the lower bound, assume ﬁrst that C2 < 0 and K(0) ≥ 0 then n−1  ∑  k=1  n−k K n  Since C2 < 0, it follows that k ≤ n−1  ∑  k=1  k 1− n  n−1 k k γk ≥ ∑ 1 − nh n k=1  K(0) −C2 nh  k K(0) +C2 nh  K(0) +C2  k nh  γk . [sent-715, score-1.515]
</p><p>89 +  and therefore min n−1, K(0) nh −C  γk = +  ∑  k=1  2  1−  k n  K(0) +C2  k γk . [sent-716, score-0.303]
</p><p>90 nh  Analogous to deriving the upper bound, we obtain for n → ∞ n−1  ∑  k=1  n−k K n  ∞ k γk ≥ K(0) ∑ γk + o(n−1 h−1 ). [sent-717, score-0.303]
</p><p>91 nh k=1  In the second case, that is, C2 > 0, the same lower bound can be obtained. [sent-718, score-0.303]
</p><p>92 nh k=1  1972  K ERNEL R EGRESSION IN THE P RESENCE OF C ORRELATED E RRORS  Appendix C. [sent-720, score-0.303]
</p><p>93 ˜ • Suppose there exists a positive deﬁnite bimodal kernel K. [sent-756, score-0.467]
</p><p>94 Consequently, a positive deﬁnite bimodal kernel K cannot exist. [sent-760, score-0.467]
</p><p>95 ˜ • Suppose there exists a positive semi-deﬁnite bimodal kernel K. [sent-761, score-0.467]
</p><p>96 A simple bootstrap bandwidth selector for local polynomial ﬁtting. [sent-839, score-0.415]
</p><p>97 A plug-in bandwidth selector a a for local polynomial regression estimator with correlated errors. [sent-861, score-0.529]
</p><p>98 On bandwidth choice in nonparametric regression with both short- and long-range dependent errors. [sent-871, score-0.403]
</p><p>99 Choice of bandwidth for kernel regression when residuals are correlated. [sent-928, score-0.513]
</p><p>100 Using bimodal kernel inference in nonparametric regression with correlated errors. [sent-958, score-0.705]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lcv', 0.394), ('bimodal', 0.336), ('nh', 0.303), ('bandwidth', 0.289), ('rabanter', 0.28), ('cv', 0.211), ('mn', 0.178), ('ei', 0.145), ('oor', 0.14), ('kernel', 0.131), ('orrelated', 0.127), ('resence', 0.127), ('rrors', 0.127), ('correlated', 0.124), ('uykens', 0.119), ('lahiri', 0.114), ('correlation', 0.111), ('epanechnikov', 0.102), ('hamise', 0.102), ('opsomer', 0.102), ('ase', 0.097), ('kepa', 0.089), ('errors', 0.085), ('bootstrap', 0.077), ('mcv', 0.076), ('leuven', 0.076), ('egression', 0.067), ('regression', 0.067), ('ernel', 0.064), ('ar', 0.064), ('mise', 0.064), ('wand', 0.064), ('xl', 0.063), ('bandwidths', 0.063), ('hb', 0.063), ('dk', 0.062), ('xi', 0.059), ('kernels', 0.058), ('suykens', 0.058), ('brabanter', 0.054), ('ndez', 0.051), ('nsch', 0.051), ('polzehl', 0.051), ('sectional', 0.051), ('selector', 0.049), ('positively', 0.047), ('nonparametric', 0.047), ('hall', 0.044), ('kuleuven', 0.043), ('cov', 0.043), ('du', 0.041), ('block', 0.041), ('hart', 0.04), ('altman', 0.038), ('amise', 0.038), ('kris', 0.038), ('sbo', 0.038), ('wiggly', 0.038), ('tuning', 0.038), ('xk', 0.038), ('kim', 0.036), ('park', 0.036), ('presence', 0.035), ('negatively', 0.033), ('gasser', 0.032), ('johan', 0.032), ('jos', 0.032), ('pilot', 0.032), ('estimates', 0.031), ('asymptotic', 0.03), ('unimodal', 0.03), ('smoother', 0.03), ('hermann', 0.029), ('autocorrelation', 0.029), ('dx', 0.029), ('noise', 0.028), ('residuals', 0.026), ('beran', 0.025), ('beveridge', 0.025), ('breakdown', 0.025), ('davison', 0.025), ('debrabanter', 0.025), ('differencing', 0.025), ('horowitz', 0.025), ('kneip', 0.025), ('legitimately', 0.025), ('mpc', 0.025), ('optec', 0.025), ('reprinted', 0.025), ('rnh', 0.025), ('sen', 0.025), ('smoothers', 0.025), ('belgium', 0.025), ('squares', 0.025), ('squared', 0.024), ('estimate', 0.024), ('fan', 0.023), ('chu', 0.022), ('eigenvalues', 0.022), ('smoothing', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="49-tfidf-1" href="./jmlr-2011-Kernel_Regression_in_the_Presence_of_Correlated_Errors.html">49 jmlr-2011-Kernel Regression in the Presence of Correlated Errors</a></p>
<p>Author: Kris De Brabanter, Jos De Brabanter, Johan A.K. Suykens, Bart De Moor</p><p>Abstract: It is a well-known problem that obtaining a correct bandwidth and/or smoothing parameter in nonparametric regression is difﬁcult in the presence of correlated errors. There exist a wide variety of methods coping with this problem, but they all critically depend on a tuning procedure which requires accurate information about the correlation structure. We propose a bandwidth selection procedure based on bimodal kernels which successfully removes the correlation without requiring any prior knowledge about its structure and its parameters. Further, we show that the form of the kernel is very important when errors are correlated which is in contrast to the independent and identically distributed (i.i.d.) case. Finally, some extensions are proposed to use the proposed criterion in support vector machines and least squares support vector machines for regression. Keywords: nonparametric regression, correlated errors, bandwidth choice, cross-validation, shortrange dependence, bimodal kernel</p><p>2 0.068470873 <a title="49-tfidf-2" href="./jmlr-2011-Smoothness%2C_Disagreement_Coefficient%2C_and_the_Label_Complexity_of_Agnostic_Active_Learning.html">85 jmlr-2011-Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning</a></p>
<p>Author: Liwei Wang</p><p>Abstract: We study pool-based active learning in the presence of noise, that is, the agnostic setting. It is known that the effectiveness of agnostic active learning depends on the learning problem and the hypothesis space. Although there are many cases on which active learning is very useful, it is also easy to construct examples that no active learning algorithm can have an advantage. Previous works have shown that the label complexity of active learning relies on the disagreement coefﬁcient which often characterizes the intrinsic difﬁculty of the learning problem. In this paper, we study the disagreement coefﬁcient of classiﬁcation problems for which the classiﬁcation boundary is smooth and the data distribution has a density that can be bounded by a smooth function. We prove upper and lower bounds for the disagreement coefﬁcients of both ﬁnitely and inﬁnitely smooth problems. Combining with existing results, it shows that active learning is superior to passive supervised learning for smooth problems. Keywords: active learning, disagreement coefﬁcient, label complexity, smooth function</p><p>3 0.064111173 <a title="49-tfidf-3" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>Author: Adam D. Bull</p><p>Abstract: In the efﬁcient global optimization problem, we minimize an unknown function f , using as few observations f (x) as possible. It can be considered a continuum-armed-bandit problem, with noiseless data, and simple regret. Expected-improvement algorithms are perhaps the most popular methods for solving the problem; in this paper, we provide theoretical results on their asymptotic behaviour. Implementing these algorithms requires a choice of Gaussian-process prior, which determines an associated space of functions, its reproducing-kernel Hilbert space (RKHS). When the prior is ﬁxed, expected improvement is known to converge on the minimum of any function in its RKHS. We provide convergence rates for this procedure, optimal for functions of low smoothness, and describe a modiﬁed algorithm attaining optimal rates for smoother functions. In practice, however, priors are typically estimated sequentially from the data. For standard estimators, we show this procedure may never ﬁnd the minimum of f . We then propose alternative estimators, chosen to minimize the constants in the rate of convergence, and show these estimators retain the convergence rates of a ﬁxed prior. Keywords: convergence rates, efﬁcient global optimization, expected improvement, continuumarmed bandit, Bayesian optimization</p><p>4 0.0595529 <a title="49-tfidf-4" href="./jmlr-2011-Locally_Defined_Principal_Curves_and_Surfaces.html">60 jmlr-2011-Locally Defined Principal Curves and Surfaces</a></p>
<p>Author: Umut Ozertem, Deniz Erdogmus</p><p>Abstract: Principal curves are deﬁned as self-consistent smooth curves passing through the middle of the data, and they have been used in many applications of machine learning as a generalization, dimensionality reduction and a feature extraction tool. We redeﬁne principal curves and surfaces in terms of the gradient and the Hessian of the probability density estimate. This provides a geometric understanding of the principal curves and surfaces, as well as a unifying view for clustering, principal curve ﬁtting and manifold learning by regarding those as principal manifolds of different intrinsic dimensionalities. The theory does not impose any particular density estimation method can be used with any density estimator that gives continuous ﬁrst and second derivatives. Therefore, we ﬁrst present our principal curve/surface deﬁnition without assuming any particular density estimation method. Afterwards, we develop practical algorithms for the commonly used kernel density estimation (KDE) and Gaussian mixture models (GMM). Results of these algorithms are presented in notional data sets as well as real applications with comparisons to other approaches in the principal curve literature. All in all, we present a novel theoretical understanding of principal curves and surfaces, practical algorithms as general purpose machine learning tools, and applications of these algorithms to several practical problems. Keywords: unsupervised learning, dimensionality reduction, principal curves, principal surfaces, subspace constrained mean-shift</p><p>5 0.056793302 <a title="49-tfidf-5" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>Author: Brian McFee, Gert Lanckriet</p><p>Abstract: In many applications involving multi-media data, the deﬁnition of similarity between items is integral to several key tasks, including nearest-neighbor retrieval, classiﬁcation, and recommendation. Data in such regimes typically exhibits multiple modalities, such as acoustic and visual content of video. Integrating such heterogeneous data to form a holistic similarity space is therefore a key challenge to be overcome in many real-world applications. We present a novel multiple kernel learning technique for integrating heterogeneous data into a single, uniﬁed similarity space. Our algorithm learns an optimal ensemble of kernel transformations which conform to measurements of human perceptual similarity, as expressed by relative comparisons. To cope with the ubiquitous problems of subjectivity and inconsistency in multimedia similarity, we develop graph-based techniques to ﬁlter similarity measurements, resulting in a simpliﬁed and robust training procedure. Keywords: multiple kernel learning, metric learning, similarity</p><p>6 0.051774438 <a title="49-tfidf-6" href="./jmlr-2011-lp-Norm_Multiple_Kernel_Learning.html">105 jmlr-2011-lp-Norm Multiple Kernel Learning</a></p>
<p>7 0.048662145 <a title="49-tfidf-7" href="./jmlr-2011-Learning_Transformation_Models_for_Ranking_and_Survival_Analysis.html">56 jmlr-2011-Learning Transformation Models for Ranking and Survival Analysis</a></p>
<p>8 0.043984327 <a title="49-tfidf-8" href="./jmlr-2011-Weisfeiler-Lehman_Graph_Kernels.html">103 jmlr-2011-Weisfeiler-Lehman Graph Kernels</a></p>
<p>9 0.041841194 <a title="49-tfidf-9" href="./jmlr-2011-Forest_Density_Estimation.html">35 jmlr-2011-Forest Density Estimation</a></p>
<p>10 0.041638393 <a title="49-tfidf-10" href="./jmlr-2011-Multiple_Kernel_Learning_Algorithms.html">66 jmlr-2011-Multiple Kernel Learning Algorithms</a></p>
<p>11 0.035064824 <a title="49-tfidf-11" href="./jmlr-2011-Dirichlet_Process_Mixtures_of_Generalized_Linear_Models.html">24 jmlr-2011-Dirichlet Process Mixtures of Generalized Linear Models</a></p>
<p>12 0.035050552 <a title="49-tfidf-12" href="./jmlr-2011-Faster_Algorithms_for_Max-Product_Message-Passing.html">34 jmlr-2011-Faster Algorithms for Max-Product Message-Passing</a></p>
<p>13 0.034314267 <a title="49-tfidf-13" href="./jmlr-2011-Information_Rates_of_Nonparametric_Gaussian_Process_Methods.html">44 jmlr-2011-Information Rates of Nonparametric Gaussian Process Methods</a></p>
<p>14 0.034115877 <a title="49-tfidf-14" href="./jmlr-2011-Learning_a_Robust_Relevance_Model_for_Search_Using_Kernel_Methods.html">57 jmlr-2011-Learning a Robust Relevance Model for Search Using Kernel Methods</a></p>
<p>15 0.033945348 <a title="49-tfidf-15" href="./jmlr-2011-Universality%2C_Characteristic_Kernels_and_RKHS_Embedding_of_Measures.html">98 jmlr-2011-Universality, Characteristic Kernels and RKHS Embedding of Measures</a></p>
<p>16 0.03329888 <a title="49-tfidf-16" href="./jmlr-2011-Regression_on_Fixed-Rank_Positive_Semidefinite_Matrices%3A_A_Riemannian_Approach.html">80 jmlr-2011-Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach</a></p>
<p>17 0.032291662 <a title="49-tfidf-17" href="./jmlr-2011-High-dimensional_Covariance_Estimation_Based_On_Gaussian_Graphical_Models.html">39 jmlr-2011-High-dimensional Covariance Estimation Based On Gaussian Graphical Models</a></p>
<p>18 0.030485027 <a title="49-tfidf-18" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>19 0.02967933 <a title="49-tfidf-19" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<p>20 0.029516501 <a title="49-tfidf-20" href="./jmlr-2011-Minimum_Description_Length_Penalization_for_Group_and_Multi-Task_Sparse_Learning.html">64 jmlr-2011-Minimum Description Length Penalization for Group and Multi-Task Sparse Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.156), (1, -0.039), (2, 0.045), (3, -0.083), (4, 0.044), (5, -0.031), (6, -0.073), (7, -0.023), (8, -0.155), (9, -0.016), (10, 0.039), (11, 0.049), (12, 0.05), (13, 0.004), (14, 0.054), (15, -0.024), (16, 0.13), (17, 0.008), (18, -0.058), (19, 0.017), (20, -0.075), (21, 0.057), (22, 0.144), (23, 0.14), (24, -0.042), (25, 0.014), (26, 0.049), (27, -0.159), (28, 0.058), (29, -0.048), (30, 0.002), (31, 0.002), (32, -0.004), (33, 0.053), (34, 0.323), (35, 0.045), (36, -0.089), (37, -0.284), (38, -0.017), (39, -0.172), (40, 0.013), (41, 0.117), (42, -0.354), (43, 0.044), (44, -0.007), (45, 0.05), (46, 0.182), (47, 0.017), (48, 0.042), (49, 0.154)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92964846 <a title="49-lsi-1" href="./jmlr-2011-Kernel_Regression_in_the_Presence_of_Correlated_Errors.html">49 jmlr-2011-Kernel Regression in the Presence of Correlated Errors</a></p>
<p>Author: Kris De Brabanter, Jos De Brabanter, Johan A.K. Suykens, Bart De Moor</p><p>Abstract: It is a well-known problem that obtaining a correct bandwidth and/or smoothing parameter in nonparametric regression is difﬁcult in the presence of correlated errors. There exist a wide variety of methods coping with this problem, but they all critically depend on a tuning procedure which requires accurate information about the correlation structure. We propose a bandwidth selection procedure based on bimodal kernels which successfully removes the correlation without requiring any prior knowledge about its structure and its parameters. Further, we show that the form of the kernel is very important when errors are correlated which is in contrast to the independent and identically distributed (i.i.d.) case. Finally, some extensions are proposed to use the proposed criterion in support vector machines and least squares support vector machines for regression. Keywords: nonparametric regression, correlated errors, bandwidth choice, cross-validation, shortrange dependence, bimodal kernel</p><p>2 0.38811308 <a title="49-lsi-2" href="./jmlr-2011-Smoothness%2C_Disagreement_Coefficient%2C_and_the_Label_Complexity_of_Agnostic_Active_Learning.html">85 jmlr-2011-Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning</a></p>
<p>Author: Liwei Wang</p><p>Abstract: We study pool-based active learning in the presence of noise, that is, the agnostic setting. It is known that the effectiveness of agnostic active learning depends on the learning problem and the hypothesis space. Although there are many cases on which active learning is very useful, it is also easy to construct examples that no active learning algorithm can have an advantage. Previous works have shown that the label complexity of active learning relies on the disagreement coefﬁcient which often characterizes the intrinsic difﬁculty of the learning problem. In this paper, we study the disagreement coefﬁcient of classiﬁcation problems for which the classiﬁcation boundary is smooth and the data distribution has a density that can be bounded by a smooth function. We prove upper and lower bounds for the disagreement coefﬁcients of both ﬁnitely and inﬁnitely smooth problems. Combining with existing results, it shows that active learning is superior to passive supervised learning for smooth problems. Keywords: active learning, disagreement coefﬁcient, label complexity, smooth function</p><p>3 0.34274995 <a title="49-lsi-3" href="./jmlr-2011-Locally_Defined_Principal_Curves_and_Surfaces.html">60 jmlr-2011-Locally Defined Principal Curves and Surfaces</a></p>
<p>Author: Umut Ozertem, Deniz Erdogmus</p><p>Abstract: Principal curves are deﬁned as self-consistent smooth curves passing through the middle of the data, and they have been used in many applications of machine learning as a generalization, dimensionality reduction and a feature extraction tool. We redeﬁne principal curves and surfaces in terms of the gradient and the Hessian of the probability density estimate. This provides a geometric understanding of the principal curves and surfaces, as well as a unifying view for clustering, principal curve ﬁtting and manifold learning by regarding those as principal manifolds of different intrinsic dimensionalities. The theory does not impose any particular density estimation method can be used with any density estimator that gives continuous ﬁrst and second derivatives. Therefore, we ﬁrst present our principal curve/surface deﬁnition without assuming any particular density estimation method. Afterwards, we develop practical algorithms for the commonly used kernel density estimation (KDE) and Gaussian mixture models (GMM). Results of these algorithms are presented in notional data sets as well as real applications with comparisons to other approaches in the principal curve literature. All in all, we present a novel theoretical understanding of principal curves and surfaces, practical algorithms as general purpose machine learning tools, and applications of these algorithms to several practical problems. Keywords: unsupervised learning, dimensionality reduction, principal curves, principal surfaces, subspace constrained mean-shift</p><p>4 0.3025752 <a title="49-lsi-4" href="./jmlr-2011-Learning_Transformation_Models_for_Ranking_and_Survival_Analysis.html">56 jmlr-2011-Learning Transformation Models for Ranking and Survival Analysis</a></p>
<p>Author: Vanya Van Belle, Kristiaan Pelckmans, Johan A. K. Suykens, Sabine Van Huffel</p><p>Abstract: This paper studies the task of learning transformation models for ranking problems, ordinal regression and survival analysis. The present contribution describes a machine learning approach termed MINLIP . The key insight is to relate ranking criteria as the Area Under the Curve to monotone transformation functions. Consequently, the notion of a Lipschitz smoothness constant is found to be useful for complexity control for learning transformation models, much in a similar vein as the ’margin’ is for Support Vector Machines for classiﬁcation. The use of this model structure in the context of high dimensional data, as well as for estimating non-linear, and additive models based on primal-dual kernel machines, and for sparse models is indicated. Given n observations, the present method solves a quadratic program existing of O (n) constraints and O (n) unknowns, where most existing risk minimization approaches to ranking problems typically result in algorithms with O (n2 ) constraints or unknowns. We specify the MINLIP method for three different cases: the ﬁrst one concerns the preference learning problem. Secondly it is speciﬁed how to adapt the method to ordinal regression with a ﬁnite set of ordered outcomes. Finally, it is shown how the method can be used in the context of survival analysis where one models failure times, typically subject to censoring. The current approach is found to be particularly useful in this context as it can handle, in contrast with the standard statistical model for analyzing survival data, all types of censoring in a straightforward way, and because of the explicit relation with the Proportional Hazard and Accelerated Failure Time models. The advantage of the current method is illustrated on different benchmark data sets, as well as for estimating a model for cancer survival based on different micro-array and clinical data sets. Keywords: support vector machines, preference learning, ranking models, ordinal regression, survival analysis c</p><p>5 0.27067643 <a title="49-lsi-5" href="./jmlr-2011-Weisfeiler-Lehman_Graph_Kernels.html">103 jmlr-2011-Weisfeiler-Lehman Graph Kernels</a></p>
<p>Author: Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, Karsten M. Borgwardt</p><p>Abstract: In this article, we propose a family of efﬁcient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be deﬁned based on this Weisfeiler-Lehman sequence of graphs, including a highly efﬁcient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classiﬁcation benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis. Keywords: graph kernels, graph classiﬁcation, similarity measures for graphs, Weisfeiler-Lehman algorithm c 2011 Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn and Karsten M. Borgwardt. S HERVASHIDZE , S CHWEITZER , VAN L EEUWEN , M EHLHORN AND B ORGWARDT</p><p>6 0.26946327 <a title="49-lsi-6" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>7 0.267887 <a title="49-lsi-7" href="./jmlr-2011-Forest_Density_Estimation.html">35 jmlr-2011-Forest Density Estimation</a></p>
<p>8 0.25802165 <a title="49-lsi-8" href="./jmlr-2011-Logistic_Stick-Breaking_Process.html">61 jmlr-2011-Logistic Stick-Breaking Process</a></p>
<p>9 0.2477126 <a title="49-lsi-9" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>10 0.24700037 <a title="49-lsi-10" href="./jmlr-2011-Faster_Algorithms_for_Max-Product_Message-Passing.html">34 jmlr-2011-Faster Algorithms for Max-Product Message-Passing</a></p>
<p>11 0.24685565 <a title="49-lsi-11" href="./jmlr-2011-MSVMpack%3A_A_Multi-Class_Support_Vector_Machine_Package.html">62 jmlr-2011-MSVMpack: A Multi-Class Support Vector Machine Package</a></p>
<p>12 0.23119579 <a title="49-lsi-12" href="./jmlr-2011-High-dimensional_Covariance_Estimation_Based_On_Gaussian_Graphical_Models.html">39 jmlr-2011-High-dimensional Covariance Estimation Based On Gaussian Graphical Models</a></p>
<p>13 0.23047875 <a title="49-lsi-13" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>14 0.2208228 <a title="49-lsi-14" href="./jmlr-2011-DirectLiNGAM%3A_A_Direct_Method_for_Learning_a_Linear_Non-Gaussian_Structural_Equation_Model.html">23 jmlr-2011-DirectLiNGAM: A Direct Method for Learning a Linear Non-Gaussian Structural Equation Model</a></p>
<p>15 0.22038975 <a title="49-lsi-15" href="./jmlr-2011-Operator_Norm_Convergence_of_Spectral_Clustering_on_Level_Sets.html">74 jmlr-2011-Operator Norm Convergence of Spectral Clustering on Level Sets</a></p>
<p>16 0.21732713 <a title="49-lsi-16" href="./jmlr-2011-lp-Norm_Multiple_Kernel_Learning.html">105 jmlr-2011-lp-Norm Multiple Kernel Learning</a></p>
<p>17 0.19510823 <a title="49-lsi-17" href="./jmlr-2011-Multiple_Kernel_Learning_Algorithms.html">66 jmlr-2011-Multiple Kernel Learning Algorithms</a></p>
<p>18 0.18505274 <a title="49-lsi-18" href="./jmlr-2011-Learning_a_Robust_Relevance_Model_for_Search_Using_Kernel_Methods.html">57 jmlr-2011-Learning a Robust Relevance Model for Search Using Kernel Methods</a></p>
<p>19 0.17816469 <a title="49-lsi-19" href="./jmlr-2011-Minimum_Description_Length_Penalization_for_Group_and_Multi-Task_Sparse_Learning.html">64 jmlr-2011-Minimum Description Length Penalization for Group and Multi-Task Sparse Learning</a></p>
<p>20 0.17657751 <a title="49-lsi-20" href="./jmlr-2011-Large_Margin_Hierarchical_Classification_with_Mutually_Exclusive_Class_Membership.html">52 jmlr-2011-Large Margin Hierarchical Classification with Mutually Exclusive Class Membership</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.027), (6, 0.017), (9, 0.028), (10, 0.026), (18, 0.483), (24, 0.027), (31, 0.069), (32, 0.019), (41, 0.028), (60, 0.02), (65, 0.018), (70, 0.01), (73, 0.061), (78, 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69093609 <a title="49-lda-1" href="./jmlr-2011-Kernel_Regression_in_the_Presence_of_Correlated_Errors.html">49 jmlr-2011-Kernel Regression in the Presence of Correlated Errors</a></p>
<p>Author: Kris De Brabanter, Jos De Brabanter, Johan A.K. Suykens, Bart De Moor</p><p>Abstract: It is a well-known problem that obtaining a correct bandwidth and/or smoothing parameter in nonparametric regression is difﬁcult in the presence of correlated errors. There exist a wide variety of methods coping with this problem, but they all critically depend on a tuning procedure which requires accurate information about the correlation structure. We propose a bandwidth selection procedure based on bimodal kernels which successfully removes the correlation without requiring any prior knowledge about its structure and its parameters. Further, we show that the form of the kernel is very important when errors are correlated which is in contrast to the independent and identically distributed (i.i.d.) case. Finally, some extensions are proposed to use the proposed criterion in support vector machines and least squares support vector machines for regression. Keywords: nonparametric regression, correlated errors, bandwidth choice, cross-validation, shortrange dependence, bimodal kernel</p><p>2 0.24309391 <a title="49-lda-2" href="./jmlr-2011-The_Sample_Complexity_of_Dictionary_Learning.html">91 jmlr-2011-The Sample Complexity of Dictionary Learning</a></p>
<p>Author: Daniel Vainsencher, Shie Mannor, Alfred M. Bruckstein</p><p>Abstract: A large set of signals can sometimes be described sparsely using a dictionary, that is, every element can be represented as a linear combination of few elements from the dictionary. Algorithms for various signal processing applications, including classiﬁcation, denoising and signal separation, learn a dictionary from a given set of signals to be represented. Can we expect that the error in representing by such a dictionary a previously unseen signal from the same source will be of similar magnitude as those for the given examples? We assume signals are generated from a ﬁxed distribution, and study these questions from a statistical learning theory perspective. We develop generalization bounds on the quality of the learned dictionary for two types of constraints on the coefﬁcient selection, as measured by the expected L2 error in representation when the dictionary is used. For the case of l1 regularized coefﬁcient selection we provide a generalnp ln(mλ)/m , where n is the dimension, p is the number of ization bound of the order of O elements in the dictionary, λ is a bound on the l1 norm of the coefﬁcient vector and m is the number of samples, which complements existing results. For the case of representing a new signal as a combination of at most k dictionary elements, we provide a bound of the order O( np ln(mk)/m) under an assumption on the closeness to orthogonality of the dictionary (low Babel function). We further show that this assumption holds for most dictionaries in high dimensions in a strong probabilistic sense. Our results also include bounds that converge as 1/m, not previously known for this problem. We provide similar results in a general setting using kernels with weak smoothness requirements. Keywords: dictionary learning, generalization bound, sparse representation</p><p>3 0.24260709 <a title="49-lda-3" href="./jmlr-2011-Operator_Norm_Convergence_of_Spectral_Clustering_on_Level_Sets.html">74 jmlr-2011-Operator Norm Convergence of Spectral Clustering on Level Sets</a></p>
<p>Author: Bruno Pelletier, Pierre Pudlo</p><p>Abstract: Following Hartigan (1975), a cluster is deﬁned as a connected component of the t-level set of the underlying density, that is, the set of points for which the density is greater than t. A clustering algorithm which combines a density estimate with spectral clustering techniques is proposed. Our algorithm is composed of two steps. First, a nonparametric density estimate is used to extract the data points for which the estimated density takes a value greater than t. Next, the extracted points are clustered based on the eigenvectors of a graph Laplacian matrix. Under mild assumptions, we prove the almost sure convergence in operator norm of the empirical graph Laplacian operator associated with the algorithm. Furthermore, we give the typical behavior of the representation of the data set into the feature space, which establishes the strong consistency of our proposed algorithm. Keywords: spectral clustering, graph, unsupervised classiﬁcation, level sets, connected components</p><p>4 0.24165188 <a title="49-lda-4" href="./jmlr-2011-A_Family_of_Simple_Non-Parametric_Kernel_Learning_Algorithms.html">4 jmlr-2011-A Family of Simple Non-Parametric Kernel Learning Algorithms</a></p>
<p>Author: Jinfeng Zhuang, Ivor W. Tsang, Steven C.H. Hoi</p><p>Abstract: Previous studies of Non-Parametric Kernel Learning (NPKL) usually formulate the learning task as a Semi-Deﬁnite Programming (SDP) problem that is often solved by some general purpose SDP solvers. However, for N data examples, the time complexity of NPKL using a standard interiorpoint SDP solver could be as high as O(N 6.5 ), which prohibits NPKL methods applicable to real applications, even for data sets of moderate size. In this paper, we present a family of efﬁcient NPKL algorithms, termed “SimpleNPKL”, which can learn non-parametric kernels from a large set of pairwise constraints efﬁciently. In particular, we propose two efﬁcient SimpleNPKL algorithms. One is SimpleNPKL algorithm with linear loss, which enjoys a closed-form solution that can be efﬁciently computed by the Lanczos sparse eigen decomposition technique. Another one is SimpleNPKL algorithm with other loss functions (including square hinge loss, hinge loss, square loss) that can be re-formulated as a saddle-point optimization problem, which can be further resolved by a fast iterative algorithm. In contrast to the previous NPKL approaches, our empirical results show that the proposed new technique, maintaining the same accuracy, is signiﬁcantly more efﬁcient and scalable. Finally, we also demonstrate that the proposed new technique is also applicable to speed up many kernel learning tasks, including colored maximum variance unfolding, minimum volume embedding, and structure preserving embedding. Keywords: non-parametric kernel learning, semi-deﬁnite programming, semi-supervised learning, side information, pairwise constraints</p><p>5 0.24126184 <a title="49-lda-5" href="./jmlr-2011-Computationally_Efficient_Convolved_Multiple_Output_Gaussian_Processes.html">17 jmlr-2011-Computationally Efficient Convolved Multiple Output Gaussian Processes</a></p>
<p>Author: Mauricio A. Álvarez, Neil D. Lawrence</p><p>Abstract: Recently there has been an increasing interest in regression methods that deal with multiple outputs. This has been motivated partly by frameworks like multitask learning, multisensor networks or structured output data. From a Gaussian processes perspective, the problem reduces to specifying an appropriate covariance function that, whilst being positive semi-deﬁnite, captures the dependencies between all the data points and across all the outputs. One approach to account for non-trivial correlations between outputs employs convolution processes. Under a latent function interpretation of the convolution transform we establish dependencies between output variables. The main drawbacks of this approach are the associated computational and storage demands. In this paper we address these issues. We present different efﬁcient approximations for dependent output Gaussian processes constructed through the convolution formalism. We exploit the conditional independencies present naturally in the model. This leads to a form of the covariance similar in spirit to the so called PITC and FITC approximations for a single output. We show experimental results with synthetic and real data, in particular, we show results in school exams score prediction, pollution prediction and gene expression data. Keywords: Gaussian processes, convolution processes, efﬁcient approximations, multitask learning, structured outputs, multivariate processes</p><p>6 0.23727804 <a title="49-lda-6" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>7 0.23715818 <a title="49-lda-7" href="./jmlr-2011-Information_Rates_of_Nonparametric_Gaussian_Process_Methods.html">44 jmlr-2011-Information Rates of Nonparametric Gaussian Process Methods</a></p>
<p>8 0.23628712 <a title="49-lda-8" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<p>9 0.23141629 <a title="49-lda-9" href="./jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</a></p>
<p>10 0.23139018 <a title="49-lda-10" href="./jmlr-2011-Efficient_Learning_with_Partially_Observed_Attributes.html">29 jmlr-2011-Efficient Learning with Partially Observed Attributes</a></p>
<p>11 0.23096178 <a title="49-lda-11" href="./jmlr-2011-Forest_Density_Estimation.html">35 jmlr-2011-Forest Density Estimation</a></p>
<p>12 0.23072675 <a title="49-lda-12" href="./jmlr-2011-Bayesian_Generalized_Kernel_Mixed_Models.html">13 jmlr-2011-Bayesian Generalized Kernel Mixed Models</a></p>
<p>13 0.23069811 <a title="49-lda-13" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>14 0.23057315 <a title="49-lda-14" href="./jmlr-2011-Robust_Gaussian_Process_Regression_with_a_Student-tLikelihood.html">82 jmlr-2011-Robust Gaussian Process Regression with a Student-tLikelihood</a></p>
<p>15 0.22941867 <a title="49-lda-15" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>16 0.22927701 <a title="49-lda-16" href="./jmlr-2011-Large_Margin_Hierarchical_Classification_with_Mutually_Exclusive_Class_Membership.html">52 jmlr-2011-Large Margin Hierarchical Classification with Mutually Exclusive Class Membership</a></p>
<p>17 0.22918308 <a title="49-lda-17" href="./jmlr-2011-Multiple_Kernel_Learning_Algorithms.html">66 jmlr-2011-Multiple Kernel Learning Algorithms</a></p>
<p>18 0.22914347 <a title="49-lda-18" href="./jmlr-2011-Super-Linear_Convergence_of_Dual_Augmented_Lagrangian_Algorithm_for_Sparsity_Regularized_Estimation.html">89 jmlr-2011-Super-Linear Convergence of Dual Augmented Lagrangian Algorithm for Sparsity Regularized Estimation</a></p>
<p>19 0.2290978 <a title="49-lda-19" href="./jmlr-2011-Neyman-Pearson_Classification%2C_Convexity_and_Stochastic_Constraints.html">69 jmlr-2011-Neyman-Pearson Classification, Convexity and Stochastic Constraints</a></p>
<p>20 0.22875124 <a title="49-lda-20" href="./jmlr-2011-Hierarchical_Knowledge_Gradient_for_Sequential_Sampling.html">38 jmlr-2011-Hierarchical Knowledge Gradient for Sequential Sampling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
