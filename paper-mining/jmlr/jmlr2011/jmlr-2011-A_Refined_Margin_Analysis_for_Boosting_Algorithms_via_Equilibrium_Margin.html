<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 jmlr-2011-A Refined Margin Analysis for Boosting Algorithms via Equilibrium Margin</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-5" href="#">jmlr2011-5</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 jmlr-2011-A Refined Margin Analysis for Boosting Algorithms via Equilibrium Margin</h1>
<br/><p>Source: <a title="jmlr-2011-5-pdf" href="http://jmlr.org/papers/volume12/wang11a/wang11a.pdf">pdf</a></p><p>Author: Liwei Wang, Masashi Sugiyama, Zhaoxiang Jing, Cheng Yang, Zhi-Hua Zhou, Jufu Feng</p><p>Abstract: Much attention has been paid to the theoretical explanation of the empirical success of AdaBoost. The most inﬂuential work is the margin theory, which is essentially an upper bound for the generalization error of any voting classiﬁer in terms of the margin distribution over the training data. However, important questions were raised about the margin explanation. Breiman (1999) proved a bound in terms of the minimum margin, which is sharper than the margin distribution bound. He argued that the minimum margin would be better in predicting the generalization error. Grove and Schuurmans (1998) developed an algorithm called LP-AdaBoost which maximizes the minimum margin while keeping all other factors the same as AdaBoost. In experiments however, LP-AdaBoost usually performs worse than AdaBoost, putting the margin explanation into serious doubt. In this paper, we make a reﬁned analysis of the margin theory. We prove a bound in terms of a new margin measure called the Equilibrium margin (Emargin). The Emargin bound is uniformly ©2011 Liwei Wang, Masashi Sugiyama, Zhaoxiang Jing, Cheng Yang, Zhi-Hua Zhou and Jufu Feng. WANG , S UGIYAMA , J ING , YANG , Z HOU AND F ENG sharper than Breiman’s minimum margin bound. Thus our result suggests that the minimum margin may be not crucial for the generalization error. We also show that a large Emargin and a small empirical error at Emargin imply a smaller bound of the generalization error. Experimental results on benchmark data sets demonstrate that AdaBoost usually has a larger Emargin and a smaller test error than LP-AdaBoost, which agrees well with our theory. Keywords: boosting, margin bounds, voting classiﬁer</p><p>Reference: <a title="jmlr-2011-5-reference" href="../jmlr2011_reference/jmlr-2011-A_Refined_Margin_Analysis_for_Boosting_Algorithms_via_Equilibrium_Margin_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The most inﬂuential work is the margin theory, which is essentially an upper bound for the generalization error of any voting classiﬁer in terms of the margin distribution over the training data. [sent-27, score-0.759]
</p><p>2 However, important questions were raised about the margin explanation. [sent-28, score-0.257]
</p><p>3 Breiman (1999) proved a bound in terms of the minimum margin, which is sharper than the margin distribution bound. [sent-29, score-0.401]
</p><p>4 He argued that the minimum margin would be better in predicting the generalization error. [sent-30, score-0.331]
</p><p>5 Grove and Schuurmans (1998) developed an algorithm called LP-AdaBoost which maximizes the minimum margin while keeping all other factors the same as AdaBoost. [sent-31, score-0.3]
</p><p>6 In experiments however, LP-AdaBoost usually performs worse than AdaBoost, putting the margin explanation into serious doubt. [sent-32, score-0.294]
</p><p>7 In this paper, we make a reﬁned analysis of the margin theory. [sent-33, score-0.257]
</p><p>8 We prove a bound in terms of a new margin measure called the Equilibrium margin (Emargin). [sent-34, score-0.555]
</p><p>9 WANG , S UGIYAMA , J ING , YANG , Z HOU AND F ENG  sharper than Breiman’s minimum margin bound. [sent-36, score-0.36]
</p><p>10 Thus our result suggests that the minimum margin may be not crucial for the generalization error. [sent-37, score-0.331]
</p><p>11 Roughly speaking, the margin of an example with respect to a classiﬁer is a measure of the conﬁdence of the classiﬁcation result. [sent-63, score-0.257]
</p><p>12 They also demonstrated that AdaBoost has the ability to produce a “good” margin distribution. [sent-65, score-0.257]
</p><p>13 This theory suggests that producing a good margin distribution is the key to the success of AdaBoost and explains well its relative resistance to overﬁtting. [sent-66, score-0.277]
</p><p>14 Soon after that however, there were serious doubt cast on this margin explanation. [sent-67, score-0.272]
</p><p>15 (Minimum margin is the smallest margin over all training examples, see Section 2 for the formal deﬁnition). [sent-69, score-0.536]
</p><p>16 Breiman (1999) then gave an upper bound for the generalization error of a voting classiﬁer in terms of the minimum margin, as well as the number of training examples and the size of the set of base classiﬁers. [sent-70, score-0.348]
</p><p>17 This bound is sharper than the bound based on the margin distribution given in Schapire et al. [sent-71, score-0.399]
</p><p>18 According to the minimum margin bound, LP-AdaBoost would have smaller generalization error than AdaBoost. [sent-79, score-0.357]
</p><p>19 This result puts the margin theory into serious doubt. [sent-81, score-0.272]
</p><p>20 In this paper we provide a reﬁned analysis of the margin theory. [sent-82, score-0.257]
</p><p>21 We propose a new upper bound for the generalization error of voting classiﬁers. [sent-83, score-0.223]
</p><p>22 This bound is uniformly sharper than Breiman’s minimum margin bound. [sent-84, score-0.401]
</p><p>23 The key factor in this bound is a new margin notion which we refer to as the Equilibrium margin (Emargin). [sent-85, score-0.555]
</p><p>24 The Emargin can be viewed as a measure of how good a margin distribution is. [sent-86, score-0.257]
</p><p>25 In fact, the Emargin depends, in a complicated way, on the margin distribution, and has little relation to the minimum margin. [sent-87, score-0.3]
</p><p>26 The margin theory has been studied and greatly improved by several authors. [sent-89, score-0.257]
</p><p>27 Especially Koltchinskii and Panchenko (2002, 2005) developed new tools for empirical processes and prove much sharper margin distribution bounds. [sent-90, score-0.317]
</p><p>28 However it is difﬁcult to compare these bounds to the minimum margin bound of Breiman (1999), since they contain unspeciﬁed constants. [sent-91, score-0.357]
</p><p>29 Nevertheless, these results suggest that the margin distribution may be more important than the minimum margin for the generalization error of voting classiﬁers. [sent-92, score-0.739]
</p><p>30 We also show that if a boosting algorithm returns a classiﬁer that minimizes the Emargin bound or the margin distribution bound of Schapire et al. [sent-93, score-0.425]
</p><p>31 The rest of this paper is organized as follows: In Section 2 we brieﬂy describe the background of the margin theory. [sent-95, score-0.257]
</p><p>32 y f (x) is called the margin for (x, y) with respect to f . [sent-114, score-0.257]
</p><p>33 If we consider the margins over the whole set of training examples, we can regard PS (y f (x) ≤ θ) as a distribution over θ (−1 ≤ θ ≤ 1), since PS (y f (x) ≤ θ) is the fraction of training examples whose margin is at most θ. [sent-115, score-0.325]
</p><p>34 This distribution is referred to as the margin distribution. [sent-116, score-0.257]
</p><p>35 , 1998) of the AdaBoost algorithm is to upper bound the generalization error of voting classiﬁers in terms of the margin distribution, the number of training examples and the complexity of the set from which the base classiﬁers are chosen. [sent-123, score-0.562]
</p><p>36 The theorem states that if the voting classiﬁer generates a good margin distribution, that is, most training examples have large margins so that PS (y f (x) ≤ θ) is small for not too small θ, then the upper bound of the generalization error is also small. [sent-141, score-0.553]
</p><p>37 These results suggest that the excellent performance of AdaBoost is due to its good margin distribution. [sent-144, score-0.257]
</p><p>38 Another important notion is the minimum margin which is the smallest margin achieved on the training set. [sent-145, score-0.579]
</p><p>39 Formally, the minimum margin, denoted by θ0 , of a voting classiﬁer f on a training set S is deﬁned as θ0 = min {y f (x) : (x, y) ∈ S } . [sent-146, score-0.19]
</p><p>40 Breiman (1999) proved an upper bound for the generalization error of voting classiﬁers which depends only on the minimum margin, not on the entire margin distribution. [sent-147, score-0.523]
</p><p>41 nθ2 0  Then for any δ > 0, with probability at least 1 − δ over the random choice of the training set S of n examples, every voting classiﬁer f whose minimum margin on S is at least θ0 satisﬁes the following bound: PD y f (x) ≤ 0 ≤ R log(2n) + log 1839  1 1 |H | + 1 + log . [sent-150, score-0.523]
</p><p>42 R n δ  WANG , S UGIYAMA , J ING , YANG , Z HOU AND F ENG  Breiman (1999) pointed out that his bound is sharper than the margin distribution bound of Schapire et al. [sent-151, score-0.399]
</p><p>43 If θ in Theorem 1 is taken to be the minimum margin θ0 , the bound in Theorem 2 is about the square of the bound in terms of the margin distribution, since the bound in Theorem 2 log n . [sent-152, score-0.718]
</p><p>44 Breiman then argued that compared to the is O log2n and the bound in Theorem 1 is O nθ0 nθ2 0 margin distribution explanation, his bound implied more strongly that the minimum margin governs the generalization error. [sent-153, score-0.67]
</p><p>45 Finding T a voting classiﬁer g = ∑t=1 βt ht such that g maximizes the minimum margin can be formulated as a linear programming problem. [sent-163, score-0.472]
</p><p>46 Comparing the performance of AdaBoost and LP-AdaBoost is a good test of signiﬁcance of the minimum margin bound. [sent-171, score-0.3]
</p><p>47 This result is different from what the minimum margin bound suggests and therefore puts the margin explanation into serious doubt. [sent-174, score-0.635]
</p><p>48 It can be shown that arc-gv converges to the maximum margin solution (R¨ tsch a and Warmuth, 2005; Rudin et al. [sent-178, score-0.257]
</p><p>49 However on some data sets AdaBoost has larger minimum margin than arc-gv after a ﬁnite number of rounds. [sent-181, score-0.3]
</p><p>50 The bounds are sharper than the minimum margin bound. [sent-186, score-0.376]
</p><p>51 log log(2|H |) + 2 log |H | + log 2 θ δ log |H |  Note that the assumption q0 < 1 in the theorem is very mild since it implies that at least one training example has a large margin (larger than 8/|H |), or equivalently the largest margin is not too small. [sent-203, score-0.715]
</p><p>52 1 This contrasts with the fact that the minimum margin bound applies when the minimum margin is not too small. [sent-204, score-0.641]
</p><p>53 It can be seen that q∗ is the empirical error at margin θ∗ , that is, q∗ = PS (y f (x) < θ∗ ). [sent-208, score-0.283]
</p><p>54 n  Theorem 3 provides an upper bound of the generalization error of a voting classiﬁer that depends on its Emargin and the Emargin error. [sent-213, score-0.223]
</p><p>55 Note that the Emargin depends, in a complicated way, on the whole margin distribution. [sent-215, score-0.257]
</p><p>56 The minimum margin is only a special case of the Emargin. [sent-217, score-0.3]
</p><p>57 Hence the Emargin is equal to the minimum margin if and only if the optimal q∗ is zero. [sent-219, score-0.3]
</p><p>58 We next compare our Emargin bound to the minimum margin bound. [sent-220, score-0.341]
</p><p>59 We show that the Emargin bound is sharper than the minimum margin bound. [sent-221, score-0.401]
</p><p>60 Since the minimum margin bound applies only to the separable case, that is, θ0 > 0, we assume that the conditions in Theorem 2 are satisﬁed. [sent-222, score-0.341]
</p><p>61 Theorem 5 Assume that the minimum margin θ0 is larger than 0. [sent-223, score-0.3]
</p><p>62 Then the bound given in Theorem 3 is uniformly sharper than the minimum margin bound in Theorem 2. [sent-224, score-0.442]
</p><p>63 Theorem 6 suggests that the Emargin and the Emargin error can be used as measures of the quality of a margin distribution. [sent-236, score-0.283]
</p><p>64 A large Emargin and a small Emargin error indicate a good margin distribution. [sent-237, score-0.283]
</p><p>65 Then for any δ > 0, with probability at least 1 − δ over the random choice of the training set S of n examples, every voting classiﬁer f satisﬁes the following bound: PD y f (x) ≤ 0 ≤  n − 1 −1 d2 + 1 ˆ + inf ·D q, u θ(q) , n−1 1 n q∈{q0 ,q0 + n ,. [sent-241, score-0.18]
</p><p>66 , n } n  (3)  where ˆ θ(q) = sup θ ∈ 0, 1 : PS y f (x) ≤ θ ≤ q , and u (θ) =  16 n en2 n 2n 16d , log log + 3 log log + 1 + log 2 2 θ d d θ d δ  1 n  provided q0 = PS (y f (x) ≤ 0) < 1. [sent-244, score-0.203]
</p><p>67 8/|H |, 1) such that 8 2n2 n log log(2|H | + log |H | + log ) , 2 θ δ log |H |  (4)  we have PD (y f (x) ≤ θ) ≤  log |H | C′ + n n  n 2n2 8 log(2|H | + log |H | + log ) , log 2 θ δ log |H |  where C′ = max(2C, 8). [sent-269, score-0.342]
</p><p>68 The ﬁrst bound in the corollary has the same order as the minimum margin bound. [sent-270, score-0.341]
</p><p>69 The third bound states that the generalization error is log O log nnθ2 |H | even in the non-zero error case, provided the margin error PS (y f (x) ≤ θ) is small enough. [sent-273, score-0.483]
</p><p>70 The price however is that this approximate bound is not uniformly sharper than the minimum margin bound. [sent-277, score-0.401]
</p><p>71 In this section we point out that the Emargin bound and the margin distribution bound in Theorem 1 imply statistical consistency. [sent-280, score-0.339]
</p><p>72 An immediate consequence of this consistency is that margin bound optimization is Bayes consistent if the linear span of the base classiﬁers is dense in the space of all measurable 1844  A R EFINED M ARGIN A NALYSIS FOR B OOSTING A LGORITHMS VIA E QUILIBRIUM M ARGIN  functions. [sent-285, score-0.358]
</p><p>73 The next theorem states that margin bound optimization is consistent. [sent-294, score-0.325]
</p><p>74 With almost the same arguments one can show that minimizing the margin distribution bound in Theorem 1 is also consistent. [sent-295, score-0.298]
</p><p>75 But there is no such result for the minimum margin bound for the non-separable problems. [sent-296, score-0.341]
</p><p>76 The difference is that we do not bound the deviation of the generalization error from the empirical margin error directly, instead we consider the difference of the generalization error to a zero-one function of a certain empirical measure. [sent-304, score-0.438]
</p><p>77 It bounds the difference of the true and empirical margin distributions as α and k vary over their ranges. [sent-328, score-0.273]
</p><p>78 We thus obtain that for ﬁxed k, with probability at  least 1 − δ over the random choice of the training set S of n examples, every f ∈ C(H ) with q0 < 1 satisﬁes PD y f (x) ≤ 0 ≤ where u=  1 n  log |H | + D−1 n  k ,u , n  1 2n2 8 log(2|H |) + 2 log |H | + log log . [sent-365, score-0.174]
</p><p>79 By Lemma 11, the Emargin bound can be relaxed to PD y f (x) ≤ 0 ≤ ≤  1 n  8 2n2 n log log(2|H |) + 3 log |H | + log 2 δ log |H | θ0  |H | 16 log(2n) log(2|H |) log n + 2 log |H | 1 + + log . [sent-387, score-0.307]
</p><p>80 For the minimum margin bound, we only consider the case that R ≤ 1, since otherwise the bound is larger than one. [sent-389, score-0.341]
</p><p>81 Simple calculations show that the right-hand side of (13) is smaller than the minimum margin bound. [sent-390, score-0.3]
</p><p>82 Theorem 6 suggests that if a voting classiﬁer f1 has a larger Emargin and a smaller Emargin error than another classiﬁer f2 , then f1 has a smaller bound of the generalization error than f2 . [sent-512, score-0.249]
</p><p>83 We run AdaBoost 100 rounds, and use the obtained base classiﬁers to train the LP-AdaBoost voting classiﬁer. [sent-531, score-0.185]
</p><p>84 We then calculate the Emargin, Emargin error, test error as well as the minimum margin of them respectively. [sent-532, score-0.326]
</p><p>85 Finally we plot in Figure 1 some margin distribution graphs and the corresponding Emargin and Emargin errors to give an illustration. [sent-1115, score-0.257]
</p><p>86 Conclusions In this paper we provided a reﬁned analysis on the margin theory for boosting algorithms, which extended our preliminary study (Wang et al. [sent-1118, score-0.343]
</p><p>87 We proposed a bound in terms of a new margin measure called the Emargin, which depends on the whole margin distribution. [sent-1120, score-0.555]
</p><p>88 This bound is uniformly sharper than the minimum margin bound whose prediction is different from the empirical observations. [sent-1121, score-0.442]
</p><p>89 Our bound suggests that the Emargin and the Emargin error play important roles to guarantee a smaller bound of the generalization error of a voting classiﬁer—a larger Emargin and a smaller 1859  WANG , S UGIYAMA , J ING , YANG , Z HOU AND F ENG  Breast  Breast  0. [sent-1123, score-0.29]
</p><p>90 The lines marked with stars are the margin distributions of LP-AdaBoost. [sent-1214, score-0.276]
</p><p>91 On the other hand we can employ the bound to “compare” voting classiﬁers with the help of Emargin and Emargin error. [sent-1223, score-0.166]
</p><p>92 A future work is to develop algorithms that generate voting classiﬁers with good margin distributions, that is, large Emargin and small Emargin error. [sent-1230, score-0.382]
</p><p>93 On the other hand, given a voting classiﬁer ∑ αt ht , it might be possible to improve its margin distribution. [sent-1232, score-0.429]
</p><p>94 , β = α), ∑ βt ht would have a uniformly better margin distribution than ∑ αt ht and therefore we expect it has a smaller generalization error. [sent-1251, score-0.382]
</p><p>95 However, there is usually no nontrivial solutions when ∑ αt ht is an AdaBoost classiﬁer—it already has a good margin distribution. [sent-1252, score-0.304]
</p><p>96 An empirical comparison of voting classiﬁcation algorithms: Bagging, boosting and variants. [sent-1280, score-0.211]
</p><p>97 Boosting in the limit: Maximizing the margin of learned ensembles. [sent-1329, score-0.257]
</p><p>98 Empirical margin distributions and bounding the generalization error of combined classiﬁers. [sent-1342, score-0.314]
</p><p>99 How boosting the margin can also boost classiﬁer complexity. [sent-1382, score-0.343]
</p><p>100 Analysis of boosting algorithms using the smooth margin function. [sent-1394, score-0.343]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('emargin', 0.696), ('ps', 0.361), ('margin', 0.257), ('yg', 0.224), ('adaboost', 0.191), ('ada', 0.19), ('pd', 0.168), ('argin', 0.129), ('voting', 0.125), ('quilibrium', 0.092), ('boosting', 0.086), ('lp', 0.085), ('efined', 0.078), ('hou', 0.078), ('oosting', 0.078), ('breiman', 0.075), ('eng', 0.07), ('ugiyama', 0.07), ('qn', 0.063), ('sharper', 0.06), ('base', 0.06), ('ers', 0.052), ('nalysis', 0.051), ('cn', 0.05), ('schapire', 0.05), ('er', 0.049), ('classi', 0.049), ('ht', 0.047), ('shuttle', 0.045), ('minimum', 0.043), ('wang', 0.042), ('bound', 0.041), ('pg', 0.039), ('lgorithms', 0.038), ('log', 0.038), ('pr', 0.038), ('yang', 0.037), ('vc', 0.034), ('epd', 0.033), ('stump', 0.033), ('inf', 0.033), ('generalization', 0.031), ('satimage', 0.03), ('ing', 0.029), ('dd', 0.028), ('grove', 0.028), ('theorem', 0.027), ('error', 0.026), ('infq', 0.026), ('breast', 0.025), ('margins', 0.024), ('beijing', 0.023), ('qi', 0.023), ('pendigits', 0.022), ('explanation', 0.022), ('training', 0.022), ('cu', 0.02), ('vehicle', 0.02), ('resistance', 0.02), ('reyzin', 0.02), ('marked', 0.019), ('agrees', 0.018), ('exp', 0.018), ('rudin', 0.017), ('optdigits', 0.017), ('pku', 0.017), ('spambase', 0.017), ('wdbc', 0.017), ('chernoff', 0.016), ('equilibrium', 0.016), ('hi', 0.016), ('bounds', 0.016), ('serious', 0.015), ('tokyo', 0.015), ('decision', 0.015), ('bagging', 0.015), ('isolet', 0.015), ('peking', 0.015), ('sugiyama', 0.015), ('waveform', 0.015), ('lemma', 0.015), ('dt', 0.014), ('letter', 0.014), ('agree', 0.014), ('electronics', 0.014), ('schuurmans', 0.014), ('emargins', 0.013), ('eps', 0.013), ('jufu', 0.013), ('meir', 0.013), ('nanjing', 0.013), ('sauer', 0.013), ('zhaoxiang', 0.013), ('cis', 0.013), ('diabetes', 0.013), ('hull', 0.013), ('sup', 0.013), ('union', 0.012), ('perception', 0.012), ('moe', 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="5-tfidf-1" href="./jmlr-2011-A_Refined_Margin_Analysis_for_Boosting_Algorithms_via_Equilibrium_Margin.html">5 jmlr-2011-A Refined Margin Analysis for Boosting Algorithms via Equilibrium Margin</a></p>
<p>Author: Liwei Wang, Masashi Sugiyama, Zhaoxiang Jing, Cheng Yang, Zhi-Hua Zhou, Jufu Feng</p><p>Abstract: Much attention has been paid to the theoretical explanation of the empirical success of AdaBoost. The most inﬂuential work is the margin theory, which is essentially an upper bound for the generalization error of any voting classiﬁer in terms of the margin distribution over the training data. However, important questions were raised about the margin explanation. Breiman (1999) proved a bound in terms of the minimum margin, which is sharper than the margin distribution bound. He argued that the minimum margin would be better in predicting the generalization error. Grove and Schuurmans (1998) developed an algorithm called LP-AdaBoost which maximizes the minimum margin while keeping all other factors the same as AdaBoost. In experiments however, LP-AdaBoost usually performs worse than AdaBoost, putting the margin explanation into serious doubt. In this paper, we make a reﬁned analysis of the margin theory. We prove a bound in terms of a new margin measure called the Equilibrium margin (Emargin). The Emargin bound is uniformly ©2011 Liwei Wang, Masashi Sugiyama, Zhaoxiang Jing, Cheng Yang, Zhi-Hua Zhou and Jufu Feng. WANG , S UGIYAMA , J ING , YANG , Z HOU AND F ENG sharper than Breiman’s minimum margin bound. Thus our result suggests that the minimum margin may be not crucial for the generalization error. We also show that a large Emargin and a small empirical error at Emargin imply a smaller bound of the generalization error. Experimental results on benchmark data sets demonstrate that AdaBoost usually has a larger Emargin and a smaller test error than LP-AdaBoost, which agrees well with our theory. Keywords: boosting, margin bounds, voting classiﬁer</p><p>2 0.10148596 <a title="5-tfidf-2" href="./jmlr-2011-On_Equivalence_Relationships_Between_Classification_and_Ranking_Algorithms.html">71 jmlr-2011-On Equivalence Relationships Between Classification and Ranking Algorithms</a></p>
<p>Author: Şeyda Ertekin, Cynthia Rudin</p><p>Abstract: We demonstrate that there are machine learning algorithms that can achieve success for two separate tasks simultaneously, namely the tasks of classiﬁcation and bipartite ranking. This means that advantages gained from solving one task can be carried over to the other task, such as the ability to obtain conditional density estimates, and an order-of-magnitude reduction in computational time for training the algorithm. It also means that some algorithms are robust to the choice of evaluation metric used; they can theoretically perform well when performance is measured either by a misclassiﬁcation error or by a statistic of the ROC curve (such as the area under the curve). Speciﬁcally, we provide such an equivalence relationship between a generalization of Freund et al.’s RankBoost algorithm, called the “P-Norm Push,” and a particular cost-sensitive classiﬁcation algorithm that generalizes AdaBoost, which we call “P-Classiﬁcation.” We discuss and validate the potential beneﬁts of this equivalence relationship, and perform controlled experiments to understand P-Classiﬁcation’s empirical performance. There is no established equivalence relationship for logistic regression and its ranking counterpart, so we introduce a logistic-regression-style algorithm that aims in between classiﬁcation and ranking, and has promising experimental performance with respect to both tasks. Keywords: supervised classiﬁcation, bipartite ranking, area under the curve, rank statistics, boosting, logistic regression</p><p>3 0.088931836 <a title="5-tfidf-3" href="./jmlr-2011-Universality%2C_Characteristic_Kernels_and_RKHS_Embedding_of_Measures.html">98 jmlr-2011-Universality, Characteristic Kernels and RKHS Embedding of Measures</a></p>
<p>Author: Bharath K. Sriperumbudur, Kenji Fukumizu, Gert R.G. Lanckriet</p><p>Abstract: Over the last few years, two different notions of positive deﬁnite (pd) kernels—universal and characteristic—have been developing in parallel in machine learning: universal kernels are proposed in the context of achieving the Bayes risk by kernel-based classiﬁcation/regression algorithms while characteristic kernels are introduced in the context of distinguishing probability measures by embedding them into a reproducing kernel Hilbert space (RKHS). However, the relation between these two notions is not well understood. The main contribution of this paper is to clarify the relation between universal and characteristic kernels by presenting a unifying study relating them to RKHS embedding of measures, in addition to clarifying their relation to other common notions of strictly pd, conditionally strictly pd and integrally strictly pd kernels. For radial kernels on Rd , all these notions are shown to be equivalent. Keywords: kernel methods, characteristic kernels, Hilbert space embeddings, universal kernels, strictly positive deﬁnite kernels, integrally strictly positive deﬁnite kernels, conditionally strictly positive deﬁnite kernels, translation invariant kernels, radial kernels, binary classiﬁcation, homogeneity testing</p><p>4 0.06305673 <a title="5-tfidf-4" href="./jmlr-2011-Unsupervised_Supervised_Learning_II%3A_Margin-Based_Classification_Without_Labels.html">100 jmlr-2011-Unsupervised Supervised Learning II: Margin-Based Classification Without Labels</a></p>
<p>Author: Krishnakumar Balasubramanian, Pinar Donmez, Guy Lebanon</p><p>Abstract: Many popular linear classiﬁers, such as logistic regression, boosting, or SVM, are trained by optimizing a margin-based risk function. Traditionally, these risk functions are computed based on a labeled data set. We develop a novel technique for estimating such risks using only unlabeled data and the marginal label distribution. We prove that the proposed risk estimator is consistent on high-dimensional data sets and demonstrate it on synthetic and real-world data. In particular, we show how the estimate is used for evaluating classiﬁers in transfer learning, and for training classiﬁers with no labeled data whatsoever. Keywords: classiﬁcation, large margin, maximum likelihood</p><p>5 0.054342356 <a title="5-tfidf-5" href="./jmlr-2011-Large_Margin_Hierarchical_Classification_with_Mutually_Exclusive_Class_Membership.html">52 jmlr-2011-Large Margin Hierarchical Classification with Mutually Exclusive Class Membership</a></p>
<p>Author: Huixin Wang, Xiaotong Shen, Wei Pan</p><p>Abstract: In hierarchical classiﬁcation, class labels are structured, that is each label value corresponds to one non-root node in a tree, where the inter-class relationship for classiﬁcation is speciﬁed by directed paths of the tree. In such a situation, the focus has been on how to leverage the interclass relationship to enhance the performance of ﬂat classiﬁcation, which ignores such dependency. This is critical when the number of classes becomes large relative to the sample size. This paper considers single-path or partial-path hierarchical classiﬁcation, where only one path is permitted from the root to a leaf node. A large margin method is introduced based on a new concept of generalized margins with respect to hierarchy. For implementation, we consider support vector machines and ψ-learning. Numerical and theoretical analyses suggest that the proposed method achieves the desired objective and compares favorably against strong competitors in the literature, including its ﬂat counterparts. Finally, an application to gene function prediction is discussed. Keywords: difference convex programming, gene function annotation, margins, multi-class classiﬁcation, structured learning</p><p>6 0.041763145 <a title="5-tfidf-6" href="./jmlr-2011-Neyman-Pearson_Classification%2C_Convexity_and_Stochastic_Constraints.html">69 jmlr-2011-Neyman-Pearson Classification, Convexity and Stochastic Constraints</a></p>
<p>7 0.031627409 <a title="5-tfidf-7" href="./jmlr-2011-High-dimensional_Covariance_Estimation_Based_On_Gaussian_Graphical_Models.html">39 jmlr-2011-High-dimensional Covariance Estimation Based On Gaussian Graphical Models</a></p>
<p>8 0.029650601 <a title="5-tfidf-8" href="./jmlr-2011-Exploiting_Best-Match_Equations_for_Efficient_Reinforcement_Learning.html">33 jmlr-2011-Exploiting Best-Match Equations for Efficient Reinforcement Learning</a></p>
<p>9 0.028151631 <a title="5-tfidf-9" href="./jmlr-2011-Smoothness%2C_Disagreement_Coefficient%2C_and_the_Label_Complexity_of_Agnostic_Active_Learning.html">85 jmlr-2011-Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning</a></p>
<p>10 0.023999423 <a title="5-tfidf-10" href="./jmlr-2011-Information%2C_Divergence_and_Risk_for_Binary_Experiments.html">43 jmlr-2011-Information, Divergence and Risk for Binary Experiments</a></p>
<p>11 0.023856757 <a title="5-tfidf-11" href="./jmlr-2011-Forest_Density_Estimation.html">35 jmlr-2011-Forest Density Estimation</a></p>
<p>12 0.023409907 <a title="5-tfidf-12" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<p>13 0.023180531 <a title="5-tfidf-13" href="./jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</a></p>
<p>14 0.02269835 <a title="5-tfidf-14" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>15 0.020425266 <a title="5-tfidf-15" href="./jmlr-2011-Learning_from_Partial_Labels.html">58 jmlr-2011-Learning from Partial Labels</a></p>
<p>16 0.020247603 <a title="5-tfidf-16" href="./jmlr-2011-The_Stationary_Subspace_Analysis_Toolbox.html">92 jmlr-2011-The Stationary Subspace Analysis Toolbox</a></p>
<p>17 0.019616041 <a title="5-tfidf-17" href="./jmlr-2011-Super-Linear_Convergence_of_Dual_Augmented_Lagrangian_Algorithm_for_Sparsity_Regularized_Estimation.html">89 jmlr-2011-Super-Linear Convergence of Dual Augmented Lagrangian Algorithm for Sparsity Regularized Estimation</a></p>
<p>18 0.018760215 <a title="5-tfidf-18" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>19 0.017633313 <a title="5-tfidf-19" href="./jmlr-2011-Learning_Transformation_Models_for_Ranking_and_Survival_Analysis.html">56 jmlr-2011-Learning Transformation Models for Ranking and Survival Analysis</a></p>
<p>20 0.016450806 <a title="5-tfidf-20" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.106), (1, 0.001), (2, 0.003), (3, -0.03), (4, 0.042), (5, -0.015), (6, -0.06), (7, -0.025), (8, -0.128), (9, 0.01), (10, -0.191), (11, -0.04), (12, -0.014), (13, -0.089), (14, -0.037), (15, 0.115), (16, -0.116), (17, -0.246), (18, 0.073), (19, -0.061), (20, 0.136), (21, 0.013), (22, -0.297), (23, 0.257), (24, 0.193), (25, 0.015), (26, 0.096), (27, -0.069), (28, 0.196), (29, -0.227), (30, -0.004), (31, 0.017), (32, 0.181), (33, 0.024), (34, -0.17), (35, -0.081), (36, -0.05), (37, -0.007), (38, 0.095), (39, -0.051), (40, -0.067), (41, -0.04), (42, 0.033), (43, -0.091), (44, 0.052), (45, 0.023), (46, 0.122), (47, -0.043), (48, -0.001), (49, 0.086)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94690377 <a title="5-lsi-1" href="./jmlr-2011-A_Refined_Margin_Analysis_for_Boosting_Algorithms_via_Equilibrium_Margin.html">5 jmlr-2011-A Refined Margin Analysis for Boosting Algorithms via Equilibrium Margin</a></p>
<p>Author: Liwei Wang, Masashi Sugiyama, Zhaoxiang Jing, Cheng Yang, Zhi-Hua Zhou, Jufu Feng</p><p>Abstract: Much attention has been paid to the theoretical explanation of the empirical success of AdaBoost. The most inﬂuential work is the margin theory, which is essentially an upper bound for the generalization error of any voting classiﬁer in terms of the margin distribution over the training data. However, important questions were raised about the margin explanation. Breiman (1999) proved a bound in terms of the minimum margin, which is sharper than the margin distribution bound. He argued that the minimum margin would be better in predicting the generalization error. Grove and Schuurmans (1998) developed an algorithm called LP-AdaBoost which maximizes the minimum margin while keeping all other factors the same as AdaBoost. In experiments however, LP-AdaBoost usually performs worse than AdaBoost, putting the margin explanation into serious doubt. In this paper, we make a reﬁned analysis of the margin theory. We prove a bound in terms of a new margin measure called the Equilibrium margin (Emargin). The Emargin bound is uniformly ©2011 Liwei Wang, Masashi Sugiyama, Zhaoxiang Jing, Cheng Yang, Zhi-Hua Zhou and Jufu Feng. WANG , S UGIYAMA , J ING , YANG , Z HOU AND F ENG sharper than Breiman’s minimum margin bound. Thus our result suggests that the minimum margin may be not crucial for the generalization error. We also show that a large Emargin and a small empirical error at Emargin imply a smaller bound of the generalization error. Experimental results on benchmark data sets demonstrate that AdaBoost usually has a larger Emargin and a smaller test error than LP-AdaBoost, which agrees well with our theory. Keywords: boosting, margin bounds, voting classiﬁer</p><p>2 0.60647523 <a title="5-lsi-2" href="./jmlr-2011-On_Equivalence_Relationships_Between_Classification_and_Ranking_Algorithms.html">71 jmlr-2011-On Equivalence Relationships Between Classification and Ranking Algorithms</a></p>
<p>Author: Şeyda Ertekin, Cynthia Rudin</p><p>Abstract: We demonstrate that there are machine learning algorithms that can achieve success for two separate tasks simultaneously, namely the tasks of classiﬁcation and bipartite ranking. This means that advantages gained from solving one task can be carried over to the other task, such as the ability to obtain conditional density estimates, and an order-of-magnitude reduction in computational time for training the algorithm. It also means that some algorithms are robust to the choice of evaluation metric used; they can theoretically perform well when performance is measured either by a misclassiﬁcation error or by a statistic of the ROC curve (such as the area under the curve). Speciﬁcally, we provide such an equivalence relationship between a generalization of Freund et al.’s RankBoost algorithm, called the “P-Norm Push,” and a particular cost-sensitive classiﬁcation algorithm that generalizes AdaBoost, which we call “P-Classiﬁcation.” We discuss and validate the potential beneﬁts of this equivalence relationship, and perform controlled experiments to understand P-Classiﬁcation’s empirical performance. There is no established equivalence relationship for logistic regression and its ranking counterpart, so we introduce a logistic-regression-style algorithm that aims in between classiﬁcation and ranking, and has promising experimental performance with respect to both tasks. Keywords: supervised classiﬁcation, bipartite ranking, area under the curve, rank statistics, boosting, logistic regression</p><p>3 0.42612779 <a title="5-lsi-3" href="./jmlr-2011-Universality%2C_Characteristic_Kernels_and_RKHS_Embedding_of_Measures.html">98 jmlr-2011-Universality, Characteristic Kernels and RKHS Embedding of Measures</a></p>
<p>Author: Bharath K. Sriperumbudur, Kenji Fukumizu, Gert R.G. Lanckriet</p><p>Abstract: Over the last few years, two different notions of positive deﬁnite (pd) kernels—universal and characteristic—have been developing in parallel in machine learning: universal kernels are proposed in the context of achieving the Bayes risk by kernel-based classiﬁcation/regression algorithms while characteristic kernels are introduced in the context of distinguishing probability measures by embedding them into a reproducing kernel Hilbert space (RKHS). However, the relation between these two notions is not well understood. The main contribution of this paper is to clarify the relation between universal and characteristic kernels by presenting a unifying study relating them to RKHS embedding of measures, in addition to clarifying their relation to other common notions of strictly pd, conditionally strictly pd and integrally strictly pd kernels. For radial kernels on Rd , all these notions are shown to be equivalent. Keywords: kernel methods, characteristic kernels, Hilbert space embeddings, universal kernels, strictly positive deﬁnite kernels, integrally strictly positive deﬁnite kernels, conditionally strictly positive deﬁnite kernels, translation invariant kernels, radial kernels, binary classiﬁcation, homogeneity testing</p><p>4 0.33537188 <a title="5-lsi-4" href="./jmlr-2011-Large_Margin_Hierarchical_Classification_with_Mutually_Exclusive_Class_Membership.html">52 jmlr-2011-Large Margin Hierarchical Classification with Mutually Exclusive Class Membership</a></p>
<p>Author: Huixin Wang, Xiaotong Shen, Wei Pan</p><p>Abstract: In hierarchical classiﬁcation, class labels are structured, that is each label value corresponds to one non-root node in a tree, where the inter-class relationship for classiﬁcation is speciﬁed by directed paths of the tree. In such a situation, the focus has been on how to leverage the interclass relationship to enhance the performance of ﬂat classiﬁcation, which ignores such dependency. This is critical when the number of classes becomes large relative to the sample size. This paper considers single-path or partial-path hierarchical classiﬁcation, where only one path is permitted from the root to a leaf node. A large margin method is introduced based on a new concept of generalized margins with respect to hierarchy. For implementation, we consider support vector machines and ψ-learning. Numerical and theoretical analyses suggest that the proposed method achieves the desired objective and compares favorably against strong competitors in the literature, including its ﬂat counterparts. Finally, an application to gene function prediction is discussed. Keywords: difference convex programming, gene function annotation, margins, multi-class classiﬁcation, structured learning</p><p>5 0.25129169 <a title="5-lsi-5" href="./jmlr-2011-Unsupervised_Supervised_Learning_II%3A_Margin-Based_Classification_Without_Labels.html">100 jmlr-2011-Unsupervised Supervised Learning II: Margin-Based Classification Without Labels</a></p>
<p>Author: Krishnakumar Balasubramanian, Pinar Donmez, Guy Lebanon</p><p>Abstract: Many popular linear classiﬁers, such as logistic regression, boosting, or SVM, are trained by optimizing a margin-based risk function. Traditionally, these risk functions are computed based on a labeled data set. We develop a novel technique for estimating such risks using only unlabeled data and the marginal label distribution. We prove that the proposed risk estimator is consistent on high-dimensional data sets and demonstrate it on synthetic and real-world data. In particular, we show how the estimate is used for evaluating classiﬁers in transfer learning, and for training classiﬁers with no labeled data whatsoever. Keywords: classiﬁcation, large margin, maximum likelihood</p><p>6 0.24958731 <a title="5-lsi-6" href="./jmlr-2011-Neyman-Pearson_Classification%2C_Convexity_and_Stochastic_Constraints.html">69 jmlr-2011-Neyman-Pearson Classification, Convexity and Stochastic Constraints</a></p>
<p>7 0.14370865 <a title="5-lsi-7" href="./jmlr-2011-Information%2C_Divergence_and_Risk_for_Binary_Experiments.html">43 jmlr-2011-Information, Divergence and Risk for Binary Experiments</a></p>
<p>8 0.14239801 <a title="5-lsi-8" href="./jmlr-2011-Learning_from_Partial_Labels.html">58 jmlr-2011-Learning from Partial Labels</a></p>
<p>9 0.13760912 <a title="5-lsi-9" href="./jmlr-2011-The_Stationary_Subspace_Analysis_Toolbox.html">92 jmlr-2011-The Stationary Subspace Analysis Toolbox</a></p>
<p>10 0.12469759 <a title="5-lsi-10" href="./jmlr-2011-Smoothness%2C_Disagreement_Coefficient%2C_and_the_Label_Complexity_of_Agnostic_Active_Learning.html">85 jmlr-2011-Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning</a></p>
<p>11 0.1159257 <a title="5-lsi-11" href="./jmlr-2011-Efficient_Learning_with_Partially_Observed_Attributes.html">29 jmlr-2011-Efficient Learning with Partially Observed Attributes</a></p>
<p>12 0.11405668 <a title="5-lsi-12" href="./jmlr-2011-Exploiting_Best-Match_Equations_for_Efficient_Reinforcement_Learning.html">33 jmlr-2011-Exploiting Best-Match Equations for Efficient Reinforcement Learning</a></p>
<p>13 0.11295109 <a title="5-lsi-13" href="./jmlr-2011-Learning_Transformation_Models_for_Ranking_and_Survival_Analysis.html">56 jmlr-2011-Learning Transformation Models for Ranking and Survival Analysis</a></p>
<p>14 0.11151344 <a title="5-lsi-14" href="./jmlr-2011-High-dimensional_Covariance_Estimation_Based_On_Gaussian_Graphical_Models.html">39 jmlr-2011-High-dimensional Covariance Estimation Based On Gaussian Graphical Models</a></p>
<p>15 0.11065166 <a title="5-lsi-15" href="./jmlr-2011-MSVMpack%3A_A_Multi-Class_Support_Vector_Machine_Package.html">62 jmlr-2011-MSVMpack: A Multi-Class Support Vector Machine Package</a></p>
<p>16 0.10137344 <a title="5-lsi-16" href="./jmlr-2011-Forest_Density_Estimation.html">35 jmlr-2011-Forest Density Estimation</a></p>
<p>17 0.099844813 <a title="5-lsi-17" href="./jmlr-2011-Differentially_Private_Empirical_Risk_Minimization.html">22 jmlr-2011-Differentially Private Empirical Risk Minimization</a></p>
<p>18 0.098568894 <a title="5-lsi-18" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>19 0.095141053 <a title="5-lsi-19" href="./jmlr-2011-Discriminative_Learning_of_Bayesian_Networks_via_Factorized_Conditional_Log-Likelihood.html">25 jmlr-2011-Discriminative Learning of Bayesian Networks via Factorized Conditional Log-Likelihood</a></p>
<p>20 0.094971627 <a title="5-lsi-20" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.039), (9, 0.013), (10, 0.024), (24, 0.023), (31, 0.081), (32, 0.568), (41, 0.023), (71, 0.012), (73, 0.018), (78, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95448333 <a title="5-lda-1" href="./jmlr-2011-MULAN%3A_A_Java_Library_for_Multi-Label_Learning.html">63 jmlr-2011-MULAN: A Java Library for Multi-Label Learning</a></p>
<p>Author: Grigorios Tsoumakas, Eleftherios Spyromitros-Xioufis, Jozef Vilcek, Ioannis Vlahavas</p><p>Abstract: M ULAN is a Java library for learning from multi-label data. It offers a variety of classiﬁcation, ranking, thresholding and dimensionality reduction algorithms, as well as algorithms for learning from hierarchically structured labels. In addition, it contains an evaluation framework that calculates a rich variety of performance measures. Keywords: multi-label data, classiﬁcation, ranking, thresholding, dimensionality reduction, hierarchical classiﬁcation, evaluation 1. Multi-Label Learning A multi-label data set consists of training examples that are associated with a subset of a ﬁnite set of labels. Nowadays, multi-label data are becoming ubiquitous. They arise in an increasing number and diversity of applications, such as semantic annotation of images and video, web page categorization, direct marketing, functional genomics and music categorization into genres and emotions. There exist two major multi-label learning tasks (Tsoumakas et al., 2010): multi-label classiﬁcation and label ranking. The former is concerned with learning a model that outputs a bipartition of the set of labels into relevant and irrelevant with respect to a query instance. The latter is concerned with learning a model that outputs a ranking of the labels according to their relevance to a query instance. Some algorithms learn models that serve both tasks. Several algorithms learn models that primarily output a vector of numerical scores, one for each label. This vector is then converted to a ranking after solving ties, or to a bipartition, after thresholding (Ioannou et al., 2010). Multi-label learning methods addressing these tasks can be grouped into two categories (Tsoumakas et al., 2010): problem transformation and algorithm adaptation. The ﬁrst group of methods are algorithm independent. They transform the learning task into one or more singlelabel classiﬁcation tasks, for which a large body of learning algorithms exists. The second group of methods extend speciﬁc learning algorithms in order to handle multi-label data directly. There exist extensions of decision tree learners, nearest neighbor classiﬁers, neural networks, ensemble methods, support vector machines, kernel methods, genetic algorithms and others. Multi-label learning stretches across several other tasks. When labels are structured as a treeshaped hierarchy or a directed acyclic graph, then we have the interesting task of hierarchical multilabel learning. Dimensionality reduction is another important task for multi-label data, as it is for c 2011 Grigorios Tsoumakas, Eleftherios Spyromitros-Xiouﬁs, Jozef Vilcek and Ioannis Vlahavas. T SOUMAKAS , S PYROMITROS -X IOUFIS , V ILCEK AND V LAHAVAS any kind of data. When bags of instances are used to represent a training object, then multi-instance multi-label learning algorithms are required. There also exist semi-supervised learning and active learning algorithms for multi-label data. 2. The M ULAN Library The main goal of M ULAN is to bring the beneﬁts of machine learning open source software (MLOSS) (Sonnenburg et al., 2007) to people working with multi-label data. The availability of MLOSS is especially important in emerging areas like multi-label learning, because it removes the burden of implementing related work and speeds up the scientiﬁc progress. In multi-label learning, an extra burden is implementing appropriate evaluation measures, since these are different compared to traditional supervised learning tasks. Evaluating multi-label algorithms with a variety of measures, is considered important by the community, due to the different types of output (bipartition, ranking) and diverse applications. Towards this goal, M ULAN offers a plethora of state-of-the-art algorithms for multi-label classiﬁcation and label ranking and an evaluation framework that computes a large variety of multi-label evaluation measures through hold-out evaluation and cross-validation. In addition, the library offers a number of thresholding strategies that produce bipartitions from score vectors, simple baseline methods for multi-label dimensionality reduction and support for hierarchical multi-label classiﬁcation, including an implemented algorithm. M ULAN is a library. As such, it offers only programmatic API to the library users. There is no graphical user interface (GUI) available. The possibility to use the library via command line, is also currently not supported. Another drawback of M ULAN is that it runs everything in main memory so there exist limitations with very large data sets. M ULAN is written in Java and is built on top of Weka (Witten and Frank, 2005). This choice was made in order to take advantage of the vast resources of Weka on supervised learning algorithms, since many state-of-the-art multi-label learning algorithms are based on problem transformation. The fact that several machine learning researchers and practitioners are familiar with Weka was another reason for this choice. However, many aspects of the library are independent of Weka and there are interfaces for most of the core classes. M ULAN is an advocate of open science in general. One of the unique features of the library is a recently introduced experiments package, whose goal is to host code that reproduces experimental results reported on published papers on multi-label learning. To the best of our knowledge, most of the general learning platforms, like Weka, don’t support multi-label data. There are currently only a number of implementations of speciﬁc multi-label learning algorithms, but not a general library like M ULAN. 3. Using M ULAN This section presents an example of how to setup an experiment for empirically evaluating two multi-label algorithms on a multi-label data set using cross-validation. We create a new Java class for this experiment, which we call MulanExp1.java. The ﬁrst thing to do is load the multi-label data set that will be used for the empirical evaluation. M ULAN requires two text ﬁles for the speciﬁcation of a data set. The ﬁrst one is in the ARFF format of Weka. The labels should be speciﬁed as nominal attributes with values “0” and “1” indicating 2412 M ULAN : A JAVA L IBRARY FOR M ULTI -L ABEL L EARNING absence and presence of the label respectively. The second ﬁle is in XML format. It speciﬁes the labels and any hierarchical relationships among them. Hierarchies of labels can be expressed in the XML ﬁle by nesting the label tag. In our example, the two ﬁlenames are given to the experiment class through command-line parameters. String arffFile = Utils.getOption(</p><p>same-paper 2 0.82517928 <a title="5-lda-2" href="./jmlr-2011-A_Refined_Margin_Analysis_for_Boosting_Algorithms_via_Equilibrium_Margin.html">5 jmlr-2011-A Refined Margin Analysis for Boosting Algorithms via Equilibrium Margin</a></p>
<p>Author: Liwei Wang, Masashi Sugiyama, Zhaoxiang Jing, Cheng Yang, Zhi-Hua Zhou, Jufu Feng</p><p>Abstract: Much attention has been paid to the theoretical explanation of the empirical success of AdaBoost. The most inﬂuential work is the margin theory, which is essentially an upper bound for the generalization error of any voting classiﬁer in terms of the margin distribution over the training data. However, important questions were raised about the margin explanation. Breiman (1999) proved a bound in terms of the minimum margin, which is sharper than the margin distribution bound. He argued that the minimum margin would be better in predicting the generalization error. Grove and Schuurmans (1998) developed an algorithm called LP-AdaBoost which maximizes the minimum margin while keeping all other factors the same as AdaBoost. In experiments however, LP-AdaBoost usually performs worse than AdaBoost, putting the margin explanation into serious doubt. In this paper, we make a reﬁned analysis of the margin theory. We prove a bound in terms of a new margin measure called the Equilibrium margin (Emargin). The Emargin bound is uniformly ©2011 Liwei Wang, Masashi Sugiyama, Zhaoxiang Jing, Cheng Yang, Zhi-Hua Zhou and Jufu Feng. WANG , S UGIYAMA , J ING , YANG , Z HOU AND F ENG sharper than Breiman’s minimum margin bound. Thus our result suggests that the minimum margin may be not crucial for the generalization error. We also show that a large Emargin and a small empirical error at Emargin imply a smaller bound of the generalization error. Experimental results on benchmark data sets demonstrate that AdaBoost usually has a larger Emargin and a smaller test error than LP-AdaBoost, which agrees well with our theory. Keywords: boosting, margin bounds, voting classiﬁer</p><p>3 0.8130216 <a title="5-lda-3" href="./jmlr-2011-Discriminative_Learning_of_Bayesian_Networks_via_Factorized_Conditional_Log-Likelihood.html">25 jmlr-2011-Discriminative Learning of Bayesian Networks via Factorized Conditional Log-Likelihood</a></p>
<p>Author: Alexandra M. Carvalho, Teemu Roos, Arlindo L. Oliveira, Petri Myllymäki</p><p>Abstract: We propose an efﬁcient and parameter-free scoring criterion, the factorized conditional log-likelihood (ˆ fCLL), for learning Bayesian network classiﬁers. The proposed score is an approximation of the conditional log-likelihood criterion. The approximation is devised in order to guarantee decomposability over the network structure, as well as efﬁcient estimation of the optimal parameters, achieving the same time and space complexity as the traditional log-likelihood scoring criterion. The resulting criterion has an information-theoretic interpretation based on interaction information, which exhibits its discriminative nature. To evaluate the performance of the proposed criterion, we present an empirical comparison with state-of-the-art classiﬁers. Results on a large suite of benchmark data sets from the UCI repository show that ˆ fCLL-trained classiﬁers achieve at least as good accuracy as the best compared classiﬁers, using signiﬁcantly less computational resources. Keywords: Bayesian networks, discriminative learning, conditional log-likelihood, scoring criterion, classiﬁcation, approximation c 2011 Alexandra M. Carvalho, Teemu Roos, Arlindo L. Oliveira and Petri Myllym¨ ki. a ¨ C ARVALHO , ROOS , O LIVEIRA AND M YLLYM AKI</p><p>4 0.75202125 <a title="5-lda-4" href="./jmlr-2011-Information%2C_Divergence_and_Risk_for_Binary_Experiments.html">43 jmlr-2011-Information, Divergence and Risk for Binary Experiments</a></p>
<p>Author: Mark D. Reid, Robert C. Williamson</p><p>Abstract: We unify f -divergences, Bregman divergences, surrogate regret bounds, proper scoring rules, cost curves, ROC-curves and statistical information. We do this by systematically studying integral and variational representations of these objects and in so doing identify their representation primitives which all are related to cost-sensitive binary classiﬁcation. As well as developing relationships between generative and discriminative views of learning, the new machinery leads to tight and more general surrogate regret bounds and generalised Pinsker inequalities relating f -divergences to variational divergence. The new viewpoint also illuminates existing algorithms: it provides a new derivation of Support Vector Machines in terms of divergences and relates maximum mean discrepancy to Fisher linear discriminants. Keywords: classiﬁcation, loss functions, divergence, statistical information, regret bounds</p><p>5 0.35718736 <a title="5-lda-5" href="./jmlr-2011-Waffles%3A_A_Machine_Learning_Toolkit.html">102 jmlr-2011-Waffles: A Machine Learning Toolkit</a></p>
<p>Author: Michael Gashler</p><p>Abstract: We present a breadth-oriented collection of cross-platform command-line tools for researchers in machine learning called Wafﬂes. The Wafﬂes tools are designed to offer a broad spectrum of functionality in a manner that is friendly for scripted automation. All functionality is also available in a C++ class library. Wafﬂes is available under the GNU Lesser General Public License. Keywords: machine learning, toolkits, data mining, C++, open source</p><p>6 0.32469365 <a title="5-lda-6" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>7 0.31564027 <a title="5-lda-7" href="./jmlr-2011-On_the_Relation_between_Realizable_and_Nonrealizable_Cases_of_the_Sequence_Prediction_Problem.html">72 jmlr-2011-On the Relation between Realizable and Nonrealizable Cases of the Sequence Prediction Problem</a></p>
<p>8 0.30020991 <a title="5-lda-8" href="./jmlr-2011-MSVMpack%3A_A_Multi-Class_Support_Vector_Machine_Package.html">62 jmlr-2011-MSVMpack: A Multi-Class Support Vector Machine Package</a></p>
<p>9 0.29671904 <a title="5-lda-9" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>10 0.28888506 <a title="5-lda-10" href="./jmlr-2011-Computationally_Efficient_Convolved_Multiple_Output_Gaussian_Processes.html">17 jmlr-2011-Computationally Efficient Convolved Multiple Output Gaussian Processes</a></p>
<p>11 0.28883749 <a title="5-lda-11" href="./jmlr-2011-The_Sample_Complexity_of_Dictionary_Learning.html">91 jmlr-2011-The Sample Complexity of Dictionary Learning</a></p>
<p>12 0.28389862 <a title="5-lda-12" href="./jmlr-2011-Large_Margin_Hierarchical_Classification_with_Mutually_Exclusive_Class_Membership.html">52 jmlr-2011-Large Margin Hierarchical Classification with Mutually Exclusive Class Membership</a></p>
<p>13 0.28368193 <a title="5-lda-13" href="./jmlr-2011-Unsupervised_Similarity-Based_Risk_Stratification_for_Cardiovascular_Events_Using_Long-Term_Time-Series_Data.html">99 jmlr-2011-Unsupervised Similarity-Based Risk Stratification for Cardiovascular Events Using Long-Term Time-Series Data</a></p>
<p>14 0.28256661 <a title="5-lda-14" href="./jmlr-2011-Smoothness%2C_Disagreement_Coefficient%2C_and_the_Label_Complexity_of_Agnostic_Active_Learning.html">85 jmlr-2011-Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning</a></p>
<p>15 0.28197923 <a title="5-lda-15" href="./jmlr-2011-Neyman-Pearson_Classification%2C_Convexity_and_Stochastic_Constraints.html">69 jmlr-2011-Neyman-Pearson Classification, Convexity and Stochastic Constraints</a></p>
<p>16 0.27927965 <a title="5-lda-16" href="./jmlr-2011-Parameter_Screening_and_Optimisation_for_ILP_using_Designed_Experiments.html">76 jmlr-2011-Parameter Screening and Optimisation for ILP using Designed Experiments</a></p>
<p>17 0.2763713 <a title="5-lda-17" href="./jmlr-2011-On_Equivalence_Relationships_Between_Classification_and_Ranking_Algorithms.html">71 jmlr-2011-On Equivalence Relationships Between Classification and Ranking Algorithms</a></p>
<p>18 0.27583125 <a title="5-lda-18" href="./jmlr-2011-A_Bayesian_Approach_for_Learning_and_Planning_in_Partially_Observable_Markov_Decision_Processes.html">1 jmlr-2011-A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes</a></p>
<p>19 0.27380651 <a title="5-lda-19" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<p>20 0.27353334 <a title="5-lda-20" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
