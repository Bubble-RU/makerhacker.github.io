<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>23 jmlr-2011-DirectLiNGAM: A Direct Method for Learning a Linear Non-Gaussian Structural Equation Model</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-23" href="#">jmlr2011-23</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>23 jmlr-2011-DirectLiNGAM: A Direct Method for Learning a Linear Non-Gaussian Structural Equation Model</h1>
<br/><p>Source: <a title="jmlr-2011-23-pdf" href="http://jmlr.org/papers/volume12/shimizu11a/shimizu11a.pdf">pdf</a></p><p>Author: Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvärinen, Yoshinobu Kawahara, Takashi Washio, Patrik O. Hoyer, Kenneth Bollen</p><p>Abstract: Structural equation models and Bayesian networks have been widely used to analyze causal relations between continuous variables. In such frameworks, linear acyclic models are typically used to model the data-generating process of variables. Recently, it was shown that use of non-Gaussianity identiﬁes the full structure of a linear acyclic model, that is, a causal ordering of variables and their connection strengths, without using any prior knowledge on the network structure, which is not the case with conventional methods. However, existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a ﬁnite number of steps. In this paper, we propose a new direct method to estimate a causal ordering and connection strengths based on non-Gaussianity. In contrast to the previous methods, our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small ﬁxed number of steps if the data strictly follows the model, that is, if all the model assumptions are met and the sample size is inﬁnite. Keywords: structural equation models, Bayesian networks, independent component analysis, non-Gaussianity, causal discovery c 2011 Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyv¨ rinen, Yoshinobu Kawahara, Takashi Washio, Patrik O. Hoyer a and Kenneth Bollen ¨ S HIMIZU , I NAZUMI , S OGAWA , H YV ARINEN , K AWAHARA , WASHIO , H OYER AND B OLLEN</p><p>Reference: <a title="jmlr-2011-23-reference" href="../jmlr2011_reference/jmlr-2011-DirectLiNGAM%3A_A_Direct_Method_for_Learning_a_Linear_Non-Gaussian_Structural_Equation_Model_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 JP  The Institute of Scientiﬁc and Industrial Research Osaka University Mihogaoka 8-1, Ibaraki, Osaka 567-0047, Japan  Aapo Hyv¨ rinen a AAPO . [sent-13, score-0.063]
</p><p>2 Abstract Structural equation models and Bayesian networks have been widely used to analyze causal relations between continuous variables. [sent-30, score-0.237]
</p><p>3 Recently, it was shown that use of non-Gaussianity identiﬁes the full structure of a linear acyclic model, that is, a causal ordering of variables and their connection strengths, without using any prior knowledge on the network structure, which is not the case with conventional methods. [sent-32, score-0.326]
</p><p>4 In this paper, we propose a new direct method to estimate a causal ordering and connection strengths based on non-Gaussianity. [sent-34, score-0.32]
</p><p>5 Keywords: structural equation models, Bayesian networks, independent component analysis, non-Gaussianity, causal discovery c 2011 Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyv¨ rinen, Yoshinobu Kawahara, Takashi Washio, Patrik O. [sent-36, score-0.199]
</p><p>6 Introduction Many empirical sciences aim to discover and understand causal mechanisms underlying various natural phenomena and human social behavior. [sent-38, score-0.199]
</p><p>7 An effective way to study causal relationships is to conduct a controlled experiment. [sent-39, score-0.199]
</p><p>8 Thus, it is necessary and important to develop methods for causal inference based on the data that do not come from such controlled experiments. [sent-42, score-0.199]
</p><p>9 , 1993) are widely applied to analyze causal relationships in many empirical studies. [sent-44, score-0.199]
</p><p>10 A linear acyclic model that is a special case of SEM and BN is typically used to analyze causal effects between continuous variables. [sent-45, score-0.254]
</p><p>11 Estimation of the model commonly uses only the covariance structure of the data and in most cases cannot identify the full structure, that is, a causal ordering and connection strengths, of the model with no prior knowledge on the structure (Pearl, 2000; Spirtes et al. [sent-46, score-0.257]
</p><p>12 (2006), a non-Gaussian variant of SEM and BN called a linear non-Gaussian acyclic model (LiNGAM) was proposed, and its full structure was shown to be identiﬁable without pre-specifying a causal order of the variables. [sent-49, score-0.23]
</p><p>13 (2006) and is closely related to independent component analysis (ICA) (Hyv¨ rinen et al. [sent-53, score-0.063]
</p><p>14 Most of major ICA algorithms including Amari (1998) and Hyv¨ rinen (1999) are iterative search a methods (Hyv¨ rinen et al. [sent-60, score-0.126]
</p><p>15 In this paper, we propose a new direct method to estimate a causal ordering of variables in the LiNGAM with no prior knowledge on the structure. [sent-67, score-0.262]
</p><p>16 The new method estimates a causal order of variables by successively subtracting the effect of each independent component from given data in the model, and this process is completed in steps equal to the number of the variables in the model. [sent-68, score-0.199]
</p><p>17 These features of the new method enable more accurate estimation of a causal order of the variables in a disambiguated and direct procedure. [sent-71, score-0.236]
</p><p>18 Once the causal orders of variables is identiﬁed, the connection strengths between the variables are easily estimated using some conventional covariance-based methods such as least squares and maximum likelihood approaches (Bollen, 1989). [sent-72, score-0.377]
</p><p>19 Assume that observed data are generated from a process represented graphically by a directed acyclic graph, that is, DAG. [sent-92, score-0.165]
</p><p>20 Let us represent this DAG by a m×m adjacency matrix B={bi j } where every bi j represents the connection strength from a variable x j to another xi in the DAG. [sent-93, score-0.16]
</p><p>21 Moreover, let us denote by k(i) a causal order of variables xi in the DAG so that no later variable determines or has a directed path on any earlier variable. [sent-94, score-0.33]
</p><p>22 (A directed path from xi to x j is a sequence of directed edges such that x j is reachable from xi . [sent-95, score-0.243]
</p><p>23 We ﬁrst evaluate pairwise independence between a variable and each of the residuals and next take the sum of the pairwise measures over the residuals. [sent-111, score-0.093]
</p><p>24 We use the following statistic to evaluate independence between a variable x j and cov(xi ,x ) ( j) its residuals ri = xi − var(x j )j x j when xi is regressed on x j : Tkernel (x j ;U) =  ∑  ( j)  MI kernel (x j , ri ). [sent-113, score-0.093]
</p><p>25 2 DirectLiNGAM Algorithm We now propose a new direct algorithm called DirectLiNGAM to estimate a causal ordering and the connection strengths in the LiNGAM (2): DirectLiNGAM algorithm 1. [sent-119, score-0.32]
</p><p>26 Repeat until p−1 subscripts are appended to K : (a) Perform least squares regressions of xi on x j for all i ∈ U\K (i = j) and compute the residual vectors r( j) and the residual data matrix R( j) from the data matrix X for all j ∈ U\K . [sent-122, score-0.07]
</p><p>27 Find a variable xm that is most independent of its residuals:  xm = arg min Tkernel (x j ;U\K), j∈U\K  where Tkernel is the independence measure deﬁned in Equation (7). [sent-123, score-0.113]
</p><p>28 Construct a strictly lower triangular matrix B by following the order in K , and estimate the connection strengths bi j by using some conventional covariance-based regression such as least squares and maximum likelihood approaches on the original random vector x and the original data matrix X. [sent-129, score-0.174]
</p><p>29 Meanwhile, the ICA-LiNGAM requires O(p4 ) time to ﬁnd a causal order in Step 5. [sent-138, score-0.199]
</p><p>30 4 Use of Prior Knowledge Although DirectLiNGAM requires no prior knowledge on the structure, more efﬁcient learning can be achieved if some prior knowledge on a part of the structure is available because then the number of causal orders and connection strengths to be estimated gets smaller. [sent-146, score-0.391]
</p><p>31 Due to the deﬁnition of exogenous variables and that of prior knowledge matrix Aknw , we readily obtain the following three lemmas. [sent-149, score-0.124]
</p><p>32 An observed variable x j is exogenous if aknw is zero for all i= j. [sent-151, score-0.25]
</p><p>33 An observed variable x j is endogenous, that is, not exogenous, if there exist such i= j that aknw is unity. [sent-153, score-0.152]
</p><p>34 An observed variable x j does not receive the effect of xi if aknw is zero. [sent-155, score-0.152]
</p><p>35 ji The principle of making DirectLiNGAM algorithm more accurate and faster based on prior knowledge is as follows. [sent-156, score-0.053]
</p><p>36 We ﬁrst ﬁnd an exogenous variable by applying Lemma 3 instead of Lemma 1 if an exogenous variable is identiﬁed based on prior knowledge. [sent-157, score-0.27]
</p><p>37 Then we do not have to evaluate independence between any observed variable and its residuals. [sent-158, score-0.061]
</p><p>38 If no exogenous variable is identiﬁed based on prior knowledge, we next ﬁnd endogenous (non-exogenous) variables by applying Lemma 4. [sent-159, score-0.181]
</p><p>39 Since endogenous variables are never exogenous we can narrow down the search space to ﬁnd an exogenous variable based on Lemma 1. [sent-160, score-0.253]
</p><p>40 We can further skip to compute the residual of an observed variable and take the variable itself as the residual if its regressor does not receive the effect of the variable due to Lemma 5. [sent-161, score-0.142]
</p><p>41 Thus, we can decrease the number of causal orders and connection strengths to be estimated, and it improves the accuracy and computational time. [sent-162, score-0.283]
</p><p>42 The principle can also be used to further analyze the residuals and ﬁnd the next exogenous residual because of Corollary 1. [sent-163, score-0.165]
</p><p>43 2a-2 Denote by V ( j) a set of such a variable subscript i ∈ U\K (i = j) that aknw = 0 for all j ∈ Uc . [sent-167, score-0.152]
</p><p>44 ij ( j)  := xi for all i ∈ V ( j) , next perform least squares regressions of xi on x j for all i ∈ U\K\V ( j) (i = j) and estimate the residual vectors r( j) and the residual data matrix R( j) from the data matrix X for all j ∈ Uc . [sent-168, score-0.07]
</p><p>45 Otherwise, ﬁnd a variable xm in Uc that is most independent of the residuals: First set ri  xm = arg min Tkernel (x j ;U\K), j∈Uc  where Tkernel is the independence measure deﬁned in Equation (7). [sent-170, score-0.113]
</p><p>46 Right: Scatterplots of the estimated bi j by ICA-LiNGAM versus the true values for sparse networks. [sent-183, score-0.108]
</p><p>47 Further we similarly generated 5 data sets based on dense (full) networks, that is, full DAGs with every pair of variables is connected by a directed edge, under each combination of number of variables p and sample size n. [sent-193, score-0.147]
</p><p>48 Then we tested DirectLiNGAM and ICA-LiNGAM on the data sets generated by sparse networks or dense (full) networks. [sent-194, score-0.078]
</p><p>49 We computed the distance between the true B and ones estimated by DirectLiNGAM and ICALiNGAM using the Frobenius norm deﬁned as trace{(Btrue − B)T (Btrue − B)}. [sent-209, score-0.056]
</p><p>50 1236  D IRECT L I NGAM: A DIRECT METHOD FOR A LINEAR NON -G AUSSIAN SEM  Sparse networks DirectLiNGAM  ICA-LiNGAM  DirectLiNGAM with prior knowledge (50%)  dim. [sent-210, score-0.064]
</p><p>51 61  Dense (full) networks DirectLiNGAM  ICA-LiNGAM  DirectLiNGAM with prior knowledge (50%)  Table 1: Median distances (Frobenius norms) between true B and estimated B of DirectLiNGAM and ICA-LiNGAM with ﬁve replications. [sent-306, score-0.12]
</p><p>52 Tables 1 and 2 show the median distances (Frobenius norms) and median computational times (CPU times), respectively. [sent-307, score-0.062]
</p><p>53 Dense (full) networks DirectLiNGAM  ICA-LiNGAM  DirectLiNGAM with prior knowledge (50%)  dim. [sent-471, score-0.064]
</p><p>54 Finally, we generated data sets in the same manner as above and gave some prior knowledge to DirectLiNGAM by creating prior knowledge matrices Aknw as follows. [sent-487, score-0.087]
</p><p>55 The bottoms of Tables 1 and 2 show the median distances and median computational times. [sent-490, score-0.062]
</p><p>56 Right: Scatterplots of the estimated bi j by ICA-LiNGAM versus the true values for dense (full) networks. [sent-492, score-0.148]
</p><p>57 The reason would probably be that for dense (full) networks more prior knowledge about where directed paths exist were likely to be given and it narrowed down the search space more efﬁciently. [sent-494, score-0.211]
</p><p>58 Both DirectLiNGAM and ICA-LiNGAM estimate a causal ordering of variables and provide a full DAG. [sent-497, score-0.199]
</p><p>59 Then we have two options to do further analysis (Hyv¨ rinen et al. [sent-498, score-0.063]
</p><p>60 , 2010): i) Find signiﬁcant dia rected edges or direct causal effects bi j and signiﬁcant total causal effects ai j with A=(I − B)−1 ; ii) Estimate redundant directed edges to ﬁnd the underlying DAG. [sent-499, score-0.7]
</p><p>61 1 Application to Physical Data We applied DirectLiNGAM and ICA-LiNGAM on a data set created from a physical system called a double-pendulum, a pendulum with another pendulum attached to its end (Meirovitch, 1986) as in Figure 3. [sent-504, score-0.088]
</p><p>62 The four variables were θ1 : the angle between the top limb and the vertical, θ2 : ˙ the angle between the bottom limb and the vertical, ω1 : the angular speed of θ1 or θ1 and ω2 : the ˙ angular speed of θ2 or θ2 . [sent-510, score-0.102]
</p><p>63 Æ  Æ    Æ  Ö    Ö  Ö    Æ   DirectLiNGAM  Ö ICA-LiNGAM  Figure 4: Left: The estimated network by DirectLiNGAM. [sent-522, score-0.056]
</p><p>64 Only signiﬁcant directed edges are shown with 5% signiﬁcance level. [sent-523, score-0.136]
</p><p>65 No signiﬁcant directed edges were found with 5% signiﬁcance level. [sent-525, score-0.136]
</p><p>66 Æ  Æ    Ö  Æ   Ö    Ö  2+    Æ  Ö  /-5  Figure 5: Left: The estimated network by PC algorithm with 5% signiﬁcance level. [sent-526, score-0.056]
</p><p>67 An undirected edge between two variables means that there is a directed edge from a variable to the other or the reverse. [sent-528, score-0.259]
</p><p>68 The estimated adjacency matrices B of θ1 , θ2 , ω1 and ω2 were as follows:  DirectLiNGAM :  ICA − LiNGAM :  θ1 θ2 ω1 ω2  θ1 0 0 0 0 θ2  −0. [sent-535, score-0.108]
</p><p>69 89 0   The estimated orderings by DirectLiNGAM and ICA-LiNGAM were identical, but the estimated connection strengths were very different. [sent-548, score-0.196]
</p><p>70 The estimated networks by DirectLiNGAM and ICA-LiNGAM are graphically shown in Figure 4, where only signiﬁcant directed edges (direct causal effects) bi j are shown with 5% signiﬁcance level. [sent-550, score-0.508]
</p><p>71 Though the directed edge from θ1 to θ2 might be a bit difﬁcult to interpret, the effect of θ1 on θ2 was estimated to be negligible since the ˆ coefﬁcient of determination (Bollen, 1989) of θ2 , that is, 1−var(e2 )/var(θ2 ), was very small and ˆ was 0. [sent-552, score-0.227]
</p><p>72 ) On the other hand, ICA-LiNGAM could not ﬁnd any signiﬁcant directed edges since it gave very different estimates for different bootstrap samples. [sent-557, score-0.171]
</p><p>73 Figure 5 shows the estimated networks by PC algorithm (Spirtes and Glymour, 1991) with 5% signiﬁcance level and GES (Chickering, 2002) with the Gaussianity assumption. [sent-559, score-0.094]
</p><p>74 PC algorithm found the same directed edge from θ1 on ω1 as DirectLiNGAM did, but did not found the directed edge from θ2 on ω2 . [sent-561, score-0.342]
</p><p>75 GES found the same directed edge from θ1 on θ2 as DirectLiNGAM did, but did not ﬁnd that the angle speeds ω1 and ω2 were determined by the angles θ1 or θ2 . [sent-562, score-0.221]
</p><p>76 We also computed the 95% conﬁdence intervals of the total causal effects ai j using bootstrap. [sent-563, score-0.223]
</p><p>77 DirectLiNGAM found signiﬁcant total causal effects from θ1 on θ2 , from θ1 on ω1 , from θ1 on ω2 , from θ2 on ω1 , and from θ2 on ω2 . [sent-564, score-0.223]
</p><p>78 ICA-LiNGAM only found a signiﬁcant total causal effect from θ2 on ω2 . [sent-566, score-0.199]
</p><p>79 A directed edge between two variables in the ﬁgure means that there could be a directed edge between the two. [sent-582, score-0.342]
</p><p>80 A bi-directed edge between two variables means that the relation is not modeled. [sent-583, score-0.064]
</p><p>81 For instance, there could be latent confounders between the two, there could be a directed edge between the two, or the two could be independent. [sent-584, score-0.222]
</p><p>82 ables, x1 : father’s occupation level, x2 : son’s income, x3 : father’s education, x4 : son’s occupation level, x5 : son’s education, x6 : number of siblings. [sent-585, score-0.306]
</p><p>83 Figure 6 shows domain knowledge about their causal relations (Duncan et al. [sent-589, score-0.199]
</p><p>84 The estimated adjacency matrices B by DirectLiNGAM and ICA-LiNGAM were as follows: x1 x2  x1 0 0  33. [sent-593, score-0.108]
</p><p>85 08 0 1242  D IRECT L I NGAM: A DIRECT METHOD FOR A LINEAR NON -G AUSSIAN SEM  We subsequently pruned redundant directed edges bi j in the full DAGs by repeatedly applying a sparse method called Adaptive Lasso (Zou, 2006) on each variable and its potential parents. [sent-624, score-0.212]
</p><p>86 08 0  The estimated networks by DirectLiNGAM and ICA-LiNGAM are graphically shown in Figure 7 and Figure 8, respectively. [sent-638, score-0.121]
</p><p>87 All the directed edges estimated by DirectLiNGAM were reasonable to the domain knowledge other than the directed edge from x5 : son’s education to x3 : father’s education. [sent-639, score-0.473]
</p><p>88 Since the sample size was large and yet the estimated model was not fully correct, the mistake on the directed edge between x5 and x3 might imply that some model assumptions might be more or less violated in the data. [sent-640, score-0.227]
</p><p>89 ICA-LiNGAM gave a similar estimated network but did one more mistake that x6 : number of siblings is determined by x5 : son’s education. [sent-641, score-0.15]
</p><p>90 Further, Figure 9 and Figure 10 show the estimated networks by PC algorithm with 5% significance level and GES with the Gaussianity assumption. [sent-642, score-0.094]
</p><p>91 The two conventional methods found a reasonable direction of the edge between x1 : father’s occupation and x3 : father’s education, but they gave a wrong direction of the edge between x1 : father’s occupation and x4 : son’s occupation. [sent-644, score-0.507]
</p><p>92 A red solid directed edge is reasonable to the domain knowledge. [sent-652, score-0.171]
</p><p>93 FatherÕs Education (x3)  SonÕs Education (x5)  FatherÕs  SonÕs  Occupation  Income  (x1) SonÕs  (x2)  Occupation Number of  (x4)  Siblings (x6)  Figure 8: The estimated network by ICA-LiNGAM and Adaptive Lasso. [sent-653, score-0.056]
</p><p>94 A red solid directed edge is reasonable to the domain knowledge. [sent-654, score-0.171]
</p><p>95 An undirected edge between two variables means that there is a directed edge from a variable to the other or the reverse. [sent-675, score-0.259]
</p><p>96 A red solid directed edge is reasonable to the domain knowledge. [sent-676, score-0.171]
</p><p>97 FatherÕs Education (x3)  SonÕs Education (x5)  FatherÕs  SonÕs  Occupation  Income  (x1) SonÕs  (x2)  Occupation (x4)  Number of Siblings (x6)  Figure 10: The estimated network by GES. [sent-677, score-0.056]
</p><p>98 An undirected edge between two variables means that there is a directed edge from a variable to the other or the reverse. [sent-678, score-0.259]
</p><p>99 A red solid directed edge is reasonable to the domain knowledge. [sent-679, score-0.171]
</p><p>100 The adaptive Lasso is a regularization technique for variable selection and assumes the same data generating process as LiNGAM: xi =  ∑  bi j x j + ei . [sent-687, score-0.076]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('directlingam', 0.641), ('son', 0.229), ('lingam', 0.218), ('father', 0.205), ('causal', 0.199), ('sem', 0.167), ('occupation', 0.153), ('aknw', 0.128), ('osaka', 0.115), ('education', 0.11), ('kawahara', 0.109), ('washio', 0.109), ('directed', 0.107), ('bij', 0.103), ('irect', 0.103), ('ngam', 0.103), ('exogenous', 0.098), ('arinen', 0.09), ('awahara', 0.09), ('bollen', 0.09), ('himizu', 0.09), ('nazumi', 0.09), ('ogawa', 0.09), ('ollen', 0.09), ('oyer', 0.09), ('shimizu', 0.087), ('yv', 0.069), ('edge', 0.064), ('sanken', 0.064), ('scatterplots', 0.064), ('tkernel', 0.064), ('hyv', 0.063), ('rinen', 0.063), ('siblings', 0.059), ('aussian', 0.057), ('estimated', 0.056), ('income', 0.054), ('hoyer', 0.054), ('adjacency', 0.052), ('strengths', 0.052), ('bi', 0.052), ('confounders', 0.051), ('ibaraki', 0.051), ('inazumi', 0.051), ('sogawa', 0.051), ('uexo', 0.051), ('ica', 0.047), ('uc', 0.045), ('spirtes', 0.045), ('pendulum', 0.044), ('sociology', 0.044), ('transitional', 0.044), ('helsinki', 0.043), ('dense', 0.04), ('ges', 0.038), ('patrik', 0.038), ('networks', 0.038), ('conventional', 0.038), ('direct', 0.037), ('independence', 0.037), ('pc', 0.036), ('gave', 0.035), ('residual', 0.035), ('non', 0.034), ('aapo', 0.033), ('endogenous', 0.033), ('fastica', 0.033), ('residuals', 0.032), ('ar', 0.032), ('connection', 0.032), ('acyclic', 0.031), ('median', 0.031), ('jp', 0.03), ('bn', 0.029), ('edges', 0.029), ('multimodal', 0.028), ('graphically', 0.027), ('guess', 0.027), ('ji', 0.027), ('prior', 0.026), ('xm', 0.026), ('japan', 0.026), ('angular', 0.026), ('append', 0.026), ('btrue', 0.026), ('duncan', 0.026), ('kalisch', 0.026), ('meirovitch', 0.026), ('mihogaoka', 0.026), ('nonsymmetric', 0.026), ('shohei', 0.026), ('takanori', 0.026), ('uend', 0.026), ('yasuhiro', 0.026), ('speeds', 0.025), ('angle', 0.025), ('variable', 0.024), ('cance', 0.024), ('cpu', 0.024), ('effects', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="23-tfidf-1" href="./jmlr-2011-DirectLiNGAM%3A_A_Direct_Method_for_Learning_a_Linear_Non-Gaussian_Structural_Equation_Model.html">23 jmlr-2011-DirectLiNGAM: A Direct Method for Learning a Linear Non-Gaussian Structural Equation Model</a></p>
<p>Author: Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvärinen, Yoshinobu Kawahara, Takashi Washio, Patrik O. Hoyer, Kenneth Bollen</p><p>Abstract: Structural equation models and Bayesian networks have been widely used to analyze causal relations between continuous variables. In such frameworks, linear acyclic models are typically used to model the data-generating process of variables. Recently, it was shown that use of non-Gaussianity identiﬁes the full structure of a linear acyclic model, that is, a causal ordering of variables and their connection strengths, without using any prior knowledge on the network structure, which is not the case with conventional methods. However, existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a ﬁnite number of steps. In this paper, we propose a new direct method to estimate a causal ordering and connection strengths based on non-Gaussianity. In contrast to the previous methods, our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small ﬁxed number of steps if the data strictly follows the model, that is, if all the model assumptions are met and the sample size is inﬁnite. Keywords: structural equation models, Bayesian networks, independent component analysis, non-Gaussianity, causal discovery c 2011 Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyv¨ rinen, Yoshinobu Kawahara, Takashi Washio, Patrik O. Hoyer a and Kenneth Bollen ¨ S HIMIZU , I NAZUMI , S OGAWA , H YV ARINEN , K AWAHARA , WASHIO , H OYER AND B OLLEN</p><p>2 0.11924003 <a title="23-tfidf-2" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>Author: Ricardo Henao, Ole Winther</p><p>Abstract: In this paper we consider sparse and identiﬁable linear latent variable (factor) and linear Bayesian network models for parsimonious analysis of multivariate data. We propose a computationally efﬁcient method for joint parameter and model inference, and model comparison. It consists of a fully Bayesian hierarchy for sparse models using slab and spike priors (two-component δ-function and continuous mixtures), non-Gaussian latent factors and a stochastic search over the ordering of the variables. The framework, which we call SLIM (Sparse Linear Identiﬁable Multivariate modeling), is validated and bench-marked on artiﬁcial and real biological data sets. SLIM is closest in spirit to LiNGAM (Shimizu et al., 2006), but differs substantially in inference, Bayesian network structure learning and model comparison. Experimentally, SLIM performs equally well or better than LiNGAM with comparable computational complexity. We attribute this mainly to the stochastic search strategy used, and to parsimony (sparsity and identiﬁability), which is an explicit part of the model. We propose two extensions to the basic i.i.d. linear framework: non-linear dependence on observed variables, called SNIM (Sparse Non-linear Identiﬁable Multivariate modeling) and allowing for correlations between latent variables, called CSLIM (Correlated SLIM), for the temporal and/or spatial data. The source code and scripts are available from http://cogsys.imm.dtu.dk/slim/. Keywords: parsimony, sparsity, identiﬁability, factor models, linear Bayesian networks</p><p>3 0.026984641 <a title="23-tfidf-3" href="./jmlr-2011-High-dimensional_Covariance_Estimation_Based_On_Gaussian_Graphical_Models.html">39 jmlr-2011-High-dimensional Covariance Estimation Based On Gaussian Graphical Models</a></p>
<p>Author: Shuheng Zhou, Philipp Rütimann, Min Xu, Peter Bühlmann</p><p>Abstract: Undirected graphs are often used to describe high dimensional distributions. Under sparsity conditions, the graph can be estimated using ℓ1 -penalization methods. We propose and study the following method. We combine a multiple regression approach with ideas of thresholding and reﬁtting: ﬁrst we infer a sparse undirected graphical model structure via thresholding of each among many ℓ1 -norm penalized regression functions; we then estimate the covariance matrix and its inverse using the maximum likelihood estimator. We show that under suitable conditions, this approach yields consistent estimation in terms of graphical structure and fast convergence rates with respect to the operator and Frobenius norm for the covariance matrix and its inverse. We also derive an explicit bound for the Kullback Leibler divergence. Keywords: graphical model selection, covariance estimation, Lasso, nodewise regression, thresholding c 2011 Shuheng Zhou, Philipp R¨ timann, Min Xu and Peter B¨ hlmann. u u ¨ ¨ Z HOU , R UTIMANN , X U AND B UHLMANN</p><p>4 0.023405747 <a title="23-tfidf-4" href="./jmlr-2011-Union_Support_Recovery_in_Multi-task_Learning.html">97 jmlr-2011-Union Support Recovery in Multi-task Learning</a></p>
<p>Author: Mladen Kolar, John Lafferty, Larry Wasserman</p><p>Abstract: We sharply characterize the performance of different penalization schemes for the problem of selecting the relevant variables in the multi-task setting. Previous work focuses on the regression problem where conditions on the design matrix complicate the analysis. A clearer and simpler picture emerges by studying the Normal means model. This model, often used in the ﬁeld of statistics, is a simpliﬁed model that provides a laboratory for studying complex procedures. Keywords: high-dimensional inference, multi-task learning, sparsity, normal means, minimax estimation</p><p>5 0.022746302 <a title="23-tfidf-5" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>Author: Brian McFee, Gert Lanckriet</p><p>Abstract: In many applications involving multi-media data, the deﬁnition of similarity between items is integral to several key tasks, including nearest-neighbor retrieval, classiﬁcation, and recommendation. Data in such regimes typically exhibits multiple modalities, such as acoustic and visual content of video. Integrating such heterogeneous data to form a holistic similarity space is therefore a key challenge to be overcome in many real-world applications. We present a novel multiple kernel learning technique for integrating heterogeneous data into a single, uniﬁed similarity space. Our algorithm learns an optimal ensemble of kernel transformations which conform to measurements of human perceptual similarity, as expressed by relative comparisons. To cope with the ubiquitous problems of subjectivity and inconsistency in multimedia similarity, we develop graph-based techniques to ﬁlter similarity measurements, resulting in a simpliﬁed and robust training procedure. Keywords: multiple kernel learning, metric learning, similarity</p><p>6 0.022398593 <a title="23-tfidf-6" href="./jmlr-2011-Efficient_Structure_Learning_of_Bayesian_Networks_using_Constraints.html">30 jmlr-2011-Efficient Structure Learning of Bayesian Networks using Constraints</a></p>
<p>7 0.02192015 <a title="23-tfidf-7" href="./jmlr-2011-Learning_Latent_Tree_Graphical_Models.html">54 jmlr-2011-Learning Latent Tree Graphical Models</a></p>
<p>8 0.021720996 <a title="23-tfidf-8" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<p>9 0.020391854 <a title="23-tfidf-9" href="./jmlr-2011-Kernel_Regression_in_the_Presence_of_Correlated_Errors.html">49 jmlr-2011-Kernel Regression in the Presence of Correlated Errors</a></p>
<p>10 0.019461624 <a title="23-tfidf-10" href="./jmlr-2011-Parallel_Algorithm_for_Learning_Optimal_Bayesian_Network_Structure.html">75 jmlr-2011-Parallel Algorithm for Learning Optimal Bayesian Network Structure</a></p>
<p>11 0.019395778 <a title="23-tfidf-11" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>12 0.018101506 <a title="23-tfidf-12" href="./jmlr-2011-An_Asymptotic_Behaviour_of_the_Marginal_Likelihood_for_General_Markov_Models.html">9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</a></p>
<p>13 0.017800823 <a title="23-tfidf-13" href="./jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</a></p>
<p>14 0.017694714 <a title="23-tfidf-14" href="./jmlr-2011-The_Indian_Buffet_Process%3A_An_Introduction_and_Review.html">90 jmlr-2011-The Indian Buffet Process: An Introduction and Review</a></p>
<p>15 0.017135626 <a title="23-tfidf-15" href="./jmlr-2011-Robust_Gaussian_Process_Regression_with_a_Student-tLikelihood.html">82 jmlr-2011-Robust Gaussian Process Regression with a Student-tLikelihood</a></p>
<p>16 0.017130166 <a title="23-tfidf-16" href="./jmlr-2011-Forest_Density_Estimation.html">35 jmlr-2011-Forest Density Estimation</a></p>
<p>17 0.0169867 <a title="23-tfidf-17" href="./jmlr-2011-Approximate_Marginals_in_Latent_Gaussian_Models.html">11 jmlr-2011-Approximate Marginals in Latent Gaussian Models</a></p>
<p>18 0.016530558 <a title="23-tfidf-18" href="./jmlr-2011-Dirichlet_Process_Mixtures_of_Generalized_Linear_Models.html">24 jmlr-2011-Dirichlet Process Mixtures of Generalized Linear Models</a></p>
<p>19 0.015421347 <a title="23-tfidf-19" href="./jmlr-2011-Adaptive_Exact_Inference_in_Graphical_Models.html">7 jmlr-2011-Adaptive Exact Inference in Graphical Models</a></p>
<p>20 0.015324106 <a title="23-tfidf-20" href="./jmlr-2011-Hyper-Sparse_Optimal_Aggregation.html">40 jmlr-2011-Hyper-Sparse Optimal Aggregation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.098), (1, -0.044), (2, -0.008), (3, 0.014), (4, -0.007), (5, -0.003), (6, 0.006), (7, 0.003), (8, 0.023), (9, 0.077), (10, 0.052), (11, -0.015), (12, 0.068), (13, 0.033), (14, -0.009), (15, -0.081), (16, 0.01), (17, 0.025), (18, -0.033), (19, 0.204), (20, 0.078), (21, 0.361), (22, -0.081), (23, 0.03), (24, 0.088), (25, -0.126), (26, 0.035), (27, 0.009), (28, 0.206), (29, -0.195), (30, -0.101), (31, -0.067), (32, -0.408), (33, 0.075), (34, 0.189), (35, 0.114), (36, 0.209), (37, -0.044), (38, -0.118), (39, 0.011), (40, -0.026), (41, 0.002), (42, -0.049), (43, -0.037), (44, 0.025), (45, -0.068), (46, -0.062), (47, 0.043), (48, -0.068), (49, -0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95682919 <a title="23-lsi-1" href="./jmlr-2011-DirectLiNGAM%3A_A_Direct_Method_for_Learning_a_Linear_Non-Gaussian_Structural_Equation_Model.html">23 jmlr-2011-DirectLiNGAM: A Direct Method for Learning a Linear Non-Gaussian Structural Equation Model</a></p>
<p>Author: Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvärinen, Yoshinobu Kawahara, Takashi Washio, Patrik O. Hoyer, Kenneth Bollen</p><p>Abstract: Structural equation models and Bayesian networks have been widely used to analyze causal relations between continuous variables. In such frameworks, linear acyclic models are typically used to model the data-generating process of variables. Recently, it was shown that use of non-Gaussianity identiﬁes the full structure of a linear acyclic model, that is, a causal ordering of variables and their connection strengths, without using any prior knowledge on the network structure, which is not the case with conventional methods. However, existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a ﬁnite number of steps. In this paper, we propose a new direct method to estimate a causal ordering and connection strengths based on non-Gaussianity. In contrast to the previous methods, our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small ﬁxed number of steps if the data strictly follows the model, that is, if all the model assumptions are met and the sample size is inﬁnite. Keywords: structural equation models, Bayesian networks, independent component analysis, non-Gaussianity, causal discovery c 2011 Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyv¨ rinen, Yoshinobu Kawahara, Takashi Washio, Patrik O. Hoyer a and Kenneth Bollen ¨ S HIMIZU , I NAZUMI , S OGAWA , H YV ARINEN , K AWAHARA , WASHIO , H OYER AND B OLLEN</p><p>2 0.64300036 <a title="23-lsi-2" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>Author: Ricardo Henao, Ole Winther</p><p>Abstract: In this paper we consider sparse and identiﬁable linear latent variable (factor) and linear Bayesian network models for parsimonious analysis of multivariate data. We propose a computationally efﬁcient method for joint parameter and model inference, and model comparison. It consists of a fully Bayesian hierarchy for sparse models using slab and spike priors (two-component δ-function and continuous mixtures), non-Gaussian latent factors and a stochastic search over the ordering of the variables. The framework, which we call SLIM (Sparse Linear Identiﬁable Multivariate modeling), is validated and bench-marked on artiﬁcial and real biological data sets. SLIM is closest in spirit to LiNGAM (Shimizu et al., 2006), but differs substantially in inference, Bayesian network structure learning and model comparison. Experimentally, SLIM performs equally well or better than LiNGAM with comparable computational complexity. We attribute this mainly to the stochastic search strategy used, and to parsimony (sparsity and identiﬁability), which is an explicit part of the model. We propose two extensions to the basic i.i.d. linear framework: non-linear dependence on observed variables, called SNIM (Sparse Non-linear Identiﬁable Multivariate modeling) and allowing for correlations between latent variables, called CSLIM (Correlated SLIM), for the temporal and/or spatial data. The source code and scripts are available from http://cogsys.imm.dtu.dk/slim/. Keywords: parsimony, sparsity, identiﬁability, factor models, linear Bayesian networks</p><p>3 0.18017891 <a title="23-lsi-3" href="./jmlr-2011-Kernel_Regression_in_the_Presence_of_Correlated_Errors.html">49 jmlr-2011-Kernel Regression in the Presence of Correlated Errors</a></p>
<p>Author: Kris De Brabanter, Jos De Brabanter, Johan A.K. Suykens, Bart De Moor</p><p>Abstract: It is a well-known problem that obtaining a correct bandwidth and/or smoothing parameter in nonparametric regression is difﬁcult in the presence of correlated errors. There exist a wide variety of methods coping with this problem, but they all critically depend on a tuning procedure which requires accurate information about the correlation structure. We propose a bandwidth selection procedure based on bimodal kernels which successfully removes the correlation without requiring any prior knowledge about its structure and its parameters. Further, we show that the form of the kernel is very important when errors are correlated which is in contrast to the independent and identically distributed (i.i.d.) case. Finally, some extensions are proposed to use the proposed criterion in support vector machines and least squares support vector machines for regression. Keywords: nonparametric regression, correlated errors, bandwidth choice, cross-validation, shortrange dependence, bimodal kernel</p><p>4 0.12820597 <a title="23-lsi-4" href="./jmlr-2011-Neyman-Pearson_Classification%2C_Convexity_and_Stochastic_Constraints.html">69 jmlr-2011-Neyman-Pearson Classification, Convexity and Stochastic Constraints</a></p>
<p>Author: Philippe Rigollet, Xin Tong</p><p>Abstract: Motivated by problems of anomaly detection, this paper implements the Neyman-Pearson paradigm to deal with asymmetric errors in binary classiﬁcation with a convex loss ϕ. Given a ﬁnite collection of classiﬁers, we combine them and obtain a new classiﬁer that satisﬁes simultaneously the two following properties with high probability: (i) its ϕ-type I error is below a pre-speciﬁed level and (ii), it has ϕ-type II error close to the minimum possible. The proposed classiﬁer is obtained by minimizing an empirical convex objective with an empirical convex constraint. The novelty of the method is that the classiﬁer output by this computationally feasible program is shown to satisfy the original constraint on type I error. New techniques to handle such problems are developed and they have consequences on chance constrained programming. We also evaluate the price to pay in terms of type II error for being conservative on type I error. Keywords: binary classiﬁcation, Neyman-Pearson paradigm, anomaly detection, empirical constraint, empirical risk minimization, chance constrained optimization</p><p>5 0.12442352 <a title="23-lsi-5" href="./jmlr-2011-An_Asymptotic_Behaviour_of_the_Marginal_Likelihood_for_General_Markov_Models.html">9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</a></p>
<p>Author: Piotr Zwiernik</p><p>Abstract: The standard Bayesian Information Criterion (BIC) is derived under regularity conditions which are not always satisÄ?Ĺš ed in the case of graphical models with hidden variables. In this paper we derive the BIC for the binary graphical tree models where all the inner nodes of a tree represent binary hidden variables. This provides an extension of a similar formula given by Rusakov and Geiger for naive Bayes models. The main tool used in this paper is the connection between the growth behavior of marginal likelihood integrals and the real log-canonical threshold. Keywords: BIC, marginal likelihood, singular models, tree models, Bayesian networks, real logcanonical threshold</p><p>6 0.12191229 <a title="23-lsi-6" href="./jmlr-2011-Anechoic_Blind_Source_Separation_Using_Wigner_Marginals.html">10 jmlr-2011-Anechoic Blind Source Separation Using Wigner Marginals</a></p>
<p>7 0.11891539 <a title="23-lsi-7" href="./jmlr-2011-The_Indian_Buffet_Process%3A_An_Introduction_and_Review.html">90 jmlr-2011-The Indian Buffet Process: An Introduction and Review</a></p>
<p>8 0.11732315 <a title="23-lsi-8" href="./jmlr-2011-Learning_with_Structured_Sparsity.html">59 jmlr-2011-Learning with Structured Sparsity</a></p>
<p>9 0.11498466 <a title="23-lsi-9" href="./jmlr-2011-Faster_Algorithms_for_Max-Product_Message-Passing.html">34 jmlr-2011-Faster Algorithms for Max-Product Message-Passing</a></p>
<p>10 0.11437555 <a title="23-lsi-10" href="./jmlr-2011-Discriminative_Learning_of_Bayesian_Networks_via_Factorized_Conditional_Log-Likelihood.html">25 jmlr-2011-Discriminative Learning of Bayesian Networks via Factorized Conditional Log-Likelihood</a></p>
<p>11 0.11160477 <a title="23-lsi-11" href="./jmlr-2011-Non-Parametric_Estimation_of_Topic_Hierarchies_from_Texts_with_Hierarchical_Dirichlet_Processes.html">70 jmlr-2011-Non-Parametric Estimation of Topic Hierarchies from Texts with Hierarchical Dirichlet Processes</a></p>
<p>12 0.11111359 <a title="23-lsi-12" href="./jmlr-2011-Union_Support_Recovery_in_Multi-task_Learning.html">97 jmlr-2011-Union Support Recovery in Multi-task Learning</a></p>
<p>13 0.10844003 <a title="23-lsi-13" href="./jmlr-2011-Unsupervised_Supervised_Learning_II%3A_Margin-Based_Classification_Without_Labels.html">100 jmlr-2011-Unsupervised Supervised Learning II: Margin-Based Classification Without Labels</a></p>
<p>14 0.10743253 <a title="23-lsi-14" href="./jmlr-2011-Learning_a_Robust_Relevance_Model_for_Search_Using_Kernel_Methods.html">57 jmlr-2011-Learning a Robust Relevance Model for Search Using Kernel Methods</a></p>
<p>15 0.1073188 <a title="23-lsi-15" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<p>16 0.10425331 <a title="23-lsi-16" href="./jmlr-2011-The_Stationary_Subspace_Analysis_Toolbox.html">92 jmlr-2011-The Stationary Subspace Analysis Toolbox</a></p>
<p>17 0.10256892 <a title="23-lsi-17" href="./jmlr-2011-Approximate_Marginals_in_Latent_Gaussian_Models.html">11 jmlr-2011-Approximate Marginals in Latent Gaussian Models</a></p>
<p>18 0.097334437 <a title="23-lsi-18" href="./jmlr-2011-High-dimensional_Covariance_Estimation_Based_On_Gaussian_Graphical_Models.html">39 jmlr-2011-High-dimensional Covariance Estimation Based On Gaussian Graphical Models</a></p>
<p>19 0.097028591 <a title="23-lsi-19" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>20 0.096805654 <a title="23-lsi-20" href="./jmlr-2011-Information_Rates_of_Nonparametric_Gaussian_Process_Methods.html">44 jmlr-2011-Information Rates of Nonparametric Gaussian Process Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.025), (9, 0.012), (10, 0.029), (24, 0.028), (31, 0.051), (41, 0.639), (73, 0.028), (78, 0.047), (90, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90632904 <a title="23-lda-1" href="./jmlr-2011-Internal_Regret_with_Partial_Monitoring%3A_Calibration-Based_Optimal_Algorithms.html">45 jmlr-2011-Internal Regret with Partial Monitoring: Calibration-Based Optimal Algorithms</a></p>
<p>Author: Vianney Perchet</p><p>Abstract: We provide consistent random algorithms for sequential decision under partial monitoring, when the decision maker does not observe the outcomes but receives instead random feedback signals. Those algorithms have no internal regret in the sense that, on the set of stages where the decision maker chose his action according to a given law, the average payoff could not have been improved in average by using any other ﬁxed law. They are based on a generalization of calibration, no longer deﬁned in terms of a Vorono¨ ı diagram but instead of a Laguerre diagram (a more general concept). This allows us to bound, for the ﬁrst time in this general framework, the expected average internal, as well as the usual external, regret at stage n by O(n−1/3 ), which is known to be optimal. Keywords: repeated games, on-line learning, regret, partial monitoring, calibration, Vorono¨ and ı Laguerre diagrams</p><p>same-paper 2 0.90592617 <a title="23-lda-2" href="./jmlr-2011-DirectLiNGAM%3A_A_Direct_Method_for_Learning_a_Linear_Non-Gaussian_Structural_Equation_Model.html">23 jmlr-2011-DirectLiNGAM: A Direct Method for Learning a Linear Non-Gaussian Structural Equation Model</a></p>
<p>Author: Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvärinen, Yoshinobu Kawahara, Takashi Washio, Patrik O. Hoyer, Kenneth Bollen</p><p>Abstract: Structural equation models and Bayesian networks have been widely used to analyze causal relations between continuous variables. In such frameworks, linear acyclic models are typically used to model the data-generating process of variables. Recently, it was shown that use of non-Gaussianity identiﬁes the full structure of a linear acyclic model, that is, a causal ordering of variables and their connection strengths, without using any prior knowledge on the network structure, which is not the case with conventional methods. However, existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a ﬁnite number of steps. In this paper, we propose a new direct method to estimate a causal ordering and connection strengths based on non-Gaussianity. In contrast to the previous methods, our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small ﬁxed number of steps if the data strictly follows the model, that is, if all the model assumptions are met and the sample size is inﬁnite. Keywords: structural equation models, Bayesian networks, independent component analysis, non-Gaussianity, causal discovery c 2011 Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyv¨ rinen, Yoshinobu Kawahara, Takashi Washio, Patrik O. Hoyer a and Kenneth Bollen ¨ S HIMIZU , I NAZUMI , S OGAWA , H YV ARINEN , K AWAHARA , WASHIO , H OYER AND B OLLEN</p><p>3 0.7862857 <a title="23-lda-3" href="./jmlr-2011-Learning_Transformation_Models_for_Ranking_and_Survival_Analysis.html">56 jmlr-2011-Learning Transformation Models for Ranking and Survival Analysis</a></p>
<p>Author: Vanya Van Belle, Kristiaan Pelckmans, Johan A. K. Suykens, Sabine Van Huffel</p><p>Abstract: This paper studies the task of learning transformation models for ranking problems, ordinal regression and survival analysis. The present contribution describes a machine learning approach termed MINLIP . The key insight is to relate ranking criteria as the Area Under the Curve to monotone transformation functions. Consequently, the notion of a Lipschitz smoothness constant is found to be useful for complexity control for learning transformation models, much in a similar vein as the ’margin’ is for Support Vector Machines for classiﬁcation. The use of this model structure in the context of high dimensional data, as well as for estimating non-linear, and additive models based on primal-dual kernel machines, and for sparse models is indicated. Given n observations, the present method solves a quadratic program existing of O (n) constraints and O (n) unknowns, where most existing risk minimization approaches to ranking problems typically result in algorithms with O (n2 ) constraints or unknowns. We specify the MINLIP method for three different cases: the ﬁrst one concerns the preference learning problem. Secondly it is speciﬁed how to adapt the method to ordinal regression with a ﬁnite set of ordered outcomes. Finally, it is shown how the method can be used in the context of survival analysis where one models failure times, typically subject to censoring. The current approach is found to be particularly useful in this context as it can handle, in contrast with the standard statistical model for analyzing survival data, all types of censoring in a straightforward way, and because of the explicit relation with the Proportional Hazard and Accelerated Failure Time models. The advantage of the current method is illustrated on different benchmark data sets, as well as for estimating a model for cancer survival based on different micro-array and clinical data sets. Keywords: support vector machines, preference learning, ranking models, ordinal regression, survival analysis c</p><p>4 0.31554064 <a title="23-lda-4" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>Author: Ricardo Henao, Ole Winther</p><p>Abstract: In this paper we consider sparse and identiﬁable linear latent variable (factor) and linear Bayesian network models for parsimonious analysis of multivariate data. We propose a computationally efﬁcient method for joint parameter and model inference, and model comparison. It consists of a fully Bayesian hierarchy for sparse models using slab and spike priors (two-component δ-function and continuous mixtures), non-Gaussian latent factors and a stochastic search over the ordering of the variables. The framework, which we call SLIM (Sparse Linear Identiﬁable Multivariate modeling), is validated and bench-marked on artiﬁcial and real biological data sets. SLIM is closest in spirit to LiNGAM (Shimizu et al., 2006), but differs substantially in inference, Bayesian network structure learning and model comparison. Experimentally, SLIM performs equally well or better than LiNGAM with comparable computational complexity. We attribute this mainly to the stochastic search strategy used, and to parsimony (sparsity and identiﬁability), which is an explicit part of the model. We propose two extensions to the basic i.i.d. linear framework: non-linear dependence on observed variables, called SNIM (Sparse Non-linear Identiﬁable Multivariate modeling) and allowing for correlations between latent variables, called CSLIM (Correlated SLIM), for the temporal and/or spatial data. The source code and scripts are available from http://cogsys.imm.dtu.dk/slim/. Keywords: parsimony, sparsity, identiﬁability, factor models, linear Bayesian networks</p><p>5 0.29529661 <a title="23-lda-5" href="./jmlr-2011-Parameter_Screening_and_Optimisation_for_ILP_using_Designed_Experiments.html">76 jmlr-2011-Parameter Screening and Optimisation for ILP using Designed Experiments</a></p>
<p>Author: Ashwin Srinivasan, Ganesh Ramakrishnan</p><p>Abstract: Reports of experiments conducted with an Inductive Logic Programming system rarely describe how speciﬁc values of parameters of the system are arrived at when constructing models. Usually, no attempt is made to identify sensitive parameters, and those that are used are often given “factory-supplied” default values, or values obtained from some non-systematic exploratory analysis. The immediate consequence of this is, of course, that it is not clear if better models could have been obtained if some form of parameter selection and optimisation had been performed. Questions follow inevitably on the experiments themselves: speciﬁcally, are all algorithms being treated fairly, and is the exploratory phase sufﬁciently well-deﬁned to allow the experiments to be replicated? In this paper, we investigate the use of parameter selection and optimisation techniques grouped under the study of experimental design. Screening and response surface methods determine, in turn, sensitive parameters and good values for these parameters. Screening is done here by constructing a stepwise regression model relating the utility of an ILP system’s hypothesis to its input parameters, using systematic combinations of values of input parameters (technically speaking, we use a two-level fractional factorial design of the input parameters). The parameters used by the regression model are taken to be the sensitive parameters for the system for that application. We then seek an assignment of values to these sensitive parameters that maximise the utility of the ILP model. This is done using the technique of constructing a local “response surface”. The parameters are then changed following the path of steepest ascent until a locally optimal value is reached. This combined use of parameter selection and response surface-driven optimisation has a long history of application in industrial engineering, and its role in ILP is demonstrated using well-known benchmarks. The results suggest that computational</p><p>6 0.28885147 <a title="23-lda-6" href="./jmlr-2011-Operator_Norm_Convergence_of_Spectral_Clustering_on_Level_Sets.html">74 jmlr-2011-Operator Norm Convergence of Spectral Clustering on Level Sets</a></p>
<p>7 0.27917016 <a title="23-lda-7" href="./jmlr-2011-On_the_Relation_between_Realizable_and_Nonrealizable_Cases_of_the_Sequence_Prediction_Problem.html">72 jmlr-2011-On the Relation between Realizable and Nonrealizable Cases of the Sequence Prediction Problem</a></p>
<p>8 0.276353 <a title="23-lda-8" href="./jmlr-2011-Cumulative_Distribution_Networks_and_the_Derivative-sum-product_Algorithm%3A_Models_and_Inference_for_Cumulative_Distribution_Functions_on_Graphs.html">21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</a></p>
<p>9 0.27125564 <a title="23-lda-9" href="./jmlr-2011-Robust_Approximate_Bilinear_Programming_for_Value_Function_Approximation.html">81 jmlr-2011-Robust Approximate Bilinear Programming for Value Function Approximation</a></p>
<p>10 0.26230091 <a title="23-lda-10" href="./jmlr-2011-Online_Learning_in_Case_of_Unbounded_Losses_Using_Follow_the_Perturbed_Leader_Algorithm.html">73 jmlr-2011-Online Learning in Case of Unbounded Losses Using Follow the Perturbed Leader Algorithm</a></p>
<p>11 0.25897345 <a title="23-lda-11" href="./jmlr-2011-Hierarchical_Knowledge_Gradient_for_Sequential_Sampling.html">38 jmlr-2011-Hierarchical Knowledge Gradient for Sequential Sampling</a></p>
<p>12 0.25868595 <a title="23-lda-12" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>13 0.25606012 <a title="23-lda-13" href="./jmlr-2011-X-Armed_Bandits.html">104 jmlr-2011-X-Armed Bandits</a></p>
<p>14 0.25590909 <a title="23-lda-14" href="./jmlr-2011-Efficient_Learning_with_Partially_Observed_Attributes.html">29 jmlr-2011-Efficient Learning with Partially Observed Attributes</a></p>
<p>15 0.2550399 <a title="23-lda-15" href="./jmlr-2011-An_Asymptotic_Behaviour_of_the_Marginal_Likelihood_for_General_Markov_Models.html">9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</a></p>
<p>16 0.24656121 <a title="23-lda-16" href="./jmlr-2011-Universality%2C_Characteristic_Kernels_and_RKHS_Embedding_of_Measures.html">98 jmlr-2011-Universality, Characteristic Kernels and RKHS Embedding of Measures</a></p>
<p>17 0.24630475 <a title="23-lda-17" href="./jmlr-2011-Hyper-Sparse_Optimal_Aggregation.html">40 jmlr-2011-Hyper-Sparse Optimal Aggregation</a></p>
<p>18 0.24621291 <a title="23-lda-18" href="./jmlr-2011-A_Bayesian_Approach_for_Learning_and_Planning_in_Partially_Observable_Markov_Decision_Processes.html">1 jmlr-2011-A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes</a></p>
<p>19 0.24578284 <a title="23-lda-19" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>20 0.2452454 <a title="23-lda-20" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
