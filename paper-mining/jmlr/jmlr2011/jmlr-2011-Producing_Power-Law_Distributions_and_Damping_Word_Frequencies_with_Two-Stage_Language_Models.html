<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>78 jmlr-2011-Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-78" href="#">jmlr2011-78</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>78 jmlr-2011-Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models</h1>
<br/><p>Source: <a title="jmlr-2011-78-pdf" href="http://jmlr.org/papers/volume12/goldwater11a/goldwater11a.pdf">pdf</a></p><p>Author: Sharon Goldwater, Thomas L. Griffiths, Mark Johnson</p><p>Abstract: Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens. We present a framework for developing statistical models that can generically produce power laws, breaking generative models into two stages. The ﬁrst stage, the generator, can be any standard probabilistic model, while the second stage, the adaptor, transforms the word frequencies of this model to provide a closer match to natural language. We show that two commonly used Bayesian models, the Dirichlet-multinomial model and the Dirichlet process, can be viewed as special cases of our framework. We discuss two stochastic processes—the Chinese restaurant process and its two-parameter generalization based on the Pitman-Yor process—that can be used as adaptors in our framework to produce power-law distributions over word frequencies. We show that these adaptors justify common estimation procedures based on logarithmic or inverse-power transformations of empirical frequencies. In addition, taking the Pitman-Yor Chinese restaurant process as an adaptor justiﬁes the appearance of type frequencies in formal analyses of natural language and improves the performance of a model for unsupervised learning of morphology. Keywords: nonparametric Bayes, Pitman-Yor process, language model, unsupervised</p><p>Reference: <a title="jmlr-2011-78-reference" href="../jmlr2011_reference/jmlr-2011-Producing_Power-Law_Distributions_and_Damping_Word_Frequencies_with_Two-Stage_Language_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 AU  Department of Computing Macquarie University Sydney, NSW 2109 Australia  Editor: Fernando Pereira  Abstract Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens. [sent-9, score-0.584]
</p><p>2 In addition, taking the Pitman-Yor Chinese restaurant process as an adaptor justiﬁes the appearance of type frequencies in formal analyses of natural language and improves the performance of a model for unsupervised learning of morphology. [sent-15, score-0.803]
</p><p>3 That is, the probability that a word w will occur with frequency nw in a sufﬁciently large corpus is proportional to n−g . [sent-21, score-0.658]
</p><p>4 In particular, postulating a separate mechanism within the model that accounts for the skewed distribution of word frequencies takes the burden of explaining this distribution off the other components of the model, effectively reducing the frequencies of those words. [sent-26, score-0.645]
</p><p>5 An extreme version of damping frequencies forms part of a tension exhibited by formal approaches to natural language: whether explanations should be based upon the distinct types of words that languages exhibit, or the frequencies with which tokens (instances) of those words occur. [sent-29, score-0.78]
</p><p>6 For example, one of the most successful forms of smoothing used in statistical language models, Kneser-Ney smoothing, explicitly interpolates between type and token frequencies (Ney et al. [sent-37, score-0.498]
</p><p>7 Adopting this two-stage framework divides responsibility for the appearance of the tokens in the corpus between the generator and the adaptor, with only a subset of the tokens being produced by the generator. [sent-46, score-0.899]
</p><p>8 The parameters of the generator will be estimated based only on the tokens for which the generator is considered responsible, rather than on the full set of tokens in the corpus. [sent-47, score-0.992]
</p><p>9 By explaining away the presence of some of the tokens, the adaptor effectively damps the word counts used to estimate the parameters of the generator. [sent-48, score-0.598]
</p><p>10 The morphological segmentation task is a good example of a situation where appropriately modeling word frequencies can signiﬁcantly affect the outcome of unsupervised learning. [sent-69, score-0.669]
</p><p>11 An adaptor grammar can be seen as a two-stage model in which the generator is a PCFG. [sent-112, score-0.64]
</p><p>12 Here, Pϕ is a discrete distribution and the lexical items are generated independently, so the same word type may occur more than once in ℓ . [sent-165, score-0.554]
</p><p>13 1 In the remainder of the paper, we use lexical item to refer to the items produced by the generator, word type to refer to unique wordforms, and word or token to refer to word tokens. [sent-166, score-1.287]
</p><p>14 , zn ) with 1 ≤ zi ≤ K, where zi = k indicates that wi = ℓk (that is, zi is the index of the lexical item corresponding to wi ). [sent-171, score-0.51]
</p><p>15 We use the notation TwoStage(Pγ , Pϕ ) to refer to a two-stage model with adaptor Pγ and generator Pϕ . [sent-184, score-0.614]
</p><p>16 Thus, the adaptor “adapts” the word frequencies produced by the generator to ﬁt a power-law distribution. [sent-189, score-1.033]
</p><p>17 Here, we show that morphological structure can be learned using a generator that produces words by choosing a stem and sufﬁx and concatenating them together. [sent-191, score-0.575]
</p><p>18 Variables associated with the generator are on the right; those associated with the adaptor are on the left. [sent-200, score-0.584]
</p><p>19 Chinese Restaurant Processes as Adaptors While any stochastic process that results in a power-law distribution over word frequencies can be used as an adaptor, the choice of adaptor will have signiﬁcant implications for the resulting model. [sent-203, score-0.732]
</p><p>20 (z )  where z−i is the seating arrangement of the previous i − 1 customers, nk −i is the number of customers already assigned to table k by z−i , K(z−i ) is the total number of occupied tables in z−i , and α ≥ 0 is a parameter of the process determining how “spread out” the customers become. [sent-220, score-0.503]
</p><p>21 Each customer represents a word token, so that the number of customers at a table corresponds to the frequency of the lexical item labeling that table. [sent-249, score-0.679]
</p><p>22 ) is an indicator function taking on the value 1 (w ) when its argument is true and 0 otherwise, and nw −i is the number of previous occurrences of the word type w in w−i (that is, the number of customers that z−i assigns to tables labeled with w). [sent-252, score-0.671]
</p><p>23 We use Pϕ (ℓ ) rather than the equivalent P(ℓ | ϕ) for consistency with our notation for the generator probability of an ℓ ℓ individual lexical item Pϕ (ℓ); both Pϕ (ℓ ) and P(ℓ | ϕ) represent the probability of producing lexical items ℓ using the generator parameterized by ϕ. [sent-259, score-1.028]
</p><p>24 Under this view, each word is generated in n one of two ways: from a cache of previously occurring lexical items (with probability n+α if we use α the CRP adaptor) or as a novel lexical item (with probability n+α ). [sent-274, score-0.709]
</p><p>25 Novel items are chosen according to the probability distribution of the lexicon generator (which means that, strictly speaking, they are not always “novel”—that is, novel word types—since the generator may produce duplicates). [sent-276, score-0.98]
</p><p>26 Prior expectations regarding the probability of encountering a novel lexical item are reﬂected in the value of α, so lower values of α will lead to an expectation of fewer lexical items (and word types) during inference. [sent-278, score-0.709]
</p><p>27 Prior expectations about the relative probabilities of different novel items are reﬂected in Pϕ , so the choice of generator determines the kinds of lexical items that are likely to be inferred from the data. [sent-279, score-0.604]
</p><p>28 If the generator is a distribution over an inﬁnite number of items, the cache model makes it clear that the number of different word types that will be observed in a ﬁnite corpus is not ﬁxed in advance. [sent-280, score-0.807]
</p><p>29 In general, the number of different word types observed in a corpus will slowly grow as the size of the corpus grows. [sent-282, score-0.618]
</p><p>30 A power-law distribution in word frequency, with the probability of a frequency of nw proportional to n−g , results in a straight line on the plot with slope 1/(g − 1). [sent-294, score-0.528]
</p><p>31 Here, w the left-hand plot shows the distribution of word frequencies in sections 0-20 from the Penn Wall Street Journal treebank, while the right-hand plot shows the distribution of the number of customers at each table produced by 500,000 draws from the PYCRP with parameters a = 0. [sent-295, score-0.616]
</p><p>32 As with the CRP, we can deﬁne a generic two-stage model with a PYCRP adaptor by assuming a generator Pϕ parameterized by ϕ. [sent-305, score-0.614]
</p><p>33 Comparing Equation 7 to Equation 2 reveals that the Dirichletmultinomial model is a special case of our two-stage framework, with a CRP adaptor and a ﬁnite generator distribution. [sent-334, score-0.614]
</p><p>34 In doing this, we return to our second motivating concern: the issue of how we might explain the damping of word frequencies, with the extreme case being reconciliation of models based on unique word types with those based on the observed frequencies of word tokens. [sent-386, score-1.078]
</p><p>35 We then explain how, in a TwoStage(PYCRP(a, b), Pϕ ) language model, the parameters of the PYCRP determine whether the parameters of the generator will be inferred based on word types, tokens, or some interpolation between the two. [sent-388, score-0.668]
</p><p>36 1 Impact of the Adaptor on Frequencies used for Estimation By introducing an adaptor into our model, we provide a route by which word tokens can appear in a corpus without having been directly produced by the generator. [sent-392, score-0.957]
</p><p>37 As a consequence, any estimate of the parameters of the generator will be based only on those tokens for which the generator is considered responsible, which will be a subset of the tokens in the corpus. [sent-393, score-0.992]
</p><p>38 The adaptor will thus have the effect of damping the frequencies from which the parameters of the generator are estimated, with the nature of this damping depending on the properties of the adaptor. [sent-394, score-0.891]
</p><p>39 In particular, we will show that using the CRP or PYCRP as adaptors is approximately equivalent to estimating the generator parameters from log transformed or inverse-power transformed token counts, respectively. [sent-395, score-0.538]
</p><p>40 We can see how the choice of adaptor affects the frequencies used for estimating the parameters ϕ of the generator by considering how to estimate ϕ from the observed corpus w. [sent-396, score-0.892]
</p><p>41 First, notice that the total frequency nw of each word type w, as obtained by summing the counts on all tables labeled with that type, will equal the frequency of w in the corpus w. [sent-407, score-0.926]
</p><p>42 Thus, we can gain insight into how estimates of ϕ are likely to be affected by the choice of adaptor by considering how the adaptor affects the relationship between the frequency of a word type and the number of tables labeled with that type. [sent-409, score-1.073]
</p><p>43 Note that P(ℓ | z, w) is equal to one if z and w are consistent with ℓ , ℓ and zero otherwise, so we can compute P(z,ℓ | w) by computing P(z | w) subject to this consistency constraint, that is, such that for each word type w, the appropriate nw tokens of w are of type w. [sent-412, score-0.693]
</p><p>44 In order to simplify the mathematics, in the rest of this section we assume that each lexical item ℓ j produced by the generator is independent and identically distributed (i. [sent-413, score-0.536]
</p><p>45 The posterior distribution is exchangeable, so we can calculate the distribution over the number of lexical entries for a given word type w by imagining that the nw instances of w are the ﬁrst nw tokens in our corpus. [sent-423, score-1.065]
</p><p>46 Dividing through yields  (z ) n −i   (w−i ) k · I(ℓk = w) 1 ≤ k ≤ K(z−i ) +αPϕ (w) nw ℓ P(zi = k | wi = w, z−i ,ℓ (z−i ), ϕ) =   (w ) α · Pϕ (w) k = K(z−i ) + 1 −i nw  (9)  +αPϕ (w)  which we can now use to calculate the expected number of occupied tables (i. [sent-425, score-0.584]
</p><p>47 , lexical entries) for a word type that occurs nw times. [sent-427, score-0.629]
</p><p>48 Inspection of Equation 9 reveals that the posterior distribution on seating assignments for all tokens of type w is given by the CRP with parameter αPϕ (w) and a total of nw customers. [sent-432, score-0.498]
</p><p>49 While the CRP treats each word type independently (that is, ignoring dependencies in the generator, in a CRP the number of tables associated with a word type is independent of the number of tables associated with other word types), this is not true for the PYCRP. [sent-435, score-1.091]
</p><p>50 Ignoring the effect of the number of tables associated with other word types, we expect the number of tables to be less than the number produced by simply running a PYCRP(a, bPϕ (w)) over the nw tokens of w. [sent-439, score-0.942]
</p><p>51 The expectation of the number of tables occupied after seating nw customers increases as O(na ) for the w PYCRP (Teh, 2006a), providing an upper bound on the number of tables we should expect a word with frequency nw to produce when the PYCRP is used as an adaptor. [sent-440, score-1.126]
</p><p>52 With β → ∞, the prior forces the estimated parameters to be the uniform distribution over all word types, so the number of tables assigned to any given word type has no effect on the estimates. [sent-453, score-0.702]
</p><p>53 For each generator, ﬁve models used a CRP adaptor with α = {1, 10, 100, 1000, 10000} and ten others used a PYCRP adaptor with a = {0. [sent-462, score-0.617]
</p><p>54 For each combination of generator and adaptor, a Markov chain Monte Carlo (MCMC) algorithm was used to calculate the expected number of occupied tables (from which the corresponding multinomial parameters were estimated) for each word in the corpus. [sent-469, score-0.777]
</p><p>55 To produce the ﬁgure, words were binned by frequency using bins that were uniform on a log scale, and the posterior mean of the number of occupied tables per word was averaged within bins. [sent-472, score-0.56]
</p><p>56 w The inﬂuence of the prior on the number of tables per word under the two-stage model with PYCRP adaptor can be understood in terms of how the prior affects the difference between the posterior distribution on the number of tables and the simpler PYCRP we use to approximate it. [sent-479, score-0.873]
</p><p>57 This behavior is due to a conﬂuence of factors, which include both the high value of a and the very large number of tables required to account for all the words in the corpus (a result of the large number of word types). [sent-484, score-0.605]
</p><p>58 Under these circumstances, when the total number of tokens of w is small, it is not possible to have a table with enough tokens of w so that the probability of placing another token of w on that table is much higher than placing the token on a new table. [sent-485, score-0.808]
</p><p>59 2 Types and Tokens The most extreme kind of frequency damping is throwing away all but a single instance of each word type, and only keeping track of the unique word types that appear in the corpus. [sent-516, score-0.708]
</p><p>60 Just as we can explain other forms of frequency damping in terms of our two-stage framework, we can show that the TwoStage(PYCRP(a, b), Pϕ ) model provides a justiﬁcation for the role of word types in formal analyses of natural language. [sent-517, score-0.517]
</p><p>61 The Kneser-Ney smoother estimates the probability that a word token will belong to a particular type by combining type and token frequencies, and has proven particularly effective for n-gram models (Ney et al. [sent-541, score-0.666]
</p><p>62 However, to complete the correspondence with IKN n-gram smoothing, we can assume that the generator for the model that computes the distribution over word types conditioned on a history of size n is another two-stage PYCRP model that computes probabilities conditioned on histories of size n − 1. [sent-569, score-0.68]
</p><p>63 Intuitively, we can imagine a separate restaurant for each history of size n, where the counts in that restaurant correspond to the distribution of word tokens given that history. [sent-572, score-0.747]
</p><p>64 Our two-stage language modeling framework allows us to create exactly these sorts of models, with the generator producing individual lexical items, and the adaptor producing the power-law distribution over words. [sent-586, score-0.9]
</p><p>65 In this section, we show that adding a PYCRP adaptor to a simple generative model for morphology can vastly improve unsupervised learning of the morphological structure of English, and we explore the effects of varying the PYCRP parameters in this task. [sent-587, score-0.71]
</p><p>66 Interestingly, previous work on unsupervised learning of morphology often ignores token frequencies, instead using word types as input (Goldsmith, 2001, 2006; Snover and Brent, 2003; Monson et al. [sent-594, score-0.684]
</p><p>67 9 This fact suggests that the additional information provided by token frequencies may actually be harmful for learning morphology using standard models. [sent-596, score-0.493]
</p><p>68 Previous morphology learning models have sidestepped the problems presented by token frequencies by simply ignoring them and using only a list of unique word types as input instead. [sent-600, score-0.824]
</p><p>69 , taking logs of token frequencies rather than removing frequencies entirely), our model is more 9. [sent-607, score-0.496]
</p><p>70 edu/faculty/goldsmith produced the same results when run on a full corpus as when run on a list of the unique word types in the corpus. [sent-611, score-0.5]
</p><p>71 So far, we have been assuming that the generator in a two-stage model is a distribution over lexical items that are strings. [sent-629, score-0.588]
</p><p>72 However, in this morphology model, the generator produces analyses of strings (class, stem, sufﬁx), rather than the strings themselves. [sent-630, score-0.571]
</p><p>73 Our generator model for morphology is inspired by the model described by Goldsmith (2001), and is intended to encode two basic linguistic intuitions. [sent-636, score-0.593]
</p><p>74 , a word stem can be further split into a smaller stem plus sufﬁx), which makes it better able to deal with complex morphology than the model presented here. [sent-644, score-0.585]
</p><p>75 Rather than sampling all the variables in our two-stage model simultaneously, our Gibbs sampler alternates between sampling the variables in the generator and those in the adaptor (here, a PYCRP). [sent-656, score-0.614]
</p><p>76 Fix the morphological analyses A(ℓ ) of the labels, and sample a new table assignment zi for each word token wi . [sent-660, score-0.783]
</p><p>77 Experiments In this section, we use the simple morphology model deﬁned above as an example to demonstrate that applying an appropriate adaptor can signiﬁcantly improve the learning of linguistic structure. [sent-700, score-0.569]
</p><p>78 Second, since different tokens of the same type may be assigned different analyses, the proportion of word tokens with each sufﬁx is also displayed. [sent-737, score-0.699]
</p><p>79 This is particularly true of frequent words, which is why token accuracy is so similar for the baseline and the generator model. [sent-751, score-0.49]
</p><p>80 Found Types  Found Tokens  Figure 8: Confusion matrices for the morphological generator model alone (equivalent to the twostage morphology model with a = 1) on the verb data set. [sent-770, score-0.831]
</p><p>81 The area of a square at location (i, j) is proportional to the number of word types (left) or tokens (right) with true sufﬁx i and found sufﬁx j. [sent-771, score-0.511]
</p><p>82 Instead, we address the problem by applying our two-stage framework, adding a PYCRP adaptor to our generator model. [sent-785, score-0.584]
</p><p>83 The proportion of word types (top) and tokens (bottom) found with each sufﬁx is shown, along with the distribution of sufﬁxes in the gold standard. [sent-854, score-0.571]
</p><p>84 There are a total of 369,443 word tokens in the corpus belonging to 6,807 types. [sent-884, score-0.623]
</p><p>85 Columns show the total number of sufﬁx types found, percentage of word types with empty sufﬁxes, and percentage of word types with the sufﬁx -z. [sent-978, score-0.653]
</p><p>86 3 Discussion Our two experiments demonstrate how the PYCRP adaptor can be used within our two-stage framework to interpolate between type and token frequencies in a model for learning non-trivial linguistic structure. [sent-993, score-0.732]
</p><p>87 Interestingly, our experiments also suggest that partially damping corpus frequencies may be as effective, or perhaps even more effective, than fully damping frequencies (i. [sent-1022, score-0.615]
</p><p>88 We have proposed that there are two main reasons that using the PYCRP adaptor to damp corpus frequencies yields better morphological segmentations than learning directly from corpus frequencies. [sent-1030, score-0.929]
</p><p>89 First, the generator model assumes that stems and sufﬁxes are independent given the morphological class, but this assumption is only approximately correct. [sent-1031, score-0.573]
</p><p>90 Damping corpus frequencies brings the assumptions of the model and the data more in line, whereas using full corpus frequencies provides more evidence that stems and sufﬁxes are not truly independent and therefore should not be split. [sent-1032, score-0.731]
</p><p>91 Second, the most frequent words in any language tend to be irregular, and due to the powerlaw distribution of word frequencies, these words strongly dominate the corpus statistics. [sent-1033, score-0.716]
</p><p>92 Further Applications and Extensions The morphological segmentation model considered in the preceding sections illustrates how different assumptions about word frequency can result in different conclusions about the latent structure expressed in linguistic data. [sent-1041, score-0.643]
</p><p>93 This model can be used as the generator for a two-stage model, with an adaptor such as the PYCRP being used to guarantee that the resulting word frequency distribution has statistical properties closer to natural language. [sent-1051, score-0.967]
</p><p>94 Similarly, nearly all probabilistic models used for language learning (most notably, hidden Markov models and PCFGs) encode strong independence assumptions similar to those in our morphology generator model. [sent-1064, score-0.641]
</p><p>95 , 2007) combine a PYCRP adaptor with a PCFG generator to create a model for learning linguistic tree structures without the strong independence assumptions made by a standard PCFG. [sent-1068, score-0.68]
</p><p>96 In an adaptor grammar, all trees produced by the generator are complete, with terminal symbols at all leaf nodes. [sent-1079, score-0.623]
</p><p>97 Details of Table Count Approximation Experiments The generator used in this model was assumed to be a multinomial distribution over 30, 114 word types, with ϕ being the probabilities assigned to these types. [sent-1134, score-0.658]
</p><p>98 Taking a symmetric Dirichlet(β) prior over ϕ, the posterior distribution over ϕ given w and a particular value of z and ℓ is Dirichlet with hyperparameters mw + β, where mw is the number of lexical items corresponding to the word type w (ie. [sent-1136, score-0.554]
</p><p>99 Since the lexical items had no internal analyses, it was only necessary to sample the table assignment zi for each word token in the corpus in each sweep of sampling. [sent-1141, score-0.917]
</p><p>100 Improving nonparametric Bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars. [sent-1338, score-0.672]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pycrp', 0.329), ('adaptor', 0.295), ('generator', 0.289), ('word', 0.259), ('tokens', 0.207), ('morphology', 0.178), ('nw', 0.175), ('lexical', 0.169), ('morphological', 0.169), ('token', 0.164), ('corpus', 0.157), ('frequencies', 0.151), ('crp', 0.148), ('oldwater', 0.136), ('xes', 0.133), ('tables', 0.131), ('language', 0.12), ('ohnson', 0.116), ('riffiths', 0.116), ('dirichlet', 0.114), ('restaurant', 0.105), ('anguage', 0.1), ('twostage', 0.091), ('goldwater', 0.087), ('adaptors', 0.085), ('stems', 0.085), ('customers', 0.08), ('damping', 0.078), ('items', 0.073), ('wo', 0.072), ('nk', 0.071), ('frequency', 0.067), ('null', 0.066), ('linguistic', 0.066), ('linguistics', 0.066), ('seating', 0.063), ('zi', 0.062), ('pitman', 0.061), ('irregular', 0.061), ('stem', 0.059), ('odels', 0.059), ('stage', 0.058), ('wi', 0.058), ('words', 0.058), ('chinese', 0.057), ('multinomial', 0.053), ('segmentation', 0.052), ('verbs', 0.052), ('goldsmith', 0.051), ('sharon', 0.051), ('walked', 0.051), ('teh', 0.046), ('goodman', 0.045), ('ikn', 0.045), ('phonemic', 0.045), ('grammars', 0.045), ('types', 0.045), ('occupied', 0.045), ('kw', 0.044), ('verb', 0.044), ('counts', 0.044), ('lexicon', 0.043), ('donnell', 0.04), ('yor', 0.04), ('item', 0.039), ('produced', 0.039), ('dog', 0.039), ('analyses', 0.038), ('ing', 0.038), ('unsupervised', 0.038), ('smoothing', 0.037), ('frequent', 0.037), ('ney', 0.034), ('strings', 0.033), ('gold', 0.033), ('wn', 0.033), ('ths', 0.033), ('english', 0.033), ('table', 0.033), ('customer', 0.032), ('corpora', 0.032), ('languages', 0.032), ('generators', 0.03), ('grif', 0.03), ('johnson', 0.03), ('cohn', 0.03), ('bayesian', 0.03), ('model', 0.03), ('outcomes', 0.029), ('ascii', 0.028), ('linguists', 0.028), ('phonology', 0.028), ('tense', 0.028), ('nonparametric', 0.028), ('meeting', 0.028), ('distribution', 0.027), ('models', 0.027), ('lda', 0.026), ('grammar', 0.026), ('type', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999821 <a title="78-tfidf-1" href="./jmlr-2011-Producing_Power-Law_Distributions_and_Damping_Word_Frequencies_with_Two-Stage_Language_Models.html">78 jmlr-2011-Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models</a></p>
<p>Author: Sharon Goldwater, Thomas L. Griffiths, Mark Johnson</p><p>Abstract: Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens. We present a framework for developing statistical models that can generically produce power laws, breaking generative models into two stages. The ﬁrst stage, the generator, can be any standard probabilistic model, while the second stage, the adaptor, transforms the word frequencies of this model to provide a closer match to natural language. We show that two commonly used Bayesian models, the Dirichlet-multinomial model and the Dirichlet process, can be viewed as special cases of our framework. We discuss two stochastic processes—the Chinese restaurant process and its two-parameter generalization based on the Pitman-Yor process—that can be used as adaptors in our framework to produce power-law distributions over word frequencies. We show that these adaptors justify common estimation procedures based on logarithmic or inverse-power transformations of empirical frequencies. In addition, taking the Pitman-Yor Chinese restaurant process as an adaptor justiﬁes the appearance of type frequencies in formal analyses of natural language and improves the performance of a model for unsupervised learning of morphology. Keywords: nonparametric Bayes, Pitman-Yor process, language model, unsupervised</p><p>2 0.20171735 <a title="78-tfidf-2" href="./jmlr-2011-Distance_Dependent_Chinese_Restaurant_Processes.html">26 jmlr-2011-Distance Dependent Chinese Restaurant Processes</a></p>
<p>Author: David M. Blei, Peter I. Frazier</p><p>Abstract: We develop the distance dependent Chinese restaurant process, a ﬂexible class of distributions over partitions that allows for dependencies between the elements. This class can be used to model many kinds of dependencies between data in inﬁnite clustering models, including dependencies arising from time, space, and network connectivity. We examine the properties of the distance dependent CRP, discuss its connections to Bayesian nonparametric mixture models, and derive a Gibbs sampler for both fully observed and latent mixture settings. We study its empirical performance with three text corpora. We show that relaxing the assumption of exchangeability with distance dependent CRPs can provide a better ﬁt to sequential data and network data. We also show that the distance dependent CRP representation of the traditional CRP mixture leads to a faster-mixing Gibbs sampling algorithm than the one based on the original formulation. Keywords: Chinese restaurant processes, Bayesian nonparametrics</p><p>3 0.14760628 <a title="78-tfidf-3" href="./jmlr-2011-The_Indian_Buffet_Process%3A_An_Introduction_and_Review.html">90 jmlr-2011-The Indian Buffet Process: An Introduction and Review</a></p>
<p>Author: Thomas L. Griffiths, Zoubin Ghahramani</p><p>Abstract: The Indian buffet process is a stochastic process deﬁning a probability distribution over equivalence classes of sparse binary matrices with a ﬁnite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially inﬁnite array of features, or that involve bipartite graphs in which the size of at least one class of nodes is unknown. We give a detailed derivation of this distribution, and illustrate its use as a prior in an inﬁnite latent feature model. We then review recent applications of the Indian buffet process in machine learning, discuss its extensions, and summarize its connections to other stochastic processes. Keywords: nonparametric Bayes, Markov chain Monte Carlo, latent variable models, Chinese restaurant processes, beta process, exchangeable distributions, sparse binary matrices</p><p>4 0.12768428 <a title="78-tfidf-4" href="./jmlr-2011-Natural_Language_Processing_%28Almost%29_from_Scratch.html">68 jmlr-2011-Natural Language Processing (Almost) from Scratch</a></p>
<p>Author: Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel Kuksa</p><p>Abstract: We propose a uniﬁed neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-speciﬁc engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements. Keywords: natural language processing, neural networks</p><p>5 0.12218677 <a title="78-tfidf-5" href="./jmlr-2011-Introduction_to_the_Special_Topic_on_Grammar_Induction%2C_Representation_of_Language_and_Language_Learning.html">46 jmlr-2011-Introduction to the Special Topic on Grammar Induction, Representation of Language and Language Learning</a></p>
<p>Author: Dorota Głowacka, John Shawe-Taylor, Alex Clark, Colin de la Higuera, Mark Johnson</p><p>Abstract: Grammar induction refers to the process of learning grammars and languages from data; this ﬁnds a variety of applications in syntactic pattern recognition, the modeling of natural language acquisition, data mining and machine translation. This special topic contains several papers presenting some of recent developments in the area of grammar induction and language learning, as applied to various problems in Natural Language Processing, including supervised and unsupervised parsing and statistical machine translation. Keywords: machine translation, Bayesian inference, grammar induction, natural language parsing</p><p>6 0.085017465 <a title="78-tfidf-6" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>7 0.083535589 <a title="78-tfidf-7" href="./jmlr-2011-Non-Parametric_Estimation_of_Topic_Hierarchies_from_Texts_with_Hierarchical_Dirichlet_Processes.html">70 jmlr-2011-Non-Parametric Estimation of Topic Hierarchies from Texts with Hierarchical Dirichlet Processes</a></p>
<p>8 0.067846745 <a title="78-tfidf-8" href="./jmlr-2011-Dirichlet_Process_Mixtures_of_Generalized_Linear_Models.html">24 jmlr-2011-Dirichlet Process Mixtures of Generalized Linear Models</a></p>
<p>9 0.065135859 <a title="78-tfidf-9" href="./jmlr-2011-Minimum_Description_Length_Penalization_for_Group_and_Multi-Task_Sparse_Learning.html">64 jmlr-2011-Minimum Description Length Penalization for Group and Multi-Task Sparse Learning</a></p>
<p>10 0.058635529 <a title="78-tfidf-10" href="./jmlr-2011-Exploitation_of_Machine_Learning_Techniques_in_Modelling_Phrase_Movements_for_Machine_Translation.html">32 jmlr-2011-Exploitation of Machine Learning Techniques in Modelling Phrase Movements for Machine Translation</a></p>
<p>11 0.056203768 <a title="78-tfidf-11" href="./jmlr-2011-Logistic_Stick-Breaking_Process.html">61 jmlr-2011-Logistic Stick-Breaking Process</a></p>
<p>12 0.041638419 <a title="78-tfidf-12" href="./jmlr-2011-Unsupervised_Supervised_Learning_II%3A_Margin-Based_Classification_Without_Labels.html">100 jmlr-2011-Unsupervised Supervised Learning II: Margin-Based Classification Without Labels</a></p>
<p>13 0.035428796 <a title="78-tfidf-13" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>14 0.034676231 <a title="78-tfidf-14" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<p>15 0.033453476 <a title="78-tfidf-15" href="./jmlr-2011-Proximal_Methods_for_Hierarchical_Sparse_Coding.html">79 jmlr-2011-Proximal Methods for Hierarchical Sparse Coding</a></p>
<p>16 0.03127075 <a title="78-tfidf-16" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>17 0.029306917 <a title="78-tfidf-17" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>18 0.029032871 <a title="78-tfidf-18" href="./jmlr-2011-Robust_Gaussian_Process_Regression_with_a_Student-tLikelihood.html">82 jmlr-2011-Robust Gaussian Process Regression with a Student-tLikelihood</a></p>
<p>19 0.027203318 <a title="78-tfidf-19" href="./jmlr-2011-Efficient_and_Effective_Visual_Codebook_Generation_Using_Additive_Kernels.html">31 jmlr-2011-Efficient and Effective Visual Codebook Generation Using Additive Kernels</a></p>
<p>20 0.026859593 <a title="78-tfidf-20" href="./jmlr-2011-Unsupervised_Similarity-Based_Risk_Stratification_for_Cardiovascular_Events_Using_Long-Term_Time-Series_Data.html">99 jmlr-2011-Unsupervised Similarity-Based Risk Stratification for Cardiovascular Events Using Long-Term Time-Series Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.184), (1, -0.173), (2, -0.146), (3, 0.006), (4, -0.307), (5, -0.007), (6, 0.115), (7, 0.364), (8, -0.054), (9, -0.122), (10, 0.011), (11, -0.034), (12, -0.142), (13, -0.115), (14, -0.034), (15, 0.04), (16, -0.021), (17, 0.051), (18, 0.007), (19, -0.064), (20, 0.033), (21, -0.042), (22, -0.031), (23, -0.019), (24, -0.018), (25, 0.005), (26, 0.055), (27, -0.009), (28, -0.121), (29, 0.012), (30, -0.093), (31, -0.027), (32, 0.006), (33, 0.049), (34, 0.027), (35, 0.08), (36, 0.012), (37, -0.069), (38, 0.061), (39, 0.017), (40, 0.036), (41, 0.051), (42, 0.013), (43, 0.019), (44, 0.094), (45, 0.069), (46, 0.012), (47, -0.007), (48, -0.069), (49, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95892465 <a title="78-lsi-1" href="./jmlr-2011-Producing_Power-Law_Distributions_and_Damping_Word_Frequencies_with_Two-Stage_Language_Models.html">78 jmlr-2011-Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models</a></p>
<p>Author: Sharon Goldwater, Thomas L. Griffiths, Mark Johnson</p><p>Abstract: Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens. We present a framework for developing statistical models that can generically produce power laws, breaking generative models into two stages. The ﬁrst stage, the generator, can be any standard probabilistic model, while the second stage, the adaptor, transforms the word frequencies of this model to provide a closer match to natural language. We show that two commonly used Bayesian models, the Dirichlet-multinomial model and the Dirichlet process, can be viewed as special cases of our framework. We discuss two stochastic processes—the Chinese restaurant process and its two-parameter generalization based on the Pitman-Yor process—that can be used as adaptors in our framework to produce power-law distributions over word frequencies. We show that these adaptors justify common estimation procedures based on logarithmic or inverse-power transformations of empirical frequencies. In addition, taking the Pitman-Yor Chinese restaurant process as an adaptor justiﬁes the appearance of type frequencies in formal analyses of natural language and improves the performance of a model for unsupervised learning of morphology. Keywords: nonparametric Bayes, Pitman-Yor process, language model, unsupervised</p><p>2 0.77547127 <a title="78-lsi-2" href="./jmlr-2011-Distance_Dependent_Chinese_Restaurant_Processes.html">26 jmlr-2011-Distance Dependent Chinese Restaurant Processes</a></p>
<p>Author: David M. Blei, Peter I. Frazier</p><p>Abstract: We develop the distance dependent Chinese restaurant process, a ﬂexible class of distributions over partitions that allows for dependencies between the elements. This class can be used to model many kinds of dependencies between data in inﬁnite clustering models, including dependencies arising from time, space, and network connectivity. We examine the properties of the distance dependent CRP, discuss its connections to Bayesian nonparametric mixture models, and derive a Gibbs sampler for both fully observed and latent mixture settings. We study its empirical performance with three text corpora. We show that relaxing the assumption of exchangeability with distance dependent CRPs can provide a better ﬁt to sequential data and network data. We also show that the distance dependent CRP representation of the traditional CRP mixture leads to a faster-mixing Gibbs sampling algorithm than the one based on the original formulation. Keywords: Chinese restaurant processes, Bayesian nonparametrics</p><p>3 0.62304407 <a title="78-lsi-3" href="./jmlr-2011-The_Indian_Buffet_Process%3A_An_Introduction_and_Review.html">90 jmlr-2011-The Indian Buffet Process: An Introduction and Review</a></p>
<p>Author: Thomas L. Griffiths, Zoubin Ghahramani</p><p>Abstract: The Indian buffet process is a stochastic process deﬁning a probability distribution over equivalence classes of sparse binary matrices with a ﬁnite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially inﬁnite array of features, or that involve bipartite graphs in which the size of at least one class of nodes is unknown. We give a detailed derivation of this distribution, and illustrate its use as a prior in an inﬁnite latent feature model. We then review recent applications of the Indian buffet process in machine learning, discuss its extensions, and summarize its connections to other stochastic processes. Keywords: nonparametric Bayes, Markov chain Monte Carlo, latent variable models, Chinese restaurant processes, beta process, exchangeable distributions, sparse binary matrices</p><p>4 0.57603151 <a title="78-lsi-4" href="./jmlr-2011-Natural_Language_Processing_%28Almost%29_from_Scratch.html">68 jmlr-2011-Natural Language Processing (Almost) from Scratch</a></p>
<p>Author: Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel Kuksa</p><p>Abstract: We propose a uniﬁed neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-speciﬁc engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements. Keywords: natural language processing, neural networks</p><p>5 0.46343142 <a title="78-lsi-5" href="./jmlr-2011-Introduction_to_the_Special_Topic_on_Grammar_Induction%2C_Representation_of_Language_and_Language_Learning.html">46 jmlr-2011-Introduction to the Special Topic on Grammar Induction, Representation of Language and Language Learning</a></p>
<p>Author: Dorota Głowacka, John Shawe-Taylor, Alex Clark, Colin de la Higuera, Mark Johnson</p><p>Abstract: Grammar induction refers to the process of learning grammars and languages from data; this ﬁnds a variety of applications in syntactic pattern recognition, the modeling of natural language acquisition, data mining and machine translation. This special topic contains several papers presenting some of recent developments in the area of grammar induction and language learning, as applied to various problems in Natural Language Processing, including supervised and unsupervised parsing and statistical machine translation. Keywords: machine translation, Bayesian inference, grammar induction, natural language parsing</p><p>6 0.4203026 <a title="78-lsi-6" href="./jmlr-2011-Non-Parametric_Estimation_of_Topic_Hierarchies_from_Texts_with_Hierarchical_Dirichlet_Processes.html">70 jmlr-2011-Non-Parametric Estimation of Topic Hierarchies from Texts with Hierarchical Dirichlet Processes</a></p>
<p>7 0.40624636 <a title="78-lsi-7" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>8 0.34901717 <a title="78-lsi-8" href="./jmlr-2011-Minimum_Description_Length_Penalization_for_Group_and_Multi-Task_Sparse_Learning.html">64 jmlr-2011-Minimum Description Length Penalization for Group and Multi-Task Sparse Learning</a></p>
<p>9 0.34554526 <a title="78-lsi-9" href="./jmlr-2011-Exploitation_of_Machine_Learning_Techniques_in_Modelling_Phrase_Movements_for_Machine_Translation.html">32 jmlr-2011-Exploitation of Machine Learning Techniques in Modelling Phrase Movements for Machine Translation</a></p>
<p>10 0.28075621 <a title="78-lsi-10" href="./jmlr-2011-Dirichlet_Process_Mixtures_of_Generalized_Linear_Models.html">24 jmlr-2011-Dirichlet Process Mixtures of Generalized Linear Models</a></p>
<p>11 0.22917557 <a title="78-lsi-11" href="./jmlr-2011-Logistic_Stick-Breaking_Process.html">61 jmlr-2011-Logistic Stick-Breaking Process</a></p>
<p>12 0.19094294 <a title="78-lsi-12" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>13 0.17307894 <a title="78-lsi-13" href="./jmlr-2011-Efficient_and_Effective_Visual_Codebook_Generation_Using_Additive_Kernels.html">31 jmlr-2011-Efficient and Effective Visual Codebook Generation Using Additive Kernels</a></p>
<p>14 0.172993 <a title="78-lsi-14" href="./jmlr-2011-Unsupervised_Supervised_Learning_II%3A_Margin-Based_Classification_Without_Labels.html">100 jmlr-2011-Unsupervised Supervised Learning II: Margin-Based Classification Without Labels</a></p>
<p>15 0.1528866 <a title="78-lsi-15" href="./jmlr-2011-Unsupervised_Similarity-Based_Risk_Stratification_for_Cardiovascular_Events_Using_Long-Term_Time-Series_Data.html">99 jmlr-2011-Unsupervised Similarity-Based Risk Stratification for Cardiovascular Events Using Long-Term Time-Series Data</a></p>
<p>16 0.15090933 <a title="78-lsi-16" href="./jmlr-2011-A_Bayesian_Approximation_Method_for_Online_Ranking.html">2 jmlr-2011-A Bayesian Approximation Method for Online Ranking</a></p>
<p>17 0.15045609 <a title="78-lsi-17" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>18 0.14698425 <a title="78-lsi-18" href="./jmlr-2011-Bayesian_Generalized_Kernel_Mixed_Models.html">13 jmlr-2011-Bayesian Generalized Kernel Mixed Models</a></p>
<p>19 0.13942727 <a title="78-lsi-19" href="./jmlr-2011-Learning_Transformation_Models_for_Ranking_and_Survival_Analysis.html">56 jmlr-2011-Learning Transformation Models for Ranking and Survival Analysis</a></p>
<p>20 0.13849834 <a title="78-lsi-20" href="./jmlr-2011-Scikit-learn%3A_Machine_Learning_in_Python.html">83 jmlr-2011-Scikit-learn: Machine Learning in Python</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.035), (9, 0.017), (10, 0.028), (13, 0.017), (24, 0.104), (31, 0.097), (32, 0.023), (41, 0.03), (60, 0.011), (70, 0.012), (71, 0.016), (73, 0.035), (78, 0.033), (82, 0.035), (90, 0.056), (96, 0.356), (99, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76411885 <a title="78-lda-1" href="./jmlr-2011-Producing_Power-Law_Distributions_and_Damping_Word_Frequencies_with_Two-Stage_Language_Models.html">78 jmlr-2011-Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models</a></p>
<p>Author: Sharon Goldwater, Thomas L. Griffiths, Mark Johnson</p><p>Abstract: Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens. We present a framework for developing statistical models that can generically produce power laws, breaking generative models into two stages. The ﬁrst stage, the generator, can be any standard probabilistic model, while the second stage, the adaptor, transforms the word frequencies of this model to provide a closer match to natural language. We show that two commonly used Bayesian models, the Dirichlet-multinomial model and the Dirichlet process, can be viewed as special cases of our framework. We discuss two stochastic processes—the Chinese restaurant process and its two-parameter generalization based on the Pitman-Yor process—that can be used as adaptors in our framework to produce power-law distributions over word frequencies. We show that these adaptors justify common estimation procedures based on logarithmic or inverse-power transformations of empirical frequencies. In addition, taking the Pitman-Yor Chinese restaurant process as an adaptor justiﬁes the appearance of type frequencies in formal analyses of natural language and improves the performance of a model for unsupervised learning of morphology. Keywords: nonparametric Bayes, Pitman-Yor process, language model, unsupervised</p><p>2 0.38613501 <a title="78-lda-2" href="./jmlr-2011-Learning_from_Partial_Labels.html">58 jmlr-2011-Learning from Partial Labels</a></p>
<p>Author: Timothee Cour, Ben Sapp, Ben Taskar</p><p>Abstract: We address the problem of partially-labeled multiclass classiﬁcation, where instead of a single label per instance, the algorithm is given a candidate set of labels, only one of which is correct. Our setting is motivated by a common scenario in many image and video collections, where only partial access to labels is available. The goal is to learn a classiﬁer that can disambiguate the partiallylabeled training instances, and generalize to unseen data. We deﬁne an intuitive property of the data distribution that sharply characterizes the ability to learn in this setting and show that effective learning is possible even when all the data is only partially labeled. Exploiting this property of the data, we propose a convex learning formulation based on minimization of a loss function appropriate for the partial label setting. We analyze the conditions under which our loss function is asymptotically consistent, as well as its generalization and transductive performance. We apply our framework to identifying faces culled from web news sources and to naming characters in TV series and movies; in particular, we annotated and experimented on a very large video data set and achieve 6% error for character naming on 16 episodes of the TV series Lost. Keywords: weakly supervised learning, multiclass classiﬁcation, convex learning, generalization bounds, names and faces</p><p>3 0.37531513 <a title="78-lda-3" href="./jmlr-2011-The_Indian_Buffet_Process%3A_An_Introduction_and_Review.html">90 jmlr-2011-The Indian Buffet Process: An Introduction and Review</a></p>
<p>Author: Thomas L. Griffiths, Zoubin Ghahramani</p><p>Abstract: The Indian buffet process is a stochastic process deﬁning a probability distribution over equivalence classes of sparse binary matrices with a ﬁnite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially inﬁnite array of features, or that involve bipartite graphs in which the size of at least one class of nodes is unknown. We give a detailed derivation of this distribution, and illustrate its use as a prior in an inﬁnite latent feature model. We then review recent applications of the Indian buffet process in machine learning, discuss its extensions, and summarize its connections to other stochastic processes. Keywords: nonparametric Bayes, Markov chain Monte Carlo, latent variable models, Chinese restaurant processes, beta process, exchangeable distributions, sparse binary matrices</p><p>4 0.37430272 <a title="78-lda-4" href="./jmlr-2011-Double_Updating_Online_Learning.html">28 jmlr-2011-Double Updating Online Learning</a></p>
<p>Author: Peilin Zhao, Steven C.H. Hoi, Rong Jin</p><p>Abstract: In most kernel based online learning algorithms, when an incoming instance is misclassiﬁed, it will be added into the pool of support vectors and assigned with a weight, which often remains unchanged during the rest of the learning process. This is clearly insufﬁcient since when a new support vector is added, we generally expect the weights of the other existing support vectors to be updated in order to reﬂect the inﬂuence of the added support vector. In this paper, we propose a new online learning method, termed Double Updating Online Learning, or DUOL for short, that explicitly addresses this problem. Instead of only assigning a ﬁxed weight to the misclassiﬁed example received at the current trial, the proposed online learning algorithm also tries to update the weight for one of the existing support vectors. We show that the mistake bound can be improved by the proposed online learning method. We conduct an extensive set of empirical evaluations for both binary and multi-class online learning tasks. The experimental results show that the proposed technique is considerably more effective than the state-of-the-art online learning algorithms. The source code is available to public at http://www.cais.ntu.edu.sg/˜chhoi/DUOL/. Keywords: online learning, kernel method, support vector machines, maximum margin learning, classiﬁcation</p><p>5 0.37104926 <a title="78-lda-5" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>Author: Ricardo Henao, Ole Winther</p><p>Abstract: In this paper we consider sparse and identiﬁable linear latent variable (factor) and linear Bayesian network models for parsimonious analysis of multivariate data. We propose a computationally efﬁcient method for joint parameter and model inference, and model comparison. It consists of a fully Bayesian hierarchy for sparse models using slab and spike priors (two-component δ-function and continuous mixtures), non-Gaussian latent factors and a stochastic search over the ordering of the variables. The framework, which we call SLIM (Sparse Linear Identiﬁable Multivariate modeling), is validated and bench-marked on artiﬁcial and real biological data sets. SLIM is closest in spirit to LiNGAM (Shimizu et al., 2006), but differs substantially in inference, Bayesian network structure learning and model comparison. Experimentally, SLIM performs equally well or better than LiNGAM with comparable computational complexity. We attribute this mainly to the stochastic search strategy used, and to parsimony (sparsity and identiﬁability), which is an explicit part of the model. We propose two extensions to the basic i.i.d. linear framework: non-linear dependence on observed variables, called SNIM (Sparse Non-linear Identiﬁable Multivariate modeling) and allowing for correlations between latent variables, called CSLIM (Correlated SLIM), for the temporal and/or spatial data. The source code and scripts are available from http://cogsys.imm.dtu.dk/slim/. Keywords: parsimony, sparsity, identiﬁability, factor models, linear Bayesian networks</p><p>6 0.36842757 <a title="78-lda-6" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>7 0.36832902 <a title="78-lda-7" href="./jmlr-2011-Logistic_Stick-Breaking_Process.html">61 jmlr-2011-Logistic Stick-Breaking Process</a></p>
<p>8 0.36238188 <a title="78-lda-8" href="./jmlr-2011-Minimum_Description_Length_Penalization_for_Group_and_Multi-Task_Sparse_Learning.html">64 jmlr-2011-Minimum Description Length Penalization for Group and Multi-Task Sparse Learning</a></p>
<p>9 0.35910589 <a title="78-lda-9" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<p>10 0.35862476 <a title="78-lda-10" href="./jmlr-2011-Introduction_to_the_Special_Topic_on_Grammar_Induction%2C_Representation_of_Language_and_Language_Learning.html">46 jmlr-2011-Introduction to the Special Topic on Grammar Induction, Representation of Language and Language Learning</a></p>
<p>11 0.3575449 <a title="78-lda-11" href="./jmlr-2011-Non-Parametric_Estimation_of_Topic_Hierarchies_from_Texts_with_Hierarchical_Dirichlet_Processes.html">70 jmlr-2011-Non-Parametric Estimation of Topic Hierarchies from Texts with Hierarchical Dirichlet Processes</a></p>
<p>12 0.35133901 <a title="78-lda-12" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>13 0.35043529 <a title="78-lda-13" href="./jmlr-2011-Natural_Language_Processing_%28Almost%29_from_Scratch.html">68 jmlr-2011-Natural Language Processing (Almost) from Scratch</a></p>
<p>14 0.34906015 <a title="78-lda-14" href="./jmlr-2011-Dirichlet_Process_Mixtures_of_Generalized_Linear_Models.html">24 jmlr-2011-Dirichlet Process Mixtures of Generalized Linear Models</a></p>
<p>15 0.34666911 <a title="78-lda-15" href="./jmlr-2011-A_Family_of_Simple_Non-Parametric_Kernel_Learning_Algorithms.html">4 jmlr-2011-A Family of Simple Non-Parametric Kernel Learning Algorithms</a></p>
<p>16 0.34502271 <a title="78-lda-16" href="./jmlr-2011-Computationally_Efficient_Convolved_Multiple_Output_Gaussian_Processes.html">17 jmlr-2011-Computationally Efficient Convolved Multiple Output Gaussian Processes</a></p>
<p>17 0.34401464 <a title="78-lda-17" href="./jmlr-2011-Exploitation_of_Machine_Learning_Techniques_in_Modelling_Phrase_Movements_for_Machine_Translation.html">32 jmlr-2011-Exploitation of Machine Learning Techniques in Modelling Phrase Movements for Machine Translation</a></p>
<p>18 0.34367999 <a title="78-lda-18" href="./jmlr-2011-Distance_Dependent_Chinese_Restaurant_Processes.html">26 jmlr-2011-Distance Dependent Chinese Restaurant Processes</a></p>
<p>19 0.34157318 <a title="78-lda-19" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>20 0.33807942 <a title="78-lda-20" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
