<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>99 jmlr-2011-Unsupervised Similarity-Based Risk Stratification for Cardiovascular Events Using Long-Term Time-Series Data</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-99" href="#">jmlr2011-99</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>99 jmlr-2011-Unsupervised Similarity-Based Risk Stratification for Cardiovascular Events Using Long-Term Time-Series Data</h1>
<br/><p>Source: <a title="jmlr-2011-99-pdf" href="http://jmlr.org/papers/volume12/syed11a/syed11a.pdf">pdf</a></p><p>Author: Zeeshan Syed, John Guttag</p><p>Abstract: In medicine, one often bases decisions upon a comparative analysis of patient data. In this paper, we build upon this observation and describe similarity-based algorithms to risk stratify patients for major adverse cardiac events. We evolve the traditional approach of comparing patient data in two ways. First, we propose similarity-based algorithms that compare patients in terms of their long-term physiological monitoring data. Symbolic mismatch identiﬁes functional units in longterm signals and measures changes in the morphology and frequency of these units across patients. Second, we describe similarity-based algorithms that are unsupervised and do not require comparisons to patients with known outcomes for risk stratiﬁcation. This is achieved by using an anomaly detection framework to identify patients who are unlike other patients in a population and may potentially be at an elevated risk. We demonstrate the potential utility of our approach by showing how symbolic mismatch-based algorithms can be used to classify patients as being at high or low risk of major adverse cardiac events by comparing their long-term electrocardiograms to that of a large population. We describe how symbolic mismatch can be used in three different existing methods: one-class support vector machines, nearest neighbor analysis, and hierarchical clustering. When evaluated on a population of 686 patients with available long-term electrocardiographic data, symbolic mismatch-based comparative approaches were able to identify patients at roughly a two-fold increased risk of major adverse cardiac events in the 90 days following acute coronary syndrome. These results were consistent even after adjusting for other clinical risk variables. Keywords: risk stratiﬁcation, cardiovascular disease, time-series comparison, symbolic analysis, anomaly detection</p><p>Reference: <a title="jmlr-2011-99-reference" href="../jmlr2011_reference/jmlr-2011-Unsupervised_Similarity-Based_Risk_Stratification_for_Cardiovascular_Events_Using_Long-Term_Time-Series_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we build upon this observation and describe similarity-based algorithms to risk stratify patients for major adverse cardiac events. [sent-6, score-1.012]
</p><p>2 Second, we describe similarity-based algorithms that are unsupervised and do not require comparisons to patients with known outcomes for risk stratiﬁcation. [sent-10, score-0.692]
</p><p>3 This is achieved by using an anomaly detection framework to identify patients who are unlike other patients in a population and may potentially be at an elevated risk. [sent-11, score-0.992]
</p><p>4 We demonstrate the potential utility of our approach by showing how symbolic mismatch-based algorithms can be used to classify patients as being at high or low risk of major adverse cardiac events by comparing their long-term electrocardiograms to that of a large population. [sent-12, score-1.383]
</p><p>5 We describe how symbolic mismatch can be used in three different existing methods: one-class support vector machines, nearest neighbor analysis, and hierarchical clustering. [sent-13, score-0.633]
</p><p>6 When evaluated on a population of 686 patients with available long-term electrocardiographic data, symbolic mismatch-based comparative approaches were able to identify patients at roughly a two-fold increased risk of major adverse cardiac events in the 90 days following acute coronary syndrome. [sent-14, score-2.157]
</p><p>7 Keywords: risk stratiﬁcation, cardiovascular disease, time-series comparison, symbolic analysis, anomaly detection  1. [sent-16, score-0.694]
</p><p>8 In this paper, we investigate the hypothesis that comparative analyses of long-term physiological activity can aid risk stratiﬁcation and present symbolic mismatch as a way to quantify differences between the physiological signals of patients. [sent-34, score-0.884]
</p><p>9 Symbolic mismatch compares long-term time-series by mapping the original signals into a symbolic domain and quantifying differences between the morphology and frequency of prototypical functional units. [sent-35, score-0.61]
</p><p>10 For example, comparing the long-term electrocardiographic (ECG) activity between two patients may involve comparing over a hundred thousand beats (with each beats having roughly 500 samples per beat). [sent-37, score-0.655]
</p><p>11 This reduction allows for symbolic mismatch to be useful in analyzing very long time-series. [sent-39, score-0.502]
</p><p>12 We also present different ways in which symbolic mismatch can be used to identify high risk patients in a population. [sent-41, score-1.131]
</p><p>13 The methods we propose are motivated by the observation that high risk patients typically constitute a small minority in a population. [sent-42, score-0.629]
</p><p>14 For example, cardiac mortality over a 90 day period following acute coronary syndrome (ACS) was reported to be 1. [sent-43, score-0.589]
</p><p>15 Our hypothesis is that these patients can be discovered as anomalies in the population, that is, their physiological activity over long periods of time is dissimilar to the majority of the patients in the population. [sent-51, score-0.998]
</p><p>16 However, matching patients with treatments that are appropriate for their risk has proven to be challenging (Bailey et al. [sent-63, score-0.629]
</p><p>17 We focus, in particular, on identifying patients at risk of major adverse cardiac events (death, myocardial infarction and severe recurrent ischemia) following coronary attacks. [sent-67, score-1.397]
</p><p>18 This work uses long-term ECG signals recorded during patient admission for acute coronary syndrome. [sent-68, score-0.512]
</p><p>19 This is followed by a discussion of acute coronary syndromes (ACS) and a summary of recently proposed long-term ECG metrics for risk stratiﬁcation following ACS. [sent-76, score-0.505]
</p><p>20 2 Acute Coronary Syndromes In our research, we mainly focus on identifying high risk patients following acute coronary syndrome (ACS), an umbrella term covering clinical symptoms compatible with reduced blood supply to the heart (i. [sent-113, score-1.279]
</p><p>21 The most common symptom of ACS is unusual and unprovoked chest pain, but this may often be absent (most notably in patients with diabetes who experience “silent” heart attacks). [sent-118, score-0.667]
</p><p>22 Patients with ST segment elevation are given the diagnosis of ST-elevation MI (STEMI) while patients without ST segment elevation are given the diagnosis of non-ST-elevation ACS (NSTEACS). [sent-125, score-0.59]
</p><p>23 Patients with STEMI are typically at a higher risk relative to patients with NSTEACS. [sent-127, score-0.629]
</p><p>24 3 Post-ACS Risk Stratiﬁcation and Long-Term ECG Techniques Since patients who experience ACS remain at an elevated risk of death even after receiving treatment for the initial index event (Newby et al. [sent-131, score-0.68]
</p><p>25 , 2003), post-ACS risk stratiﬁcation is an important clinical step in determining which patients should be monitored and treated more (or less) aggressively subsequently. [sent-132, score-0.735]
</p><p>26 Roughly speaking, the goal of risk stratiﬁcation is to identify groups of patients within the post-ACS population with different rates of adverse outcomes despite receiving similar care. [sent-133, score-0.863]
</p><p>27 A number of different methods have been proposed to risk stratify patients following ACS with an excellent review provided by Breall and Simons (2010) and Alpert (2010). [sent-135, score-0.629]
</p><p>28 This corresponds to the “turbulence” in the heart rate and is present in patients with a healthy autonomic nervous system. [sent-162, score-0.632]
</p><p>29 The theory underlying DC is that decreased decelerations in the heart rate suggest an increased unresponsiveness of the heart to cardioprotective signals from the vagal system 1004  S IMILARITY-BASED C ARDIOVASCULAR R ISK S TRATIFICATION U SING T IME -S ERIES DATA  for the heart to slow down. [sent-170, score-0.64]
</p><p>30 These methods study the rates of adverse outcomes in patients assigned to different groups (e. [sent-174, score-0.611]
</p><p>31 However, data from clinical studies often consists of a mix of patients who either remain event free throughout the duration of the study, experience events at fairly different times within the study, or leave the study before it is complete (a phenomenon termed censoring). [sent-178, score-0.613]
</p><p>32 Survival analysis focuses on using information in all these cases, that is, by factoring in both the timing of events in patients who experience adverse outcomes, and by using data from patients who leave the study early for the period during which they participated. [sent-179, score-1.12]
</p><p>33 , differences in the rates of events over time in patients belonging to two or more groups) can be quantiﬁed through a variety of methods. [sent-184, score-0.507]
</p><p>34 Symbolic Mismatch In this section, we describe the process through which symbolic mismatch is computed between a pair of long-term ECG time-series. [sent-190, score-0.502]
</p><p>35 The use of symbolic mismatch for risk stratiﬁcation is presented in Section 4. [sent-191, score-0.695]
</p><p>36 Intuitively, the symbolic mismatch between patients p and q corresponds to an estimate of the expected difference in morphology between any two randomly chosen beats from these patients. [sent-232, score-1.086]
</p><p>37 1006  S IMILARITY-BASED C ARDIOVASCULAR R ISK S TRATIFICATION U SING T IME -S ERIES DATA  The symbolic mismatch computation achieves this by weighting the difference between the centroids for every pair of symbols for the patients by the frequencies with which these symbols occur. [sent-233, score-1.082]
</p><p>38 An important feature of symbolic mismatch is that it is explicitly designed to avoid the need to set up a correspondence between the symbols of patients p and q. [sent-234, score-0.991]
</p><p>39 , 1997; Cohen and Richman, 2002) to compare data for two patients by ﬁrst making an assignment from symbols in one patient to the other, symbolic mismatch does not require any cross-patient registration of symbols. [sent-236, score-1.139]
</p><p>40 3 Spectrum Clipping The formulation for symbolic mismatch in Equation 1 gives rise to a symmetric dissimilarity matrix. [sent-240, score-0.502]
</p><p>41 For both the dissimilarity and similarity case, however, symbolic mismatch can produce a matrix that is indeﬁnite. [sent-242, score-0.502]
</p><p>42 We use spectrum clipping to generalize the use of symbolic mismatch for classiﬁcation. [sent-247, score-0.645]
</p><p>43 We note that while we introduce spectrum clipping mainly for the purpose of broadening the applicability of symbolic mismatch to kernel-based methods, this approach offers additional advantages. [sent-262, score-0.645]
</p><p>44 Risk Stratiﬁcation Using Symbolic Mismatch We now present three different approaches using symbolic mismatch to identify high risk patients in a population. [sent-267, score-1.131]
</p><p>45 The other two approaches, nearest neighbor analysis and hierarchical clustering, use the symbolic mismatch dissimilarity matrix. [sent-271, score-0.633]
</p><p>46 In each case, the symbolic mismatch matrix is processed using spectrum clipping. [sent-272, score-0.56]
</p><p>47 The resulting predicted label in terms of the Lagrange multipliers αi and the spectrum clipped symbolic mismatch similarity matrix Ψclip is then: y j = sgn(∑ αi Ψclip (i, j) − p). [sent-285, score-0.56]
</p><p>48 The distance between two clusters is deﬁned as the average of the pairwise symbolic mismatch of the patients in each cluster. [sent-299, score-0.938]
</p><p>49 We deﬁne a major adverse cardiac event to be any of these three adverse events. [sent-313, score-0.526]
</p><p>50 Consistent with earlier studies to evaluate new methods for risk stratiﬁcation in the setting of acute coronary syndrome (Shlipak et al. [sent-318, score-0.505]
</p><p>51 , 2008), we classiﬁed patients in the highest quartile as the high risk group. [sent-319, score-0.629]
</p><p>52 For the nearest neighbor approach we investigated all odd values of k from 3 to 9, and patients with anomaly scores in the top 25% of the population were classiﬁed as being at high risk. [sent-321, score-0.687]
</p><p>53 We employed Kaplan-Meier survival analysis (Efron, 1988) to compare the rates for major adverse cardiac events between patients declared to be at high and low risk for all three approaches. [sent-327, score-1.144]
</p><p>54 68  Table 2: Univariate association of high risk predictions from different approaches using symbolic mismatch with major adverse cardiac events over a 90 day period following acute coronary syndrome. [sent-382, score-1.5]
</p><p>55 05) association with major adverse cardiac events following acute coronary syndrome. [sent-391, score-0.733]
</p><p>56 The results in Table 2 can be interpreted as roughly a doubled rate of adverse outcomes per unit time in patients identiﬁed as being at high risk by the nearest neighbor and clustering approaches. [sent-392, score-0.978]
</p><p>57 For the classiﬁcation approach, patients identiﬁed as being at high risk had a nearly 40% increased risk of adverse outcomes. [sent-393, score-0.965]
</p><p>58 Both the nearest neighbor and clustering approaches had a higher hazard ratio in this patient population than any of the clinical and ECG risk variables studied. [sent-399, score-0.737]
</p><p>59 Of the clinical risk variables, only age was found to be associated on univariate analysis with major cardiac events after acute coronary syndrome. [sent-400, score-0.968]
</p><p>60 Of the ECG risk variables, both HRT and DC showed a univariate association with major adverse cardiac events during follow-up. [sent-403, score-0.692]
</p><p>61 These results suggest that unsupervised risk stratiﬁcation using symbolic mismatch can successfully identify patients at an elevated risk of major adverse cardiac events following ACS. [sent-404, score-1.809]
</p><p>62 In particular, our data shows that patients categorized as high risk by our methods continue to experience an increased risk of adverse outcomes throughout the entire 90 day period post-ACS (Figures 3-5). [sent-405, score-1.069]
</p><p>63 Our ﬁndings are also encouraging in that the relative increase in patient risk found using our methods compares quite favorably with other metrics based on specialized knowledge that are used in existing cardiac risk scoring tools. [sent-406, score-0.811]
</p><p>64 26  Table 3: Univariate association of existing clinical risk variables with major adverse cardiac events over a 90 day period following acute coronary syndrome. [sent-469, score-1.104]
</p><p>65 54  Table 4: Univariate association of existing ECG risk variables with major adverse cardiac events over a 90 day period following acute coronary syndrome. [sent-483, score-0.998]
</p><p>66 2 Multivariate Results We measured the correlation between the predictions of the unsupervised symbolic mismatch-based approaches and both the clinical and ECG risk variables. [sent-578, score-0.63]
</p><p>67 Our results on multivariate analysis reﬂect this low correlation between the symbolic mismatchbased approaches and existing clinical and ECG risk variables. [sent-582, score-0.599]
</p><p>68 Both the nearest neighbor and clustering approaches predicted patients with an approximately two-fold increased risk of adverse outcomes. [sent-585, score-0.946]
</p><p>69 46  Table 7: Multivariate association of high risk predictions from different approaches using symbolic mismatch with major adverse cardiac events over a 90 day period following acute coronary syndrome. [sent-612, score-1.5]
</p><p>70 For example, our study found that nearest neighbor-based risk stratiﬁcation using symbolic mismatch can identify individuals who are at a two- to three-fold increased risk of adverse outcomes, even after adjusting for an extensive set of existing risk variables. [sent-656, score-1.271]
</p><p>71 For each of the unsupervised risk stratiﬁcation approaches, the addition of the information produced by these methods increased the ability of models developed using existing risk variables to discriminate between high and low risk patients. [sent-662, score-0.61]
</p><p>72 We note that while our motivation for using spectrum clipping was largely to broaden the applicability of symbolic mismatch to kernel-based methods, the ability of spectrum clipping to reduce noise provided a positive effect for all methods. [sent-669, score-0.788]
</p><p>73 We denote the number of patients by n, the maximum number of symbols for any patient by θ, the maximum number of heart beats for any patient by m, and the maximum length of any heart beat (in samples) by l. [sent-688, score-1.33]
</p><p>74 While the computational cost of clustering is signiﬁcant, it leads to a θ2 factor in the runtime associated with ﬁnding anomalies using symbolic mismatch rather than an m2 factor (where m is much larger than θ as shown by our results). [sent-691, score-0.624]
</p><p>75 , by addressing the quadratic runtime of DTW or by avoiding comparisons between all pairs of patients in the population for anomaly detection), the values of n and l are both much smaller, and therefore represent smaller gains, than the m2 factor reduced by symbolization. [sent-695, score-0.595]
</p><p>76 Our belief was that for applications like risk stratifying patients for major cardiac events, focusing on a set of specialized features leads to important information being potentially missed. [sent-710, score-0.941]
</p><p>77 Most importantly, our approach helps address the scenario where symbolizing long-term time-series in a patient-speciﬁc manner leads to the symbolic sequences for patients being drawn from different alphabets (Syed et al. [sent-727, score-0.736]
</p><p>78 Symbolic mismatch addresses this scenario and complements existing work on sequence comparison through a simple, computationally efﬁcient measure that quantiﬁes differences across patients while retaining information on how the symbols for these patients differ. [sent-731, score-1.127]
</p><p>79 Finally, we also distinguish our work from earlier efforts to risk stratify patients using long-term data. [sent-732, score-0.629]
</p><p>80 In particular, we supplement our symbolic mismatch kernel with the idea of detecting high risk patients as those individuals in the population with unusual long-term time-series. [sent-733, score-1.19]
</p><p>81 , 1994) have all been shown to be useful in risk stratifying patients at risk for future cardiovascular events following acute coronary syndromes. [sent-738, score-1.345]
</p><p>82 The focus of these methods is to calculate a particular pre-deﬁned feature from the raw ECG signal, and to use it to rank patients along a risk continuum. [sent-739, score-0.629]
</p><p>83 Our approach, focusing on detecting patients with high symbolic mismatch relative to other patients in the population, is orthogonal to the use of specialized high risk features along two important dimensions. [sent-740, score-1.606]
</p><p>84 For the cardiovascular care, we only assume that ECG signals from patients who are at high risk differ from those of the rest of the population. [sent-742, score-0.821]
</p><p>85 Second, the ability to partition patients into groups with similar ECG characteristics and potentially common risk proﬁles potentially allows for a more ﬁne-grained understanding of a how a patient’s future health may evolve over time. [sent-744, score-0.662]
</p><p>86 Matching patients to past cases with similar ECG signals could lead to more accurate assignments of risk scores for particular events such as death and recurring heart attacks. [sent-745, score-0.999]
</p><p>87 Discussion In this paper, we proposed using symbolic mismatch to quantify differences in long-term physiological time-series. [sent-747, score-0.553]
</p><p>88 We developed multiple comparative approaches to detect such patients, and evaluated these methods in a real-world application of risk stratiﬁcation for major adverse cardiac events following acute coronary syndrome. [sent-750, score-0.926]
</p><p>89 Our results suggest that symbolic mismatch-based comparative approaches may have clinical utility in identifying high risk patients, and can provide information that is complementary to existing clinical risk variables. [sent-751, score-0.898]
</p><p>90 We observed a similar result in our patient population, where all of the existing clinical and ECG risk variables had a hazard ratio less than 2. [sent-757, score-0.504]
</p><p>91 We further believe that the eventual use case of our tools will be to assess individual patients that present at different times as anomalies relative to a continuously increasing data set of patients seen previously. [sent-765, score-0.912]
</p><p>92 In the context of cardiovascular disease, techniques such as heart rate variability, heart rate turbulence, T wave alternans, and morphologic variability have all been shown to be useful in risk stratifying patients at risk for future cardiovascular events following acute coronary syndromes. [sent-767, score-1.933]
</p><p>93 The focus of these methods is to calculate a speciﬁc pre-deﬁned feature from the raw ECG signal, and to use it to rank patients along a risk continuum. [sent-768, score-0.629]
</p><p>94 Our approach, focusing on detecting patients with high symbolic mismatch relative to other patients in the population, is orthogonal (and perhaps complementary) to the use of specialized high risk features. [sent-769, score-1.606]
</p><p>95 For the cardiovascular care, we only assume that ECG signals from patients who are at high risk differ from those of the rest of the population. [sent-771, score-0.821]
</p><p>96 Second, the ability to partition patients into groups with similar ECG characteristics and potentially common risk proﬁles potentially allows for a more ﬁne-grained understanding of a how a patient’s future health may evolve over time. [sent-773, score-0.662]
</p><p>97 Matching patients to past cases with similar ECG signals could lead to more accurate assignments of risk scores for particular events such as death and recurring heart attacks. [sent-774, score-0.999]
</p><p>98 This hypothesis, that is, of symbolic mismatch being useful in the setting of supervised learning, therefore needs to be evaluated more fully on larger patient populations. [sent-779, score-0.65]
</p><p>99 Risk stratiﬁcation for cardiac events after acute ST elevation myocardial infarction. [sent-794, score-0.549]
</p><p>100 Predictors of 90-day outcome in patients stabilized after acute coronary syndromes. [sent-1084, score-0.715]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('patients', 0.436), ('ecg', 0.336), ('symbolic', 0.3), ('cardiac', 0.205), ('mismatch', 0.202), ('heart', 0.196), ('risk', 0.193), ('strati', 0.175), ('coronary', 0.16), ('patient', 0.148), ('adverse', 0.143), ('cardiovascular', 0.14), ('acute', 0.119), ('hrt', 0.107), ('myocardial', 0.107), ('clinical', 0.106), ('acs', 0.094), ('beats', 0.092), ('uttag', 0.087), ('yed', 0.087), ('clipping', 0.085), ('neighbor', 0.084), ('ardiovascular', 0.08), ('tratification', 0.08), ('events', 0.071), ('isk', 0.068), ('angina', 0.068), ('symbolization', 0.067), ('survival', 0.061), ('anomaly', 0.061), ('eries', 0.061), ('beat', 0.061), ('hrv', 0.06), ('twa', 0.06), ('population', 0.059), ('spectrum', 0.058), ('hazard', 0.057), ('ime', 0.056), ('morphology', 0.056), ('chd', 0.053), ('copd', 0.053), ('depolarization', 0.053), ('qrs', 0.053), ('symbols', 0.053), ('signals', 0.052), ('death', 0.051), ('physiological', 0.051), ('alternans', 0.047), ('barthel', 0.047), ('elevation', 0.047), ('infarction', 0.047), ('turbulence', 0.047), ('sing', 0.047), ('nearest', 0.047), ('univariate', 0.045), ('clustering', 0.043), ('mi', 0.041), ('history', 0.041), ('dtw', 0.04), ('eskin', 0.04), ('hyperlipidemia', 0.04), ('hypertension', 0.04), ('keogh', 0.04), ('syed', 0.04), ('anomalies', 0.04), ('runtime', 0.039), ('dc', 0.039), ('specialized', 0.039), ('day', 0.038), ('centroids', 0.038), ('blood', 0.036), ('activity', 0.035), ('diabetes', 0.035), ('major', 0.035), ('period', 0.034), ('depression', 0.034), ('clip', 0.034), ('malik', 0.034), ('mellitus', 0.034), ('age', 0.034), ('disease', 0.034), ('admission', 0.033), ('arrhythmia', 0.033), ('cardiology', 0.033), ('necrosis', 0.033), ('repolarization', 0.033), ('routinely', 0.033), ('stratifying', 0.033), ('syndrome', 0.033), ('voltage', 0.033), ('metrics', 0.033), ('health', 0.033), ('outcomes', 0.032), ('unsupervised', 0.031), ('hr', 0.031), ('diagnosis', 0.03), ('st', 0.029), ('bauer', 0.028), ('wave', 0.028), ('variability', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000019 <a title="99-tfidf-1" href="./jmlr-2011-Unsupervised_Similarity-Based_Risk_Stratification_for_Cardiovascular_Events_Using_Long-Term_Time-Series_Data.html">99 jmlr-2011-Unsupervised Similarity-Based Risk Stratification for Cardiovascular Events Using Long-Term Time-Series Data</a></p>
<p>Author: Zeeshan Syed, John Guttag</p><p>Abstract: In medicine, one often bases decisions upon a comparative analysis of patient data. In this paper, we build upon this observation and describe similarity-based algorithms to risk stratify patients for major adverse cardiac events. We evolve the traditional approach of comparing patient data in two ways. First, we propose similarity-based algorithms that compare patients in terms of their long-term physiological monitoring data. Symbolic mismatch identiﬁes functional units in longterm signals and measures changes in the morphology and frequency of these units across patients. Second, we describe similarity-based algorithms that are unsupervised and do not require comparisons to patients with known outcomes for risk stratiﬁcation. This is achieved by using an anomaly detection framework to identify patients who are unlike other patients in a population and may potentially be at an elevated risk. We demonstrate the potential utility of our approach by showing how symbolic mismatch-based algorithms can be used to classify patients as being at high or low risk of major adverse cardiac events by comparing their long-term electrocardiograms to that of a large population. We describe how symbolic mismatch can be used in three different existing methods: one-class support vector machines, nearest neighbor analysis, and hierarchical clustering. When evaluated on a population of 686 patients with available long-term electrocardiographic data, symbolic mismatch-based comparative approaches were able to identify patients at roughly a two-fold increased risk of major adverse cardiac events in the 90 days following acute coronary syndrome. These results were consistent even after adjusting for other clinical risk variables. Keywords: risk stratiﬁcation, cardiovascular disease, time-series comparison, symbolic analysis, anomaly detection</p><p>2 0.11100575 <a title="99-tfidf-2" href="./jmlr-2011-Learning_Transformation_Models_for_Ranking_and_Survival_Analysis.html">56 jmlr-2011-Learning Transformation Models for Ranking and Survival Analysis</a></p>
<p>Author: Vanya Van Belle, Kristiaan Pelckmans, Johan A. K. Suykens, Sabine Van Huffel</p><p>Abstract: This paper studies the task of learning transformation models for ranking problems, ordinal regression and survival analysis. The present contribution describes a machine learning approach termed MINLIP . The key insight is to relate ranking criteria as the Area Under the Curve to monotone transformation functions. Consequently, the notion of a Lipschitz smoothness constant is found to be useful for complexity control for learning transformation models, much in a similar vein as the ’margin’ is for Support Vector Machines for classiﬁcation. The use of this model structure in the context of high dimensional data, as well as for estimating non-linear, and additive models based on primal-dual kernel machines, and for sparse models is indicated. Given n observations, the present method solves a quadratic program existing of O (n) constraints and O (n) unknowns, where most existing risk minimization approaches to ranking problems typically result in algorithms with O (n2 ) constraints or unknowns. We specify the MINLIP method for three different cases: the ﬁrst one concerns the preference learning problem. Secondly it is speciﬁed how to adapt the method to ordinal regression with a ﬁnite set of ordered outcomes. Finally, it is shown how the method can be used in the context of survival analysis where one models failure times, typically subject to censoring. The current approach is found to be particularly useful in this context as it can handle, in contrast with the standard statistical model for analyzing survival data, all types of censoring in a straightforward way, and because of the explicit relation with the Proportional Hazard and Accelerated Failure Time models. The advantage of the current method is illustrated on different benchmark data sets, as well as for estimating a model for cancer survival based on different micro-array and clinical data sets. Keywords: support vector machines, preference learning, ranking models, ordinal regression, survival analysis c</p><p>3 0.07236363 <a title="99-tfidf-3" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>Author: Shipeng Yu, Balaji Krishnapuram, Rómer Rosales, R. Bharat Rao</p><p>Abstract: Co-training (or more generally, co-regularization) has been a popular algorithm for semi-supervised learning in data with two feature representations (or views), but the fundamental assumptions underlying this type of models are still unclear. In this paper we propose a Bayesian undirected graphical model for co-training, or more generally for semi-supervised multi-view learning. This makes explicit the previously unstated assumptions of a large class of co-training type algorithms, and also clariﬁes the circumstances under which these assumptions fail. Building upon new insights from this model, we propose an improved method for co-training, which is a novel co-training kernel for Gaussian process classiﬁers. The resulting approach is convex and avoids local-maxima problems, and it can also automatically estimate how much each view should be trusted to accommodate noisy or unreliable views. The Bayesian co-training approach can also elegantly handle data samples with missing views, that is, some of the views are not available for some data points at learning time. This is further extended to an active sensing framework, in which the missing (sample, view) pairs are actively acquired to improve learning performance. The strength of active sensing model is that one actively sensed (sample, view) pair would improve the joint multi-view classiﬁcation on all the samples. Experiments on toy data and several real world data sets illustrate the beneﬁts of this approach. Keywords: co-training, multi-view learning, semi-supervised learning, Gaussian processes, undirected graphical models, active sensing</p><p>4 0.058227692 <a title="99-tfidf-4" href="./jmlr-2011-Information%2C_Divergence_and_Risk_for_Binary_Experiments.html">43 jmlr-2011-Information, Divergence and Risk for Binary Experiments</a></p>
<p>Author: Mark D. Reid, Robert C. Williamson</p><p>Abstract: We unify f -divergences, Bregman divergences, surrogate regret bounds, proper scoring rules, cost curves, ROC-curves and statistical information. We do this by systematically studying integral and variational representations of these objects and in so doing identify their representation primitives which all are related to cost-sensitive binary classiﬁcation. As well as developing relationships between generative and discriminative views of learning, the new machinery leads to tight and more general surrogate regret bounds and generalised Pinsker inequalities relating f -divergences to variational divergence. The new viewpoint also illuminates existing algorithms: it provides a new derivation of Support Vector Machines in terms of divergences and relates maximum mean discrepancy to Fisher linear discriminants. Keywords: classiﬁcation, loss functions, divergence, statistical information, regret bounds</p><p>5 0.050448753 <a title="99-tfidf-5" href="./jmlr-2011-Learning_a_Robust_Relevance_Model_for_Search_Using_Kernel_Methods.html">57 jmlr-2011-Learning a Robust Relevance Model for Search Using Kernel Methods</a></p>
<p>Author: Wei Wu, Jun Xu, Hang Li, Satoshi Oyama</p><p>Abstract: This paper points out that many search relevance models in information retrieval, such as the Vector Space Model, BM25 and Language Models for Information Retrieval, can be viewed as a similarity function between pairs of objects of different types, referred to as an S-function. An S-function is speciﬁcally deﬁned as the dot product between the images of two objects in a Hilbert space mapped from two different input spaces. One advantage of taking this view is that one can take a uniﬁed and principled approach to address the issues with regard to search relevance. The paper then proposes employing a kernel method to learn a robust relevance model as an S-function, which can effectively deal with the term mismatch problem, one of the biggest challenges in search. The kernel method exploits a positive semi-deﬁnite kernel referred to as an S-kernel. The paper shows that when using an S-kernel the model learned by the kernel method is guaranteed to be an S-function. The paper then gives more general principles for constructing S-kernels. A speciﬁc implementation of the kernel method is proposed using the Ranking SVM techniques and click-through data. The proposed approach is employed to learn a relevance model as an extension of BM25, referred to as Robust BM25. Experimental results on web search and enterprise search data show that Robust BM25 signiﬁcantly outperforms baseline methods and can successfully tackle the term mismatch problem. Keywords: search, term mismatch, kernel machines, similarity learning, s-function, s-kernel</p><p>6 0.049633853 <a title="99-tfidf-6" href="./jmlr-2011-Unsupervised_Supervised_Learning_II%3A_Margin-Based_Classification_Without_Labels.html">100 jmlr-2011-Unsupervised Supervised Learning II: Margin-Based Classification Without Labels</a></p>
<p>7 0.031955678 <a title="99-tfidf-7" href="./jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</a></p>
<p>8 0.030061737 <a title="99-tfidf-8" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<p>9 0.026922531 <a title="99-tfidf-9" href="./jmlr-2011-Anechoic_Blind_Source_Separation_Using_Wigner_Marginals.html">10 jmlr-2011-Anechoic Blind Source Separation Using Wigner Marginals</a></p>
<p>10 0.026859593 <a title="99-tfidf-10" href="./jmlr-2011-Producing_Power-Law_Distributions_and_Damping_Word_Frequencies_with_Two-Stage_Language_Models.html">78 jmlr-2011-Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models</a></p>
<p>11 0.025935568 <a title="99-tfidf-11" href="./jmlr-2011-Neyman-Pearson_Classification%2C_Convexity_and_Stochastic_Constraints.html">69 jmlr-2011-Neyman-Pearson Classification, Convexity and Stochastic Constraints</a></p>
<p>12 0.024276745 <a title="99-tfidf-12" href="./jmlr-2011-Weisfeiler-Lehman_Graph_Kernels.html">103 jmlr-2011-Weisfeiler-Lehman Graph Kernels</a></p>
<p>13 0.023586752 <a title="99-tfidf-13" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>14 0.021429179 <a title="99-tfidf-14" href="./jmlr-2011-Information_Rates_of_Nonparametric_Gaussian_Process_Methods.html">44 jmlr-2011-Information Rates of Nonparametric Gaussian Process Methods</a></p>
<p>15 0.020866968 <a title="99-tfidf-15" href="./jmlr-2011-Forest_Density_Estimation.html">35 jmlr-2011-Forest Density Estimation</a></p>
<p>16 0.020658392 <a title="99-tfidf-16" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>17 0.020067967 <a title="99-tfidf-17" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>18 0.019520093 <a title="99-tfidf-18" href="./jmlr-2011-CARP%3A_Software_for_Fishing_Out_Good_Clustering_Algorithms.html">15 jmlr-2011-CARP: Software for Fishing Out Good Clustering Algorithms</a></p>
<p>19 0.01941292 <a title="99-tfidf-19" href="./jmlr-2011-Efficient_and_Effective_Visual_Codebook_Generation_Using_Additive_Kernels.html">31 jmlr-2011-Efficient and Effective Visual Codebook Generation Using Additive Kernels</a></p>
<p>20 0.019229911 <a title="99-tfidf-20" href="./jmlr-2011-Multiple_Kernel_Learning_Algorithms.html">66 jmlr-2011-Multiple Kernel Learning Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.12), (1, -0.031), (2, -0.01), (3, -0.04), (4, -0.017), (5, -0.048), (6, -0.042), (7, -0.011), (8, -0.095), (9, 0.011), (10, -0.097), (11, 0.022), (12, 0.093), (13, -0.028), (14, -0.028), (15, 0.06), (16, -0.123), (17, 0.01), (18, 0.02), (19, -0.127), (20, -0.366), (21, 0.144), (22, 0.069), (23, -0.357), (24, 0.081), (25, -0.225), (26, 0.23), (27, 0.132), (28, -0.073), (29, 0.012), (30, -0.088), (31, -0.135), (32, 0.059), (33, -0.062), (34, -0.098), (35, 0.078), (36, -0.021), (37, -0.009), (38, 0.123), (39, 0.03), (40, -0.019), (41, 0.07), (42, -0.028), (43, 0.016), (44, -0.087), (45, -0.076), (46, -0.025), (47, -0.04), (48, -0.083), (49, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97924435 <a title="99-lsi-1" href="./jmlr-2011-Unsupervised_Similarity-Based_Risk_Stratification_for_Cardiovascular_Events_Using_Long-Term_Time-Series_Data.html">99 jmlr-2011-Unsupervised Similarity-Based Risk Stratification for Cardiovascular Events Using Long-Term Time-Series Data</a></p>
<p>Author: Zeeshan Syed, John Guttag</p><p>Abstract: In medicine, one often bases decisions upon a comparative analysis of patient data. In this paper, we build upon this observation and describe similarity-based algorithms to risk stratify patients for major adverse cardiac events. We evolve the traditional approach of comparing patient data in two ways. First, we propose similarity-based algorithms that compare patients in terms of their long-term physiological monitoring data. Symbolic mismatch identiﬁes functional units in longterm signals and measures changes in the morphology and frequency of these units across patients. Second, we describe similarity-based algorithms that are unsupervised and do not require comparisons to patients with known outcomes for risk stratiﬁcation. This is achieved by using an anomaly detection framework to identify patients who are unlike other patients in a population and may potentially be at an elevated risk. We demonstrate the potential utility of our approach by showing how symbolic mismatch-based algorithms can be used to classify patients as being at high or low risk of major adverse cardiac events by comparing their long-term electrocardiograms to that of a large population. We describe how symbolic mismatch can be used in three different existing methods: one-class support vector machines, nearest neighbor analysis, and hierarchical clustering. When evaluated on a population of 686 patients with available long-term electrocardiographic data, symbolic mismatch-based comparative approaches were able to identify patients at roughly a two-fold increased risk of major adverse cardiac events in the 90 days following acute coronary syndrome. These results were consistent even after adjusting for other clinical risk variables. Keywords: risk stratiﬁcation, cardiovascular disease, time-series comparison, symbolic analysis, anomaly detection</p><p>2 0.66105956 <a title="99-lsi-2" href="./jmlr-2011-Learning_Transformation_Models_for_Ranking_and_Survival_Analysis.html">56 jmlr-2011-Learning Transformation Models for Ranking and Survival Analysis</a></p>
<p>Author: Vanya Van Belle, Kristiaan Pelckmans, Johan A. K. Suykens, Sabine Van Huffel</p><p>Abstract: This paper studies the task of learning transformation models for ranking problems, ordinal regression and survival analysis. The present contribution describes a machine learning approach termed MINLIP . The key insight is to relate ranking criteria as the Area Under the Curve to monotone transformation functions. Consequently, the notion of a Lipschitz smoothness constant is found to be useful for complexity control for learning transformation models, much in a similar vein as the ’margin’ is for Support Vector Machines for classiﬁcation. The use of this model structure in the context of high dimensional data, as well as for estimating non-linear, and additive models based on primal-dual kernel machines, and for sparse models is indicated. Given n observations, the present method solves a quadratic program existing of O (n) constraints and O (n) unknowns, where most existing risk minimization approaches to ranking problems typically result in algorithms with O (n2 ) constraints or unknowns. We specify the MINLIP method for three different cases: the ﬁrst one concerns the preference learning problem. Secondly it is speciﬁed how to adapt the method to ordinal regression with a ﬁnite set of ordered outcomes. Finally, it is shown how the method can be used in the context of survival analysis where one models failure times, typically subject to censoring. The current approach is found to be particularly useful in this context as it can handle, in contrast with the standard statistical model for analyzing survival data, all types of censoring in a straightforward way, and because of the explicit relation with the Proportional Hazard and Accelerated Failure Time models. The advantage of the current method is illustrated on different benchmark data sets, as well as for estimating a model for cancer survival based on different micro-array and clinical data sets. Keywords: support vector machines, preference learning, ranking models, ordinal regression, survival analysis c</p><p>3 0.34835991 <a title="99-lsi-3" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>Author: Shipeng Yu, Balaji Krishnapuram, Rómer Rosales, R. Bharat Rao</p><p>Abstract: Co-training (or more generally, co-regularization) has been a popular algorithm for semi-supervised learning in data with two feature representations (or views), but the fundamental assumptions underlying this type of models are still unclear. In this paper we propose a Bayesian undirected graphical model for co-training, or more generally for semi-supervised multi-view learning. This makes explicit the previously unstated assumptions of a large class of co-training type algorithms, and also clariﬁes the circumstances under which these assumptions fail. Building upon new insights from this model, we propose an improved method for co-training, which is a novel co-training kernel for Gaussian process classiﬁers. The resulting approach is convex and avoids local-maxima problems, and it can also automatically estimate how much each view should be trusted to accommodate noisy or unreliable views. The Bayesian co-training approach can also elegantly handle data samples with missing views, that is, some of the views are not available for some data points at learning time. This is further extended to an active sensing framework, in which the missing (sample, view) pairs are actively acquired to improve learning performance. The strength of active sensing model is that one actively sensed (sample, view) pair would improve the joint multi-view classiﬁcation on all the samples. Experiments on toy data and several real world data sets illustrate the beneﬁts of this approach. Keywords: co-training, multi-view learning, semi-supervised learning, Gaussian processes, undirected graphical models, active sensing</p><p>4 0.29979512 <a title="99-lsi-4" href="./jmlr-2011-Information%2C_Divergence_and_Risk_for_Binary_Experiments.html">43 jmlr-2011-Information, Divergence and Risk for Binary Experiments</a></p>
<p>Author: Mark D. Reid, Robert C. Williamson</p><p>Abstract: We unify f -divergences, Bregman divergences, surrogate regret bounds, proper scoring rules, cost curves, ROC-curves and statistical information. We do this by systematically studying integral and variational representations of these objects and in so doing identify their representation primitives which all are related to cost-sensitive binary classiﬁcation. As well as developing relationships between generative and discriminative views of learning, the new machinery leads to tight and more general surrogate regret bounds and generalised Pinsker inequalities relating f -divergences to variational divergence. The new viewpoint also illuminates existing algorithms: it provides a new derivation of Support Vector Machines in terms of divergences and relates maximum mean discrepancy to Fisher linear discriminants. Keywords: classiﬁcation, loss functions, divergence, statistical information, regret bounds</p><p>5 0.20786026 <a title="99-lsi-5" href="./jmlr-2011-Learning_a_Robust_Relevance_Model_for_Search_Using_Kernel_Methods.html">57 jmlr-2011-Learning a Robust Relevance Model for Search Using Kernel Methods</a></p>
<p>Author: Wei Wu, Jun Xu, Hang Li, Satoshi Oyama</p><p>Abstract: This paper points out that many search relevance models in information retrieval, such as the Vector Space Model, BM25 and Language Models for Information Retrieval, can be viewed as a similarity function between pairs of objects of different types, referred to as an S-function. An S-function is speciﬁcally deﬁned as the dot product between the images of two objects in a Hilbert space mapped from two different input spaces. One advantage of taking this view is that one can take a uniﬁed and principled approach to address the issues with regard to search relevance. The paper then proposes employing a kernel method to learn a robust relevance model as an S-function, which can effectively deal with the term mismatch problem, one of the biggest challenges in search. The kernel method exploits a positive semi-deﬁnite kernel referred to as an S-kernel. The paper shows that when using an S-kernel the model learned by the kernel method is guaranteed to be an S-function. The paper then gives more general principles for constructing S-kernels. A speciﬁc implementation of the kernel method is proposed using the Ranking SVM techniques and click-through data. The proposed approach is employed to learn a relevance model as an extension of BM25, referred to as Robust BM25. Experimental results on web search and enterprise search data show that Robust BM25 signiﬁcantly outperforms baseline methods and can successfully tackle the term mismatch problem. Keywords: search, term mismatch, kernel machines, similarity learning, s-function, s-kernel</p><p>6 0.19503073 <a title="99-lsi-6" href="./jmlr-2011-Unsupervised_Supervised_Learning_II%3A_Margin-Based_Classification_Without_Labels.html">100 jmlr-2011-Unsupervised Supervised Learning II: Margin-Based Classification Without Labels</a></p>
<p>7 0.18267027 <a title="99-lsi-7" href="./jmlr-2011-Anechoic_Blind_Source_Separation_Using_Wigner_Marginals.html">10 jmlr-2011-Anechoic Blind Source Separation Using Wigner Marginals</a></p>
<p>8 0.13647218 <a title="99-lsi-8" href="./jmlr-2011-CARP%3A_Software_for_Fishing_Out_Good_Clustering_Algorithms.html">15 jmlr-2011-CARP: Software for Fishing Out Good Clustering Algorithms</a></p>
<p>9 0.12970451 <a title="99-lsi-9" href="./jmlr-2011-Efficient_and_Effective_Visual_Codebook_Generation_Using_Additive_Kernels.html">31 jmlr-2011-Efficient and Effective Visual Codebook Generation Using Additive Kernels</a></p>
<p>10 0.1283517 <a title="99-lsi-10" href="./jmlr-2011-Producing_Power-Law_Distributions_and_Damping_Word_Frequencies_with_Two-Stage_Language_Models.html">78 jmlr-2011-Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models</a></p>
<p>11 0.12756172 <a title="99-lsi-11" href="./jmlr-2011-Neyman-Pearson_Classification%2C_Convexity_and_Stochastic_Constraints.html">69 jmlr-2011-Neyman-Pearson Classification, Convexity and Stochastic Constraints</a></p>
<p>12 0.12407658 <a title="99-lsi-12" href="./jmlr-2011-Exploitation_of_Machine_Learning_Techniques_in_Modelling_Phrase_Movements_for_Machine_Translation.html">32 jmlr-2011-Exploitation of Machine Learning Techniques in Modelling Phrase Movements for Machine Translation</a></p>
<p>13 0.12261456 <a title="99-lsi-13" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>14 0.1191825 <a title="99-lsi-14" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<p>15 0.11110765 <a title="99-lsi-15" href="./jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</a></p>
<p>16 0.11083067 <a title="99-lsi-16" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<p>17 0.099316552 <a title="99-lsi-17" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>18 0.098761752 <a title="99-lsi-18" href="./jmlr-2011-Learning_from_Partial_Labels.html">58 jmlr-2011-Learning from Partial Labels</a></p>
<p>19 0.097856872 <a title="99-lsi-19" href="./jmlr-2011-Weisfeiler-Lehman_Graph_Kernels.html">103 jmlr-2011-Weisfeiler-Lehman Graph Kernels</a></p>
<p>20 0.094956473 <a title="99-lsi-20" href="./jmlr-2011-A_Family_of_Simple_Non-Parametric_Kernel_Learning_Algorithms.html">4 jmlr-2011-A Family of Simple Non-Parametric Kernel Learning Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.027), (5, 0.195), (6, 0.01), (9, 0.025), (10, 0.016), (13, 0.012), (16, 0.288), (24, 0.031), (31, 0.085), (32, 0.032), (36, 0.011), (41, 0.035), (60, 0.012), (71, 0.014), (73, 0.025), (78, 0.038), (90, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77414048 <a title="99-lda-1" href="./jmlr-2011-Unsupervised_Similarity-Based_Risk_Stratification_for_Cardiovascular_Events_Using_Long-Term_Time-Series_Data.html">99 jmlr-2011-Unsupervised Similarity-Based Risk Stratification for Cardiovascular Events Using Long-Term Time-Series Data</a></p>
<p>Author: Zeeshan Syed, John Guttag</p><p>Abstract: In medicine, one often bases decisions upon a comparative analysis of patient data. In this paper, we build upon this observation and describe similarity-based algorithms to risk stratify patients for major adverse cardiac events. We evolve the traditional approach of comparing patient data in two ways. First, we propose similarity-based algorithms that compare patients in terms of their long-term physiological monitoring data. Symbolic mismatch identiﬁes functional units in longterm signals and measures changes in the morphology and frequency of these units across patients. Second, we describe similarity-based algorithms that are unsupervised and do not require comparisons to patients with known outcomes for risk stratiﬁcation. This is achieved by using an anomaly detection framework to identify patients who are unlike other patients in a population and may potentially be at an elevated risk. We demonstrate the potential utility of our approach by showing how symbolic mismatch-based algorithms can be used to classify patients as being at high or low risk of major adverse cardiac events by comparing their long-term electrocardiograms to that of a large population. We describe how symbolic mismatch can be used in three different existing methods: one-class support vector machines, nearest neighbor analysis, and hierarchical clustering. When evaluated on a population of 686 patients with available long-term electrocardiographic data, symbolic mismatch-based comparative approaches were able to identify patients at roughly a two-fold increased risk of major adverse cardiac events in the 90 days following acute coronary syndrome. These results were consistent even after adjusting for other clinical risk variables. Keywords: risk stratiﬁcation, cardiovascular disease, time-series comparison, symbolic analysis, anomaly detection</p><p>2 0.30449796 <a title="99-lda-2" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<p>Author: Graham W. Taylor, Geoffrey E. Hinton, Sam T. Roweis</p><p>Abstract: In this paper we develop a class of nonlinear generative models for high-dimensional time series. We ﬁrst propose a model based on the restricted Boltzmann machine (RBM) that uses an undirected model with binary latent variables and real-valued “visible” variables. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. This “conditional” RBM (CRBM) makes on-line inference efﬁcient and allows us to use a simple approximate learning procedure. We demonstrate the power of our approach by synthesizing various sequences from a model trained on motion capture data and by performing on-line ﬁlling in of data lost during capture. We extend the CRBM in a way that preserves its most important computational properties and introduces multiplicative three-way interactions that allow the effective interaction weight between two variables to be modulated by the dynamic state of a third variable. We introduce a factoring of the implied three-way weight tensor to permit a more compact parameterization. The resulting model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve its ability to blend motion styles or to transition smoothly among them. Videos and source code can be found at http://www.cs.nyu.edu/˜gwtaylor/publications/ jmlr2011. Keywords: unsupervised learning, restricted Boltzmann machines, time series, generative models, motion capture</p><p>3 0.29335254 <a title="99-lda-3" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>Author: Ricardo Henao, Ole Winther</p><p>Abstract: In this paper we consider sparse and identiﬁable linear latent variable (factor) and linear Bayesian network models for parsimonious analysis of multivariate data. We propose a computationally efﬁcient method for joint parameter and model inference, and model comparison. It consists of a fully Bayesian hierarchy for sparse models using slab and spike priors (two-component δ-function and continuous mixtures), non-Gaussian latent factors and a stochastic search over the ordering of the variables. The framework, which we call SLIM (Sparse Linear Identiﬁable Multivariate modeling), is validated and bench-marked on artiﬁcial and real biological data sets. SLIM is closest in spirit to LiNGAM (Shimizu et al., 2006), but differs substantially in inference, Bayesian network structure learning and model comparison. Experimentally, SLIM performs equally well or better than LiNGAM with comparable computational complexity. We attribute this mainly to the stochastic search strategy used, and to parsimony (sparsity and identiﬁability), which is an explicit part of the model. We propose two extensions to the basic i.i.d. linear framework: non-linear dependence on observed variables, called SNIM (Sparse Non-linear Identiﬁable Multivariate modeling) and allowing for correlations between latent variables, called CSLIM (Correlated SLIM), for the temporal and/or spatial data. The source code and scripts are available from http://cogsys.imm.dtu.dk/slim/. Keywords: parsimony, sparsity, identiﬁability, factor models, linear Bayesian networks</p><p>4 0.29303828 <a title="99-lda-4" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>Author: Jennifer Gillenwater, Kuzman Ganchev, João Graça, Fernando Pereira, Ben Taskar</p><p>Abstract: A strong inductive bias is essential in unsupervised grammar induction. In this paper, we explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. We use part-of-speech (POS) tags to group dependencies by parent-child types and investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In experiments with 12 different languages, we achieve signiﬁcant gains in directed attachment accuracy over the standard expectation maximization (EM) baseline, with an average accuracy improvement of 6.5%, outperforming EM by at least 1% for 9 out of 12 languages. Furthermore, the new method outperforms models based on standard Bayesian sparsity-inducing parameter priors with an average improvement of 5% and positive gains of at least 1% for 9 out of 12 languages. On English text in particular, we show that our approach improves performance over other state-of-the-art techniques.</p><p>5 0.292476 <a title="99-lda-5" href="./jmlr-2011-Operator_Norm_Convergence_of_Spectral_Clustering_on_Level_Sets.html">74 jmlr-2011-Operator Norm Convergence of Spectral Clustering on Level Sets</a></p>
<p>Author: Bruno Pelletier, Pierre Pudlo</p><p>Abstract: Following Hartigan (1975), a cluster is deﬁned as a connected component of the t-level set of the underlying density, that is, the set of points for which the density is greater than t. A clustering algorithm which combines a density estimate with spectral clustering techniques is proposed. Our algorithm is composed of two steps. First, a nonparametric density estimate is used to extract the data points for which the estimated density takes a value greater than t. Next, the extracted points are clustered based on the eigenvectors of a graph Laplacian matrix. Under mild assumptions, we prove the almost sure convergence in operator norm of the empirical graph Laplacian operator associated with the algorithm. Furthermore, we give the typical behavior of the representation of the data set into the feature space, which establishes the strong consistency of our proposed algorithm. Keywords: spectral clustering, graph, unsupervised classiﬁcation, level sets, connected components</p><p>6 0.29136965 <a title="99-lda-6" href="./jmlr-2011-Training_SVMs_Without_Offset.html">95 jmlr-2011-Training SVMs Without Offset</a></p>
<p>7 0.29041067 <a title="99-lda-7" href="./jmlr-2011-Variable_Sparsity_Kernel_Learning.html">101 jmlr-2011-Variable Sparsity Kernel Learning</a></p>
<p>8 0.28961343 <a title="99-lda-8" href="./jmlr-2011-Information%2C_Divergence_and_Risk_for_Binary_Experiments.html">43 jmlr-2011-Information, Divergence and Risk for Binary Experiments</a></p>
<p>9 0.28898829 <a title="99-lda-9" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>10 0.28646713 <a title="99-lda-10" href="./jmlr-2011-A_Family_of_Simple_Non-Parametric_Kernel_Learning_Algorithms.html">4 jmlr-2011-A Family of Simple Non-Parametric Kernel Learning Algorithms</a></p>
<p>11 0.28575829 <a title="99-lda-11" href="./jmlr-2011-Distance_Dependent_Chinese_Restaurant_Processes.html">26 jmlr-2011-Distance Dependent Chinese Restaurant Processes</a></p>
<p>12 0.28495675 <a title="99-lda-12" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>13 0.28464645 <a title="99-lda-13" href="./jmlr-2011-Semi-Supervised_Learning_with_Measure_Propagation.html">84 jmlr-2011-Semi-Supervised Learning with Measure Propagation</a></p>
<p>14 0.28324422 <a title="99-lda-14" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>15 0.28203261 <a title="99-lda-15" href="./jmlr-2011-Computationally_Efficient_Convolved_Multiple_Output_Gaussian_Processes.html">17 jmlr-2011-Computationally Efficient Convolved Multiple Output Gaussian Processes</a></p>
<p>16 0.28114963 <a title="99-lda-16" href="./jmlr-2011-Minimum_Description_Length_Penalization_for_Group_and_Multi-Task_Sparse_Learning.html">64 jmlr-2011-Minimum Description Length Penalization for Group and Multi-Task Sparse Learning</a></p>
<p>17 0.27980998 <a title="99-lda-17" href="./jmlr-2011-Logistic_Stick-Breaking_Process.html">61 jmlr-2011-Logistic Stick-Breaking Process</a></p>
<p>18 0.27964967 <a title="99-lda-18" href="./jmlr-2011-Neyman-Pearson_Classification%2C_Convexity_and_Stochastic_Constraints.html">69 jmlr-2011-Neyman-Pearson Classification, Convexity and Stochastic Constraints</a></p>
<p>19 0.2793372 <a title="99-lda-19" href="./jmlr-2011-The_Sample_Complexity_of_Dictionary_Learning.html">91 jmlr-2011-The Sample Complexity of Dictionary Learning</a></p>
<p>20 0.27907985 <a title="99-lda-20" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
