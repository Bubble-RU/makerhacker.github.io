<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>41 jmlr-2011-Improved Moves for Truncated Convex Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-41" href="#">jmlr2011-41</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>41 jmlr-2011-Improved Moves for Truncated Convex Models</h1>
<br/><p>Source: <a title="jmlr-2011-41-pdf" href="http://jmlr.org/papers/volume12/kumar11a/kumar11a.pdf">pdf</a></p><p>Author: M. Pawan Kumar, Olga Veksler, Philip H.S. Torr</p><p>Abstract: We consider the problem of obtaining an approximate maximum a posteriori estimate of a discrete random ﬁeld characterized by pairwise potentials that form a truncated convex model. For this problem, we propose two st-MINCUT based move making algorithms that we call Range Swap and Range Expansion. Our algorithms can be thought of as extensions of αβ-Swap and α-Expansion respectively that fully exploit the form of the pairwise potentials. Speciﬁcally, instead of dealing with one or two labels at each iteration, our methods explore a large search space by considering a range of labels (that is, an interval of consecutive labels). Furthermore, we show that Range Expansion provides the same multiplicative bounds as the standard linear programming (LP) relaxation in polynomial time. Compared to previous approaches based on the LP relaxation, for example interior-point algorithms or tree-reweighted message passing (TRW), our methods are faster as they use only the efﬁcient st-MINCUT algorithm in their design. We demonstrate the usefulness of the proposed approaches on both synthetic and standard real data problems. Keywords: truncated convex models, move making algorithms, range moves, multiplicative bounds, linear programming relaxation</p><p>Reference: <a title="jmlr-2011-41-reference" href="../jmlr2011_reference/jmlr-2011-Improved_Moves_for_Truncated_Convex_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 (b)-(c) Two examples of truncated convex potentials that will be of interest to us in this work: truncated linear metric (b) and truncated quadratic semi-metric (c). [sent-31, score-0.601]
</p><p>2 It is common practice in computer vision to specify an energy function with arbitrary unary potentials and truncated convex pairwise potentials (Boykov et al. [sent-32, score-0.733]
</p><p>3 (a,b)∈E  Here, θa ( f (a)) denotes unary potentials and θab ( f (a), f (b)) denotes pairwise potentials, that is, θa ( f (a)) is the cost of assigning label l f (a) to variable va and θab ( f (a), f (b)) is the cost of assigning labels l f (a) and l f (b) to variables va and vb respectively. [sent-127, score-0.867]
</p><p>4 Formally speaking, the pairwise potentials are of the form θab ( f (a), f (b)) = wab min{d( f (a) − f (b)), M}, 35  K UMAR , V EKSLER AND T ORR  where wab ≥ 0 for all (a, b) ∈ E , d(·) is a convex function and M > 0 is the truncation factor. [sent-131, score-0.943]
</p><p>5 Examples of pairwise potentials of this form include the truncated linear metric and the truncated quadratic semi-metric, that is, θab ( f (a), f (b)) = wab min{| f (a) − f (b)|, M},  θab ( f (a), f (b)) = wab min{( f (a) − f (b))2 , M}. [sent-135, score-1.186]
</p><p>6 Formally, let f be the labeling obtained by an algorithm A (randomized or deterministic) for an instance of the MAP estimation problem belonging to a particular class (in our case when the pairwise potentials form a truncated convex model). [sent-140, score-0.577]
</p><p>7 The pairwise potential wab min{d(i − j), M} of assigning labels li and l j to neighboring random variables va and vb respectively. [sent-169, score-0.721]
</p><p>8 Deﬁnition 2: A labeling fˆ is said to be a local minimum over smooth labelings if the energy cannot be reduced further by changing the labels of any subset of random variables, say deﬁned by S, such that the new labeling f is smooth with respect to S. [sent-197, score-0.656]
</p><p>9 Note that this labeling is smooth since we can ﬁnd a path from va to vc via vb such that the edges in the path lie in the convex part. [sent-205, score-0.57]
</p><p>10 Iteration • Set im = 0 (where im indexes the interval to be used). [sent-218, score-1.188]
</p><p>11 • While im < h — Deﬁne interval Im = [im + 1, jm ] where jm = min{im + L, h − 1} and d(L) ≥ M. [sent-219, score-1.093]
</p><p>12 — Move from current labeling fm to a new labeling fm+1 using st-MINCUT such that (i) if fm+1 (a) = fm (a) then fm+1 (a) ∈ Im , (ii) Q( fm+1 , D; θ) ≤ Q( fm , D; θ). [sent-220, score-1.547]
</p><p>13 At an iteration m, the Range Swap algorithm only considers the random variables va whose current labeling fm (a) lies in the interval Im = [im + 1, jm ] of length L. [sent-230, score-1.092]
</p><p>14 In what follows, we will assume that jm = im + L instead of jm = min{im + L, h − 1}. [sent-236, score-1.055]
</p><p>15 The new labeling fm+1 is obtained by constructing a graph such that every st-cut on the graph corresponds to a labeling f of the random variables that satisﬁes: f (a) ∈ Im , ∀va ∈ v(Sm ),  f (a) = fm (a), ∀va ∈ v − v(Sm ). [sent-240, score-0.859]
</p><p>16 At each iteration of our algorithm, we are given an interval Im = [im + 1, jm ] of L labels (that is, jm = im + L) where d(L) = M. [sent-248, score-1.135]
</p><p>17 We also have the current labeling fm for all the random variables. [sent-249, score-0.585]
</p><p>18 We construct a directed weighted graph (with non-negative weights) Gm = {Vm , Em , cm (·, ·)} such that for each va ∈ v(Sm ), we deﬁne vertices {aim +1 , aim +2 , · · · , a jm } ∈ Vm . [sent-250, score-0.608]
</p><p>19 Note that all other potentials that specify the energy of the labeling are ﬁxed during the iteration. [sent-253, score-0.483]
</p><p>20 1 R EPRESENTING U NARY P OTENTIALS For all random variables va ∈ v(Sm ), we deﬁne the following edges that belong to the set Em : • For all k ∈ [im + 1, jm ), edges (ak , ak+1 ) have capacity cm (ak , ak+1 ) = θa (k), that is, the cost of assigning label lk to variable va . [sent-256, score-1.009]
</p><p>21 • For all k ∈ [im + 1, jm ), edges (ak+1 , ak ) have capacity cm (ak+1 , ak ) = ∞. [sent-257, score-0.571]
</p><p>22 40  I MPROVED M OVES FOR T RUNCATED C ONVEX M ODELS  • Edges (a jm ,t) have capacity cm (a jm ,t) = θa ( jm ). [sent-258, score-0.849]
</p><p>23 • Edges (t, a jm ) have capacity cm (t, a jm ) = ∞. [sent-259, score-0.609]
</p><p>24 We interpret a ﬁnite cost st-cut as a relabeling of the random variables as follows: f (a) =  k if st-cut includes edge (ak , ak+1 ) where k ∈ [im + 1, jm ), jm if st-cut includes edge (a jm ,t). [sent-267, score-0.785]
</p><p>25 (3)  Note that the sum of the unary potentials for the labeling f is exactly equal to the cost of the st-cut over the edges deﬁned above. [sent-268, score-0.554]
</p><p>26 Since fm+1 (b) is ﬁxed to fm (b), the pairwise potential θab (i, fm+1 (b)) = θab (i, fm (b)) can be effectively treated as a unary potential of va . [sent-275, score-1.164]
</p><p>27 Hence, similar to unary potentials, it can be formulated using the following edge in set Em : • For all k ∈ [im + 1, jm ), edges (ak , ak+1 ) have capacity cm (ak , ak+1 ) = θab (k, fm (b)), that is, the cost of assigning label lk to variable va and keeping the label of vb ﬁxed to fm (b). [sent-276, score-1.632]
</p><p>28 • For all k ∈ [im + 1, jm ), edges (ak+1 , ak ) have capacity cm (ak+1 , ak ) = ∞. [sent-277, score-0.571]
</p><p>29 • Edges (a jm ,t) have capacity cm (a jm ,t) = θab ( jm , fm (b)). [sent-278, score-1.226]
</p><p>30 • Edges (t, a jm ) have capacity cm (t, a jm ) = ∞. [sent-279, score-0.609]
</p><p>31 41  K UMAR , V EKSLER AND T ORR  Figure 3: Edges that are used to represent the pairwise potentials of two neighboring random variables va and vb such that (a, b) ∈ A (Sm ) are shown. [sent-282, score-0.486]
</p><p>32 3 R EPRESENTING PAIRWISE P OTENTIALS  WITH  N O F IXED VARIABLES  For all random variables va and vb such that (a, b) ∈ A (Sm ), we deﬁne edges (ak , bk′ ) ∈ Em where either one or both of k and k′ belong to the set (im + 1, jm ] (that is, at least one of them is not im + 1). [sent-287, score-1.133]
</p><p>33 The capacity of these edges is given by cm (ak , bk′ ) =  wab d(k − k′ + 1) − 2d(k − k′ ) + d(k − k′ − 1) . [sent-288, score-0.542]
</p><p>34 Lemma 1: For the capacities deﬁned in Equations (4) and (5), the cost of the st-cut which includes the edges (ak , ak+1 ) and (bk′ , bk′ +1 ) (that is, va and vb take labels lk and lk′ respectively) is given by wab d(k − k′ ) + κab , where the constant κab = wab d(L) (Proof in Appendix B). [sent-297, score-1.101]
</p><p>35 In this case, we deﬁne the set Sm such that Sm = {a| fm (a) ∈ Im , d( fm (a), fm (b)) ≤ M, ∀(a, b) ∈ E , fm (b) ∈ Im }. [sent-305, score-1.508]
</p><p>36 In other words, Sm consists of those random variables whose current label belongs to the interval Im and whose pairwise potential with all its neighboring random variables vb such that fm (b) ∈ Im lies in the convex part of the truncated convex model. [sent-306, score-0.757]
</p><p>37 Property 2: For (a, b) ∈ B1 (Sm ), the cost of the st-cut exactly represents the pairwise potential θab ( f (a), fm (b)). [sent-314, score-0.49]
</p><p>38 Similarly, for (a, b) ∈ B2 (Sm ), the cost of the st-cut exactly represents the pairwise potential θab ( fm (a), f (b)). [sent-315, score-0.49]
</p><p>39 Property 3: For (a, b) ∈ A (Sm ), if f (a) ∈ Im and f (b) ∈ Im such that d( f (a) − f (b)) ≤ M, 43  K UMAR , V EKSLER AND T ORR  then the cost of the st-cut exactly represents the pairwise potential θab ( f (a), f (b)) plus a constant κab , that is, wab d( f (a) − f (b)) + κab . [sent-316, score-0.452]
</p><p>40 This follows from the fact that our graph construction overestimates the truncation part by the convex function wab d(·). [sent-319, score-0.461]
</p><p>41 Since the potentials are either modeled exactly or are overestimated, it follows that the energy of the labeling fm+1 is less than or equal to the cost of the st-MINCUT on Gm . [sent-322, score-0.512]
</p><p>42 We show that this interval provides a labeling that is at least as good as the labeling obtained by considering any of its subsets for which the optimal move can be computed. [sent-331, score-0.504]
</p><p>43 Formally, let fm+1 be the labeling obtained by using ′ an interval of length L such that d(L) > M and let fm+1 be the labeling obtained by using a subset of the interval of length L′ such that d(L′ ) = M. [sent-332, score-0.492]
</p><p>44 2), it is worth noting that the corresponding graph construction ensures that the cut corresponding to the labeling fm exactly models the energy Q( fm , D; θ) up to a constant. [sent-345, score-1.149]
</p><p>45 This implies that the energy of the new labeling fm+1 is less than or equal to the energy of fm , that is, Q( fm+1 , D; θ) ≤ Q( fm , D; θ). [sent-346, score-1.204]
</p><p>46 This follows from the fact that the cost of the st-MINCUT is less than or equal to the energy of the labeling fm but is greater than or equal to the energy of fm+1 . [sent-347, score-0.856]
</p><p>47 It is worth noting that, unlike previous move making algorithms, Range Swap is not guaranteed to compute the optimal move other than in the special case when d(L) = M (where L = jm − im is the length of the interval). [sent-349, score-0.915]
</p><p>48 In other words, for the case where d(L) > M, if in the mth iteration we ′ move from label fm to fm+1 then it is possible that there exists another labeling fm+1 such that ′ Q( fm+1 , D; θ) ≤ Q( fm+1 , D; θ), ′ fm+1 (a) ∈ Im , ∀va ∈ v(Sm ),  ′ fm+1 (a) = fm (a), ∀va ∈ v − v(Sm ). [sent-350, score-1.047]
</p><p>49 Unlike Range Swap, at an iteration m it considers all the random variables va regardless of whether their current labeling fm (a) lies in the interval Im . [sent-356, score-0.852]
</p><p>50 It provides the option for each random variable va to either retain its old label fm (a) or change its label to fm+1 (a) ∈ Im . [sent-357, score-0.631]
</p><p>51 Formally, the Range Expansion algorithm moves from labeling fm to fm+1 such that Q( fm+1 , D; θ) ≤ Q( fm , D; θ),  fm+1 (a) = fm (a) OR fm+1 (a) ∈ Im , ∀va ∈ v. [sent-358, score-1.358]
</p><p>52 In other words, if in the mth iteration we move from label fm to fm+1 then it is possible that there ′ exists another labeling fm+1 such that ′ Q( fm+1 , D; θ) < Q( fm+1 , D; θ), ′ ′ fm+1 (a) = fm (a) OR fm+1 (a) ∈ Im , ∀va ∈ v. [sent-360, score-1.047]
</p><p>53 As in the 45  K UMAR , V EKSLER AND T ORR  case of Range Swap, we move from labeling fm to fm+1 by constructing a graph such that every st-cut on the graph corresponds to a labeling f of the random variables that satisﬁes: f (a) = fm (a) OR f (a) ∈ Im , ∀va ∈ v. [sent-363, score-1.286]
</p><p>54 The new labeling fm+1 is obtained in two steps: (i) we obtain a labeling f that corresponds to the st-MINCUT on our graph; and (ii) we choose the new labeling fm+1 as fm+1 =  f if Q( f , D; θ) ≤ Q( fm , D; θ), fm otherwise. [sent-364, score-1.378]
</p><p>55 (6)  Note that, unlike Range Swap, step (ii) is required in Range Expansion since the labeling f obtained in step (i) may have greater energy than fm . [sent-365, score-0.706]
</p><p>56 1 Graph Construction We construct a directed weighted graph (with non-negative weights) Gm = {Vm , Em , cm (·, ·)} such that Vm contains the source s, the sink t and the vertices {aim +1 , aim +2 , · · · , a jm } for each random variable va ∈ v. [sent-368, score-0.625]
</p><p>57 The edges e ∈ Em with capacity cm (e) are of two types: (i) those that represent the unary potentials of a labeling corresponding to an st-cut in the graph and; (ii) those that represent the pairwise potentials of the labeling. [sent-369, score-0.902]
</p><p>58 To this end, we change the capacity of the edge (s, aim +1 ) to cm (s, aim +1 ) =  θa ( fm (a)) if ∞ otherwise. [sent-377, score-0.592]
</p><p>59 We interpret a ﬁnite cost st-cut as a relabeling of the random variables as follows:  if st-cut includes edge (ak , ak+1 ) where k ∈ [im + 1, jm ),  k jm if st-cut includes edge (a jm ,t), f (a) = (8)  fm (a) if st-cut includes edge (s, aim +1 ). [sent-380, score-1.214]
</p><p>60 46  I MPROVED M OVES FOR T RUNCATED C ONVEX M ODELS  Note that the sum of the unary potentials for the labeling f is exactly equal to the cost of the st-cut over the edges deﬁned above. [sent-381, score-0.554]
</p><p>61 In order to model these cases, we incorporate the following additional edges: • If fm (a) ∈ Im and fm (b) ∈ Im then we add an edge (aim +1 , bim +1 ) with capacity wab M + κab /2 / (see Fig. [sent-395, score-1.216]
</p><p>62 • If fm (a) ∈ Im and fm (b) ∈ Im then we add an edge (bim +1 , aim +1 ) with capacity wab M + κab /2 / (see Fig. [sent-397, score-1.201]
</p><p>63 • If fm (a) ∈ Im and fm (b) ∈ Im , we introduce a new vertex pab . [sent-399, score-0.834]
</p><p>64 5(c)): cm (aim +1 , pab ) = cm (pab , aim +1 ) = wab M + κab /2, cm (bim +1 , pab ) = cm (pab , bim +1 ) = wab M + κab /2, cm (s, pab ) = θab ( fm (a), fm (b)) + κab . [sent-401, score-2.12]
</p><p>65 Property 5: The cost of the st-cut exactly represents the sum of the unary potentials associated with the corresponding labeling f , that is, ∑va ∈v θa ( f (a)). [sent-410, score-0.48]
</p><p>66 Property 6: For (a, b) ∈ E , if f (a) = fm (a) ∈ Im and f (b) = fm (b) ∈ Im then the cost of the st-cut / / exactly represents the pairwise potential θab ( f (a), f (b)) plus a constant κab . [sent-411, score-0.867]
</p><p>67 This is due to the fact that the st-cut contains the edge (s, pab ) whose capacity is θab ( fm (a), fm (b)) + κab . [sent-412, score-0.908]
</p><p>68 This follows from the fact that our graph construction overestimates the truncation part by the convex function wab d(·) (see Lemma 1). [sent-418, score-0.461]
</p><p>69 and d(x) Similarly, if f (a) = fm (a) ∈ Im and f (b) ∈ Im then the cost of the st-cut incorrectly represents / the pairwise potential θab ( f (a), f (b)), being ˆ wab d( f (b) − (im + 1)) + wab d( f (b) − (im + 1)) + wab M + κab . [sent-423, score-1.507]
</p><p>70 In other words, the energy of the labeling f , and hence the energy of fm+1 , is less than or equal to the cost of the st-MINCUT on Gm . [sent-429, score-0.479]
</p><p>71 Clearly, the following equation holds true:  ∑ θa ( f ∗ (a)) = ∑  va ∈v  ∑  θa ( f ∗ (a)),  (11)  Im ∈Ir va ∈v( f ∗ ,Im )  since f ∗ (a) belongs to exactly one interval in Ir for all va ∈ v. [sent-451, score-0.68]
</p><p>72 ab ˆ • For (a, b) ∈ B1 ( f ∗ , Im ), we denote wab d( f ∗ (a) − (im + 1)) + wab d( f ∗ (a) − (im + 1)) + wab M m. [sent-453, score-1.206]
</p><p>73 by ea ˆ • For (a, b) ∈ B2 ( f ∗ , Im ), we denote wab d( f ∗ (b) − (im + 1)) + wab d( f ∗ (b) − (im + 1)) + wab M m. [sent-454, score-1.017]
</p><p>74 In other words,  ∑  ≤  ∑  θa ( f (a)) +  ∑  θab ( f (a), f (b))  (a,b)∈A ( f ∗ ,Im ) B ( f ∗ ,Im )  va ∈v( f ∗ ,Im )  θa ( f ∗ (a)) +  ∑  em + ab  em + a  ∑  (a,b)∈B2 ( f ∗ ,Im )  (a,b)∈B1 ( f ∗ ,Im )  (a,b)∈A ( f ∗ ,Im )  va ∈v( f ∗ ,Im )  ∑  em , ∀Im . [sent-460, score-0.875]
</p><p>75 Furthermore, using Equation (11), the summation of the above inequality can be written as Q( f , D; θ) ≤  ∑  Im ∈Ir  ∑ θa ( f ∗ (a)) +  va ∈v  ∑  (a,b)∈A ( f ∗ ,Im )  ∑  em + ab  (a,b)∈B1 ( f ∗ ,Im )  50  em + a  ∑  (a,b)∈B2 ( f ∗ ,Im )  em . [sent-463, score-0.661]
</p><p>76 Hence, we get Q( f , D; θ) ≤ 1 ∑ L ∑ Im ∈Ir r  ∑ θa ( f ∗ (a)) +  va ∈v  ∑  ∑  em + ab  (a,b)∈A ( f ∗ ,Im )  ∑  em + a  (a,b)∈B1 ( f ∗ ,Im )  em . [sent-466, score-0.661]
</p><p>77 Speciﬁcally, the unary potentials θa (i) were sampled uniformly from the interval [0, 10] while the weights wab , which determine the pairwise potentials, were sampled uniformly from [0, 5]. [sent-502, score-0.681]
</p><p>78 The Range Swap algorithm guarantees that at each iteration the energy of the new labeling obtained by the st-MINCUT algorithm is less than or equal to the energy of the previous labeling. [sent-555, score-0.465]
</p><p>79 labeling may have a higher energy than the previous labeling (in which case the new labeling is discarded and the previous labeling is retained). [sent-602, score-0.953]
</p><p>80 By canceling out the common terms, we see that  ∑  ∑  ≤  ∑  ≤  ∑  θa ( f ∗ (a)) +  θab ( fi (a), fi (b))  ∑  θa ( fi (a)) +  va ∈v(Si )  θab ( fˆ(a), fˆ(b))  ∑  θa ( fˆ(a)) +  va ∈v(Si )  θab ( f ∗ (a), f ∗ (b)). [sent-644, score-0.47]
</p><p>81 Proof of Lemma 1 Lemma 1: For the capacities deﬁned in Equations (4) and (5), the cost of the st-cut which includes the edges (ak , ak+1 ) and (bk′ , bk′ +1 ) (that is, va and vb take labels lk and lk′ respectively) is given by wab d(k − k′ ) + κab , where the constant κab = wab d(L). [sent-650, score-1.101]
</p><p>82 +d(i′ − jm + 2) − 2d(i′ − jm + 1) + d(i′ − jm )  +d(i′ − jm + 1) − 2d(i′ − jm ) + d(i′ − jm − 1)  d(i′ − k′ ) − d(i′ − k′ − 1) − d(i′ − jm ) + d(i′ − jm + 1). [sent-656, score-1.92]
</p><p>83 =  (14)  Hence, it follows that jm  k  ∑  ∑  =  d(i′ − j′ + 1) − 2d(i′ − j′ ) + d(i′ − j′ − 1)  i′ =im +1 j′ =k′ +1 d(im + 1 − k′ ) − d(im − k′ ) − d(im − jm + 1) + d(im − jm ) +d(im + 2 − k′ ) − d(im + 1 − k′ ) − d(im − jm + 2) + d(im − jm + 1)  . [sent-657, score-1.2]
</p><p>84 +d(k − k′ − 1) − d(k − k′ − 2) − d(k − jm − 1) + d(k − jm − 2)  = =  +d(k − k′ ) − d(k − k′ − 1) − d(k − jm ) + d(k − jm − 1)  d(k − k′ ) − d( jm − k) − d(k′ − im ) + d( jm − im ) d(k − k′ ) − d(L − k + im ) − d(im − k′ ) + d(L),  (15)  where the last expression holds because L = jm − im . [sent-660, score-3.98]
</p><p>85 Similarly, it can be shown that jm  ∑  =  k′  ∑  i′ =k+1 j′ =im +1 ′  d(i′ − j′ + 1) − 2d(i′ − j′ ) + d(i′ − j′ − 1)  d(k − k ) − d(L − k′ + im ) − d(im − k) + d(L). [sent-662, score-0.815]
</p><p>86 (a)  (b)  Figure 8: The st-cut (the dashed curve between the two sets of nodes {aim +1 , · · · , a jm } and {bim +1 , · · · , b jm }; shown in red if viewed in color) that assigns f (a) ∈ Im and f (b) ∈ Im . [sent-665, score-0.48]
</p><p>87 59  K UMAR , V EKSLER AND T ORR  There are two possible cases to be considered: (i) fm (a) ∈ Im ; and (ii) fm (a) ∈ Im . [sent-676, score-0.754]
</p><p>88 8(a)) (a f (a) , a f (a)+1 ) ∪ {(ai′ , b j′ ), im + 2 ≤ i′ ≤ f (a), im + 1 ≤ j′ ≤ jm } ∪{(aim +1 , b j′ ), im + 2 ≤ j′ ≤ jm } ∪ (aim +1 , bim +1 ). [sent-678, score-2.254]
</p><p>89 8(b)) (a f (a) , a f (a)+1 ) ∪ {(ai′ , b j′ ), im + 2 ≤ i′ ≤ f (a), im + 1 ≤ j′ ≤ jm } ∪{(aim +1 , b j′ ), im + 2 ≤ j′ ≤ jm } ∪ (pab , bim +1 ). [sent-680, score-2.254]
</p><p>90 However, the capacity of both these edges is equal to wab M + κab /2. [sent-684, score-0.469]
</p><p>91 The cost of the st-cut for the edges in Equation (17) is given by wab (d(L − f (a) + im ) + d( f (a) − im )) 2 f (a) jm wab + ∑ ∑ 2 d(i′ − j′ + 1) − 2d(i′ − j′ ) + d(i′ − j′ − 1) i′ =im +2 j′ =im +1 jm  wab d(im − j′ + 2) − 2d(im − j′ + 1) + d(im − j′ ) ′ =i +2 2 j m κab +wab M + . [sent-687, score-2.75]
</p><p>92 2 In order to simplify the above expression, we begin by observing that +  ∑  jm  ∑  j′ =im +1 ′  (18)  d(i′ − j′ + 1) − 2d(i′ − j′ ) + d(i′ − j′ − 1)  d(i − im ) − d(i′ − im − 1) − d(i′ − jm ) + d(i′ − jm − 1). [sent-688, score-1.87]
</p><p>93 =  The above equation is obtained by substituting k′ = im in Equation (14). [sent-689, score-0.575]
</p><p>94 It follows that f (a)  jm  wab d(i′ − j′ + 1) − 2d(i′ − j′ ) + d(i′ − j′ − 1) 2 i′ =im +2 j′ =im +1  ∑  =  ∑  d(2) − d(1) − d(im − jm + 2) + d(im − jm + 1)  +d(3) − d(2) − d(im − jm + 3) + d(im − jm + 2) . [sent-690, score-1.539]
</p><p>95 Similarly, by substituting k′ = im + 1 in Equation (14), we get jm  wab d(im − j′ + 2) − 2d(im − j′ + 1) + d(im − j′ ) ′ =i +2 2 j m  ∑  =  d(0) − d(1) − d( jm − im − 1) + d( jm − im )  =  d(0) − d(1) − d(L − 1) + d(L). [sent-695, score-2.784]
</p><p>96 Consider one such st-cut that results in the following labeling: f ∗ (a) if va ∈ v( f ∗ , Im ) f (a) = fm (a) otherwise. [sent-703, score-0.591]
</p><p>97 We consider the following six cases: • For random variables va ∈ v( f ∗ , Im ) it follows from Property 5 that the cost of the st-cut will / include the unary potentials associated with such variables exactly, that is,  ∑  va ∈v( f ∗ ,Im ) /  61  θa ( fm (a)). [sent-706, score-1.077]
</p><p>98 (22)  (a,b)∈A ( f ∗ ,Im ) B ( f ∗ ,Im ) /  • For random variables va ∈ v( f ∗ , Im ), it follows from Property 5 that the cost of the st-cut will include the unary potentials associated with such variables exactly, that is,  ∑  θa ( f ∗ (a)). [sent-708, score-0.486]
</p><p>99 Since we are dealing with the truncated linear metric, the terms em , em and em can be simpliﬁed as b ab a em = wab | f ∗ (a) − f ∗ (b)|, em = wab ( f ∗ (a) − im − 1 + M), em = wab ( f ∗ (b) − im − 1 + M). [sent-720, score-2.999]
</p><p>100 Furthermore, the conditions for (a, b) ∈ B1 ( f ∗ , Im ) and (a, b) ∈ B2 ( f ∗ , Im ) are given by (a, b) ∈ B1 ( f ∗ , Im ) ⇐⇒ im ∈ [ f ∗ (a) − L, f ∗ (a) − 1],  (a, b) ∈ B2 ( f ∗ , Im ) ⇐⇒ im ∈ [ f ∗ (b) − L, f ∗ (b) − 1]. [sent-725, score-1.15]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('im', 0.575), ('fm', 0.377), ('wab', 0.339), ('jm', 0.24), ('va', 0.214), ('labeling', 0.208), ('ab', 0.189), ('potentials', 0.154), ('truncated', 0.127), ('energy', 0.121), ('sm', 0.1), ('swap', 0.091), ('unary', 0.089), ('em', 0.086), ('eksler', 0.081), ('oves', 0.081), ('runcated', 0.081), ('pab', 0.08), ('edges', 0.074), ('cm', 0.073), ('onvex', 0.069), ('mproved', 0.069), ('umar', 0.069), ('boykov', 0.065), ('ak', 0.064), ('pairwise', 0.061), ('labelings', 0.058), ('orr', 0.057), ('si', 0.056), ('capacity', 0.056), ('tardos', 0.054), ('move', 0.05), ('bim', 0.049), ('chekuri', 0.049), ('lp', 0.049), ('range', 0.047), ('multiplicative', 0.044), ('relaxation', 0.04), ('komodakis', 0.038), ('interval', 0.038), ('expansion', 0.038), ('odels', 0.036), ('schlesinger', 0.036), ('veksler', 0.036), ('bp', 0.035), ('capacities', 0.034), ('aim', 0.034), ('rounding', 0.034), ('mrf', 0.034), ('graph', 0.033), ('ishikawa', 0.031), ('gm', 0.031), ('stereo', 0.03), ('vb', 0.03), ('cost', 0.029), ('gupta', 0.027), ('neighboring', 0.027), ('bk', 0.027), ('labels', 0.027), ('convex', 0.027), ('kumar', 0.026), ('truncation', 0.023), ('potential', 0.023), ('tziritas', 0.023), ('pami', 0.023), ('szeliski', 0.023), ('epresenting', 0.022), ('otentials', 0.022), ('overestimates', 0.022), ('trw', 0.022), ('quadratic', 0.02), ('label', 0.02), ('ir', 0.019), ('metric', 0.019), ('elds', 0.019), ('moves', 0.019), ('map', 0.019), ('potts', 0.018), ('edge', 0.018), ('construction', 0.017), ('kolmogorov', 0.017), ('smooth', 0.017), ('sink', 0.017), ('felzenszwalb', 0.017), ('partitioning', 0.017), ('cut', 0.016), ('pawan', 0.015), ('lhs', 0.015), ('olga', 0.015), ('iteration', 0.015), ('intervals', 0.015), ('lk', 0.015), ('passing', 0.014), ('pixels', 0.014), ('vertices', 0.014), ('vm', 0.014), ('torr', 0.014), ('fi', 0.014), ('alahari', 0.013), ('message', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="41-tfidf-1" href="./jmlr-2011-Improved_Moves_for_Truncated_Convex_Models.html">41 jmlr-2011-Improved Moves for Truncated Convex Models</a></p>
<p>Author: M. Pawan Kumar, Olga Veksler, Philip H.S. Torr</p><p>Abstract: We consider the problem of obtaining an approximate maximum a posteriori estimate of a discrete random ﬁeld characterized by pairwise potentials that form a truncated convex model. For this problem, we propose two st-MINCUT based move making algorithms that we call Range Swap and Range Expansion. Our algorithms can be thought of as extensions of αβ-Swap and α-Expansion respectively that fully exploit the form of the pairwise potentials. Speciﬁcally, instead of dealing with one or two labels at each iteration, our methods explore a large search space by considering a range of labels (that is, an interval of consecutive labels). Furthermore, we show that Range Expansion provides the same multiplicative bounds as the standard linear programming (LP) relaxation in polynomial time. Compared to previous approaches based on the LP relaxation, for example interior-point algorithms or tree-reweighted message passing (TRW), our methods are faster as they use only the efﬁcient st-MINCUT algorithm in their design. We demonstrate the usefulness of the proposed approaches on both synthetic and standard real data problems. Keywords: truncated convex models, move making algorithms, range moves, multiplicative bounds, linear programming relaxation</p><p>2 0.15385212 <a title="41-tfidf-2" href="./jmlr-2011-Double_Updating_Online_Learning.html">28 jmlr-2011-Double Updating Online Learning</a></p>
<p>Author: Peilin Zhao, Steven C.H. Hoi, Rong Jin</p><p>Abstract: In most kernel based online learning algorithms, when an incoming instance is misclassiﬁed, it will be added into the pool of support vectors and assigned with a weight, which often remains unchanged during the rest of the learning process. This is clearly insufﬁcient since when a new support vector is added, we generally expect the weights of the other existing support vectors to be updated in order to reﬂect the inﬂuence of the added support vector. In this paper, we propose a new online learning method, termed Double Updating Online Learning, or DUOL for short, that explicitly addresses this problem. Instead of only assigning a ﬁxed weight to the misclassiﬁed example received at the current trial, the proposed online learning algorithm also tries to update the weight for one of the existing support vectors. We show that the mistake bound can be improved by the proposed online learning method. We conduct an extensive set of empirical evaluations for both binary and multi-class online learning tasks. The experimental results show that the proposed technique is considerably more effective than the state-of-the-art online learning algorithms. The source code is available to public at http://www.cais.ntu.edu.sg/˜chhoi/DUOL/. Keywords: online learning, kernel method, support vector machines, maximum margin learning, classiﬁcation</p><p>3 0.14494702 <a title="41-tfidf-3" href="./jmlr-2011-Faster_Algorithms_for_Max-Product_Message-Passing.html">34 jmlr-2011-Faster Algorithms for Max-Product Message-Passing</a></p>
<p>Author: Julian J. McAuley, TibĂŠrio S. Caetano</p><p>Abstract: Maximum A Posteriori inference in graphical models is often solved via message-passing algorithms, such as the junction-tree algorithm or loopy belief-propagation. The exact solution to this problem is well-known to be exponential in the size of the maximal cliques of the triangulated model, while approximate inference is typically exponential in the size of the modelĂ˘&euro;&trade;s factors. In this paper, we take advantage of the fact that many models have maximal cliques that are larger than their constituent factors, and also of the fact that many factors consist only of latent variables (i.e., they do not depend on an observation). This is a common case in a wide variety of applications that deal with grid-, tree-, and ring-structured models. In such cases, we are able to decrease the exponent of complexity for message-passing by 0.5 for both exact and approximate inference. We demonstrate that message-passing operations in such models are equivalent to some variant of matrix multiplication in the tropical semiring, for which we offer an O(N 2.5 ) expected-case solution. Keywords: graphical models, belief-propagation, tropical matrix multiplication</p><p>4 0.058421552 <a title="41-tfidf-4" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>Author: Shipeng Yu, Balaji Krishnapuram, Rómer Rosales, R. Bharat Rao</p><p>Abstract: Co-training (or more generally, co-regularization) has been a popular algorithm for semi-supervised learning in data with two feature representations (or views), but the fundamental assumptions underlying this type of models are still unclear. In this paper we propose a Bayesian undirected graphical model for co-training, or more generally for semi-supervised multi-view learning. This makes explicit the previously unstated assumptions of a large class of co-training type algorithms, and also clariﬁes the circumstances under which these assumptions fail. Building upon new insights from this model, we propose an improved method for co-training, which is a novel co-training kernel for Gaussian process classiﬁers. The resulting approach is convex and avoids local-maxima problems, and it can also automatically estimate how much each view should be trusted to accommodate noisy or unreliable views. The Bayesian co-training approach can also elegantly handle data samples with missing views, that is, some of the views are not available for some data points at learning time. This is further extended to an active sensing framework, in which the missing (sample, view) pairs are actively acquired to improve learning performance. The strength of active sensing model is that one actively sensed (sample, view) pair would improve the joint multi-view classiﬁcation on all the samples. Experiments on toy data and several real world data sets illustrate the beneﬁts of this approach. Keywords: co-training, multi-view learning, semi-supervised learning, Gaussian processes, undirected graphical models, active sensing</p><p>5 0.052945536 <a title="41-tfidf-5" href="./jmlr-2011-The_Sample_Complexity_of_Dictionary_Learning.html">91 jmlr-2011-The Sample Complexity of Dictionary Learning</a></p>
<p>Author: Daniel Vainsencher, Shie Mannor, Alfred M. Bruckstein</p><p>Abstract: A large set of signals can sometimes be described sparsely using a dictionary, that is, every element can be represented as a linear combination of few elements from the dictionary. Algorithms for various signal processing applications, including classiﬁcation, denoising and signal separation, learn a dictionary from a given set of signals to be represented. Can we expect that the error in representing by such a dictionary a previously unseen signal from the same source will be of similar magnitude as those for the given examples? We assume signals are generated from a ﬁxed distribution, and study these questions from a statistical learning theory perspective. We develop generalization bounds on the quality of the learned dictionary for two types of constraints on the coefﬁcient selection, as measured by the expected L2 error in representation when the dictionary is used. For the case of l1 regularized coefﬁcient selection we provide a generalnp ln(mλ)/m , where n is the dimension, p is the number of ization bound of the order of O elements in the dictionary, λ is a bound on the l1 norm of the coefﬁcient vector and m is the number of samples, which complements existing results. For the case of representing a new signal as a combination of at most k dictionary elements, we provide a bound of the order O( np ln(mk)/m) under an assumption on the closeness to orthogonality of the dictionary (low Babel function). We further show that this assumption holds for most dictionaries in high dimensions in a strong probabilistic sense. Our results also include bounds that converge as 1/m, not previously known for this problem. We provide similar results in a general setting using kernels with weak smoothness requirements. Keywords: dictionary learning, generalization bound, sparse representation</p><p>6 0.031500466 <a title="41-tfidf-6" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>7 0.030954711 <a title="41-tfidf-7" href="./jmlr-2011-Weisfeiler-Lehman_Graph_Kernels.html">103 jmlr-2011-Weisfeiler-Lehman Graph Kernels</a></p>
<p>8 0.030260231 <a title="41-tfidf-8" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>9 0.027053766 <a title="41-tfidf-9" href="./jmlr-2011-CARP%3A_Software_for_Fishing_Out_Good_Clustering_Algorithms.html">15 jmlr-2011-CARP: Software for Fishing Out Good Clustering Algorithms</a></p>
<p>10 0.026959959 <a title="41-tfidf-10" href="./jmlr-2011-Learning_from_Partial_Labels.html">58 jmlr-2011-Learning from Partial Labels</a></p>
<p>11 0.026905848 <a title="41-tfidf-11" href="./jmlr-2011-Convex_and_Network_Flow_Optimization_for_Structured_Sparsity.html">20 jmlr-2011-Convex and Network Flow Optimization for Structured Sparsity</a></p>
<p>12 0.02680237 <a title="41-tfidf-12" href="./jmlr-2011-Inverse_Reinforcement_Learning_in_Partially_Observable_Environments.html">47 jmlr-2011-Inverse Reinforcement Learning in Partially Observable Environments</a></p>
<p>13 0.023005137 <a title="41-tfidf-13" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>14 0.022466948 <a title="41-tfidf-14" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<p>15 0.021772206 <a title="41-tfidf-15" href="./jmlr-2011-Adaptive_Exact_Inference_in_Graphical_Models.html">7 jmlr-2011-Adaptive Exact Inference in Graphical Models</a></p>
<p>16 0.020671779 <a title="41-tfidf-16" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>17 0.020569263 <a title="41-tfidf-17" href="./jmlr-2011-Robust_Gaussian_Process_Regression_with_a_Student-tLikelihood.html">82 jmlr-2011-Robust Gaussian Process Regression with a Student-tLikelihood</a></p>
<p>18 0.020031968 <a title="41-tfidf-18" href="./jmlr-2011-High-dimensional_Covariance_Estimation_Based_On_Gaussian_Graphical_Models.html">39 jmlr-2011-High-dimensional Covariance Estimation Based On Gaussian Graphical Models</a></p>
<p>19 0.019703336 <a title="41-tfidf-19" href="./jmlr-2011-Semi-Supervised_Learning_with_Measure_Propagation.html">84 jmlr-2011-Semi-Supervised Learning with Measure Propagation</a></p>
<p>20 0.019148607 <a title="41-tfidf-20" href="./jmlr-2011-Differentially_Private_Empirical_Risk_Minimization.html">22 jmlr-2011-Differentially Private Empirical Risk Minimization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.119), (1, -0.004), (2, -0.03), (3, 0.012), (4, 0.034), (5, 0.044), (6, 0.044), (7, 0.019), (8, 0.082), (9, -0.012), (10, -0.222), (11, 0.117), (12, 0.302), (13, -0.105), (14, 0.324), (15, 0.012), (16, 0.065), (17, 0.113), (18, -0.24), (19, 0.119), (20, 0.153), (21, -0.207), (22, -0.096), (23, -0.124), (24, 0.139), (25, 0.104), (26, 0.1), (27, 0.102), (28, -0.166), (29, -0.086), (30, -0.158), (31, 0.025), (32, -0.164), (33, -0.061), (34, -0.044), (35, -0.077), (36, -0.02), (37, -0.099), (38, 0.048), (39, -0.056), (40, -0.013), (41, -0.012), (42, 0.028), (43, 0.031), (44, 0.055), (45, 0.026), (46, 0.004), (47, -0.037), (48, 0.044), (49, 0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98940402 <a title="41-lsi-1" href="./jmlr-2011-Improved_Moves_for_Truncated_Convex_Models.html">41 jmlr-2011-Improved Moves for Truncated Convex Models</a></p>
<p>Author: M. Pawan Kumar, Olga Veksler, Philip H.S. Torr</p><p>Abstract: We consider the problem of obtaining an approximate maximum a posteriori estimate of a discrete random ﬁeld characterized by pairwise potentials that form a truncated convex model. For this problem, we propose two st-MINCUT based move making algorithms that we call Range Swap and Range Expansion. Our algorithms can be thought of as extensions of αβ-Swap and α-Expansion respectively that fully exploit the form of the pairwise potentials. Speciﬁcally, instead of dealing with one or two labels at each iteration, our methods explore a large search space by considering a range of labels (that is, an interval of consecutive labels). Furthermore, we show that Range Expansion provides the same multiplicative bounds as the standard linear programming (LP) relaxation in polynomial time. Compared to previous approaches based on the LP relaxation, for example interior-point algorithms or tree-reweighted message passing (TRW), our methods are faster as they use only the efﬁcient st-MINCUT algorithm in their design. We demonstrate the usefulness of the proposed approaches on both synthetic and standard real data problems. Keywords: truncated convex models, move making algorithms, range moves, multiplicative bounds, linear programming relaxation</p><p>2 0.54765451 <a title="41-lsi-2" href="./jmlr-2011-Faster_Algorithms_for_Max-Product_Message-Passing.html">34 jmlr-2011-Faster Algorithms for Max-Product Message-Passing</a></p>
<p>Author: Julian J. McAuley, TibĂŠrio S. Caetano</p><p>Abstract: Maximum A Posteriori inference in graphical models is often solved via message-passing algorithms, such as the junction-tree algorithm or loopy belief-propagation. The exact solution to this problem is well-known to be exponential in the size of the maximal cliques of the triangulated model, while approximate inference is typically exponential in the size of the modelĂ˘&euro;&trade;s factors. In this paper, we take advantage of the fact that many models have maximal cliques that are larger than their constituent factors, and also of the fact that many factors consist only of latent variables (i.e., they do not depend on an observation). This is a common case in a wide variety of applications that deal with grid-, tree-, and ring-structured models. In such cases, we are able to decrease the exponent of complexity for message-passing by 0.5 for both exact and approximate inference. We demonstrate that message-passing operations in such models are equivalent to some variant of matrix multiplication in the tropical semiring, for which we offer an O(N 2.5 ) expected-case solution. Keywords: graphical models, belief-propagation, tropical matrix multiplication</p><p>3 0.53441578 <a title="41-lsi-3" href="./jmlr-2011-Double_Updating_Online_Learning.html">28 jmlr-2011-Double Updating Online Learning</a></p>
<p>Author: Peilin Zhao, Steven C.H. Hoi, Rong Jin</p><p>Abstract: In most kernel based online learning algorithms, when an incoming instance is misclassiﬁed, it will be added into the pool of support vectors and assigned with a weight, which often remains unchanged during the rest of the learning process. This is clearly insufﬁcient since when a new support vector is added, we generally expect the weights of the other existing support vectors to be updated in order to reﬂect the inﬂuence of the added support vector. In this paper, we propose a new online learning method, termed Double Updating Online Learning, or DUOL for short, that explicitly addresses this problem. Instead of only assigning a ﬁxed weight to the misclassiﬁed example received at the current trial, the proposed online learning algorithm also tries to update the weight for one of the existing support vectors. We show that the mistake bound can be improved by the proposed online learning method. We conduct an extensive set of empirical evaluations for both binary and multi-class online learning tasks. The experimental results show that the proposed technique is considerably more effective than the state-of-the-art online learning algorithms. The source code is available to public at http://www.cais.ntu.edu.sg/˜chhoi/DUOL/. Keywords: online learning, kernel method, support vector machines, maximum margin learning, classiﬁcation</p><p>4 0.1942586 <a title="41-lsi-4" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>Author: Shipeng Yu, Balaji Krishnapuram, Rómer Rosales, R. Bharat Rao</p><p>Abstract: Co-training (or more generally, co-regularization) has been a popular algorithm for semi-supervised learning in data with two feature representations (or views), but the fundamental assumptions underlying this type of models are still unclear. In this paper we propose a Bayesian undirected graphical model for co-training, or more generally for semi-supervised multi-view learning. This makes explicit the previously unstated assumptions of a large class of co-training type algorithms, and also clariﬁes the circumstances under which these assumptions fail. Building upon new insights from this model, we propose an improved method for co-training, which is a novel co-training kernel for Gaussian process classiﬁers. The resulting approach is convex and avoids local-maxima problems, and it can also automatically estimate how much each view should be trusted to accommodate noisy or unreliable views. The Bayesian co-training approach can also elegantly handle data samples with missing views, that is, some of the views are not available for some data points at learning time. This is further extended to an active sensing framework, in which the missing (sample, view) pairs are actively acquired to improve learning performance. The strength of active sensing model is that one actively sensed (sample, view) pair would improve the joint multi-view classiﬁcation on all the samples. Experiments on toy data and several real world data sets illustrate the beneﬁts of this approach. Keywords: co-training, multi-view learning, semi-supervised learning, Gaussian processes, undirected graphical models, active sensing</p><p>5 0.17722131 <a title="41-lsi-5" href="./jmlr-2011-CARP%3A_Software_for_Fishing_Out_Good_Clustering_Algorithms.html">15 jmlr-2011-CARP: Software for Fishing Out Good Clustering Algorithms</a></p>
<p>Author: Volodymyr Melnykov, Ranjan Maitra</p><p>Abstract: This paper presents the C LUSTERING A LGORITHMS ’ R EFEREE PACKAGE or CARP, an open source GNU GPL-licensed C package for evaluating clustering algorithms. Calibrating performance of such algorithms is important and CARP addresses this need by generating datasets of different clustering complexity and by assessing the performance of the concerned algorithm in terms of its ability to classify each dataset relative to the true grouping. This paper brieﬂy describes the software and its capabilities. Keywords: CARP, M IX S IM, clustering algorithm, Gaussian mixture, overlap</p><p>6 0.17264466 <a title="41-lsi-6" href="./jmlr-2011-The_Sample_Complexity_of_Dictionary_Learning.html">91 jmlr-2011-The Sample Complexity of Dictionary Learning</a></p>
<p>7 0.13725254 <a title="41-lsi-7" href="./jmlr-2011-Weisfeiler-Lehman_Graph_Kernels.html">103 jmlr-2011-Weisfeiler-Lehman Graph Kernels</a></p>
<p>8 0.12317001 <a title="41-lsi-8" href="./jmlr-2011-Learning_from_Partial_Labels.html">58 jmlr-2011-Learning from Partial Labels</a></p>
<p>9 0.11834177 <a title="41-lsi-9" href="./jmlr-2011-Information%2C_Divergence_and_Risk_for_Binary_Experiments.html">43 jmlr-2011-Information, Divergence and Risk for Binary Experiments</a></p>
<p>10 0.11193668 <a title="41-lsi-10" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>11 0.10862324 <a title="41-lsi-11" href="./jmlr-2011-Semi-Supervised_Learning_with_Measure_Propagation.html">84 jmlr-2011-Semi-Supervised Learning with Measure Propagation</a></p>
<p>12 0.10460746 <a title="41-lsi-12" href="./jmlr-2011-High-dimensional_Covariance_Estimation_Based_On_Gaussian_Graphical_Models.html">39 jmlr-2011-High-dimensional Covariance Estimation Based On Gaussian Graphical Models</a></p>
<p>13 0.10205837 <a title="41-lsi-13" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>14 0.10123192 <a title="41-lsi-14" href="./jmlr-2011-MSVMpack%3A_A_Multi-Class_Support_Vector_Machine_Package.html">62 jmlr-2011-MSVMpack: A Multi-Class Support Vector Machine Package</a></p>
<p>15 0.093370698 <a title="41-lsi-15" href="./jmlr-2011-Cumulative_Distribution_Networks_and_the_Derivative-sum-product_Algorithm%3A_Models_and_Inference_for_Cumulative_Distribution_Functions_on_Graphs.html">21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</a></p>
<p>16 0.089510322 <a title="41-lsi-16" href="./jmlr-2011-Convex_and_Network_Flow_Optimization_for_Structured_Sparsity.html">20 jmlr-2011-Convex and Network Flow Optimization for Structured Sparsity</a></p>
<p>17 0.088412642 <a title="41-lsi-17" href="./jmlr-2011-Anechoic_Blind_Source_Separation_Using_Wigner_Marginals.html">10 jmlr-2011-Anechoic Blind Source Separation Using Wigner Marginals</a></p>
<p>18 0.086808883 <a title="41-lsi-18" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>19 0.086767375 <a title="41-lsi-19" href="./jmlr-2011-Internal_Regret_with_Partial_Monitoring%3A_Calibration-Based_Optimal_Algorithms.html">45 jmlr-2011-Internal Regret with Partial Monitoring: Calibration-Based Optimal Algorithms</a></p>
<p>20 0.084565006 <a title="41-lsi-20" href="./jmlr-2011-Natural_Language_Processing_%28Almost%29_from_Scratch.html">68 jmlr-2011-Natural Language Processing (Almost) from Scratch</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.046), (9, 0.025), (10, 0.079), (11, 0.479), (24, 0.031), (31, 0.058), (32, 0.017), (41, 0.019), (60, 0.012), (73, 0.034), (78, 0.059), (90, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83859289 <a title="41-lda-1" href="./jmlr-2011-Improved_Moves_for_Truncated_Convex_Models.html">41 jmlr-2011-Improved Moves for Truncated Convex Models</a></p>
<p>Author: M. Pawan Kumar, Olga Veksler, Philip H.S. Torr</p><p>Abstract: We consider the problem of obtaining an approximate maximum a posteriori estimate of a discrete random ﬁeld characterized by pairwise potentials that form a truncated convex model. For this problem, we propose two st-MINCUT based move making algorithms that we call Range Swap and Range Expansion. Our algorithms can be thought of as extensions of αβ-Swap and α-Expansion respectively that fully exploit the form of the pairwise potentials. Speciﬁcally, instead of dealing with one or two labels at each iteration, our methods explore a large search space by considering a range of labels (that is, an interval of consecutive labels). Furthermore, we show that Range Expansion provides the same multiplicative bounds as the standard linear programming (LP) relaxation in polynomial time. Compared to previous approaches based on the LP relaxation, for example interior-point algorithms or tree-reweighted message passing (TRW), our methods are faster as they use only the efﬁcient st-MINCUT algorithm in their design. We demonstrate the usefulness of the proposed approaches on both synthetic and standard real data problems. Keywords: truncated convex models, move making algorithms, range moves, multiplicative bounds, linear programming relaxation</p><p>2 0.69465852 <a title="41-lda-2" href="./jmlr-2011-Regression_on_Fixed-Rank_Positive_Semidefinite_Matrices%3A_A_Riemannian_Approach.html">80 jmlr-2011-Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach</a></p>
<p>Author: Gilles Meyer, Silvère Bonnabel, Rodolphe Sepulchre</p><p>Abstract: The paper addresses the problem of learning a regression model parameterized by a ﬁxed-rank positive semideﬁnite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of ﬁxedrank positive semideﬁnite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semideﬁnite matrix. Good performance is observed on classical benchmarks. Keywords: linear regression, positive semideﬁnite matrices, low-rank approximation, Riemannian geometry, gradient-based learning</p><p>3 0.26767823 <a title="41-lda-3" href="./jmlr-2011-A_Family_of_Simple_Non-Parametric_Kernel_Learning_Algorithms.html">4 jmlr-2011-A Family of Simple Non-Parametric Kernel Learning Algorithms</a></p>
<p>Author: Jinfeng Zhuang, Ivor W. Tsang, Steven C.H. Hoi</p><p>Abstract: Previous studies of Non-Parametric Kernel Learning (NPKL) usually formulate the learning task as a Semi-Deﬁnite Programming (SDP) problem that is often solved by some general purpose SDP solvers. However, for N data examples, the time complexity of NPKL using a standard interiorpoint SDP solver could be as high as O(N 6.5 ), which prohibits NPKL methods applicable to real applications, even for data sets of moderate size. In this paper, we present a family of efﬁcient NPKL algorithms, termed “SimpleNPKL”, which can learn non-parametric kernels from a large set of pairwise constraints efﬁciently. In particular, we propose two efﬁcient SimpleNPKL algorithms. One is SimpleNPKL algorithm with linear loss, which enjoys a closed-form solution that can be efﬁciently computed by the Lanczos sparse eigen decomposition technique. Another one is SimpleNPKL algorithm with other loss functions (including square hinge loss, hinge loss, square loss) that can be re-formulated as a saddle-point optimization problem, which can be further resolved by a fast iterative algorithm. In contrast to the previous NPKL approaches, our empirical results show that the proposed new technique, maintaining the same accuracy, is signiﬁcantly more efﬁcient and scalable. Finally, we also demonstrate that the proposed new technique is also applicable to speed up many kernel learning tasks, including colored maximum variance unfolding, minimum volume embedding, and structure preserving embedding. Keywords: non-parametric kernel learning, semi-deﬁnite programming, semi-supervised learning, side information, pairwise constraints</p><p>4 0.24724248 <a title="41-lda-4" href="./jmlr-2011-Faster_Algorithms_for_Max-Product_Message-Passing.html">34 jmlr-2011-Faster Algorithms for Max-Product Message-Passing</a></p>
<p>Author: Julian J. McAuley, TibĂŠrio S. Caetano</p><p>Abstract: Maximum A Posteriori inference in graphical models is often solved via message-passing algorithms, such as the junction-tree algorithm or loopy belief-propagation. The exact solution to this problem is well-known to be exponential in the size of the maximal cliques of the triangulated model, while approximate inference is typically exponential in the size of the modelĂ˘&euro;&trade;s factors. In this paper, we take advantage of the fact that many models have maximal cliques that are larger than their constituent factors, and also of the fact that many factors consist only of latent variables (i.e., they do not depend on an observation). This is a common case in a wide variety of applications that deal with grid-, tree-, and ring-structured models. In such cases, we are able to decrease the exponent of complexity for message-passing by 0.5 for both exact and approximate inference. We demonstrate that message-passing operations in such models are equivalent to some variant of matrix multiplication in the tropical semiring, for which we offer an O(N 2.5 ) expected-case solution. Keywords: graphical models, belief-propagation, tropical matrix multiplication</p><p>5 0.24327986 <a title="41-lda-5" href="./jmlr-2011-Convex_and_Network_Flow_Optimization_for_Structured_Sparsity.html">20 jmlr-2011-Convex and Network Flow Optimization for Structured Sparsity</a></p>
<p>Author: Julien Mairal, Rodolphe Jenatton, Guillaume Obozinski, Francis Bach</p><p>Abstract: We consider a class of learning problems regularized by a structured sparsity-inducing norm deﬁned as the sum of ℓ2 - or ℓ∞ -norms over groups of variables. Whereas much effort has been put in developing fast optimization techniques when the groups are disjoint or embedded in a hierarchy, we address here the case of general overlapping groups. To this end, we present two different strategies: On the one hand, we show that the proximal operator associated with a sum of ℓ∞ norms can be computed exactly in polynomial time by solving a quadratic min-cost ﬂow problem, allowing the use of accelerated proximal gradient methods. On the other hand, we use proximal splitting techniques, and address an equivalent formulation with non-overlapping groups, but in higher dimension and with additional constraints. We propose efﬁcient and scalable algorithms exploiting these two strategies, which are signiﬁcantly faster than alternative approaches. We illustrate these methods with several problems such as CUR matrix factorization, multi-task learning of tree-structured dictionaries, background subtraction in video sequences, image denoising with wavelets, and topographic dictionary learning of natural image patches. Keywords: convex optimization, proximal methods, sparse coding, structured sparsity, matrix factorization, network ﬂow optimization, alternating direction method of multipliers</p><p>6 0.24075381 <a title="41-lda-6" href="./jmlr-2011-Adaptive_Exact_Inference_in_Graphical_Models.html">7 jmlr-2011-Adaptive Exact Inference in Graphical Models</a></p>
<p>7 0.23743556 <a title="41-lda-7" href="./jmlr-2011-Learning_with_Structured_Sparsity.html">59 jmlr-2011-Learning with Structured Sparsity</a></p>
<p>8 0.23610184 <a title="41-lda-8" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>9 0.23280503 <a title="41-lda-9" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>10 0.23252568 <a title="41-lda-10" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>11 0.22901887 <a title="41-lda-11" href="./jmlr-2011-The_Sample_Complexity_of_Dictionary_Learning.html">91 jmlr-2011-The Sample Complexity of Dictionary Learning</a></p>
<p>12 0.22900881 <a title="41-lda-12" href="./jmlr-2011-Efficient_Learning_with_Partially_Observed_Attributes.html">29 jmlr-2011-Efficient Learning with Partially Observed Attributes</a></p>
<p>13 0.22869165 <a title="41-lda-13" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>14 0.22839801 <a title="41-lda-14" href="./jmlr-2011-X-Armed_Bandits.html">104 jmlr-2011-X-Armed Bandits</a></p>
<p>15 0.22732896 <a title="41-lda-15" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<p>16 0.22685923 <a title="41-lda-16" href="./jmlr-2011-Convergence_of_Distributed_Asynchronous_Learning_Vector_Quantization_Algorithms.html">19 jmlr-2011-Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms</a></p>
<p>17 0.22652338 <a title="41-lda-17" href="./jmlr-2011-Cumulative_Distribution_Networks_and_the_Derivative-sum-product_Algorithm%3A_Models_and_Inference_for_Cumulative_Distribution_Functions_on_Graphs.html">21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</a></p>
<p>18 0.2264618 <a title="41-lda-18" href="./jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</a></p>
<p>19 0.22566138 <a title="41-lda-19" href="./jmlr-2011-Parameter_Screening_and_Optimisation_for_ILP_using_Designed_Experiments.html">76 jmlr-2011-Parameter Screening and Optimisation for ILP using Designed Experiments</a></p>
<p>20 0.22532161 <a title="41-lda-20" href="./jmlr-2011-Minimum_Description_Length_Penalization_for_Group_and_Multi-Task_Sparse_Learning.html">64 jmlr-2011-Minimum Description Length Penalization for Group and Multi-Task Sparse Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
