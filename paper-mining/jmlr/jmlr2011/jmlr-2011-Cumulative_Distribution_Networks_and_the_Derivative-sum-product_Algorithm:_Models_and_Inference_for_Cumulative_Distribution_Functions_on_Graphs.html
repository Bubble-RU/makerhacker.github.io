<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-21" href="#">jmlr2011-21</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</h1>
<br/><p>Source: <a title="jmlr-2011-21-pdf" href="http://jmlr.org/papers/volume12/huang11a/huang11a.pdf">pdf</a></p><p>Author: Jim C. Huang, Brendan J. Frey</p><p>Abstract: We present a class of graphical models for directly representing the joint cumulative distribution function (CDF) of many random variables, called cumulative distribution networks (CDNs). Unlike graphs for probability density and mass functions, for CDFs the marginal probabilities for any subset of variables are obtained by computing limits of functions in the model, and conditional probabilities correspond to computing mixed derivatives. We will show that the conditional independence properties in a CDN are distinct from the conditional independence properties of directed, undirected and factor graphs, but include the conditional independence properties of bi-directed graphs. In order to perform inference in such models, we describe the ‘derivative-sum-product’ (DSP) message-passing algorithm in which messages correspond to derivatives of the joint CDF. We will then apply CDNs to the problem of learning to rank players in multiplayer team-based games and suggest several future directions for research. Keywords: graphical models, cumulative distribution function, message-passing algorithm, inference</p><p>Reference: <a title="jmlr-2011-21-reference" href="../jmlr2011_reference/jmlr-2011-Cumulative_Distribution_Networks_and_the_Derivative-sum-product_Algorithm%3A_Models_and_Inference_for_Cumulative_Distribution_Functions_on_Graphs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We will show that the conditional independence properties in a CDN are distinct from the conditional independence properties of directed, undirected and factor graphs, but include the conditional independence properties of bi-directed graphs. [sent-9, score-0.363]
</p><p>2 Each variable in the model corresponds to a node in a graph and edges between nodes in the graph convey statistical dependence relationships between the variables in the model. [sent-14, score-0.25]
</p><p>3 The graphical formalism allows one to obtain the independence relationships between random variables in a model by inspecting the corresponding graph, where the separation of nodes in the graph implies a particular conditional independence relationship between the corresponding variables. [sent-15, score-0.381]
</p><p>4 An example of this type of problem is that of predicting multiplayer game outcomes with a team structure (Herbrich, Minka and Graepel, 2007). [sent-29, score-0.377]
</p><p>5 We will show that the conditional independence properties in a CDN include, but are not limited to, the conditional independence properties for bi-directed graphs (Drton and Richardson, 2008; Richardson and Spirtes, 2002; Richardson, 2003). [sent-36, score-0.252]
</p><p>6 Deﬁnition 5 The cumulative distribution network (CDN) is an undirected bipartite graphical model consisting of a bipartite graph G = (V, S, E), where V denotes variable nodes and S denotes factor nodes, with edges in E connecting factor nodes to variable nodes. [sent-101, score-0.348]
</p><p>7 The CDN also includes a speciﬁcation of functions φs (xs ) for each function node s ∈ S, where xs ≡ xN (s) , ∪s∈S N (s) = V and each function φs : R|N (s)| → [0, 1] satisﬁes the properties of a CDF. [sent-102, score-0.265]
</p><p>8 However, as we will see shortly, this leads to a different set of conditional independence properties as compared to the conditional independence properties of directed, undirected and factor graphs. [sent-107, score-0.248]
</p><p>9 Furthermore, if for any xs →∞  x→∞  s∈S  given α ∈ V and for s ∈ N (α), we have lim φs (xs ) = 0, then lim xα →−∞  xα →−∞  ∏ φs (xs ) = 0. [sent-113, score-0.275]
</p><p>10 In a bipartite graph G = (V, S, E), a (undirected) path of length K between two variable nodes α, β ∈ V consists of a sequence of distinct variable and function nodes α0 , s0 , α1 , s1 , · · · , sK , αK such that α0 = α, αK = β and (αk , sk ) = (sk , αk ) ∈ E for all k = 0, · · · , K. [sent-167, score-0.301]
</p><p>11 Having deﬁned graph separation for bipartite graphs, we begin with the conditional inequality independence property of CDNs, from which other marginal and conditional independence properties for a CDN will follow. [sent-172, score-0.335]
</p><p>12 Theorem 9 (Factorization property of a CDN) Let G = (V, S, E) be a bipartite graph and let the CDF F(x) satisfy the conditional independence property implied by the CDN described by G , so that graph separation of A and B by V \ (A ∪ B) with respect to G implies A ⊥ B|ω xW for any ⊥ |W | . [sent-178, score-0.248]
</p><p>13 A bi-directed graph G = (V, E) consists of nodes α ∈ V and bi-directed edges e ∈ E 315  H UANG AND F REY  (a)  (b)  Figure 8: Example of conditional independence due to graph separation in a CDN. [sent-232, score-0.279]
</p><p>14 Then both G and G imply the same set of marginal independence constraints, as we have shown above that in a CDN, two nodes that do not share any function nodes in common are marginally independent (Theorem 10). [sent-256, score-0.246]
</p><p>15 In addition to implying the same marginal independence constraints as a bi-directed graphical model, the conditional independence property given in Theorem 11 for CDNs corresponds to the dual global Markov property of Kauermann (1996) for bi-directed graphical models, which we now present. [sent-258, score-0.309]
</p><p>16 (a)  (b)  (c)  Figure 10: Graphical models over four variables X1 , X2 , X3 , X4 in which graph separation of variable nodes imply the marginal independence relations X1 ⊥ X3 , X2 ⊥ X4 . [sent-262, score-0.283]
</p><p>17 We have shown that the conditional independence relationships that follow from graph separation in CDNs are different from the relationships implied by graph separation in Bayesian networks, Markov random ﬁelds and factor graph models. [sent-318, score-0.29]
</p><p>18 In the next section, we show how these two operations can be performed efﬁciently for tree-structured CDNs using message-passing, where messages being passed in the graph for the CDN correspond to mixed derivatives of the joint CDF with respect to variables in subtrees of the graph. [sent-322, score-0.303]
</p><p>19 Since that the CDF factorizes for a CDN, the global mixed derivative can then be decomposed into a series of local mixed derivative computations, where each function s ∈ S and its derivatives is evaluated for observations xs . [sent-328, score-0.321]
</p><p>20 Furthermore, we can compute xN (s)\α →∞  any conditional cumulative distribution of the type F(xA |ω(xB )) in the same fashion by marginalizing the joint CDF over variables in V \ (A ∪ B) and computing F(xA , xB ) F(xA |ω(xB )) = = F(xB )  lim  xV \(A∪B) →∞  F(x)  lim F(x)  . [sent-337, score-0.25]
</p><p>21 The resulting algorithm consists of passing messages µα→s (x), µs→α (x) from variable nodes to function nodes and from function nodes to variable nodes, analogous to the operation of the sum-product algorithm in factor graphs. [sent-344, score-0.387]
</p><p>22 Then the messages from leaf variable nodes to the root are given by µX→g (x) = 1, µU→g (u) = 1, µg→Y (y; u, x) = ∂u,x g(u, x, y)µX→g (x)µU→g (u) , µY →h (y; u, x) = µg→Y (y; u, x), µh→Z (z; u, x, y) = ∂y h(y, z)µY →h (y; u, x) . [sent-351, score-0.266]
</p><p>23 To compute the conditional CDF for any variable node in the network, we can pass messages from leaf nodes to root and then from the root node back to the leaves. [sent-371, score-0.461]
</p><p>24 Messages are passed once from all 326  C UMULATIVE D ISTRIBUTION N ETWORKS AND THE D ERIVATIVE - SUM - PRODUCT A LGORITHM  variable nodes on which we are conditioning to the root node: in the example, messages are passed from variable nodes X, Z to variable node Y in order to compute F(y|x, z). [sent-384, score-0.417]
</p><p>25 If we wished to compute, say, F(x|y, z), then messages would be passed from variable nodes Y, Z to variable node X. [sent-385, score-0.306]
</p><p>26 For graphs deﬁned over discrete ordinal variables that take 327  H UANG AND F REY  on one of K values, for an observed x, each message µα→s , µs→α consists of a K-vector, analogous to messages in the sum-product algorithm. [sent-399, score-0.277]
</p><p>27 a) Computation of the message from a function node s to a variable node α; b) Computation of the message from a variable node α to a function node s. [sent-413, score-0.354]
</p><p>28 • Input: A tree-structured CDN G = (V, S, E), root node α ∈ V and a vector x of observations • Output: The probability density function (PDF) P(x) • For each leaf variable node α′ and for all function nodes s ∈ N (α′ ), propagate µα′ →s (x) = 1, λα′ →s (x) = 0. [sent-414, score-0.251]
</p><p>29 • Sample xi∗ from F(xk |x1 , · · · , xk−1 ) =  ∂x1 ,··· ,xk−1 F(x1 , · · · , xk ) lim ∂x1 ,··· ,xk−1 F(x1 , · · · , xk )  xk →∞  330  . [sent-431, score-0.315]
</p><p>30 For this problem, we observe the scores achieved by several players over many games t = 1 · · · , T in which players interactively compete in groups, or teams, which change with each game. [sent-446, score-0.257]
</p><p>31 For any given game, players compete in teams so that at the end of each game, each player will have achieved a score as a result of actions taken by all players during the game. [sent-447, score-0.456]
</p><p>32 For example, these player scores could correspond to the number of targets destroyed or the number of ﬂags stolen, so that a higher player score reﬂects a better performance for that player. [sent-448, score-0.419]
</p><p>33 For example, a game involving six players labeled 1, 2, 3, 4, 5, 6 organized into three teams of two players each could correspond to Pt = {1, 2, 3, 4, 5, 6} and Tt = {{1, 2}, {3, 4}, {5, 6}}. [sent-450, score-0.4]
</p><p>34 Without loss of generality we will label the teams in a game by n = 1, · · · , N where each team corresponds to a set in the partition Tt . [sent-451, score-0.423]
</p><p>35 In addition to the above, we will denote by Ot the outcome of a game that consists of the pair (xPt , rTt ), where xPt ∈ R|Pt | is a vector of player scores for game Γt and the set rTt is deﬁned as a partially ordered set of team performances, or set of ranks for each team. [sent-452, score-0.679]
</p><p>36 Such ranks are obtained by ﬁrst computing the sum of the player scores for each team n = 1, · · · , N, and then ranking the 331  H UANG AND F REY  teams by sorting the resulting sums. [sent-453, score-0.528]
</p><p>37 We will also denote by xn ∈ R|Tt | the vector of player scores for team n in game Γt . [sent-456, score-0.566]
</p><p>38 For example, a ”SmallTeam” game type would consist of two teams with at most two players per team, whereas a ”FreeForAll” game type would constrain the number of teams to be at most eight, with one player per team. [sent-458, score-0.739]
</p><p>39 Given the above, the goal is to construct a model that will allow us to predict the outcome Ot of the new game before it begins, given Pt and previous game outcomes O1 , · · · , Ot−1 . [sent-460, score-0.258]
</p><p>40 In particular, we wish to construct a model that will minimize the number of mis-ordered teams based on the set of team performances rTt for game Γt . [sent-461, score-0.464]
</p><p>41 Here, the probability model for the given game should account for the team-based structure of games, such that team performances are determined by individual player scores and a game outcome is determined by the ordering of team scores. [sent-462, score-0.914]
</p><p>42 (2007) for skill rating in Halo 2TM , whereby each player k ∈ Pt is assigned a probability distribution over latent skill variables Sk , which is then inferred from individual player scores over multiple games using the expectation propagation algorithm for approximate inference (Minka, 2001). [sent-466, score-0.649]
</p><p>43 Inference in the TrueSkillTM model thus consists of applying expectation propagation to a factor graph for a given game in order to update probabilities over player skills. [sent-467, score-0.35]
</p><p>44 Finally, for teams n, n + 1, there is a difference variable Hn,n+1 and a corresponding factor which declares a tied rank between two teams if the difference between the two team scores is below some threshold parameter. [sent-470, score-0.472]
</p><p>45 The model will be designed on a game-by-game basis where the team assignments of players for a given game determines the connectivity of the graph G for the CDN. [sent-473, score-0.434]
</p><p>46 In our model the team variables will correspond to the ranks of teams: we will call such variables team performances and denote these as Rn for team n in order to contrast these with the team score variables Tn in the TrueSkill model. [sent-474, score-0.854]
</p><p>47 Our model will account for player scores Xk for each player k ∈ Pt in the game, the team performances Rn for each team n = 1, · · · , N in the game and each player’s skill function sk (xk ), which is a CDF speciﬁc to each player. [sent-475, score-1.073]
</p><p>48 The variables H12 , H23 correspond to differences in team scores which determine the ranking of teams, so that teams n and n + 1 are tied in their rankings if the difference in their team scores is below a threshold parameter. [sent-477, score-0.606]
</p><p>49 Each player k = 1, 2, 3, 4, 5, 6 is assigned a skill function that reﬂects the distribution of that player’s skill level Sk given past game outcomes. [sent-480, score-0.436]
</p><p>50 Each player then achieves score Xk in any given game and team scores Tn , n = 1, 2, 3 are then determined as the sum of player scores for each team. [sent-481, score-0.784]
</p><p>51 The set of observed team performances rTt will be given by the joint conﬁguration of the Rn variables for that game. [sent-483, score-0.281]
</p><p>52 The goal will then be to adapt player skill functions sk (xk ) for each game as a function of game outcome. [sent-484, score-0.584]
</p><p>53 Second, team performance variables depend on those of other teams in the game, so that each team’s performance should be linked to that of other teams in a game. [sent-487, score-0.428]
</p><p>54 To address the ﬁrst point, we will require a set of CDN functions that connect player scores to team performances. [sent-489, score-0.436]
</p><p>55 In the context of multiplayer games, we perform separate ordinal regressions for different game types, as the cutpoints that are learned for a given game type may vary between different game types due to differing team sizes between game types. [sent-492, score-0.86]
</p><p>56 For a given game 333  H UANG AND F REY  type, we treat the set of all games as a bag of pairs of player score vectors xn and team performances rn from which cutpoints in an ordinal regression model can be learned. [sent-493, score-0.763]
</p><p>57 We will model multiplayer games using a CDN in which players are grouped into teams and teams compete with one another. [sent-496, score-0.401]
</p><p>58 To model dependence between player scores and team performance, we will combine the above cumulative model for ordinal regression with prior player score distributions under the assumptions that players contribute equally to team performance and that players perform equally well on average. [sent-497, score-1.057]
</p><p>59 The regression function in the cumulative model is given by f (x) = wT x with w set to the vector of ones 1, as we weigh the contributions of players on a team equally. [sent-499, score-0.311]
</p><p>60 Furthermore, θ(rn ) are the cutpoints that deﬁne contiguous intervals in which rn is the ranking for team n based on that team’s performance and P(u) is a probability density over a vector of latent player scores u. [sent-500, score-0.559]
</p><p>61 By combining functions gn (which assume equal player skills on average) with individual player skills whilst accounting for the dependence between players’ skills and team performances, we can update each player’s skill function sk conditioned on the outcome of a game. [sent-502, score-0.808]
</p><p>62 To address the fact that teams compete in any given game, we model ordinal relationships between team performance using the notion of stochastic orderings (Section 2. [sent-503, score-0.387]
</p><p>63 3), so that for two teams with team performances RX , RY , RX RY if FRX (t) ≥ FRY (t) ∀ t ∈ R, where FRX (·), FRY (·) are the marginal CDFs of RX , RY . [sent-504, score-0.35]
</p><p>64 This then allows us to design models in which we can express differences in team performances in the form of pairwise constraints on their marginal CDFs. [sent-505, score-0.259]
</p><p>65 Thus, although each of the Rn variables is a deterministic function of the sum of player scores, we can nevertheless model them as being stochastic using the framework of CDNs to specify orderings among the Rn variables. [sent-508, score-0.247]
</p><p>66 Finally, we will use a skill function sk (xk ) for each player k to model that player’s distribution over game scores given previous game outcomes. [sent-511, score-0.618]
</p><p>67 The player performance nodes in the CDN will then be connected to the team performance nodes via the above CDN functions gn and team performance variable nodes Rn are linked to one another via the above CDN functions hn,n+1 . [sent-512, score-0.931]
</p><p>68 The joint CDF for a given game Γt with N teams is then given by N  N−1  n=1  n=1  F(xPt , rTt ) = ∏ g(xn , rn ) ∏ hn,n+1 (rn , rn+1 ) ∏ sk (xk ). [sent-513, score-0.424]
</p><p>69 ˜ ˜ ˜  Figure 16: The CDN for the player scores and team performances in a game of Halo 2TM for a game with three teams with two players each. [sent-517, score-0.893]
</p><p>70 Each player k = 1, 2, 3, 4, 5, 6 achieves score Xk in a match and team performances Rn , n = 1, 2, 3 are determined based on the sum of player scores for each team. [sent-518, score-0.631]
</p><p>71 Having presented the CDN for modeling multiplayer games, we will now proceed to describe a method for predicting game outcomes in which we update player skill functions after each game using message-passing. [sent-519, score-0.58]
</p><p>72 2 Ranking Players in Multiplayer Games Using the Derivative-sum-product Algorithm Here we will apply the DSP algorithm in the context of ranking players in multiplayer games with a team structure, where the problem consists of jointly predicting multiple ordinal output variables. [sent-521, score-0.432]
</p><p>73 Given the above CDN model for multiplayer games, we would like to then estimate the player skill functions sk (xk ) for each player k from previous games played by that player. [sent-528, score-0.616]
</p><p>74 We then seek to estimate sk (xk ) for player k given previous team performances rTt ,t ∈ Tk and player scores for all other players xPt \k for all games t ∈ Tk in which player k participated. [sent-530, score-1.007]
</p><p>75 Denote by Ot−k the outcome of a game with the player score for player k removed from xPt . [sent-531, score-0.499]
</p><p>76 We will deﬁne the skill function sk (xk ) for a player to be given by sk (xk ) = F xk |{Ot−k }t∈Tk = ∏ F(xk |Ot−k ). [sent-532, score-0.467]
</p><p>77 The skill function sk can then be readily estimated by the DSP algorithm, since each game outcome is modeled by a tree-structured CDN. [sent-534, score-0.271]
</p><p>78 For each game Γt we can perform message-passing to obtain the conditional CDF F(xk |Ot−k ) = µgn →Xk (rTt , xPt \k ) for player k (assuming the message µgn →Xk has been properly normalized as described above) and then perform a multiplicative update sk (xk ) ← sk (xk )µgn →Xk (rTt , xPt \k ). [sent-536, score-0.521]
</p><p>79 The skill function sk (xk ) can then be used to make predictions for player k’s scores in future games. [sent-538, score-0.36]
</p><p>80 1)2 consists of player scores for four game types (“HeadToHead”, “FreeForAll”, “SmallTeams” and “LargeTeams”) over a total of 6,465 players. [sent-542, score-0.363]
</p><p>81 We initialized all player skill functions to sk (xk ) = Φ(xk ; µ, β2 ). [sent-545, score-0.326]
</p><p>82 3 For each of these game modes, we applied the DSP algorithm as described above in order to obtain updates for the player skill functions sk (xk ). [sent-555, score-0.455]
</p><p>83 Before each game, we can predict the team performances using the player skills learned thus ∗ far via the rule xk = arg max ∂xk sk (xk ) . [sent-558, score-0.553]
</p><p>84 For each game, the set of team performances is then xk  ∗ deﬁned by the ordering of teams once the game is over, where we add the predicted player scores xk together for each team and sorting the resulting sums in ascending order. [sent-559, score-1.074]
</p><p>85 For any predicted set of team performances, an error is incurred for that game if two teams for that game were mis-ranked N−1 such that the number of errors for a given game is ∑n=1 ∑N [Rn ≤ Rm ] ∧ [Rtrue > Rtrue ]. [sent-560, score-0.681]
</p><p>86 Our model represent both statistical dependence relationships and stochastic orderings of variables in the model such as team performances and individual player scores. [sent-588, score-0.459]
</p><p>87 We described the DSP algorithm for computing such derivatives/ﬁnite differences by passing messages in the CDN where each message corresponds to local derivatives of the joint CDF. [sent-598, score-0.248]
</p><p>88 The DSP algorithm allowed us to compute distributions over player scores given previous game outcomes while accounting for the team-based structure of the games, whereby we were able to show improved results over previous methods. [sent-601, score-0.363]
</p><p>89 As with the sum-product algorithm for factor graphs, if the graph contains cycles, then the derivative-sum-product is no longer guaranteed to yield the correct mixed derivatives of the joint CDF, so that messages may begin to ‘oscillate’ as they propagate around cycles in the graph. [sent-613, score-0.276]
</p><p>90 We can thus view the functions µs→α (x) as messages being passed from each function node s s ∈ N (α) in the CDN to a neighboring variable node α. [sent-643, score-0.32]
</p><p>91 β Finally, to compute the messages µβ→s (x) from variables to functions, we can write each of the functions Tβ xτs as a product such that β Tβ xτs = β  ∏  s′ ∈N (β)\s  Ts′ xτβ , s′  where Ts′ is deﬁned identically to Ts above but for function node s′ . [sent-648, score-0.259]
</p><p>92 Thus, to compute messages from variables to functions, we simply take the product of all incoming messages except for the message coming from the destination function node. [sent-650, score-0.355]
</p><p>93 We see here that the process of differentiation in a CDN can be implemented as an algorithm in which we pass messages µα→s from variables to neighboring function nodes and messages µs→α from functions to neighboring variable nodes. [sent-652, score-0.512]
</p><p>94 Messages can be computed recursively from one another as described above: we start from an arbitrary root variable node α and propagate messages up from leaf nodes to the root node. [sent-653, score-0.344]
</p><p>95 As in the sum-product algorithm, leaf variable nodes α′ send the message µα′ →s (x) = 1 while leaf function nodes φs (xα′ ) send the message µs→α′ (x) = φs (xα′ ). [sent-654, score-0.28]
</p><p>96 The message-passing algorithm proceeds until messages have been propagated along every edge in the network and the root variable node has received all incoming messages from the remainder of the network. [sent-655, score-0.366]
</p><p>97 Once all messages have been sent, we can obtain the probability density of the 343  H UANG AND F REY  variables in the graph from differentiating the product of incoming messages at the root node α, so that P(x) = ∂xα  ∏  µs→α (x) . [sent-656, score-0.432]
</p><p>98 • Initialize for each player score node Xk :  µXk →gn (xk ) = sk (xk ), λXk →gn (xk ) = ∂xk sk (xk ) . [sent-670, score-0.374]
</p><p>99 • Pass messages from each team performance node Rn to neighboring function nodes gn : µRn →gn (r, x) = µhn−1,n →Rn (r, x)µhn,n+1 →Rn (r, x),  λRn →gn (r, x) = λhn−1,n →Rn (r, x)µhn,n+1 →Rn (r, x)  + µhn−1,n →Rn (r, x)λhn,n+1 →Rn (r, x). [sent-678, score-0.591]
</p><p>100 • Update player skill functions sk (xk ) using the multiplicative rule sk (xk ) ← sk (xk )µgn →Xk (x, r). [sent-681, score-0.456]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cdn', 0.589), ('xa', 0.352), ('cdf', 0.236), ('xc', 0.209), ('xs', 0.191), ('team', 0.187), ('cdns', 0.186), ('player', 0.185), ('xb', 0.16), ('dsp', 0.156), ('messages', 0.133), ('cdfs', 0.129), ('game', 0.129), ('gn', 0.11), ('teams', 0.107), ('rey', 0.091), ('xk', 0.091), ('erivative', 0.087), ('umulative', 0.087), ('players', 0.082), ('rn', 0.081), ('xw', 0.077), ('independence', 0.075), ('istribution', 0.074), ('nodes', 0.07), ('uang', 0.07), ('etworks', 0.067), ('sk', 0.065), ('skill', 0.061), ('multiplayer', 0.061), ('node', 0.059), ('ordinal', 0.058), ('rk', 0.052), ('xv', 0.05), ('scores', 0.049), ('games', 0.044), ('lim', 0.042), ('cumulative', 0.042), ('joint', 0.042), ('conditional', 0.04), ('cutpoints', 0.038), ('richardson', 0.037), ('hn', 0.037), ('message', 0.037), ('graph', 0.036), ('graphical', 0.036), ('derivatives', 0.036), ('lgorithm', 0.035), ('xpt', 0.034), ('halo', 0.032), ('rtt', 0.032), ('neighboring', 0.032), ('marginal', 0.031), ('differentiation', 0.03), ('marginalization', 0.029), ('trueskilltm', 0.029), ('tt', 0.029), ('mixed', 0.029), ('separated', 0.028), ('variables', 0.027), ('pdfs', 0.027), ('ot', 0.026), ('performances', 0.025), ('product', 0.025), ('implied', 0.023), ('copula', 0.023), ('ordering', 0.023), ('elo', 0.023), ('graphs', 0.022), ('leaf', 0.022), ('variable', 0.022), ('separation', 0.022), ('rl', 0.021), ('disjoint', 0.021), ('pt', 0.02), ('separates', 0.02), ('orderings', 0.02), ('latent', 0.019), ('freeforall', 0.019), ('root', 0.019), ('pass', 0.018), ('derivative', 0.018), ('undirected', 0.018), ('inference', 0.018), ('modeled', 0.016), ('xn', 0.016), ('directed', 0.016), ('wish', 0.016), ('drton', 0.016), ('bipartite', 0.016), ('bivariate', 0.016), ('spirtes', 0.016), ('constraints', 0.016), ('unobserved', 0.016), ('toronto', 0.015), ('functions', 0.015), ('stochastic', 0.015), ('headtohead', 0.015), ('pdf', 0.015), ('marginalizing', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="21-tfidf-1" href="./jmlr-2011-Cumulative_Distribution_Networks_and_the_Derivative-sum-product_Algorithm%3A_Models_and_Inference_for_Cumulative_Distribution_Functions_on_Graphs.html">21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</a></p>
<p>Author: Jim C. Huang, Brendan J. Frey</p><p>Abstract: We present a class of graphical models for directly representing the joint cumulative distribution function (CDF) of many random variables, called cumulative distribution networks (CDNs). Unlike graphs for probability density and mass functions, for CDFs the marginal probabilities for any subset of variables are obtained by computing limits of functions in the model, and conditional probabilities correspond to computing mixed derivatives. We will show that the conditional independence properties in a CDN are distinct from the conditional independence properties of directed, undirected and factor graphs, but include the conditional independence properties of bi-directed graphs. In order to perform inference in such models, we describe the ‘derivative-sum-product’ (DSP) message-passing algorithm in which messages correspond to derivatives of the joint CDF. We will then apply CDNs to the problem of learning to rank players in multiplayer team-based games and suggest several future directions for research. Keywords: graphical models, cumulative distribution function, message-passing algorithm, inference</p><p>2 0.14894925 <a title="21-tfidf-2" href="./jmlr-2011-A_Bayesian_Approximation_Method_for_Online_Ranking.html">2 jmlr-2011-A Bayesian Approximation Method for Online Ranking</a></p>
<p>Author: Ruby C. Weng, Chih-Jen Lin</p><p>Abstract: This paper describes a Bayesian approximation method to obtain online ranking algorithms for games with multiple teams and multiple players. Recently for Internet games large online ranking systems are much needed. We consider game models in which a k-team game is treated as several two-team games. By approximating the expectation of teams’ (or players’) performances, we derive simple analytic update rules. These update rules, without numerical integrations, are very easy to interpret and implement. Experiments on game data show that the accuracy of our approach is competitive with state of the art systems such as TrueSkill, but the running time as well as the code is much shorter. Keywords: Bayesian inference, rating system, Bradley-Terry model, Thurstone-Mosteller model, Plackett-Luce model</p><p>3 0.1202815 <a title="21-tfidf-3" href="./jmlr-2011-Double_Updating_Online_Learning.html">28 jmlr-2011-Double Updating Online Learning</a></p>
<p>Author: Peilin Zhao, Steven C.H. Hoi, Rong Jin</p><p>Abstract: In most kernel based online learning algorithms, when an incoming instance is misclassiﬁed, it will be added into the pool of support vectors and assigned with a weight, which often remains unchanged during the rest of the learning process. This is clearly insufﬁcient since when a new support vector is added, we generally expect the weights of the other existing support vectors to be updated in order to reﬂect the inﬂuence of the added support vector. In this paper, we propose a new online learning method, termed Double Updating Online Learning, or DUOL for short, that explicitly addresses this problem. Instead of only assigning a ﬁxed weight to the misclassiﬁed example received at the current trial, the proposed online learning algorithm also tries to update the weight for one of the existing support vectors. We show that the mistake bound can be improved by the proposed online learning method. We conduct an extensive set of empirical evaluations for both binary and multi-class online learning tasks. The experimental results show that the proposed technique is considerably more effective than the state-of-the-art online learning algorithms. The source code is available to public at http://www.cais.ntu.edu.sg/˜chhoi/DUOL/. Keywords: online learning, kernel method, support vector machines, maximum margin learning, classiﬁcation</p><p>4 0.051279098 <a title="21-tfidf-4" href="./jmlr-2011-Faster_Algorithms_for_Max-Product_Message-Passing.html">34 jmlr-2011-Faster Algorithms for Max-Product Message-Passing</a></p>
<p>Author: Julian J. McAuley, TibĂŠrio S. Caetano</p><p>Abstract: Maximum A Posteriori inference in graphical models is often solved via message-passing algorithms, such as the junction-tree algorithm or loopy belief-propagation. The exact solution to this problem is well-known to be exponential in the size of the maximal cliques of the triangulated model, while approximate inference is typically exponential in the size of the modelĂ˘&euro;&trade;s factors. In this paper, we take advantage of the fact that many models have maximal cliques that are larger than their constituent factors, and also of the fact that many factors consist only of latent variables (i.e., they do not depend on an observation). This is a common case in a wide variety of applications that deal with grid-, tree-, and ring-structured models. In such cases, we are able to decrease the exponent of complexity for message-passing by 0.5 for both exact and approximate inference. We demonstrate that message-passing operations in such models are equivalent to some variant of matrix multiplication in the tropical semiring, for which we offer an O(N 2.5 ) expected-case solution. Keywords: graphical models, belief-propagation, tropical matrix multiplication</p><p>5 0.049630702 <a title="21-tfidf-5" href="./jmlr-2011-Learning_Latent_Tree_Graphical_Models.html">54 jmlr-2011-Learning Latent Tree Graphical Models</a></p>
<p>Author: Myung Jin Choi, Vincent Y. F. Tan, Animashree Anandkumar, Alan S. Willsky</p><p>Abstract: We study the problem of learning a latent tree graphical model where samples are available only from a subset of variables. We propose two consistent and computationally efﬁcient algorithms for learning minimal latent trees, that is, trees without any redundant hidden nodes. Unlike many existing methods, the observed nodes (or variables) are not constrained to be leaf nodes. Our algorithms can be applied to both discrete and Gaussian random variables and our learned models are such that all the observed and latent variables have the same domain (state space). Our ﬁrst algorithm, recursive grouping, builds the latent tree recursively by identifying sibling groups using so-called information distances. One of the main contributions of this work is our second algorithm, which we refer to as CLGrouping. CLGrouping starts with a pre-processing procedure in which a tree over the observed variables is constructed. This global step groups the observed nodes that are likely to be close to each other in the true latent tree, thereby guiding subsequent recursive grouping (or equivalent procedures such as neighbor-joining) on much smaller subsets of variables. This results in more accurate and efﬁcient learning of latent trees. We also present regularized versions of our algorithms that learn latent tree approximations of arbitrary distributions. We compare the proposed algorithms to other methods by performing extensive numerical experiments on various latent tree graphical models such as hidden Markov models and star graphs. In addition, we demonstrate the applicability of our methods on real-world data sets by modeling the dependency structure of monthly stock returns in the S&P; index and of the words in the 20 newsgroups data set. Keywords: graphical models, Markov random ﬁelds, hidden variables, latent tree models, structure learning c 2011 Myung Jin Choi, Vincent Y. F. Tan, Animashree Anandkumar and Alan S. Willsky. C HOI , TAN , A NANDKUMAR AND W ILLSKY</p><p>6 0.044260982 <a title="21-tfidf-6" href="./jmlr-2011-Domain_Decomposition_Approach_for_Fast_Gaussian_Process_Regression_of_Large_Spatial_Data_Sets.html">27 jmlr-2011-Domain Decomposition Approach for Fast Gaussian Process Regression of Large Spatial Data Sets</a></p>
<p>7 0.041345935 <a title="21-tfidf-7" href="./jmlr-2011-X-Armed_Bandits.html">104 jmlr-2011-X-Armed Bandits</a></p>
<p>8 0.040695816 <a title="21-tfidf-8" href="./jmlr-2011-Unsupervised_Supervised_Learning_II%3A_Margin-Based_Classification_Without_Labels.html">100 jmlr-2011-Unsupervised Supervised Learning II: Margin-Based Classification Without Labels</a></p>
<p>9 0.039531525 <a title="21-tfidf-9" href="./jmlr-2011-A_Simpler_Approach_to_Matrix_Completion.html">6 jmlr-2011-A Simpler Approach to Matrix Completion</a></p>
<p>10 0.036827926 <a title="21-tfidf-10" href="./jmlr-2011-Adaptive_Exact_Inference_in_Graphical_Models.html">7 jmlr-2011-Adaptive Exact Inference in Graphical Models</a></p>
<p>11 0.036174111 <a title="21-tfidf-11" href="./jmlr-2011-Efficient_Structure_Learning_of_Bayesian_Networks_using_Constraints.html">30 jmlr-2011-Efficient Structure Learning of Bayesian Networks using Constraints</a></p>
<p>12 0.035639498 <a title="21-tfidf-12" href="./jmlr-2011-Weisfeiler-Lehman_Graph_Kernels.html">103 jmlr-2011-Weisfeiler-Lehman Graph Kernels</a></p>
<p>13 0.034378942 <a title="21-tfidf-13" href="./jmlr-2011-Forest_Density_Estimation.html">35 jmlr-2011-Forest Density Estimation</a></p>
<p>14 0.033505637 <a title="21-tfidf-14" href="./jmlr-2011-Parallel_Algorithm_for_Learning_Optimal_Bayesian_Network_Structure.html">75 jmlr-2011-Parallel Algorithm for Learning Optimal Bayesian Network Structure</a></p>
<p>15 0.03292555 <a title="21-tfidf-15" href="./jmlr-2011-Online_Learning_in_Case_of_Unbounded_Losses_Using_Follow_the_Perturbed_Leader_Algorithm.html">73 jmlr-2011-Online Learning in Case of Unbounded Losses Using Follow the Perturbed Leader Algorithm</a></p>
<p>16 0.028793512 <a title="21-tfidf-16" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>17 0.028752023 <a title="21-tfidf-17" href="./jmlr-2011-Internal_Regret_with_Partial_Monitoring%3A_Calibration-Based_Optimal_Algorithms.html">45 jmlr-2011-Internal Regret with Partial Monitoring: Calibration-Based Optimal Algorithms</a></p>
<p>18 0.02685819 <a title="21-tfidf-18" href="./jmlr-2011-Approximate_Marginals_in_Latent_Gaussian_Models.html">11 jmlr-2011-Approximate Marginals in Latent Gaussian Models</a></p>
<p>19 0.026224762 <a title="21-tfidf-19" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>20 0.025591351 <a title="21-tfidf-20" href="./jmlr-2011-An_Asymptotic_Behaviour_of_the_Marginal_Likelihood_for_General_Markov_Models.html">9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.135), (1, -0.002), (2, -0.059), (3, -0.008), (4, 0.087), (5, 0.041), (6, 0.058), (7, 0.058), (8, 0.07), (9, 0.141), (10, -0.096), (11, 0.021), (12, 0.197), (13, -0.056), (14, 0.217), (15, 0.199), (16, -0.066), (17, 0.283), (18, 0.37), (19, -0.153), (20, 0.201), (21, 0.105), (22, 0.107), (23, 0.085), (24, -0.165), (25, -0.123), (26, -0.077), (27, -0.068), (28, 0.03), (29, 0.056), (30, 0.04), (31, -0.096), (32, 0.003), (33, 0.028), (34, 0.022), (35, -0.032), (36, -0.015), (37, 0.031), (38, -0.001), (39, 0.011), (40, -0.015), (41, -0.008), (42, 0.01), (43, 0.02), (44, -0.022), (45, 0.015), (46, -0.038), (47, -0.05), (48, -0.035), (49, -0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95929891 <a title="21-lsi-1" href="./jmlr-2011-Cumulative_Distribution_Networks_and_the_Derivative-sum-product_Algorithm%3A_Models_and_Inference_for_Cumulative_Distribution_Functions_on_Graphs.html">21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</a></p>
<p>Author: Jim C. Huang, Brendan J. Frey</p><p>Abstract: We present a class of graphical models for directly representing the joint cumulative distribution function (CDF) of many random variables, called cumulative distribution networks (CDNs). Unlike graphs for probability density and mass functions, for CDFs the marginal probabilities for any subset of variables are obtained by computing limits of functions in the model, and conditional probabilities correspond to computing mixed derivatives. We will show that the conditional independence properties in a CDN are distinct from the conditional independence properties of directed, undirected and factor graphs, but include the conditional independence properties of bi-directed graphs. In order to perform inference in such models, we describe the ‘derivative-sum-product’ (DSP) message-passing algorithm in which messages correspond to derivatives of the joint CDF. We will then apply CDNs to the problem of learning to rank players in multiplayer team-based games and suggest several future directions for research. Keywords: graphical models, cumulative distribution function, message-passing algorithm, inference</p><p>2 0.82391644 <a title="21-lsi-2" href="./jmlr-2011-A_Bayesian_Approximation_Method_for_Online_Ranking.html">2 jmlr-2011-A Bayesian Approximation Method for Online Ranking</a></p>
<p>Author: Ruby C. Weng, Chih-Jen Lin</p><p>Abstract: This paper describes a Bayesian approximation method to obtain online ranking algorithms for games with multiple teams and multiple players. Recently for Internet games large online ranking systems are much needed. We consider game models in which a k-team game is treated as several two-team games. By approximating the expectation of teams’ (or players’) performances, we derive simple analytic update rules. These update rules, without numerical integrations, are very easy to interpret and implement. Experiments on game data show that the accuracy of our approach is competitive with state of the art systems such as TrueSkill, but the running time as well as the code is much shorter. Keywords: Bayesian inference, rating system, Bradley-Terry model, Thurstone-Mosteller model, Plackett-Luce model</p><p>3 0.39353585 <a title="21-lsi-3" href="./jmlr-2011-Double_Updating_Online_Learning.html">28 jmlr-2011-Double Updating Online Learning</a></p>
<p>Author: Peilin Zhao, Steven C.H. Hoi, Rong Jin</p><p>Abstract: In most kernel based online learning algorithms, when an incoming instance is misclassiﬁed, it will be added into the pool of support vectors and assigned with a weight, which often remains unchanged during the rest of the learning process. This is clearly insufﬁcient since when a new support vector is added, we generally expect the weights of the other existing support vectors to be updated in order to reﬂect the inﬂuence of the added support vector. In this paper, we propose a new online learning method, termed Double Updating Online Learning, or DUOL for short, that explicitly addresses this problem. Instead of only assigning a ﬁxed weight to the misclassiﬁed example received at the current trial, the proposed online learning algorithm also tries to update the weight for one of the existing support vectors. We show that the mistake bound can be improved by the proposed online learning method. We conduct an extensive set of empirical evaluations for both binary and multi-class online learning tasks. The experimental results show that the proposed technique is considerably more effective than the state-of-the-art online learning algorithms. The source code is available to public at http://www.cais.ntu.edu.sg/˜chhoi/DUOL/. Keywords: online learning, kernel method, support vector machines, maximum margin learning, classiﬁcation</p><p>4 0.18728617 <a title="21-lsi-4" href="./jmlr-2011-Faster_Algorithms_for_Max-Product_Message-Passing.html">34 jmlr-2011-Faster Algorithms for Max-Product Message-Passing</a></p>
<p>Author: Julian J. McAuley, TibĂŠrio S. Caetano</p><p>Abstract: Maximum A Posteriori inference in graphical models is often solved via message-passing algorithms, such as the junction-tree algorithm or loopy belief-propagation. The exact solution to this problem is well-known to be exponential in the size of the maximal cliques of the triangulated model, while approximate inference is typically exponential in the size of the modelĂ˘&euro;&trade;s factors. In this paper, we take advantage of the fact that many models have maximal cliques that are larger than their constituent factors, and also of the fact that many factors consist only of latent variables (i.e., they do not depend on an observation). This is a common case in a wide variety of applications that deal with grid-, tree-, and ring-structured models. In such cases, we are able to decrease the exponent of complexity for message-passing by 0.5 for both exact and approximate inference. We demonstrate that message-passing operations in such models are equivalent to some variant of matrix multiplication in the tropical semiring, for which we offer an O(N 2.5 ) expected-case solution. Keywords: graphical models, belief-propagation, tropical matrix multiplication</p><p>5 0.18588634 <a title="21-lsi-5" href="./jmlr-2011-A_Simpler_Approach_to_Matrix_Completion.html">6 jmlr-2011-A Simpler Approach to Matrix Completion</a></p>
<p>Author: Benjamin Recht   </p><p>Abstract: This paper provides the best bounds to date on the number of randomly sampled entries required to reconstruct an unknown low-rank matrix. These results improve on prior work by Cand` s and e Recht (2009), Cand` s and Tao (2009), and Keshavan et al. (2009). The reconstruction is accome plished by minimizing the nuclear norm, or sum of the singular values, of the hidden matrix subject to agreement with the provided entries. If the underlying matrix satisﬁes a certain incoherence condition, then the number of entries required is equal to a quadratic logarithmic factor times the number of parameters in the singular value decomposition. The proof of this assertion is short, self contained, and uses very elementary analysis. The novel techniques herein are based on recent work in quantum information theory. Keywords: matrix completion, low-rank matrices, convex optimization, nuclear norm minimization, random matrices, operator Chernoff bound, compressed sensing</p><p>6 0.18530576 <a title="21-lsi-6" href="./jmlr-2011-Learning_Latent_Tree_Graphical_Models.html">54 jmlr-2011-Learning Latent Tree Graphical Models</a></p>
<p>7 0.17401546 <a title="21-lsi-7" href="./jmlr-2011-Adaptive_Exact_Inference_in_Graphical_Models.html">7 jmlr-2011-Adaptive Exact Inference in Graphical Models</a></p>
<p>8 0.17377818 <a title="21-lsi-8" href="./jmlr-2011-Unsupervised_Supervised_Learning_II%3A_Margin-Based_Classification_Without_Labels.html">100 jmlr-2011-Unsupervised Supervised Learning II: Margin-Based Classification Without Labels</a></p>
<p>9 0.15742713 <a title="21-lsi-9" href="./jmlr-2011-An_Asymptotic_Behaviour_of_the_Marginal_Likelihood_for_General_Markov_Models.html">9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</a></p>
<p>10 0.15260252 <a title="21-lsi-10" href="./jmlr-2011-Domain_Decomposition_Approach_for_Fast_Gaussian_Process_Regression_of_Large_Spatial_Data_Sets.html">27 jmlr-2011-Domain Decomposition Approach for Fast Gaussian Process Regression of Large Spatial Data Sets</a></p>
<p>11 0.1520208 <a title="21-lsi-11" href="./jmlr-2011-Weisfeiler-Lehman_Graph_Kernels.html">103 jmlr-2011-Weisfeiler-Lehman Graph Kernels</a></p>
<p>12 0.15115166 <a title="21-lsi-12" href="./jmlr-2011-Forest_Density_Estimation.html">35 jmlr-2011-Forest Density Estimation</a></p>
<p>13 0.14723009 <a title="21-lsi-13" href="./jmlr-2011-Parallel_Algorithm_for_Learning_Optimal_Bayesian_Network_Structure.html">75 jmlr-2011-Parallel Algorithm for Learning Optimal Bayesian Network Structure</a></p>
<p>14 0.13057296 <a title="21-lsi-14" href="./jmlr-2011-X-Armed_Bandits.html">104 jmlr-2011-X-Armed Bandits</a></p>
<p>15 0.12715955 <a title="21-lsi-15" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>16 0.12589191 <a title="21-lsi-16" href="./jmlr-2011-Online_Learning_in_Case_of_Unbounded_Losses_Using_Follow_the_Perturbed_Leader_Algorithm.html">73 jmlr-2011-Online Learning in Case of Unbounded Losses Using Follow the Perturbed Leader Algorithm</a></p>
<p>17 0.11959951 <a title="21-lsi-17" href="./jmlr-2011-Learning_Transformation_Models_for_Ranking_and_Survival_Analysis.html">56 jmlr-2011-Learning Transformation Models for Ranking and Survival Analysis</a></p>
<p>18 0.11913875 <a title="21-lsi-18" href="./jmlr-2011-Internal_Regret_with_Partial_Monitoring%3A_Calibration-Based_Optimal_Algorithms.html">45 jmlr-2011-Internal Regret with Partial Monitoring: Calibration-Based Optimal Algorithms</a></p>
<p>19 0.10781126 <a title="21-lsi-19" href="./jmlr-2011-Dirichlet_Process_Mixtures_of_Generalized_Linear_Models.html">24 jmlr-2011-Dirichlet Process Mixtures of Generalized Linear Models</a></p>
<p>20 0.10538527 <a title="21-lsi-20" href="./jmlr-2011-Convex_and_Network_Flow_Optimization_for_Structured_Sparsity.html">20 jmlr-2011-Convex and Network Flow Optimization for Structured Sparsity</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.071), (4, 0.08), (9, 0.018), (10, 0.039), (24, 0.057), (31, 0.065), (32, 0.015), (41, 0.039), (60, 0.011), (65, 0.012), (70, 0.015), (71, 0.012), (73, 0.029), (78, 0.042), (88, 0.37), (90, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71780783 <a title="21-lda-1" href="./jmlr-2011-Cumulative_Distribution_Networks_and_the_Derivative-sum-product_Algorithm%3A_Models_and_Inference_for_Cumulative_Distribution_Functions_on_Graphs.html">21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</a></p>
<p>Author: Jim C. Huang, Brendan J. Frey</p><p>Abstract: We present a class of graphical models for directly representing the joint cumulative distribution function (CDF) of many random variables, called cumulative distribution networks (CDNs). Unlike graphs for probability density and mass functions, for CDFs the marginal probabilities for any subset of variables are obtained by computing limits of functions in the model, and conditional probabilities correspond to computing mixed derivatives. We will show that the conditional independence properties in a CDN are distinct from the conditional independence properties of directed, undirected and factor graphs, but include the conditional independence properties of bi-directed graphs. In order to perform inference in such models, we describe the ‘derivative-sum-product’ (DSP) message-passing algorithm in which messages correspond to derivatives of the joint CDF. We will then apply CDNs to the problem of learning to rank players in multiplayer team-based games and suggest several future directions for research. Keywords: graphical models, cumulative distribution function, message-passing algorithm, inference</p><p>2 0.53064305 <a title="21-lda-2" href="./jmlr-2011-On_Equivalence_Relationships_Between_Classification_and_Ranking_Algorithms.html">71 jmlr-2011-On Equivalence Relationships Between Classification and Ranking Algorithms</a></p>
<p>Author: Şeyda Ertekin, Cynthia Rudin</p><p>Abstract: We demonstrate that there are machine learning algorithms that can achieve success for two separate tasks simultaneously, namely the tasks of classiﬁcation and bipartite ranking. This means that advantages gained from solving one task can be carried over to the other task, such as the ability to obtain conditional density estimates, and an order-of-magnitude reduction in computational time for training the algorithm. It also means that some algorithms are robust to the choice of evaluation metric used; they can theoretically perform well when performance is measured either by a misclassiﬁcation error or by a statistic of the ROC curve (such as the area under the curve). Speciﬁcally, we provide such an equivalence relationship between a generalization of Freund et al.’s RankBoost algorithm, called the “P-Norm Push,” and a particular cost-sensitive classiﬁcation algorithm that generalizes AdaBoost, which we call “P-Classiﬁcation.” We discuss and validate the potential beneﬁts of this equivalence relationship, and perform controlled experiments to understand P-Classiﬁcation’s empirical performance. There is no established equivalence relationship for logistic regression and its ranking counterpart, so we introduce a logistic-regression-style algorithm that aims in between classiﬁcation and ranking, and has promising experimental performance with respect to both tasks. Keywords: supervised classiﬁcation, bipartite ranking, area under the curve, rank statistics, boosting, logistic regression</p><p>3 0.35585418 <a title="21-lda-3" href="./jmlr-2011-A_Bayesian_Approximation_Method_for_Online_Ranking.html">2 jmlr-2011-A Bayesian Approximation Method for Online Ranking</a></p>
<p>Author: Ruby C. Weng, Chih-Jen Lin</p><p>Abstract: This paper describes a Bayesian approximation method to obtain online ranking algorithms for games with multiple teams and multiple players. Recently for Internet games large online ranking systems are much needed. We consider game models in which a k-team game is treated as several two-team games. By approximating the expectation of teams’ (or players’) performances, we derive simple analytic update rules. These update rules, without numerical integrations, are very easy to interpret and implement. Experiments on game data show that the accuracy of our approach is competitive with state of the art systems such as TrueSkill, but the running time as well as the code is much shorter. Keywords: Bayesian inference, rating system, Bradley-Terry model, Thurstone-Mosteller model, Plackett-Luce model</p><p>4 0.31125823 <a title="21-lda-4" href="./jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</a></p>
<p>Author: Vincent Y.F. Tan, Animashree Anandkumar, Alan S. Willsky</p><p>Abstract: The problem of learning forest-structured discrete graphical models from i.i.d. samples is considered. An algorithm based on pruning of the Chow-Liu tree through adaptive thresholding is proposed. It is shown that this algorithm is both structurally consistent and risk consistent and the error probability of structure learning decays faster than any polynomial in the number of samples under ﬁxed model size. For the high-dimensional scenario where the size of the model d and the number of edges k scale with the number of samples n, sufﬁcient conditions on (n, d, k) are given for the algorithm to satisfy structural and risk consistencies. In addition, the extremal structures for learning are identiﬁed; we prove that the independent (resp., tree) model is the hardest (resp., easiest) to learn using the proposed algorithm in terms of error rates for structure learning. Keywords: graphical models, forest distributions, structural consistency, risk consistency, method of types</p><p>5 0.30904904 <a title="21-lda-5" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>Author: Ricardo Henao, Ole Winther</p><p>Abstract: In this paper we consider sparse and identiﬁable linear latent variable (factor) and linear Bayesian network models for parsimonious analysis of multivariate data. We propose a computationally efﬁcient method for joint parameter and model inference, and model comparison. It consists of a fully Bayesian hierarchy for sparse models using slab and spike priors (two-component δ-function and continuous mixtures), non-Gaussian latent factors and a stochastic search over the ordering of the variables. The framework, which we call SLIM (Sparse Linear Identiﬁable Multivariate modeling), is validated and bench-marked on artiﬁcial and real biological data sets. SLIM is closest in spirit to LiNGAM (Shimizu et al., 2006), but differs substantially in inference, Bayesian network structure learning and model comparison. Experimentally, SLIM performs equally well or better than LiNGAM with comparable computational complexity. We attribute this mainly to the stochastic search strategy used, and to parsimony (sparsity and identiﬁability), which is an explicit part of the model. We propose two extensions to the basic i.i.d. linear framework: non-linear dependence on observed variables, called SNIM (Sparse Non-linear Identiﬁable Multivariate modeling) and allowing for correlations between latent variables, called CSLIM (Correlated SLIM), for the temporal and/or spatial data. The source code and scripts are available from http://cogsys.imm.dtu.dk/slim/. Keywords: parsimony, sparsity, identiﬁability, factor models, linear Bayesian networks</p><p>6 0.3089996 <a title="21-lda-6" href="./jmlr-2011-Learning_with_Structured_Sparsity.html">59 jmlr-2011-Learning with Structured Sparsity</a></p>
<p>7 0.30847839 <a title="21-lda-7" href="./jmlr-2011-Learning_Latent_Tree_Graphical_Models.html">54 jmlr-2011-Learning Latent Tree Graphical Models</a></p>
<p>8 0.30399653 <a title="21-lda-8" href="./jmlr-2011-The_Sample_Complexity_of_Dictionary_Learning.html">91 jmlr-2011-The Sample Complexity of Dictionary Learning</a></p>
<p>9 0.29977891 <a title="21-lda-9" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>10 0.29905671 <a title="21-lda-10" href="./jmlr-2011-Minimum_Description_Length_Penalization_for_Group_and_Multi-Task_Sparse_Learning.html">64 jmlr-2011-Minimum Description Length Penalization for Group and Multi-Task Sparse Learning</a></p>
<p>11 0.29645976 <a title="21-lda-11" href="./jmlr-2011-Adaptive_Exact_Inference_in_Graphical_Models.html">7 jmlr-2011-Adaptive Exact Inference in Graphical Models</a></p>
<p>12 0.29514012 <a title="21-lda-12" href="./jmlr-2011-X-Armed_Bandits.html">104 jmlr-2011-X-Armed Bandits</a></p>
<p>13 0.29475608 <a title="21-lda-13" href="./jmlr-2011-Operator_Norm_Convergence_of_Spectral_Clustering_on_Level_Sets.html">74 jmlr-2011-Operator Norm Convergence of Spectral Clustering on Level Sets</a></p>
<p>14 0.29408348 <a title="21-lda-14" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<p>15 0.29286623 <a title="21-lda-15" href="./jmlr-2011-Producing_Power-Law_Distributions_and_Damping_Word_Frequencies_with_Two-Stage_Language_Models.html">78 jmlr-2011-Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models</a></p>
<p>16 0.29283723 <a title="21-lda-16" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>17 0.29135311 <a title="21-lda-17" href="./jmlr-2011-Efficient_Learning_with_Partially_Observed_Attributes.html">29 jmlr-2011-Efficient Learning with Partially Observed Attributes</a></p>
<p>18 0.29044724 <a title="21-lda-18" href="./jmlr-2011-An_Asymptotic_Behaviour_of_the_Marginal_Likelihood_for_General_Markov_Models.html">9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</a></p>
<p>19 0.28990155 <a title="21-lda-19" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>20 0.28963324 <a title="21-lda-20" href="./jmlr-2011-A_Family_of_Simple_Non-Parametric_Kernel_Learning_Algorithms.html">4 jmlr-2011-A Family of Simple Non-Parametric Kernel Learning Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
