<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>27 jmlr-2011-Domain Decomposition Approach for Fast Gaussian Process Regression of Large Spatial Data Sets</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-27" href="#">jmlr2011-27</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>27 jmlr-2011-Domain Decomposition Approach for Fast Gaussian Process Regression of Large Spatial Data Sets</h1>
<br/><p>Source: <a title="jmlr-2011-27-pdf" href="http://jmlr.org/papers/volume12/park11a/park11a.pdf">pdf</a></p><p>Author: Chiwoo Park, Jianhua Z. Huang, Yu Ding</p><p>Abstract: Gaussian process regression is a ﬂexible and powerful tool for machine learning, but the high computational complexity hinders its broader applications. In this paper, we propose a new approach for fast computation of Gaussian process regression with a focus on large spatial data sets. The approach decomposes the domain of a regression function into small subdomains and infers a local piece of the regression function for each subdomain. We explicitly address the mismatch problem of the local pieces on the boundaries of neighboring subdomains by imposing continuity constraints. The new approach has comparable or better computation complexity as other competing methods, but it is easier to be parallelized for faster computation. Moreover, the method can be adaptive to non-stationary features because of its local nature and, in particular, its use of different hyperparameters of the covariance function for different local regions. We illustrate application of the method and demonstrate its advantages over existing methods using two synthetic data sets and two real spatial data sets. Keywords: domain decomposition, boundary value problem, Gaussian process regression, parallel computation, spatial prediction</p><p>Reference: <a title="jmlr-2011-27-reference" href="../jmlr2011_reference/jmlr-2011-Domain_Decomposition_Approach_for_Fast_Gaussian_Process_Regression_of_Large_Spatial_Data_Sets_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The approach decomposes the domain of a regression function into small subdomains and infers a local piece of the regression function for each subdomain. [sent-10, score-0.33]
</p><p>2 We explicitly address the mismatch problem of the local pieces on the boundaries of neighboring subdomains by imposing continuity constraints. [sent-11, score-0.425]
</p><p>3 One way to implement this idea, called local kriging, is to decompose the entire domain into smaller subdomains and to predict at a test site using the training points only in the subdomain which the test site belongs to. [sent-43, score-0.429]
</p><p>4 It is well known that local kriging suffers from having discontinuities in prediction on the boundaries of subdomains. [sent-44, score-0.258]
</p><p>5 Being aware of the advantages and disadvantages of the local kriging along with computational limitations of the averaging-based localized regression, we propose a new local kriging approach that explicitly addresses the problem of discontinuity in prediction on the boundaries of subdomains. [sent-54, score-0.413]
</p><p>6 x∗ x∗  (1)  The predictive mean kt (σ2 I + Kxx )−1 y gives the point prediction of f (x) at location x∗ , whose x∗ uncertainty is measured by the predictive variance k∗∗ − kt (σ2 I + Kxx )−1 kx∗ . [sent-98, score-0.462]
</p><p>7 x∗ x∗  Minimize A∈RN×N  (4)  The ﬁrst order necessary condition (FONC) for solving (4) is dz[A] = 2(σ2 I + Kxx )Akx∗ kt − 2kx∗ kt = 0. [sent-111, score-0.328]
</p><p>8 Consider an initial local problem as follows: for x∗ ∈ Ω j , Minimize n ×n A j ∈R  j  j  kt j ∗ Atj (σ2 I + Kx j x j )A j kx j ∗ − 2kt j ∗ Atj kx j ∗ , x x j  (6)  where we introduced the subdomain-dependent noise variance σ2 . [sent-131, score-1.234]
</p><p>9 The minimizer A j = (σ2 I + j j Kx j x j )−1 provides a local predictor, µ j (x∗ ) = kt j ∗ Atj y j , for x∗ ∈ Ω j . [sent-132, score-0.244]
</p><p>10 As we mentioned in the introduction, the above local kriging will suffer from discontinuities in prediction on boundaries of subdomains. [sent-135, score-0.258]
</p><p>11 Suppose that two neighboring subdomains Ω j and Ωk have a common boundary Γ jk := Ω j ∩ Ωk , where Ω j means the closure of Ω j . [sent-142, score-0.467]
</p><p>12 Using kx j ◦ as the abbreviation of kx j x◦ , we have discontinuities on Γ jk , that is, kt j ◦ Atj y j = kt k ◦ Atk yk for x◦ ∈ Γ jk . [sent-143, score-1.708]
</p><p>13 To ﬁx the problem, we impose continuity constraints on subdomain boundaries when combining the local predictors. [sent-145, score-0.251]
</p><p>14 Speciﬁcally, we impose (Continuity) kt j ◦ Atj y j = kt k ◦ Atk yk for x◦ ∈ Γ jk . [sent-146, score-0.53]
</p><p>15 x x This continuity requirement implies that two mean predictions obtained from local predictors of two neighboring subdomains are the same on the common subdomain boundary. [sent-147, score-0.452]
</p><p>16 To incorporate the continuity condition to the local kriging problems, deﬁne r jk (x◦ ) as a consistent prediction at x◦ on Γ jk . [sent-150, score-0.594]
</p><p>17 The continuity condition is converted to the following two separate conditions: kt j ◦ Atj y j = r jk (x◦ ) and kt k ◦ Atk yk = r jk (x◦ ) for x◦ ∈ Γ jk . [sent-151, score-0.937]
</p><p>18 x x Adding these conditions as constraints, we revise the initial local problem (6) to the following constrained local problem: for x∗ ∈ Ω j LP(j) :  Minimize kt j ∗ Atj (σ2 I + Kx j x j )A j kx j ∗ − 2kt j ∗ Atj kx j ∗ x x j n ×n A j ∈R  j  j  s. [sent-152, score-1.296]
</p><p>19 kt j ◦ Atj y j = r jk (x◦ ) x  for x◦ ∈ Γ jk and k ∈ N( j),  (7)  where N( j) := {k : Ωk is next to Ω j }. [sent-154, score-0.534]
</p><p>20 Note that r jk (x◦ ) is a function of x◦ and is referred to as a boundary value function on Γ jk . [sent-155, score-0.434]
</p><p>21 We ensure the continuity of the prediction across the boundary Γ jk by using a common boundary value function r jk for two neighboring subdomains Ω j and Ωk . [sent-156, score-0.789]
</p><p>22 Since the solution depends on a set of r jk ’s, denoted collectively as r j = {r jk ; k ∈ N( j)}, we denote the solution of (7) as A j (r j ). [sent-158, score-0.37]
</p><p>23 The predictive variance at a boundary point x◦ is given by the objective function of (7), which depends on r j and can be written as kt j ◦ A j (r j )t (σ2 I + Kx j x j )A j (r j )kx j ◦ − 2kt j ◦ A j (r j )t kx j ◦ . [sent-162, score-0.772]
</p><p>24 x x j To obtain the collection of all boundary value functions, {r j }m , we solve the following optimizaj=1 tion problem m  Minimize m {r j } j=1  ∑ ∑ ∑  kt j ◦ A j (r j )t (σ2 I + Kx j x j )A j (r j )kx j ◦ − 2kt j ◦ A j (r j )t kx j ◦ . [sent-163, score-0.714]
</p><p>25 x x j  j=1 k∈N( j) x◦ ∈Γ jk  1702  (8)  D OMAIN D ECOMPOSITION FOR FAST G AUSSIAN P ROCESS R EGRESSION  Note that we cannot solve optimization over each r j separately since r jk in r j is equivalent to rk j in rk so the optimization over r j is essentially linked to the optimization over rk . [sent-164, score-0.415]
</p><p>26 To summarize, we reformulate the spatial prediction problem as a collection of local prediction optimization problems, and impose continuity restrictions to these local problems. [sent-168, score-0.307]
</p><p>27 Numerical Algorithm Based on Domain Decomposition To solve (7) and (8) numerically, we make one simpliﬁcation that restricts the boundary value functions r jk ’s to be polynomials of a certain degree. [sent-172, score-0.268]
</p><p>28 Let r jk be a p × 1 vector that denotes the boundary function r jk evaluated at the p interpolation points. [sent-177, score-0.434]
</p><p>29 Then r jk (x◦ ) can be written as a linear combination r jk (x◦ ) = T jk (x◦ )t r jk , (9) where T jk (x◦ ) is a p×1 vector with the values of p Lagrange basis polynomials at x◦ as its elements. [sent-178, score-0.944]
</p><p>30 Plugging (9) into (7), the local prediction problem becomes for x∗ ∈ Ω j , LP(j) :  Minimize kt j ∗ Atj (σ2 I + Kx j x j )A j kx j ∗ − 2kt j ∗ Atj kx j ∗ x x j n ×n A j ∈R  j  j  s. [sent-179, score-1.252]
</p><p>31 kt j ◦ Atj y j = T jk (x◦ )t r jk x  for x◦ ∈ Γ jk and k ∈ N( j). [sent-181, score-0.719]
</p><p>32 One way to handle these constraints is to merge the inﬁnitely many constraints into one constraint by considering the following integral equation: Γ jk  [kt j ◦ Atj y j − T jk (x◦ )t r jk ]2 dx◦ = 0. [sent-183, score-0.555]
</p><p>33 We thus adopt another simpliﬁcation, which is to check the constraint only at q uniformly spaced points on Γ jk ; these constraint-checking points on a boundary are referred to as control points. [sent-188, score-0.309]
</p><p>34 Although this approach does not guarantee that the continuity constraint is met at all points on Γ jk , we ﬁnd that the difference of kt j ◦ Atj y j and x 1703  PARK , H UANG AND D ING  r jk (x◦ ) is small for all x◦ on Γ jk when q is chosen to be reasonably large; see Section 6. [sent-189, score-0.775]
</p><p>35 Speciﬁcally, let xb denote the q uniformly spaced points on Γ jk . [sent-191, score-0.415]
</p><p>36 Evaluate kx j ◦ and T jk (x◦ ) jk when x◦ is taken to be an element of xb and denote the results collectively as the n j × q matrix jk Kx j xb and the q × p matrix T jk , respectively. [sent-192, score-1.648]
</p><p>37 Then, the continuity constraints at the q points are jk expressed as follows: for x∗ ∈ Ω j , t t Kx xb Atj y j = T jk r jk . [sent-193, score-0.822]
</p><p>38 j jk  Consequently, the optimization problem (10) can be rewritten as: for x∗ ∈ Ω j , LP(j)′ :  Minimize kt j ∗ Atj (σ2 I + Kx j x j )A j kx j ∗ − 2kt j ∗ Atj kx j ∗ x j x n ×n A j ∈R  j  j  t t s. [sent-194, score-1.321]
</p><p>39 Kx xb Atj y j = T jk r jk j jk  (11) for k ∈ N( j). [sent-196, score-0.766]
</p><p>40 Introducing Lagrange multipliers λ jk (x∗ ) (a q × 1 vector), the problem becomes an unconstrained problem to minimize the Lagrangian: for x∗ ∈ Ω j , L(A j , λ jk (x∗ )) := kt j ∗ Atj (σ2 I + Kx j x j )A j kx j ∗ − 2kt j ∗ Atj kx j ∗ x x j −  ∑  t t λ jk (x∗ )t [Kx xb Atj y j − T jk r jk ]. [sent-197, score-2.272]
</p><p>41 (12)  j jk  k∈N( j)  Let λ j (x∗ ) denote a q j × 1 vector formed by stacking those λ jk (x∗ )’s for k ∈ N( j) where q j := q|N( j)|. [sent-198, score-0.37]
</p><p>42 Let xb denote the collection of xb for all k ∈ N( j). [sent-199, score-0.422]
</p><p>43 We form Kx j xb by columnwise binding j jk j t Kx j xb and form T jt r j by row-wise binding T jk r jk . [sent-200, score-1.038]
</p><p>44 The Lagrangian becomes: for x∗ ∈ Ω j , jk  L(A j , λ jk (x∗ )) := kt j ∗ Atj (σ2 I + Kx j x j )A j kx j ∗ − 2kt j ∗ Atj kx j ∗ x j x t − λ j (x∗ )t [Kx xb Atj y j − T jt r j ]. [sent-201, score-1.778]
</p><p>45 j j  The ﬁrst order necessary conditions (FONC) for local optima are: for x∗ ∈ Ω j , d t L(A j , λ jk ) = 2(σ2 I + Kx j x j )A j kx j ∗ kt j ∗ − 2kx j ∗ kt j ∗ − y j λtj (x∗ )Kx xb = 0, x x j j j dA j d t L(A j , λ j ) = Kx xb Atj y j − T jt r j = 0. [sent-202, score-1.562]
</p><p>46 Equation (13) provides only n j distinguishable equations due to the matrix of rank one, kx j ∗ kt j ∗ , and Equation (14) adds q j x (= q|N( j)|) linear equations. [sent-205, score-0.65]
</p><p>47 To proceed, ﬁrst, we change our target of obtaining the optimal A j to an easier task of obtaining u(x∗ ) = A j kx j ∗ , which is the quantity directly needed for the local predictor u(x∗ )t y j . [sent-207, score-0.594]
</p><p>48 From Equation (13), we have that 1 t A j kx j ∗ = (σ2 I + Kx j x j )−1 kx j ∗ + y j λ j (x∗ )t Kx xb (kt j ∗ kx j ∗ )−1 kx j ∗ . [sent-208, score-2.155]
</p><p>49 Speciﬁcally, we let λ j (x∗ ) be proportional to kxb ∗ , which is inversely related to the distance of x∗ from the boundary points in j  xb . [sent-211, score-0.443]
</p><p>50 The optimal λ j is obtained by using (15) to evaluate A j kx j ∗ at the q j points xb (k ∈ N( j)) on jk the boundaries and then solving (14). [sent-214, score-0.951]
</p><p>51 To simplify the expression, we deﬁne t ¯ h j := (σ2 I + Kx j x j )−1 y j and kxb ∗ := [(kt b ∗ kxb ∗ )−1/2 kxb ∗ ] ◦ [Kx xb kx j ∗ (kt j ∗ kx j ∗ )−1 ]. [sent-216, score-1.63]
</p><p>52 x j x j  j  j  j j  j  The optimal solution becomes   A j kx j ∗ = (σ2 I + Kx j x j )−1 kx j ∗ + j  t y j (T jt r j − Kx xb h j )t G j j j  ytj h j    ¯ k xb ∗  . [sent-217, score-0.99]
</p><p>53 j  (17)  It follows from (17) that the local mean predictor is t ¯ p j (x∗ ; r j ) := kt j ∗ Atj y j = kt j ∗ h j + kt b ∗ G j (T jt r j − Kx xb h j ), ˆ x x x j  j j  (18)  for x∗ ∈ Ω j . [sent-218, score-0.872]
</p><p>54 The r j is equivalent to mean prediction (18) at xb . [sent-222, score-0.247]
</p><p>55 We choose the solution of r j such that it minimizes the predictive j 1705  PARK , H UANG AND D ING  variance at xb . [sent-223, score-0.269]
</p><p>56 The local predictive variance is computed by j ˆ σ j (x∗ ; r j ) = k∗∗ + kt j ∗ Atj (σ2 I + Kx j x j )A j kx j ∗ − 2kt j ∗ Atj kx j ∗ x j x = k∗∗ − kt j ∗ (σ2 I + Kx j x j )−1 kx j ∗ x j ¯ ¯t b G j (T t r j − K t b h j )(T t r j − K t b h j )t G j k b kx ∗ j j xj∗ x jx j x jx j j . [sent-224, score-2.41]
</p><p>57 Because this is a quadratic programming with respect to r j , we can easily see that the optimal boundary values r jk at xb are jk given by (derivation in Appendix B) t t r jk = (T jk T jk )−1 T jk  htj y j htk yk t Kx x b h j + t K t b hk . [sent-230, score-1.442]
</p><p>58 j jk htj y j + htk yk h j y j + htk yk xk x jk  (21)  Apparently, the minimizer of (20) is a weighted average of the mean predictions from two standard local GP predictors of neighboring subdomains. [sent-231, score-0.573]
</p><p>59 Given r j ’s, we solve each local problem LP(j)′ , whose solution is given by (16) and yields the local mean predictors in (18) and the local predictive variance in (19). [sent-233, score-0.316]
</p><p>60 Precompute H j , h j , G j and c j for each subdomain Ω j using H j ← (σ2 I + Kx j x j )−1 , h j ← H j y j , c j ← ytj h j , j t and G j ← ({diag1/2 [(Kxb xb Kxb xb )−1 ]Kxb xb } j j  j j  j j  t t ◦{Kx xb Kx j xb diag[(Kx xb Kx j xb )−1 ]})−1 . [sent-243, score-1.582]
</p><p>61 , m and k ∈ N( j): t t r jk = (T jk T jk )−1 T jk  cj ck t t c j +ck Kx j xb h j + c j +ck Kxk xb hk jk jk  . [sent-248, score-1.532]
</p><p>62 For each Ω j , t i) u j ← G j (T jt r j − Kx xb h j ). [sent-251, score-0.272]
</p><p>63 If x∗ is in Ω j , t ¯ i) kxb ∗ = [(kt b ∗ kxb ∗ )−1/2 kxb ∗ ] ◦ [Kx xb kx j ∗ (kt j ∗ kx j ∗ )−1 ]. [sent-254, score-1.63]
</p><p>64 ii) p j (x∗ ; r j ) ← kx ∗ j ¯ b j ˆ xj∗ t H k ¯ ¯ k∗∗ − kx j ∗ j x j ∗ + kt b ∗ u j utj kxb ∗ /c j . [sent-256, score-1.285]
</p><p>65 When d = 1, Γ jk has only one point and there is no need to deﬁne a polynomial boundary value function. [sent-278, score-0.249]
</p><p>66 Denote by r jk 1707  PARK , H UANG AND D ING  the prediction at the boundary Γ jk , the local prediction problem LP(j)′ is simply Minimize kt j ∗ Atj (σ2 I + Kx j x j )A j kx j ∗ − 2kt j ∗ Atj kx j ∗ x x j n ×n A∈R  j  j  t s. [sent-279, score-1.722]
</p><p>67 The local mean predictor is straightforwardly obtained from expression (18) by replacing T jk with 1. [sent-282, score-0.293]
</p><p>68 We propose to learn local hyperparameters by maximizing the local marginal likelihood functions. [sent-295, score-0.276]
</p><p>69 FIC and PIC does not allow the use of local hyperparameters for reﬂecting local variations of data, so we used global hyperparameters for both FIC and PIC, and for DDM as well, for the sake of 1709  PARK , H UANG AND D ING  fairness. [sent-325, score-0.298]
</p><p>70 , E}, located on the boundary between subdomains Ωi and Ω j , the MSM is deﬁned as 1 E (i) ( j) MSM = ∑ (µe − µe )2 , E e=1 (i)  ( j)  where µe and µe are mean predictions from Ωi and Ω j , respectively. [sent-357, score-0.281]
</p><p>71 2 Mismatch of Predictions on Boundaries DDM puts continuity constraints on local GP regressors so that predictions from neighboring local GP regressors are the same on boundaries for 1-d data and are well controlled for 2-d data. [sent-371, score-0.278]
</p><p>72 The same subdomains are used for local GP, PIC and DDM. [sent-374, score-0.282]
</p><p>73 This is not surprising, because the degrees of freedom determines the complexity of a boundary function, and if we pursue better match with too simple boundary function, we would distort local predictors a lot, which will in the end hurt the accuracy of the local predictors. [sent-400, score-0.306]
</p><p>74 Based on this empirical evidence, we recommend to choose the mesh size so that the number of training data points in a subdomain ranges from 200 to 600. [sent-421, score-0.248]
</p><p>75 4 DDM Versus Local GP We compared DDM with local GP for different mesh sizes and in terms of overall prediction accuracy and mismatch on boundaries. [sent-426, score-0.277]
</p><p>76 01 0 0  200  400 600 mesh size(B)  800  1000  −1 0  0  500 1000 mesh size(B)  1500  −1 0  200  400 600 mesh size(B)  800  1000  Figure 3: Marginal MSE loss versus mesh size(B). [sent-441, score-0.503]
</p><p>77 Figure 4 shows the three performance measures as a function of the number of subdomains for G-DDM, L-DDM and local GP, using the TCO data and the synthetic-2d data, respectively. [sent-445, score-0.282]
</p><p>78 DDM adds more computation to local GP for imposing the continuity on boundaries, but the increased computation is very small relative to the original computation of local GP. [sent-446, score-0.254]
</p><p>79 Hence, the comparison of DDM with local GP as a function of the number of subdomains is almost equivalent to the comparison in terms of the total time (i. [sent-447, score-0.282]
</p><p>80 In terms of MSE and NLPD, L-DDM is appreciably better than local GP when the number of subdomains is small, but the two methods perform comparably when the number of subdomains is large. [sent-455, score-0.484]
</p><p>81 More importantly, the improvement in prediction can be further enhanced by a proper effort to smooth out the boundary mismatches in localized methods (L-DDM versus local GP). [sent-463, score-0.244]
</p><p>82 1  600  500  400 300 200 # of subdomains  100  0  Figure 4: Prediction accuracy of DDM and local GP for different mesh sizes. [sent-524, score-0.403]
</p><p>83 G−DDM L−DDM local GP  12  10  10  8  8  6  6 600  500  400 300 200 # of subdomains  100  0  600  (c) MOD08−CL. [sent-531, score-0.282]
</p><p>84 5  600  500  400 300 200 # of subdomains  G−DDM L−DDM local GP  2  1. [sent-554, score-0.282]
</p><p>85 5  0  600  500  400 300 200 # of subdomains  100  0  0  600  500  400 300 200 # of subdomains  100  0  Figure 5: Prediction accuracy of DDM and local GP for the MOD08-CL data set. [sent-561, score-0.484]
</p><p>86 BGP uses different hyperparameters for each bootstrap sample, but strictly speaking, these hyperparameters cannot be called “local hyperparameters” since each bootstrap sample is from the whole domain, not a local region. [sent-578, score-0.262]
</p><p>87 However, BGP can be converted to have local hyperparameters by making bootstrap samples to come from local regions in the same way as BCM, that is, via k-means clustering. [sent-579, score-0.251]
</p><p>88 Concluding Remarks and Discussions We develop a fast computation method for GP regression, which revises the local kriging predictor to provide consistent predictions on the boundaries of subdomains. [sent-688, score-0.277]
</p><p>89 However, the uniform mesh applies the equalsized subdomains to both the slowly changing regions and the fast changing regions. [sent-748, score-0.337]
</p><p>90 As a remedy, one can consider using the adaptive mesh generation (Becker and Rannacher, 2001) which adjusts the size of subdomains so that they are adaptive to local changes. [sent-750, score-0.403]
</p><p>91 The basic idea is to start with a relatively coarse uniform mesh and to split subdomains until the approximation error is smaller than a prescribed tolerance. [sent-751, score-0.323]
</p><p>92 In each iteration followed, a certain percentage of the subdomains having higher local error estimates, for example, the top 20% of 1725  PARK , H UANG AND D ING  those, are split. [sent-752, score-0.282]
</p><p>93 Thus the adaptive mesh generation in DDM could be performed as follows: Start with a coarse mesh and continue splitting the subdomains corresponding to the top ˆ ˆ 100 · α% of the ηΩ j ’s until η is less than a pre-speciﬁed tolerance. [sent-761, score-0.444]
</p><p>94 The expression (24) can be rewritten as 1 t A j kx j ∗ = (σ2 I + Kx j x j )−1 kx j ∗ + y j λtj [(kt b ∗ kxb ∗ )−1/2 kxb ∗ ] ◦ [Kx xb kx j ∗ (kt j ∗ kx j ∗ )−1 ] . [sent-766, score-2.453]
</p><p>95 j j j  j j  j j  j j  j j  j  j  Substitute the transpose of (27) into (25) to get 1 t Kx xb + G−1 λ j ytj (σ2 I + Kx j x j )−1 y j = T jt r j . [sent-768, score-0.293]
</p><p>96 j j j 2 j After some simple algebra, we obtain the optimal λ j value λ j = 2G j  t T jt r j − Kx xb (σ2 I + Kx j x j )−1 y j j j j  ytj (σ2 I + Kx j x j )−1 y j j  . [sent-769, score-0.293]
</p><p>97 Derivation of (21) for Interface Equation t Note that T jt r j is a rowwise binding of T jk r jk . [sent-771, score-0.431]
</p><p>98 Ignoring a constant, the objective function to be minimized can be written as m  1  j=1  j  ∑ ht y j ∑  t t t t (T jk r jk − Kx xb h j )t (T jk r jk − Kx xb h j ). [sent-772, score-1.162]
</p><p>99 k∈N( j)  j jk  j jk  (28)  To ﬁnd the optimal r jk , we only need pay attention to the relevant terms in (28). [sent-773, score-0.555]
</p><p>100 Since r jk = rk j and T jk = Tk j , the objective function for ﬁnding optimal r jk reduces to 1 t t t (T t r jk − Kx xb h j )t (T jk r jk − Kx xb h j ) j jk j jk htj y j jk 1 t t + t (Tkt j rk j − Kx xb hk )t (Tkt j rk j − Kx xb hk ), k kj k kj hk yk the minimization of which gives (21). [sent-774, score-2.611]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ddm', 0.5), ('kx', 0.486), ('xb', 0.211), ('subdomains', 0.202), ('bgp', 0.198), ('bcm', 0.188), ('jk', 0.185), ('kt', 0.164), ('kxb', 0.149), ('lpr', 0.149), ('atj', 0.135), ('pic', 0.135), ('fic', 0.132), ('gp', 0.122), ('mesh', 0.121), ('mse', 0.112), ('nlpd', 0.089), ('rpic', 0.089), ('subdomain', 0.084), ('local', 0.08), ('msm', 0.079), ('ecomposition', 0.074), ('kxx', 0.074), ('tco', 0.074), ('kriging', 0.071), ('hyperparameters', 0.069), ('kpic', 0.064), ('boundary', 0.064), ('omain', 0.063), ('uang', 0.061), ('sec', 0.061), ('jt', 0.061), ('boundaries', 0.05), ('ranged', 0.046), ('park', 0.046), ('inducing', 0.046), ('rocess', 0.043), ('aussian', 0.041), ('mismatch', 0.04), ('predictive', 0.04), ('htj', 0.04), ('egression', 0.039), ('spatial', 0.038), ('snelson', 0.038), ('continuity', 0.037), ('prediction', 0.036), ('synthetic', 0.036), ('meshing', 0.035), ('marginal', 0.033), ('fonc', 0.03), ('tamu', 0.03), ('predictor', 0.028), ('hyperparameter', 0.026), ('ing', 0.025), ('mgp', 0.025), ('schwaighofer', 0.025), ('localized', 0.025), ('locations', 0.024), ('training', 0.024), ('big', 0.023), ('bootstrap', 0.022), ('control', 0.022), ('xj', 0.021), ('pde', 0.021), ('ytj', 0.021), ('discontinuities', 0.021), ('parallelized', 0.021), ('akx', 0.02), ('chiwoo', 0.02), ('jianhua', 0.02), ('mismatches', 0.02), ('tj', 0.02), ('domain', 0.02), ('versus', 0.019), ('polynomials', 0.019), ('points', 0.019), ('computation', 0.019), ('ghahramani', 0.019), ('inputs', 0.019), ('variance', 0.018), ('nm', 0.018), ('predictors', 0.018), ('train', 0.018), ('parallelization', 0.017), ('gramacy', 0.017), ('tgp', 0.017), ('yk', 0.017), ('interface', 0.017), ('texas', 0.016), ('neighboring', 0.016), ('predictions', 0.015), ('rk', 0.015), ('atk', 0.015), ('diagc', 0.015), ('dqn', 0.015), ('ern', 0.015), ('station', 0.015), ('regression', 0.014), ('fast', 0.014), ('likelihood', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="27-tfidf-1" href="./jmlr-2011-Domain_Decomposition_Approach_for_Fast_Gaussian_Process_Regression_of_Large_Spatial_Data_Sets.html">27 jmlr-2011-Domain Decomposition Approach for Fast Gaussian Process Regression of Large Spatial Data Sets</a></p>
<p>Author: Chiwoo Park, Jianhua Z. Huang, Yu Ding</p><p>Abstract: Gaussian process regression is a ﬂexible and powerful tool for machine learning, but the high computational complexity hinders its broader applications. In this paper, we propose a new approach for fast computation of Gaussian process regression with a focus on large spatial data sets. The approach decomposes the domain of a regression function into small subdomains and infers a local piece of the regression function for each subdomain. We explicitly address the mismatch problem of the local pieces on the boundaries of neighboring subdomains by imposing continuity constraints. The new approach has comparable or better computation complexity as other competing methods, but it is easier to be parallelized for faster computation. Moreover, the method can be adaptive to non-stationary features because of its local nature and, in particular, its use of different hyperparameters of the covariance function for different local regions. We illustrate application of the method and demonstrate its advantages over existing methods using two synthetic data sets and two real spatial data sets. Keywords: domain decomposition, boundary value problem, Gaussian process regression, parallel computation, spatial prediction</p><p>2 0.11272023 <a title="27-tfidf-2" href="./jmlr-2011-Learning_a_Robust_Relevance_Model_for_Search_Using_Kernel_Methods.html">57 jmlr-2011-Learning a Robust Relevance Model for Search Using Kernel Methods</a></p>
<p>Author: Wei Wu, Jun Xu, Hang Li, Satoshi Oyama</p><p>Abstract: This paper points out that many search relevance models in information retrieval, such as the Vector Space Model, BM25 and Language Models for Information Retrieval, can be viewed as a similarity function between pairs of objects of different types, referred to as an S-function. An S-function is speciﬁcally deﬁned as the dot product between the images of two objects in a Hilbert space mapped from two different input spaces. One advantage of taking this view is that one can take a uniﬁed and principled approach to address the issues with regard to search relevance. The paper then proposes employing a kernel method to learn a robust relevance model as an S-function, which can effectively deal with the term mismatch problem, one of the biggest challenges in search. The kernel method exploits a positive semi-deﬁnite kernel referred to as an S-kernel. The paper shows that when using an S-kernel the model learned by the kernel method is guaranteed to be an S-function. The paper then gives more general principles for constructing S-kernels. A speciﬁc implementation of the kernel method is proposed using the Ranking SVM techniques and click-through data. The proposed approach is employed to learn a relevance model as an extension of BM25, referred to as Robust BM25. Experimental results on web search and enterprise search data show that Robust BM25 signiﬁcantly outperforms baseline methods and can successfully tackle the term mismatch problem. Keywords: search, term mismatch, kernel machines, similarity learning, s-function, s-kernel</p><p>3 0.1104015 <a title="27-tfidf-3" href="./jmlr-2011-Variable_Sparsity_Kernel_Learning.html">101 jmlr-2011-Variable Sparsity Kernel Learning</a></p>
<p>Author: Jonathan Aflalo, Aharon Ben-Tal, Chiranjib Bhattacharyya, Jagarlapudi Saketha Nath, Sankaran Raman</p><p>Abstract: paper1 This presents novel algorithms and applications for a particular class of mixed-norm regularization based Multiple Kernel Learning (MKL) formulations. The formulations assume that the given kernels are grouped and employ l1 norm regularization for promoting sparsity within RKHS norms of each group and ls , s ≥ 2 norm regularization for promoting non-sparse combinations across groups. Various sparsity levels in combining the kernels can be achieved by varying the grouping of kernels—hence we name the formulations as Variable Sparsity Kernel Learning (VSKL) formulations. While previous attempts have a non-convex formulation, here we present a convex formulation which admits efﬁcient Mirror-Descent (MD) based solving techniques. The proposed MD based algorithm optimizes over product of simplices and has a computational complexity of O m2 ntot log nmax /ε2 where m is no. training data points, nmax , ntot are the maximum no. kernels in any group, total no. kernels respectively and ε is the error in approximating the objective. A detailed proof of convergence of the algorithm is also presented. Experimental results show that the VSKL formulations are well-suited for multi-modal learning tasks like object categorization. Results also show that the MD based algorithm outperforms state-of-the-art MKL solvers in terms of computational efﬁciency. Keywords: multiple kernel learning, mirror descent, mixed-norm, object categorization, scalability 1. All authors contributed equally. The author names appear in alphabetical order. c 2011 Jonathan Aﬂalo, Aharon Ben-Tal, Chiranjib Bhattacharyya, Jagarlapudi Saketha Nath and Sankaran Raman. A FLALO , B EN -TAL , B HATTACHARYYA , NATH AND R AMAN</p><p>4 0.097432539 <a title="27-tfidf-4" href="./jmlr-2011-Efficient_Structure_Learning_of_Bayesian_Networks_using_Constraints.html">30 jmlr-2011-Efficient Structure Learning of Bayesian Networks using Constraints</a></p>
<p>Author: Cassio P. de Campos, Qiang Ji</p><p>Abstract: This paper addresses the problem of learning Bayesian network structures from data based on score functions that are decomposable. It describes properties that strongly reduce the time and memory costs of many known methods without losing global optimality guarantees. These properties are derived for different score criteria such as Minimum Description Length (or Bayesian Information Criterion), Akaike Information Criterion and Bayesian Dirichlet Criterion. Then a branch-andbound algorithm is presented that integrates structural constraints with data in a way to guarantee global optimality. As an example, structural constraints are used to map the problem of structure learning in Dynamic Bayesian networks into a corresponding augmented Bayesian network. Finally, we show empirically the beneﬁts of using the properties with state-of-the-art methods and with the new algorithm, which is able to handle larger data sets than before. Keywords: Bayesian networks, structure learning, properties of decomposable scores, structural constraints, branch-and-bound technique</p><p>5 0.067777358 <a title="27-tfidf-5" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>Author: Adam D. Bull</p><p>Abstract: In the efﬁcient global optimization problem, we minimize an unknown function f , using as few observations f (x) as possible. It can be considered a continuum-armed-bandit problem, with noiseless data, and simple regret. Expected-improvement algorithms are perhaps the most popular methods for solving the problem; in this paper, we provide theoretical results on their asymptotic behaviour. Implementing these algorithms requires a choice of Gaussian-process prior, which determines an associated space of functions, its reproducing-kernel Hilbert space (RKHS). When the prior is ﬁxed, expected improvement is known to converge on the minimum of any function in its RKHS. We provide convergence rates for this procedure, optimal for functions of low smoothness, and describe a modiﬁed algorithm attaining optimal rates for smoother functions. In practice, however, priors are typically estimated sequentially from the data. For standard estimators, we show this procedure may never ﬁnd the minimum of f . We then propose alternative estimators, chosen to minimize the constants in the rate of convergence, and show these estimators retain the convergence rates of a ﬁxed prior. Keywords: convergence rates, efﬁcient global optimization, expected improvement, continuumarmed bandit, Bayesian optimization</p><p>6 0.060054362 <a title="27-tfidf-6" href="./jmlr-2011-Double_Updating_Online_Learning.html">28 jmlr-2011-Double Updating Online Learning</a></p>
<p>7 0.048810355 <a title="27-tfidf-7" href="./jmlr-2011-Computationally_Efficient_Convolved_Multiple_Output_Gaussian_Processes.html">17 jmlr-2011-Computationally Efficient Convolved Multiple Output Gaussian Processes</a></p>
<p>8 0.044260982 <a title="27-tfidf-8" href="./jmlr-2011-Cumulative_Distribution_Networks_and_the_Derivative-sum-product_Algorithm%3A_Models_and_Inference_for_Cumulative_Distribution_Functions_on_Graphs.html">21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</a></p>
<p>9 0.042074475 <a title="27-tfidf-9" href="./jmlr-2011-Information_Rates_of_Nonparametric_Gaussian_Process_Methods.html">44 jmlr-2011-Information Rates of Nonparametric Gaussian Process Methods</a></p>
<p>10 0.040955883 <a title="27-tfidf-10" href="./jmlr-2011-Robust_Gaussian_Process_Regression_with_a_Student-tLikelihood.html">82 jmlr-2011-Robust Gaussian Process Regression with a Student-tLikelihood</a></p>
<p>11 0.040797915 <a title="27-tfidf-11" href="./jmlr-2011-Bayesian_Generalized_Kernel_Mixed_Models.html">13 jmlr-2011-Bayesian Generalized Kernel Mixed Models</a></p>
<p>12 0.038469441 <a title="27-tfidf-12" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>13 0.031173866 <a title="27-tfidf-13" href="./jmlr-2011-Dirichlet_Process_Mixtures_of_Generalized_Linear_Models.html">24 jmlr-2011-Dirichlet Process Mixtures of Generalized Linear Models</a></p>
<p>14 0.031123782 <a title="27-tfidf-14" href="./jmlr-2011-Learning_Latent_Tree_Graphical_Models.html">54 jmlr-2011-Learning Latent Tree Graphical Models</a></p>
<p>15 0.028544286 <a title="27-tfidf-15" href="./jmlr-2011-A_Cure_for_Variance_Inflation_in_High_Dimensional_Kernel_Principal_Component_Analysis.html">3 jmlr-2011-A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis</a></p>
<p>16 0.027978553 <a title="27-tfidf-16" href="./jmlr-2011-High-dimensional_Covariance_Estimation_Based_On_Gaussian_Graphical_Models.html">39 jmlr-2011-High-dimensional Covariance Estimation Based On Gaussian Graphical Models</a></p>
<p>17 0.025769122 <a title="27-tfidf-17" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>18 0.022626614 <a title="27-tfidf-18" href="./jmlr-2011-Locally_Defined_Principal_Curves_and_Surfaces.html">60 jmlr-2011-Locally Defined Principal Curves and Surfaces</a></p>
<p>19 0.021520693 <a title="27-tfidf-19" href="./jmlr-2011-Smoothness%2C_Disagreement_Coefficient%2C_and_the_Label_Complexity_of_Agnostic_Active_Learning.html">85 jmlr-2011-Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning</a></p>
<p>20 0.020773456 <a title="27-tfidf-20" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.136), (1, -0.051), (2, 0.009), (3, -0.128), (4, 0.097), (5, 0.029), (6, -0.013), (7, 0.07), (8, 0.08), (9, 0.087), (10, 0.154), (11, -0.045), (12, 0.131), (13, 0.131), (14, 0.065), (15, 0.3), (16, 0.135), (17, 0.015), (18, 0.144), (19, 0.054), (20, -0.169), (21, -0.046), (22, -0.151), (23, -0.153), (24, -0.109), (25, 0.344), (26, -0.139), (27, 0.072), (28, 0.074), (29, -0.057), (30, -0.067), (31, -0.023), (32, 0.089), (33, 0.034), (34, -0.006), (35, -0.023), (36, 0.051), (37, 0.042), (38, 0.065), (39, -0.004), (40, -0.013), (41, 0.018), (42, -0.097), (43, -0.098), (44, 0.084), (45, -0.044), (46, -0.021), (47, 0.078), (48, -0.102), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96798009 <a title="27-lsi-1" href="./jmlr-2011-Domain_Decomposition_Approach_for_Fast_Gaussian_Process_Regression_of_Large_Spatial_Data_Sets.html">27 jmlr-2011-Domain Decomposition Approach for Fast Gaussian Process Regression of Large Spatial Data Sets</a></p>
<p>Author: Chiwoo Park, Jianhua Z. Huang, Yu Ding</p><p>Abstract: Gaussian process regression is a ﬂexible and powerful tool for machine learning, but the high computational complexity hinders its broader applications. In this paper, we propose a new approach for fast computation of Gaussian process regression with a focus on large spatial data sets. The approach decomposes the domain of a regression function into small subdomains and infers a local piece of the regression function for each subdomain. We explicitly address the mismatch problem of the local pieces on the boundaries of neighboring subdomains by imposing continuity constraints. The new approach has comparable or better computation complexity as other competing methods, but it is easier to be parallelized for faster computation. Moreover, the method can be adaptive to non-stationary features because of its local nature and, in particular, its use of different hyperparameters of the covariance function for different local regions. We illustrate application of the method and demonstrate its advantages over existing methods using two synthetic data sets and two real spatial data sets. Keywords: domain decomposition, boundary value problem, Gaussian process regression, parallel computation, spatial prediction</p><p>2 0.50502485 <a title="27-lsi-2" href="./jmlr-2011-Learning_a_Robust_Relevance_Model_for_Search_Using_Kernel_Methods.html">57 jmlr-2011-Learning a Robust Relevance Model for Search Using Kernel Methods</a></p>
<p>Author: Wei Wu, Jun Xu, Hang Li, Satoshi Oyama</p><p>Abstract: This paper points out that many search relevance models in information retrieval, such as the Vector Space Model, BM25 and Language Models for Information Retrieval, can be viewed as a similarity function between pairs of objects of different types, referred to as an S-function. An S-function is speciﬁcally deﬁned as the dot product between the images of two objects in a Hilbert space mapped from two different input spaces. One advantage of taking this view is that one can take a uniﬁed and principled approach to address the issues with regard to search relevance. The paper then proposes employing a kernel method to learn a robust relevance model as an S-function, which can effectively deal with the term mismatch problem, one of the biggest challenges in search. The kernel method exploits a positive semi-deﬁnite kernel referred to as an S-kernel. The paper shows that when using an S-kernel the model learned by the kernel method is guaranteed to be an S-function. The paper then gives more general principles for constructing S-kernels. A speciﬁc implementation of the kernel method is proposed using the Ranking SVM techniques and click-through data. The proposed approach is employed to learn a relevance model as an extension of BM25, referred to as Robust BM25. Experimental results on web search and enterprise search data show that Robust BM25 signiﬁcantly outperforms baseline methods and can successfully tackle the term mismatch problem. Keywords: search, term mismatch, kernel machines, similarity learning, s-function, s-kernel</p><p>3 0.39158344 <a title="27-lsi-3" href="./jmlr-2011-Variable_Sparsity_Kernel_Learning.html">101 jmlr-2011-Variable Sparsity Kernel Learning</a></p>
<p>Author: Jonathan Aflalo, Aharon Ben-Tal, Chiranjib Bhattacharyya, Jagarlapudi Saketha Nath, Sankaran Raman</p><p>Abstract: paper1 This presents novel algorithms and applications for a particular class of mixed-norm regularization based Multiple Kernel Learning (MKL) formulations. The formulations assume that the given kernels are grouped and employ l1 norm regularization for promoting sparsity within RKHS norms of each group and ls , s ≥ 2 norm regularization for promoting non-sparse combinations across groups. Various sparsity levels in combining the kernels can be achieved by varying the grouping of kernels—hence we name the formulations as Variable Sparsity Kernel Learning (VSKL) formulations. While previous attempts have a non-convex formulation, here we present a convex formulation which admits efﬁcient Mirror-Descent (MD) based solving techniques. The proposed MD based algorithm optimizes over product of simplices and has a computational complexity of O m2 ntot log nmax /ε2 where m is no. training data points, nmax , ntot are the maximum no. kernels in any group, total no. kernels respectively and ε is the error in approximating the objective. A detailed proof of convergence of the algorithm is also presented. Experimental results show that the VSKL formulations are well-suited for multi-modal learning tasks like object categorization. Results also show that the MD based algorithm outperforms state-of-the-art MKL solvers in terms of computational efﬁciency. Keywords: multiple kernel learning, mirror descent, mixed-norm, object categorization, scalability 1. All authors contributed equally. The author names appear in alphabetical order. c 2011 Jonathan Aﬂalo, Aharon Ben-Tal, Chiranjib Bhattacharyya, Jagarlapudi Saketha Nath and Sankaran Raman. A FLALO , B EN -TAL , B HATTACHARYYA , NATH AND R AMAN</p><p>4 0.33848029 <a title="27-lsi-4" href="./jmlr-2011-Efficient_Structure_Learning_of_Bayesian_Networks_using_Constraints.html">30 jmlr-2011-Efficient Structure Learning of Bayesian Networks using Constraints</a></p>
<p>Author: Cassio P. de Campos, Qiang Ji</p><p>Abstract: This paper addresses the problem of learning Bayesian network structures from data based on score functions that are decomposable. It describes properties that strongly reduce the time and memory costs of many known methods without losing global optimality guarantees. These properties are derived for different score criteria such as Minimum Description Length (or Bayesian Information Criterion), Akaike Information Criterion and Bayesian Dirichlet Criterion. Then a branch-andbound algorithm is presented that integrates structural constraints with data in a way to guarantee global optimality. As an example, structural constraints are used to map the problem of structure learning in Dynamic Bayesian networks into a corresponding augmented Bayesian network. Finally, we show empirically the beneﬁts of using the properties with state-of-the-art methods and with the new algorithm, which is able to handle larger data sets than before. Keywords: Bayesian networks, structure learning, properties of decomposable scores, structural constraints, branch-and-bound technique</p><p>5 0.24767731 <a title="27-lsi-5" href="./jmlr-2011-Computationally_Efficient_Convolved_Multiple_Output_Gaussian_Processes.html">17 jmlr-2011-Computationally Efficient Convolved Multiple Output Gaussian Processes</a></p>
<p>Author: Mauricio A. Álvarez, Neil D. Lawrence</p><p>Abstract: Recently there has been an increasing interest in regression methods that deal with multiple outputs. This has been motivated partly by frameworks like multitask learning, multisensor networks or structured output data. From a Gaussian processes perspective, the problem reduces to specifying an appropriate covariance function that, whilst being positive semi-deﬁnite, captures the dependencies between all the data points and across all the outputs. One approach to account for non-trivial correlations between outputs employs convolution processes. Under a latent function interpretation of the convolution transform we establish dependencies between output variables. The main drawbacks of this approach are the associated computational and storage demands. In this paper we address these issues. We present different efﬁcient approximations for dependent output Gaussian processes constructed through the convolution formalism. We exploit the conditional independencies present naturally in the model. This leads to a form of the covariance similar in spirit to the so called PITC and FITC approximations for a single output. We show experimental results with synthetic and real data, in particular, we show results in school exams score prediction, pollution prediction and gene expression data. Keywords: Gaussian processes, convolution processes, efﬁcient approximations, multitask learning, structured outputs, multivariate processes</p><p>6 0.23742911 <a title="27-lsi-6" href="./jmlr-2011-Double_Updating_Online_Learning.html">28 jmlr-2011-Double Updating Online Learning</a></p>
<p>7 0.23740037 <a title="27-lsi-7" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>8 0.17822228 <a title="27-lsi-8" href="./jmlr-2011-Bayesian_Generalized_Kernel_Mixed_Models.html">13 jmlr-2011-Bayesian Generalized Kernel Mixed Models</a></p>
<p>9 0.17221095 <a title="27-lsi-9" href="./jmlr-2011-Information_Rates_of_Nonparametric_Gaussian_Process_Methods.html">44 jmlr-2011-Information Rates of Nonparametric Gaussian Process Methods</a></p>
<p>10 0.1674367 <a title="27-lsi-10" href="./jmlr-2011-A_Cure_for_Variance_Inflation_in_High_Dimensional_Kernel_Principal_Component_Analysis.html">3 jmlr-2011-A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis</a></p>
<p>11 0.1518507 <a title="27-lsi-11" href="./jmlr-2011-Cumulative_Distribution_Networks_and_the_Derivative-sum-product_Algorithm%3A_Models_and_Inference_for_Cumulative_Distribution_Functions_on_Graphs.html">21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</a></p>
<p>12 0.14732529 <a title="27-lsi-12" href="./jmlr-2011-Efficient_Learning_with_Partially_Observed_Attributes.html">29 jmlr-2011-Efficient Learning with Partially Observed Attributes</a></p>
<p>13 0.14730211 <a title="27-lsi-13" href="./jmlr-2011-Learning_Multi-modal_Similarity.html">55 jmlr-2011-Learning Multi-modal Similarity</a></p>
<p>14 0.13588592 <a title="27-lsi-14" href="./jmlr-2011-Laplacian_Support_Vector_Machines__Trained_in_the_Primal.html">51 jmlr-2011-Laplacian Support Vector Machines  Trained in the Primal</a></p>
<p>15 0.13413888 <a title="27-lsi-15" href="./jmlr-2011-Non-Parametric_Estimation_of_Topic_Hierarchies_from_Texts_with_Hierarchical_Dirichlet_Processes.html">70 jmlr-2011-Non-Parametric Estimation of Topic Hierarchies from Texts with Hierarchical Dirichlet Processes</a></p>
<p>16 0.13064542 <a title="27-lsi-16" href="./jmlr-2011-Learning_Latent_Tree_Graphical_Models.html">54 jmlr-2011-Learning Latent Tree Graphical Models</a></p>
<p>17 0.12245733 <a title="27-lsi-17" href="./jmlr-2011-Locally_Defined_Principal_Curves_and_Surfaces.html">60 jmlr-2011-Locally Defined Principal Curves and Surfaces</a></p>
<p>18 0.11679611 <a title="27-lsi-18" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>19 0.11586005 <a title="27-lsi-19" href="./jmlr-2011-Robust_Gaussian_Process_Regression_with_a_Student-tLikelihood.html">82 jmlr-2011-Robust Gaussian Process Regression with a Student-tLikelihood</a></p>
<p>20 0.1153532 <a title="27-lsi-20" href="./jmlr-2011-On_the_Relation_between_Realizable_and_Nonrealizable_Cases_of_the_Sequence_Prediction_Problem.html">72 jmlr-2011-On the Relation between Realizable and Nonrealizable Cases of the Sequence Prediction Problem</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.023), (9, 0.024), (10, 0.035), (13, 0.013), (21, 0.464), (24, 0.052), (31, 0.084), (32, 0.025), (39, 0.013), (41, 0.018), (60, 0.013), (70, 0.034), (71, 0.011), (73, 0.033), (78, 0.042), (90, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69832808 <a title="27-lda-1" href="./jmlr-2011-Domain_Decomposition_Approach_for_Fast_Gaussian_Process_Regression_of_Large_Spatial_Data_Sets.html">27 jmlr-2011-Domain Decomposition Approach for Fast Gaussian Process Regression of Large Spatial Data Sets</a></p>
<p>Author: Chiwoo Park, Jianhua Z. Huang, Yu Ding</p><p>Abstract: Gaussian process regression is a ﬂexible and powerful tool for machine learning, but the high computational complexity hinders its broader applications. In this paper, we propose a new approach for fast computation of Gaussian process regression with a focus on large spatial data sets. The approach decomposes the domain of a regression function into small subdomains and infers a local piece of the regression function for each subdomain. We explicitly address the mismatch problem of the local pieces on the boundaries of neighboring subdomains by imposing continuity constraints. The new approach has comparable or better computation complexity as other competing methods, but it is easier to be parallelized for faster computation. Moreover, the method can be adaptive to non-stationary features because of its local nature and, in particular, its use of different hyperparameters of the covariance function for different local regions. We illustrate application of the method and demonstrate its advantages over existing methods using two synthetic data sets and two real spatial data sets. Keywords: domain decomposition, boundary value problem, Gaussian process regression, parallel computation, spatial prediction</p><p>2 0.24831381 <a title="27-lda-2" href="./jmlr-2011-Robust_Gaussian_Process_Regression_with_a_Student-tLikelihood.html">82 jmlr-2011-Robust Gaussian Process Regression with a Student-tLikelihood</a></p>
<p>Author: Pasi Jylänki, Jarno Vanhatalo, Aki Vehtari</p><p>Abstract: This paper considers the robust and efﬁcient implementation of Gaussian process regression with a Student-t observation model, which has a non-log-concave likelihood. The challenge with the Student-t model is the analytically intractable inference which is why several approximative methods have been proposed. Expectation propagation (EP) has been found to be a very accurate method in many empirical studies but the convergence of EP is known to be problematic with models containing non-log-concave site functions. In this paper we illustrate the situations where standard EP fails to converge and review different modiﬁcations and alternative algorithms for improving the convergence. We demonstrate that convergence problems may occur during the type-II maximum a posteriori (MAP) estimation of the hyperparameters and show that standard EP may not converge in the MAP values with some difﬁcult data sets. We present a robust implementation which relies primarily on parallel EP updates and uses a moment-matching-based double-loop algorithm with adaptively selected step size in difﬁcult cases. The predictive performance of EP is compared with Laplace, variational Bayes, and Markov chain Monte Carlo approximations. Keywords: Gaussian process, robust regression, Student-t distribution, approximate inference, expectation propagation</p><p>3 0.24294929 <a title="27-lda-3" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>Author: Ricardo Henao, Ole Winther</p><p>Abstract: In this paper we consider sparse and identiﬁable linear latent variable (factor) and linear Bayesian network models for parsimonious analysis of multivariate data. We propose a computationally efﬁcient method for joint parameter and model inference, and model comparison. It consists of a fully Bayesian hierarchy for sparse models using slab and spike priors (two-component δ-function and continuous mixtures), non-Gaussian latent factors and a stochastic search over the ordering of the variables. The framework, which we call SLIM (Sparse Linear Identiﬁable Multivariate modeling), is validated and bench-marked on artiﬁcial and real biological data sets. SLIM is closest in spirit to LiNGAM (Shimizu et al., 2006), but differs substantially in inference, Bayesian network structure learning and model comparison. Experimentally, SLIM performs equally well or better than LiNGAM with comparable computational complexity. We attribute this mainly to the stochastic search strategy used, and to parsimony (sparsity and identiﬁability), which is an explicit part of the model. We propose two extensions to the basic i.i.d. linear framework: non-linear dependence on observed variables, called SNIM (Sparse Non-linear Identiﬁable Multivariate modeling) and allowing for correlations between latent variables, called CSLIM (Correlated SLIM), for the temporal and/or spatial data. The source code and scripts are available from http://cogsys.imm.dtu.dk/slim/. Keywords: parsimony, sparsity, identiﬁability, factor models, linear Bayesian networks</p><p>4 0.24258621 <a title="27-lda-4" href="./jmlr-2011-Two_Distributed-State_Models_For_Generating_High-Dimensional_Time_Series.html">96 jmlr-2011-Two Distributed-State Models For Generating High-Dimensional Time Series</a></p>
<p>Author: Graham W. Taylor, Geoffrey E. Hinton, Sam T. Roweis</p><p>Abstract: In this paper we develop a class of nonlinear generative models for high-dimensional time series. We ﬁrst propose a model based on the restricted Boltzmann machine (RBM) that uses an undirected model with binary latent variables and real-valued “visible” variables. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. This “conditional” RBM (CRBM) makes on-line inference efﬁcient and allows us to use a simple approximate learning procedure. We demonstrate the power of our approach by synthesizing various sequences from a model trained on motion capture data and by performing on-line ﬁlling in of data lost during capture. We extend the CRBM in a way that preserves its most important computational properties and introduces multiplicative three-way interactions that allow the effective interaction weight between two variables to be modulated by the dynamic state of a third variable. We introduce a factoring of the implied three-way weight tensor to permit a more compact parameterization. The resulting model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve its ability to blend motion styles or to transition smoothly among them. Videos and source code can be found at http://www.cs.nyu.edu/˜gwtaylor/publications/ jmlr2011. Keywords: unsupervised learning, restricted Boltzmann machines, time series, generative models, motion capture</p><p>5 0.23760089 <a title="27-lda-5" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>Author: Jennifer Gillenwater, Kuzman Ganchev, João Graça, Fernando Pereira, Ben Taskar</p><p>Abstract: A strong inductive bias is essential in unsupervised grammar induction. In this paper, we explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. We use part-of-speech (POS) tags to group dependencies by parent-child types and investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In experiments with 12 different languages, we achieve signiﬁcant gains in directed attachment accuracy over the standard expectation maximization (EM) baseline, with an average accuracy improvement of 6.5%, outperforming EM by at least 1% for 9 out of 12 languages. Furthermore, the new method outperforms models based on standard Bayesian sparsity-inducing parameter priors with an average improvement of 5% and positive gains of at least 1% for 9 out of 12 languages. On English text in particular, we show that our approach improves performance over other state-of-the-art techniques.</p><p>6 0.23671675 <a title="27-lda-6" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>7 0.23654062 <a title="27-lda-7" href="./jmlr-2011-Logistic_Stick-Breaking_Process.html">61 jmlr-2011-Logistic Stick-Breaking Process</a></p>
<p>8 0.23636582 <a title="27-lda-8" href="./jmlr-2011-Computationally_Efficient_Convolved_Multiple_Output_Gaussian_Processes.html">17 jmlr-2011-Computationally Efficient Convolved Multiple Output Gaussian Processes</a></p>
<p>9 0.23496462 <a title="27-lda-9" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>10 0.23455302 <a title="27-lda-10" href="./jmlr-2011-Minimum_Description_Length_Penalization_for_Group_and_Multi-Task_Sparse_Learning.html">64 jmlr-2011-Minimum Description Length Penalization for Group and Multi-Task Sparse Learning</a></p>
<p>11 0.23399749 <a title="27-lda-11" href="./jmlr-2011-A_Family_of_Simple_Non-Parametric_Kernel_Learning_Algorithms.html">4 jmlr-2011-A Family of Simple Non-Parametric Kernel Learning Algorithms</a></p>
<p>12 0.23392045 <a title="27-lda-12" href="./jmlr-2011-Forest_Density_Estimation.html">35 jmlr-2011-Forest Density Estimation</a></p>
<p>13 0.23357613 <a title="27-lda-13" href="./jmlr-2011-Bayesian_Generalized_Kernel_Mixed_Models.html">13 jmlr-2011-Bayesian Generalized Kernel Mixed Models</a></p>
<p>14 0.23173958 <a title="27-lda-14" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>15 0.23070471 <a title="27-lda-15" href="./jmlr-2011-Producing_Power-Law_Distributions_and_Damping_Word_Frequencies_with_Two-Stage_Language_Models.html">78 jmlr-2011-Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models</a></p>
<p>16 0.22968268 <a title="27-lda-16" href="./jmlr-2011-Dirichlet_Process_Mixtures_of_Generalized_Linear_Models.html">24 jmlr-2011-Dirichlet Process Mixtures of Generalized Linear Models</a></p>
<p>17 0.22950287 <a title="27-lda-17" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<p>18 0.22950028 <a title="27-lda-18" href="./jmlr-2011-Multiple_Kernel_Learning_Algorithms.html">66 jmlr-2011-Multiple Kernel Learning Algorithms</a></p>
<p>19 0.22787844 <a title="27-lda-19" href="./jmlr-2011-Distance_Dependent_Chinese_Restaurant_Processes.html">26 jmlr-2011-Distance Dependent Chinese Restaurant Processes</a></p>
<p>20 0.22778293 <a title="27-lda-20" href="./jmlr-2011-Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization.html">8 jmlr-2011-Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
