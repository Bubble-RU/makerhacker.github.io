<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-9" href="#">jmlr2011-9</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</h1>
<br/><p>Source: <a title="jmlr-2011-9-pdf" href="http://jmlr.org/papers/volume12/zwiernik11a/zwiernik11a.pdf">pdf</a></p><p>Author: Piotr Zwiernik</p><p>Abstract: The standard Bayesian Information Criterion (BIC) is derived under regularity conditions which are not always satisÄ?Ĺš ed in the case of graphical models with hidden variables. In this paper we derive the BIC for the binary graphical tree models where all the inner nodes of a tree represent binary hidden variables. This provides an extension of a similar formula given by Rusakov and Geiger for naive Bayes models. The main tool used in this paper is the connection between the growth behavior of marginal likelihood integrals and the real log-canonical threshold. Keywords: BIC, marginal likelihood, singular models, tree models, Bayesian networks, real logcanonical threshold</p><p>Reference: <a title="jmlr-2011-9-reference" href="../jmlr2011_reference/jmlr-2011-An_Asymptotic_Behaviour_of_the_Marginal_Likelihood_for_General_Markov_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper we derive the BIC for the binary graphical tree models where all the inner nodes of a tree represent binary hidden variables. [sent-5, score-0.213]
</p><p>2 Keywords: BIC, marginal likelihood, singular models, tree models, Bayesian networks, real logcanonical threshold  1. [sent-8, score-0.172]
</p><p>3 Let T r denote T rooted in r, that is a tree with one distinguished vertex r and all the edges directed away from r. [sent-78, score-0.199]
</p><p>4 Then, the general Markov model is a family of marginal distributions over the subvector of Y corresponding to the leaves of T r . [sent-81, score-0.153]
</p><p>5 For a tree T with n leaves we denote the subvector of Y corresponding to the leaves of T by X = (X1 , . [sent-84, score-0.263]
</p><p>6 We say that two nodes u, v of T are separated by another node w, if w lies on the unique path between u and v. [sent-92, score-0.176]
</p><p>7 Let l2 denote the number of inner nodes v of T such that for each triple i, j, k of leaves separated in T by v we have Ă&sbquo;Äži j Ă&sbquo;Äžik Ă&sbquo;Äž jk = 0 but there exist leaves i, j separated by v Ă&lsaquo;&dagger; Ă&lsaquo;&dagger; Ă&lsaquo;&dagger; such that Ă&sbquo;Äži j = 0. [sent-93, score-0.373]
</p><p>8 Ă˘&Scaron;Ä˝ Ă˘&Scaron;Ä˝ In addition, we say that an inner node v is degenerate (or q-degenerate) if for any two leaves i, j separated by v we have Ă&sbquo;Äži j = 0. [sent-96, score-0.282]
</p><p>9 Ă˘&Scaron;Ä˝ We denote by ne the number of edges of T and by nv the number of its nodes. [sent-99, score-0.12]
</p><p>10 34):  3285  Z WIERNIK  Theorem 1 Let T r = (V, E) be a tree rooted in r and X (N) be a random sample from q. [sent-101, score-0.143]
</p><p>11 With assumptions (A1), (A2) and (A3), if there are no q-degenerate nodes then, as N Ă˘&dagger;&rsquo; Ă˘&circ;&#x17E;, EFN = N S +  nv + ne Ă˘&circ;&rsquo; 2l2 log N + O(1). [sent-102, score-0.12]
</p><p>12 Ĺš cient of log N is  Figure 2: A quartet tree rooted in r. [sent-106, score-0.194]
</p><p>13 Here the dashed edge means that for every pair i, j of leaves separated by this edge Ă&sbquo;Äži j = 0 Ă&lsaquo;&dagger; and an inner node contributes to l2 if its valency, in the forest with the dashed edges removed, is 2. [sent-112, score-0.338]
</p><p>14 Example 2 (Naive Bayes model) Consider a star tree with one inner node and n leaves. [sent-119, score-0.146]
</p><p>15 If there are no degenerate nodes this corresponds to q being either a regular point or a type 1 singularity as deÄ? [sent-120, score-0.149]
</p><p>16 If the inner node is degenerate this corresponds to the type 2 singularity which does not satisfy assumptions of Theorem 1. [sent-127, score-0.188]
</p><p>17 If q is such that there are degenerate nodes the computation of the BIC is much harder because the likelihood in this case maximizes over a singular subset of the parameter space. [sent-128, score-0.145]
</p><p>18 The case of star  Figure 3: Three graphs representing submodels of the quartet tree model with some additional marginal independencies. [sent-129, score-0.149]
</p><p>19 In this paper we obtain a closed form formula for the BIC in the case of trivalent trees, whose all inner nodes have valency three. [sent-132, score-0.234]
</p><p>20 The importance of trivalent trees follows mainly from the fact that any other tree model is a submodel of a model for a trivalent tree. [sent-134, score-0.247]
</p><p>21 If T is trivalent then for every inner node v Ă˘&circ;&circ; V there exist A, B,C Ă˘&Scaron;&dagger; [n] such that A Ă˘&circ;Ĺ&#x17E; B Ă˘&circ;Ĺ&#x17E; C = [n] and A, B,C are separated by v. [sent-136, score-0.218]
</p><p>22 Ă˘&Scaron;Ä˝ Ă˘&Scaron;Ä˝ Theorem 2 Let T r = (V, E) be a rooted trivalent tree with n Ă˘&permil;Ä˝ 3 leaves and root r. [sent-141, score-0.347]
</p><p>23 With assumptions (A1), (A2) and (A3) if r is degenerate but all its neighbors are not, then, as N Ă˘&dagger;&rsquo; Ă˘&circ;&#x17E;, EFN = N S +  nv + ne Ă˘&circ;&rsquo; 2l2 5l0 + 1 log N + O(1). [sent-142, score-0.12]
</p><p>24 The edge (r, a) is dashed since for any two leaves separated by this edge the corresponding covariance is zero. [sent-153, score-0.187]
</p><p>25 Asymptotics of Marginal Likelihood Integrals In this section we introduce the real log-canonical threshold and link it to the asymptotic behavior of marginal likelihood integrals. [sent-179, score-0.154]
</p><p>26 Hence we consider induced marginal 3290  A SYMPTOTICS OF THE M ARGINAL L IKELIHOOD F OR T REE M ODELS  probability distributions over the leaves of T r . [sent-274, score-0.153]
</p><p>27 , Xn ) denotes the variables represented by the leaves of T r and H denotes the vector of variables represented by inner nodes, that is X = (Yv )vĂ˘&circ;&circ;L and H = (Yv )vĂ˘&circ;&circ;V \L . [sent-280, score-0.161]
</p><p>28 By construction all the inner nodes of T have either degree zero in T or the degree is strictly greater than one. [sent-305, score-0.169]
</p><p>29 If each of the inner nodes of T has degree at least two in T then Ă&#x17D;&tilde;T is a manifold with corners and dim Ă&#x17D;&tilde;T = 2l2 , where l2 is the number of nodes which have degree two in T. [sent-315, score-0.267]
</p><p>30 Let q Ă˘&circ;&circ; MT be the real distribution generating the data such that each inner node of T has degree at least two in T . [sent-319, score-0.171]
</p><p>31 2 Proof Since every inner node of T has degree at least two in T then by Proposition 7 there exists a smooth manifold M Ă˘&Scaron;&dagger; Rnv +ne such that Ă&#x17D;&tilde;T = M Ă˘&circ;Ĺ  Ă&#x17D;&tilde;T and dim Ă&#x17D;&tilde; = 2l2 . [sent-321, score-0.166]
</p><p>32 By Theorem 4, Proposition 8 implies Theorem 1 since l2 in its statement is exactly the number of inner nodes v such that the degree of v in T is two. [sent-323, score-0.14]
</p><p>33 In Theorem 14 we apply a useful change of coordinates which enables us to work out the real log-canonical threshold in the singular case. [sent-332, score-0.13]
</p><p>34 5) the real log-canonical threshold does not depend on the choice of generators of I. [sent-358, score-0.12]
</p><p>35 Equation (19) in Zwiernik and Smith, 2011b) Ă˘&circ;&rsquo;(1 + sv ) Ă˘&permil;Â¤ (1 Ă˘&circ;&rsquo; su )Ă&#x17D;Ë&Dagger;uv Ă˘&permil;Â¤ (1 Ă˘&circ;&rsquo; sv ) Ă˘&circ;&rsquo;(1 Ă˘&circ;&rsquo; sv ) Ă˘&permil;Â¤ (1 + su )Ă&#x17D;Ë&Dagger;uv Ă˘&permil;Â¤ (1 + sv ). [sent-409, score-0.284]
</p><p>36 Proposition 12 (Zwiernik and Smith, 2011b) Let T r = (V, E) be a rooted trivalent tree with n leaves. [sent-411, score-0.241]
</p><p>37 Let I denote the pullback of the ideal I Ă˘&Scaron;&dagger; AĂ&#x17D;&tilde;T to the ideal in AĂ˘&bdquo;Ĺ&scaron;T induced by fĂ&#x17D;Â¸Ä&#x17D;&permil; . [sent-418, score-0.133]
</p><p>38 Here the sum of ideals results in another ideal with the generating set which is the sum of generating sets of the summands. [sent-427, score-0.124]
</p><p>39 Theorem 14 Let T r be a rooted tree with n leaves and q Ă˘&circ;&circ; MT . [sent-431, score-0.249]
</p><p>40 If T is rooted uv v in an inner leaf then by Proposition 12 the ideal J does not depend on s1 , . [sent-442, score-0.281]
</p><p>41 ThereJ depend on sr because Ă&#x17D;Ĺ&Yuml;I (Ä&#x17D;&permil;) = (1 Ă˘&circ;&rsquo; sr I I fore, we cannot use Proposition 13 directly. [sent-460, score-0.128]
</p><p>42 By r Proposition 10 (iv) the real log canonical threshold of J in W is equal to the real log-canonical threshold of a an ideal with generators induced from the generators of J by replacing each 1 Ă˘&circ;&rsquo; s2 r by 1. [sent-467, score-0.295]
</p><p>43 Let T be a trivalent tree with n Ă˘&permil;Ä˝ 3 leaves and let q Ă˘&circ;&circ; MT . [sent-475, score-0.255]
</p><p>44 If all the equivalence classes in [E] are singletones or [E] is empty, which is equivalent to every inner node being of degree at least two in T , then Theorem 1 gives us the asymptotic behavior of the marginal likelihood. [sent-476, score-0.171]
</p><p>45 For each Si i the number of nodes, edges and nodes of degree 2 in T is denoted by ni , ni and l2 respectively. [sent-497, score-0.239]
</p><p>46 Lemma 15 Let T = (V, E) be a trivalent rooted tree with n Ă˘&permil;Ä˝ 4 leaves and let q Ă˘&circ;&circ; MT . [sent-500, score-0.347]
</p><p>47 We note that, by Proposition 8 and the formula in (12) for each Si : RLCT(J (Si )) +  i ni ni + ni Ă˘&circ;&rsquo; 2l2 e = v . [sent-568, score-0.172]
</p><p>48 Ă˘&Scaron;Ä˝ Ă˘&Scaron;Ä˝ Proposition 16 Let T be a trivalent tree with n = 3 leaves rooted in r Ă˘&circ;&circ; V . [sent-573, score-0.347]
</p><p>49 Then min RLCTÄ&#x17D;&permil;0 (J ) =  Ä&#x17D;&permil;0 Ă˘&circ;&circ;Ă˘&bdquo;Ĺ&scaron;T  n ,m , 4  where m = 1 if either r is a leaf of T or r together with all its neighbors are all inner nodes of T . [sent-577, score-0.137]
</p><p>50 2, we present a method to compute the real log-canonical threshold of a monomial ideal. [sent-583, score-0.132]
</p><p>51 It follows that the computations can be reduced only to v points satisfying sv Ă˘&permil;Ä˝ 0 for all inner nodes v of T . [sent-589, score-0.213]
</p><p>52 Ĺš ne the deepest singularity of Ă˘&bdquo;Ĺ&scaron;T as Ă˘&bdquo;Ĺ&scaron;deep := {Ä&#x17D;&permil; Ă˘&circ;&circ; Ă˘&bdquo;Ĺ&scaron;T : Ă&#x17D;Ë&Dagger;e = 0 for all e Ă˘&circ;&circ; E, sv = 1 for all v Ă˘&circ;&circ; V }. [sent-592, score-0.131]
</p><p>53 Let VĂ&#x17D;Â´ = Rne +|Ă&#x17D;Â´| = R|Ă&#x17D;Â´| Ä&sbquo;&mdash;Rne , where |Ă&#x17D;Â´| = Ă˘&circ;&lsquo;v Ă&#x17D;Â´v , be the real space with variables representing the edges (xe )eĂ˘&circ;&circ;E and nodes (yv ) for all v such that Ă&#x17D;Â´v = 1. [sent-697, score-0.136]
</p><p>54 (20) uv j) i= jĂ˘&circ;&circ;[n]  (u,v)Ă˘&circ;&circ;E(i j)  The convex hull, in VĂ&#x17D;Â´ , of the exponents of the terms in QĂ&#x17D;Â´ is called the Newton polytope of QĂ&#x17D;Â´ and denoted Ă&#x17D;&ldquo;(QĂ&#x17D;Â´ ). [sent-706, score-0.13]
</p><p>55 We now investigate this polytope which is needed to understand the polyhedron Ă&#x17D;&ldquo;+ (QĂ&#x17D;Â´ ), which is needed to use Theorem 20. [sent-707, score-0.128]
</p><p>56 Since each term of QĂ&#x17D;Â´ corresponds to a path between two leaves then the construction of the Newton polytope Ă&#x17D;&ldquo;(QĂ&#x17D;Â´ ) Ă˘&Scaron;&sbquo; VĂ&#x17D;Â´ gives a direct relationship between paths in T and the points generating the polytope. [sent-708, score-0.283]
</p><p>57 Let E0 Ă˘&Scaron;&dagger; E be the subset of edges of T such that one of the ends is in the set of leaves of T . [sent-710, score-0.162]
</p><p>58 This follows from the fact that each of these points corresponds to a path between two leaves in T and every such a path need to cross exactly two terminal edges. [sent-714, score-0.188]
</p><p>59 The induced facet of the Newton polyhedron Ă&#x17D;&ldquo;+ (QĂ&#x17D;Â´ ) is given as F0 = {(y, x) Ă˘&circ;&circ; Ă&#x17D;&ldquo;+ (QĂ&#x17D;Â´ ) :  Ă˘&circ;&lsquo; xe = 4}  (21)  eĂ˘&circ;&circ;E0  and each point of Ă&#x17D;&ldquo;+ (QĂ&#x17D;Â´ ) satisÄ? [sent-716, score-0.243]
</p><p>60 Construction 23 Let T = (V, E) be a trivalent tree with n Ă˘&permil;Ä˝ 4 leaves, rooted in r. [sent-729, score-0.241]
</p><p>61 We present two constructions of networks of paths between the leaves of T . [sent-730, score-0.156]
</p><p>62 In this case T is necessarily rooted in an inner node. [sent-733, score-0.147]
</p><p>63 The barycenter of the points corresponding to all the four paths in the network is (1, 1; 1, 1, 0, 1, 1) both if T is rooted in a or b. [sent-738, score-0.165]
</p><p>64 Assume that T is rooted in an inner node a and pick an inner edge (a, b). [sent-740, score-0.27]
</p><p>65 Label the edges incident with a and b as for the quartet tree above and consider the subtree given by the quartet tree. [sent-741, score-0.244]
</p><p>66 Let v be any leaf of the quartet subtree which is not a leaf of T and label the two additional edges incident with v by e6 and e7 . [sent-743, score-0.194]
</p><p>67 Let 4 1 q = n Ă˘&circ;&lsquo;n qi then q Ă˘&circ;&circ; Ă&#x17D;&ldquo;(QĂ&#x17D;Â´ ) is given by xab = 0, xe = n for all e Ă˘&circ;&circ; E \ (a, b). [sent-755, score-0.132]
</p><p>68 The other coordinates i=1 4 4 by construction satisfy ya = n , yb = n if Ă&#x17D;Â´b = 1, and yv = 2 for all v Ă˘&circ;&circ; V \ {a, b} such that Ă&#x17D;Â´v = 1. [sent-756, score-0.174]
</p><p>69 For n = 4 consider a network of all the possible paths all counted with multiplicity one apart from the cherry paths (paths of length two) counted with multiplicity two. [sent-758, score-0.206]
</p><p>70 The coordinates of the point representing the barycenter of all paths in the network satisfy xe = 1 for all e Ă˘&circ;&circ; E and 1 yv = 2 for all v such that Ă&#x17D;Â´v = 1. [sent-760, score-0.33]
</p><p>71 This construction generalizes recursively in a similar way as the one for T rooted in an inner node. [sent-761, score-0.147]
</p><p>72 The network induces a point q Ă˘&circ;&circ; Ă&#x17D;&ldquo;(QĂ&#x17D;Â´ ) with coordinates given by yv = 2 for all v Ă˘&circ;&circ; V such n 4 that Ă&#x17D;Â´v = 1 and xe = n for e Ă˘&circ;&circ; E. [sent-763, score-0.257]
</p><p>73 Ă&#x17D;&ldquo;+ (QĂ&#x17D;Â´ ) and therefore rlct0 (J 4  To compute the multiplicity of the real log-canonical threshold of QĂ&#x17D;Â´ we have to get a better understanding of the polyhedron Ă&#x17D;&ldquo;+ (QĂ&#x17D;Â´ ). [sent-774, score-0.162]
</p><p>74 Ĺš nition 24 (A pair-edge incidence polytope) Let T = (V, E) be a trivalent tree with n Ă˘&permil;Ä˝ 4 leaves. [sent-780, score-0.188]
</p><p>75 Ĺš ne a polytope Pn Ă˘&Scaron;&sbquo; Rne , where ne = 2n Ă˘&circ;&rsquo; 3, as the convex combination of points (qi j )i, jĂ˘&circ;&circ;[n] where k-th coordinate of qi j is one if the k-th edge is in the path between i and j and there is zero otherwise. [sent-782, score-0.155]
</p><p>76 We call Pn a pair-edge incidence polytope by analogy to the pair-edge incidence matrix deÄ? [sent-783, score-0.155]
</p><p>77 3302  A SYMPTOTICS OF THE M ARGINAL L IKELIHOOD F OR T REE M ODELS  The reason to study the pair-edge incidence polytope is that its structure can be handled easily and it can be shown to be afÄ? [sent-786, score-0.116]
</p><p>78 Ĺš es (id Ä&sbquo;&mdash; fr )(2Pn ) = Ă&#x17D;&ldquo;(QĂ&#x17D;Â´ ) because, for each point, yr = 2 if and only if the path crosses r and for any other node yv = 2 if and only if the path crosses v and v is the root of the path, that is if the path crosses both children of v. [sent-798, score-0.272]
</p><p>79 Lemma 25 Let Pn Ă˘&Scaron;&sbquo; Rne be the pair-edge incidence polytope for a trivalent tree with n leaves where n Ă˘&permil;Ä˝ 4. [sent-799, score-0.371]
</p><p>80 Moreover since each path necessarily crosses two terminal edges then each point generating Pn satisÄ? [sent-810, score-0.134]
</p><p>81 Consider any cherry {e1 , e2 } Ă˘&Scaron;&sbquo; E in the tree given by two leaves, which we denote by 1, 2, and the separating inner node a. [sent-822, score-0.178]
</p><p>82 The projection Ä&#x17D;&euro;(Qn ) is described by all the triples of inequalities for all the inner nodes apart from the one incident with the cherry and the deÄ? [sent-826, score-0.178]
</p><p>83 eĂ˘&circ;&circ;E0 \{e1 ,e2 }  Denote the edge incident with e1 , e2 by e3 and the related coordinates of x by xe1 , xe2 , xe3 . [sent-828, score-0.119]
</p><p>84 Ĺš ne a polyhedral cone and the equation Ă˘&circ;&lsquo;eĂ˘&circ;&circ;E0 \{e1 ,e2 } xe = t for t Ă˘&permil;Ä˝ 0 cuts out a bounded slice of the cone which is equal to t Ă&sbquo;Ë&Dagger; PnĂ˘&circ;&rsquo;1 . [sent-833, score-0.161]
</p><p>85 Since PnĂ˘&circ;&rsquo;1 = QnĂ˘&circ;&rsquo;1 and QnĂ˘&circ;&rsquo;1 satisfy the equation Ă˘&circ;&lsquo;eĂ˘&circ;&circ;E0 \{e1 ,e2 } xe + xe3 = 2, sum of all the other coordinates related to the terminal edges of the smaller tree is 2. [sent-847, score-0.3]
</p><p>86 But since ri j Ă˘&circ;&circ; Qn must also satisfy the equation Ă˘&circ;&lsquo;eĂ˘&circ;&circ;E0 xe = 2, and, since we already have  Ă˘&circ;&lsquo;  xe = 2,  eĂ˘&circ;&circ;E0 \{e1 ,e2 }  then xe1 + xe2 = 0 and hence xe1 = xe2 = 0. [sent-849, score-0.218]
</p><p>87 Second, if pi j is a vertex of PnĂ˘&circ;&rsquo;1 such that xe3 = 1, then the sum of all the other coordinates of pi j related to the terminal edges of the smaller tree is 1. [sent-851, score-0.191]
</p><p>88 Finally, we can easily check that zero lifts uniquely to a point in Pn corresponding to the path E(12) joining the leaves 1 and 2. [sent-855, score-0.133]
</p><p>89 Lemma 28 (Computing multiplicities) Let T be a trivalent tree with n Ă˘&permil;Ä˝ 4 leaves, rooted in r. [sent-874, score-0.241]
</p><p>90 Each facet of Ă&#x17D;&ldquo;+ (QĂ&#x17D;Â´ ) is decomposed as a face of the standard cone RĂ˘&permil;Ä˝0 Ă˘&Scaron;&sbquo; VĂ&#x17D;Â´ plus a face of Ă&#x17D;&ldquo;(QĂ&#x17D;Â´ ). [sent-881, score-0.169]
</p><p>91 We say that a face of Ă&#x17D;&ldquo;(QĂ&#x17D;Â´ ) induces a facet of Ă&#x17D;&ldquo;+ (QĂ&#x17D;Â´ ) if there exists a face of the standard ne +|Ă&#x17D;Â´| cone RĂ˘&permil;Ä˝0 such that the Minkowski sum of these two faces gives a facet of Ă&#x17D;&ldquo;+ (QĂ&#x17D;Â´ ). [sent-882, score-0.276]
</p><p>92 Since the dimension Ă&#x17D;&ldquo;(QĂ&#x17D;Â´ ) is lower than the dimension of the resulting polyhedron it turns out that one face of Ă&#x17D;&ldquo;(QĂ&#x17D;Â´ ) can induce more than one facet of Ă&#x17D;&ldquo;+ (QĂ&#x17D;Â´ ). [sent-883, score-0.164]
</p><p>93 n Second, if Ă&#x17D;Â´r = 1 and Ă&#x17D;Â´v = 1 for all children of r in T then since all the nodes adjacent to r (denote them by a, b, c) are inner we have three different ways of conducting the construction of the n-path network in Construction 23 (by omitting each of the incident edges). [sent-914, score-0.146]
</p><p>94 Ĺš es xra = xrb = xrc = 3n and xe = n for all the other edges; 4 8 2 yr = n , ya = yb = yc = 3n and yv = n for all the other inner nodes. [sent-916, score-0.317]
</p><p>95 By the facet description of Ă&#x17D;&ldquo;(QĂ&#x17D;Â´ ) derived in Proposition 26 we can check that this point cannot lie in any of the facets deÄ? [sent-918, score-0.123]
</p><p>96 Ĺš cient of xrb one deduces 4 that tv = 0 for all inner nodes v. [sent-936, score-0.143]
</p><p>97 The facet description of the Newton polyhedron Ă&#x17D;&ldquo;+ (QĂ&#x17D;Â´ ) can be easily computed using P OLYMAKE Gawrilow and Joswig (2005). [sent-941, score-0.134]
</p><p>98 By Theorem 11 and Theorem 14 this real log-canonical threshold is equal to RLCTĂ˘&bdquo;Ĺ&scaron;T (I ), where I is the ideal deÄ? [sent-950, score-0.129]
</p><p>99 If T is rooted in the inner node the expansion for EFN follows from Theorem 4 in Rusakov and Geiger (2005). [sent-954, score-0.187]
</p><p>100 Ĺš ned in the introduction is the number of inner nodes of T whose degree in T is i. [sent-959, score-0.14]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rlct', 0.814), ('efn', 0.128), ('xe', 0.109), ('leaves', 0.106), ('wiernik', 0.106), ('pn', 0.101), ('symptotics', 0.098), ('trivalent', 0.098), ('yv', 0.092), ('rooted', 0.092), ('zwiernik', 0.09), ('mult', 0.09), ('bic', 0.084), ('arginal', 0.083), ('facet', 0.083), ('ree', 0.083), ('qn', 0.08), ('polytope', 0.077), ('rne', 0.075), ('ww', 0.074), ('sv', 0.071), ('mt', 0.069), ('nv', 0.064), ('watanabe', 0.064), ('sr', 0.064), ('proposition', 0.061), ('rusakov', 0.06), ('monomial', 0.058), ('ikelihood', 0.057), ('edges', 0.056), ('nodes', 0.056), ('degenerate', 0.056), ('coordinates', 0.056), ('ideal', 0.055), ('inner', 0.055), ('uv', 0.053), ('tree', 0.051), ('lift', 0.051), ('polyhedron', 0.051), ('quartet', 0.051), ('rd', 0.05), ('paths', 0.05), ('threshold', 0.05), ('ni', 0.049), ('marginal', 0.047), ('generators', 0.046), ('minkowski', 0.045), ('odels', 0.044), ('newton', 0.044), ('dim', 0.042), ('facets', 0.04), ('node', 0.04), ('geiger', 0.039), ('incidence', 0.039), ('piotr', 0.038), ('multiplicity', 0.037), ('singularity', 0.037), ('deep', 0.037), ('si', 0.036), ('incident', 0.035), ('yr', 0.035), ('likelihood', 0.033), ('cherry', 0.032), ('tv', 0.032), ('computations', 0.031), ('ti', 0.03), ('face', 0.03), ('codimension', 0.03), ('semianalytic', 0.03), ('semple', 0.03), ('theorem', 0.029), ('degree', 0.029), ('lies', 0.028), ('edge', 0.028), ('terminal', 0.028), ('coef', 0.028), ('path', 0.027), ('smith', 0.026), ('yb', 0.026), ('cone', 0.026), ('leaf', 0.026), ('formula', 0.025), ('separated', 0.025), ('fr', 0.024), ('interior', 0.024), ('real', 0.024), ('faces', 0.024), ('arnold', 0.023), ('diagram', 0.023), ('generating', 0.023), ('analytic', 0.023), ('qi', 0.023), ('barycenter', 0.023), ('deepest', 0.023), ('gawrilow', 0.023), ('ideals', 0.023), ('mihaescu', 0.023), ('pullback', 0.023), ('rooting', 0.023), ('sumio', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="9-tfidf-1" href="./jmlr-2011-An_Asymptotic_Behaviour_of_the_Marginal_Likelihood_for_General_Markov_Models.html">9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</a></p>
<p>Author: Piotr Zwiernik</p><p>Abstract: The standard Bayesian Information Criterion (BIC) is derived under regularity conditions which are not always satisÄ?Ĺš ed in the case of graphical models with hidden variables. In this paper we derive the BIC for the binary graphical tree models where all the inner nodes of a tree represent binary hidden variables. This provides an extension of a similar formula given by Rusakov and Geiger for naive Bayes models. The main tool used in this paper is the connection between the growth behavior of marginal likelihood integrals and the real log-canonical threshold. Keywords: BIC, marginal likelihood, singular models, tree models, Bayesian networks, real logcanonical threshold</p><p>2 0.075733013 <a title="9-tfidf-2" href="./jmlr-2011-Learning_Latent_Tree_Graphical_Models.html">54 jmlr-2011-Learning Latent Tree Graphical Models</a></p>
<p>Author: Myung Jin Choi, Vincent Y. F. Tan, Animashree Anandkumar, Alan S. Willsky</p><p>Abstract: We study the problem of learning a latent tree graphical model where samples are available only from a subset of variables. We propose two consistent and computationally efﬁcient algorithms for learning minimal latent trees, that is, trees without any redundant hidden nodes. Unlike many existing methods, the observed nodes (or variables) are not constrained to be leaf nodes. Our algorithms can be applied to both discrete and Gaussian random variables and our learned models are such that all the observed and latent variables have the same domain (state space). Our ﬁrst algorithm, recursive grouping, builds the latent tree recursively by identifying sibling groups using so-called information distances. One of the main contributions of this work is our second algorithm, which we refer to as CLGrouping. CLGrouping starts with a pre-processing procedure in which a tree over the observed variables is constructed. This global step groups the observed nodes that are likely to be close to each other in the true latent tree, thereby guiding subsequent recursive grouping (or equivalent procedures such as neighbor-joining) on much smaller subsets of variables. This results in more accurate and efﬁcient learning of latent trees. We also present regularized versions of our algorithms that learn latent tree approximations of arbitrary distributions. We compare the proposed algorithms to other methods by performing extensive numerical experiments on various latent tree graphical models such as hidden Markov models and star graphs. In addition, we demonstrate the applicability of our methods on real-world data sets by modeling the dependency structure of monthly stock returns in the S&P; index and of the words in the 20 newsgroups data set. Keywords: graphical models, Markov random ﬁelds, hidden variables, latent tree models, structure learning c 2011 Myung Jin Choi, Vincent Y. F. Tan, Animashree Anandkumar and Alan S. Willsky. C HOI , TAN , A NANDKUMAR AND W ILLSKY</p><p>3 0.067602798 <a title="9-tfidf-3" href="./jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</a></p>
<p>Author: Vincent Y.F. Tan, Animashree Anandkumar, Alan S. Willsky</p><p>Abstract: The problem of learning forest-structured discrete graphical models from i.i.d. samples is considered. An algorithm based on pruning of the Chow-Liu tree through adaptive thresholding is proposed. It is shown that this algorithm is both structurally consistent and risk consistent and the error probability of structure learning decays faster than any polynomial in the number of samples under ﬁxed model size. For the high-dimensional scenario where the size of the model d and the number of edges k scale with the number of samples n, sufﬁcient conditions on (n, d, k) are given for the algorithm to satisfy structural and risk consistencies. In addition, the extremal structures for learning are identiﬁed; we prove that the independent (resp., tree) model is the hardest (resp., easiest) to learn using the proposed algorithm in terms of error rates for structure learning. Keywords: graphical models, forest distributions, structural consistency, risk consistency, method of types</p><p>4 0.058835536 <a title="9-tfidf-4" href="./jmlr-2011-Efficient_Structure_Learning_of_Bayesian_Networks_using_Constraints.html">30 jmlr-2011-Efficient Structure Learning of Bayesian Networks using Constraints</a></p>
<p>Author: Cassio P. de Campos, Qiang Ji</p><p>Abstract: This paper addresses the problem of learning Bayesian network structures from data based on score functions that are decomposable. It describes properties that strongly reduce the time and memory costs of many known methods without losing global optimality guarantees. These properties are derived for different score criteria such as Minimum Description Length (or Bayesian Information Criterion), Akaike Information Criterion and Bayesian Dirichlet Criterion. Then a branch-andbound algorithm is presented that integrates structural constraints with data in a way to guarantee global optimality. As an example, structural constraints are used to map the problem of structure learning in Dynamic Bayesian networks into a corresponding augmented Bayesian network. Finally, we show empirically the beneﬁts of using the properties with state-of-the-art methods and with the new algorithm, which is able to handle larger data sets than before. Keywords: Bayesian networks, structure learning, properties of decomposable scores, structural constraints, branch-and-bound technique</p><p>5 0.047569394 <a title="9-tfidf-5" href="./jmlr-2011-Information_Rates_of_Nonparametric_Gaussian_Process_Methods.html">44 jmlr-2011-Information Rates of Nonparametric Gaussian Process Methods</a></p>
<p>Author: Aad van der Vaart, Harry van Zanten</p><p>Abstract: We consider the quality of learning a response function by a nonparametric Bayesian approach using a Gaussian process (GP) prior on the response function. We upper bound the quadratic risk of the learning procedure, which in turn is an upper bound on the Kullback-Leibler information between the predictive and true data distribution. The upper bound is expressed in small ball probabilities and concentration measures of the GP prior. We illustrate the computation of the upper bound for the Mat´ rn and squared exponential kernels. For these priors the risk, and hence the e information criterion, tends to zero for all continuous response functions. However, the rate at which this happens depends on the combination of true response function and Gaussian prior, and is expressible in a certain concentration function. In particular, the results show that for good performance, the regularity of the GP prior should match the regularity of the unknown response function. Keywords: Bayesian learning, Gaussian prior, information rate, risk, Mat´ rn kernel, squared e exponential kernel</p><p>6 0.047019362 <a title="9-tfidf-6" href="./jmlr-2011-Adaptive_Exact_Inference_in_Graphical_Models.html">7 jmlr-2011-Adaptive Exact Inference in Graphical Models</a></p>
<p>7 0.044232868 <a title="9-tfidf-7" href="./jmlr-2011-Forest_Density_Estimation.html">35 jmlr-2011-Forest Density Estimation</a></p>
<p>8 0.03879365 <a title="9-tfidf-8" href="./jmlr-2011-Weisfeiler-Lehman_Graph_Kernels.html">103 jmlr-2011-Weisfeiler-Lehman Graph Kernels</a></p>
<p>9 0.036960296 <a title="9-tfidf-9" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>10 0.033877354 <a title="9-tfidf-10" href="./jmlr-2011-Discriminative_Learning_of_Bayesian_Networks_via_Factorized_Conditional_Log-Likelihood.html">25 jmlr-2011-Discriminative Learning of Bayesian Networks via Factorized Conditional Log-Likelihood</a></p>
<p>11 0.031762902 <a title="9-tfidf-11" href="./jmlr-2011-Convergence_of_Distributed_Asynchronous_Learning_Vector_Quantization_Algorithms.html">19 jmlr-2011-Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms</a></p>
<p>12 0.031093635 <a title="9-tfidf-12" href="./jmlr-2011-Internal_Regret_with_Partial_Monitoring%3A_Calibration-Based_Optimal_Algorithms.html">45 jmlr-2011-Internal Regret with Partial Monitoring: Calibration-Based Optimal Algorithms</a></p>
<p>13 0.030215716 <a title="9-tfidf-13" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<p>14 0.030066304 <a title="9-tfidf-14" href="./jmlr-2011-High-dimensional_Covariance_Estimation_Based_On_Gaussian_Graphical_Models.html">39 jmlr-2011-High-dimensional Covariance Estimation Based On Gaussian Graphical Models</a></p>
<p>15 0.029079935 <a title="9-tfidf-15" href="./jmlr-2011-Operator_Norm_Convergence_of_Spectral_Clustering_on_Level_Sets.html">74 jmlr-2011-Operator Norm Convergence of Spectral Clustering on Level Sets</a></p>
<p>16 0.027422238 <a title="9-tfidf-16" href="./jmlr-2011-X-Armed_Bandits.html">104 jmlr-2011-X-Armed Bandits</a></p>
<p>17 0.025591351 <a title="9-tfidf-17" href="./jmlr-2011-Cumulative_Distribution_Networks_and_the_Derivative-sum-product_Algorithm%3A_Models_and_Inference_for_Cumulative_Distribution_Functions_on_Graphs.html">21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</a></p>
<p>18 0.025435273 <a title="9-tfidf-18" href="./jmlr-2011-On_the_Relation_between_Realizable_and_Nonrealizable_Cases_of_the_Sequence_Prediction_Problem.html">72 jmlr-2011-On the Relation between Realizable and Nonrealizable Cases of the Sequence Prediction Problem</a></p>
<p>19 0.024784729 <a title="9-tfidf-19" href="./jmlr-2011-Approximate_Marginals_in_Latent_Gaussian_Models.html">11 jmlr-2011-Approximate Marginals in Latent Gaussian Models</a></p>
<p>20 0.024483249 <a title="9-tfidf-20" href="./jmlr-2011-In_All_Likelihood%2C_Deep_Belief_Is_Not_Enough.html">42 jmlr-2011-In All Likelihood, Deep Belief Is Not Enough</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.145), (1, -0.058), (2, -0.028), (3, 0.004), (4, 0.055), (5, -0.003), (6, 0.009), (7, -0.012), (8, 0.03), (9, 0.22), (10, -0.107), (11, 0.032), (12, -0.077), (13, 0.002), (14, -0.01), (15, -0.057), (16, 0.026), (17, -0.043), (18, 0.029), (19, 0.007), (20, 0.082), (21, -0.047), (22, 0.057), (23, 0.049), (24, -0.052), (25, 0.023), (26, -0.09), (27, -0.038), (28, -0.16), (29, -0.027), (30, 0.027), (31, -0.016), (32, -0.067), (33, 0.12), (34, -0.175), (35, 0.124), (36, 0.203), (37, -0.076), (38, 0.109), (39, 0.013), (40, 0.172), (41, 0.097), (42, -0.028), (43, 0.056), (44, -0.091), (45, -0.245), (46, -0.293), (47, 0.272), (48, -0.025), (49, 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92859477 <a title="9-lsi-1" href="./jmlr-2011-An_Asymptotic_Behaviour_of_the_Marginal_Likelihood_for_General_Markov_Models.html">9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</a></p>
<p>Author: Piotr Zwiernik</p><p>Abstract: The standard Bayesian Information Criterion (BIC) is derived under regularity conditions which are not always satisÄ?Ĺš ed in the case of graphical models with hidden variables. In this paper we derive the BIC for the binary graphical tree models where all the inner nodes of a tree represent binary hidden variables. This provides an extension of a similar formula given by Rusakov and Geiger for naive Bayes models. The main tool used in this paper is the connection between the growth behavior of marginal likelihood integrals and the real log-canonical threshold. Keywords: BIC, marginal likelihood, singular models, tree models, Bayesian networks, real logcanonical threshold</p><p>2 0.33078137 <a title="9-lsi-2" href="./jmlr-2011-Learning_Latent_Tree_Graphical_Models.html">54 jmlr-2011-Learning Latent Tree Graphical Models</a></p>
<p>Author: Myung Jin Choi, Vincent Y. F. Tan, Animashree Anandkumar, Alan S. Willsky</p><p>Abstract: We study the problem of learning a latent tree graphical model where samples are available only from a subset of variables. We propose two consistent and computationally efﬁcient algorithms for learning minimal latent trees, that is, trees without any redundant hidden nodes. Unlike many existing methods, the observed nodes (or variables) are not constrained to be leaf nodes. Our algorithms can be applied to both discrete and Gaussian random variables and our learned models are such that all the observed and latent variables have the same domain (state space). Our ﬁrst algorithm, recursive grouping, builds the latent tree recursively by identifying sibling groups using so-called information distances. One of the main contributions of this work is our second algorithm, which we refer to as CLGrouping. CLGrouping starts with a pre-processing procedure in which a tree over the observed variables is constructed. This global step groups the observed nodes that are likely to be close to each other in the true latent tree, thereby guiding subsequent recursive grouping (or equivalent procedures such as neighbor-joining) on much smaller subsets of variables. This results in more accurate and efﬁcient learning of latent trees. We also present regularized versions of our algorithms that learn latent tree approximations of arbitrary distributions. We compare the proposed algorithms to other methods by performing extensive numerical experiments on various latent tree graphical models such as hidden Markov models and star graphs. In addition, we demonstrate the applicability of our methods on real-world data sets by modeling the dependency structure of monthly stock returns in the S&P; index and of the words in the 20 newsgroups data set. Keywords: graphical models, Markov random ﬁelds, hidden variables, latent tree models, structure learning c 2011 Myung Jin Choi, Vincent Y. F. Tan, Animashree Anandkumar and Alan S. Willsky. C HOI , TAN , A NANDKUMAR AND W ILLSKY</p><p>3 0.29566178 <a title="9-lsi-3" href="./jmlr-2011-Adaptive_Exact_Inference_in_Graphical_Models.html">7 jmlr-2011-Adaptive Exact Inference in Graphical Models</a></p>
<p>Author: Özgür Sümer, Umut A. Acar, Alexander T. Ihler, Ramgopal R. Mettu</p><p>Abstract: Many algorithms and applications involve repeatedly solving variations of the same inference problem, for example to introduce new evidence to the model or to change conditional dependencies. As the model is updated, the goal of adaptive inference is to take advantage of previously computed quantities to perform inference more rapidly than from scratch. In this paper, we present algorithms for adaptive exact inference on general graphs that can be used to efﬁciently compute marginals and update MAP conﬁgurations under arbitrary changes to the input factor graph and its associated elimination tree. After a linear time preprocessing step, our approach enables updates to the model and the computation of any marginal in time that is logarithmic in the size of the input model. Moreover, in contrast to max-product our approach can also be used to update MAP conﬁgurations in time that is roughly proportional to the number of updated entries, rather than the size of the input model. To evaluate the practical effectiveness of our algorithms, we implement and test them using synthetic data as well as for two real-world computational biology applications. Our experiments show that adaptive inference can achieve substantial speedups over performing complete inference as the model undergoes small changes over time. Keywords: exact inference, factor graphs, factor elimination, marginalization, dynamic programming, MAP computation, model updates, parallel tree contraction ¨ u u c 2011 Ozg¨ r S¨ mer, Umut A. Acar, Alexander T. Ihler and Ramgopal R. Mettu. ¨ S UMER , ACAR , I HLER AND M ETTU</p><p>4 0.27779272 <a title="9-lsi-4" href="./jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</a></p>
<p>Author: Vincent Y.F. Tan, Animashree Anandkumar, Alan S. Willsky</p><p>Abstract: The problem of learning forest-structured discrete graphical models from i.i.d. samples is considered. An algorithm based on pruning of the Chow-Liu tree through adaptive thresholding is proposed. It is shown that this algorithm is both structurally consistent and risk consistent and the error probability of structure learning decays faster than any polynomial in the number of samples under ﬁxed model size. For the high-dimensional scenario where the size of the model d and the number of edges k scale with the number of samples n, sufﬁcient conditions on (n, d, k) are given for the algorithm to satisfy structural and risk consistencies. In addition, the extremal structures for learning are identiﬁed; we prove that the independent (resp., tree) model is the hardest (resp., easiest) to learn using the proposed algorithm in terms of error rates for structure learning. Keywords: graphical models, forest distributions, structural consistency, risk consistency, method of types</p><p>5 0.25140098 <a title="9-lsi-5" href="./jmlr-2011-Information_Rates_of_Nonparametric_Gaussian_Process_Methods.html">44 jmlr-2011-Information Rates of Nonparametric Gaussian Process Methods</a></p>
<p>Author: Aad van der Vaart, Harry van Zanten</p><p>Abstract: We consider the quality of learning a response function by a nonparametric Bayesian approach using a Gaussian process (GP) prior on the response function. We upper bound the quadratic risk of the learning procedure, which in turn is an upper bound on the Kullback-Leibler information between the predictive and true data distribution. The upper bound is expressed in small ball probabilities and concentration measures of the GP prior. We illustrate the computation of the upper bound for the Mat´ rn and squared exponential kernels. For these priors the risk, and hence the e information criterion, tends to zero for all continuous response functions. However, the rate at which this happens depends on the combination of true response function and Gaussian prior, and is expressible in a certain concentration function. In particular, the results show that for good performance, the regularity of the GP prior should match the regularity of the unknown response function. Keywords: Bayesian learning, Gaussian prior, information rate, risk, Mat´ rn kernel, squared e exponential kernel</p><p>6 0.24110882 <a title="9-lsi-6" href="./jmlr-2011-Weisfeiler-Lehman_Graph_Kernels.html">103 jmlr-2011-Weisfeiler-Lehman Graph Kernels</a></p>
<p>7 0.23071343 <a title="9-lsi-7" href="./jmlr-2011-Double_Updating_Online_Learning.html">28 jmlr-2011-Double Updating Online Learning</a></p>
<p>8 0.22822328 <a title="9-lsi-8" href="./jmlr-2011-Operator_Norm_Convergence_of_Spectral_Clustering_on_Level_Sets.html">74 jmlr-2011-Operator Norm Convergence of Spectral Clustering on Level Sets</a></p>
<p>9 0.20901164 <a title="9-lsi-9" href="./jmlr-2011-Neyman-Pearson_Classification%2C_Convexity_and_Stochastic_Constraints.html">69 jmlr-2011-Neyman-Pearson Classification, Convexity and Stochastic Constraints</a></p>
<p>10 0.18996562 <a title="9-lsi-10" href="./jmlr-2011-Convergence_of_Distributed_Asynchronous_Learning_Vector_Quantization_Algorithms.html">19 jmlr-2011-Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms</a></p>
<p>11 0.18528651 <a title="9-lsi-11" href="./jmlr-2011-Discriminative_Learning_of_Bayesian_Networks_via_Factorized_Conditional_Log-Likelihood.html">25 jmlr-2011-Discriminative Learning of Bayesian Networks via Factorized Conditional Log-Likelihood</a></p>
<p>12 0.18091603 <a title="9-lsi-12" href="./jmlr-2011-Structured_Variable_Selection_with_Sparsity-Inducing_Norms.html">88 jmlr-2011-Structured Variable Selection with Sparsity-Inducing Norms</a></p>
<p>13 0.18014757 <a title="9-lsi-13" href="./jmlr-2011-The_arules_R-Package_Ecosystem%3A_Analyzing_Interesting_Patterns_from_Large_Transaction_Data_Sets.html">93 jmlr-2011-The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets</a></p>
<p>14 0.17994477 <a title="9-lsi-14" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>15 0.17587306 <a title="9-lsi-15" href="./jmlr-2011-On_the_Relation_between_Realizable_and_Nonrealizable_Cases_of_the_Sequence_Prediction_Problem.html">72 jmlr-2011-On the Relation between Realizable and Nonrealizable Cases of the Sequence Prediction Problem</a></p>
<p>16 0.17316599 <a title="9-lsi-16" href="./jmlr-2011-X-Armed_Bandits.html">104 jmlr-2011-X-Armed Bandits</a></p>
<p>17 0.1651735 <a title="9-lsi-17" href="./jmlr-2011-Large_Margin_Hierarchical_Classification_with_Mutually_Exclusive_Class_Membership.html">52 jmlr-2011-Large Margin Hierarchical Classification with Mutually Exclusive Class Membership</a></p>
<p>18 0.16182244 <a title="9-lsi-18" href="./jmlr-2011-Cumulative_Distribution_Networks_and_the_Derivative-sum-product_Algorithm%3A_Models_and_Inference_for_Cumulative_Distribution_Functions_on_Graphs.html">21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</a></p>
<p>19 0.15851553 <a title="9-lsi-19" href="./jmlr-2011-Efficient_Structure_Learning_of_Bayesian_Networks_using_Constraints.html">30 jmlr-2011-Efficient Structure Learning of Bayesian Networks using Constraints</a></p>
<p>20 0.15334715 <a title="9-lsi-20" href="./jmlr-2011-Convergence_Rates_of_Efficient_Global_Optimization_Algorithms.html">18 jmlr-2011-Convergence Rates of Efficient Global Optimization Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.062), (9, 0.026), (10, 0.023), (24, 0.043), (31, 0.076), (32, 0.028), (41, 0.041), (60, 0.016), (65, 0.012), (71, 0.034), (73, 0.035), (78, 0.059), (90, 0.044), (94, 0.4)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.64783663 <a title="9-lda-1" href="./jmlr-2011-Hyper-Sparse_Optimal_Aggregation.html">40 jmlr-2011-Hyper-Sparse Optimal Aggregation</a></p>
<p>Author: Stéphane Gaïffas, Guillaume Lecué</p><p>Abstract: Given a ﬁnite set F of functions and a learning sample, the aim of an aggregation procedure is to have a risk as close as possible to risk of the best function in F. Up to now, optimal aggregation procedures are convex combinations of every elements of F. In this paper, we prove that optimal aggregation procedures combining only two functions in F exist. Such algorithms are of particular interest when F contains many irrelevant functions that should not appear in the aggregation procedure. Since selectors are suboptimal aggregation procedures, this proves that two is the minimal number of elements of F required for the construction of an optimal aggregation procedure in every situations. Then, we perform a numerical study for the problem of selection of the regularization parameters of the Lasso and the Elastic-net estimators. We compare on simulated examples our aggregation algorithms to aggregation with exponential weights, to Mallow’s Cp and to cross-validation selection procedures. Keywords: aggregation, exact oracle inequality, empirical risk minimization, empirical process theory, sparsity, Lasso, Lars</p><p>same-paper 2 0.64185393 <a title="9-lda-2" href="./jmlr-2011-An_Asymptotic_Behaviour_of_the_Marginal_Likelihood_for_General_Markov_Models.html">9 jmlr-2011-An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models</a></p>
<p>Author: Piotr Zwiernik</p><p>Abstract: The standard Bayesian Information Criterion (BIC) is derived under regularity conditions which are not always satisÄ?Ĺš ed in the case of graphical models with hidden variables. In this paper we derive the BIC for the binary graphical tree models where all the inner nodes of a tree represent binary hidden variables. This provides an extension of a similar formula given by Rusakov and Geiger for naive Bayes models. The main tool used in this paper is the connection between the growth behavior of marginal likelihood integrals and the real log-canonical threshold. Keywords: BIC, marginal likelihood, singular models, tree models, Bayesian networks, real logcanonical threshold</p><p>3 0.31226194 <a title="9-lda-3" href="./jmlr-2011-The_Sample_Complexity_of_Dictionary_Learning.html">91 jmlr-2011-The Sample Complexity of Dictionary Learning</a></p>
<p>Author: Daniel Vainsencher, Shie Mannor, Alfred M. Bruckstein</p><p>Abstract: A large set of signals can sometimes be described sparsely using a dictionary, that is, every element can be represented as a linear combination of few elements from the dictionary. Algorithms for various signal processing applications, including classiﬁcation, denoising and signal separation, learn a dictionary from a given set of signals to be represented. Can we expect that the error in representing by such a dictionary a previously unseen signal from the same source will be of similar magnitude as those for the given examples? We assume signals are generated from a ﬁxed distribution, and study these questions from a statistical learning theory perspective. We develop generalization bounds on the quality of the learned dictionary for two types of constraints on the coefﬁcient selection, as measured by the expected L2 error in representation when the dictionary is used. For the case of l1 regularized coefﬁcient selection we provide a generalnp ln(mλ)/m , where n is the dimension, p is the number of ization bound of the order of O elements in the dictionary, λ is a bound on the l1 norm of the coefﬁcient vector and m is the number of samples, which complements existing results. For the case of representing a new signal as a combination of at most k dictionary elements, we provide a bound of the order O( np ln(mk)/m) under an assumption on the closeness to orthogonality of the dictionary (low Babel function). We further show that this assumption holds for most dictionaries in high dimensions in a strong probabilistic sense. Our results also include bounds that converge as 1/m, not previously known for this problem. We provide similar results in a general setting using kernels with weak smoothness requirements. Keywords: dictionary learning, generalization bound, sparse representation</p><p>4 0.31034285 <a title="9-lda-4" href="./jmlr-2011-Operator_Norm_Convergence_of_Spectral_Clustering_on_Level_Sets.html">74 jmlr-2011-Operator Norm Convergence of Spectral Clustering on Level Sets</a></p>
<p>Author: Bruno Pelletier, Pierre Pudlo</p><p>Abstract: Following Hartigan (1975), a cluster is deﬁned as a connected component of the t-level set of the underlying density, that is, the set of points for which the density is greater than t. A clustering algorithm which combines a density estimate with spectral clustering techniques is proposed. Our algorithm is composed of two steps. First, a nonparametric density estimate is used to extract the data points for which the estimated density takes a value greater than t. Next, the extracted points are clustered based on the eigenvectors of a graph Laplacian matrix. Under mild assumptions, we prove the almost sure convergence in operator norm of the empirical graph Laplacian operator associated with the algorithm. Furthermore, we give the typical behavior of the representation of the data set into the feature space, which establishes the strong consistency of our proposed algorithm. Keywords: spectral clustering, graph, unsupervised classiﬁcation, level sets, connected components</p><p>5 0.3086184 <a title="9-lda-5" href="./jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</a></p>
<p>Author: Vincent Y.F. Tan, Animashree Anandkumar, Alan S. Willsky</p><p>Abstract: The problem of learning forest-structured discrete graphical models from i.i.d. samples is considered. An algorithm based on pruning of the Chow-Liu tree through adaptive thresholding is proposed. It is shown that this algorithm is both structurally consistent and risk consistent and the error probability of structure learning decays faster than any polynomial in the number of samples under ﬁxed model size. For the high-dimensional scenario where the size of the model d and the number of edges k scale with the number of samples n, sufﬁcient conditions on (n, d, k) are given for the algorithm to satisfy structural and risk consistencies. In addition, the extremal structures for learning are identiﬁed; we prove that the independent (resp., tree) model is the hardest (resp., easiest) to learn using the proposed algorithm in terms of error rates for structure learning. Keywords: graphical models, forest distributions, structural consistency, risk consistency, method of types</p><p>6 0.30811161 <a title="9-lda-6" href="./jmlr-2011-Posterior_Sparsity_in_Unsupervised_Dependency_Parsing.html">77 jmlr-2011-Posterior Sparsity in Unsupervised Dependency Parsing</a></p>
<p>7 0.30525613 <a title="9-lda-7" href="./jmlr-2011-Sparse_Linear_Identifiable_Multivariate_Modeling.html">86 jmlr-2011-Sparse Linear Identifiable Multivariate Modeling</a></p>
<p>8 0.3023617 <a title="9-lda-8" href="./jmlr-2011-Learning_with_Structured_Sparsity.html">59 jmlr-2011-Learning with Structured Sparsity</a></p>
<p>9 0.30107182 <a title="9-lda-9" href="./jmlr-2011-Multitask_Sparsity_via_Maximum_Entropy_Discrimination.html">67 jmlr-2011-Multitask Sparsity via Maximum Entropy Discrimination</a></p>
<p>10 0.30099112 <a title="9-lda-10" href="./jmlr-2011-A_Family_of_Simple_Non-Parametric_Kernel_Learning_Algorithms.html">4 jmlr-2011-A Family of Simple Non-Parametric Kernel Learning Algorithms</a></p>
<p>11 0.3005459 <a title="9-lda-11" href="./jmlr-2011-Minimum_Description_Length_Penalization_for_Group_and_Multi-Task_Sparse_Learning.html">64 jmlr-2011-Minimum Description Length Penalization for Group and Multi-Task Sparse Learning</a></p>
<p>12 0.300318 <a title="9-lda-12" href="./jmlr-2011-X-Armed_Bandits.html">104 jmlr-2011-X-Armed Bandits</a></p>
<p>13 0.300051 <a title="9-lda-13" href="./jmlr-2011-Clustering_Algorithms_for_Chains.html">16 jmlr-2011-Clustering Algorithms for Chains</a></p>
<p>14 0.29922971 <a title="9-lda-14" href="./jmlr-2011-Cumulative_Distribution_Networks_and_the_Derivative-sum-product_Algorithm%3A_Models_and_Inference_for_Cumulative_Distribution_Functions_on_Graphs.html">21 jmlr-2011-Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs</a></p>
<p>15 0.29812944 <a title="9-lda-15" href="./jmlr-2011-Efficient_Learning_with_Partially_Observed_Attributes.html">29 jmlr-2011-Efficient Learning with Partially Observed Attributes</a></p>
<p>16 0.29741201 <a title="9-lda-16" href="./jmlr-2011-Efficient_Structure_Learning_of_Bayesian_Networks_using_Constraints.html">30 jmlr-2011-Efficient Structure Learning of Bayesian Networks using Constraints</a></p>
<p>17 0.29709572 <a title="9-lda-17" href="./jmlr-2011-Bayesian_Co-Training.html">12 jmlr-2011-Bayesian Co-Training</a></p>
<p>18 0.29673594 <a title="9-lda-18" href="./jmlr-2011-Kernel_Analysis_of_Deep_Networks.html">48 jmlr-2011-Kernel Analysis of Deep Networks</a></p>
<p>19 0.29557797 <a title="9-lda-19" href="./jmlr-2011-Parameter_Screening_and_Optimisation_for_ILP_using_Designed_Experiments.html">76 jmlr-2011-Parameter Screening and Optimisation for ILP using Designed Experiments</a></p>
<p>20 0.29411843 <a title="9-lda-20" href="./jmlr-2011-Information%2C_Divergence_and_Risk_for_Binary_Experiments.html">43 jmlr-2011-Information, Divergence and Risk for Binary Experiments</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
