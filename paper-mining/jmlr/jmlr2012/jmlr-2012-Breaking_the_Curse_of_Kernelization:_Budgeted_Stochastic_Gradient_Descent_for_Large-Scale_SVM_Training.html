<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>23 jmlr-2012-Breaking the Curse of Kernelization: Budgeted Stochastic Gradient Descent for Large-Scale SVM Training</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-23" href="#">jmlr2012-23</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>23 jmlr-2012-Breaking the Curse of Kernelization: Budgeted Stochastic Gradient Descent for Large-Scale SVM Training</h1>
<br/><p>Source: <a title="jmlr-2012-23-pdf" href="http://jmlr.org/papers/volume13/wang12b/wang12b.pdf">pdf</a></p><p>Author: Zhuang Wang, Koby Crammer, Slobodan Vucetic</p><p>Abstract: Online algorithms that process one example at a time are advantageous when dealing with very large data or with data streams. Stochastic Gradient Descent (SGD) is such an algorithm and it is an attractive choice for online Support Vector Machine (SVM) training due to its simplicity and effectiveness. When equipped with kernel functions, similarly to other SVM learning algorithms, SGD is susceptible to the curse of kernelization that causes unbounded linear growth in model size and update time with data size. This may render SGD inapplicable to large data sets. We address this issue by presenting a class of Budgeted SGD (BSGD) algorithms for large-scale kernel SVM training which have constant space and constant time complexity per update. Speciﬁcally, BSGD keeps the number of support vectors bounded during training through several budget maintenance strategies. We treat the budget maintenance as a source of the gradient error, and show that the gap between the BSGD and the optimal SVM solutions depends on the model degradation due to budget maintenance. To minimize the gap, we study greedy budget maintenance methods based on removal, projection, and merging of support vectors. We propose budgeted versions of several popular online SVM algorithms that belong to the SGD family. We further derive BSGD algorithms for multi-class SVM training. Comprehensive empirical results show that BSGD achieves higher accuracy than the state-of-the-art budgeted online algorithms and comparable to non-budget algorithms, while achieving impressive computational efﬁciency both in time and space during training and prediction. Keywords: SVM, large-scale learning, online learning, stochastic gradient descent, kernel methods</p><p>Reference: <a title="jmlr-2012-23-reference" href="../jmlr2012_reference/jmlr-2012-Breaking_the_Curse_of_Kernelization%3A_Budgeted_Stochastic_Gradient_Descent_for_Large-Scale_SVM_Training_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Speciﬁcally, BSGD keeps the number of support vectors bounded during training through several budget maintenance strategies. [sent-12, score-0.621]
</p><p>2 We treat the budget maintenance as a source of the gradient error, and show that the gap between the BSGD and the optimal SVM solutions depends on the model degradation due to budget maintenance. [sent-13, score-1.054]
</p><p>3 To minimize the gap, we study greedy budget maintenance methods based on removal, projection, and merging of support vectors. [sent-14, score-0.733]
</p><p>4 We propose budgeted versions of several popular online SVM algorithms that belong to the SGD family. [sent-15, score-0.372]
</p><p>5 Comprehensive empirical results show that BSGD achieves higher accuracy than the state-of-the-art budgeted online algorithms and comparable to non-budget algorithms, while achieving impressive computational efﬁciency both in time and space during training and prediction. [sent-17, score-0.446]
</p><p>6 To solve the problem, budgeted online SVM algorithms (Crammer et al. [sent-57, score-0.372]
</p><p>7 In practice, the assigned budget depends on the speciﬁc application requirements, such as memory limitations, processing speed, or data throughput. [sent-59, score-0.344]
</p><p>8 First, we propose a budgeted version of the kernelized SGD for SVM that has constant update time and constant space. [sent-62, score-0.311]
</p><p>9 This is achieved by controlling the number of SVs through one of the several budget maintenance strategies. [sent-63, score-0.592]
</p><p>10 We study the impact of budget maintenance on SGD optimization and show that, in the limit, the gap between the loss of BSGD and the loss of the optimal solution is upper-bounded by the average model degradation induced by budget maintenance. [sent-64, score-1.031]
</p><p>11 Having shown that the quality of BSGD directly depends on the quality of budget maintenance, our ﬁnal contribution is exploring computationally efﬁcient methods to maintain an accurate low-budget classiﬁer. [sent-67, score-0.344]
</p><p>12 In this work we consider three major budget maintenance strategies: removal, projection, and merging. [sent-68, score-0.592]
</p><p>13 Both space and update time of BSGD scale quadratically with the budget size when projection is used and linearly when merging or removal are used. [sent-74, score-0.685]
</p><p>14 TVM (Wang and Vucetic, 2010b) is a recently proposed budgeted online SVM algorithm which has constant update time and constant space. [sent-155, score-0.399]
</p><p>15 To prevent the curse of kernelization, several budget maintenance strategies for the kernel perceptron have been proposed in recent work. [sent-170, score-0.748]
</p><p>16 Stoptron is a truncated version of kernel perceptron that terminates when number of SVs reaches budget B. [sent-172, score-0.46]
</p><p>17 The intuition is that the oldest SV was created when the quality of perceptron was the lowest and that its removal would be the least hurtful. [sent-182, score-0.218]
</p><p>18 The budget maintenance strategy proposed by Weston et al. [sent-187, score-0.592]
</p><p>19 The projection is designed to minimize the model weight degradation caused by removal of an SV, which requires updating the weights of the remaining SVs. [sent-198, score-0.286]
</p><p>20 It can be easily converted to the budgeted version by projecting when the budget is exceeded. [sent-200, score-0.653]
</p><p>21 SILK discards the example with the lowest absolute coefﬁcient value once the budget is exceeded (Cheng et al. [sent-201, score-0.344]
</p><p>22 The properties of budgeted online algorithms described in this subsection as well as and the BSGD algorithms presented in following sections are summarized in Table 1. [sent-208, score-0.372]
</p><p>23 After introducing a nonlinear function Φ that maps x from the input to the feature space and replacing x with Φ(x), wt can be described as t wt = ∑ j=1 α j Φ(x j ), where αj = βj  t  ∏  k= j+1  (1 − ηk λ). [sent-232, score-0.494]
</p><p>24 3109  WANG , C RAMMER AND V UCETIC  Algorithm 1 BSGD 1: Input: data S, kernel k, regularization parameter λ, budget B; 2: Initialize: b = 0, w1 = 0; 3: for i = t, 2, . [sent-240, score-0.375]
</p><p>25 We can now represent ft (x) as the kernel expansion ft (x) = wtT Φ(x) = ∑ j∈I α j k(x j , x), t  where k is the Mercer kernel induced by Φ and It is the set of indexes of all SVs in wt . [sent-245, score-0.309]
</p><p>26 3 Budgeted SGD (BSGD) To maintain a ﬁxed number of SVs, BSGD executes a budget maintenance step whenever the number of SVs exceeds a pre-deﬁned budget B (i. [sent-251, score-0.936]
</p><p>27 Here, we denote by ∆t the weight degradation caused by budget maintenance at t-th round, which is deﬁned5 as the difference between model weights before and after budget maintenance (Line 10 of Algorithm 1). [sent-257, score-1.297]
</p><p>28 We note that all budget maintenance strategies mentioned in Section 2. [sent-258, score-0.615]
</p><p>29 We describe several budget maintenance strategies for BSGD in Section 6. [sent-261, score-0.615]
</p><p>30 In the next section, we motivate different strategies by studying in the next section how budget maintenance inﬂuences the performance of SGD. [sent-262, score-0.615]
</p><p>31 Impact of Budget Maintenance on SGD This section provides an insight into the impact of budget maintenance on SGD. [sent-264, score-0.592]
</p><p>32 In the following, we quantify the optimization error introduced by budget maintenance on three known SGD algorithms. [sent-265, score-0.592]
</p><p>33 Remarks on Theorem 1: • In Theorem 1 we quantify how budget maintenance impacts the quality of SGD optimization. [sent-277, score-0.592]
</p><p>34 The ¯ ¯ results suggest that an optimal budget maintenance should attempt to minimize E. [sent-280, score-0.592]
</p><p>35 Let us assume the removal-based budget maintenance method, where, at round t, SV with index t is removed. [sent-283, score-0.592]
</p><p>36 Since our proposed budget maintenance strategy is to minimize ||Et || at each round, ||Et || ≤ 1 holds. [sent-289, score-0.592]
</p><p>37 The assumption ||Et || ≤ 1 holds when budget maintenance is achieved by removing the smallest SV, that is, t ′ = arg min j∈It ′ +1 ||α j Φ(x j )|| . [sent-302, score-0.615]
</p><p>38 (i)  When used in conjunction with kernel, wt can be described as wt = ∑ j=1 α j Φ(x j ), (i)  (i)  t  where (i)  (i)  αj = βj  t  ∏  k= j+1  (1 − ηk λ). [sent-347, score-0.494]
</p><p>39 The budget maintenance step can be achieved as (i)  (i)  (i)  Wt+1 ← Wt+1 − ∆t ⇒ wt+1 ← wt+1 − ∆t , (1)  (c)  (i)  where ∆t = [∆t . [sent-348, score-0.592]
</p><p>40 Algorithm 1 can be applied to the multi-class version after replacing scalar βt with vector βt , vector wt with matrix Wt and vector ∆t with matrix ∆t . [sent-352, score-0.247]
</p><p>41 Budget Maintenance Strategies The analysis in Sections 4 and 5 indicates that budget maintenance should attempt to minimize the averaged ¯ ¯ gradient error E. [sent-357, score-0.615]
</p><p>42 (12)  In the following, we address Problem (12) through three budget maintenance strategies: removing, projecting and merging of SV(s). [sent-360, score-0.758]
</p><p>43 Three styles of budget maintenance update rules are summarized in Algorithm 2 and discussed in more detail in the following three subsections. [sent-362, score-0.619]
</p><p>44 1 Budget Maintenance through Removal If budget maintenance removes the p-th SV, then ∆t = Φ(x p )α p , (1)  (c)  where the row vector α p = [α p . [sent-364, score-0.607]
</p><p>45 j∈It+1  3113  WANG , C RAMMER AND V UCETIC  Algorithm 2 Budget maintenance Removal: 1. [sent-369, score-0.248]
</p><p>46 2 Budget Maintenance through Projection Let us consider budget maintenance through projection. [sent-394, score-0.592]
</p><p>47 p p p∈It+1  (15)  Considering there are B + 1 SVs, evaluation of (15) requires O(B3 ) time for each budget maintenance step. [sent-402, score-0.592]
</p><p>48 It should be observed that, by selecting the smallest SV to project, it can be guaranteed that weight degradation of projection is upper bounded by weight degradation of removal for any t, for all three BSGD variants from Table 1. [sent-408, score-0.422]
</p><p>49 Since weight degradation for projection is expected to be, on average, smaller than that for removal, it is expected that the average error ¯ E would be smaller too, thus resulting in smaller gap in the average instantaneous loss . [sent-410, score-0.217]
</p><p>50 It should be observed that, by selecting the smallest SV to merge, it can be guaranteed that weight degradation of merging is upper bounded by weight degradation of removal for any t, for all three BSGD variants from Table 1. [sent-437, score-0.508]
</p><p>51 4 Relationship between Budget and Weight Degradation When budget maintenance is achieved by projection and merging, there is an additional impact of budget size ¯ on E. [sent-441, score-0.991]
</p><p>52 As budget size B grows, the density of SVs is expected to grow. [sent-442, score-0.344]
</p><p>53 As a result, the weight degradation of ¯ projection and merging is expected to decrease, thus leading to decrease in E. [sent-443, score-0.309]
</p><p>54 3 Algorithms We evaluated several budget maintenance strategies for BSGD algorithms BPegasos, BNorma, and BMP. [sent-456, score-0.615]
</p><p>55 Speciﬁcally, we explored the following budgeted online algorithms: • BPegasos+remove: multi-class BPegasos with arbitrary SV removal;11 9. [sent-457, score-0.372]
</p><p>56 These algorithms were compared to the following ofﬂine, online, and budgeted online algorithms: Ofﬂine algorithms: • LIBSVM: state-of-art ofﬂine SVM solver (Chang and Lin, 2001); we used the 1 vs rest method as the default setting for the multi-class tasks. [sent-481, score-0.372]
</p><p>57 In our experiments, we set the Projectron++ threshold such that the number of SVs equals B of the budgeted algorithms at the end of training. [sent-487, score-0.284]
</p><p>58 08  Table 3: Comparison of ofﬂine, online, and budgeted online algorithms on 5 benchmark binary classiﬁcation data sets. [sent-776, score-0.372]
</p><p>59 Among the budgeted online algorithms, for each combination of data set and budget, the best accuracy is in bold, while the accuracies that were not signiﬁcantly worse ( with p > 0. [sent-779, score-0.402]
</p><p>60 36  Table 4: Comparison of ofﬂine, online, and budgeted online algorithms on 5 benchmark multi-class data sets  Pegasos, MP, and Norma. [sent-1006, score-0.372]
</p><p>61 6 Comparison of Budgeted Algorithms On the budgeted side, BPegasos+merge and BMP+merge are the two most accurate algorithms and their accuracies are comparable on most data sets. [sent-1011, score-0.284]
</p><p>62 17  Table 5: Comparison of ofﬂine, online, and budgeted online algorithms on 4 benchmark multi-class data sets  7. [sent-1182, score-0.372]
</p><p>63 7 Best Budgeted Algorithm vs Non-budgeted Algorithms Comparing the best budgeted algorithm BPegasos+merge with modest budgets of B = 100 and 500 with the non-budgeted Pegasos and LIBSVM, we can see that it achieves very competitive accuracy. [sent-1183, score-0.284]
</p><p>64 On Covertype, Phoneme and Letter data, the accuracy gap between budget B = 500 and non-budgeted algorithms remained large and it can be explained by the complexity of these problems; for example, 30% of Covertype examples, 50% of Letter examples, and 100% of Phoneme examples became SVs in Pegasos. [sent-1187, score-0.394]
</p><p>65 In all 3 data sets, the accuracy clearly improved from B = 100 to 500, which indicates that extra budget is needed for comparable accuracy. [sent-1189, score-0.374]
</p><p>66 To better illustrate the importance of budget size on some data sets, Figure 2 shows that on Letter and Covertype, the accuracy of BPegasos+merge approaches that of Pegasos as the larger budget is used. [sent-1190, score-0.718]
</p><p>67 Interestingly, while 16K examples appear to be sufﬁcient for convergence on Letter data set, it is evident that Covertype could beneﬁt from a much larger budget than the available half million labeled examples. [sent-1191, score-0.344]
</p><p>68 5 2  3  10  1  10  10  2  3  10  10  Budget  (a)  4  10  Budget  (b)  Figure 2: The accuracy of BPegasos as a function of the budget size  accuracy improvement  2. [sent-1212, score-0.404]
</p><p>69 8 Multi-epoch vs Single-pass Training For the most accurate budgeted online algorithm BPegasos+merge, we also report its accuracy after allowing it to make 5 passes through training data. [sent-1218, score-0.431]
</p><p>70 By comparing BPegasos+merge with non-budgeted SVMs and several other budgeted algorithms from Tables 3, 4, and 5, we observe that the accuracy of BPegasos+merge consistently increases with 3121  WANG , C RAMMER AND V UCETIC  1 0. [sent-1225, score-0.314]
</p><p>71 5K Pegasos, #SV=41K TVM, B=100, finished in 29Ks BPA, B=100, finished in 4Ks BPegasos+merge, B=100, finished in 2. [sent-1230, score-0.264]
</p><p>72 IDSVM and its budgeted version TVM exhibit faster accuracy growth initially, but are surpassed by BPegasos+merge as more training examples become available. [sent-1252, score-0.343]
</p><p>73 5 2 10  3  10  4  10  5  10  6  10  length of data stream (c) Covertype  Figure 5: Accuracy evolution curves of BPegasos for different budget maintenance strategies In Figure 5 we compare evolution of accuracy of BPegasos for 3 proposed budget maintenance strategies. [sent-1279, score-1.302]
</p><p>74 As could be seen, removal is inferior to projection and merging on all 3 data sets. [sent-1280, score-0.314]
</p><p>75 On Checkerboard, removal even causes a gradual drop in accuracy after an initial moderate increase, while on the other two sets the accuracy ﬂuctuates around a very small value close to that achieved by training on 100 initial examples. [sent-1281, score-0.207]
</p><p>76 On the other hand, projection and merging result in strong and consistent accuracy increase with data stream size. [sent-1282, score-0.261]
</p><p>77 2GHz Pentium Dual Core 2 PC, Figure 4 indicates a rather impressive speed of the budgeted algorithms. [sent-1286, score-0.299]
</p><p>78 From Figure 5, it can be seen that merging and projection are very fast and are comparable to removal. [sent-1287, score-0.196]
</p><p>79 On the budgeted side, the runtime time of BPegasos with merging and projecting increases linearly with data size. [sent-1291, score-0.45]
</p><p>80 However, it is evident that BPegasos with projecting grow much faster with the budget size than costs of BPegasos with merging. [sent-1292, score-0.369]
</p><p>81 This conﬁrms the expected O(B) scaling of the merging and O(B2 ) scaling of the projection version. [sent-1293, score-0.196]
</p><p>82 11 Weight Degradation ¯ Theorems 1, 2, and 3 indicate that lower E leads to lower gap between the optimal solution and the budgeted ¯ solution. [sent-1295, score-0.304]
</p><p>83 We also argued that E decreases with budget size through three mechanisms. [sent-1296, score-0.344]
</p><p>84 In Table 6, we show ¯ how the value of E on Checkerboard data is being inﬂuenced by the budget B and, in turn, how the change ¯ in E inﬂuences accuracy. [sent-1297, score-0.344]
</p><p>85 The results also show that projection and merging achieve signiﬁcantly ¯ lower value than removal and that lower E indeed results in higher accuracy. [sent-1299, score-0.314]
</p><p>86 12 Merged vs Projected SVs In order to gain further insight into the projection and merging-based budget maintenance, in Figure 7 we compare ﬁnal SVs generated by these two strategies on the USPS data where classes are 10 digits. [sent-1301, score-0.422]
</p><p>87 We used budget B = 10 for BPegasos to explore how successful the algorithm was in revealing the 10 digits. [sent-1302, score-0.344]
</p><p>88 08  Table 6: Comparison of accuracy and averaged weight degradation for three versions of BPegasos as a function of budget size B on 10M Checkerboard examples, using same parameters (λ = 10−4 , kernel width σ = 0. [sent-1334, score-0.518]
</p><p>89 This example is a useful illustration of the main difference between projection and merging, and it can be helpful in selecting the appropriate budget maintenance strategy for a particular learning task. [sent-1341, score-0.647]
</p><p>90 We showed that budgeted versions of three popular online algorithms, Pegasos, Norma, and Margin Perceptron, can be studied under this framework. [sent-1344, score-0.372]
</p><p>91 We obtained theoretical bounds on their performance that indicate that decrease in BSGD accuracy is closely related to the model degradation due to budget maintenance. [sent-1345, score-0.449]
</p><p>92 Based on the analysis, we studied budget maintenance strategies based on removal, projection, and merging. [sent-1346, score-0.615]
</p><p>93 We experimentally evaluated the proposed BSGD algorithms in terms of accuracy, and training time and compared them with a 3125  WANG , C RAMMER AND V UCETIC  number of ofﬂine and online, as well as non-budgeted, and budgeted alternatives. [sent-1347, score-0.313]
</p><p>94 Particularly, the results show that merging is a highly attractive budget maintenance strategy for BSGD algorithms as it results in relative accurate classiﬁers while achieving linear training time scaling with support vector budget and data size. [sent-1349, score-1.106]
</p><p>95 , wN be a sequence of vectors such that w1 ∈ C and for any t > 1 wt+1 ← ∏C (wt − ηt ∇t − ∆t ), ∇t is the (sub)gradient of Pt at wt , ηt is a learning rate function, ∆t is a vector, and ∏C (w) = arg minw′ ∈C ||w′ − w||, is a projection operation that projects w to C. [sent-1361, score-0.302]
</p><p>96 ||w||2 /2 and ∇t is the subgradient of Pt (w) at wt (according to Lemma 1 by Shalev-Shwartz and Singer (2007). [sent-1372, score-0.247]
</p><p>97 Thus the update rule wt+1 ← ∏C (wt − ηt ∇t − ∆t ) in Lemma 1 is reduced to wt+1 ← wt − ηt ∇t − ∆t . [sent-1381, score-0.274]
</p><p>98 Thus the update rule wt+1 ← ∏C (wt − ηt ∇t − ∆t ) in Lemma 1 is reduced to wt+1 ← wt − ηt ∇t − ∆t . [sent-1390, score-0.274]
</p><p>99 Tracking the best hyperplane with a simple budget perceptron. [sent-1424, score-0.344]
</p><p>100 Online training on a budget of support vector machines using twin prototypes. [sent-1709, score-0.373]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bpegasos', 0.473), ('budget', 0.344), ('svs', 0.296), ('budgeted', 0.284), ('maintenance', 0.248), ('wt', 0.247), ('bsgd', 0.233), ('sv', 0.175), ('merge', 0.165), ('merging', 0.141), ('removal', 0.118), ('pegasos', 0.11), ('norma', 0.107), ('sgd', 0.106), ('svm', 0.095), ('finished', 0.088), ('ucetic', 0.088), ('udgeted', 0.088), ('online', 0.088), ('perceptron', 0.085), ('checkerboard', 0.082), ('idsvm', 0.076), ('escent', 0.076), ('radient', 0.076), ('rammer', 0.076), ('mp', 0.075), ('degradation', 0.075), ('bmp', 0.07), ('tochastic', 0.068), ('bnorma', 0.063), ('pt', 0.061), ('tvm', 0.057), ('projection', 0.055), ('covertype', 0.053), ('bpa', 0.05), ('wang', 0.047), ('crammer', 0.045), ('libsvm', 0.045), ('bordes', 0.045), ('vucetic', 0.044), ('ine', 0.041), ('yt', 0.039), ('lasvm', 0.038), ('weight', 0.038), ('xt', 0.037), ('stream', 0.035), ('letter', 0.032), ('forgetron', 0.032), ('ijcnn', 0.032), ('projectron', 0.032), ('waveform', 0.031), ('kernel', 0.031), ('accuracy', 0.03), ('training', 0.029), ('rhs', 0.029), ('instantaneous', 0.029), ('chang', 0.028), ('kivinen', 0.027), ('update', 0.027), ('zhuang', 0.025), ('tsang', 0.025), ('orabona', 0.025), ('projecting', 0.025), ('phoneme', 0.024), ('project', 0.023), ('smallest', 0.023), ('gradient', 0.023), ('margin', 0.023), ('strategies', 0.023), ('kernelization', 0.022), ('xm', 0.022), ('cauwenberghs', 0.022), ('vishwanathan', 0.021), ('pass', 0.021), ('wn', 0.02), ('dekel', 0.02), ('gap', 0.02), ('svms', 0.019), ('cvm', 0.019), ('shuttle', 0.019), ('temple', 0.019), ('incrementally', 0.019), ('usps', 0.018), ('remove', 0.018), ('singer', 0.017), ('curse', 0.017), ('dt', 0.017), ('enclosing', 0.017), ('koby', 0.017), ('hsieh', 0.017), ('prototypes', 0.016), ('rahimi', 0.016), ('dna', 0.016), ('oldest', 0.015), ('impressive', 0.015), ('removes', 0.015), ('evolution', 0.015), ('satimage', 0.015), ('adult', 0.015), ('duda', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="23-tfidf-1" href="./jmlr-2012-Breaking_the_Curse_of_Kernelization%3A_Budgeted_Stochastic_Gradient_Descent_for_Large-Scale_SVM_Training.html">23 jmlr-2012-Breaking the Curse of Kernelization: Budgeted Stochastic Gradient Descent for Large-Scale SVM Training</a></p>
<p>Author: Zhuang Wang, Koby Crammer, Slobodan Vucetic</p><p>Abstract: Online algorithms that process one example at a time are advantageous when dealing with very large data or with data streams. Stochastic Gradient Descent (SGD) is such an algorithm and it is an attractive choice for online Support Vector Machine (SVM) training due to its simplicity and effectiveness. When equipped with kernel functions, similarly to other SVM learning algorithms, SGD is susceptible to the curse of kernelization that causes unbounded linear growth in model size and update time with data size. This may render SGD inapplicable to large data sets. We address this issue by presenting a class of Budgeted SGD (BSGD) algorithms for large-scale kernel SVM training which have constant space and constant time complexity per update. Speciﬁcally, BSGD keeps the number of support vectors bounded during training through several budget maintenance strategies. We treat the budget maintenance as a source of the gradient error, and show that the gap between the BSGD and the optimal SVM solutions depends on the model degradation due to budget maintenance. To minimize the gap, we study greedy budget maintenance methods based on removal, projection, and merging of support vectors. We propose budgeted versions of several popular online SVM algorithms that belong to the SGD family. We further derive BSGD algorithms for multi-class SVM training. Comprehensive empirical results show that BSGD achieves higher accuracy than the state-of-the-art budgeted online algorithms and comparable to non-budget algorithms, while achieving impressive computational efﬁciency both in time and space during training and prediction. Keywords: SVM, large-scale learning, online learning, stochastic gradient descent, kernel methods</p><p>2 0.12879138 <a title="23-tfidf-2" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>Author: Trinh Minh Tri Do, Thierry Artières</p><p>Abstract: Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efﬁcient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efﬁcient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random ﬁelds, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy ε with a rate O(1/λε) where λ is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difﬁcult and requires a stronger and more disputable assumption. Yet we provide experimental results on artiﬁcial test problems, and on ﬁve standard and difﬁcult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms. Keywords: optimization, non-convex, non-smooth, cutting plane, bundle method, regularized risk</p><p>3 0.10482959 <a title="23-tfidf-3" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>Author: Francesco Orabona, Luo Jie, Barbara Caputo</p><p>Abstract: In recent years there has been a lot of interest in designing principled classiﬁcation algorithms over multiple cues, based on the intuitive notion that using more features should lead to better performance. In the domain of kernel methods, a principled way to use multiple features is the Multi Kernel Learning (MKL) approach. Here we present a MKL optimization algorithm based on stochastic gradient descent that has a guaranteed convergence rate. We directly solve the MKL problem in the primal formulation. By having a p-norm formulation of MKL, we introduce a parameter that controls the level of sparsity of the solution, while leading to an easier optimization problem. We prove theoretically and experimentally that 1) our algorithm has a faster convergence rate as the number of kernels grows; 2) the training complexity is linear in the number of training examples; 3) very few iterations are sufﬁcient to reach good solutions. Experiments on standard benchmark databases support our claims. Keywords: multiple kernel learning, learning kernels, online optimization, stochastic subgradient descent, convergence bounds, large scale</p><p>4 0.095652819 <a title="23-tfidf-4" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>Author: Sham M. Kakade, Shai Shalev-Shwartz, Ambuj Tewari</p><p>Abstract: There is growing body of learning problems for which it is natural to organize the parameters into a matrix. As a result, it becomes easy to impose sophisticated prior knowledge by appropriately regularizing the parameters under some matrix norm. This work describes and analyzes a systematic method for constructing such matrix-based regularization techniques. In particular, we focus on how the underlying statistical properties of a given problem can help us decide which regularization function is appropriate. Our methodology is based on a known duality phenomenon: a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. We demonstrate the potential of this framework by deriving novel generalization and regret bounds for multi-task learning, multi-class learning, and multiple kernel learning. Keywords: regularization, strong convexity, regret bounds, generalization bounds, multi-task learning, multi-class learning, multiple kernel learning</p><p>5 0.08238858 <a title="23-tfidf-5" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>Author: Sangkyun Lee, Stephen J. Wright</p><p>Abstract: Iterative methods that calculate their steps from approximate subgradient directions have proved to be useful for stochastic learning problems over large and streaming data sets. When the objective consists of a loss function plus a nonsmooth regularization term, the solution often lies on a lowdimensional manifold of parameter space along which the regularizer is smooth. (When an ℓ1 regularizer is used to induce sparsity in the solution, for example, this manifold is deﬁned by the set of nonzero components of the parameter vector.) This paper shows that a regularized dual averaging algorithm can identify this manifold, with high probability, before reaching the solution. This observation motivates an algorithmic strategy in which, once an iterate is suspected of lying on an optimal or near-optimal manifold, we switch to a “local phase” that searches in this manifold, thus converging rapidly to a near-optimal point. Computational results are presented to verify the identiﬁcation property and to illustrate the effectiveness of this approach. Keywords: regularization, dual averaging, partly smooth manifold, manifold identiﬁcation</p><p>6 0.079238348 <a title="23-tfidf-6" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>7 0.077407889 <a title="23-tfidf-7" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>8 0.065564588 <a title="23-tfidf-8" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>9 0.056152776 <a title="23-tfidf-9" href="./jmlr-2012-An_Introduction_to_Artificial_Prediction_Markets_for_Classification.html">19 jmlr-2012-An Introduction to Artificial Prediction Markets for Classification</a></p>
<p>10 0.044676095 <a title="23-tfidf-10" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>11 0.043471754 <a title="23-tfidf-11" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>12 0.043380704 <a title="23-tfidf-12" href="./jmlr-2012-PAC-Bayes_Bounds_with_Data_Dependent_Priors.html">87 jmlr-2012-PAC-Bayes Bounds with Data Dependent Priors</a></p>
<p>13 0.039921347 <a title="23-tfidf-13" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>14 0.036635332 <a title="23-tfidf-14" href="./jmlr-2012-Hope_and_Fear_for_Discriminative_Training_of_Statistical_Translation_Models.html">49 jmlr-2012-Hope and Fear for Discriminative Training of Statistical Translation Models</a></p>
<p>15 0.033565801 <a title="23-tfidf-15" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>16 0.032353453 <a title="23-tfidf-16" href="./jmlr-2012-Large-scale_Linear_Support_Vector_Regression.html">54 jmlr-2012-Large-scale Linear Support Vector Regression</a></p>
<p>17 0.032332141 <a title="23-tfidf-17" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>18 0.029365227 <a title="23-tfidf-18" href="./jmlr-2012-Online_Submodular_Minimization.html">84 jmlr-2012-Online Submodular Minimization</a></p>
<p>19 0.029315706 <a title="23-tfidf-19" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>20 0.028824231 <a title="23-tfidf-20" href="./jmlr-2012-Activized_Learning%3A_Transforming_Passive_to_Active_with_Improved_Label_Complexity.html">14 jmlr-2012-Activized Learning: Transforming Passive to Active with Improved Label Complexity</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.152), (1, -0.178), (2, 0.017), (3, 0.106), (4, 0.091), (5, 0.05), (6, 0.174), (7, -0.009), (8, 0.071), (9, -0.037), (10, -0.007), (11, 0.297), (12, -0.019), (13, -0.08), (14, 0.058), (15, 0.07), (16, 0.052), (17, 0.067), (18, -0.119), (19, 0.168), (20, -0.021), (21, 0.095), (22, -0.028), (23, -0.085), (24, -0.063), (25, 0.058), (26, -0.013), (27, 0.071), (28, -0.073), (29, -0.025), (30, 0.038), (31, -0.04), (32, 0.036), (33, -0.008), (34, -0.091), (35, -0.163), (36, -0.023), (37, 0.092), (38, -0.143), (39, -0.061), (40, -0.0), (41, 0.002), (42, 0.004), (43, -0.029), (44, -0.029), (45, -0.001), (46, -0.201), (47, -0.01), (48, -0.08), (49, 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93771458 <a title="23-lsi-1" href="./jmlr-2012-Breaking_the_Curse_of_Kernelization%3A_Budgeted_Stochastic_Gradient_Descent_for_Large-Scale_SVM_Training.html">23 jmlr-2012-Breaking the Curse of Kernelization: Budgeted Stochastic Gradient Descent for Large-Scale SVM Training</a></p>
<p>Author: Zhuang Wang, Koby Crammer, Slobodan Vucetic</p><p>Abstract: Online algorithms that process one example at a time are advantageous when dealing with very large data or with data streams. Stochastic Gradient Descent (SGD) is such an algorithm and it is an attractive choice for online Support Vector Machine (SVM) training due to its simplicity and effectiveness. When equipped with kernel functions, similarly to other SVM learning algorithms, SGD is susceptible to the curse of kernelization that causes unbounded linear growth in model size and update time with data size. This may render SGD inapplicable to large data sets. We address this issue by presenting a class of Budgeted SGD (BSGD) algorithms for large-scale kernel SVM training which have constant space and constant time complexity per update. Speciﬁcally, BSGD keeps the number of support vectors bounded during training through several budget maintenance strategies. We treat the budget maintenance as a source of the gradient error, and show that the gap between the BSGD and the optimal SVM solutions depends on the model degradation due to budget maintenance. To minimize the gap, we study greedy budget maintenance methods based on removal, projection, and merging of support vectors. We propose budgeted versions of several popular online SVM algorithms that belong to the SGD family. We further derive BSGD algorithms for multi-class SVM training. Comprehensive empirical results show that BSGD achieves higher accuracy than the state-of-the-art budgeted online algorithms and comparable to non-budget algorithms, while achieving impressive computational efﬁciency both in time and space during training and prediction. Keywords: SVM, large-scale learning, online learning, stochastic gradient descent, kernel methods</p><p>2 0.67902195 <a title="23-lsi-2" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>Author: Trinh Minh Tri Do, Thierry Artières</p><p>Abstract: Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efﬁcient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efﬁcient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random ﬁelds, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy ε with a rate O(1/λε) where λ is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difﬁcult and requires a stronger and more disputable assumption. Yet we provide experimental results on artiﬁcial test problems, and on ﬁve standard and difﬁcult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms. Keywords: optimization, non-convex, non-smooth, cutting plane, bundle method, regularized risk</p><p>3 0.48337102 <a title="23-lsi-3" href="./jmlr-2012-An_Introduction_to_Artificial_Prediction_Markets_for_Classification.html">19 jmlr-2012-An Introduction to Artificial Prediction Markets for Classification</a></p>
<p>Author: Adrian Barbu, Nathan Lay</p><p>Abstract: Prediction markets are used in real life to predict outcomes of interest such as presidential elections. This paper presents a mathematical theory of artiﬁcial prediction markets for supervised learning of conditional probability estimators. The artiﬁcial prediction market is a novel method for fusing the prediction information of features or trained classiﬁers, where the fusion result is the contract price on the possible outcomes. The market can be trained online by updating the participants’ budgets using training examples. Inspired by the real prediction markets, the equations that govern the market are derived from simple and reasonable assumptions. Efﬁcient numerical algorithms are presented for solving these equations. The obtained artiﬁcial prediction market is shown to be a maximum likelihood estimator. It generalizes linear aggregation, existent in boosting and random forest, as well as logistic regression and some kernel methods. Furthermore, the market mechanism allows the aggregation of specialized classiﬁers that participate only on speciﬁc instances. Experimental comparisons show that the artiﬁcial prediction markets often outperform random forest and implicit online learning on synthetic data and real UCI data sets. Moreover, an extensive evaluation for pelvic and abdominal lymph node detection in CT data shows that the prediction market improves adaboost’s detection rate from 79.6% to 81.2% at 3 false positives/volume. Keywords: online learning, ensemble methods, supervised learning, random forest, implicit online learning</p><p>4 0.4452821 <a title="23-lsi-4" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>Author: Gary B. Huang, Andrew Kae, Carl Doersch, Erik Learned-Miller</p><p>Abstract: We consider a model for which it is important, early in processing, to estimate some variables with high precision, but perhaps at relatively low recall. If some variables can be identiﬁed with near certainty, they can be conditioned upon, allowing further inference to be done efﬁciently. Speciﬁcally, we consider optical character recognition (OCR) systems that can be bootstrapped by identifying a subset of correctly translated document words with very high precision. This “clean set” is subsequently used as document-speciﬁc training data. While OCR systems produce conﬁdence measures for the identity of each letter or word, thresholding these values still produces a signiﬁcant number of errors. We introduce a novel technique for identifying a set of correct words with very high precision. Rather than estimating posterior probabilities, we bound the probability that any given word is incorrect using an approximate worst case analysis. We give empirical results on a data set of difﬁcult historical newspaper scans, demonstrating that our method for identifying correct words makes only two errors in 56 documents. Using document-speciﬁc character models generated from this data, we are able to reduce the error over properly segmented characters by 34.1% from an initial OCR system’s translation.1 Keywords: optical character recognition, probability bounding, document-speciﬁc modeling, computer vision 1. This work is an expanded and revised version of Kae et al. (2010). Supported by NSF Grant IIS-0916555. c 2012 Gary B. Huang, Andrew Kae, Carl Doersch and Erik Learned-Miller. H UANG , K AE , D OERSCH AND L EARNED -M ILLER</p><p>5 0.40380812 <a title="23-lsi-5" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>Author: Koby Crammer, Mark Dredze, Fernando Pereira</p><p>Abstract: Conﬁdence-weighted online learning is a generalization of margin-based learning of linear classiﬁers in which the margin constraint is replaced by a probabilistic constraint based on a distribution over classiﬁer weights that is updated online as examples are observed. The distribution captures a notion of conﬁdence on classiﬁer weights, and in some cases it can also be interpreted as replacing a single learning rate by adaptive per-weight rates. Conﬁdence-weighted learning was motivated by the statistical properties of natural-language classiﬁcation tasks, where most of the informative features are relatively rare. We investigate several versions of conﬁdence-weighted learning that use a Gaussian distribution over weight vectors, updated at each observed example to achieve high probability of correct classiﬁcation for the example. Empirical evaluation on a range of textcategorization tasks show that our algorithms improve over other state-of-the-art online and batch methods, learn faster in the online setting, and lead to better classiﬁer combination for a type of distributed training commonly used in cloud computing. Keywords: online learning, conﬁdence prediction, text categorization</p><p>6 0.39661616 <a title="23-lsi-6" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>7 0.39170265 <a title="23-lsi-7" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>8 0.35415366 <a title="23-lsi-8" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>9 0.32209757 <a title="23-lsi-9" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>10 0.31205642 <a title="23-lsi-10" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>11 0.26687014 <a title="23-lsi-11" href="./jmlr-2012-PAC-Bayes_Bounds_with_Data_Dependent_Priors.html">87 jmlr-2012-PAC-Bayes Bounds with Data Dependent Priors</a></p>
<p>12 0.2515555 <a title="23-lsi-12" href="./jmlr-2012-Large-scale_Linear_Support_Vector_Regression.html">54 jmlr-2012-Large-scale Linear Support Vector Regression</a></p>
<p>13 0.24642372 <a title="23-lsi-13" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>14 0.23372658 <a title="23-lsi-14" href="./jmlr-2012-Hope_and_Fear_for_Discriminative_Training_of_Statistical_Translation_Models.html">49 jmlr-2012-Hope and Fear for Discriminative Training of Statistical Translation Models</a></p>
<p>15 0.18200077 <a title="23-lsi-15" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>16 0.17289695 <a title="23-lsi-16" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>17 0.15054253 <a title="23-lsi-17" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>18 0.13770854 <a title="23-lsi-18" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>19 0.13459156 <a title="23-lsi-19" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>20 0.12781905 <a title="23-lsi-20" href="./jmlr-2012-Activized_Learning%3A_Transforming_Passive_to_Active_with_Improved_Label_Complexity.html">14 jmlr-2012-Activized Learning: Transforming Passive to Active with Improved Label Complexity</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.023), (14, 0.011), (21, 0.021), (26, 0.03), (27, 0.011), (29, 0.02), (35, 0.012), (44, 0.418), (56, 0.02), (64, 0.014), (69, 0.011), (75, 0.107), (77, 0.024), (92, 0.054), (96, 0.117)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71989381 <a title="23-lda-1" href="./jmlr-2012-Breaking_the_Curse_of_Kernelization%3A_Budgeted_Stochastic_Gradient_Descent_for_Large-Scale_SVM_Training.html">23 jmlr-2012-Breaking the Curse of Kernelization: Budgeted Stochastic Gradient Descent for Large-Scale SVM Training</a></p>
<p>Author: Zhuang Wang, Koby Crammer, Slobodan Vucetic</p><p>Abstract: Online algorithms that process one example at a time are advantageous when dealing with very large data or with data streams. Stochastic Gradient Descent (SGD) is such an algorithm and it is an attractive choice for online Support Vector Machine (SVM) training due to its simplicity and effectiveness. When equipped with kernel functions, similarly to other SVM learning algorithms, SGD is susceptible to the curse of kernelization that causes unbounded linear growth in model size and update time with data size. This may render SGD inapplicable to large data sets. We address this issue by presenting a class of Budgeted SGD (BSGD) algorithms for large-scale kernel SVM training which have constant space and constant time complexity per update. Speciﬁcally, BSGD keeps the number of support vectors bounded during training through several budget maintenance strategies. We treat the budget maintenance as a source of the gradient error, and show that the gap between the BSGD and the optimal SVM solutions depends on the model degradation due to budget maintenance. To minimize the gap, we study greedy budget maintenance methods based on removal, projection, and merging of support vectors. We propose budgeted versions of several popular online SVM algorithms that belong to the SGD family. We further derive BSGD algorithms for multi-class SVM training. Comprehensive empirical results show that BSGD achieves higher accuracy than the state-of-the-art budgeted online algorithms and comparable to non-budget algorithms, while achieving impressive computational efﬁciency both in time and space during training and prediction. Keywords: SVM, large-scale learning, online learning, stochastic gradient descent, kernel methods</p><p>2 0.57196838 <a title="23-lda-2" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>Author: David P. Helmbold, Philip M. Long</p><p>Abstract: This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classiﬁers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high conﬁdence whether or not any individual variable is relevant. Keywords: feature selection, generalization, learning theory</p><p>3 0.37301648 <a title="23-lda-3" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>Author: Fei Yan, Josef Kittler, Krystian Mikolajczyk, Atif Tahir</p><p>Abstract: Sparsity-inducing multiple kernel Fisher discriminant analysis (MK-FDA) has been studied in the literature. Building on recent advances in non-sparse multiple kernel learning (MKL), we propose a non-sparse version of MK-FDA, which imposes a general ℓ p norm regularisation on the kernel weights. We formulate the associated optimisation problem as a semi-inﬁnite program (SIP), and adapt an iterative wrapper algorithm to solve it. We then discuss, in light of latest advances in MKL optimisation techniques, several reformulations and optimisation strategies that can potentially lead to signiﬁcant improvements in the efﬁciency and scalability of MK-FDA. We carry out extensive experiments on six datasets from various application areas, and compare closely the performance of ℓ p MK-FDA, ﬁxed norm MK-FDA, and several variants of SVM-based MKL (MK-SVM). Our results demonstrate that ℓ p MK-FDA improves upon sparse MK-FDA in many practical situations. The results also show that on image categorisation problems, ℓ p MK-FDA tends to outperform its SVM counterpart. Finally, we also discuss the connection between (MK-)FDA and (MK-)SVM, under the uniﬁed framework of regularised kernel machines. Keywords: multiple kernel learning, kernel ﬁsher discriminant analysis, regularised least squares, support vector machines</p><p>4 0.37151587 <a title="23-lda-4" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>Author: Francesco Orabona, Luo Jie, Barbara Caputo</p><p>Abstract: In recent years there has been a lot of interest in designing principled classiﬁcation algorithms over multiple cues, based on the intuitive notion that using more features should lead to better performance. In the domain of kernel methods, a principled way to use multiple features is the Multi Kernel Learning (MKL) approach. Here we present a MKL optimization algorithm based on stochastic gradient descent that has a guaranteed convergence rate. We directly solve the MKL problem in the primal formulation. By having a p-norm formulation of MKL, we introduce a parameter that controls the level of sparsity of the solution, while leading to an easier optimization problem. We prove theoretically and experimentally that 1) our algorithm has a faster convergence rate as the number of kernels grows; 2) the training complexity is linear in the number of training examples; 3) very few iterations are sufﬁcient to reach good solutions. Experiments on standard benchmark databases support our claims. Keywords: multiple kernel learning, learning kernels, online optimization, stochastic subgradient descent, convergence bounds, large scale</p><p>5 0.36449143 <a title="23-lda-5" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>Author: Ofer Dekel, Claudio Gentile, Karthik Sridharan</p><p>Abstract: We present a new online learning algorithm in the selective sampling framework, where labels must be actively queried before they are revealed. We prove bounds on the regret of our algorithm and on the number of labels it queries when faced with an adaptive adversarial strategy of generating the instances. Our bounds both generalize and strictly improve over previous bounds in similar settings. Additionally, our selective sampling algorithm can be converted into an efﬁcient statistical active learning algorithm. We extend our algorithm and analysis to the multiple-teacher setting, where the algorithm can choose which subset of teachers to query for each label. Finally, we demonstrate the effectiveness of our techniques on a real-world Internet search problem. Keywords: online learning, regret, label-efﬁcient, crowdsourcing</p><p>6 0.3637341 <a title="23-lda-6" href="./jmlr-2012-Feature_Selection_via_Dependence_Maximization.html">44 jmlr-2012-Feature Selection via Dependence Maximization</a></p>
<p>7 0.36104521 <a title="23-lda-7" href="./jmlr-2012-Activized_Learning%3A_Transforming_Passive_to_Active_with_Improved_Label_Complexity.html">14 jmlr-2012-Activized Learning: Transforming Passive to Active with Improved Label Complexity</a></p>
<p>8 0.35755765 <a title="23-lda-8" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>9 0.35735059 <a title="23-lda-9" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>10 0.35272104 <a title="23-lda-10" href="./jmlr-2012-Static_Prediction_Games_for_Adversarial_Learning_Problems.html">110 jmlr-2012-Static Prediction Games for Adversarial Learning Problems</a></p>
<p>11 0.34973493 <a title="23-lda-11" href="./jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems.html">89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</a></p>
<p>12 0.34833747 <a title="23-lda-12" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>13 0.34745973 <a title="23-lda-13" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>14 0.34660649 <a title="23-lda-14" href="./jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</a></p>
<p>15 0.34647414 <a title="23-lda-15" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>16 0.34564048 <a title="23-lda-16" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>17 0.34400743 <a title="23-lda-17" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>18 0.34287602 <a title="23-lda-18" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>19 0.34237415 <a title="23-lda-19" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>20 0.34152234 <a title="23-lda-20" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
