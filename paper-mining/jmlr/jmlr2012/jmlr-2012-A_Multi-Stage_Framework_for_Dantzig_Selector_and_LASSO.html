<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-7" href="#">jmlr2012-7</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</h1>
<br/><p>Source: <a title="jmlr-2012-7-pdf" href="http://jmlr.org/papers/volume13/liu12a/liu12a.pdf">pdf</a></p><p>Author: Ji Liu, Peter Wonka, Jieping Ye</p><p>Abstract: We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ Rn×m (m ≫ n) and a noisy observation vector y ∈ Rn satisfying y = Xβ∗ + ε where ε is the noise vector following a Gaussian distribution N(0, σ2 I), how to recover the signal (or parameter vector) β∗ when the signal is sparse? The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively reﬁnes the target signal β∗ . We show that if X obeys a certain condition, then with a large probability ˆ the difference between the solution β estimated by the proposed method and the true solution β∗ measured in terms of the ℓ p norm (p ≥ 1) is bounded as ˆ β − β∗ p ≤ C(s − N)1/p log m + ∆ σ, where C is a constant, s is the number of nonzero entries in β∗ , the risk of the oracle estimator ∆ is independent of m and is much smaller than the ﬁrst term, and N is the number of entries of β∗ larger √ than a certain value in the order of O (σ log m). The proposed √ method improves the estimation √ bound of the standard Dantzig selector approximately from Cs1/p log mσ to C(s − N)1/p log mσ where the value N depends on the number of large entries in β∗ . When N = s, the proposed algorithm achieves the oracle solution with a high probability, where the oracle solution is the projection of the observation vector y onto true features. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector. Finally, we extend this multi-stage procedure to the LASSO case. Keywords: multi-stage, Dantzig selector, LASSO, sparse signal recovery</p><p>Reference: <a title="jmlr-2012-7-reference" href="../jmlr2012_reference/jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. [sent-8, score-0.526]
</p><p>2 In this paper, we propose a multi-stage Dantzig selector method, which iteratively reﬁnes the target signal β∗ . [sent-9, score-0.372]
</p><p>3 The proposed √ method improves the estimation √ bound of the standard Dantzig selector approximately from Cs1/p log mσ to C(s − N)1/p log mσ where the value N depends on the number of large entries in β∗ . [sent-11, score-0.438]
</p><p>4 When N = s, the proposed algorithm achieves the oracle solution with a high probability, where the oracle solution is the projection of the observation vector y onto true features. [sent-12, score-0.473]
</p><p>5 Keywords: multi-stage, Dantzig selector, LASSO, sparse signal recovery  1. [sent-15, score-0.218]
</p><p>6 Introduction The sparse signal recovery problem has been studied in many areas including machine learning (Zhang, 2009b; Zhao and Yu, 2006), signal processing (Donoho et al. [sent-16, score-0.301]
</p><p>7 In the sparse signal recovery problem, one is mainly interested in the signal ˆ recovery accuracy, that is, the distance between the estimation β and the original signal or the true ∗ . [sent-21, score-0.495]
</p><p>8 If the design matrix X is considered as a feature matrix, that is, each column is a feature solution β vector, and the observation y as a target object vector, then the sparse signal recovery problem is c 2012 Ji Liu, Peter Wonka and Jieping Ye. [sent-22, score-0.319]
</p><p>9 Typically, a group of features corresponding to the coefﬁcient values in β larger than a threshold form the supporting feature set. [sent-25, score-0.22]
</p><p>10 Two well-known algorithms for learning sparse signals include LASSO (Tibshirani, 1996) and Dantzig selector (Cand` s and Tao, 2007): e LASSO  min : β  Dantzig Selector  1 Xβ − y 2  2 ′ 2 + λ ||β||1 ,  min : ||β||1 β  s. [sent-29, score-0.313]
</p><p>11 Strong theoretical results concerning LASSO and Dantzig selector have been established in the literature (Cai et al. [sent-32, score-0.289]
</p><p>12 1 Contributions In this paper, we propose a multi-stage procedure based on the Dantzig selector, which estimates ˆ the supporting feature set F0 and the signal β iteratively. [sent-35, score-0.214]
</p><p>13 At each iteration, we employ the basic framework of Dantzig selector and the ˆ information about the current supporting feature set F0 to estimate the new signal β. [sent-38, score-0.503]
</p><p>14 In addition, we select the supporting feature candidates in F0 among all features in the data at each iteration, thus allowing to remove incorrect features from the previous supporting feature set. [sent-39, score-0.31]
</p><p>15 2 Related Work Sparse signal recovery without observation noise was studied by Cand` s and Tao (2005), which e showed under the restricted isometry property (RIP) sparse signals can be perfectly recovered by solving an ℓ1 norm minimization problem. [sent-44, score-0.234]
</p><p>16 LASSO and Dantzig selector can be considered as its noisy versions. [sent-45, score-0.289]
</p><p>17 A comprehensive analysis for LASSO, including the recovery accuracy in an arbitrary ℓ p norm (p ≥ 1) and the feature selection consistency, was presented in Zhang (2009a). [sent-48, score-0.176]
</p><p>18 Cand` s and Tao (2007) proposed the Dantzig selector (which is a e linear programming problem) for sparse signal recovery and presented a bound of recovery accuracy with the same order as LASSO under the uniform uncertainty principle (UUP). [sent-49, score-0.665]
</p><p>19 An approximate equivalence between the LASSO estimator and the Dantzig selector was given by Bickel et al. [sent-50, score-0.289]
</p><p>20 (2009) provided conditions on the design matrix X under which the LASSO and Dantzig selector coefﬁcient estimates are identical for certain tuning parameters. [sent-54, score-0.308]
</p><p>21 Since convex regularization methods like LASSO and Dantzig selector give biased estimation due to convex regularization, many heuristic methods have been proposed to correct the bias of convex relaxation recently, including orthogonal matching pursuit (OMP) (Tropp, 2004; Donoho et al. [sent-56, score-0.352]
</p><p>22 It was shown that under exact recovery condition (ERC) (similar to MIP) the solution of OMP guarantees the feature selection consistency in the noiseless case (Tropp, 2004). [sent-59, score-0.258]
</p><p>23 A multiple thresholding procedure was proposed to reﬁne the solution of LASSO or Dantzig selector (Zhou, 2009). [sent-62, score-0.361]
</p><p>24 The FoBa algorithm was proposed by Zhang (2011b), and it was shown that under RIP the feature selection √ consistency is achieved if the minimal nonzero entry in the true solution is larger than O (σ log m). [sent-63, score-0.261]
</p><p>25 The adaptive LASSO was proposed to adaptively tune the weight value for the ℓ1 norm penalty, and it was shown to enjoy the oracle properties (Zou, 2006). [sent-64, score-0.209]
</p><p>26 It was also shown that a speciﬁc case “least square loss + nonconvex sparse regularization” can eliminate the bias in signal recovery (Zhang, 2010b) and achieve the feature selection consistency (Zhang, 2011c) under the sparse eigenvalue condition (SEC) if the true signal is strong enough. [sent-66, score-0.44]
</p><p>27 Overall, the ℓ2 conditions are considered to be weaker than the ℓ∞ conditions, since the ℓ∞ conditions require about O (s2 log m) random projections while the ℓ2 conditions only need O (s log m) random projections. [sent-69, score-0.161]
</p><p>28 3 Deﬁnitions, Notations, and Basic Assumptions We use X ∈ Rn×m to denote the design matrix and focus on the case m ≫ n, that is, the signal dimension is much larger than the observation dimension. [sent-71, score-0.129]
</p><p>29 F denotes the supporting set of the ¯ original signal β∗ . [sent-78, score-0.19]
</p><p>30 The ¯ oracle solution β is deﬁned as T T ¯¯ ¯ βF = (XF XF )−1 XF y and βF = 0. [sent-86, score-0.227]
</p><p>31 The Multi-Stage Dantzig Selector Algorithm In this section, we introduce the multi-stage Dantzig selector algorithm. [sent-109, score-0.289]
</p><p>32 At each iteration, we employ the basic framework of Dantzig selector and the information about the current supporting set F0 to estimate ˆ the new signal β by solving the following linear program: min βF0 ¯ s. [sent-111, score-0.479]
</p><p>33 Since the features in F0 are considered as the supporting candidates, it is natural to enforce them to be orthogonal to the residual vector Xβ − y, that is, one should make use of them for reconstructing T the overestimation y. [sent-114, score-0.131]
</p><p>34 , the true feature set F) are chosen, the proposed algorithm can be shown to converge to the oracle solution. [sent-118, score-0.217]
</p><p>35 In other words, the oracle solution satisﬁes this constraint with F. [sent-119, score-0.227]
</p><p>36 1 Motivation To motivate the proposed multi-stage algorithm, we ﬁrst consider a simple case where some knowledge about the supporting features is known in advance. [sent-126, score-0.15]
</p><p>37 If we assume that the features belonging to a set F0 are known as supporting features, that is, F0 ⊂ F, we have the following result: Theorem 1 Assume that Assumption 1 holds. [sent-128, score-0.131]
</p><p>38 This coincides with our motivation that more knowledge about the supporting features can lead to a better signal ˆ estimation. [sent-133, score-0.214]
</p><p>39 From the analysis in the next section, we can see that the ﬁrst term is the upper bound of the distance ˆ ¯ from the optimizer to the oracle solution, that is, β − β p and the second term is the upper bound ¯ of the distance from the oracle solution to the true solution, that is, β − β∗ p . [sent-139, score-0.51]
</p><p>40 Setting λ p = σ 2 log(m/η) (0 < η ≤ 1), with a probability at least 1 − η(π log m)−1/2 , the solution of the ˆ standard Dantzig selector βD obeys ˆ βD − β∗ (2)  2  ≤  4  s1/2 σ 2 log(m/η),  (2)  1 − δ2s − θA,s,2s  (5)  (2)  where δ2s = max(ρA,2s − 1, 1 − µA,2s ). [sent-147, score-0.449]
</p><p>41 2 1 √ The theorem above indicates that under the given condition, if min j∈J |β∗ | > O (σ log m) (asj (∞)  (∞)  suming that there exists l ≥ s such that µA,s+l − θA,s+l,l s > 0), then with high probability the l selected |J| features by Dantzig selector belong to the true supporting set. [sent-164, score-0.549]
</p><p>42 The result above is comparable to the ones for other feature selection algorithms, including LASSO/two stage LASSO (Cand` s and Plan, 2009; Zhao and Yu, 2006), OMP (Tropp, 2004; e Donoho et al. [sent-172, score-0.145]
</p><p>43 If one wants to use the ℓ2 conditions in feature √ selection, the minimal nonzero entry of the true solution must be in the order of O (σ s log m), ˆ ˆ ¯ ˆ ˆ ¯ which can be obtained by simply using β(0) − β∗ ∞ + β(0) − β ∞ ≤ β(0) − β∗ 2 + β(0) − β 2 . [sent-176, score-0.163]
</p><p>44 , |J| correct features are selected) with probability larger than 1196  M ULTI -S TAGE DANTZIG S ELECTOR  Assume that one aims to select N correct features by the standard Dantzig selector and the multistage method. [sent-185, score-0.489]
</p><p>45 These two theorems show that the standard Dantzig selector requires that at least N of |β∗ |’s with j ∈ F are larger than the threshold value α0 , while the proposed multi-stage method j requires that at least i of the |β∗ |’s are larger than the threshold value αi−1 , for i = 1, · · · , N. [sent-186, score-0.438]
</p><p>46 , |J| − 1}, where the αi ’s are deﬁned J in Theorem 4, then (1) taking F0 = ∅, N = 0 and λ = σ  2 log  m−s η1  into Algorithm 1, with probability larger than  ˆ ˆ 1 − η′ − η′ , the solution of the Dantzig selector βD (i. [sent-197, score-0.468]
</p><p>47 , the distance from β to the oracle solution β) dominates in the estimated bounds. [sent-204, score-0.227]
</p><p>48 Thus, the performance of the multi-stage method approximately √ √ improves the standard Dantzig selector from Cs1/p log mσ to C(s − N)1/p log mσ. [sent-205, score-0.377]
</p><p>49 5 The Oracle Solution ˆ The oracle solution β deﬁned in Equation (1) is the minimum-variance unbiased estimator of the true solution given the noisy observation. [sent-208, score-0.28]
</p><p>50 , s − 1}, where the αi ’s are deﬁned F in Theorem 4, then taking F0 = ∅, N = s and λ = σ 2 log (N)  solution can be achieved, that is, F0  m−s η1  into Algorithm 1, the oracle  ˆ ¯ = F and β(N) = β, with probability larger than 1 − η′ − η′ . [sent-212, score-0.353]
</p><p>51 1 2  The theorem above shows that when the nonzero elements of the true coefﬁcients vector β∗ are large enough, the oracle solution can be achieved with high probability. [sent-213, score-0.299]
</p><p>52 We apply the same framework in Dantzig selector to analyze the multi-stage LASSO to obtain a bound estimation for any p ∈ [1, ∞] and show that similar improvements can be achieved over the standard LASSO. [sent-221, score-0.317]
</p><p>53 An advantage of MSCR is that it requires a weaker condition, that √ is, mini∈F |β∗ | > O (σ log m) and an ℓ2 condition, to achieve the consistency on feature selection and signal recovery. [sent-225, score-0.219]
</p><p>54 Our comparison includes two aspects: signal recovery accuracy and feature selection accuracy. [sent-228, score-0.243]
</p><p>55 The signal recovery accuracy ˆ ˆ is measured by the relative signal error: SRA = −20 log10 ( β − β∗ 2 / β∗ 2 ), where β is the solution of a speciﬁc algorithm. [sent-229, score-0.33]
</p><p>56 First we compare the standard Dantzig selector and the multi-stage version. [sent-238, score-0.289]
</p><p>57 Note that the solution of the standard Dantzig selector algorithm is ˆ ˆ equivalent to β(N) with N = 0. [sent-244, score-0.342]
</p><p>58 Overall, the recovery accuracy curve increases with an increasing value of N before reaching the sparsity level s and decreases slowly after that, and the feature selection accuracy curve increases while N ≤ s and becomes ﬂat after N goes beyond s. [sent-249, score-0.194]
</p><p>59 Conclusion In this paper, we propose a multi-stage procedure to improve the performance of the Dantzig selector and the LASSO by iteratively selecting the supporting features and recovering the original signal. [sent-256, score-0.42]
</p><p>60 The proposed method makes use of the information of supporting features to estimate the signal and simultaneously makes use of the information of the estimated signal to select the supporting features. [sent-257, score-0.423]
</p><p>61 88  35  standard oracle multi−stage  30 25 0  2  4  6  8  10  12  14  16  18  0. [sent-266, score-0.174]
</p><p>62 8  15  standard oracle multi−stage  10  0  2  4  6  8  10  12  14  16  18  standard oracle multi−stage  0. [sent-274, score-0.348]
</p><p>63 9  standard oracle multi−stage  40  35  0  5  10  standard oracle multi−stage  0. [sent-283, score-0.348]
</p><p>64 8  standard oracle multi−stage  15  0  5  10  standard oracle multi−stage  0. [sent-290, score-0.348]
</p><p>65 We compare the solutions of the standard Dantzig selector method (N = 0), the proposed method for different values of N = 0, 1, · · · , s, · · · , s + 5, and the oracle solution. [sent-292, score-0.482]
</p><p>66 001  standard oracle two−stage multi−stage  70  60  standard oracle two−stage multi−stage  1 0. [sent-297, score-0.348]
</p><p>67 1 35  standard oracle two−stage multi−stage  30  8  10  12  14  16  18  20  16  18  20  standard oracle two−stage multi−stage  1  0. [sent-308, score-0.348]
</p><p>68 001  standard oracle two−stage multi−stage  75  2  standard oracle two−stage multi−stage  1  0. [sent-317, score-0.348]
</p><p>69 1  standard oracle two−stage multi−stage  30  5  standard oracle two−stage multi−stage  1  0. [sent-327, score-0.348]
</p><p>70 We compare the solutions of the standard Dantzig selector method (N = 0), the two-stage LASSO algorithm, the proposed method for different values of N = 0, 1, · · · , s, · · · , s + 5, and the oracle solution. [sent-333, score-0.482]
</p><p>71 1201  L IU , W ONKA AND Y E  Dantzig selector and the LASSO in both signal recovery and supporting feature selection. [sent-336, score-0.614]
</p><p>72 Lemma 8 With probability larger than 1 − η(π log m−s )−1/2 , the following bound holds: η T ¯ XF (X β − y) ¯  ∞  ≤ λ,  where λ = σ 2 log(m − s)/η. [sent-364, score-0.154]
</p><p>73 )1/2 ,s  , Lemma 8 with η = η1 implies that Assumption 2 holds with  1 − η′ 1  probability larger than and Lemma 7 with η = η2 implies that (9) holds with probability ′ . [sent-416, score-0.188]
</p><p>74 2 1  Remark 13 Cand` s and Tao (2007) provided a more general upper bound for the Dantzig selece √ √ (2) (p) tor solution in the order of O k1/2 σ log m + rk (β∗ ) log m , where 1 ≤ k ≤ s and rk (β) = 1/p  (Lk is the index set of the k largest entries in β). [sent-418, score-0.273]
</p><p>75 Setting F0 = ∅ (equiva¯ lent to the standard Dantzig selector) and l = k with k = |F1 | in Theorem 12, it is easy to verify √ (1) ¯ ˆ ¯ that the order of the bound for βD − β p is determined by O k1/p σ log m + k1/p−1 rk (β) , or √ (1) O k1/p σ log m + k1/p−1 rk (β∗ ) due to Lemma 7. [sent-420, score-0.191]
</p><p>76 This bound achieves the same order as the bound of the LASSO solution given by Zhang (2009a), which is the sharpest bound for LASSO to our knowledge. [sent-421, score-0.137]
</p><p>77 Since in β, Assumption 2 and the inequality (9) hold, the bounds (3) and (4) in Theorem 1 hold with probability larger than 1 − η′ − η′ . [sent-429, score-0.148]
</p><p>78 Thus the claim above holds with probability larger than 1 − η′ − η′ . [sent-430, score-0.131]
</p><p>79 Note 1 2 1 2 that the probability will not accumulate, as we only need the holding probability of Assumption 2 and the inequality (9). [sent-431, score-0.129]
</p><p>80 Proof of Theorem 4: From the proof in Theorem 12, the bounds (3) and (4) in Theorem 1 hold with probability 1 if assumption 2 and the inequality (9) hold. [sent-433, score-0.153]
</p><p>81 F0 ⊂ F holds with probability larger than 1 − η1 2 Proof of Theorem 5: From Theorem 1, the ﬁrst conclusion holds with probability larger than 1 − η′ − η′ by choosing F0 = ∅ and l = s. [sent-442, score-0.234]
</p><p>82 Since Assumption 2 and the inequality (9) hold, the bounds (3) and (4) in Theorem 1 hold with probability larger than 1 − η′ − η′ . [sent-447, score-0.148]
</p><p>83 Thus the 1 2 1209  L IU , W ONKA AND Y E  claim above holds with probability larger than 1 − η′ − η′ . [sent-448, score-0.131]
</p><p>84 (11)  The oracle solution minimizes the objective function to 0. [sent-457, score-0.227]
</p><p>85 Since Assumption 2 indeed implies that the oracle is a feasible solution, the oracle solution is one optimizer. [sent-458, score-0.401]
</p><p>86 If there is another optimizer β = β, then βF = 0 and βF = (XF XF )−1 XF y, which ¯ is identical to the deﬁnition of the oracle solution. [sent-460, score-0.227]
</p><p>87 Thus, we conclude that the oracle is the unique optimizer for the optimization problem (11) with probability 1. [sent-461, score-0.263]
</p><p>88 Since the holding probability of Assumption 2 and the inequality (9) is larger than 1 − η′ − η′ , the oracle solution can be achieved 2 1 with the same probability. [sent-462, score-0.366]
</p><p>89 Thus, feature selection and signal recovery can beneﬁt from each other. [sent-474, score-0.243]
</p><p>90 (b) Second, like Theorem 5 the following theorem shows that with a high probability the multi√ stage procedure can improve the upper bound of the standard LASSO from Cs1/p log m + ∆ to √ C(s − N)1/p log m + ∆, where C is a constant and ∆ is a small number independent from m. [sent-489, score-0.297]
</p><p>91 , s − 1}, where αi follows the F  deﬁnition in theorem 15, then taking F0 = ∅, N = s and λ′ = 2σ  2 log (N)  stage LASSO algorithm, the oracle solution can be achieved, that is, F0 probability larger than 1 − η′ − η′ . [sent-497, score-0.498]
</p><p>92 Let us assume that the oracle solution satisﬁes the following assumption. [sent-503, score-0.227]
</p><p>93 )1/2 ,s  Finally, taking λ′ = 2σ 2 log  m−s , η1  Lemma 8 (letting η = η1 ) implies that Assumption 3 holds with probability larger than 1 − η′ and 1 Lemma 7 (letting η = η2 ) implies that Equation (9) holds with probability larger than 1 − η′ . [sent-530, score-0.278]
</p><p>94 Because of Assumption 3, the oracle solution satisﬁes these two conditions. [sent-542, score-0.227]
</p><p>95 Since the objective function is not strictly convex, we need to show that the oracle solution is the unique minimizer. [sent-543, score-0.227]
</p><p>96 ¯ ¯ ¯ 2 2 Because the oracle solution is a minimizer of the Equation (12), “0” should be one of the minimizers of f (βF ). [sent-546, score-0.242]
</p><p>97 Next we show that “0” is the unique minimizer, which implies that the oracle solution ¯ f (βF ) = ¯  1216  M ULTI -S TAGE DANTZIG S ELECTOR  is the unique minimizer for Equation (12). [sent-547, score-0.242]
</p><p>98 ¯ Finally, because the probability of Assumption 3 and the inequality (9) holding is larger than 1 − η′ − η′ , the oracle solution is achieved with the same probability. [sent-550, score-0.366]
</p><p>99 Stable recovery of sparse overcomplete representations in the presence of noise. [sent-600, score-0.135]
</p><p>100 A uniﬁed approach to model selection and sparse recovery using regularized least squares. [sent-631, score-0.16]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xf', 0.618), ('dantzig', 0.442), ('selector', 0.289), ('lasso', 0.208), ('oracle', 0.174), ('xh', 0.147), ('onka', 0.142), ('fsa', 0.132), ('elector', 0.121), ('tage', 0.121), ('recovery', 0.111), ('supporting', 0.107), ('vtj', 0.104), ('cand', 0.1), ('stage', 0.096), ('iu', 0.094), ('sra', 0.088), ('multi', 0.083), ('signal', 0.083), ('ulti', 0.08), ('zhang', 0.077), ('supp', 0.073), ('htj', 0.057), ('solution', 0.053), ('optimizer', 0.053), ('theorem', 0.049), ('omp', 0.047), ('larger', 0.046), ('log', 0.044), ('tao', 0.042), ('foba', 0.038), ('mul', 0.036), ('probability', 0.036), ('holds', 0.035), ('inequality', 0.034), ('lemma', 0.033), ('mip', 0.032), ('assumption', 0.03), ('asu', 0.028), ('dzi', 0.028), ('erc', 0.028), ('mscr', 0.028), ('wonka', 0.028), ('bound', 0.028), ('lv', 0.027), ('tropp', 0.027), ('xit', 0.027), ('rip', 0.027), ('consistency', 0.027), ('obeys', 0.027), ('subdifferential', 0.025), ('selection', 0.025), ('rk', 0.025), ('verify', 0.025), ('jt', 0.024), ('sparse', 0.024), ('features', 0.024), ('index', 0.024), ('feature', 0.024), ('tj', 0.024), ('donoho', 0.024), ('nonempty', 0.024), ('correct', 0.024), ('plan', 0.024), ('nonzero', 0.023), ('holding', 0.023), ('cai', 0.022), ('multistage', 0.022), ('nonconvex', 0.021), ('proof', 0.021), ('jieping', 0.02), ('pursuit', 0.02), ('fan', 0.02), ('threshold', 0.019), ('uup', 0.019), ('proposed', 0.019), ('zhao', 0.019), ('conditions', 0.019), ('bounds', 0.018), ('condition', 0.018), ('dz', 0.018), ('jersey', 0.018), ('lounici', 0.018), ('curve', 0.017), ('equation', 0.017), ('norm', 0.016), ('weaker', 0.016), ('xtj', 0.016), ('cyan', 0.016), ('expound', 0.016), ('piscataway', 0.016), ('tor', 0.016), ('minimizer', 0.015), ('sgn', 0.015), ('wainwright', 0.015), ('please', 0.015), ('directional', 0.015), ('hold', 0.014), ('claim', 0.014), ('entries', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="7-tfidf-1" href="./jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</a></p>
<p>Author: Ji Liu, Peter Wonka, Jieping Ye</p><p>Abstract: We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ Rn×m (m ≫ n) and a noisy observation vector y ∈ Rn satisfying y = Xβ∗ + ε where ε is the noise vector following a Gaussian distribution N(0, σ2 I), how to recover the signal (or parameter vector) β∗ when the signal is sparse? The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively reﬁnes the target signal β∗ . We show that if X obeys a certain condition, then with a large probability ˆ the difference between the solution β estimated by the proposed method and the true solution β∗ measured in terms of the ℓ p norm (p ≥ 1) is bounded as ˆ β − β∗ p ≤ C(s − N)1/p log m + ∆ σ, where C is a constant, s is the number of nonzero entries in β∗ , the risk of the oracle estimator ∆ is independent of m and is much smaller than the ﬁrst term, and N is the number of entries of β∗ larger √ than a certain value in the order of O (σ log m). The proposed √ method improves the estimation √ bound of the standard Dantzig selector approximately from Cs1/p log mσ to C(s − N)1/p log mσ where the value N depends on the number of large entries in β∗ . When N = s, the proposed algorithm achieves the oracle solution with a high probability, where the oracle solution is the projection of the observation vector y onto true features. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector. Finally, we extend this multi-stage procedure to the LASSO case. Keywords: multi-stage, Dantzig selector, LASSO, sparse signal recovery</p><p>2 0.32717669 <a title="7-tfidf-2" href="./jmlr-2012-Query_Strategies_for_Evading_Convex-Inducing_Classifiers.html">94 jmlr-2012-Query Strategies for Evading Convex-Inducing Classifiers</a></p>
<p>Author: Blaine Nelson, Benjamin I. P. Rubinstein, Ling Huang, Anthony D. Joseph, Steven J. Lee, Satish Rao, J. D. Tygar</p><p>Abstract: Classiﬁers are often used to detect miscreant activities. We study how an adversary can systematically query a classiﬁer to elicit information that allows the attacker to evade detection while incurring a near-minimal cost of modifying their intended malfeasance. We generalize the theory of Lowd and Meek (2005) to the family of convex-inducing classiﬁers that partition their feature space into two sets, one of which is convex. We present query algorithms for this family that construct undetected instances of approximately minimal cost using only polynomially-many queries in the dimension of the space and in the level of approximation. Our results demonstrate that nearoptimal evasion can be accomplished for this family without reverse engineering the classiﬁer’s decision boundary. We also consider general ℓ p costs and show that near-optimal evasion on the family of convex-inducing classiﬁers is generally efﬁcient for both positive and negative convexity for all levels of approximation if p = 1. Keywords: query algorithms, evasion, reverse engineering, adversarial learning</p><p>3 0.15755774 <a title="7-tfidf-3" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>Author: Jian Huang, Cun-Hui Zhang</p><p>Abstract: The ℓ1 -penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ1 -penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1 -penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results. Keywords: variable selection, penalized estimation, oracle inequality, generalized linear models, selection consistency, sparsity</p><p>4 0.12120181 <a title="7-tfidf-4" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>Author: Matus Telgarsky</p><p>Abstract: Boosting combines weak learners into a predictor with low empirical risk. Its dual constructs a high entropy distribution upon which weak learners and training labels are uncorrelated. This manuscript studies this primal-dual relationship under a broad family of losses, including the exponential loss of AdaBoost and the logistic loss, revealing: • Weak learnability aids the whole loss family: for any ε > 0, O (ln(1/ε)) iterations sufﬁce to produce a predictor with empirical risk ε-close to the inﬁmum; • The circumstances granting the existence of an empirical risk minimizer may be characterized in terms of the primal and dual problems, yielding a new proof of the known rate O (ln(1/ε)); • Arbitrary instances may be decomposed into the above two, granting rate O (1/ε), with a matching lower bound provided for the logistic loss. Keywords: boosting, convex analysis, weak learnability, coordinate descent, maximum entropy</p><p>5 0.11831932 <a title="7-tfidf-5" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>Author: Christopher R. Genovese, Jiashun Jin, Larry Wasserman, Zhigang Yao</p><p>Abstract: The lasso is an important method for sparse, high-dimensional regression problems, with efﬁcient algorithms available, a long history of practical success, and a large body of theoretical results supporting and explaining its performance. But even with the best available algorithms, ﬁnding the lasso solutions remains a computationally challenging task in cases where the number of covariates vastly exceeds the number of data points. Marginal regression, where each dependent variable is regressed separately on each covariate, offers a promising alternative in this case because the estimates can be computed roughly two orders faster than the lasso solutions. The question that remains is how the statistical performance of the method compares to that of the lasso in these cases. In this paper, we study the relative statistical performance of the lasso and marginal regression for sparse, high-dimensional regression problems. We consider the problem of learning which coefﬁcients are non-zero. Our main results are as follows: (i) we compare the conditions under which the lasso and marginal regression guarantee exact recovery in the ﬁxed design, noise free case; (ii) we establish conditions under which marginal regression provides exact recovery with high probability in the ﬁxed design, noise free, random coefﬁcients case; and (iii) we derive rates of convergence for both procedures, where performance is measured by the number of coefﬁcients with incorrect sign, and characterize the regions in the parameter space recovery is and is not possible under this metric. In light of the computational advantages of marginal regression in very high dimensional problems, our theoretical and simulations results suggest that the procedure merits further study. Keywords: high-dimensional regression, lasso, phase diagram, regularization</p><p>6 0.071047433 <a title="7-tfidf-6" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>7 0.050777029 <a title="7-tfidf-7" href="./jmlr-2012-The_huge_Package_for_High-dimensional_Undirected_Graph_Estimation_in_R.html">113 jmlr-2012-The huge Package for High-dimensional Undirected Graph Estimation in R</a></p>
<p>8 0.047770999 <a title="7-tfidf-8" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>9 0.047745451 <a title="7-tfidf-9" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>10 0.04545667 <a title="7-tfidf-10" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>11 0.044832729 <a title="7-tfidf-11" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>12 0.041612454 <a title="7-tfidf-12" href="./jmlr-2012-EP-GIG_Priors_and_Applications_in_Bayesian_Sparse_Learning.html">35 jmlr-2012-EP-GIG Priors and Applications in Bayesian Sparse Learning</a></p>
<p>13 0.040906988 <a title="7-tfidf-13" href="./jmlr-2012-Iterative_Reweighted_Algorithms_for_Matrix_Rank_Minimization.html">52 jmlr-2012-Iterative Reweighted Algorithms for Matrix Rank Minimization</a></p>
<p>14 0.040599354 <a title="7-tfidf-14" href="./jmlr-2012-MULTIBOOST%3A_A_Multi-purpose_Boosting_Package.html">62 jmlr-2012-MULTIBOOST: A Multi-purpose Boosting Package</a></p>
<p>15 0.038948584 <a title="7-tfidf-15" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>16 0.036699347 <a title="7-tfidf-16" href="./jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</a></p>
<p>17 0.034960501 <a title="7-tfidf-17" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>18 0.031845603 <a title="7-tfidf-18" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>19 0.030954048 <a title="7-tfidf-19" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>20 0.029755373 <a title="7-tfidf-20" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.176), (1, 0.085), (2, -0.159), (3, -0.009), (4, -0.208), (5, 0.349), (6, 0.112), (7, -0.34), (8, 0.023), (9, 0.347), (10, -0.151), (11, -0.114), (12, -0.02), (13, 0.211), (14, 0.055), (15, 0.072), (16, -0.017), (17, -0.004), (18, 0.032), (19, -0.027), (20, -0.036), (21, 0.021), (22, -0.004), (23, -0.075), (24, 0.063), (25, -0.012), (26, 0.003), (27, -0.032), (28, 0.054), (29, 0.011), (30, -0.087), (31, 0.006), (32, -0.145), (33, 0.012), (34, -0.044), (35, -0.083), (36, -0.092), (37, 0.003), (38, -0.082), (39, -0.001), (40, -0.047), (41, -0.091), (42, -0.007), (43, 0.025), (44, -0.058), (45, -0.068), (46, -0.062), (47, 0.05), (48, 0.075), (49, -0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92998838 <a title="7-lsi-1" href="./jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</a></p>
<p>Author: Ji Liu, Peter Wonka, Jieping Ye</p><p>Abstract: We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ Rn×m (m ≫ n) and a noisy observation vector y ∈ Rn satisfying y = Xβ∗ + ε where ε is the noise vector following a Gaussian distribution N(0, σ2 I), how to recover the signal (or parameter vector) β∗ when the signal is sparse? The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively reﬁnes the target signal β∗ . We show that if X obeys a certain condition, then with a large probability ˆ the difference between the solution β estimated by the proposed method and the true solution β∗ measured in terms of the ℓ p norm (p ≥ 1) is bounded as ˆ β − β∗ p ≤ C(s − N)1/p log m + ∆ σ, where C is a constant, s is the number of nonzero entries in β∗ , the risk of the oracle estimator ∆ is independent of m and is much smaller than the ﬁrst term, and N is the number of entries of β∗ larger √ than a certain value in the order of O (σ log m). The proposed √ method improves the estimation √ bound of the standard Dantzig selector approximately from Cs1/p log mσ to C(s − N)1/p log mσ where the value N depends on the number of large entries in β∗ . When N = s, the proposed algorithm achieves the oracle solution with a high probability, where the oracle solution is the projection of the observation vector y onto true features. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector. Finally, we extend this multi-stage procedure to the LASSO case. Keywords: multi-stage, Dantzig selector, LASSO, sparse signal recovery</p><p>2 0.74835408 <a title="7-lsi-2" href="./jmlr-2012-Query_Strategies_for_Evading_Convex-Inducing_Classifiers.html">94 jmlr-2012-Query Strategies for Evading Convex-Inducing Classifiers</a></p>
<p>Author: Blaine Nelson, Benjamin I. P. Rubinstein, Ling Huang, Anthony D. Joseph, Steven J. Lee, Satish Rao, J. D. Tygar</p><p>Abstract: Classiﬁers are often used to detect miscreant activities. We study how an adversary can systematically query a classiﬁer to elicit information that allows the attacker to evade detection while incurring a near-minimal cost of modifying their intended malfeasance. We generalize the theory of Lowd and Meek (2005) to the family of convex-inducing classiﬁers that partition their feature space into two sets, one of which is convex. We present query algorithms for this family that construct undetected instances of approximately minimal cost using only polynomially-many queries in the dimension of the space and in the level of approximation. Our results demonstrate that nearoptimal evasion can be accomplished for this family without reverse engineering the classiﬁer’s decision boundary. We also consider general ℓ p costs and show that near-optimal evasion on the family of convex-inducing classiﬁers is generally efﬁcient for both positive and negative convexity for all levels of approximation if p = 1. Keywords: query algorithms, evasion, reverse engineering, adversarial learning</p><p>3 0.44551802 <a title="7-lsi-3" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>Author: Jian Huang, Cun-Hui Zhang</p><p>Abstract: The ℓ1 -penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ1 -penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1 -penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results. Keywords: variable selection, penalized estimation, oracle inequality, generalized linear models, selection consistency, sparsity</p><p>4 0.37195736 <a title="7-lsi-4" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>Author: Matus Telgarsky</p><p>Abstract: Boosting combines weak learners into a predictor with low empirical risk. Its dual constructs a high entropy distribution upon which weak learners and training labels are uncorrelated. This manuscript studies this primal-dual relationship under a broad family of losses, including the exponential loss of AdaBoost and the logistic loss, revealing: • Weak learnability aids the whole loss family: for any ε > 0, O (ln(1/ε)) iterations sufﬁce to produce a predictor with empirical risk ε-close to the inﬁmum; • The circumstances granting the existence of an empirical risk minimizer may be characterized in terms of the primal and dual problems, yielding a new proof of the known rate O (ln(1/ε)); • Arbitrary instances may be decomposed into the above two, granting rate O (1/ε), with a matching lower bound provided for the logistic loss. Keywords: boosting, convex analysis, weak learnability, coordinate descent, maximum entropy</p><p>5 0.37168238 <a title="7-lsi-5" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>Author: Christopher R. Genovese, Jiashun Jin, Larry Wasserman, Zhigang Yao</p><p>Abstract: The lasso is an important method for sparse, high-dimensional regression problems, with efﬁcient algorithms available, a long history of practical success, and a large body of theoretical results supporting and explaining its performance. But even with the best available algorithms, ﬁnding the lasso solutions remains a computationally challenging task in cases where the number of covariates vastly exceeds the number of data points. Marginal regression, where each dependent variable is regressed separately on each covariate, offers a promising alternative in this case because the estimates can be computed roughly two orders faster than the lasso solutions. The question that remains is how the statistical performance of the method compares to that of the lasso in these cases. In this paper, we study the relative statistical performance of the lasso and marginal regression for sparse, high-dimensional regression problems. We consider the problem of learning which coefﬁcients are non-zero. Our main results are as follows: (i) we compare the conditions under which the lasso and marginal regression guarantee exact recovery in the ﬁxed design, noise free case; (ii) we establish conditions under which marginal regression provides exact recovery with high probability in the ﬁxed design, noise free, random coefﬁcients case; and (iii) we derive rates of convergence for both procedures, where performance is measured by the number of coefﬁcients with incorrect sign, and characterize the regions in the parameter space recovery is and is not possible under this metric. In light of the computational advantages of marginal regression in very high dimensional problems, our theoretical and simulations results suggest that the procedure merits further study. Keywords: high-dimensional regression, lasso, phase diagram, regularization</p><p>6 0.29561976 <a title="7-lsi-6" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>7 0.19577692 <a title="7-lsi-7" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>8 0.16694017 <a title="7-lsi-8" href="./jmlr-2012-Iterative_Reweighted_Algorithms_for_Matrix_Rank_Minimization.html">52 jmlr-2012-Iterative Reweighted Algorithms for Matrix Rank Minimization</a></p>
<p>9 0.16063118 <a title="7-lsi-9" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>10 0.15986151 <a title="7-lsi-10" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>11 0.15168783 <a title="7-lsi-11" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>12 0.14150628 <a title="7-lsi-12" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>13 0.1388779 <a title="7-lsi-13" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>14 0.13880095 <a title="7-lsi-14" href="./jmlr-2012-An_Active_Learning_Algorithm_for_Ranking_from_Pairwise_Preferences_with_an_Almost_Optimal_Query_Complexity.html">17 jmlr-2012-An Active Learning Algorithm for Ranking from Pairwise Preferences with an Almost Optimal Query Complexity</a></p>
<p>15 0.13535373 <a title="7-lsi-15" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>16 0.12667613 <a title="7-lsi-16" href="./jmlr-2012-The_huge_Package_for_High-dimensional_Undirected_Graph_Estimation_in_R.html">113 jmlr-2012-The huge Package for High-dimensional Undirected Graph Estimation in R</a></p>
<p>17 0.12550844 <a title="7-lsi-17" href="./jmlr-2012-Quantum_Set_Intersection_and_its_Application_to_Associative_Memory.html">93 jmlr-2012-Quantum Set Intersection and its Application to Associative Memory</a></p>
<p>18 0.12207859 <a title="7-lsi-18" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>19 0.1219447 <a title="7-lsi-19" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<p>20 0.12103572 <a title="7-lsi-20" href="./jmlr-2012-Feature_Selection_via_Dependence_Maximization.html">44 jmlr-2012-Feature Selection via Dependence Maximization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.022), (21, 0.108), (22, 0.342), (26, 0.038), (29, 0.022), (49, 0.019), (57, 0.01), (75, 0.064), (77, 0.028), (79, 0.012), (92, 0.118), (96, 0.094)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69507992 <a title="7-lda-1" href="./jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</a></p>
<p>Author: Ji Liu, Peter Wonka, Jieping Ye</p><p>Abstract: We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ Rn×m (m ≫ n) and a noisy observation vector y ∈ Rn satisfying y = Xβ∗ + ε where ε is the noise vector following a Gaussian distribution N(0, σ2 I), how to recover the signal (or parameter vector) β∗ when the signal is sparse? The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively reﬁnes the target signal β∗ . We show that if X obeys a certain condition, then with a large probability ˆ the difference between the solution β estimated by the proposed method and the true solution β∗ measured in terms of the ℓ p norm (p ≥ 1) is bounded as ˆ β − β∗ p ≤ C(s − N)1/p log m + ∆ σ, where C is a constant, s is the number of nonzero entries in β∗ , the risk of the oracle estimator ∆ is independent of m and is much smaller than the ﬁrst term, and N is the number of entries of β∗ larger √ than a certain value in the order of O (σ log m). The proposed √ method improves the estimation √ bound of the standard Dantzig selector approximately from Cs1/p log mσ to C(s − N)1/p log mσ where the value N depends on the number of large entries in β∗ . When N = s, the proposed algorithm achieves the oracle solution with a high probability, where the oracle solution is the projection of the observation vector y onto true features. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector. Finally, we extend this multi-stage procedure to the LASSO case. Keywords: multi-stage, Dantzig selector, LASSO, sparse signal recovery</p><p>2 0.64189494 <a title="7-lda-2" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>Author: Zhihua Zhang, Dehua Liu, Guang Dai, Michael I. Jordan</p><p>Abstract: Support vector machines (SVMs) naturally embody sparseness due to their use of hinge loss functions. However, SVMs can not directly estimate conditional class probabilities. In this paper we propose and study a family of coherence functions, which are convex and differentiable, as surrogates of the hinge function. The coherence function is derived by using the maximum-entropy principle and is characterized by a temperature parameter. It bridges the hinge function and the logit function in logistic regression. The limit of the coherence function at zero temperature corresponds to the hinge function, and the limit of the minimizer of its expected error is the minimizer of the expected error of the hinge loss. We refer to the use of the coherence function in large-margin classiﬁcation as “C -learning,” and we present efﬁcient coordinate descent algorithms for the training of regularized C -learning models. Keywords: large-margin classiﬁers, hinge functions, logistic functions, coherence functions, C learning</p><p>3 0.48968804 <a title="7-lda-3" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>Author: Gérard Biau</p><p>Abstract: Random forests are a scheme proposed by Leo Breiman in the 2000’s for building a predictor ensemble with a set of decision trees that grow in randomly selected subspaces of data. Despite growing interest and practical use, there has been little exploration of the statistical properties of random forests, and little is known about the mathematical forces driving the algorithm. In this paper, we offer an in-depth analysis of a random forests model suggested by Breiman (2004), which is very close to the original algorithm. We show in particular that the procedure is consistent and adapts to sparsity, in the sense that its rate of convergence depends only on the number of strong features and not on how many noise variables are present. Keywords: random forests, randomization, sparsity, dimension reduction, consistency, rate of convergence</p><p>4 0.47611803 <a title="7-lda-4" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>Author: Jian Huang, Cun-Hui Zhang</p><p>Abstract: The ℓ1 -penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ1 -penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1 -penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results. Keywords: variable selection, penalized estimation, oracle inequality, generalized linear models, selection consistency, sparsity</p><p>5 0.47328237 <a title="7-lda-5" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>Author: Lan Xue, Annie Qu</p><p>Abstract: The varying-coefﬁcient model is ﬂexible and powerful for modeling the dynamic changes of regression coefﬁcients. It is important to identify signiﬁcant covariates associated with response variables, especially for high-dimensional settings where the number of covariates can be larger than the sample size. We consider model selection in the high-dimensional setting and adopt difference convex programming to approximate the L0 penalty, and we investigate the global optimality properties of the varying-coefﬁcient estimator. The challenge of the variable selection problem here is that the dimension of the nonparametric form for the varying-coefﬁcient modeling could be inﬁnite, in addition to dealing with the high-dimensional linear covariates. We show that the proposed varying-coefﬁcient estimator is consistent, enjoys the oracle property and achieves an optimal convergence rate for the non-zero nonparametric components for high-dimensional data. Our simulations and numerical examples indicate that the difference convex algorithm is efﬁcient using the coordinate decent algorithm, and is able to select the true model at a higher frequency than the least absolute shrinkage and selection operator (LASSO), the adaptive LASSO and the smoothly clipped absolute deviation (SCAD) approaches. Keywords: coordinate decent algorithm, difference convex programming, L0 - regularization, large-p small-n, model selection, nonparametric function, oracle property, truncated L1 penalty</p><p>6 0.46709585 <a title="7-lda-6" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>7 0.46176061 <a title="7-lda-7" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>8 0.45841354 <a title="7-lda-8" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>9 0.45838401 <a title="7-lda-9" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>10 0.45726159 <a title="7-lda-10" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>11 0.45618927 <a title="7-lda-11" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>12 0.45102993 <a title="7-lda-12" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>13 0.44780496 <a title="7-lda-13" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>14 0.44255441 <a title="7-lda-14" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>15 0.44237363 <a title="7-lda-15" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>16 0.43841675 <a title="7-lda-16" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>17 0.43695515 <a title="7-lda-17" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>18 0.43674338 <a title="7-lda-18" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>19 0.43593746 <a title="7-lda-19" href="./jmlr-2012-EP-GIG_Priors_and_Applications_in_Bayesian_Sparse_Learning.html">35 jmlr-2012-EP-GIG Priors and Applications in Bayesian Sparse Learning</a></p>
<p>20 0.4357633 <a title="7-lda-20" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
