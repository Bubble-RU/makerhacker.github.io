<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-10" href="#">jmlr2012-10</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</h1>
<br/><p>Source: <a title="jmlr-2012-10-pdf" href="http://jmlr.org/papers/volume13/hernandez-orallo12a/hernandez-orallo12a.pdf">pdf</a></p><p>Author: José Hernández-Orallo, Peter Flach, Cèsar Ferri</p><p>Abstract: Many performance metrics have been introduced in the literature for the evaluation of classiﬁcation performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into reﬁnement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassiﬁcation costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: ﬁxed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the reﬁnement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be summarised as follows: given a model, apply the threshold choice methods that correspond with the available information about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibra</p><p>Reference: <a title="jmlr-2012-10-reference" href="../jmlr2012_reference/jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the reﬁnement loss respectively. [sent-16, score-0.821]
</p><p>2 In addition, it is assumed that the threshold will be set in such a way that the estimated probability where the threshold is set is made equal to the operating condition. [sent-49, score-0.836]
</p><p>3 In our work we make these assumptions explicit through the concept of a threshold choice method, which systematically links performance metrics and expected loss. [sent-72, score-0.578]
</p><p>4 Building on this notion of threshold choice method, we are able to systematically explore how known performance metrics are linked to expected loss, resulting in a range of results that are not only theoretically well-founded but also practically relevant. [sent-74, score-0.578]
</p><p>5 One of these decision rules is the score-driven threshold choice method, which sets the threshold equal to the operating condition or, more precisely, to a cost proportion c. [sent-129, score-1.131]
</p><p>6 Using these three different threshold choice methods for the models A and B, and assuming cost proportions are uniformly distributed, we get the expected losses shown in Table 2. [sent-131, score-0.742]
</p><p>7 In addition, a seventh threshold choice method, known as optimal threshold choice method, denoted by T o , has been (implicitly) used in a few works (Drummond and Holte, 2000, 2006; Hand, 2009). [sent-154, score-0.766]
</p><p>8 For both families we can ﬁx a threshold or assume them ranging uniformly, which makes the threshold choice method independent from the operating condition. [sent-159, score-0.908]
</p><p>9 The results in Table 2 allow us to conclude that model A achieves the lowest expected loss for uniformly sampled cost proportions, if we are wise enough to choose the appropriate threshold choice method (in this case the rate-driven method) to turn model A into a successful classiﬁer. [sent-170, score-0.604]
</p><p>10 The expected loss of a model can only be determined if we select a distribution of operating conditions and a threshold choice method. [sent-176, score-0.721]
</p><p>11 Section 2 introduces some notation, the basic deﬁnitions for operating condition, threshold, expected loss, and particularly the notion of threshold choice method, which we will use throughout the paper. [sent-201, score-0.652]
</p><p>12 We show that, not surprisingly, the expected loss for these threshold choice method are the 0-1 loss (weighted or unweighted accuracy depending on whether we use cost proportions or skews). [sent-203, score-0.883]
</p><p>13 Section 4 presents the results that the score-uniform threshold choice method has MAE as associate performance metric and the score-driven threshold choice method leads to the Brier score. [sent-204, score-0.801]
</p><p>14 Somewhat surprisingly, both the rate-uniform threshold choice method and the rate-driven threshold choice method lead to linear functions of AUC, with the latter always been better than the former. [sent-207, score-0.766]
</p><p>15 Section 6 uses the optimal threshold choice method, connects the expected loss in this case with the area under the optimal cost curve, and derives its corresponding metric, which is reﬁnement loss, one of the components of the Brier score decomposition. [sent-209, score-0.733]
</p><p>16 Section 7 analyses the connections between the previous threshold choice methods and metrics by considering several properties of the scores: evenly-spaced scores and perfectly calibrated scores. [sent-210, score-0.805]
</p><p>17 Two appendices include a derivation of univariate operating conditions for costs and skews and some technical results for the optimal threshold choice method. [sent-213, score-0.815]
</p><p>18 Given a model and a threshold t, we denote by R(t) the predicted positive rate, that is, the proportion of examples that will be predicted positive (class 0) if the threshold is set at t. [sent-238, score-0.715]
</p><p>19 The key idea proposed in this paper is the notion of a threshold choice method, a function which converts an operating condition into an appropriate threshold for the classiﬁer. [sent-274, score-0.941]
</p><p>20 A threshold choice method1 is a (possibly non-deterministic) function T : Θ → R such that given an operating condition it returns a decision threshold. [sent-276, score-0.63]
</p><p>21 The notion of threshold choice method could be further generalised to cover situations where we have some information about the operating condition which cannot be expressed in terms of a speciﬁc value of Θ, such a distribution on Θ or information about E{b}, E{bc}, etc. [sent-279, score-0.63]
</p><p>22 2821  ´ H ERN ANDEZ -O RALLO , F LACH AND F ERRI  the threshold choice method as an abstract concept since there are several reasonable options for the function T , essentially because there may be different degrees of information about the model and the operating conditions at evaluation time. [sent-281, score-0.597]
</p><p>23 Given a threshold choice function T , the loss for a particular operating condition θ is given by Q(T (θ); θ). [sent-285, score-0.699]
</p><p>24 (2)  Calculating this integral for a particular case depends on the threshold choice method and the kind of model, but particularly on the space of operating conditions Θ and its associated distribution w(θ). [sent-288, score-0.597]
</p><p>25 Some threshold choice methods we consider in this paper take additional information into account, such as a default threshold or a target predicted positive rate; such information is indicated by square brackets. [sent-324, score-0.694]
</p><p>26 So, for example, the score-ﬁxed threshold choice method for cost proportions considered in the next section is indicated sf thus: Tc [t](c). [sent-325, score-0.697]
</p><p>27 The expected loss for costs and skews is then adapted from Equation (2) as follows: Deﬁnition 3 Given a threshold choice method for cost proportions Tc and a probability density function over cost proportions wc , expected loss Lc is deﬁned as 1  Lc  Qc (Tc (c); c)wc (c)dc. [sent-328, score-1.533]
</p><p>28 0  (4)  Incorporating the class distribution into the operating condition as skews and deﬁning a distribution over skews wz , we obtain expected loss over a distribution of skews: 1  Lz  0  Qz (Tz (z); z)wz (z)dz. [sent-329, score-0.783]
</p><p>29 Except for cases where the threshold choice is independent of the operating condition, we will assume a uniform distribution for wc (c) and wz (z). [sent-338, score-0.826]
</p><p>30 Only when we know the deployment operating condition at evaluation time is it reasonable to ﬁx the threshold according to this information. [sent-363, score-0.683]
</p><p>31 So either by common choice or because we have this latter case, consider then that we are going to use the same threshold t independently of skews or cost proportions. [sent-364, score-0.652]
</p><p>32 Given this threshold choice method, then the question is: if we must evaluate a model before application for a wide range of skews and cost proportions, which performance metric should be used? [sent-365, score-0.687]
</p><p>33 Theorem 5 If a classiﬁer sets the decision threshold at a ﬁxed value t irrespective of the operating condition or the model, then expected loss for any cost distribution wc is given by: s Lc f (t) = 2Ewc {c} (1 − Acc(t)) + 4π1 F1 (t)  1 − Ewc {c} . [sent-369, score-0.909]
</p><p>34 2  Corollary 6 If a classiﬁer sets the decision threshold at a ﬁxed value irrespective of the operating condition or the model, then expected loss under a distribution of cost proportions wc with expected value Ewc {c} = 1/2 is equal to the error rate at that decision threshold. [sent-374, score-1.144]
</p><p>35 So the expected loss under a distribution of cost proportions with mean 1/2 for the score-ﬁxed threshold choice method is the error rate of the classiﬁer at that threshold. [sent-377, score-0.784]
</p><p>36 The previous results show that 0-1 losses are appropriate to evaluate models in a range of operating conditions if the threshold is ﬁxed for all of them and we do not have any information about a possible asymmetry in the cost matrix at deployment time. [sent-386, score-0.774]
</p><p>37 The situation occurs when one assumes a particular operating condition at evaluation time while the classiﬁer has to deal with a range of operating conditions in deployment time. [sent-388, score-0.586]
</p><p>38 First, we explore a threshold choice method which considers that we have no information at all about the operating condition, neither at evaluation time nor at deployment time. [sent-441, score-0.722]
</p><p>39 It can be argued that this threshold choice method is unrealistic, because we almost always have some information about the operating condition, especially at deployment time. [sent-443, score-0.722]
</p><p>40 So what we show next is that there are evaluation metrics which can be expressed as an expected loss under these assumptions, adding support to the idea that the metrics related to this threshold choice method are blind to (or unaware of) any cost information. [sent-446, score-0.884]
</p><p>41 Given this threshold choice method, then the question is: if we must evaluate a model before application for a wide range of skews and cost proportions, which performance metric should be used? [sent-448, score-0.687]
</p><p>42 Theorem 11 Assuming probabilistic scores and the score-uniform threshold choice method, expected loss under a distribution of cost proportions wc is equal to: su Lc = 2{Ewc {c}π0 (s0 ) + (1 − Ewc {c})π1 (1 − s1 )}. [sent-449, score-1.13]
</p><p>43 Corollary 12 Assuming probabilistic scores and the score-uniform threshold choice method, expected loss under a distribution of cost proportions wc with expected value Ewc {c} = 1/2 is equal to the model’s mean absolute error. [sent-456, score-1.113]
</p><p>44 2 The Score-Driven Threshold Choice Method Leads to the Brier Score We will now consider the ﬁrst threshold choice method to take the operating condition into account. [sent-461, score-0.63]
</p><p>45 Since we are dealing with probabilistic scores, this method simply sets the threshold equal to the operating condition (cost proportion or skew). [sent-462, score-0.651]
</p><p>46 More recently, and in a different context from proper scoring rules, Drummond and Holte (2006) say “the performance independent criterion, in this case, is to set the threshold to correspond to the operating conditions. [sent-466, score-0.627]
</p><p>47 Deﬁnition 13 Assuming the model’s scores are expressed on a probability scale [0, 1], the scoredriven threshold choice method is deﬁned for cost proportions as follows: Tcsd (c)  c  Tzsd (z)  z. [sent-471, score-0.804]
</p><p>48 (7)  and for skews as Given this threshold choice method, then the question is: if we must evaluate a model before application for a wide range of skews and cost proportions, which performance metric should be used? [sent-472, score-0.859]
</p><p>49 , 2011) Assuming probabilistic scores and the score-driven a threshold choice method, expected loss under a uniform distribution of cost proportions is equal to the model’s Brier score. [sent-475, score-0.959]
</p><p>50 We calculated the expected loss for the score-driven threshold choice method for a uniform distribution of cost proportions as its Brier score. [sent-483, score-0.815]
</p><p>51 But Theorem 14 can now be seen as a result which connects these two different dimensions: cost distribution and threshold choice method, so placing the Brier score at an even more predominant role. [sent-500, score-0.581]
</p><p>52 The Brier score is seen as expected loss for the score-driven threshold choice method, while accuracy assumes a ﬁxed threshold. [sent-515, score-0.608]
</p><p>53 Threshold Choice Methods Using Rates We show in this section that AUC can be translated into expected loss for varying operating conditions in more than one way, depending on the threshold choice method used. [sent-518, score-0.721]
</p><p>54 We consider two threshold choice methods, where each of them sets the threshold to achieve a particular predicted positive rate: the rate-uniform method, which sets the rate in a uniform way; and the rate-driven method, which sets the rate equal to the operating condition. [sent-519, score-0.939]
</p><p>55 1 The Rate Uniform Threshold Choice Method Leads to AUC The rate-ﬁxed threshold choice method places the threshold in such a way that a given predictive positive rate is achieved. [sent-533, score-0.694]
</p><p>56 Deﬁnition 17 The rate-uniform threshold choice method non-deterministically sets the threshold to achieve a uniformly randomly selected rate: Tcru (c)  Tcr f [U0,1 ](c). [sent-539, score-0.694]
</p><p>57 This threshold choice method is a generalisation of the rate-ﬁxed threshold choice method which considers all the imbalances (class proportions) equally likely whenever we make a classiﬁcation. [sent-544, score-0.766]
</p><p>58 As done before for other threshold choice methods, we analyse the question: given this threshold choice method, if we must evaluate a model before application for a wide range of skews and cost proportions, which performance metric should be used? [sent-546, score-1.07]
</p><p>59 Theorem 18 Assuming the rate-uniform threshold choice method and invertible R, expected loss under a distribution of cost proportions wc decreases linearly with AUC as follows: ru Lc = π0 π1 (1 − 2AUC) + π0 Ewc {c} + π1 (1 − Ewc {c}). [sent-549, score-1.007]
</p><p>60 Corollary 19 Assuming the rate-uniform threshold choice method, invertible R, and a distribution of cost proportions wc with expected value Ewc {c} = 1/2, expected loss decreases linearly with AUC as follows: ru LE{c}=1/2 = π0 π1 (1 − 2AUC) + 1/2. [sent-559, score-1.062]
</p><p>61 Corollary 20 For any distribution of skews wz , assuming the rate-uniform threshold choice method and invertible R, expected loss decreases linearly with AUC as follows: ru Lz = (1 − 2AUC)/4 + 1/2. [sent-560, score-0.84]
</p><p>62 2 The Rate-Driven Threshold Choice Method Leads to AUC Naturally, if we can have precise information of the operating condition at deployment time, we can use the information about the skew or cost to adjust the rate of positives and negatives to that proportion. [sent-580, score-0.585]
</p><p>63 This leads to a new threshold selection method: if we are given skew (or cost proportion) z (or c), we choose the threshold t in such a way that we get a proportion of z (or c) positives. [sent-581, score-0.894]
</p><p>64 This is an elaboration of the rate-ﬁxed threshold choice method which does take the operating condition into account. [sent-582, score-0.63]
</p><p>65 Deﬁnition 21 The rate-driven threshold choice method for cost proportions is deﬁned as Tcr f [c](c) = R−1 (c). [sent-583, score-0.66]
</p><p>66 z  Tzrd (z)  Given this threshold choice method, the question is again: if we must evaluate a model before application for a wide range of skews and cost proportions, which performance metric should be used? [sent-585, score-0.687]
</p><p>67 Theorem 22 Expected loss for uniform cost proportions using the rate-driven threshold choice method is linearly related to AUC as follows: rd LU(c) = π1 π0 (1 − 2AUC) + 1/3. [sent-589, score-0.76]
</p><p>68 Corollary 23 Expected loss for uniform skews using the rate-driven threshold choice method is linearly related to AUC as follows: rd LU(z) = (1 − 2AUC)/4 + 1/3. [sent-596, score-0.655]
</p><p>69 Logically, LU(c) and LU(z) work upon ru ru information about the operating condition at deployment time, while LU(c) and LU(z) may be suited when this information is unavailable or unreliable. [sent-601, score-0.558]
</p><p>70 This threshold choice method, denoted by Tco , is deﬁned as follows: 2837  ´ H ERN ANDEZ -O RALLO , F LACH AND F ERRI  Figure 3: Illustration of the rate-driven threshold choice method. [sent-605, score-0.766]
</p><p>71 o Corollary 30 For every model m the expected loss for the optimal threshold choice method LU(c) is equal to the reﬁnement loss using the convex hull. [sent-710, score-0.611]
</p><p>72 Before analysing what the meaning of this threshold choice method is and how it relates to the rest, we have to consider whether this threshold choice method is realistic or not. [sent-714, score-0.766]
</p><p>73 In the beginning of this section we said that the optimal method assumes that (1) we are having complete information about the operating condition at deployment time and (2) we are able to use that information to choose the threshold that will minimise the loss at deployment time. [sent-715, score-0.877]
</p><p>74 In addition, this may shed light on which threshold choice su sd ru rd method is best. [sent-748, score-0.608]
</p><p>75 In order to answer these questions we need to analyse transformations on the scores and see how these affect the expected loss given by each threshold choice method. [sent-767, score-0.651]
</p><p>76 The following results connect the score-driven threshold choice method with the rate-driven threshold choice method: Theorem 34 Given a model and data set with set of scores σ, such that they are evenly-spaced, when n → ∞: 1 sd rd BS = LU(c) = LU(c) = π0 π1 (1 − 2AUC) + . [sent-797, score-0.97]
</p><p>77 Similarly, we get the same results for the score-uniform threshold choice method and the rateuniform threshold choice method. [sent-804, score-0.766]
</p><p>78 Now we analyse what happens with perfectly calibrated models for the score-driven threshold choice and the score-uniform threshold choice methods. [sent-850, score-0.904]
</p><p>79 In the following result, we see that for a calibrated model the optimal threshold T for a given cost proportion c is T = c, which is exactly the score-driven threshold choice method. [sent-866, score-0.973]
</p><p>80 o Starting with the expected loss for the optimal threshold choice method, that is, Lc (which uses Tco ), we have, from Theorem 43, that Tco (c) = Tcsd (c) = c when the model is perfectly calibrated. [sent-873, score-0.556]
</p><p>81 3 Choosing a Threshold Choice Method It is enlightening to see that many of the most popular classiﬁcation performance metrics are just expected losses by changing the threshold choice method and the use of cost proportions or skews. [sent-884, score-0.882]
</p><p>82 Some threshold choice methods can be seen as a score transformation followed by the score-driven threshold choice method. [sent-888, score-0.894]
</p><p>83 Even the ﬁxed threshold choice method can be seen as a crisp transformation where scores are set to 1 if si > t and 0 otherwise. [sent-889, score-0.583]
</p><p>84 Hence, this threshold choice method assumes that the calibration which is performed with the convex hull over the training (or a validation data set) is going to be perfect and hold for the test set. [sent-897, score-0.629]
</p><p>85 Figure 4 su ru also gives the impression that LU(c) and LU(c) are so bad that their corresponding threshold choice methods and metrics are useless. [sent-898, score-0.688]
</p><p>86 If we know the deployment operating condition at evaluation time, then we can ﬁx the threshold and get the expected loss. [sent-902, score-0.738]
</p><p>87 at deployment time  Chosen uniformly score-uniform (T su ) rate-uniform (T ru ) no information  Table 6: Information which is required (and when) for the seven threshold choice methods so that they become reasonable (or just not totally unreasonable). [sent-911, score-0.673]
</p><p>88 • The score-driven threshold choice method considers that the scores are estimated probabilities and that they are reliable, in the tradition of proper scoring rules. [sent-918, score-0.629]
</p><p>89 It can be seen as the score-driven threshold choice method where the scores have been calibrated by the PAV method. [sent-927, score-0.616]
</p><p>90 Now that we better understand the meaning of the threshold choice methods we may state the difﬁcult question more clearly: given a model, which threshold choice method should we use to make classiﬁcations? [sent-928, score-0.766]
</p><p>91 We evaluated the model for several threshold choice methods and from there we clearly saw which models were better calibrated and we ﬁnally made a decision about which model to use and with which threshold choice methods. [sent-945, score-0.855]
</p><p>92 Discussion This paper builds upon the notion of threshold choice method and the expected loss we can obtain for a range of cost proportions (or skews) for each of the threshold choice methods we have investigated. [sent-949, score-1.167]
</p><p>93 The links between threshold choice methods, between performance metrics, in general and for speciﬁc score arrangements, have provided us with a much broader (and more elaborate) view of classiﬁcation performance metrics and the way thresholds can be chosen. [sent-950, score-0.655]
</p><p>94 The starting point of this unifying view is that all the previous works above worked with only two threshold choice methods, which we have called the score-driven threshold choice method and the optimal threshold choice method. [sent-971, score-1.149]
</p><p>95 This is related to the ﬁxed threshold choice method, or the rate-uniform and score-uniform threshold choice methods used here. [sent-975, score-0.766]
</p><p>96 Many threshold choice methods give rise to particular kinds of curves that provide at each operating point, rather than just an aggregate. [sent-1016, score-0.627]
</p><p>97 , cost proportion or skews), we have to determine ﬁrst which threshold choice method is to be used. [sent-1020, score-0.573]
</p><p>98 Furthermore, we see that if the model is perfectly calibrated, the expected loss using the score-driven threshold choice method equals the optimal threshold choice method. [sent-1030, score-0.939]
</p><p>99 However, for a mere convenience that will become clear below, we include the factor E{b} in the loss produced at a decision threshold t and a cost proportion c, adapting Equation (16): Qc (t; c)  E{b}Qη (t; c, π0 ) = E{b}{cπ0 (1 − F0 (t)) + (1 − c)π1 F1 (t)}. [sent-1066, score-0.57]
</p><p>100 Intervals in the codomain of 2858  A U NIFIED V IEW OF P ERFORMANCE M ETRICS  thresholds will be represented with the letter τ and intervals in the domain of cost proportions or scores between 0 and 1 will be denoted by letter σ. [sent-1103, score-0.568]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('threshold', 0.311), ('lu', 0.261), ('brier', 0.241), ('operating', 0.214), ('ewc', 0.204), ('auc', 0.203), ('proportions', 0.18), ('skews', 0.172), ('lach', 0.151), ('iew', 0.146), ('nified', 0.146), ('rallo', 0.146), ('scores', 0.144), ('metrics', 0.14), ('qc', 0.135), ('wc', 0.13), ('roc', 0.126), ('andez', 0.125), ('deployment', 0.125), ('ern', 0.125), ('erri', 0.125), ('etrics', 0.125), ('bs', 0.122), ('calibration', 0.122), ('ds', 0.11), ('score', 0.101), ('cost', 0.097), ('proportion', 0.093), ('ru', 0.093), ('erformance', 0.091), ('intervals', 0.09), ('calibrated', 0.089), ('pav', 0.084), ('skew', 0.082), ('mae', 0.08), ('drummond', 0.078), ('dt', 0.074), ('flach', 0.072), ('su', 0.072), ('choice', 0.072), ('hern', 0.071), ('loss', 0.069), ('lc', 0.068), ('wz', 0.068), ('dc', 0.064), ('conv', 0.063), ('scoring', 0.063), ('murphy', 0.062), ('sd', 0.06), ('nement', 0.059), ('hull', 0.058), ('ferri', 0.058), ('holte', 0.058), ('expected', 0.055), ('fawcett', 0.052), ('beta', 0.05), ('qz', 0.049), ('perfectly', 0.049), ('rl', 0.048), ('wh', 0.047), ('costs', 0.046), ('bijective', 0.045), ('reid', 0.044), ('rlrocch', 0.042), ('wieand', 0.042), ('yb', 0.042), ('curve', 0.041), ('cl', 0.04), ('proper', 0.039), ('cal', 0.037), ('sf', 0.037), ('swets', 0.037), ('metric', 0.035), ('convex', 0.035), ('positives', 0.034), ('condition', 0.033), ('tc', 0.032), ('perfect', 0.031), ('uniform', 0.031), ('convexi', 0.031), ('lz', 0.031), ('rlconv', 0.031), ('tco', 0.031), ('tcsd', 0.031), ('vh', 0.031), ('classi', 0.031), ('thresholds', 0.031), ('curves', 0.03), ('unweighted', 0.03), ('si', 0.029), ('area', 0.028), ('williamson', 0.028), ('losses', 0.027), ('transformation', 0.027), ('sb', 0.027), ('tcs', 0.027), ('tz', 0.027), ('bins', 0.027), ('anagnostopoulos', 0.026), ('codomain', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000019 <a title="10-tfidf-1" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<p>Author: José Hernández-Orallo, Peter Flach, Cèsar Ferri</p><p>Abstract: Many performance metrics have been introduced in the literature for the evaluation of classiﬁcation performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into reﬁnement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassiﬁcation costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: ﬁxed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the reﬁnement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be summarised as follows: given a model, apply the threshold choice methods that correspond with the available information about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibra</p><p>2 0.062005918 <a title="10-tfidf-2" href="./jmlr-2012-Fast_Approximation_of_Matrix_Coherence_and_Statistical_Leverage.html">43 jmlr-2012-Fast Approximation of Matrix Coherence and Statistical Leverage</a></p>
<p>Author: Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, David P. Woodruff</p><p>Abstract: The statistical leverage scores of a matrix A are the squared row-norms of the matrix containing its (top) left singular vectors and the coherence is the largest leverage score. These quantities are of interest in recently-popular problems such as matrix completion and Nystr¨ m-based low-rank o matrix approximation as well as in large-scale statistical data analysis applications more generally; moreover, they are of interest since they deﬁne the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms. Our main result is a randomized algorithm that takes as input an arbitrary n × d matrix A, with n ≫ d, and that returns as output relative-error approximations to all n of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of n and d) in O(nd log n) time, as opposed to the O(nd 2 ) time required by the na¨ve algorithm that involves computing an orthogonal basis for the ı range of A. Our analysis may be viewed in terms of computing a relative-error approximation to an underconstrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with n ≈ d, and the extension to streaming environments. Keywords: matrix coherence, statistical leverage, randomized algorithm</p><p>3 0.061401904 <a title="10-tfidf-3" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>Author: Jian Huang, Cun-Hui Zhang</p><p>Abstract: The ℓ1 -penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ1 -penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1 -penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results. Keywords: variable selection, penalized estimation, oracle inequality, generalized linear models, selection consistency, sparsity</p><p>4 0.057337761 <a title="10-tfidf-4" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>Author: Tim van Erven, Mark D. Reid, Robert C. Williamson</p><p>Abstract: Mixability of a loss characterizes fast rates in the online learning setting of prediction with expert advice. The determination of the mixability constant for binary losses is straightforward but opaque. In the binary case we make this transparent and simpler by characterising mixability in terms of the second derivative of the Bayes risk of proper losses. We then extend this result to multiclass proper losses where there are few existing results. We show that mixability is governed by the maximum eigenvalue of the Hessian of the Bayes risk, relative to the Hessian of the Bayes risk for log loss. We conclude by comparing our result to other work that bounds prediction performance in terms of the geometry of the Bayes risk. Although all calculations are for proper losses, we also show how to carry the results across to improper losses. Keywords: mixability, multiclass, prediction with expert advice, proper loss, learning rates</p><p>5 0.049435023 <a title="10-tfidf-5" href="./jmlr-2012-Towards_Integrative_Causal_Analysis_of_Heterogeneous_Data_Sets_and_Studies.html">114 jmlr-2012-Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies</a></p>
<p>Author: Ioannis Tsamardinos, Sofia Triantafillou, Vincenzo Lagani</p><p>Abstract: We present methods able to predict the presence and strength of conditional and unconditional dependencies (correlations) between two variables Y and Z never jointly measured on the same samples, based on multiple data sets measuring a set of common variables. The algorithms are specializations of prior work on learning causal structures from overlapping variable sets. This problem has also been addressed in the ﬁeld of statistical matching. The proposed methods are applied to a wide range of domains and are shown to accurately predict the presence of thousands of dependencies. Compared against prototypical statistical matching algorithms and within the scope of our experiments, the proposed algorithms make predictions that are better correlated with the sample estimates of the unknown parameters on test data ; this is particularly the case when the number of commonly measured variables is low. The enabling idea behind the methods is to induce one or all causal models that are simultaneously consistent with (ﬁt) all available data sets and prior knowledge and reason with them. This allows constraints stemming from causal assumptions (e.g., Causal Markov Condition, Faithfulness) to propagate. Several methods have been developed based on this idea, for which we propose the unifying name Integrative Causal Analysis (INCA). A contrived example is presented demonstrating the theoretical potential to develop more general methods for co-analyzing heterogeneous data sets. The computational experiments with the novel methods provide evidence that causallyinspired assumptions such as Faithfulness often hold to a good degree of approximation in many real systems and could be exploited for statistical inference. Code, scripts, and data are available at www.mensxmachina.org. Keywords: integrative causal analysis, causal discovery, Bayesian networks, maximal ancestral graphs, structural equation models, causality, statistical matching, data fusion</p><p>6 0.048978709 <a title="10-tfidf-6" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>7 0.044856638 <a title="10-tfidf-7" href="./jmlr-2012-PAC-Bayes_Bounds_with_Data_Dependent_Priors.html">87 jmlr-2012-PAC-Bayes Bounds with Data Dependent Priors</a></p>
<p>8 0.044221148 <a title="10-tfidf-8" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>9 0.044120453 <a title="10-tfidf-9" href="./jmlr-2012-Eliminating_Spammers_and_Ranking_Annotators_for_Crowdsourced_Labeling_Tasks.html">37 jmlr-2012-Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks</a></p>
<p>10 0.043250415 <a title="10-tfidf-10" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>11 0.042080916 <a title="10-tfidf-11" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>12 0.041210622 <a title="10-tfidf-12" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>13 0.038720466 <a title="10-tfidf-13" href="./jmlr-2012-Mal-ID%3A_Automatic_Malware_Detection_Using_Common_Segment_Analysis_and_Meta-Features.html">63 jmlr-2012-Mal-ID: Automatic Malware Detection Using Common Segment Analysis and Meta-Features</a></p>
<p>14 0.036509871 <a title="10-tfidf-14" href="./jmlr-2012-Sparse_and_Unique_Nonnegative_Matrix_Factorization_Through_Data_Preprocessing.html">108 jmlr-2012-Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing</a></p>
<p>15 0.036441918 <a title="10-tfidf-15" href="./jmlr-2012-Refinement_of_Operator-valued_Reproducing_Kernels.html">96 jmlr-2012-Refinement of Operator-valued Reproducing Kernels</a></p>
<p>16 0.033628188 <a title="10-tfidf-16" href="./jmlr-2012-Query_Strategies_for_Evading_Convex-Inducing_Classifiers.html">94 jmlr-2012-Query Strategies for Evading Convex-Inducing Classifiers</a></p>
<p>17 0.033369776 <a title="10-tfidf-17" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>18 0.033124212 <a title="10-tfidf-18" href="./jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</a></p>
<p>19 0.031353973 <a title="10-tfidf-19" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>20 0.03040871 <a title="10-tfidf-20" href="./jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering.html">101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.156), (1, 0.059), (2, 0.053), (3, -0.023), (4, -0.019), (5, 0.035), (6, -0.005), (7, 0.037), (8, 0.018), (9, -0.019), (10, -0.057), (11, -0.043), (12, 0.082), (13, 0.001), (14, 0.065), (15, 0.053), (16, 0.167), (17, 0.014), (18, 0.058), (19, 0.012), (20, 0.012), (21, 0.058), (22, -0.022), (23, -0.016), (24, 0.101), (25, 0.18), (26, -0.181), (27, 0.025), (28, 0.073), (29, -0.14), (30, -0.165), (31, -0.248), (32, 0.205), (33, -0.205), (34, -0.06), (35, 0.204), (36, -0.188), (37, 0.044), (38, 0.165), (39, 0.195), (40, 0.012), (41, -0.012), (42, 0.048), (43, -0.012), (44, 0.06), (45, -0.038), (46, -0.05), (47, -0.099), (48, 0.047), (49, -0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96562153 <a title="10-lsi-1" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<p>Author: José Hernández-Orallo, Peter Flach, Cèsar Ferri</p><p>Abstract: Many performance metrics have been introduced in the literature for the evaluation of classiﬁcation performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into reﬁnement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassiﬁcation costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: ﬁxed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the reﬁnement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be summarised as follows: given a model, apply the threshold choice methods that correspond with the available information about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibra</p><p>2 0.48318481 <a title="10-lsi-2" href="./jmlr-2012-Eliminating_Spammers_and_Ranking_Annotators_for_Crowdsourced_Labeling_Tasks.html">37 jmlr-2012-Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks</a></p>
<p>Author: Vikas C. Raykar, Shipeng Yu</p><p>Abstract: With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a data set labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Since we do not have control over the quality of the annotators, very often the annotations can be dominated by spammers, deﬁned as annotators who assign labels randomly without actually looking at the instance. Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the ﬁnal consensus labels. In this paper we propose an empirical Bayesian algorithm called SpEM that iteratively eliminates the spammers and estimates the consensus labels based only on the good annotators. The algorithm is motivated by deﬁning a spammer score that can be used to rank the annotators. Experiments on simulated and real data show that the proposed approach is better than (or as good as) the earlier approaches in terms of the accuracy and uses a signiﬁcantly smaller number of annotators. Keywords: crowdsourcing, multiple annotators, ranking annotators, spammers</p><p>3 0.42602611 <a title="10-lsi-3" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>Author: Tim van Erven, Mark D. Reid, Robert C. Williamson</p><p>Abstract: Mixability of a loss characterizes fast rates in the online learning setting of prediction with expert advice. The determination of the mixability constant for binary losses is straightforward but opaque. In the binary case we make this transparent and simpler by characterising mixability in terms of the second derivative of the Bayes risk of proper losses. We then extend this result to multiclass proper losses where there are few existing results. We show that mixability is governed by the maximum eigenvalue of the Hessian of the Bayes risk, relative to the Hessian of the Bayes risk for log loss. We conclude by comparing our result to other work that bounds prediction performance in terms of the geometry of the Bayes risk. Although all calculations are for proper losses, we also show how to carry the results across to improper losses. Keywords: mixability, multiclass, prediction with expert advice, proper loss, learning rates</p><p>4 0.32813558 <a title="10-lsi-4" href="./jmlr-2012-Mal-ID%3A_Automatic_Malware_Detection_Using_Common_Segment_Analysis_and_Meta-Features.html">63 jmlr-2012-Mal-ID: Automatic Malware Detection Using Common Segment Analysis and Meta-Features</a></p>
<p>Author: Gil Tahan, Lior Rokach, Yuval Shahar</p><p>Abstract: This paper proposes several novel methods, based on machine learning, to detect malware in executable ﬁles without any need for preprocessing, such as unpacking or disassembling. The basic method (Mal-ID) is a new static (form-based) analysis methodology that uses common segment analysis in order to detect malware ﬁles. By using common segment analysis, Mal-ID is able to discard malware parts that originate from benign code. In addition, Mal-ID uses a new kind of feature, termed meta-feature, to better capture the properties of the analyzed segments. Rather than using the entire ﬁle, as is usually the case with machine learning based techniques, the new approach detects malware on the segment level. This study also introduces two Mal-ID extensions that improve the Mal-ID basic method in various aspects. We rigorously evaluated Mal-ID and its two extensions with more than ten performance measures, and compared them to the highly rated boosted decision tree method under identical settings. The evaluation demonstrated that Mal-ID and the two Mal-ID extensions outperformed the boosted decision tree method in almost all respects. In addition, the results indicated that by extracting meaningful features, it is sufﬁcient to employ one simple detection rule for classifying executable ﬁles. Keywords: computer security, malware detection, common segment analysis, supervised learning</p><p>5 0.32679895 <a title="10-lsi-5" href="./jmlr-2012-Towards_Integrative_Causal_Analysis_of_Heterogeneous_Data_Sets_and_Studies.html">114 jmlr-2012-Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies</a></p>
<p>Author: Ioannis Tsamardinos, Sofia Triantafillou, Vincenzo Lagani</p><p>Abstract: We present methods able to predict the presence and strength of conditional and unconditional dependencies (correlations) between two variables Y and Z never jointly measured on the same samples, based on multiple data sets measuring a set of common variables. The algorithms are specializations of prior work on learning causal structures from overlapping variable sets. This problem has also been addressed in the ﬁeld of statistical matching. The proposed methods are applied to a wide range of domains and are shown to accurately predict the presence of thousands of dependencies. Compared against prototypical statistical matching algorithms and within the scope of our experiments, the proposed algorithms make predictions that are better correlated with the sample estimates of the unknown parameters on test data ; this is particularly the case when the number of commonly measured variables is low. The enabling idea behind the methods is to induce one or all causal models that are simultaneously consistent with (ﬁt) all available data sets and prior knowledge and reason with them. This allows constraints stemming from causal assumptions (e.g., Causal Markov Condition, Faithfulness) to propagate. Several methods have been developed based on this idea, for which we propose the unifying name Integrative Causal Analysis (INCA). A contrived example is presented demonstrating the theoretical potential to develop more general methods for co-analyzing heterogeneous data sets. The computational experiments with the novel methods provide evidence that causallyinspired assumptions such as Faithfulness often hold to a good degree of approximation in many real systems and could be exploited for statistical inference. Code, scripts, and data are available at www.mensxmachina.org. Keywords: integrative causal analysis, causal discovery, Bayesian networks, maximal ancestral graphs, structural equation models, causality, statistical matching, data fusion</p><p>6 0.31817991 <a title="10-lsi-6" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>7 0.2786305 <a title="10-lsi-7" href="./jmlr-2012-Fast_Approximation_of_Matrix_Coherence_and_Statistical_Leverage.html">43 jmlr-2012-Fast Approximation of Matrix Coherence and Statistical Leverage</a></p>
<p>8 0.23893051 <a title="10-lsi-8" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>9 0.22754675 <a title="10-lsi-9" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>10 0.22386988 <a title="10-lsi-10" href="./jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering.html">101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</a></p>
<p>11 0.22149652 <a title="10-lsi-11" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>12 0.21072963 <a title="10-lsi-12" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>13 0.20964441 <a title="10-lsi-13" href="./jmlr-2012-Random_Search_for_Hyper-Parameter_Optimization.html">95 jmlr-2012-Random Search for Hyper-Parameter Optimization</a></p>
<p>14 0.19760217 <a title="10-lsi-14" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>15 0.19699316 <a title="10-lsi-15" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>16 0.19539361 <a title="10-lsi-16" href="./jmlr-2012-PAC-Bayes_Bounds_with_Data_Dependent_Priors.html">87 jmlr-2012-PAC-Bayes Bounds with Data Dependent Priors</a></p>
<p>17 0.18872967 <a title="10-lsi-17" href="./jmlr-2012-Refinement_of_Operator-valued_Reproducing_Kernels.html">96 jmlr-2012-Refinement of Operator-valued Reproducing Kernels</a></p>
<p>18 0.18747729 <a title="10-lsi-18" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>19 0.17873321 <a title="10-lsi-19" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>20 0.17627251 <a title="10-lsi-20" href="./jmlr-2012-Security_Analysis_of_Online_Centroid_Anomaly_Detection.html">104 jmlr-2012-Security Analysis of Online Centroid Anomaly Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.015), (21, 0.044), (26, 0.034), (29, 0.042), (35, 0.018), (49, 0.011), (55, 0.456), (56, 0.018), (57, 0.011), (64, 0.022), (69, 0.024), (75, 0.059), (77, 0.013), (92, 0.068), (96, 0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70366907 <a title="10-lda-1" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<p>Author: José Hernández-Orallo, Peter Flach, Cèsar Ferri</p><p>Abstract: Many performance metrics have been introduced in the literature for the evaluation of classiﬁcation performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into reﬁnement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassiﬁcation costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: ﬁxed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the reﬁnement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be summarised as follows: given a model, apply the threshold choice methods that correspond with the available information about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibra</p><p>2 0.5790391 <a title="10-lda-2" href="./jmlr-2012-Algorithms_for_Learning_Kernels_Based_on_Centered_Alignment.html">16 jmlr-2012-Algorithms for Learning Kernels Based on Centered Alignment</a></p>
<p>Author: Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difﬁcult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classiﬁcation and regression. Our algorithms are based on the notion of centered alignment which is used as a similarity measure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efﬁcient algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization. Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classiﬁcation and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classiﬁcation and regression. Keywords: kernel methods, learning kernels, feature selection</p><p>3 0.2707954 <a title="10-lda-3" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>Author: Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, Lin Xiao</p><p>Abstract: Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment. We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem. Keywords: distributed computing, online learning, stochastic optimization, regret bounds, convex optimization</p><p>4 0.26971054 <a title="10-lda-4" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>Author: Sangkyun Lee, Stephen J. Wright</p><p>Abstract: Iterative methods that calculate their steps from approximate subgradient directions have proved to be useful for stochastic learning problems over large and streaming data sets. When the objective consists of a loss function plus a nonsmooth regularization term, the solution often lies on a lowdimensional manifold of parameter space along which the regularizer is smooth. (When an ℓ1 regularizer is used to induce sparsity in the solution, for example, this manifold is deﬁned by the set of nonzero components of the parameter vector.) This paper shows that a regularized dual averaging algorithm can identify this manifold, with high probability, before reaching the solution. This observation motivates an algorithmic strategy in which, once an iterate is suspected of lying on an optimal or near-optimal manifold, we switch to a “local phase” that searches in this manifold, thus converging rapidly to a near-optimal point. Computational results are presented to verify the identiﬁcation property and to illustrate the effectiveness of this approach. Keywords: regularization, dual averaging, partly smooth manifold, manifold identiﬁcation</p><p>5 0.26954919 <a title="10-lda-5" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>Author: Matus Telgarsky</p><p>Abstract: Boosting combines weak learners into a predictor with low empirical risk. Its dual constructs a high entropy distribution upon which weak learners and training labels are uncorrelated. This manuscript studies this primal-dual relationship under a broad family of losses, including the exponential loss of AdaBoost and the logistic loss, revealing: • Weak learnability aids the whole loss family: for any ε > 0, O (ln(1/ε)) iterations sufﬁce to produce a predictor with empirical risk ε-close to the inﬁmum; • The circumstances granting the existence of an empirical risk minimizer may be characterized in terms of the primal and dual problems, yielding a new proof of the known rate O (ln(1/ε)); • Arbitrary instances may be decomposed into the above two, granting rate O (1/ε), with a matching lower bound provided for the logistic loss. Keywords: boosting, convex analysis, weak learnability, coordinate descent, maximum entropy</p><p>6 0.26541057 <a title="10-lda-6" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>7 0.26453179 <a title="10-lda-7" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>8 0.26442331 <a title="10-lda-8" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>9 0.26388061 <a title="10-lda-9" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>10 0.26297563 <a title="10-lda-10" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>11 0.26266235 <a title="10-lda-11" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>12 0.26073056 <a title="10-lda-12" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>13 0.26009929 <a title="10-lda-13" href="./jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</a></p>
<p>14 0.25938988 <a title="10-lda-14" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>15 0.25865445 <a title="10-lda-15" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>16 0.25851557 <a title="10-lda-16" href="./jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</a></p>
<p>17 0.25839382 <a title="10-lda-17" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>18 0.2582494 <a title="10-lda-18" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>19 0.25822166 <a title="10-lda-19" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>20 0.258185 <a title="10-lda-20" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
