<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>100 jmlr-2012-Robust Kernel Density Estimation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-100" href="#">jmlr2012-100</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>100 jmlr-2012-Robust Kernel Density Estimation</h1>
<br/><p>Source: <a title="jmlr-2012-100-pdf" href="http://jmlr.org/papers/volume13/kim12b/kim12b.pdf">pdf</a></p><p>Author: JooSeuk Kim, Clayton D. Scott</p><p>Abstract: We propose a method for nonparametric density estimation that exhibits robustness to contamination of the training sample. This method achieves robustness by combining a traditional kernel density estimator (KDE) with ideas from classical M-estimation. We interpret the KDE based on a positive semi-deﬁnite kernel as a sample mean in the associated reproducing kernel Hilbert space. Since the sample mean is sensitive to outliers, we estimate it robustly via M-estimation, yielding a robust kernel density estimator (RKDE). An RKDE can be computed efﬁciently via a kernelized iteratively re-weighted least squares (IRWLS) algorithm. Necessary and sufﬁcient conditions are given for kernelized IRWLS to converge to the global minimizer of the M-estimator objective function. The robustness of the RKDE is demonstrated with a representer theorem, the inﬂuence function, and experimental results for density estimation and anomaly detection. Keywords: outlier, reproducing kernel Hilbert space, kernel trick, inﬂuence function, M-estimation</p><p>Reference: <a title="jmlr-2012-100-reference" href="../jmlr2012_reference/jmlr-2012-Robust_Kernel_Density_Estimation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Electrical Engineering and Computer Science University of Michigan Ann Arbor, MI 48109-2122 USA  Editor: Kenji Fukumizu  Abstract We propose a method for nonparametric density estimation that exhibits robustness to contamination of the training sample. [sent-4, score-0.652]
</p><p>2 This method achieves robustness by combining a traditional kernel density estimator (KDE) with ideas from classical M-estimation. [sent-5, score-0.612]
</p><p>3 We interpret the KDE based on a positive semi-deﬁnite kernel as a sample mean in the associated reproducing kernel Hilbert space. [sent-6, score-0.431]
</p><p>4 Since the sample mean is sensitive to outliers, we estimate it robustly via M-estimation, yielding a robust kernel density estimator (RKDE). [sent-7, score-0.789]
</p><p>5 An RKDE can be computed efﬁciently via a kernelized iteratively re-weighted least squares (IRWLS) algorithm. [sent-8, score-0.138]
</p><p>6 Necessary and sufﬁcient conditions are given for kernelized IRWLS to converge to the global minimizer of the M-estimator objective function. [sent-9, score-0.093]
</p><p>7 The robustness of the RKDE is demonstrated with a representer theorem, the inﬂuence function, and experimental results for density estimation and anomaly detection. [sent-10, score-0.529]
</p><p>8 Keywords: outlier, reproducing kernel Hilbert space, kernel trick, inﬂuence function, M-estimation  1. [sent-11, score-0.431]
</p><p>9 Introduction The kernel density estimator (KDE) is a well-known nonparametric estimator of univariate or multivariate densities, and numerous articles have been written on its properties, applications, and extensions (Silverman, 1986; Scott, 1992). [sent-12, score-0.624]
</p><p>10 This paper addresses a method of nonparametric density estimation that generalizes the KDE, and exhibits robustness to contamination of the training sample. [sent-14, score-0.652]
</p><p>11 1 Consider training data following a contamination model iid  X1 , . [sent-15, score-0.207]
</p><p>12 , Xn ∼ (1 − p) f0 + p f1 , where f0 is the “nominal” density to be estimated, f1 is the density of the contaminating distribution, 1 and p < 2 is the proportion of contamination. [sent-18, score-0.533]
</p><p>13 The objective is to estimate f0 while making no parametric assumptions about the nominal or contaminating distributions. [sent-20, score-0.356]
</p><p>14 Instead, we will focus on a set of nonparametric conditions that are reasonable in many practical applications. [sent-28, score-0.07]
</p><p>15 As a motivating application, consider anomaly detection in a computer network. [sent-30, score-0.127]
</p><p>16 For example, each Xi may record the volume of trafﬁc along certain links in the network, at a certain instant in time (Chhabra et al. [sent-35, score-0.035]
</p><p>17 If each measurement is collected when the network is in a nominal state, these data could be used to construct an anomaly detector by ﬁrst estimating the density f0 of nominal measurements, and then thresholding that estimate at some level to obtain decision regions. [sent-37, score-0.806]
</p><p>18 Hence, it is necessary to estimate the nominal density (or a level set thereof) from contaminated data. [sent-40, score-0.61]
</p><p>19 Furthermore, the distributions of both nominal and anomalous measurements are potentially complex, and it is therefore desirable to avoid parametric models. [sent-41, score-0.348]
</p><p>20 The proposed method achieves robustness by combining a traditional kernel density estimator with ideas from M-estimation (Huber, 1964; Hampel, 1974). [sent-42, score-0.612]
</p><p>21 The KDE based on a translation invariant, positive semi-deﬁnite (PSD) kernel is interpreted as a sample mean in the reproducing kernel Hilbert space (RKHS) associated with the kernel. [sent-43, score-0.482]
</p><p>22 Since the sample mean is sensitive to outliers, we estimate it robustly via M-estimation, yielding a robust kernel density estimator (RKDE). [sent-44, score-0.789]
</p><p>23 We describe a kernelized iteratively re-weighted least squares (KIRWLS) algorithm to efﬁciently compute the RKDE, and provide necessary and sufﬁcient conditions for the convergence of KIRWLS to the RKDE. [sent-45, score-0.138]
</p><p>24 We also offer three arguments to support the claim that the RKDE robustly estimates the nominal density and its level sets. [sent-46, score-0.517]
</p><p>25 First, we characterize the RKDE by a representer theorem. [sent-47, score-0.067]
</p><p>26 This theorem shows that the RKDE is a weighted KDE, and the weights are smaller for more outlying data points. [sent-48, score-0.071]
</p><p>27 Second, we study the inﬂuence function of the RKDE, and show through an exact formula and numerical results that the RKDE is less sensitive to contamination by outliers than the KDE. [sent-49, score-0.366]
</p><p>28 Third, we conduct experiments on several benchmark data sets that demonstrate the improved performance of the RKDE, relative to competing methods, at both density estimation and anomaly detection. [sent-50, score-0.383]
</p><p>29 One motivation for this work is that the traditional kernel density estimator is well-known to be sensitive to outliers. [sent-51, score-0.556]
</p><p>30 Even without contamination, the standard KDE tends to overestimate the density in regions where the true density is low. [sent-52, score-0.477]
</p><p>31 This has motivated several authors to consider variable kernel density estimators (VKDEs), which employ a data-dependent bandwidth at each data point (Breiman et al. [sent-53, score-0.46]
</p><p>32 This bandwidth is adapted to be larger where the data are less dense, with the aim of decreasing the aforementioned bias. [sent-55, score-0.098]
</p><p>33 Such methods have been applied in outlier detection and computer vision applications (Comaniciu et al. [sent-56, score-0.041]
</p><p>34 , 2007), and are one possible approach to robust nonparametric density estimation. [sent-58, score-0.418]
</p><p>35 2530  ROBUST K ERNEL D ENSITY E STIMATION  Density estimation with positive semi-deﬁnite kernels has been studied by several authors. [sent-60, score-0.071]
</p><p>36 Vapnik and Mukherjee (2000) optimize a criterion based on the empirical cumulative distribution function over the class of weighted KDEs based on a PSD kernel. [sent-61, score-0.025]
</p><p>37 Shawe-Taylor and Dolia (2007) provide a reﬁned theoretical treatment of this approach. [sent-62, score-0.028]
</p><p>38 (2008) adopt a different criterion based on Hilbert space embeddings of probability distributions. [sent-64, score-0.051]
</p><p>39 Our approach is somewhat similar in that we attempt to match the mean of the empirical distribution in the RKHS, but our criterion is different. [sent-65, score-0.025]
</p><p>40 These methods were also not designed with contaminated data in mind. [sent-66, score-0.141]
</p><p>41 We show that the standard kernel density estimator can be viewed as the solution to a certain least squares problem in the RKHS. [sent-67, score-0.505]
</p><p>42 The use of quadratic criteria in density estimation has also been previously developed. [sent-68, score-0.256]
</p><p>43 optimizes the norm-squared in Hilbert space, whereas Kim (1995), Girolami and He (2003), Kim and Scott (2010) and Mahapatruni and Gray (2011) adopt the integrated squared error. [sent-70, score-0.026]
</p><p>44 Once again, these methods are not designed for contaminated data. [sent-71, score-0.141]
</p><p>45 Previous work combining robust estimation and kernel methods has focused primarily on supervised learning problems. [sent-72, score-0.322]
</p><p>46 M-estimation applied to kernel regression has been studied by various authors (Christmann and Steinwart, 2007; Debruyne et al. [sent-73, score-0.168]
</p><p>47 In unsupervised learning, a robust way of doing kernel principal component analysis, called spherical KPCA, has been proposed, which applies PCA to feature vectors projected onto a unit sphere around the spatial median in a kernel feature space (Debruyne et al. [sent-79, score-0.493]
</p><p>48 The kernelized spatial depth was also proposed to estimate depth contours nonparametrically (Chen et al. [sent-81, score-0.317]
</p><p>49 To our knowledge, the RKDE is the ﬁrst application of M-estimation ideas in kernel density estimation. [sent-83, score-0.426]
</p><p>50 In Section 2 we propose robust kernel density estimation. [sent-84, score-0.516]
</p><p>51 In Section 3 we present a representer theorem for the RKDE. [sent-85, score-0.067]
</p><p>52 , Xn ∈ Rd be a random sample from a distribution F with a density f . [sent-98, score-0.225]
</p><p>53 The kernel density estimate of f , also called the Parzen window estimate, is a nonparametric estimate given by fKDE (x) =  1 n ∑ kσ (x, Xi ) n i=1  where kσ is a kernel function with bandwidth σ. [sent-99, score-0.766]
</p><p>54 To ensure that fKDE (x) is a density, we assume the kernel function satisﬁes kσ ( · , · ) ≥ 0 and kσ (x, · ) dx = 1. [sent-100, score-0.198]
</p><p>55 We will also assume that kσ (x, x′ ) is translation invariant, in that kσ (x − z, x′ − z) = kσ (x, x′ ) for all x, x′ , and z. [sent-101, score-0.051]
</p><p>56 Every PSD kernel kσ is associated with a unique Hilbert space of functions called its reproducing kernel Hilbert space (RKHS) which we will denote H , and kσ is called the reproducing kernel of H . [sent-108, score-0.694]
</p><p>57 See Steinwart and Christmann (2008) for a thorough treatment of PSD kernels and RKHSs. [sent-110, score-0.068]
</p><p>58 For our purposes, the critical property of H is the so-called reproducing property. [sent-111, score-0.095]
</p><p>59 We also note that, by translation invariance, the functions Φ(x) have constant norm in H because Φ(x) 2 = Φ(x), Φ(x) H = kσ (x, x) = kσ (0, 0). [sent-114, score-0.051]
</p><p>60 H g∈H i=1  Being the solution of a least squares problem, the KDE is sensitive to the presence of outliers among the Φ(Xi )’s. [sent-118, score-0.204]
</p><p>61 To reduce the effect of outliers, we propose to use M-estimation (Huber, 1964) to ﬁnd a robust sample mean of the Φ(Xi )’s. [sent-119, score-0.123]
</p><p>62 For a robust loss function ρ(x) on x ≥ 0, the robust kernel density estimate is deﬁned as n  fRKDE = arg min ∑ ρ Φ(Xi ) − g H . [sent-120, score-0.673]
</p><p>63 g∈H  (2)  i=1  Well-known examples of robust loss functions are Huber’s or Hampel’s ρ. [sent-121, score-0.123]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rkde', 0.497), ('kde', 0.414), ('density', 0.225), ('nominal', 0.21), ('contamination', 0.207), ('kernel', 0.168), ('fkde', 0.166), ('scott', 0.165), ('contaminated', 0.141), ('anomaly', 0.127), ('huber', 0.127), ('hampel', 0.124), ('kirwls', 0.124), ('robust', 0.123), ('outliers', 0.103), ('reproducing', 0.095), ('hilbert', 0.095), ('kernelized', 0.093), ('kim', 0.093), ('psd', 0.089), ('contaminating', 0.083), ('cott', 0.083), ('debruyne', 0.083), ('irwls', 0.083), ('jooseuk', 0.083), ('robustly', 0.082), ('robustness', 0.079), ('outlying', 0.071), ('umich', 0.071), ('ensity', 0.071), ('nonparametric', 0.07), ('estimator', 0.067), ('bandwidth', 0.067), ('representer', 0.067), ('clayton', 0.064), ('christmann', 0.064), ('anomalous', 0.064), ('rkhs', 0.058), ('sensitive', 0.056), ('stimation', 0.052), ('translation', 0.051), ('ernel', 0.049), ('song', 0.047), ('depth', 0.047), ('squares', 0.045), ('measurements', 0.045), ('steinwart', 0.045), ('outlier', 0.041), ('im', 0.041), ('kernels', 0.04), ('exhibits', 0.04), ('traditional', 0.04), ('cd', 0.038), ('uence', 0.038), ('dolia', 0.035), ('kpca', 0.035), ('kenji', 0.035), ('wellknown', 0.035), ('labor', 0.035), ('nonparametrically', 0.035), ('instant', 0.035), ('anomalies', 0.035), ('abramson', 0.035), ('chhabra', 0.035), ('estimate', 0.034), ('xi', 0.034), ('densities', 0.034), ('spatial', 0.034), ('yielding', 0.034), ('ideas', 0.033), ('girolami', 0.032), ('silverman', 0.032), ('abundant', 0.032), ('ann', 0.032), ('arbor', 0.032), ('scovel', 0.032), ('rd', 0.031), ('estimation', 0.031), ('aforementioned', 0.031), ('dx', 0.03), ('tedious', 0.029), ('fukumizu', 0.029), ('minority', 0.029), ('diffuse', 0.029), ('parametric', 0.029), ('treatment', 0.028), ('parzen', 0.027), ('contours', 0.027), ('articles', 0.027), ('overestimate', 0.027), ('thereof', 0.026), ('spatially', 0.026), ('acoustics', 0.026), ('traf', 0.026), ('adopt', 0.026), ('criterion', 0.025), ('intensive', 0.025), ('michigan', 0.025), ('laplacian', 0.025), ('offered', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="100-tfidf-1" href="./jmlr-2012-Robust_Kernel_Density_Estimation.html">100 jmlr-2012-Robust Kernel Density Estimation</a></p>
<p>Author: JooSeuk Kim, Clayton D. Scott</p><p>Abstract: We propose a method for nonparametric density estimation that exhibits robustness to contamination of the training sample. This method achieves robustness by combining a traditional kernel density estimator (KDE) with ideas from classical M-estimation. We interpret the KDE based on a positive semi-deﬁnite kernel as a sample mean in the associated reproducing kernel Hilbert space. Since the sample mean is sensitive to outliers, we estimate it robustly via M-estimation, yielding a robust kernel density estimator (RKDE). An RKDE can be computed efﬁciently via a kernelized iteratively re-weighted least squares (IRWLS) algorithm. Necessary and sufﬁcient conditions are given for kernelized IRWLS to converge to the global minimizer of the M-estimator objective function. The robustness of the RKDE is demonstrated with a representer theorem, the inﬂuence function, and experimental results for density estimation and anomaly detection. Keywords: outlier, reproducing kernel Hilbert space, kernel trick, inﬂuence function, M-estimation</p><p>2 0.087345466 <a title="100-tfidf-2" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>Author: Aharon Ben-Tal, Sahely Bhadra, Chiranjib Bhattacharyya, Arkadi Nemirovski</p><p>Abstract: In this paper we study the problem of designing SVM classiﬁers when the kernel matrix, K, is affected by uncertainty. Speciﬁcally K is modeled as a positive afﬁne combination of given positive semi deﬁnite kernels, with the coefﬁcients ranging in a norm-bounded uncertainty set. We treat the problem using the Robust Optimization methodology. This reduces the uncertain SVM problem into a deterministic conic quadratic problem which can be solved in principle by a polynomial time Interior Point (IP) algorithm. However, for large-scale classiﬁcation problems, IP methods become intractable and one has to resort to ﬁrst-order gradient type methods. The strategy we use here is to reformulate the robust counterpart of the uncertain SVM problem as a saddle point problem and employ a special gradient scheme which works directly on the convex-concave saddle function. The algorithm is a simpliﬁed version of a general scheme due to Juditski and Nemirovski (2011). It achieves an O(1/T 2 ) reduction of the initial error after T iterations. A comprehensive empirical study on both synthetic data and real-world protein structure data sets show that the proposed formulations achieve the desired robustness, and the saddle point based algorithm outperforms the IP method signiﬁcantly. Keywords: robust optimization, uncertain classiﬁcation, kernel functions</p><p>3 0.086499676 <a title="100-tfidf-3" href="./jmlr-2012-Refinement_of_Operator-valued_Reproducing_Kernels.html">96 jmlr-2012-Refinement of Operator-valued Reproducing Kernels</a></p>
<p>Author: Haizhang Zhang, Yuesheng Xu, Qinghui Zhang</p><p>Abstract: This paper studies the construction of a reﬁnement kernel for a given operator-valued reproducing kernel such that the vector-valued reproducing kernel Hilbert space of the reﬁnement kernel contains that of the given kernel as a subspace. The study is motivated from the need of updating the current operator-valued reproducing kernel in multi-task learning when underﬁtting or overﬁtting occurs. Numerical simulations conﬁrm that the established reﬁnement kernel method is able to meet this need. Various characterizations are provided based on feature maps and vector-valued integral representations of operator-valued reproducing kernels. Concrete examples of reﬁning translation invariant and ﬁnite Hilbert-Schmidt operator-valued reproducing kernels are provided. Other examples include reﬁnement of Hessian of scalar-valued translation-invariant kernels and transformation kernels. Existence and properties of operator-valued reproducing kernels preserved during the reﬁnement process are also investigated. Keywords: vector-valued reproducing kernel Hilbert spaces, operator-valued reproducing kernels, reﬁnement, embedding, translation invariant kernels, Hessian of Gaussian kernels, Hilbert-Schmidt kernels, numerical experiments</p><p>4 0.061162952 <a title="100-tfidf-4" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>Author: Garvesh Raskutti, Martin J. Wainwright, Bin Yu</p><p>Abstract: Sparse additive models are families of d-variate functions with the additive decomposition f ∗ = ∑ j∈S f j∗ , where S is an unknown subset of cardinality s ≪ d. In this paper, we consider the case where each univariate component function f j∗ lies in a reproducing kernel Hilbert space (RKHS), and analyze a method for estimating the unknown function f ∗ based on kernels combined with ℓ1 -type convex regularization. Working within a high-dimensional framework that allows both the dimension d and sparsity s to increase with n, we derive convergence rates in the L2 (P) and L2 (Pn ) norms over the class Fd,s,H of sparse additive models with each univariate function f j∗ in the unit ball of a univariate RKHS with bounded kernel function. We complement our upper bounds by deriving minimax lower bounds on the L2 (P) error, thereby showing the optimality of our method. Thus, we obtain optimal minimax rates for many interesting classes of sparse additive models, including polynomials, splines, and Sobolev classes. We also show that if, in contrast to our univariate conditions, the d-variate function class is assumed to be globally bounded, then much √ faster estimation rates are possible for any sparsity s = Ω( n), showing that global boundedness is a signiﬁcant restriction in the high-dimensional setting. Keywords: sparsity, kernel, non-parametric, convex, minimax</p><p>5 0.060482126 <a title="100-tfidf-5" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>Author: Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, Alexander Smola</p><p>Abstract: We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distributionfree tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efﬁcient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the ﬁrst such tests. ∗. †. ‡. §. Also at Gatsby Computational Neuroscience Unit, CSML, 17 Queen Square, London WC1N 3AR, UK. This work was carried out while K.M.B. was with the Ludwig-Maximilians-Universit¨ t M¨ nchen. a u This work was carried out while M.J.R. was with the Graz University of Technology. Also at The Australian National University, Canberra, ACT 0200, Australia. c 2012 Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch¨ lkopf and Alexander Smola. o ¨ G RETTON , B ORGWARDT, R ASCH , S CH OLKOPF AND S MOLA Keywords: kernel methods, two-sample test, uniform convergence bounds, schema matching, integral probability metric, hypothesis testing</p><p>6 0.055180389 <a title="100-tfidf-6" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>7 0.052888222 <a title="100-tfidf-7" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>8 0.051363207 <a title="100-tfidf-8" href="./jmlr-2012-Feature_Selection_via_Dependence_Maximization.html">44 jmlr-2012-Feature Selection via Dependence Maximization</a></p>
<p>9 0.050528824 <a title="100-tfidf-9" href="./jmlr-2012-Stability_of_Density-Based_Clustering.html">109 jmlr-2012-Stability of Density-Based Clustering</a></p>
<p>10 0.041455641 <a title="100-tfidf-10" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>11 0.040643729 <a title="100-tfidf-11" href="./jmlr-2012-Algorithms_for_Learning_Kernels_Based_on_Centered_Alignment.html">16 jmlr-2012-Algorithms for Learning Kernels Based on Centered Alignment</a></p>
<p>12 0.040623564 <a title="100-tfidf-12" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>13 0.040457569 <a title="100-tfidf-13" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>14 0.035479993 <a title="100-tfidf-14" href="./jmlr-2012-Security_Analysis_of_Online_Centroid_Anomaly_Detection.html">104 jmlr-2012-Security Analysis of Online Centroid Anomaly Detection</a></p>
<p>15 0.032067042 <a title="100-tfidf-15" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>16 0.031728487 <a title="100-tfidf-16" href="./jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems.html">89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</a></p>
<p>17 0.030149532 <a title="100-tfidf-17" href="./jmlr-2012-Minimax_Manifold_Estimation.html">68 jmlr-2012-Minimax Manifold Estimation</a></p>
<p>18 0.02923944 <a title="100-tfidf-18" href="./jmlr-2012-On_the_Convergence_Rate_oflp-Norm_Multiple_Kernel_Learning.html">81 jmlr-2012-On the Convergence Rate oflp-Norm Multiple Kernel Learning</a></p>
<p>19 0.028438071 <a title="100-tfidf-19" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>20 0.028220262 <a title="100-tfidf-20" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.134), (1, 0.052), (2, 0.04), (3, 0.133), (4, 0.096), (5, -0.031), (6, -0.121), (7, -0.016), (8, -0.001), (9, 0.005), (10, 0.066), (11, -0.099), (12, -0.074), (13, 0.038), (14, 0.088), (15, 0.2), (16, 0.011), (17, -0.132), (18, -0.018), (19, 0.051), (20, 0.112), (21, 0.093), (22, 0.019), (23, 0.024), (24, -0.019), (25, -0.066), (26, 0.071), (27, 0.064), (28, -0.093), (29, 0.233), (30, 0.142), (31, 0.019), (32, 0.048), (33, -0.144), (34, -0.122), (35, -0.028), (36, -0.205), (37, 0.14), (38, 0.149), (39, -0.24), (40, -0.068), (41, -0.066), (42, 0.018), (43, 0.101), (44, -0.048), (45, 0.102), (46, 0.03), (47, -0.039), (48, 0.213), (49, -0.086)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94867885 <a title="100-lsi-1" href="./jmlr-2012-Robust_Kernel_Density_Estimation.html">100 jmlr-2012-Robust Kernel Density Estimation</a></p>
<p>Author: JooSeuk Kim, Clayton D. Scott</p><p>Abstract: We propose a method for nonparametric density estimation that exhibits robustness to contamination of the training sample. This method achieves robustness by combining a traditional kernel density estimator (KDE) with ideas from classical M-estimation. We interpret the KDE based on a positive semi-deﬁnite kernel as a sample mean in the associated reproducing kernel Hilbert space. Since the sample mean is sensitive to outliers, we estimate it robustly via M-estimation, yielding a robust kernel density estimator (RKDE). An RKDE can be computed efﬁciently via a kernelized iteratively re-weighted least squares (IRWLS) algorithm. Necessary and sufﬁcient conditions are given for kernelized IRWLS to converge to the global minimizer of the M-estimator objective function. The robustness of the RKDE is demonstrated with a representer theorem, the inﬂuence function, and experimental results for density estimation and anomaly detection. Keywords: outlier, reproducing kernel Hilbert space, kernel trick, inﬂuence function, M-estimation</p><p>2 0.62276334 <a title="100-lsi-2" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>Author: Aharon Ben-Tal, Sahely Bhadra, Chiranjib Bhattacharyya, Arkadi Nemirovski</p><p>Abstract: In this paper we study the problem of designing SVM classiﬁers when the kernel matrix, K, is affected by uncertainty. Speciﬁcally K is modeled as a positive afﬁne combination of given positive semi deﬁnite kernels, with the coefﬁcients ranging in a norm-bounded uncertainty set. We treat the problem using the Robust Optimization methodology. This reduces the uncertain SVM problem into a deterministic conic quadratic problem which can be solved in principle by a polynomial time Interior Point (IP) algorithm. However, for large-scale classiﬁcation problems, IP methods become intractable and one has to resort to ﬁrst-order gradient type methods. The strategy we use here is to reformulate the robust counterpart of the uncertain SVM problem as a saddle point problem and employ a special gradient scheme which works directly on the convex-concave saddle function. The algorithm is a simpliﬁed version of a general scheme due to Juditski and Nemirovski (2011). It achieves an O(1/T 2 ) reduction of the initial error after T iterations. A comprehensive empirical study on both synthetic data and real-world protein structure data sets show that the proposed formulations achieve the desired robustness, and the saddle point based algorithm outperforms the IP method signiﬁcantly. Keywords: robust optimization, uncertain classiﬁcation, kernel functions</p><p>3 0.4592509 <a title="100-lsi-3" href="./jmlr-2012-Refinement_of_Operator-valued_Reproducing_Kernels.html">96 jmlr-2012-Refinement of Operator-valued Reproducing Kernels</a></p>
<p>Author: Haizhang Zhang, Yuesheng Xu, Qinghui Zhang</p><p>Abstract: This paper studies the construction of a reﬁnement kernel for a given operator-valued reproducing kernel such that the vector-valued reproducing kernel Hilbert space of the reﬁnement kernel contains that of the given kernel as a subspace. The study is motivated from the need of updating the current operator-valued reproducing kernel in multi-task learning when underﬁtting or overﬁtting occurs. Numerical simulations conﬁrm that the established reﬁnement kernel method is able to meet this need. Various characterizations are provided based on feature maps and vector-valued integral representations of operator-valued reproducing kernels. Concrete examples of reﬁning translation invariant and ﬁnite Hilbert-Schmidt operator-valued reproducing kernels are provided. Other examples include reﬁnement of Hessian of scalar-valued translation-invariant kernels and transformation kernels. Existence and properties of operator-valued reproducing kernels preserved during the reﬁnement process are also investigated. Keywords: vector-valued reproducing kernel Hilbert spaces, operator-valued reproducing kernels, reﬁnement, embedding, translation invariant kernels, Hessian of Gaussian kernels, Hilbert-Schmidt kernels, numerical experiments</p><p>4 0.44398135 <a title="100-lsi-4" href="./jmlr-2012-Stability_of_Density-Based_Clustering.html">109 jmlr-2012-Stability of Density-Based Clustering</a></p>
<p>Author: Alessandro Rinaldo, Aarti Singh, Rebecca Nugent, Larry Wasserman</p><p>Abstract: High density clusters can be characterized by the connected components of a level set L(λ) = {x : p(x) > λ} of the underlying probability density function p generating the data, at some appropriate level λ ≥ 0. The complete hierarchical clustering can be characterized by a cluster tree T = λ L(λ). In this paper, we study the behavior of a density level set estimate L(λ) and cluster tree estimate T based on a kernel density estimator with kernel bandwidth h. We deﬁne two notions of instability to measure the variability of L(λ) and T as a function of h, and investigate the theoretical properties of these instability measures. Keywords: clustering, density estimation, level sets, stability, model selection</p><p>5 0.35707939 <a title="100-lsi-5" href="./jmlr-2012-Algorithms_for_Learning_Kernels_Based_on_Centered_Alignment.html">16 jmlr-2012-Algorithms for Learning Kernels Based on Centered Alignment</a></p>
<p>Author: Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difﬁcult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classiﬁcation and regression. Our algorithms are based on the notion of centered alignment which is used as a similarity measure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efﬁcient algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization. Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classiﬁcation and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classiﬁcation and regression. Keywords: kernel methods, learning kernels, feature selection</p><p>6 0.31634969 <a title="100-lsi-6" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>7 0.27937889 <a title="100-lsi-7" href="./jmlr-2012-Security_Analysis_of_Online_Centroid_Anomaly_Detection.html">104 jmlr-2012-Security Analysis of Online Centroid Anomaly Detection</a></p>
<p>8 0.27603978 <a title="100-lsi-8" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>9 0.25824651 <a title="100-lsi-9" href="./jmlr-2012-Feature_Selection_via_Dependence_Maximization.html">44 jmlr-2012-Feature Selection via Dependence Maximization</a></p>
<p>10 0.2443704 <a title="100-lsi-10" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>11 0.24247065 <a title="100-lsi-11" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>12 0.22930939 <a title="100-lsi-12" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>13 0.22023503 <a title="100-lsi-13" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>14 0.1903397 <a title="100-lsi-14" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>15 0.171827 <a title="100-lsi-15" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>16 0.15722837 <a title="100-lsi-16" href="./jmlr-2012-Algebraic_Geometric_Comparison_of_Probability_Distributions.html">15 jmlr-2012-Algebraic Geometric Comparison of Probability Distributions</a></p>
<p>17 0.15439779 <a title="100-lsi-17" href="./jmlr-2012-Static_Prediction_Games_for_Adversarial_Learning_Problems.html">110 jmlr-2012-Static Prediction Games for Adversarial Learning Problems</a></p>
<p>18 0.14999379 <a title="100-lsi-18" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>19 0.1357238 <a title="100-lsi-19" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>20 0.13468942 <a title="100-lsi-20" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(21, 0.026), (26, 0.051), (29, 0.069), (56, 0.012), (67, 0.429), (69, 0.05), (75, 0.067), (77, 0.012), (79, 0.011), (92, 0.062), (96, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69107926 <a title="100-lda-1" href="./jmlr-2012-Robust_Kernel_Density_Estimation.html">100 jmlr-2012-Robust Kernel Density Estimation</a></p>
<p>Author: JooSeuk Kim, Clayton D. Scott</p><p>Abstract: We propose a method for nonparametric density estimation that exhibits robustness to contamination of the training sample. This method achieves robustness by combining a traditional kernel density estimator (KDE) with ideas from classical M-estimation. We interpret the KDE based on a positive semi-deﬁnite kernel as a sample mean in the associated reproducing kernel Hilbert space. Since the sample mean is sensitive to outliers, we estimate it robustly via M-estimation, yielding a robust kernel density estimator (RKDE). An RKDE can be computed efﬁciently via a kernelized iteratively re-weighted least squares (IRWLS) algorithm. Necessary and sufﬁcient conditions are given for kernelized IRWLS to converge to the global minimizer of the M-estimator objective function. The robustness of the RKDE is demonstrated with a representer theorem, the inﬂuence function, and experimental results for density estimation and anomaly detection. Keywords: outlier, reproducing kernel Hilbert space, kernel trick, inﬂuence function, M-estimation</p><p>2 0.31646609 <a title="100-lda-2" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>Author: Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, Alexander Smola</p><p>Abstract: We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distributionfree tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efﬁcient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the ﬁrst such tests. ∗. †. ‡. §. Also at Gatsby Computational Neuroscience Unit, CSML, 17 Queen Square, London WC1N 3AR, UK. This work was carried out while K.M.B. was with the Ludwig-Maximilians-Universit¨ t M¨ nchen. a u This work was carried out while M.J.R. was with the Graz University of Technology. Also at The Australian National University, Canberra, ACT 0200, Australia. c 2012 Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch¨ lkopf and Alexander Smola. o ¨ G RETTON , B ORGWARDT, R ASCH , S CH OLKOPF AND S MOLA Keywords: kernel methods, two-sample test, uniform convergence bounds, schema matching, integral probability metric, hypothesis testing</p><p>3 0.30877253 <a title="100-lda-3" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random ﬁelds (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter ﬁtting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random ﬁeld via the graphical lasso.</p><p>4 0.30669621 <a title="100-lda-4" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>Author: Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, Lin Xiao</p><p>Abstract: Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment. We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem. Keywords: distributed computing, online learning, stochastic optimization, regret bounds, convex optimization</p><p>5 0.30635533 <a title="100-lda-5" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>Author: Fei Yan, Josef Kittler, Krystian Mikolajczyk, Atif Tahir</p><p>Abstract: Sparsity-inducing multiple kernel Fisher discriminant analysis (MK-FDA) has been studied in the literature. Building on recent advances in non-sparse multiple kernel learning (MKL), we propose a non-sparse version of MK-FDA, which imposes a general ℓ p norm regularisation on the kernel weights. We formulate the associated optimisation problem as a semi-inﬁnite program (SIP), and adapt an iterative wrapper algorithm to solve it. We then discuss, in light of latest advances in MKL optimisation techniques, several reformulations and optimisation strategies that can potentially lead to signiﬁcant improvements in the efﬁciency and scalability of MK-FDA. We carry out extensive experiments on six datasets from various application areas, and compare closely the performance of ℓ p MK-FDA, ﬁxed norm MK-FDA, and several variants of SVM-based MKL (MK-SVM). Our results demonstrate that ℓ p MK-FDA improves upon sparse MK-FDA in many practical situations. The results also show that on image categorisation problems, ℓ p MK-FDA tends to outperform its SVM counterpart. Finally, we also discuss the connection between (MK-)FDA and (MK-)SVM, under the uniﬁed framework of regularised kernel machines. Keywords: multiple kernel learning, kernel ﬁsher discriminant analysis, regularised least squares, support vector machines</p><p>6 0.30392176 <a title="100-lda-6" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>7 0.30292761 <a title="100-lda-7" href="./jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</a></p>
<p>8 0.30281785 <a title="100-lda-8" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>9 0.30217123 <a title="100-lda-9" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>10 0.30001602 <a title="100-lda-10" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>11 0.2999396 <a title="100-lda-11" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>12 0.29794598 <a title="100-lda-12" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>13 0.29718548 <a title="100-lda-13" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>14 0.29675129 <a title="100-lda-14" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>15 0.29615307 <a title="100-lda-15" href="./jmlr-2012-Towards_Integrative_Causal_Analysis_of_Heterogeneous_Data_Sets_and_Studies.html">114 jmlr-2012-Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies</a></p>
<p>16 0.29594588 <a title="100-lda-16" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>17 0.29529458 <a title="100-lda-17" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>18 0.29443544 <a title="100-lda-18" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>19 0.29440472 <a title="100-lda-19" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>20 0.29359239 <a title="100-lda-20" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
