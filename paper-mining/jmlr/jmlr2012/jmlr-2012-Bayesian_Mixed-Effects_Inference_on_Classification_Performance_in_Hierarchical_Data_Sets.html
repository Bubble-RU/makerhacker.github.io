<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-21" href="#">jmlr2012-21</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</h1>
<br/><p>Source: <a title="jmlr-2012-21-pdf" href="http://jmlr.org/papers/volume13/brodersen12a/brodersen12a.pdf">pdf</a></p><p>Author: Kay H. Brodersen, Christoph Mathys, Justin R. Chumbley, Jean Daunizeau, Cheng Soon Ong, Joachim M. Buhmann, Klaas E. Stephan</p><p>Abstract: Classiﬁcation algorithms are frequently used on data with a natural hierarchical structure. For instance, classiﬁers are often trained and tested on trial-wise measurements, separately for each subject within a group. One important question is how classiﬁcation outcomes observed in individual subjects can be generalized to the population from which the group was sampled. To address this question, this paper introduces novel statistical models that are guided by three desiderata. First, all models explicitly respect the hierarchical nature of the data, that is, they are mixed-effects models that simultaneously account for within-subjects (ﬁxed-effects) and across-subjects (random-effects) variance components. Second, maximum-likelihood estimation is replaced by full Bayesian inference in order to enable natural regularization of the estimation problem and to afford conclusions in terms of posterior probability statements. Third, inference on classiﬁcation accuracy is complemented by inference on the balanced accuracy, which avoids inﬂated accuracy estimates for imbalanced data sets. We introduce hierarchical models that satisfy these criteria and demonstrate their advantages over conventional methods using MCMC implementations for model inversion and model selection on both synthetic and empirical data. We envisage that our approach will improve the sensitivity and validity of statistical inference in future hierarchical classiﬁcation studies. Keywords: beta-binomial, normal-binomial, balanced accuracy, Bayesian inference, group studies</p><p>Reference: <a title="jmlr-2012-21-reference" href="../jmlr2012_reference/jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 One important question is how classiﬁcation outcomes observed in individual subjects can be generalized to the population from which the group was sampled. [sent-39, score-0.872]
</p><p>2 Third, inference on classiﬁcation accuracy is complemented by inference on the balanced accuracy, which avoids inﬂated accuracy estimates for imbalanced data sets. [sent-43, score-0.709]
</p><p>3 The typical question of interest for studies as those described above is: What is the accuracy of the classiﬁer in the general population from which the subjects were sampled? [sent-55, score-0.822]
</p><p>4 Rather than treating classiﬁcation outcomes obtained in different subjects as samples from the same distribution, a hierarchical setting requires us to account for the fact that each subject itself has been sampled from a heterogeneous population (Beckmann et al. [sent-61, score-0.986]
</p><p>5 , betweensubjects variability) that results from the distribution of true accuracies in the population from which 3134  M IXED -E FFECTS I NFERENCE ON C LASSIFICATION P ERFORMANCE  subjects were drawn. [sent-68, score-0.962]
</p><p>6 This is addressed by inference on the mean classiﬁcation accuracy in the population from which subjects were drawn. [sent-123, score-1.005]
</p><p>7 In particular, we wish to predict how well a trial-wise classiﬁer will perform ‘out of sample’, that is, on trials from an unseen subject drawn from the same population as the one underlying the presently studied group. [sent-128, score-0.739]
</p><p>8 This is achieved by modelling subject-wise accuracies as drawn from a population distribution described by a Beta density, p(π j | α, β) = Beta(π j | α, β) =  Γ(α + β) α−1 π (1 − π j )β−1 , Γ(α)Γ(β) j  (5)  such that α and β characterize the population as a whole. [sent-197, score-1.201]
</p><p>9 Formally, a particular subject’s π j is drawn from a population characterized by α and β: subject-speciﬁc accuracies are assumed to be i. [sent-200, score-0.724]
</p><p>10 To describe our uncertainty about the population parameters, we use a diffuse prior on α and β which ensures that the posterior will be dominated by the data. [sent-204, score-0.864]
</p><p>11 One option would be to assign uniform densities to both the prior expected accuracy α/(α + β) and the prior virtual sample size α + β, using logistic and logarithmic transformations to put each on a (−∞, ∞) scale; but this prior would lead to an improper posterior density (Gelman et al. [sent-205, score-0.667]
</p><p>12 2 M ODEL I NVERSION Inverting the beta-binomial model allows us to infer on (i) the posterior population mean accuracy, (ii) the subject-speciﬁc posterior accuracies, and (iii) the posterior predictive accuracy. [sent-219, score-1.535]
</p><p>13 First, to obtain the posterior density over the population parameters α and β we need to evaluate p(k1:m | α, β) p(α, β) p(k1:m | α, β) p(α, β) dα dβ  p(α, β | k1:m ) =  (8)  with k1:m := (k1 , k2 , . [sent-222, score-0.835]
</p><p>14 This set allows us to obtain samples from the posterior population mean accuracy, p  α k1:m . [sent-236, score-0.85]
</p><p>15 α+β  We can use these samples in various ways, for example, to obtain a point estimate of the population mean accuracy using the posterior mean, ˆ 1 c α(τ) ∑ α(τ) + β(τ) . [sent-237, score-0.957]
</p><p>16 ˆ c τ=1 ˆ We could also numerically evaluate the posterior probability that the mean classiﬁcation accuracy in the population does not exceed chance, p = Pr  α ≤ 0. [sent-238, score-0.93]
</p><p>17 Finally, we could compute the posterior probability that the mean accuracy in one population is greater than in another, p = Pr  α(2) α(1) k (1) , k1:m(2) . [sent-243, score-0.93]
</p><p>18 Subjects with fewer trials will exert a smaller effect on the group and shrink more, while subjects with more trials will have a larger inﬂuence on the group and shrink less. [sent-253, score-0.716]
</p><p>19 In this case, we are typically less interested in the average effect in the group but more in the effect that a new subject from the same population would display, as this estimate takes into account both the population mean and the population variance. [sent-256, score-1.619]
</p><p>20 The expected performance is expressed by the posterior predictive density, ˜ p(π | k1:m ), ˜ in which π denotes the classiﬁcation accuracy in a new subject drawn from the same population as the existing group of subjects with latent accuracies π1 , . [sent-257, score-1.57]
</p><p>21 4 Samples for this density can easily be obtained using the samples α(τ) and β(τ) from the posterior population mean. [sent-262, score-0.862]
</p><p>22 In the same way, we can obtain approximations to the posterior mean, the posterior mode, or a posterior probability interval. [sent-313, score-0.921]
</p><p>23 For instance, we can obtain − + the posterior population parameters, p(α+ , β+ | k1:m ) and p(α− , β− | k1:m ) using the same sampling procedure as summarized in Section 2. [sent-323, score-0.784]
</p><p>24 The two sets of samples can then be averaged in a pairwise fashion to obtain samples from the posterior mean balanced accuracy in the population, − + p φ | k1:m , k1:m ,  3142  M IXED -E FFECTS I NFERENCE ON C LASSIFICATION P ERFORMANCE  where we have deﬁned φ :=  1 2  α− α+ + − α+ + β+ α + β−  . [sent-325, score-0.662]
</p><p>25 Similarly, we can average pairs of posterior samples from π+ and π− to obtain samples from the j j posterior densities of subject-speciﬁc balanced accuracies, − + p φ j k1:m , k1:m . [sent-326, score-0.852]
</p><p>26 Using the same idea, we can obtain samples from the posterior predictive density of the balanced accuracy that can be expected in a new subject from the same population, ˜ + − p φ k1:m , k1:m . [sent-327, score-0.778]
</p><p>27 In this case, an unbiased classiﬁer yields high accuracies on either class in some subjects and lower accuracies in others, inducing a positive correlation between class-speciﬁc accuracies. [sent-334, score-0.732]
</p><p>28 We therefore turn to an alternative model for mixed-effects inference on the balanced accuracy that embraces potential dependencies between class-speciﬁc accuracies (Figure 2b). [sent-339, score-0.68]
</p><p>29 Instead, we use a bivariate population density whose covariance structure deﬁnes the form and extent of the dependency between π+ and π− . [sent-343, score-0.661]
</p><p>30 2 M ODEL I NVERSION In contrast to the twofold beta-binomial model discussed in the previous section, the bivariate normal-binomial model makes it difﬁcult to sample from the posterior densities over model pa− + rameters using a Metropolis implementation. [sent-381, score-0.674]
</p><p>31 First, population parameter estimates can be obtained by sampling from the posterior density − + p(µ, Σ | k1:m , k1:m ) using a Metropolis-Hastings approach. [sent-389, score-0.835]
</p><p>32 Second, subject-speciﬁc accuracies are − + estimated by ﬁrst sampling from p(ρ j | k1:m , k1:m ) and then applying a sigmoid transform to obtain − + samples from the posterior density over subject-speciﬁc balanced accuracies, p(φ j | k1:m , k1:m ). [sent-390, score-0.787]
</p><p>33 The best model can then be used for posterior inferences on the mean accuracy in the population or the predictive accuracy in a new subject from the new population. [sent-409, score-1.259]
</p><p>34 M  Similarly, we can obtain the posterior predictive distribution of the balanced accuracy in a new subject from the same population, − + ˜ + − ˜ + − p φ k1:m , k1:m = ∑ p φ k1:m , k1:m , M p M k1:m , k1:m . [sent-414, score-0.7]
</p><p>35 We then contrast inference on accuracies with inference on balanced accuracies (Section 3. [sent-436, score-0.937]
</p><p>36 Their empirical sample accuracies are shown in Figure 4b, along with the ground-truth density of the population accuracy. [sent-452, score-0.801]
</p><p>37 1 (Figure 4c), and examining the posterior distribution over the population mean accuracy showed that more than 99. [sent-454, score-0.93]
</p><p>38 This is in contrast to the dispersion of the posterior over the population mean, which becomes more and more precise with an increasing amount of data. [sent-464, score-0.822]
</p><p>39 8 FFX RFX posterior confidence interval interval  4  (f) predictive inference  8 MFX posterior interval  2 log(a/b)  10  p(>0. [sent-487, score-0.959]
</p><p>40 5 predictive accuracy  1  Figure 4: Inference on the population mean and the predictive accuracy. [sent-494, score-0.713]
</p><p>41 (b) Empirical sample accuracies (blue) and their underlying population distribution (green). [sent-497, score-0.75]
</p><p>42 (c) Inverting the beta-binomial model yields samples from the posterior distribution over the population parameters, visualized using a nonparametric (bivariate Gaussian kernel) density estimate (contour lines). [sent-498, score-0.889]
</p><p>43 (d) The posterior about the population mean accuracy, plotted using a kernel density estimator (black), is sharply peaked around the true population mean (green). [sent-499, score-1.422]
</p><p>44 (f) The posterior predictive distribution over ˜ π represents the posterior belief of the accuracy expected in a new subject (black). [sent-505, score-0.852]
</p><p>45 75 1 true population mean  Figure 5: Inference on the population mean under varying population heterogeneity. [sent-534, score-1.509]
</p><p>46 The ﬁgure shows Bayesian estimates of the frequentist probability of above-chance classiﬁcation performance, as a function of the true population mean, separately for three different level of population heterogeneity (a,b,c). [sent-535, score-1.043]
</p><p>47 By contrast, a ﬁxed-effects approach (orange) offers invalid population inference as it disregards between-subjects variability; at a true population mean of 0. [sent-538, score-1.164]
</p><p>48 Insets show the distribution of the true underlying population accuracy (green) for a population mean accuracy of 0. [sent-546, score-1.207]
</p><p>49 We then varied the true population mean and plotted the fraction of decisions for an above-chance classiﬁer as a function of population mean (Figure 5a). [sent-549, score-1.032]
</p><p>50 Since the population variance was chosen to be very low in this initial simulation, the inferences afforded by a ﬁxed-effects analysis (yellow) prove very similar as well; but this changes drastically when increasing the population variance to more realistic levels, as described below. [sent-556, score-1.078]
</p><p>51 5 1 population mean accuracy  beta- conven- convenbinomial tional tional model FFX RFX interval interval  0  Figure 6: Inadequate inferences provided by ﬁxed-effects and random-effects models. [sent-567, score-0.818]
</p><p>52 (b) The (mixed-effects) posterior density of the population mean (black) provides a good estimate of ground truth (green). [sent-570, score-0.94]
</p><p>53 example, given a fairly homogeneous population with a true population mean accuracy of 60% and a variance of 0. [sent-574, score-1.13]
</p><p>54 The above simulations show that a ﬁxed-effects analysis (yellow) becomes an invalid procedure to infer on the population mean when the population variance is non-negligible. [sent-581, score-1.049]
</p><p>55 Classiﬁcation outcomes were generated using the betabinomial model with a population mean of 0. [sent-590, score-0.662]
</p><p>56 The example shows that the proposed beta-binomial model yields a posterior density with the necessary asymmetry; it comfortably includes the true population mean (Figure 6b). [sent-594, score-0.901]
</p><p>57 This simulation was based on 45 subjects overall; 40 subjects were characterized by a relatively moderate number of trials (n = 20) while 5 subjects had even fewer trials (n = 5). [sent-605, score-1.066]
</p><p>58 The plot shows that, in each subject, the posterior mode (black) represents a compromise between the observed sample accuracy (blue) and the population mean (0. [sent-611, score-0.956]
</p><p>59 Another way of demonstrating the shrinkage effect is by illustrating the transition from ground truth to sample accuracies (with its increase in dispersion) and from sample accuracies to posterior means (with its decrease in dispersion). [sent-615, score-0.963]
</p><p>60 This shows how the high variability in sample accuracies is reduced, informed by what has been learned about the population (Figure 7b). [sent-616, score-0.75]
</p><p>61 Here, subjects with only 5 trials were shrunk more than subjects with 20 trials. [sent-618, score-0.652]
</p><p>62 4 ground truth sample accuracies posterior interval (bb)  20 40 subjects (sorted)  0. [sent-638, score-0.936]
</p><p>63 2 0  subjects with very few trials  (c) power curves  ground sample posterior truth accuracies mean (bb)  1 0. [sent-639, score-1.099]
</p><p>64 (a) Classiﬁcation outcomes were generated for a synthetic heterogeneous group of 45 subjects (40 subjects with 20 trials each, 5 subjects with 5 trials each). [sent-650, score-1.266]
</p><p>65 (b) Another way of visualizing the shrinking effect is to contrast the increase in dispersion (as we move from ground truth to sample accuracies) with the decrease in dispersion (as we move from sample accuracies to posterior means) enforced by the hierarchical model. [sent-656, score-0.812]
</p><p>66 Shrinking changes the order of subjects (when sorted by posterior mean as opposed to by sample accuracy) as the amount of shrinking depends on the subject-speciﬁc (ﬁrst-level) posterior uncertainty. [sent-657, score-0.917]
</p><p>67 (d) Across the same 1 000 simulations, a Bayes estimator, based on the posterior means of subject-speciﬁc accuracies (black), was superior to both a classical ML estimator (blue) and a James-Stein estimator (red). [sent-661, score-0.664]
</p><p>68 An initial simulation speciﬁed a high population accuracy on the positive class and a low accuracy on the negative class, with equal variance in both (Figure 8a,b). [sent-675, score-0.721]
</p><p>69 2  60 40  +ve trials  20 0  TPR  correct predictions  (c) population-mean intervals 1  1  –ve trials  5  10 15 subjects  20  0 0  0. [sent-708, score-0.637]
</p><p>70 5 TNR  1  0  accuracy (bb)  balanced accuracy (nb)  balanced accuracy (bb) ln ! [sent-709, score-0.69]
</p><p>71 75 1 population mean on positive trials  Figure 8: Inference on the balanced accuracy. [sent-718, score-0.847]
</p><p>72 The underlying true population distribution is represented by a bivariate Gaussian kernel density estimate (contour lines). [sent-722, score-0.661]
</p><p>73 The plot shows that the population accuracy is high on positive trials and low on negative trials. [sent-723, score-0.76]
</p><p>74 (c) Central 95% posterior probability intervals based on three models: the simple beta-binomial model for inference on the population accuracy; and the twofold beta-binomial model as well as the bivariate normal-binomial model for inference on the balanced accuracy. [sent-724, score-1.586]
</p><p>75 The true mean balanced accuracy in the population is at chance (green). [sent-725, score-0.841]
</p><p>76 To examine this dependence, we carried out a sensitivity analysis in which we considered the infraliminal probability of the posterior population mean as a function of prior moments (Figure 9). [sent-738, score-0.97]
</p><p>77 We found that inferences were extremely robust, in the sense that the inﬂuence of the prior moments on the resulting posterior densities was negligible in relation to the variance resulting from the fact that we are using a (stochastic) approximate inference method for model inversion. [sent-739, score-0.65]
</p><p>78 Similarly, varying µ0 , κ0 , or ν0 in the normal-binomial model had practically no inﬂuence on the infraliminal probability of the posterior balanced accuracy (Figure 9c,d,e). [sent-741, score-0.66]
</p><p>79 Each graph shows the infraliminal probability of the population mean accuracy (i. [sent-769, score-0.687]
</p><p>80 Inferences on the population balanced accuracy are based on the bivariate normal-binomial model. [sent-777, score-0.872]
</p><p>81 To illustrate the generic applicability of our approach when its assumptions are not satisﬁed by construction, we applied models for mixed-effects inference to classiﬁcation outcomes obtained on synthetic data features for a group of 20 subjects with 100 trials each (Figure 10). [sent-781, score-0.786]
</p><p>82 The underlying population distribution is represented by a bivariate Gaussian kernel density estimate (contour lines). [sent-825, score-0.661]
</p><p>83 tribution was symmetric with regard to class-speciﬁc accuracies while these accuracies themselves were strongly positively correlated, as one would expect from a linear classiﬁer tested on perfectly balanced data sets. [sent-833, score-0.649]
</p><p>84 Central 95% posterior probability intervals about the population mean are shown in Figure 10c, along with a frequentist conﬁdence interval of the population mean accuracy. [sent-838, score-1.527]
</p><p>85 In stark contrast, using the single betabinomial model or a conventional mean of sample accuracies to infer on the population accuracy (as opposed to balanced accuracy) resulted in estimates that were overly optimistic and therefore misleading. [sent-861, score-1.129]
</p><p>86 Inverting the former model, which captures potential dependencies between class-speciﬁc accuracies, yields a posterior distribution over the population mean balanced accuracy (black) which shows that the classiﬁer is performing above chance. [sent-884, score-1.085]
</p><p>87 The plot contrasts sample accuracies (blue) with central 95% posterior probability intervals (black), which avoid overﬁtting by shrinking to the population mean. [sent-886, score-1.104]
</p><p>88 Using the beta-binomial model for inference on the population mean balanced accuracy, we obtained very strong evidence (infraliminal probability p < 0. [sent-902, score-0.88]
</p><p>89 The shrinkage effect in these subject-speciﬁc accuracies was rather small: the average absolute difference between sample accuracies and posterior means amounted to 1. [sent-905, score-0.871]
</p><p>90 Speciﬁcally, the posterior distribution of the accuracy of one subject is partially inﬂuenced by the data from all other subjects, correctly weighted by their respective posterior precisions (see Section 3. [sent-923, score-0.833]
</p><p>91 In this way, one would explicitly model task- or session-speciﬁc accuracies to be conditionally independent from one another given an overall subject-speciﬁc effect π j , and conditionally independent from other subjects given the population parameters. [sent-954, score-0.989]
</p><p>92 4 we showed how alternative a priori as3161  B RODERSEN , M ATHYS , C HUMBLEY, DAUNIZEAU , O NG , B UHMANN AND S TEPHAN  sumptions about the population covariance of class-speciﬁc accuracies can be evaluated, relative to the priors of the models, using Bayesian model selection. [sent-969, score-0.784]
</p><p>93 (2012), who carry out inference on the population mean accuracy by comparing two beta-binomial models: one with a population mean prior at 0. [sent-1037, score-1.332]
</p><p>94 In order to assess whether the mean classiﬁcation performance achieved in the population is above chance, we must evaluate our posterior knowledge about the population parameters α and β. [sent-1074, score-1.3]
</p><p>95 k1:m ≈ ∑ (τ) α+β c τ=1 α + β(τ)  Another informative measure is the posterior probability that the mean classiﬁcation accuracy in the population does not exceed chance, p = Pr  α ≤ 0. [sent-1077, score-0.93]
</p><p>96 10 In order to derive an expression for the posterior predictive distribution in closed form, one would need to integrate out the population parameters α and β, ˜ p(π | k1:m ) =  ˜ p(π | α, β) p(α, β | k1:m ) dα dβ,  which is analytically intractable. [sent-1095, score-0.829]
</p><p>97 m  We then complete the ﬁrst step by drawing µ(τ) ∼ N 2 µ(τ) µm , Σ(τ) /κm , which we can use to obtain samples from the posterior mean balanced accuracy using φ(τ) :=  1 (τ) (τ) µ + µ2 . [sent-1123, score-0.635]
</p><p>98 2 j,1 3168  M IXED -E FFECTS I NFERENCE ON C LASSIFICATION P ERFORMANCE  Apart from using µ(τ) and Σ(τ) to obtain samples from the posterior distributions over ρ j , we can ˜ 1:m − further use the two vectors to draw samples from the posterior predictive distribution p(π+ , k1:m ). [sent-1138, score-0.713]
</p><p>99 For this we ﬁrst draw ˜ ˜ ρ(τ) ∼ N 2 ρ(τ) µ(τ) , Σ(τ) , and then obtain the desired sample using ˜ ˜ π(τ) = σ ρ(τ) , from which we can obtain samples from the posterior predictive balanced accuracy using 1 (τ) ˜ ˜ ˜ (τ) π + π2 . [sent-1139, score-0.667]
</p><p>100 Additionally, it is common practice to indicate the uncertainty about the population mean of the classiﬁcation accuracy by reporting the 95% conﬁdence interval ˆ σm−1 ¯ π ± t0. [sent-1160, score-0.706]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('population', 0.477), ('posterior', 0.307), ('accuracies', 0.247), ('subjects', 0.238), ('trials', 0.176), ('daunizeau', 0.166), ('balanced', 0.155), ('inference', 0.144), ('athys', 0.14), ('humbley', 0.14), ('rodersen', 0.14), ('tephan', 0.14), ('ffects', 0.134), ('bivariate', 0.133), ('uhmann', 0.12), ('zurich', 0.118), ('ixed', 0.114), ('accuracy', 0.107), ('bayesian', 0.099), ('twofold', 0.098), ('beta', 0.097), ('nference', 0.095), ('outcomes', 0.094), ('doi', 0.091), ('frequentist', 0.089), ('subject', 0.086), ('erformance', 0.084), ('issn', 0.078), ('bb', 0.074), ('classi', 0.07), ('bin', 0.068), ('hierarchical', 0.064), ('lassification', 0.064), ('inferences', 0.064), ('infraliminal', 0.064), ('group', 0.063), ('chance', 0.063), ('bayes', 0.062), ('metropolis', 0.06), ('ln', 0.059), ('ng', 0.059), ('brodersen', 0.054), ('gelman', 0.054), ('favour', 0.054), ('imbalanced', 0.052), ('interval', 0.052), ('density', 0.051), ('prior', 0.049), ('fmri', 0.049), ('intervals', 0.047), ('classical', 0.046), ('predictive', 0.045), ('kass', 0.045), ('shrinkage', 0.044), ('stephan', 0.044), ('raftery', 0.044), ('binomial', 0.043), ('synthetic', 0.043), ('mean', 0.039), ('evidence', 0.038), ('ffx', 0.038), ('neuroimaging', 0.038), ('rfx', 0.038), ('tnr', 0.038), ('dispersion', 0.038), ('er', 0.034), ('truth', 0.034), ('sensitivity', 0.034), ('priors', 0.033), ('madigan', 0.033), ('neuroimage', 0.033), ('ground', 0.032), ('estimator', 0.032), ('bluemlisalpstrasse', 0.032), ('mbb', 0.032), ('mnb', 0.032), ('sns', 0.032), ('uncertainty', 0.031), ('variance', 0.03), ('tpr', 0.029), ('densities', 0.029), ('black', 0.028), ('models', 0.028), ('model', 0.027), ('disregards', 0.027), ('falsely', 0.027), ('mfx', 0.027), ('samples', 0.027), ('switzerland', 0.026), ('brain', 0.026), ('red', 0.026), ('infer', 0.026), ('correctly', 0.026), ('sample', 0.026), ('akbani', 0.025), ('betabinomial', 0.025), ('chawla', 0.025), ('friston', 0.025), ('gustafsson', 0.025), ('overdispersed', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999875 <a title="21-tfidf-1" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>Author: Kay H. Brodersen, Christoph Mathys, Justin R. Chumbley, Jean Daunizeau, Cheng Soon Ong, Joachim M. Buhmann, Klaas E. Stephan</p><p>Abstract: Classiﬁcation algorithms are frequently used on data with a natural hierarchical structure. For instance, classiﬁers are often trained and tested on trial-wise measurements, separately for each subject within a group. One important question is how classiﬁcation outcomes observed in individual subjects can be generalized to the population from which the group was sampled. To address this question, this paper introduces novel statistical models that are guided by three desiderata. First, all models explicitly respect the hierarchical nature of the data, that is, they are mixed-effects models that simultaneously account for within-subjects (ﬁxed-effects) and across-subjects (random-effects) variance components. Second, maximum-likelihood estimation is replaced by full Bayesian inference in order to enable natural regularization of the estimation problem and to afford conclusions in terms of posterior probability statements. Third, inference on classiﬁcation accuracy is complemented by inference on the balanced accuracy, which avoids inﬂated accuracy estimates for imbalanced data sets. We introduce hierarchical models that satisfy these criteria and demonstrate their advantages over conventional methods using MCMC implementations for model inversion and model selection on both synthetic and empirical data. We envisage that our approach will improve the sensitivity and validity of statistical inference in future hierarchical classiﬁcation studies. Keywords: beta-binomial, normal-binomial, balanced accuracy, Bayesian inference, group studies</p><p>2 0.11824343 <a title="21-tfidf-2" href="./jmlr-2012-Random_Search_for_Hyper-Parameter_Optimization.html">95 jmlr-2012-Random Search for Hyper-Parameter Optimization</a></p>
<p>Author: James Bergstra, Yoshua Bengio</p><p>Abstract: Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efÄ?Ĺš cient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to conÄ?Ĺš gure neural networks and deep belief networks. Compared with neural networks conÄ?Ĺš gured by a pure grid search, we Ä?Ĺš nd that random search over the same domain is able to Ä?Ĺš nd models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search Ä?Ĺš nds better models by effectively searching a larger, less promising conÄ?Ĺš guration space. Compared with deep belief networks conÄ?Ĺš gured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional conÄ?Ĺš guration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for conÄ?Ĺš guring algorithms for new data sets. Our analysis casts some light on why recent Ă˘&euro;&oelig;High ThroughputĂ˘&euro;? methods achieve surprising successĂ˘&euro;&rdquo;they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms. Keyword</p><p>3 0.093944304 <a title="21-tfidf-3" href="./jmlr-2012-PAC-Bayes_Bounds_with_Data_Dependent_Priors.html">87 jmlr-2012-PAC-Bayes Bounds with Data Dependent Priors</a></p>
<p>Author: Emilio Parrado-Hernández, Amiran Ambroladze, John Shawe-Taylor, Shiliang Sun</p><p>Abstract: This paper presents the prior PAC-Bayes bound and explores its capabilities as a tool to provide tight predictions of SVMs’ generalization. The computation of the bound involves estimating a prior of the distribution of classiﬁers from the available data, and then manipulating this prior in the usual PAC-Bayes generalization bound. We explore two alternatives: to learn the prior from a separate data set, or to consider an expectation prior that does not need this separate data set. The prior PAC-Bayes bound motivates two SVM-like classiﬁcation algorithms, prior SVM and ηprior SVM, whose regularization term pushes towards the minimization of the prior PAC-Bayes bound. The experimental work illustrates that the new bounds can be signiﬁcantly tighter than the original PAC-Bayes bound when applied to SVMs, and among them the combination of the prior PAC-Bayes bound and the prior SVM algorithm gives the tightest bound. Keywords: PAC-Bayes bound, support vector machine, generalization capability prediction, classiﬁcation</p><p>4 0.085465744 <a title="21-tfidf-4" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>Author: Hannes Nickisch</p><p>Abstract: The glm-ie toolbox contains functionality for estimation and inference in generalised linear models over continuous-valued variables. Besides a variety of penalised least squares solvers for estimation, it offers inference based on (convex) variational bounds, on expectation propagation and on factorial mean ﬁeld. Scalable and efﬁcient inference in fully-connected undirected graphical models or Markov random ﬁelds with Gaussian and non-Gaussian potentials is achieved by casting all the computations as matrix vector multiplications. We provide a wide choice of penalty functions for estimation, potential functions for inference and matrix classes with lazy evaluation for convenient modelling. We designed the glm-ie package to be simple, generic and easily expansible. Most of the code is written in Matlab including some MEX ﬁles to be fully compatible to both Matlab 7.x and GNU Octave 3.3.x. Large scale probabilistic classiﬁcation as well as sparse linear modelling can be performed in a common algorithmical framework by the glm-ie toolkit. Keywords: sparse linear models, generalised linear models, Bayesian inference, approximate inference, probabilistic regression and classiﬁcation, penalised least squares estimation, lazy evaluation matrix class</p><p>5 0.064528838 <a title="21-tfidf-5" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>Author: David P. Helmbold, Philip M. Long</p><p>Abstract: This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classiﬁers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high conﬁdence whether or not any individual variable is relevant. Keywords: feature selection, generalization, learning theory</p><p>6 0.064161249 <a title="21-tfidf-6" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>7 0.061556138 <a title="21-tfidf-7" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>8 0.052465536 <a title="21-tfidf-8" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>9 0.050612941 <a title="21-tfidf-9" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>10 0.04939758 <a title="21-tfidf-10" href="./jmlr-2012-Towards_Integrative_Causal_Analysis_of_Heterogeneous_Data_Sets_and_Studies.html">114 jmlr-2012-Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies</a></p>
<p>11 0.048978709 <a title="21-tfidf-11" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<p>12 0.047621015 <a title="21-tfidf-12" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>13 0.047010776 <a title="21-tfidf-13" href="./jmlr-2012-Optimistic_Bayesian_Sampling_in_Contextual-Bandit_Problems.html">86 jmlr-2012-Optimistic Bayesian Sampling in Contextual-Bandit Problems</a></p>
<p>14 0.045475379 <a title="21-tfidf-14" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>15 0.044329558 <a title="21-tfidf-15" href="./jmlr-2012-EP-GIG_Priors_and_Applications_in_Bayesian_Sparse_Learning.html">35 jmlr-2012-EP-GIG Priors and Applications in Bayesian Sparse Learning</a></p>
<p>16 0.042264633 <a title="21-tfidf-16" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>17 0.042200316 <a title="21-tfidf-17" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>18 0.041035861 <a title="21-tfidf-18" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>19 0.040860213 <a title="21-tfidf-19" href="./jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems.html">89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</a></p>
<p>20 0.039848045 <a title="21-tfidf-20" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.181), (1, 0.083), (2, 0.13), (3, -0.056), (4, 0.117), (5, -0.014), (6, 0.104), (7, 0.164), (8, -0.113), (9, 0.095), (10, 0.026), (11, -0.05), (12, 0.099), (13, 0.096), (14, 0.07), (15, 0.01), (16, 0.17), (17, 0.185), (18, 0.125), (19, -0.077), (20, 0.083), (21, 0.091), (22, 0.017), (23, 0.121), (24, -0.161), (25, -0.088), (26, -0.171), (27, 0.06), (28, 0.043), (29, -0.087), (30, -0.065), (31, 0.014), (32, 0.058), (33, -0.093), (34, -0.087), (35, -0.115), (36, -0.185), (37, 0.076), (38, -0.054), (39, 0.016), (40, -0.03), (41, 0.177), (42, -0.013), (43, -0.058), (44, -0.074), (45, 0.03), (46, 0.078), (47, 0.02), (48, -0.042), (49, -0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9660092 <a title="21-lsi-1" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>Author: Kay H. Brodersen, Christoph Mathys, Justin R. Chumbley, Jean Daunizeau, Cheng Soon Ong, Joachim M. Buhmann, Klaas E. Stephan</p><p>Abstract: Classiﬁcation algorithms are frequently used on data with a natural hierarchical structure. For instance, classiﬁers are often trained and tested on trial-wise measurements, separately for each subject within a group. One important question is how classiﬁcation outcomes observed in individual subjects can be generalized to the population from which the group was sampled. To address this question, this paper introduces novel statistical models that are guided by three desiderata. First, all models explicitly respect the hierarchical nature of the data, that is, they are mixed-effects models that simultaneously account for within-subjects (ﬁxed-effects) and across-subjects (random-effects) variance components. Second, maximum-likelihood estimation is replaced by full Bayesian inference in order to enable natural regularization of the estimation problem and to afford conclusions in terms of posterior probability statements. Third, inference on classiﬁcation accuracy is complemented by inference on the balanced accuracy, which avoids inﬂated accuracy estimates for imbalanced data sets. We introduce hierarchical models that satisfy these criteria and demonstrate their advantages over conventional methods using MCMC implementations for model inversion and model selection on both synthetic and empirical data. We envisage that our approach will improve the sensitivity and validity of statistical inference in future hierarchical classiﬁcation studies. Keywords: beta-binomial, normal-binomial, balanced accuracy, Bayesian inference, group studies</p><p>2 0.57495844 <a title="21-lsi-2" href="./jmlr-2012-PAC-Bayes_Bounds_with_Data_Dependent_Priors.html">87 jmlr-2012-PAC-Bayes Bounds with Data Dependent Priors</a></p>
<p>Author: Emilio Parrado-Hernández, Amiran Ambroladze, John Shawe-Taylor, Shiliang Sun</p><p>Abstract: This paper presents the prior PAC-Bayes bound and explores its capabilities as a tool to provide tight predictions of SVMs’ generalization. The computation of the bound involves estimating a prior of the distribution of classiﬁers from the available data, and then manipulating this prior in the usual PAC-Bayes generalization bound. We explore two alternatives: to learn the prior from a separate data set, or to consider an expectation prior that does not need this separate data set. The prior PAC-Bayes bound motivates two SVM-like classiﬁcation algorithms, prior SVM and ηprior SVM, whose regularization term pushes towards the minimization of the prior PAC-Bayes bound. The experimental work illustrates that the new bounds can be signiﬁcantly tighter than the original PAC-Bayes bound when applied to SVMs, and among them the combination of the prior PAC-Bayes bound and the prior SVM algorithm gives the tightest bound. Keywords: PAC-Bayes bound, support vector machine, generalization capability prediction, classiﬁcation</p><p>3 0.56777579 <a title="21-lsi-3" href="./jmlr-2012-Random_Search_for_Hyper-Parameter_Optimization.html">95 jmlr-2012-Random Search for Hyper-Parameter Optimization</a></p>
<p>Author: James Bergstra, Yoshua Bengio</p><p>Abstract: Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efÄ?Ĺš cient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to conÄ?Ĺš gure neural networks and deep belief networks. Compared with neural networks conÄ?Ĺš gured by a pure grid search, we Ä?Ĺš nd that random search over the same domain is able to Ä?Ĺš nd models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search Ä?Ĺš nds better models by effectively searching a larger, less promising conÄ?Ĺš guration space. Compared with deep belief networks conÄ?Ĺš gured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional conÄ?Ĺš guration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for conÄ?Ĺš guring algorithms for new data sets. Our analysis casts some light on why recent Ă˘&euro;&oelig;High ThroughputĂ˘&euro;? methods achieve surprising successĂ˘&euro;&rdquo;they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms. Keyword</p><p>4 0.50183195 <a title="21-lsi-4" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>Author: Hannes Nickisch</p><p>Abstract: The glm-ie toolbox contains functionality for estimation and inference in generalised linear models over continuous-valued variables. Besides a variety of penalised least squares solvers for estimation, it offers inference based on (convex) variational bounds, on expectation propagation and on factorial mean ﬁeld. Scalable and efﬁcient inference in fully-connected undirected graphical models or Markov random ﬁelds with Gaussian and non-Gaussian potentials is achieved by casting all the computations as matrix vector multiplications. We provide a wide choice of penalty functions for estimation, potential functions for inference and matrix classes with lazy evaluation for convenient modelling. We designed the glm-ie package to be simple, generic and easily expansible. Most of the code is written in Matlab including some MEX ﬁles to be fully compatible to both Matlab 7.x and GNU Octave 3.3.x. Large scale probabilistic classiﬁcation as well as sparse linear modelling can be performed in a common algorithmical framework by the glm-ie toolkit. Keywords: sparse linear models, generalised linear models, Bayesian inference, approximate inference, probabilistic regression and classiﬁcation, penalised least squares estimation, lazy evaluation matrix class</p><p>5 0.39396575 <a title="21-lsi-5" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>Author: Philipp Hennig, Christian J. Schuler</p><p>Abstract: Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly addresses the decision problem of maximizing information gain from each evaluation. Keywords: optimization, probability, information, Gaussian processes, expectation propagation</p><p>6 0.37887076 <a title="21-lsi-6" href="./jmlr-2012-Towards_Integrative_Causal_Analysis_of_Heterogeneous_Data_Sets_and_Studies.html">114 jmlr-2012-Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies</a></p>
<p>7 0.36236772 <a title="21-lsi-7" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<p>8 0.35976017 <a title="21-lsi-8" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>9 0.33543101 <a title="21-lsi-9" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>10 0.31045777 <a title="21-lsi-10" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>11 0.30070493 <a title="21-lsi-11" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>12 0.29836637 <a title="21-lsi-12" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>13 0.26535204 <a title="21-lsi-13" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<p>14 0.26487914 <a title="21-lsi-14" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>15 0.24981821 <a title="21-lsi-15" href="./jmlr-2012-EP-GIG_Priors_and_Applications_in_Bayesian_Sparse_Learning.html">35 jmlr-2012-EP-GIG Priors and Applications in Bayesian Sparse Learning</a></p>
<p>16 0.2476472 <a title="21-lsi-16" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>17 0.24645194 <a title="21-lsi-17" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>18 0.24094403 <a title="21-lsi-18" href="./jmlr-2012-Optimistic_Bayesian_Sampling_in_Contextual-Bandit_Problems.html">86 jmlr-2012-Optimistic Bayesian Sampling in Contextual-Bandit Problems</a></p>
<p>19 0.23085846 <a title="21-lsi-19" href="./jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems.html">89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</a></p>
<p>20 0.23044857 <a title="21-lsi-20" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.017), (21, 0.035), (23, 0.377), (26, 0.054), (29, 0.066), (35, 0.037), (49, 0.036), (56, 0.021), (57, 0.013), (69, 0.019), (75, 0.044), (77, 0.02), (79, 0.015), (81, 0.011), (92, 0.058), (96, 0.099)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74814165 <a title="21-lda-1" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>Author: Kay H. Brodersen, Christoph Mathys, Justin R. Chumbley, Jean Daunizeau, Cheng Soon Ong, Joachim M. Buhmann, Klaas E. Stephan</p><p>Abstract: Classiﬁcation algorithms are frequently used on data with a natural hierarchical structure. For instance, classiﬁers are often trained and tested on trial-wise measurements, separately for each subject within a group. One important question is how classiﬁcation outcomes observed in individual subjects can be generalized to the population from which the group was sampled. To address this question, this paper introduces novel statistical models that are guided by three desiderata. First, all models explicitly respect the hierarchical nature of the data, that is, they are mixed-effects models that simultaneously account for within-subjects (ﬁxed-effects) and across-subjects (random-effects) variance components. Second, maximum-likelihood estimation is replaced by full Bayesian inference in order to enable natural regularization of the estimation problem and to afford conclusions in terms of posterior probability statements. Third, inference on classiﬁcation accuracy is complemented by inference on the balanced accuracy, which avoids inﬂated accuracy estimates for imbalanced data sets. We introduce hierarchical models that satisfy these criteria and demonstrate their advantages over conventional methods using MCMC implementations for model inversion and model selection on both synthetic and empirical data. We envisage that our approach will improve the sensitivity and validity of statistical inference in future hierarchical classiﬁcation studies. Keywords: beta-binomial, normal-binomial, balanced accuracy, Bayesian inference, group studies</p><p>2 0.36057317 <a title="21-lda-2" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random ﬁelds (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter ﬁtting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random ﬁeld via the graphical lasso.</p><p>3 0.35957974 <a title="21-lda-3" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>Author: Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, Lin Xiao</p><p>Abstract: Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment. We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem. Keywords: distributed computing, online learning, stochastic optimization, regret bounds, convex optimization</p><p>4 0.35793114 <a title="21-lda-4" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel</p><p>Abstract: The success of many machine learning and pattern recognition methods relies heavily upon the identiﬁcation of an appropriate distance metric on the input data. It is often beneﬁcial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semideﬁnite. Semideﬁnite programming is often used to enforce this constraint, but does not scale well and is not easy to implement. B OOST M ETRIC is instead based on the observation that any positive semideﬁnite matrix can be decomposed into a linear combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting methods are easy to implement, efﬁcient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semideﬁnite matrix with trace and rank being one rather than a classiﬁer or regressor. Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. Keywords: Mahalanobis distance, semideﬁnite programming, column generation, boosting, Lagrange duality, large margin nearest neighbor</p><p>5 0.35384044 <a title="21-lda-5" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>Author: Sanjiv Kumar, Mehryar Mohri, Ameet Talwalkar</p><p>Abstract: The Nystr¨ m method is an efﬁcient technique to generate low-rank matrix approximations and is o used in several large-scale learning applications. A key aspect of this method is the procedure according to which columns are sampled from the original matrix. In this work, we explore the efﬁcacy of a variety of ﬁxed and adaptive sampling schemes. We also propose a family of ensemble-based sampling algorithms for the Nystr¨ m method. We report results of extensive experiments o that provide a detailed comparison of various ﬁxed and adaptive sampling techniques, and demonstrate the performance improvement associated with the ensemble Nystr¨ m method when used in o conjunction with either ﬁxed or adaptive sampling schemes. Corroborating these empirical ﬁndings, we present a theoretical analysis of the Nystr¨ m method, providing novel error bounds guaro anteeing a better convergence rate of the ensemble Nystr¨ m method in comparison to the standard o Nystr¨ m method. o Keywords: low-rank approximation, nystr¨ m method, ensemble methods, large-scale learning o</p><p>6 0.35364652 <a title="21-lda-6" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>7 0.35296857 <a title="21-lda-7" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>8 0.3527661 <a title="21-lda-8" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>9 0.34935701 <a title="21-lda-9" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>10 0.34806472 <a title="21-lda-10" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>11 0.34802371 <a title="21-lda-11" href="./jmlr-2012-On_the_Convergence_Rate_oflp-Norm_Multiple_Kernel_Learning.html">81 jmlr-2012-On the Convergence Rate oflp-Norm Multiple Kernel Learning</a></p>
<p>12 0.34751314 <a title="21-lda-12" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>13 0.34745675 <a title="21-lda-13" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>14 0.34700492 <a title="21-lda-14" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>15 0.34637249 <a title="21-lda-15" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>16 0.34623265 <a title="21-lda-16" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>17 0.34622425 <a title="21-lda-17" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>18 0.34609404 <a title="21-lda-18" href="./jmlr-2012-Towards_Integrative_Causal_Analysis_of_Heterogeneous_Data_Sets_and_Studies.html">114 jmlr-2012-Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies</a></p>
<p>19 0.34564573 <a title="21-lda-19" href="./jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</a></p>
<p>20 0.34546649 <a title="21-lda-20" href="./jmlr-2012-Robust_Kernel_Density_Estimation.html">100 jmlr-2012-Robust Kernel Density Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
