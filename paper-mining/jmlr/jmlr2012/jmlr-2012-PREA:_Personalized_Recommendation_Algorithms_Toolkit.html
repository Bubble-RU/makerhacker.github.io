<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-88" href="#">jmlr2012-88</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</h1>
<br/><p>Source: <a title="jmlr-2012-88-pdf" href="http://jmlr.org/papers/volume13/lee12b/lee12b.pdf">pdf</a></p><p>Author: Joonseok Lee, Mingxuan Sun, Guy Lebanon</p><p>Abstract: Recommendation systems are important business applications with signiﬁcant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms. Keywords: recommender systems, collaborative ﬁltering, evaluation metrics</p><p>Reference: <a title="jmlr-2012-88-reference" href="../jmlr2012_reference/jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  College of Computing Georgia Institute of Technology Atlanta, Georgia 30332, USA  Editor: Soeren Sonnenburg  Abstract Recommendation systems are important business applications with signiﬁcant economic impact. [sent-5, score-0.038]
</p><p>2 In recent years, a large number of algorithms have been proposed for recommendation systems. [sent-6, score-0.489]
</p><p>3 In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. [sent-7, score-0.797]
</p><p>4 In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms. [sent-8, score-0.26]
</p><p>5 Keywords: recommender systems, collaborative ﬁltering, evaluation metrics  1. [sent-9, score-0.411]
</p><p>6 Introduction As the demand for personalized services in E-commerce increases, recommendation systems are emerging as an important business application. [sent-10, score-0.632]
</p><p>7 com, for example, provides personalized product recommendations based on previous purchases. [sent-12, score-0.184]
</p><p>8 A wide variety of algorithms have been proposed by the research community for recommendation systems. [sent-17, score-0.489]
</p><p>9 Unlike classiﬁcation where comprehensive packages are available, existing recommendation systems toolkits lag behind. [sent-18, score-0.763]
</p><p>10 They concentrate on implementing traditional algorithms rather than the rapidly evolving state-of-the-art. [sent-19, score-0.126]
</p><p>11 Implementations of modern algorithms are scattered over different sources which makes it hard to have a fair and comprehensive comparison. [sent-20, score-0.108]
</p><p>12 In this paper we describe a new toolkit PREA (Personalized Recommendation Algorithms Toolkit), implementing a wide variety of recommendation systems algorithms. [sent-21, score-0.73]
</p><p>13 PREA offers implementation of modern state-of-the-art recommendation algorithms as well as most traditional ones. [sent-22, score-0.563]
</p><p>14 In addition, it provides many popular evaluation methods and data sets. [sent-23, score-0.067]
</p><p>15 As a result, it can be used to conduct fair and comprehensive comparisons between different recommendation algorithms. [sent-24, score-0.573]
</p><p>16 The implemented evaluation routines, data sets, and the open-source nature of PREA makes it easy for third parties to contribute additional implementations. [sent-25, score-0.067]
</p><p>17 Not surprisingly, the performance of recommendation algorithms depends on the data characteristics. [sent-26, score-0.489]
</p><p>18 , 2012a,b) For example, some algorithms may work better or worse depending on the amount of missing data (sparsity), distribution of ratings, and the number of users and items. [sent-28, score-0.029]
</p><p>19 Furthermore, the different evaluation methods that have been proposed in the literature may have conﬂicting orderings over algorithms (Gunawardana and Shani, 2009). [sent-30, score-0.116]
</p><p>20 L EE , S UN AND L EBANON  algorithms using a variety of evaluation metrics may clarify which algorithms perform better under what circumstance. [sent-32, score-0.183]
</p><p>21 The toolkit consists of three groups of classes, as shown in Figure 1. [sent-37, score-0.189]
</p><p>22 The top-level routines of the toolkit may be directly called from other programming environments like Matlab. [sent-38, score-0.229]
</p><p>23 The top two groups implement basic data structures and recommendation algorithms. [sent-39, score-0.489]
</p><p>24 The third group (bottom) implements evaluation metrics, statistical distributions, and other utility functions. [sent-40, score-0.138]
</p><p>25 1 Input Data File Format As in the WEKA1 toolkit PREA accepts the data in ARFF2 (Attribute-Relation File Format). [sent-42, score-0.189]
</p><p>26 Since virtually all recommendation systems data sets are sparse, PREA implements sparse (rather than dense) ARFF format. [sent-43, score-0.588]
</p><p>27 2 Evaluation Framework For ease of comparison, PREA provides the following uniﬁed interface for running the different recommendation algorithms. [sent-45, score-0.489]
</p><p>28 • • • •  Instantiate a class instance according to the type of the recommendation algorithm. [sent-46, score-0.489]
</p><p>29 Given a test set, predict user ratings over unseen items. [sent-48, score-0.215]
</p><p>30 Given the above prediction and held-out ground truth ratings, evaluate the prediction using various evaluation metrics. [sent-49, score-0.067]
</p><p>31 Note that lazy learning algorithms like the constant models or memory-based algorithms may skip the second step. [sent-50, score-0.023]
</p><p>32 A simple train and test split is constructed by choosing a certain proportion of all ratings as test set and assigning the remaining ratings to the train set. [sent-51, score-0.318]
</p><p>33 3 Data Structures PREA uses a sparse matrix representation for the rating matrices which are generally extremely sparse (users provide ratings for only a small subset of items). [sent-67, score-0.269]
</p><p>34 Figure 2 (left) shows an example of a sparse vector, containing data only in indices 1, 3, and 6. [sent-70, score-0.028]
</p><p>35 This design is also useful for fast transposing of sparse matrices by interchanging rows and columns. [sent-73, score-0.055]
</p><p>36 Figure 2 (right) shows an example of sparse matrix. [sent-74, score-0.028]
</p><p>37 Figure 2: Sparse Vector (left) and Matrix (right) Implementation PREA also uses dense matrices in some cases. [sent-75, score-0.038]
</p><p>38 For example, dense representations are used for low-rank matrix factorizations or other algebraic operations that do not maintain sparsity. [sent-76, score-0.089]
</p><p>39 The dense representations are based on the matrix implementations in the Universal Java Matrix Package (UJMP) (http://www. [sent-77, score-0.059]
</p><p>40 4 Implemented Algorithms and Evaluation Metrics PREA implements the following prediction algorithms: • Baselines (constant, random, overall average, user average, item average): make little use of personalized information for recommending items. [sent-81, score-0.266]
</p><p>41 • Memory-based Neighborhood algorithms (user-based, item-based collaborative ﬁltering and their extensions including default vote, inverse user frequency): predict ratings of unseen items by referring those of similar users or items. [sent-82, score-0.478]
</p><p>42 • Matrix Factorization methods (SVD, NMF, PMF, Bayesian PMF, Non-linear PMF): build low-rank user/item proﬁles by factorizing training data set with linear algebraic techniques. [sent-83, score-0.054]
</p><p>43 • Others: recent state-of-the-art algorithms such as Fast NPCA and Rank-based collaborative ﬁltering. [sent-84, score-0.183]
</p><p>44 We provide popular evaluation metrics as follows: • Accuracy for rating predictions (RMSE, MAE, NMAE): measure how much the predictions are similar to the actual ratings. [sent-85, score-0.216]
</p><p>45 • Rank-based evaluation metrics (HLU, NDCG, Kendall’s Tau, and Spearman): score depending on similarity between orderings of predicted ratings and those of ground truth. [sent-86, score-0.391]
</p><p>46 Related Work Several other open-source recommendation toolkits are available. [sent-88, score-0.66]
</p><p>47 Table 1 summarizes the implemented features in these toolkits and compares them to those in PREA. [sent-89, score-0.171]
</p><p>48 It also supports powerful mathematical and statistical operations as it is a general-purpose machine learning toolkit. [sent-91, score-0.022]
</p><p>49 Coﬁ6 implements several traditional algorithms with a simple design based on providing wrappers for publicly available data sets. [sent-93, score-0.121]
</p><p>50 MyMedia7 is a C#-based recommendation toolkit which supports most traditional algorithms and several evaluation metrics. [sent-94, score-0.817]
</p><p>51 As indicated in Table 1, existing toolkits widely provide simple memory-based algorithms, while recent state-of-the-art algorithms are often not supported. [sent-95, score-0.171]
</p><p>52 Also, these toolkits are generally limited in their evaluation metrics (with the notable exception of MyMedia). [sent-96, score-0.354]
</p><p>53 In contrast, PREA provides a wide coverage of the the most up-to-date algorithms as well as various evaluation metrics, facilitating a comprehensive comparison between state-of-the-art and newly proposed algorithms in the research community. [sent-97, score-0.137]
</p><p>54 The open-source nature of the software (available under GPL) may encourage other recommendation systems experts to add their own algorithms to PREA. [sent-104, score-0.489]
</p><p>55 More documentation for developers as well as user tutorials are available on the web (http://prea. [sent-105, score-0.084]
</p><p>56 A survey of accuracy evaluation metrics of recommendation tasks. [sent-140, score-0.672]
</p><p>57 Bayesian probabilistic matrix factorization using markov chain monte carlo. [sent-185, score-0.077]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('prea', 0.571), ('recommendation', 0.489), ('toolkit', 0.189), ('collaborative', 0.183), ('toolkits', 0.171), ('pmf', 0.163), ('ratings', 0.159), ('java', 0.139), ('metrics', 0.116), ('personalized', 0.105), ('sun', 0.09), ('lebanon', 0.09), ('gpl', 0.09), ('cf', 0.084), ('salakhutdinov', 0.084), ('recommendations', 0.079), ('implements', 0.071), ('evaluation', 0.067), ('lee', 0.064), ('breese', 0.063), ('ebanon', 0.063), ('ecommendation', 0.063), ('ersonalized', 0.063), ('gunawardana', 0.063), ('guy', 0.063), ('hashmap', 0.063), ('joonseok', 0.063), ('lemire', 0.063), ('mingxuan', 0.063), ('mymedia', 0.063), ('npca', 0.063), ('sarwar', 0.063), ('sparsevector', 0.063), ('gatech', 0.063), ('factorization', 0.056), ('ltering', 0.056), ('mnih', 0.054), ('oolkit', 0.054), ('implementing', 0.052), ('traditional', 0.05), ('tau', 0.049), ('orderings', 0.049), ('lgpl', 0.049), ('arff', 0.049), ('file', 0.049), ('nmf', 0.049), ('comprehensive', 0.047), ('kendall', 0.045), ('spearman', 0.045), ('rmse', 0.045), ('recommender', 0.045), ('mae', 0.042), ('su', 0.04), ('routines', 0.04), ('dense', 0.038), ('lgorithms', 0.038), ('business', 0.038), ('fair', 0.037), ('georgia', 0.036), ('baselines', 0.036), ('item', 0.033), ('vote', 0.033), ('svd', 0.033), ('rating', 0.033), ('co', 0.033), ('format', 0.032), ('algebraic', 0.03), ('ee', 0.03), ('user', 0.03), ('web', 0.03), ('packages', 0.029), ('items', 0.029), ('users', 0.029), ('lawrence', 0.028), ('sparse', 0.028), ('interchanging', 0.027), ('lag', 0.027), ('instantiate', 0.027), ('konstan', 0.027), ('nmae', 0.027), ('year', 0.027), ('recommending', 0.027), ('soeren', 0.027), ('unseen', 0.026), ('un', 0.025), ('factorizing', 0.024), ('release', 0.024), ('evolving', 0.024), ('friend', 0.024), ('urtasun', 0.024), ('sigir', 0.024), ('tutorials', 0.024), ('modern', 0.024), ('industry', 0.023), ('facilitating', 0.023), ('lazy', 0.023), ('filtering', 0.023), ('default', 0.022), ('supports', 0.022), ('matrix', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="88-tfidf-1" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>Author: Joonseok Lee, Mingxuan Sun, Guy Lebanon</p><p>Abstract: Recommendation systems are important business applications with signiﬁcant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms. Keywords: recommender systems, collaborative ﬁltering, evaluation metrics</p><p>2 0.14643867 <a title="88-tfidf-2" href="./jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering.html">101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</a></p>
<p>Author: Tianqi Chen, Weinan Zhang, Qiuxia Lu, Kailong Chen, Zhao Zheng, Yong Yu</p><p>Abstract: In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative ﬁltering. SVDFeature is designed to efﬁciently solve the feature-based matrix factorization. The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information. The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efﬁcient training on large-scale data set. Using this toolkit, we built solutions to win KDD Cup for two consecutive years. Keywords: large-scale collaborative ﬁltering, context-aware recommendation, ranking</p><p>3 0.059609868 <a title="88-tfidf-3" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>Author: Marinka Žitnik, Blaž Zupan</p><p>Abstract: NIMFA is an open-source Python library that provides a uniﬁed interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA’s component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks. Keywords: nonnegative matrix factorization, initialization methods, quality measures, scripting, Python</p><p>4 0.054698061 <a title="88-tfidf-4" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>5 0.041038256 <a title="88-tfidf-5" href="./jmlr-2012-Sparse_and_Unique_Nonnegative_Matrix_Factorization_Through_Data_Preprocessing.html">108 jmlr-2012-Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing</a></p>
<p>Author: Nicolas Gillis</p><p>Abstract: Nonnegative matrix factorization (NMF) has become a very popular technique in machine learning because it automatically extracts meaningful features through a sparse and part-based representation. However, NMF has the drawback of being highly ill-posed, that is, there typically exist many different but equivalent factorizations. In this paper, we introduce a completely new way to obtaining more well-posed NMF problems whose solutions are sparser. Our technique is based on the preprocessing of the nonnegative input data matrix, and relies on the theory of M-matrices and the geometric interpretation of NMF. This approach provably leads to optimal and sparse solutions under the separability assumption of Donoho and Stodden (2003), and, for rank-three matrices, makes the number of exact factorizations ﬁnite. We illustrate the effectiveness of our technique on several image data sets. Keywords: nonnegative matrix factorization, data preprocessing, uniqueness, sparsity, inversepositive matrices</p><p>6 0.036372881 <a title="88-tfidf-6" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>7 0.033369776 <a title="88-tfidf-7" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<p>8 0.033315904 <a title="88-tfidf-8" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>9 0.029364999 <a title="88-tfidf-9" href="./jmlr-2012-Iterative_Reweighted_Algorithms_for_Matrix_Rank_Minimization.html">52 jmlr-2012-Iterative Reweighted Algorithms for Matrix Rank Minimization</a></p>
<p>10 0.028173175 <a title="88-tfidf-10" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>11 0.024160856 <a title="88-tfidf-11" href="./jmlr-2012-Optimistic_Bayesian_Sampling_in_Contextual-Bandit_Problems.html">86 jmlr-2012-Optimistic Bayesian Sampling in Contextual-Bandit Problems</a></p>
<p>12 0.02121095 <a title="88-tfidf-12" href="./jmlr-2012-MULTIBOOST%3A_A_Multi-purpose_Boosting_Package.html">62 jmlr-2012-MULTIBOOST: A Multi-purpose Boosting Package</a></p>
<p>13 0.020288756 <a title="88-tfidf-13" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>14 0.019756826 <a title="88-tfidf-14" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>15 0.018666174 <a title="88-tfidf-15" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<p>16 0.018606238 <a title="88-tfidf-16" href="./jmlr-2012-Algebraic_Geometric_Comparison_of_Probability_Distributions.html">15 jmlr-2012-Algebraic Geometric Comparison of Probability Distributions</a></p>
<p>17 0.017617101 <a title="88-tfidf-17" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<p>18 0.017482437 <a title="88-tfidf-18" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>19 0.01744728 <a title="88-tfidf-19" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>20 0.017368991 <a title="88-tfidf-20" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.073), (1, 0.029), (2, 0.14), (3, -0.032), (4, -0.021), (5, 0.051), (6, 0.051), (7, -0.035), (8, -0.029), (9, -0.183), (10, -0.177), (11, -0.123), (12, 0.215), (13, -0.089), (14, -0.014), (15, 0.105), (16, -0.026), (17, -0.028), (18, -0.103), (19, -0.125), (20, -0.007), (21, 0.168), (22, 0.163), (23, -0.096), (24, -0.045), (25, 0.412), (26, 0.069), (27, -0.097), (28, -0.031), (29, 0.024), (30, -0.009), (31, 0.044), (32, -0.081), (33, -0.137), (34, -0.035), (35, 0.015), (36, 0.029), (37, 0.062), (38, -0.02), (39, 0.014), (40, -0.081), (41, 0.001), (42, -0.006), (43, -0.051), (44, -0.035), (45, 0.082), (46, 0.04), (47, -0.031), (48, 0.01), (49, 0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97852045 <a title="88-lsi-1" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>Author: Joonseok Lee, Mingxuan Sun, Guy Lebanon</p><p>Abstract: Recommendation systems are important business applications with signiﬁcant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms. Keywords: recommender systems, collaborative ﬁltering, evaluation metrics</p><p>2 0.9278779 <a title="88-lsi-2" href="./jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering.html">101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</a></p>
<p>Author: Tianqi Chen, Weinan Zhang, Qiuxia Lu, Kailong Chen, Zhao Zheng, Yong Yu</p><p>Abstract: In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative ﬁltering. SVDFeature is designed to efﬁciently solve the feature-based matrix factorization. The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information. The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efﬁcient training on large-scale data set. Using this toolkit, we built solutions to win KDD Cup for two consecutive years. Keywords: large-scale collaborative ﬁltering, context-aware recommendation, ranking</p><p>3 0.37527779 <a title="88-lsi-3" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>4 0.26260397 <a title="88-lsi-4" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>Author: Marinka Žitnik, Blaž Zupan</p><p>Abstract: NIMFA is an open-source Python library that provides a uniﬁed interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA’s component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks. Keywords: nonnegative matrix factorization, initialization methods, quality measures, scripting, Python</p><p>5 0.19876102 <a title="88-lsi-5" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<p>Author: José Hernández-Orallo, Peter Flach, Cèsar Ferri</p><p>Abstract: Many performance metrics have been introduced in the literature for the evaluation of classiﬁcation performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into reﬁnement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassiﬁcation costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: ﬁxed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the reﬁnement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be summarised as follows: given a model, apply the threshold choice methods that correspond with the available information about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibra</p><p>6 0.19861965 <a title="88-lsi-6" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>7 0.18036284 <a title="88-lsi-7" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>8 0.16613457 <a title="88-lsi-8" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<p>9 0.13328178 <a title="88-lsi-9" href="./jmlr-2012-Iterative_Reweighted_Algorithms_for_Matrix_Rank_Minimization.html">52 jmlr-2012-Iterative Reweighted Algorithms for Matrix Rank Minimization</a></p>
<p>10 0.13101861 <a title="88-lsi-10" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<p>11 0.11519351 <a title="88-lsi-11" href="./jmlr-2012-Optimistic_Bayesian_Sampling_in_Contextual-Bandit_Problems.html">86 jmlr-2012-Optimistic Bayesian Sampling in Contextual-Bandit Problems</a></p>
<p>12 0.1099764 <a title="88-lsi-12" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>13 0.10571464 <a title="88-lsi-13" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>14 0.10331957 <a title="88-lsi-14" href="./jmlr-2012-Algebraic_Geometric_Comparison_of_Probability_Distributions.html">15 jmlr-2012-Algebraic Geometric Comparison of Probability Distributions</a></p>
<p>15 0.10161251 <a title="88-lsi-15" href="./jmlr-2012-Sparse_and_Unique_Nonnegative_Matrix_Factorization_Through_Data_Preprocessing.html">108 jmlr-2012-Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing</a></p>
<p>16 0.10053387 <a title="88-lsi-16" href="./jmlr-2012-Towards_Integrative_Causal_Analysis_of_Heterogeneous_Data_Sets_and_Studies.html">114 jmlr-2012-Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies</a></p>
<p>17 0.098740846 <a title="88-lsi-17" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>18 0.086707003 <a title="88-lsi-18" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>19 0.081136376 <a title="88-lsi-19" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>20 0.079965338 <a title="88-lsi-20" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.046), (21, 0.025), (26, 0.023), (27, 0.016), (29, 0.013), (35, 0.014), (49, 0.021), (56, 0.057), (57, 0.015), (69, 0.587), (75, 0.019), (92, 0.027), (96, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88377845 <a title="88-lda-1" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>Author: Joonseok Lee, Mingxuan Sun, Guy Lebanon</p><p>Abstract: Recommendation systems are important business applications with signiﬁcant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms. Keywords: recommender systems, collaborative ﬁltering, evaluation metrics</p><p>2 0.84942144 <a title="88-lda-2" href="./jmlr-2012-A_Model_of_the_Perception_of_Facial_Expressions_of_Emotion_by_Humans%3A_Research_Overview_and_Perspectives.html">6 jmlr-2012-A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives</a></p>
<p>Author: Aleix Martinez, Shichuan Du</p><p>Abstract: In cognitive science and neuroscience, there have been two leading models describing how humans perceive and classify facial expressions of emotion—the continuous and the categorical model. The continuous model deﬁnes each facial expression of emotion as a feature vector in a face space. This model explains, for example, how expressions of emotion can be seen at different intensities. In contrast, the categorical model consists of C classiﬁers, each tuned to a speciﬁc emotion category. This model explains, among other ﬁndings, why the images in a morphing sequence between a happy and a surprise face are perceived as either happy or surprise but not something in between. While the continuous model has a more difﬁcult time justifying this latter ﬁnding, the categorical model is not as good when it comes to explaining how expressions are recognized at different intensities or modes. Most importantly, both models have problems explaining how one can recognize combinations of emotion categories such as happily surprised versus angrily surprised versus surprise. To resolve these issues, in the past several years, we have worked on a revised model that justiﬁes the results reported in the cognitive science and neuroscience literature. This model consists of C distinct continuous spaces. Multiple (compound) emotion categories can be recognized by linearly combining these C face spaces. The dimensions of these spaces are shown to be mostly conﬁgural. According to this model, the major task for the classiﬁcation of facial expressions of emotion is precise, detailed detection of facial landmarks rather than recognition. We provide an overview of the literature justifying the model, show how the resulting model can be employed to build algorithms for the recognition of facial expression of emotion, and propose research directions in machine learning and computer vision researchers to keep pushing the state of the art in these areas. We also discuss how the model can aid in stu</p><p>3 0.83684272 <a title="88-lda-3" href="./jmlr-2012-Human_Gesture_Recognition_on_Product_Manifolds.html">50 jmlr-2012-Human Gesture Recognition on Product Manifolds</a></p>
<p>Author: Yui Man Lui</p><p>Abstract: Action videos are multidimensional data and can be naturally represented as data tensors. While tensor computing is widely used in computer vision, the geometry of tensor space is often ignored. The aim of this paper is to demonstrate the importance of the intrinsic geometry of tensor space which yields a very discriminating structure for action recognition. We characterize data tensors as points on a product manifold and model it statistically using least squares regression. To this aim, we factorize a data tensor relating to each order of the tensor using Higher Order Singular Value Decomposition (HOSVD) and then impose each factorized element on a Grassmann manifold. Furthermore, we account for underlying geometry on manifolds and formulate least squares regression as a composite function. This gives a natural extension from Euclidean space to manifolds. Consequently, classiﬁcation is performed using geodesic distance on a product manifold where each factor manifold is Grassmannian. Our method exploits appearance and motion without explicitly modeling the shapes and dynamics. We assess the proposed method using three gesture databases, namely the Cambridge hand-gesture, the UMD Keck body-gesture, and the CHALEARN gesture challenge data sets. Experimental results reveal that not only does the proposed method perform well on the standard benchmark data sets, but also it generalizes well on the one-shot-learning gesture challenge. Furthermore, it is based on a simple statistical model and the intrinsic geometry of tensor space. Keywords: gesture recognition, action recognition, Grassmann manifolds, product manifolds, one-shot-learning, kinect data</p><p>4 0.2766979 <a title="88-lda-4" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>Author: Helen Cooper, Eng-Jon Ong, Nicolas Pugeault, Richard Bowden</p><p>Abstract: This paper discusses sign language recognition using linguistic sub-units. It presents three types of sub-units for consideration; those learnt from appearance data as well as those inferred from both 2D or 3D tracking data. These sub-units are then combined using a sign level classiﬁer; here, two options are presented. The ﬁrst uses Markov Models to encode the temporal changes between sub-units. The second makes use of Sequential Pattern Boosting to apply discriminative feature selection at the same time as encoding temporal information. This approach is more robust to noise and performs well in signer independent tests, improving results from the 54% achieved by the Markov Chains to 76%. Keywords: sign language recognition, sequential pattern boosting, depth cameras, sub-units, signer independence, data set</p><p>5 0.26546454 <a title="88-lda-5" href="./jmlr-2012-Finding_Recurrent_Patterns_from_Continuous_Sign_Language_Sentences_for_Automated_Extraction_of_Signs.html">45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</a></p>
<p>Author: Sunita Nayak, Kester Duncan, Sudeep Sarkar, Barbara Loeding</p><p>Abstract: We present a probabilistic framework to automatically learn models of recurring signs from multiple sign language video sequences containing the vocabulary of interest. We extract the parts of the signs that are present in most occurrences of the sign in context and are robust to the variations produced by adjacent signs. Each sentence video is ﬁrst transformed into a multidimensional time series representation, capturing the motion and shape aspects of the sign. Skin color blobs are extracted from frames of color video sequences, and a probabilistic relational distribution is formed for each frame using the contour and edge pixels from the skin blobs. Each sentence is represented as a trajectory in a low dimensional space called the space of relational distributions. Given these time series trajectories, we extract signemes from multiple sentences concurrently using iterated conditional modes (ICM). We show results by learning single signs from a collection of sentences with one common pervading sign, multiple signs from a collection of sentences with more than one common sign, and single signs from a mixed collection of sentences. The extracted signemes demonstrate that our approach is robust to some extent to the variations produced within a sign due to different contexts. We also show results whereby these learned sign models are used for spotting signs in test sequences. Keywords: pattern extraction, sign language recognition, signeme extraction, sign modeling, iterated conditional modes</p><p>6 0.25754765 <a title="88-lda-6" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>7 0.25533265 <a title="88-lda-7" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>8 0.24384405 <a title="88-lda-8" href="./jmlr-2012-Robust_Kernel_Density_Estimation.html">100 jmlr-2012-Robust Kernel Density Estimation</a></p>
<p>9 0.24279729 <a title="88-lda-9" href="./jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering.html">101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</a></p>
<p>10 0.21047123 <a title="88-lda-10" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>11 0.20620744 <a title="88-lda-11" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>12 0.19290671 <a title="88-lda-12" href="./jmlr-2012-MULTIBOOST%3A_A_Multi-purpose_Boosting_Package.html">62 jmlr-2012-MULTIBOOST: A Multi-purpose Boosting Package</a></p>
<p>13 0.1886487 <a title="88-lda-13" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>14 0.18422741 <a title="88-lda-14" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>15 0.18416663 <a title="88-lda-15" href="./jmlr-2012-A_Local_Spectral_Method_for_Graphs%3A_With_Applications_to_Improving_Graph_Partitions_and_Exploring_Data_Graphs_Locally.html">5 jmlr-2012-A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally</a></p>
<p>16 0.18370074 <a title="88-lda-16" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>17 0.18312331 <a title="88-lda-17" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>18 0.17956445 <a title="88-lda-18" href="./jmlr-2012-Fast_Approximation_of_Matrix_Coherence_and_Statistical_Leverage.html">43 jmlr-2012-Fast Approximation of Matrix Coherence and Statistical Leverage</a></p>
<p>19 0.17867383 <a title="88-lda-19" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>20 0.1780367 <a title="88-lda-20" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
