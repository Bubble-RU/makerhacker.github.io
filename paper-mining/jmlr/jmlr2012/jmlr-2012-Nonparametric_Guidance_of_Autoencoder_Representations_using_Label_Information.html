<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>78 jmlr-2012-Nonparametric Guidance of Autoencoder Representations using Label Information</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-78" href="#">jmlr2012-78</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>78 jmlr-2012-Nonparametric Guidance of Autoencoder Representations using Label Information</h1>
<br/><p>Source: <a title="jmlr-2012-78-pdf" href="http://jmlr.org/papers/volume13/snoek12a/snoek12a.pdf">pdf</a></p><p>Author: Jasper Snoek, Ryan P. Adams, Hugo Larochelle</p><p>Abstract: While unsupervised learning has long been useful for density modeling, exploratory data analysis and visualization, it has become increasingly important for discovering features that will later be used for discriminative tasks. Discriminative algorithms often work best with highly-informative features; remarkably, such features can often be learned without the labels. One particularly effective way to perform such unsupervised learning has been to use autoencoder neural networks, which ﬁnd latent representations that are constrained but nevertheless informative for reconstruction. However, pure unsupervised learning with autoencoders can ﬁnd representations that may or may not be useful for the ultimate discriminative task. It is a continuing challenge to guide the training of an autoencoder so that it ﬁnds features which will be useful for predicting labels. Similarly, we often have a priori information regarding what statistical variation will be irrelevant to the ultimate discriminative task, and we would like to be able to use this for guidance as well. Although a typical strategy would be to include a parametric discriminative model as part of the autoencoder training, here we propose a nonparametric approach that uses a Gaussian process to guide the representation. By using a nonparametric model, we can ensure that a useful discriminative function exists for a given set of features, without explicitly instantiating it. We demonstrate the superiority of this guidance mechanism on four data sets, including a real-world application to rehabilitation research. We also show how our proposed approach can learn to explicitly ignore statistically signiﬁcant covariate information that is label-irrelevant, by evaluating on the small NORB image recognition problem in which pose and lighting labels are available. Keywords: autoencoder, gaussian process, gaussian process latent variable model, representation learning, unsupervised learning</p><p>Reference: <a title="jmlr-2012-78-reference" href="../jmlr2012_reference/jmlr-2012-Nonparametric_Guidance_of_Autoencoder_Representations_using_Label_Information_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 One particularly effective way to perform such unsupervised learning has been to use autoencoder neural networks, which ﬁnd latent representations that are constrained but nevertheless informative for reconstruction. [sent-10, score-0.934]
</p><p>2 However, pure unsupervised learning with autoencoders can ﬁnd representations that may or may not be useful for the ultimate discriminative task. [sent-11, score-0.298]
</p><p>3 It is a continuing challenge to guide the training of an autoencoder so that it ﬁnds features which will be useful for predicting labels. [sent-12, score-0.627]
</p><p>4 Similarly, we often have a priori information regarding what statistical variation will be irrelevant to the ultimate discriminative task, and we would like to be able to use this for guidance as well. [sent-13, score-0.302]
</p><p>5 Although a typical strategy would be to include a parametric discriminative model as part of the autoencoder training, here we propose a nonparametric approach that uses a Gaussian process to guide the representation. [sent-14, score-0.869]
</p><p>6 We demonstrate the superiority of this guidance mechanism on four data sets, including a real-world application to rehabilitation research. [sent-16, score-0.356]
</p><p>7 Keywords: autoencoder, gaussian process, gaussian process latent variable model, representation learning, unsupervised learning  1. [sent-18, score-0.369]
</p><p>8 In this work, we are interested in the discovery of latent features which can be later used as alternate representations of data for discriminative tasks. [sent-26, score-0.319]
</p><p>9 Neural networks have proven to be an effective way to perform such processing, and autoencoder neural networks, speciﬁcally, have been used to ﬁnd representatons for a variety of downstream machine learning tasks, for example, image classiﬁcation (Vincent et al. [sent-30, score-0.675]
</p><p>10 The critical insight of the autoencoder neural network is the idea of using a constrained (typically either sparse or low-dimensional) representation within a feedforward neural network. [sent-34, score-0.749]
</p><p>11 (2007) introduced weak supervision into the autoencoder training objective by adding label-speciﬁc output units in addition to the reconstruction. [sent-41, score-0.719]
</p><p>12 The difﬁculty of this approach is that it complicates the task of learning the autoencoder representation. [sent-43, score-0.627]
</p><p>13 Here we propose a different take on the issue of introducing supervised guidance into autoencoder representations. [sent-47, score-0.869]
</p><p>14 We consider Gaussian process priors on the discriminative function that maps the latent codes into labels. [sent-48, score-0.297]
</p><p>15 We are then able to combine the efﬁcient parametric feed-forward aspects of the autoencoder with a ﬂexible Bayesian nonparametric model for the labels. [sent-51, score-0.762]
</p><p>16 This also leads to an interesting interpretation of the back-constrained GPLVM itself as a limiting case of an autoencoder in which the decoder has been marginalized out. [sent-52, score-0.743]
</p><p>17 We also examine a data set that highlights the value of our approach, in which we cannot only use guidance from desired labels, but also introduce guidance away from irrelevant representations. [sent-54, score-0.44]
</p><p>18 Unsupervised Learning of Latent Representations The nonparametrically-guided autoencoder presented in this paper is motivated largely by the relationship between two different approaches to latent variable modeling. [sent-56, score-0.817]
</p><p>19 In this section, we review these two approaches, the GPLVM and autoencoder neural network, and examine precisely how they are related. [sent-57, score-0.653]
</p><p>20 , 1987) is a neural network architecture that is designed to create a latent representation that is informative of the input data. [sent-60, score-0.286]
</p><p>21 Through training the model to reproduce the input data at its output, a latent embedding must arise within the hidden layer of the model. [sent-61, score-0.324]
</p><p>22 However, these are difﬁcult to learn because a trivial minimum of the autoencoder 2569  S NOEK , A DAMS AND L AROCHELLE  reconstruction objective is reached when the autoencoder learns the identity transformation. [sent-77, score-1.254]
</p><p>23 The denoising autoencoder forces the model to learn more interesting structure from the data by providing as input a corrupted training example, while evaluating reconstruction on the noiseless original. [sent-78, score-0.699]
</p><p>24 Such a manifold is difﬁcult to deﬁne a priori, however, and thus the problem is often framed as learning the latent embedding under an assumed smooth functional mapping between the visible and latent spaces. [sent-83, score-0.46]
</p><p>25 Using a Gaussian process prior, the GPLVM marginalizes over the inﬁnite possible mappings from the latent to visible spaces and optimizes the latent embedding over a distribution of mappings. [sent-86, score-0.438]
</p><p>26 Not only does this introduce arbitrary gaps in the latent manifold, but it also complicates the encoding of novel data points into the latent space as there is no direct mapping. [sent-136, score-0.408]
</p><p>27 The latent representations of out-of-sample data must thus be optimized, conditioned on the latent embedding of the training examples. [sent-137, score-0.46]
</p><p>28 The NeuroScale algorithm is a radial basis function network that creates a one-way mapping from data to a latent space using a heuristic loss that attempts to preserve pairwise distances between data cases. [sent-143, score-0.277]
</p><p>29 An interesting and overlooked consequence of this relationship is that it establishes a connection between autoencoders and the back-constrained Gaussian process latent variable model. [sent-148, score-0.34]
</p><p>30 A GPLVM with the covariance function of Williams (1998), although it does not impose a density over the data, is similar to a density network (MacKay, 1994) with an inﬁnite number of hidden units in the single hidden layer. [sent-149, score-0.403]
</p><p>31 We can transform this density network into a semiparametric autoencoder by applying a neural network as the backconstraint network of the GPLVM. [sent-150, score-0.773]
</p><p>32 The encoder of the resulting model is a parametric neural network and the decoder a Gaussian process. [sent-151, score-0.372]
</p><p>33 After training, the decoder network of an autoencoder is generally superﬂuous. [sent-157, score-0.783]
</p><p>34 Thus, for very high dimensional data, a standard autoencoder may be more desirable. [sent-162, score-0.627]
</p><p>35 Supervised Guidance of Latent Representations Unsupervised learning has proven to be effective for learning latent representations that excel in discriminative tasks. [sent-164, score-0.319]
</p><p>36 (2007) demonstrated, for example, that while a purely supervised signal can lead to overﬁtting, mild supervised guidance can be beneﬁcial when initializing a discriminative deep neural network. [sent-167, score-0.411]
</p><p>37 (2007) proposed a hybrid approach under which the unsupervised model’s latent representation also be trained to predict the label information, by adding a parametric mapping c(x ; Λ) : X → Z from the latent space X to the labels Z and backpropagating error gradients from the output. [sent-169, score-0.66]
</p><p>38 This “partial supervision” thus encourages the model to encode statistics within the latent representation that are useful for a speciﬁc (but learned) parameterization of such a linear mapping. [sent-172, score-0.28]
</p><p>39 The assumption of a speciﬁc parametric form for the mapping c(x ; Λ) restricts the supervised guidance to classiﬁers within that family of mappings. [sent-175, score-0.372]
</p><p>40 At every iteration t of descent (with current state φt , ψt , Λt ), the gradient from supervised guidance encourages the latent representation (currently parametrized by φt , ψt ) to become more predictive of the labels under the 2573  S NOEK , A DAMS AND L AROCHELLE  current label map c(x ; Λt ). [sent-178, score-0.564]
</p><p>41 That is, rather than learning a latent representation that is tied to a speciﬁc parameterized mapping to the labels, we would instead prefer to ﬁnd a latent representation that is consistent with an entire class of mappings. [sent-183, score-0.487]
</p><p>42 The result is a hybrid of the autoencoder and back-constrained GPLVM, where the encoder is shared across models. [sent-189, score-0.734]
</p><p>43 For notation, we will refer to this approach to guided latent representation as a nonparametrically guided autoencoder, or NPGA. [sent-190, score-0.295]
</p><p>44 As is common for autoencoders and to reduce the number of free parameters in the model, the encoder and decoder weights are tied. [sent-196, score-0.348]
</p><p>45 2574  N ONPARAMETRIC G UIDANCE OF AUTOENCODERS  denoising autoencoder variant of Vincent et al. [sent-199, score-0.674]
</p><p>46 That is, we update the denoising autoencoder noise every three iterations of conjugate gradient descent optimization. [sent-201, score-0.674]
</p><p>47 Note also that when the salient variations of the data are not relevant to a given discriminative task, the initial RBM training will not encourage the encoding of the discriminative information in the latent representation. [sent-210, score-0.415]
</p><p>48 The NPGA circumvents these issues by applying a GP to small mini-batches during the learning of the latent representation and uses the GP to learn a representation that is better even for a linear discriminative model. [sent-211, score-0.332]
</p><p>49 Salakhutdinov and Hinton (2007) combined autoencoder training with neighborhood component analysis (Goldberger et al. [sent-213, score-0.627]
</p><p>50 , 2004), which encouraged the model to encode similar latent representations for inputs belonging to the same class. [sent-214, score-0.296]
</p><p>51 Thus, the GPLVM enforces that examples close in label space will be closer in the latent representation than examples that are distant in label space. [sent-224, score-0.3]
</p><p>52 Encoding such periodic signals in a parametric neural network and blending this with unsupervised learning can be challenging (Zemel et al. [sent-229, score-0.279]
</p><p>53 In all experiments, the discriminative value of the learned representation is evaluated by training a linear (logistic) classiﬁer, a standard practice for evaluating latent representations. [sent-244, score-0.324]
</p><p>54 We use these data primarily to explore two questions: • To what extent does the nonparametric guidance of an unsupervised parametric autoencoder improve the learned feature representation with respect to the classiﬁcation objective? [sent-249, score-1.078]
</p><p>55 • What additional beneﬁt is gained through using nonparametric guidance over simply incorporating a parametric mapping to the labels? [sent-250, score-0.402]
</p><p>56 In order to address these concerns, we linearly blend our nonparametric guidance cost LGP (φ, Γ) with the one Bengio et al. [sent-251, score-0.272]
</p><p>57 Thus, α allows us to adjust the relative contribution of the unsupervised guidance while β weighs the relative contributions of the parametric and nonparametric supervised guidance. [sent-253, score-0.421]
</p><p>58 05 was added to the inputs of the denoising autoencoder cost. [sent-274, score-0.699]
</p><p>59 However, in Figure 1a we can see that some parametric guidance can be beneﬁcial, presumably because it is from the same discriminative family as the ﬁnal classiﬁer. [sent-284, score-0.385]
</p><p>60 We observe also that using a GP with a linear covariance function within the NPGA outperforms the parametric guidance (see Fig. [sent-287, score-0.372]
</p><p>61 Full images: A one-layer autoencoder with 2400 NReLU units was trained on the raw data (which was reduced from 32×32×3 = 3072 to 400 dimensions using PCA). [sent-336, score-0.719]
</p><p>62 28 × 28 patches: An autoencoder with 1500 logistic hidden units was trained on 28×28×3 patches subsampled from the full images, then reduced to 400 dimensions using PCA. [sent-339, score-0.898]
</p><p>63 1600 NReLU units were used in the autoencoder but the GP was applied to only 400 of them. [sent-350, score-0.719]
</p><p>64 When PCA preprocessing was used for autoencoder training, the inputs were corrupted with zero-mean Gaussian noise with standard deviation 0. [sent-353, score-0.677]
</p><p>65 2579  S NOEK , A DAMS AND L AROCHELLE  After training, a logistic regression classiﬁer was applied to the features resulting from the hidden layer of each autoencoder to evaluate their quality with respect to the classiﬁcation objective. [sent-360, score-0.766]
</p><p>66 The use of different architectures, methodologies and hidden unit activations demonstrates that the nonparametric guidance can be beneﬁcial for a wide variety of formulations. [sent-362, score-0.394]
</p><p>67 Certainly, the squared pixel difference objective of the autoencoder will be affected more by signiﬁcant lighting changes than object categories. [sent-379, score-0.691]
</p><p>68 In our empirical analysis we examine the following: As the autoencoder attempts to coalesce the various sources of structure into its hidden layer, can the NPGA guide the learning in such a way as to separate the class-invariant transformations of the data from the class-relevant information? [sent-382, score-0.728]
</p><p>69 In order to separate the latent embedding of the salient information related to each label, the GPs were applied to disjoint subsets of the hidden units of the autoencoder. [sent-384, score-0.449]
</p><p>70 Thus a GP mapping from a four dimensional latent space, H = 4, to class labels was applied to 1200 hidden units. [sent-386, score-0.374]
</p><p>71 Finally, because the azimuth is a periodic signal, a periodic kernel was used for the azimuth GP. [sent-390, score-0.276]
</p><p>72 2580  N ONPARAMETRIC G UIDANCE OF AUTOENCODERS  Class  Elevation  Lighting  Figure 3: Visualisations of the NORB training (top) and test (bottom) data latent space representations in the NPGA, corresponding to class (ﬁrst column), elevation (second column), and lighting (third column). [sent-392, score-0.334]
</p><p>73 To validate this conﬁguration, we empirically compared it to a standard autoencoder (i. [sent-395, score-0.627]
</p><p>74 , α = 0), an autoencoder with parametric logistic regression guidance and an NPGA with a single GP applied to all hidden units mapping to the class labels. [sent-397, score-1.208]
</p><p>75 For denoising autoencoder training, the raw pixels were corrupted by setting 20% of pixels to zero in the inputs. [sent-402, score-0.699]
</p><p>76 This implies that the half of the latent representation that encodes the information to which the model should be invariant can be discarded with virtually no discriminative penalty. [sent-439, score-0.302]
</p><p>77 Given the signiﬁcant difference in accuracy between this formulation and the other models, it appears to be very important to separate the encoding of different sources of variation within the autoencoder hidden layer. [sent-440, score-0.756]
</p><p>78 An autoencoder with parametric guidance to all four labels, mimicking the conﬁguration of the NPGA, achieved the poorest performance of the models tested, with 86% accuracy. [sent-446, score-0.93]
</p><p>79 Rehabilitation patients beneﬁt from performing repetitive rehabilitation exercises as frequently as possible but are limited due to a shortage of rehabilitation therapists. [sent-454, score-0.293]
</p><p>80 (2012) developed a system to automate the role of a therapist guiding rehabilitation patients through repetitive upper limb rehabilitation exercises. [sent-459, score-0.298]
</p><p>81 In our analysis of this problem we use a NPGA to encode a latent embedding of postures that facilitates better discrimination between different posture types. [sent-482, score-0.307]
</p><p>82 We interpolate between a standard autoencoder (α = 0), a classiﬁcation neural net (α = 1, β = 1), and a nonparametrically guided autoencoder by linear blending of their objectives according to Equation 5. [sent-485, score-1.332]
</p><p>83 (2012), to search over α ∈ [0, 1], β ∈ [0, 1], 10 − 1000 hidden units in the autoencoder and the GP latent dimensionality H ∈ {1. [sent-497, score-1.01]
</p><p>84 Thus, in Figure 5 we explore how the relationship between validation error and the amount of nonparametric guidance α, and parametric guidance β is expected to change as the number of autoencoder hidden units is varied. [sent-509, score-1.415]
</p><p>85 1, it seems clear that the best region in hyperparameter space is a combination of all three objectives, the parametric 2584  N ONPARAMETRIC G UIDANCE OF AUTOENCODERS  H=2, 10 hidden units  H=2, 1000 hidden units  H=2, 500 hidden units  1  1  1  0. [sent-512, score-0.662]
</p><p>86 8  1  10  (c)  Figure 5: The posterior mean learned by Bayesian optimization over the validation set classiﬁcation error (in percent) for α and β with H ﬁxed at 2 and three different settings of autoencoder hidden units: (a) 10, (b) 500, and (c) 1000. [sent-536, score-0.77]
</p><p>87 This shows how the relationship between validation error and the amount of nonparametric guidance, α, and parametric guidance, β, is expected to change as the number of autoencoder hidden units is increased. [sent-537, score-0.975]
</p><p>88 Also, as we increase the number of hidden units in the autoencoder, the amount of guidance required appears to decrease. [sent-541, score-0.413]
</p><p>89 As the capacity of the autoencoder is increased, it is likely that the autoencoder encodes increasingly subtle statistical structure in the data. [sent-542, score-1.254]
</p><p>90 When there are fewer hidden units, this structure is not encoded unless the autoencoder objective is augmented to reﬂect a preference for it. [sent-543, score-0.728]
</p><p>91 With the best performing NPGA reported above, a nearest neighbors classiﬁer applied to the hidden units of the autoencoder achieved an accuracy of 85. [sent-546, score-0.82]
</p><p>92 This likely reﬂects the fact that the autoencoder must still encode information that is useful for reconstruction but not discrimination. [sent-549, score-0.661]
</p><p>93 Conclusion In this paper we present an interesting theoretical link between the autoencoder neural network and the back-constrained Gaussian process latent variable model. [sent-554, score-0.908]
</p><p>94 A particular formulation of the back-constrained GPLVM can be interpreted as an autoencoder in which the decoder has an in2585  S NOEK , A DAMS AND L AROCHELLE  ﬁnite number of hidden units. [sent-555, score-0.844]
</p><p>95 This formulation exhibits some attractive properties as it allows one to learn the encoder half of the autoencoder while marginalizing over decoders. [sent-556, score-0.734]
</p><p>96 We examine the use of this model to guide the latent representation of an autoencoder to encode auxiliary label information without instantiating a parametric mapping to the labels. [sent-557, score-1.051]
</p><p>97 The resulting nonparametric guidance encourages the autoencoder to encode a latent representation that captures salient structure within the input data that is harmonious with the labels. [sent-558, score-1.212]
</p><p>98 Conceptually, this approach enforces simply that a smooth mapping exists from the latent representation to the labels rather than choosing or learning a speciﬁc parameterization. [sent-559, score-0.303]
</p><p>99 The approach is empirically validated on four data sets, demonstrating that the nonparametrically guided autoencoder encourages latent representations that are better with respect to a discriminative task. [sent-560, score-1.024]
</p><p>100 We demonstrate on the NORB data that this model can also be used to discourage latent representations that capture statistical structure that is known to be irrelevant through guiding the autoencoder to separate multiple sources of variation. [sent-567, score-0.864]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('autoencoder', 0.627), ('npga', 0.343), ('gplvm', 0.3), ('guidance', 0.22), ('latent', 0.19), ('gp', 0.151), ('rehabilitation', 0.136), ('autoencoders', 0.125), ('decoder', 0.116), ('encoder', 0.107), ('hidden', 0.101), ('dams', 0.092), ('noek', 0.092), ('units', 0.092), ('periodic', 0.086), ('onparametric', 0.084), ('uidance', 0.084), ('parametric', 0.083), ('discriminative', 0.082), ('arochelle', 0.079), ('gps', 0.071), ('covariance', 0.069), ('norb', 0.067), ('taati', 0.067), ('lighting', 0.064), ('larochelle', 0.053), ('nonparametric', 0.052), ('hugo', 0.051), ('cifar', 0.05), ('coates', 0.05), ('posture', 0.05), ('denoising', 0.047), ('mapping', 0.047), ('representations', 0.047), ('salakhutdinov', 0.044), ('unsupervised', 0.044), ('azimuth', 0.042), ('huq', 0.042), ('jasper', 0.042), ('label', 0.04), ('gaussian', 0.04), ('network', 0.04), ('patches', 0.04), ('deep', 0.039), ('logistic', 0.038), ('hyperparameters', 0.037), ('labels', 0.036), ('robotic', 0.036), ('snoek', 0.036), ('encode', 0.034), ('elevation', 0.033), ('lauto', 0.033), ('lgp', 0.033), ('nair', 0.033), ('neuroscale', 0.033), ('nrelu', 0.033), ('rajibul', 0.033), ('sgplvm', 0.033), ('yk', 0.033), ('exponentiated', 0.033), ('salient', 0.033), ('embedding', 0.033), ('alpha', 0.032), ('lawrence', 0.03), ('bayesian', 0.03), ('aaron', 0.03), ('representation', 0.03), ('oil', 0.029), ('nonparametrically', 0.029), ('encoding', 0.028), ('bengio', 0.026), ('neural', 0.026), ('geoffrey', 0.026), ('limb', 0.026), ('urtasun', 0.026), ('recti', 0.026), ('ruslan', 0.026), ('convolutional', 0.026), ('encourages', 0.026), ('inputs', 0.025), ('corrupted', 0.025), ('process', 0.025), ('regressor', 0.025), ('guided', 0.023), ('supervised', 0.022), ('adams', 0.022), ('image', 0.022), ('learned', 0.022), ('stroke', 0.021), ('activations', 0.021), ('exercises', 0.021), ('kan', 0.021), ('qui', 0.021), ('skeletal', 0.021), ('ryan', 0.021), ('alex', 0.021), ('classi', 0.021), ('validation', 0.02), ('kernel', 0.02), ('neil', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="78-tfidf-1" href="./jmlr-2012-Nonparametric_Guidance_of_Autoencoder_Representations_using_Label_Information.html">78 jmlr-2012-Nonparametric Guidance of Autoencoder Representations using Label Information</a></p>
<p>Author: Jasper Snoek, Ryan P. Adams, Hugo Larochelle</p><p>Abstract: While unsupervised learning has long been useful for density modeling, exploratory data analysis and visualization, it has become increasingly important for discovering features that will later be used for discriminative tasks. Discriminative algorithms often work best with highly-informative features; remarkably, such features can often be learned without the labels. One particularly effective way to perform such unsupervised learning has been to use autoencoder neural networks, which ﬁnd latent representations that are constrained but nevertheless informative for reconstruction. However, pure unsupervised learning with autoencoders can ﬁnd representations that may or may not be useful for the ultimate discriminative task. It is a continuing challenge to guide the training of an autoencoder so that it ﬁnds features which will be useful for predicting labels. Similarly, we often have a priori information regarding what statistical variation will be irrelevant to the ultimate discriminative task, and we would like to be able to use this for guidance as well. Although a typical strategy would be to include a parametric discriminative model as part of the autoencoder training, here we propose a nonparametric approach that uses a Gaussian process to guide the representation. By using a nonparametric model, we can ensure that a useful discriminative function exists for a given set of features, without explicitly instantiating it. We demonstrate the superiority of this guidance mechanism on four data sets, including a real-world application to rehabilitation research. We also show how our proposed approach can learn to explicitly ignore statistically signiﬁcant covariate information that is label-irrelevant, by evaluating on the small NORB image recognition problem in which pose and lighting labels are available. Keywords: autoencoder, gaussian process, gaussian process latent variable model, representation learning, unsupervised learning</p><p>2 0.080089681 <a title="78-tfidf-2" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>Author: Odalric-Ambrym Maillard, Rémi Munos</p><p>Abstract: We investigate a method for regression that makes use of a randomly generated subspace GP ⊂ F (of ﬁnite dimension P) of a given large (possibly inﬁnite) dimensional function space F , for example, L2 ([0, 1]d ; R). GP is deﬁned as the span of P random features that are linear combinations of a basis functions of F weighted by random Gaussian i.i.d. coefﬁcients. We show practical motivation for the use of this approach, detail the link that this random projections method share with RKHS and Gaussian objects theory and prove, both in deterministic and random design, approximation error bounds when searching for the best regression function in GP rather than in F , and derive excess risk bounds for a speciﬁc regression algorithm (least squares regression in GP ). This paper stresses the motivation to study such methods, thus the analysis developed is kept simple for explanations purpose and leaves room for future developments. Keywords: regression, random matrices, dimension reduction</p><p>3 0.079384588 <a title="78-tfidf-3" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>Author: Hugo Larochelle, Michael Mandel, Razvan Pascanu, Yoshua Bengio</p><p>Abstract: Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artiﬁcial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classiﬁers. We study the Classiﬁcation RBM (ClassRBM), a variant on the RBM adapted to the classiﬁcation setting. We study different strategies for training the ClassRBM and show that competitive classiﬁcation performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classiﬁcation problems, namely semi-supervised and multitask learning. Keywords: restricted Boltzmann machine, classiﬁcation, discriminative learning, generative learning</p><p>4 0.075541802 <a title="78-tfidf-4" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<p>Author: Chiwoo Park, Jianhua Z. Huang, Yu Ding</p><p>Abstract: This paper presents the Getting-started style documentation for the local and parallel computation toolbox for Gaussian process regression (GPLP), an open source software package written in Matlab (but also compatible with Octave). The working environment and the usage of the software package will be presented in this paper. Keywords: Gaussian process regression, domain decomposition method, partial independent conditional, bagging for Gaussian process, local probabilistic regression</p><p>5 0.061898638 <a title="78-tfidf-5" href="./jmlr-2012-Random_Search_for_Hyper-Parameter_Optimization.html">95 jmlr-2012-Random Search for Hyper-Parameter Optimization</a></p>
<p>Author: James Bergstra, Yoshua Bengio</p><p>Abstract: Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efÄ?Ĺš cient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to conÄ?Ĺš gure neural networks and deep belief networks. Compared with neural networks conÄ?Ĺš gured by a pure grid search, we Ä?Ĺš nd that random search over the same domain is able to Ä?Ĺš nd models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search Ä?Ĺš nds better models by effectively searching a larger, less promising conÄ?Ĺš guration space. Compared with deep belief networks conÄ?Ĺš gured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional conÄ?Ĺš guration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for conÄ?Ĺš guring algorithms for new data sets. Our analysis casts some light on why recent Ă˘&euro;&oelig;High ThroughputĂ˘&euro;? methods achieve surprising successĂ˘&euro;&rdquo;they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms. Keyword</p><p>6 0.051795695 <a title="78-tfidf-6" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>7 0.048061702 <a title="78-tfidf-7" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>8 0.045756567 <a title="78-tfidf-8" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>9 0.041865967 <a title="78-tfidf-9" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>10 0.038239803 <a title="78-tfidf-10" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>11 0.037901945 <a title="78-tfidf-11" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>12 0.028488681 <a title="78-tfidf-12" href="./jmlr-2012-Discriminative_Hierarchical_Part-based_Models_for_Human_Parsing_and_Action_Recognition.html">32 jmlr-2012-Discriminative Hierarchical Part-based Models for Human Parsing and Action Recognition</a></p>
<p>13 0.02804153 <a title="78-tfidf-13" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>14 0.027455948 <a title="78-tfidf-14" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>15 0.027073052 <a title="78-tfidf-15" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>16 0.026972992 <a title="78-tfidf-16" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>17 0.02658492 <a title="78-tfidf-17" href="./jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</a></p>
<p>18 0.024767034 <a title="78-tfidf-18" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>19 0.024263076 <a title="78-tfidf-19" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>20 0.024254628 <a title="78-tfidf-20" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.123), (1, 0.046), (2, 0.151), (3, -0.04), (4, 0.077), (5, -0.008), (6, 0.059), (7, 0.041), (8, -0.175), (9, 0.006), (10, 0.012), (11, 0.033), (12, -0.115), (13, 0.136), (14, -0.066), (15, 0.062), (16, -0.052), (17, 0.118), (18, 0.023), (19, 0.057), (20, 0.027), (21, -0.097), (22, 0.186), (23, -0.002), (24, 0.09), (25, 0.074), (26, 0.179), (27, -0.046), (28, -0.161), (29, -0.105), (30, 0.043), (31, -0.087), (32, -0.08), (33, 0.071), (34, 0.032), (35, 0.084), (36, -0.004), (37, -0.073), (38, 0.143), (39, -0.118), (40, 0.013), (41, -0.176), (42, 0.105), (43, 0.117), (44, 0.099), (45, -0.081), (46, 0.017), (47, 0.099), (48, -0.109), (49, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93451816 <a title="78-lsi-1" href="./jmlr-2012-Nonparametric_Guidance_of_Autoencoder_Representations_using_Label_Information.html">78 jmlr-2012-Nonparametric Guidance of Autoencoder Representations using Label Information</a></p>
<p>Author: Jasper Snoek, Ryan P. Adams, Hugo Larochelle</p><p>Abstract: While unsupervised learning has long been useful for density modeling, exploratory data analysis and visualization, it has become increasingly important for discovering features that will later be used for discriminative tasks. Discriminative algorithms often work best with highly-informative features; remarkably, such features can often be learned without the labels. One particularly effective way to perform such unsupervised learning has been to use autoencoder neural networks, which ﬁnd latent representations that are constrained but nevertheless informative for reconstruction. However, pure unsupervised learning with autoencoders can ﬁnd representations that may or may not be useful for the ultimate discriminative task. It is a continuing challenge to guide the training of an autoencoder so that it ﬁnds features which will be useful for predicting labels. Similarly, we often have a priori information regarding what statistical variation will be irrelevant to the ultimate discriminative task, and we would like to be able to use this for guidance as well. Although a typical strategy would be to include a parametric discriminative model as part of the autoencoder training, here we propose a nonparametric approach that uses a Gaussian process to guide the representation. By using a nonparametric model, we can ensure that a useful discriminative function exists for a given set of features, without explicitly instantiating it. We demonstrate the superiority of this guidance mechanism on four data sets, including a real-world application to rehabilitation research. We also show how our proposed approach can learn to explicitly ignore statistically signiﬁcant covariate information that is label-irrelevant, by evaluating on the small NORB image recognition problem in which pose and lighting labels are available. Keywords: autoencoder, gaussian process, gaussian process latent variable model, representation learning, unsupervised learning</p><p>2 0.58296043 <a title="78-lsi-2" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>Author: Hugo Larochelle, Michael Mandel, Razvan Pascanu, Yoshua Bengio</p><p>Abstract: Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artiﬁcial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classiﬁers. We study the Classiﬁcation RBM (ClassRBM), a variant on the RBM adapted to the classiﬁcation setting. We study different strategies for training the ClassRBM and show that competitive classiﬁcation performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classiﬁcation problems, namely semi-supervised and multitask learning. Keywords: restricted Boltzmann machine, classiﬁcation, discriminative learning, generative learning</p><p>3 0.5405829 <a title="78-lsi-3" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<p>Author: Chiwoo Park, Jianhua Z. Huang, Yu Ding</p><p>Abstract: This paper presents the Getting-started style documentation for the local and parallel computation toolbox for Gaussian process regression (GPLP), an open source software package written in Matlab (but also compatible with Octave). The working environment and the usage of the software package will be presented in this paper. Keywords: Gaussian process regression, domain decomposition method, partial independent conditional, bagging for Gaussian process, local probabilistic regression</p><p>4 0.39927804 <a title="78-lsi-4" href="./jmlr-2012-Random_Search_for_Hyper-Parameter_Optimization.html">95 jmlr-2012-Random Search for Hyper-Parameter Optimization</a></p>
<p>Author: James Bergstra, Yoshua Bengio</p><p>Abstract: Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efÄ?Ĺš cient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to conÄ?Ĺš gure neural networks and deep belief networks. Compared with neural networks conÄ?Ĺš gured by a pure grid search, we Ä?Ĺš nd that random search over the same domain is able to Ä?Ĺš nd models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search Ä?Ĺš nds better models by effectively searching a larger, less promising conÄ?Ĺš guration space. Compared with deep belief networks conÄ?Ĺš gured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional conÄ?Ĺš guration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for conÄ?Ĺš guring algorithms for new data sets. Our analysis casts some light on why recent Ă˘&euro;&oelig;High ThroughputĂ˘&euro;? methods achieve surprising successĂ˘&euro;&rdquo;they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms. Keyword</p><p>5 0.3716473 <a title="78-lsi-5" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>Author: Odalric-Ambrym Maillard, Rémi Munos</p><p>Abstract: We investigate a method for regression that makes use of a randomly generated subspace GP ⊂ F (of ﬁnite dimension P) of a given large (possibly inﬁnite) dimensional function space F , for example, L2 ([0, 1]d ; R). GP is deﬁned as the span of P random features that are linear combinations of a basis functions of F weighted by random Gaussian i.i.d. coefﬁcients. We show practical motivation for the use of this approach, detail the link that this random projections method share with RKHS and Gaussian objects theory and prove, both in deterministic and random design, approximation error bounds when searching for the best regression function in GP rather than in F , and derive excess risk bounds for a speciﬁc regression algorithm (least squares regression in GP ). This paper stresses the motivation to study such methods, thus the analysis developed is kept simple for explanations purpose and leaves room for future developments. Keywords: regression, random matrices, dimension reduction</p><p>6 0.36947805 <a title="78-lsi-6" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>7 0.30978253 <a title="78-lsi-7" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>8 0.28984734 <a title="78-lsi-8" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>9 0.23566681 <a title="78-lsi-9" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>10 0.21767254 <a title="78-lsi-10" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>11 0.21723466 <a title="78-lsi-11" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>12 0.20308758 <a title="78-lsi-12" href="./jmlr-2012-Sally%3A_A_Tool_for_Embedding_Strings_in_Vector_Spaces.html">102 jmlr-2012-Sally: A Tool for Embedding Strings in Vector Spaces</a></p>
<p>13 0.19213045 <a title="78-lsi-13" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>14 0.16924892 <a title="78-lsi-14" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>15 0.16707709 <a title="78-lsi-15" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>16 0.16622548 <a title="78-lsi-16" href="./jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering.html">101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</a></p>
<p>17 0.16488241 <a title="78-lsi-17" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>18 0.1616271 <a title="78-lsi-18" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>19 0.16103594 <a title="78-lsi-19" href="./jmlr-2012-Quantum_Set_Intersection_and_its_Application_to_Associative_Memory.html">93 jmlr-2012-Quantum Set Intersection and its Application to Associative Memory</a></p>
<p>20 0.15836088 <a title="78-lsi-20" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.015), (14, 0.034), (21, 0.02), (26, 0.047), (29, 0.028), (35, 0.042), (49, 0.03), (56, 0.014), (57, 0.013), (69, 0.023), (75, 0.05), (77, 0.012), (79, 0.013), (81, 0.445), (92, 0.049), (96, 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79686421 <a title="78-lda-1" href="./jmlr-2012-Nonparametric_Guidance_of_Autoencoder_Representations_using_Label_Information.html">78 jmlr-2012-Nonparametric Guidance of Autoencoder Representations using Label Information</a></p>
<p>Author: Jasper Snoek, Ryan P. Adams, Hugo Larochelle</p><p>Abstract: While unsupervised learning has long been useful for density modeling, exploratory data analysis and visualization, it has become increasingly important for discovering features that will later be used for discriminative tasks. Discriminative algorithms often work best with highly-informative features; remarkably, such features can often be learned without the labels. One particularly effective way to perform such unsupervised learning has been to use autoencoder neural networks, which ﬁnd latent representations that are constrained but nevertheless informative for reconstruction. However, pure unsupervised learning with autoencoders can ﬁnd representations that may or may not be useful for the ultimate discriminative task. It is a continuing challenge to guide the training of an autoencoder so that it ﬁnds features which will be useful for predicting labels. Similarly, we often have a priori information regarding what statistical variation will be irrelevant to the ultimate discriminative task, and we would like to be able to use this for guidance as well. Although a typical strategy would be to include a parametric discriminative model as part of the autoencoder training, here we propose a nonparametric approach that uses a Gaussian process to guide the representation. By using a nonparametric model, we can ensure that a useful discriminative function exists for a given set of features, without explicitly instantiating it. We demonstrate the superiority of this guidance mechanism on four data sets, including a real-world application to rehabilitation research. We also show how our proposed approach can learn to explicitly ignore statistically signiﬁcant covariate information that is label-irrelevant, by evaluating on the small NORB image recognition problem in which pose and lighting labels are available. Keywords: autoencoder, gaussian process, gaussian process latent variable model, representation learning, unsupervised learning</p><p>2 0.78522688 <a title="78-lda-2" href="./jmlr-2012-Mal-ID%3A_Automatic_Malware_Detection_Using_Common_Segment_Analysis_and_Meta-Features.html">63 jmlr-2012-Mal-ID: Automatic Malware Detection Using Common Segment Analysis and Meta-Features</a></p>
<p>Author: Gil Tahan, Lior Rokach, Yuval Shahar</p><p>Abstract: This paper proposes several novel methods, based on machine learning, to detect malware in executable ﬁles without any need for preprocessing, such as unpacking or disassembling. The basic method (Mal-ID) is a new static (form-based) analysis methodology that uses common segment analysis in order to detect malware ﬁles. By using common segment analysis, Mal-ID is able to discard malware parts that originate from benign code. In addition, Mal-ID uses a new kind of feature, termed meta-feature, to better capture the properties of the analyzed segments. Rather than using the entire ﬁle, as is usually the case with machine learning based techniques, the new approach detects malware on the segment level. This study also introduces two Mal-ID extensions that improve the Mal-ID basic method in various aspects. We rigorously evaluated Mal-ID and its two extensions with more than ten performance measures, and compared them to the highly rated boosted decision tree method under identical settings. The evaluation demonstrated that Mal-ID and the two Mal-ID extensions outperformed the boosted decision tree method in almost all respects. In addition, the results indicated that by extracting meaningful features, it is sufﬁcient to employ one simple detection rule for classifying executable ﬁles. Keywords: computer security, malware detection, common segment analysis, supervised learning</p><p>3 0.67145675 <a title="78-lda-3" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>Author: Uri Shalit, Daphna Weinshall, Gal Chechik</p><p>Abstract: When learning models that are represented in matrix forms, enforcing a low-rank constraint can dramatically improve the memory and run time complexity, while providing a natural regularization of the model. However, naive approaches to minimizing functions over the set of low-rank matrices are either prohibitively time consuming (repeated singular value decomposition of the matrix) or numerically unstable (optimizing a factored representation of the low-rank matrix). We build on recent advances in optimization over manifolds, and describe an iterative online learning procedure, consisting of a gradient step, followed by a second-order retraction back to the manifold. While the ideal retraction is costly to compute, and so is the projection operator that approximates it, we describe another retraction that can be computed efﬁciently. It has run time and memory complexity of O ((n + m)k) for a rank-k matrix of dimension m × n, when using an online procedure with rank-one gradients. We use this algorithm, L ORETA, to learn a matrix-form similarity measure over pairs of documents represented as high dimensional vectors. L ORETA improves the mean average precision over a passive-aggressive approach in a factorized model, and also improves over a full model trained on pre-selected features using the same memory requirements. We further adapt L ORETA to learn positive semi-deﬁnite low-rank matrices, providing an online algorithm for low-rank metric learning. L ORETA also shows consistent improvement over standard weakly supervised methods in a large (1600 classes and 1 million images, using ImageNet) multi-label image classiﬁcation task. Keywords: low rank, Riemannian manifolds, metric learning, retractions, multitask learning, online learning</p><p>4 0.35716951 <a title="78-lda-4" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>Author: Jun Zhu, Amr Ahmed, Eric P. Xing</p><p>Abstract: A supervised topic model can use side information such as ratings or labels associated with documents or images to discover more predictive low dimensional topical representations of the data. However, existing supervised topic models predominantly employ likelihood-driven objective functions for learning and inference, leaving the popular and potentially powerful max-margin principle unexploited for seeking predictive representations of data and more discriminative topic bases for the corpus. In this paper, we propose the maximum entropy discrimination latent Dirichlet allocation (MedLDA) model, which integrates the mechanism behind the max-margin prediction models (e.g., SVMs) with the mechanism behind the hierarchical Bayesian topic models (e.g., LDA) under a uniﬁed constrained optimization framework, and yields latent topical representations that are more discriminative and more suitable for prediction tasks such as document classiﬁcation or regression. The principle underlying the MedLDA formalism is quite general and can be applied for jointly max-margin and maximum likelihood learning of directed or undirected topic models when supervising side information is available. Efﬁcient variational methods for posterior inference and parameter estimation are derived and extensive empirical studies on several real data sets are also provided. Our experimental results demonstrate qualitatively and quantitatively that MedLDA could: 1) discover sparse and highly discriminative topical representations; 2) achieve state of the art prediction performance; and 3) be more efﬁcient than existing supervised topic models, especially for classiﬁcation. Keywords: supervised topic models, max-margin learning, maximum entropy discrimination, latent Dirichlet allocation, support vector machines</p><p>5 0.34807757 <a title="78-lda-5" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>Author: Hugo Larochelle, Michael Mandel, Razvan Pascanu, Yoshua Bengio</p><p>Abstract: Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artiﬁcial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classiﬁers. We study the Classiﬁcation RBM (ClassRBM), a variant on the RBM adapted to the classiﬁcation setting. We study different strategies for training the ClassRBM and show that competitive classiﬁcation performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classiﬁcation problems, namely semi-supervised and multitask learning. Keywords: restricted Boltzmann machine, classiﬁcation, discriminative learning, generative learning</p><p>6 0.32626927 <a title="78-lda-6" href="./jmlr-2012-Security_Analysis_of_Online_Centroid_Anomaly_Detection.html">104 jmlr-2012-Security Analysis of Online Centroid Anomaly Detection</a></p>
<p>7 0.30761379 <a title="78-lda-7" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>8 0.29566556 <a title="78-lda-8" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>9 0.29190555 <a title="78-lda-9" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>10 0.28940073 <a title="78-lda-10" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>11 0.2878685 <a title="78-lda-11" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>12 0.28319514 <a title="78-lda-12" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>13 0.28093201 <a title="78-lda-13" href="./jmlr-2012-Finding_Recurrent_Patterns_from_Continuous_Sign_Language_Sentences_for_Automated_Extraction_of_Signs.html">45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</a></p>
<p>14 0.27654833 <a title="78-lda-14" href="./jmlr-2012-Robust_Kernel_Density_Estimation.html">100 jmlr-2012-Robust Kernel Density Estimation</a></p>
<p>15 0.27357763 <a title="78-lda-15" href="./jmlr-2012-An_Introduction_to_Artificial_Prediction_Markets_for_Classification.html">19 jmlr-2012-An Introduction to Artificial Prediction Markets for Classification</a></p>
<p>16 0.27319825 <a title="78-lda-16" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>17 0.27213565 <a title="78-lda-17" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>18 0.26677671 <a title="78-lda-18" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>19 0.26250201 <a title="78-lda-19" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>20 0.25900695 <a title="78-lda-20" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
