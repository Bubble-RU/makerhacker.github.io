<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-11" href="#">jmlr2012-11</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</h1>
<br/><p>Source: <a title="jmlr-2012-11-pdf" href="http://jmlr.org/papers/volume13/lawrence12a/lawrence12a.pdf">pdf</a></p><p>Author: Neil D. Lawrence</p><p>Abstract: We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random ﬁelds (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter ﬁtting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random ﬁeld via the graphical lasso.</p><p>Reference: <a title="jmlr-2012-11-reference" href="../jmlr2012_reference/jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 UK  Department of Computer Science University of Shefﬁeld Regent Court Portobello S1 4DP United Kingdom  Editor: Tony Jebara  Abstract We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random ﬁelds (GRFs). [sent-6, score-0.251]
</p><p>2 Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. [sent-7, score-0.272]
</p><p>3 The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. [sent-8, score-0.379]
</p><p>4 We relate the model to Laplacian eigenmaps and isomap. [sent-9, score-0.287]
</p><p>5 We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. [sent-12, score-0.35]
</p><p>6 Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random ﬁeld via the graphical lasso. [sent-13, score-0.291]
</p><p>7 1 Spectral Dimensionality Reduction Spectral approaches to dimensionality reduction involve taking a data set containing n points and forming a matrix of size n × n from which eigenvectors are extracted to give a representation of the data in a low dimensional space. [sent-58, score-0.35]
</p><p>8 , 2000), locally linear embeddings (LLE, Roweis and Saul, 2000), Laplacian eigenmaps (Belkin and Niyogi, 2003) and maximum variance unfolding (MVU, Weinberger et al. [sent-60, score-0.554]
</p><p>9 In classical multidimensional scaling an n × n symmetric distance matrix, whose elements contain the distance between two data points, is converted to a similarity matrix and visualized through its principal eigenvectors. [sent-69, score-0.324]
</p><p>10 In this paper we introduce a probabilistic approach to constructing the distance matrix: maximum entropy unfolding (MEU). [sent-71, score-0.366]
</p><p>11 We describe how isomap, LLE, Laplacian eigenmaps and MVU are related to MEU using the unifying perspective of Gaussian random ﬁelds and CMDS. [sent-72, score-0.365]
</p><p>12 We show that the locally linear embedding algorithm is an approximation to maximum entropy unfolding where pseudolikelihood is maximized as an approximation to the model likelihood. [sent-75, score-0.507]
</p><p>13 We introduce an exact version of locally linear embedding based on an acyclic graph structure that maximizes the true model likelihood (acyclic locally linear embedding, ALLE). [sent-77, score-0.312]
</p><p>14 In Section 2 we derive our model through using standard assumptions from the ﬁeld of dimensionality reduction and the maximum entropy principle (Jaynes, 1986). [sent-89, score-0.286]
</p><p>15 We then relate the model to other popular spectral approaches for dimensionality reduction and show how the parameters of the model can be ﬁtted through maximum likelihood. [sent-90, score-0.252]
</p><p>16 One way of doing this is to associate a q dimensional latent vector with each data point, yi,: , and deﬁne a set of dissimilarities 2 between each latent point, δi, j = xi,: − x j,: 2 (where · 2 represents the L2-norm) to give a matrix ∆. [sent-98, score-0.251]
</p><p>17 Maximum Entropy Unfolding Classical multidimensional scaling provides the optimal linear transformation of the space in which the squared distances are expressed. [sent-112, score-0.223]
</p><p>18 The key contribution of recently developed spectral approaches in machine learning is to compute these distances in a space which is nonlinearly related to the data thereby ensuring a nonlinear dimensionality reduction algorithm. [sent-113, score-0.309]
</p><p>19 In kernel PCA the squared distances are computed between points o in a Hilbert space and related to the original data through a kernel function, di, j = k(yi,: , yi,: ) − 2k(yi,: , y j,: ) + k(y j,: , y j,: ). [sent-116, score-0.267]
</p><p>20 (2)  For the linear kernel function, k(yi,: , y j,: ) = y⊤ y j,: this reduces to the squared Euclidean distance, but for i,: 2  nonlinear kernel functions such as k(yi,: , y j,: ) = exp(−γ yi,: − y j,: 2 ) the distances are nonlinearly related to the data space. [sent-117, score-0.229]
</p><p>21 To exactly reconstruct the squared distances computed in feature space all but one of the eigenvectors of B need to be retained for our latent representation, X. [sent-132, score-0.323]
</p><p>22 If the dimensionality of the data, p, is smaller than the number of data points, n, then we have a latent representation for our data which has higher dimensionality than the original data. [sent-133, score-0.256]
</p><p>23 The observation that KPCA does not reduce the data dimensionality motivated the maximum variance unfolding algorithm (MVU, Weinberger et al. [sent-134, score-0.315]
</p><p>24 These distances are speciﬁed as constraints, and the other elements of the kernel matrix are ﬁlled in by maximizing its trace, tr (K), that is, the total variance of the data in feature space, while respecting the distance constraints and keeping the resulting matrix centered. [sent-143, score-0.373]
</p><p>25 Maximizing tr (K) maximizes the interpoint squared distances for all points that are unconnected in the neighborhood graph, thereby unravelling the manifold. [sent-144, score-0.415]
</p><p>26 Importantly, our interpretation will also enable us to relate our algorithm to other well known spectral techniques as they each turn out to approximate maximum entropy unfolding in some way. [sent-147, score-0.39]
</p><p>27 In our case the observations will be squared distances between data points, but we will derive a density over Y directly (not over the squared distances). [sent-153, score-0.287]
</p><p>28 However, since we do not construct a density over the squared distance matrix, modeling based on that information alone should be thought of as maximum entropy under distance constraints rather than maximum likelihood. [sent-158, score-0.445]
</p><p>29 In the maximum entropy formalism, we specify the density by a free form maximization of the entropy subject to the imposed expectation constraints. [sent-159, score-0.31]
</p><p>30 The constraints we use will correspond to the constraints applied to maximum variance unfolding: the expectations of the squared distances between two neighboring data points sampled from the model. [sent-160, score-0.364]
</p><p>31 However, the maximum entropy solution for this differential entropy turns out to be undeﬁned. [sent-166, score-0.25]
</p><p>32 We choose a base density to be a very broad, spherical, Gaussian density with covariance γ−1 I. [sent-171, score-0.221]
</p><p>33 The density 1613  L AWRENCE  that minimizes the KL divergence under the constraints on the expectations is then   1 1 p(Y) ∝ exp − tr γYY⊤ exp − ∑ ∑ λi, j di, j  , 2 2 i j∈N (i)  where N (i) represents the set of neighbors of data point i, and Y = [y1,: , . [sent-174, score-0.266]
</p><p>34 If each feature is constrained to only have K neighbors in the graph, then the inverse covariance (and correspondingly the covariance) is only parameterized by K p+ p parameters. [sent-215, score-0.242]
</p><p>35 4 Independence Over Data Features  The Gaussian Markov random ﬁeld (GRF) for maximum entropy unfolding is unusual in that the independence is being expressed over data features (in the p-dimensional direction) instead of over data points (in the n-dimensional direction). [sent-218, score-0.389]
</p><p>36 5 Maximum Likelihood and Blessing of Dimensionality Once the form of a maximum entropy density is determined, ﬁnding the Lagrange multipliers in the model is equivalent to maximizing the likelihood of the model, where the Lagrange multipliers are now considered to be parameters. [sent-233, score-0.38]
</p><p>37 This result is a consequence of the maximum entropy formulation: the Lagrange multipliers have a gradient of zero when the constraints are satisﬁed. [sent-283, score-0.244]
</p><p>38 To compute gradients we need the expectation of the squared distance given by di, j = y⊤ yi,: − 2 y⊤ y j,: + y⊤ y j,: , j,: i,: i,: which we can compute directly from the covariance matrix of the GRF, K = (L + γI)−1 , di, j =  p (ki,i − 2ki, j + k j, j ) . [sent-284, score-0.257]
</p><p>39 We use the notation ki, j to denote an element of such a covariance (or similarity matrix) and only use k(·, ·) notation when the value of the similarity matrix can be explicitly represented as a Mercer kernel. [sent-294, score-0.227]
</p><p>40 This reﬂects an insensitivity of the covariance matrix to the data mean, and this in turn arises because that information is lost when we specify the expectation constraints only through interpoint distances. [sent-298, score-0.294]
</p><p>41 ” Once the maximum likelihood solution is recovered the data can be visualized, as for MVU and kernel PCA, by looking at the eigenvectors of the centered covariance matrix HKH. [sent-306, score-0.383]
</p><p>42 The maximum variance unfolding (MVU) algorithm maximizes the trace of the covariance matrix (given by the sum of its eigenvalues, {λi }), n  tr (K) = ∑ λi , i=1  subject to constraints on the elements of K arising from the squared distances. [sent-309, score-0.527]
</p><p>43 Maximum variance unfolding and maximum entropy unfolding also provide non linear generalizations of PCA. [sent-326, score-0.496]
</p><p>44 For these algorithms, if we increase the neighborhood size to K = n − 1, then all squared distances implied by the GRF model are constrained to match the observed inter data point squared distances and L becomes non-sparse. [sent-327, score-0.387]
</p><p>45 Classical multidimensional scaling on the resulting squared distance matrix is known as principal coordinate analysis and is equivalent to principal component analysis (see Mardia et al. [sent-328, score-0.328]
</p><p>46 6 Relation to Laplacian Eigenmaps Laplacian eigenmaps is a spectral algorithm introduced by Belkin and Niyogi (2003). [sent-331, score-0.355]
</p><p>47 In Laplacian eigenmaps a symmetric sparse (possibly weighted) adjacency matrix, A ∈ ℜn×n , is deﬁned whose i, jth element, ai, j is non-zero if the ith and jth data points are neighbors. [sent-334, score-0.325]
</p><p>48 In this case CMDS proceeds by computing the eigendecomposition of the centred negative squared distance matrix, which is the eigendecomposition of the centred inner product matrix as is performed for principal coordinate analysis. [sent-336, score-0.379]
</p><p>49 4 i=1 j=1  For a multidimensional embedding we can rewrite this objective in terms of the squared distance between 2 two latent points, δi, j = xi,: − x j,: 2 , as E(X) =  1 n n ∑ ∑ ai, j δi, j . [sent-338, score-0.321]
</p><p>50 4 i=1 j=1  The motivation behind this objective function is that neighboring points have non-zero entries in the adjacency matrix, therefore their inter point squared distances in latent space need to be minimized. [sent-339, score-0.342]
</p><p>51 If the latent points are all placed on top of one another the interpoint distance matrix will be all zeros. [sent-349, score-0.3]
</p><p>52 Note that the generalized eigenvalue problem underlying Laplacian eigenmaps can be readily converted to the related, symmetric, eigenvalue problem. [sent-358, score-0.377]
</p><p>53 2 R ELATING L APLACIAN E IGENMAPS TO MEU The relationship of MEU to Laplacian eigenmaps is starting to become clear. [sent-366, score-0.287]
</p><p>54 In Laplacian eigenmaps a graph Laplacian is speciﬁed across the data points just as in maximum entropy unfolding. [sent-367, score-0.525]
</p><p>55 In classical multidimensional scaling, as applied in MEU and MVU, the eigenvectors associated with the largest eigenvalues of the centred covariance matrix, B = H (L + γI)−1 H (6) are used for visualization. [sent-368, score-0.328]
</p><p>56 In Laplacian eigenmaps the smallest eigenvectors of L are used, disregarding the eigenvector associated with the null space. [sent-369, score-0.41]
</p><p>57 Note that if we deﬁne the eigendecomposition of the covariance in the GRF as K = UΛU⊤ it is easy to show that the eigendecomposition of the associated Laplacian matrix is L = U Λ−1 − γI U⊤ . [sent-370, score-0.231]
</p><p>58 To make the analogy with Laplacian eigenmaps direct we consider the formulation of its eigenvalue problem with the normalized graph Laplacian as given in (4). [sent-376, score-0.384]
</p><p>59 Substituting the normalized graph Laplacian into our covariance matrix, K, we see that for Laplacian eigenmaps we are visualizing a Gaussian random ﬁeld with a covariance as follows, ˆ K = (L + γI)−1 . [sent-377, score-0.541]
</p><p>60 :,i :,i This shows the relationship between the eigenvalue problems for Laplacian eigenmaps and CMDS. [sent-380, score-0.332]
</p><p>61 However, in CMDS this would be removed by the centering operation and in Laplacian eigenmaps it is discarded. [sent-383, score-0.316]
</p><p>62 3 L APLACIAN E IGENMAPS S UMMARY The Laplacian eigenmaps procedure does not ﬁt parameters through maximum likelihood. [sent-387, score-0.333]
</p><p>63 The implied squared distance matrix used for CMDS will not preserve the interneighbor distances as it will for MVU and MEU. [sent-390, score-0.259]
</p><p>64 In 1620  U NIFYING S PECTRAL D IMENSIONALITY R EDUCTION  fact since the covariance matrix is never explicitly computed it is not possible to make speciﬁc statements about what these distances will be in general. [sent-391, score-0.254]
</p><p>65 However, Laplacian eigenmaps gains signiﬁcant computational advantage by not representing the covariance matrix explicitly. [sent-392, score-0.438]
</p><p>66 This means that Laplacian eigenmaps can be applied to much larger data sets than would be possible for MEU or MVU. [sent-394, score-0.287]
</p><p>67 7 Relation of MEU to Locally Linear Embedding The locally linear embedding (LLE Roweis and Saul, 2000) is a dimensionality reduction that was originally motivated by the idea that a non-linear manifold could be approximated by small linear patches. [sent-396, score-0.236]
</p><p>68 If the distance between data points is small relative to the curvature of the manifold at a particular point, then the manifold encircling a data point and its nearest neighbors may be approximated locally by a linear patch. [sent-397, score-0.241]
</p><p>69 :,i To facilitate the comparison with the maximum entropy unfolding algorithm we now introduce an alternative approach to enforcing translation invariance. [sent-409, score-0.322]
</p><p>70 This form of the objective also shows us that mi,i has the role of scaling each data point’s contribution to the overall objective function (rather like the degree, di,i would do in the unnormalized variant of Laplacian eigenmaps we discussed in Section 2. [sent-418, score-0.367]
</p><p>71 The key difference between LLE and Laplacian eigenmaps is the manner in which the Laplacian is parameterized. [sent-463, score-0.287]
</p><p>72 Indeed LLE turns out to be the speciﬁc case of maximum entropy unfolding where: 1. [sent-475, score-0.322]
</p><p>73 In maximum entropy unfolding the constraint arises because the regression weights are constrained to be w j,i /mi,i and mi,i = ∑ j∈N (i) w j,i . [sent-496, score-0.35]
</p><p>74 In the resulting directed acyclic graph the neighbors of each data point are its parents. [sent-516, score-0.217]
</p><p>75 This graph is then ﬁlled in for all non-neighboring points by ﬁnding the shortest distance between any two neighboring points in the graph (along the edges speciﬁed by the neighbors). [sent-565, score-0.257]
</p><p>76 The resulting matrix is then element-wise squared to give a matrix of square distances which is then processed in the usual manner (centering and multiplying by -0. [sent-566, score-0.265]
</p><p>77 For both MVU and MEU the covariance matrix, K, is guaranteed positive semideﬁnite because the distances are implied by an underlying covariance matrix that is constrained positive deﬁnite. [sent-572, score-0.355]
</p><p>78 For isomap the shortest path algorithm is effectively approximating the distances between non-neighboring points. [sent-573, score-0.242]
</p><p>79 The algorithm is still slower than LLE and Laplacian eigenmaps because it requires a dense eigenvalue problem and the application of a shortest path algorithm to the graph provided by the neighbors. [sent-576, score-0.384]
</p><p>80 Estimating Graph Structure The relationship between spectral dimensionality reduction algorithms and Gaussian random ﬁelds now leads us to consider a novel approach to dimensionality reduction. [sent-578, score-0.301]
</p><p>81 Before introducing the method, we need to ﬁrst re-derive the maximum entropy approach by constraining the second moment of neighboring data points to equal the empirical observation instead of the expected inter data point squared distances. [sent-584, score-0.281]
</p><p>82 j,: i,: i,: So the expected interpoint squared distance will match the empirically observed interpoint squared distance from the data. [sent-587, score-0.416]
</p><p>83 In other words, whilst we have formulated the constraints slightly differently, the ﬁnal model will respect the same interpoint squared distance constraints as our original formulation of maximum entropy unfolding. [sent-588, score-0.438]
</p><p>84 That will lead to an implied covariance matrix, K = (Λ + γI)−1 , which once again should be centred, B = HKH, and the principal eigenvectors extracted to visualize the embedding. [sent-591, score-0.25]
</p><p>85 There is a clear difference in quality between the methods that constrain local distances (ALLE, MVU, isomap, MEU and DRILL) which are much better under the score than those that do not (Laplacian eigenmaps and LLE). [sent-626, score-0.39]
</p><p>86 1629  L AWRENCE  2  1  0  -1 -2  -1  0  1  (a) MEU  2  1  0  -1 -2  -1  0  1  (b) Acyclic LLE  2  1  0  -1 -2  -1  0  1  (c) DRILL  Figure 4: Motion capture data visualized in two dimensions for models derived from the maximum entropy perspective. [sent-633, score-0.218]
</p><p>87 Discussion and Conclusions We have introduced a new perspective on dimensionality reduction algorithms based around maximum entropy. [sent-663, score-0.229]
</p><p>88 Our starting point was the maximum variance unfolding and our end point was a novel approach to dimensionality reduction based on Gaussian random ﬁelds and lasso based structure learning. [sent-664, score-0.358]
</p><p>89 Interpoint distances between neighbors are fed to the algorithms which provide a similarity matrix. [sent-673, score-0.253]
</p><p>90 The relationship between points in the similarity matrix is visualized using the eigenvectors of the similarity matrix. [sent-676, score-0.289]
</p><p>91 The main difference between the different approaches to spectral dimensionality reduction is how the entries of the similarity matrix are determined. [sent-682, score-0.294]
</p><p>92 Maximum variance unfolding looks to maximize the trace under the distance constraints from the neighbours. [sent-683, score-0.259]
</p><p>93 Laplacian eigenmaps parameterize the inverse similarity through appealing to physical analogies. [sent-686, score-0.354]
</p><p>94 Finally, isomap uses shortest path algorithms to compute interpoint distances and centres the resulting matrix to give the similarities. [sent-687, score-0.394]
</p><p>95 (a) the motion capture data visualizations, (b) the robot navigation example visualizations. [sent-693, score-0.228]
</p><p>96 Using the fact that the principal eigenvectors of the similarity are the minor eigenvalues of the Laplacian and exploiting fast eigensolvers that act on sparse matrices very large data sets can be addressed. [sent-730, score-0.221]
</p><p>97 1 Gaussian Process Latent Variable Models Finally, there are similarities between maximum entropy unfolding and the Gaussian process latent variable model (GP-LVM). [sent-733, score-0.388]
</p><p>98 Maximum entropy unfolding leads to a Gauss Markov Random ﬁeld, where the conditional dependencies are between neighbors. [sent-737, score-0.276]
</p><p>99 If such a covariance was deﬁned in a GP-LVM with a one dimensional latent space k(x, x′ ) = exp(− x − x′ 1 ) then the inverse covariance will be sparse with only the nearest neighbors in the one dimensional latent space connected. [sent-740, score-0.539]
</p><p>100 The elements of the inverse covariance would be dependent on the distance between the two latent points, which in the GP-LVM is optimized as part of the training procedure. [sent-741, score-0.24]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lle', 0.426), ('meu', 0.324), ('laplacian', 0.324), ('eigenmaps', 0.287), ('mvu', 0.213), ('unfolding', 0.174), ('awrence', 0.148), ('isomap', 0.139), ('drill', 0.13), ('nifying', 0.13), ('neighbors', 0.112), ('eduction', 0.111), ('imensionality', 0.111), ('yy', 0.11), ('distances', 0.103), ('entropy', 0.102), ('cmds', 0.102), ('interpoint', 0.102), ('covariance', 0.101), ('pectral', 0.1), ('dimensionality', 0.095), ('eigenvectors', 0.092), ('pseudolikelihood', 0.087), ('alle', 0.083), ('motion', 0.078), ('grf', 0.074), ('mm', 0.07), ('spectral', 0.068), ('latent', 0.066), ('robot', 0.066), ('tenenbaum', 0.064), ('squared', 0.062), ('likelihood', 0.062), ('density', 0.06), ('multidimensional', 0.058), ('lagrange', 0.058), ('neighborhood', 0.057), ('principal', 0.057), ('wifi', 0.056), ('multipliers', 0.055), ('yi', 0.054), ('visualization', 0.054), ('tr', 0.053), ('acyclic', 0.053), ('graph', 0.052), ('embedding', 0.051), ('matrix', 0.05), ('belkin', 0.05), ('navigation', 0.047), ('locally', 0.047), ('mardia', 0.046), ('maximum', 0.046), ('perspective', 0.045), ('eigenvalue', 0.045), ('distance', 0.044), ('saul', 0.043), ('centred', 0.043), ('reduction', 0.043), ('eld', 0.042), ('gaussian', 0.042), ('constraints', 0.041), ('wi', 0.041), ('objective', 0.04), ('eigendecomposition', 0.04), ('kemp', 0.04), ('kl', 0.039), ('similarity', 0.038), ('points', 0.038), ('capture', 0.037), ('dissimilarities', 0.037), ('niyogi', 0.037), ('ham', 0.036), ('eigenvalues', 0.034), ('pca', 0.034), ('unifying', 0.033), ('diagonal', 0.033), ('visualized', 0.033), ('neighboring', 0.033), ('dimensional', 0.032), ('formalism', 0.032), ('animals', 0.032), ('kpca', 0.032), ('kernel', 0.032), ('eigenvector', 0.031), ('roweis', 0.031), ('lawrence', 0.029), ('centering', 0.029), ('inverse', 0.029), ('independence', 0.029), ('cliques', 0.029), ('const', 0.029), ('constraint', 0.028), ('doi', 0.028), ('aplacian', 0.028), ('blessing', 0.028), ('hkh', 0.028), ('igenmaps', 0.028), ('neil', 0.027), ('di', 0.027), ('recover', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="11-tfidf-1" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random ﬁelds (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter ﬁtting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random ﬁeld via the graphical lasso.</p><p>2 0.069025837 <a title="11-tfidf-2" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>Author: Prateek Jain, Brian Kulis, Jason V. Davis, Inderjit S. Dhillon</p><p>Abstract: Metric and kernel learning arise in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem. In particular, we propose a general optimization framework for learning metrics via linear transformations, and analyze in detail a special case of our framework—that of minimizing the LogDet divergence subject to linear constraints. We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework. Our theoretical connections between metric and kernel learning have two main consequences: 1) the learned kernel matrix parameterizes a linear transformation kernel function and can be applied inductively to new data points, 2) our result yields a constructive method for kernelizing most existing Mahalanobis metric learning formulations. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction. Keywords: divergence metric learning, kernel learning, linear transformation, matrix divergences, logdet</p><p>3 0.067942642 <a title="11-tfidf-3" href="./jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</a></p>
<p>Author: Rahul Mazumder,  Trevor Hastie</p><p>Abstract: We consider the sparse inverse covariance regularization problem or graphical lasso with regularization parameter λ. Suppose the sample covariance graph formed by thresholding the entries of the sample covariance matrix at λ is decomposed into connected components. We show that the vertex-partition induced by the connected components of the thresholded sample covariance graph (at λ) is exactly equal to that induced by the connected components of the estimated concentration graph, obtained by solving the graphical lasso problem for the same λ. This characterizes a very interesting property of a path of graphical lasso solutions. Furthermore, this simple rule, when used as a wrapper around existing algorithms for the graphical lasso, leads to enormous performance gains. For a range of values of λ, our proposal splits a large graphical lasso problem into smaller tractable problems, making it possible to solve an otherwise infeasible large-scale problem. We illustrate the graceful scalability of our proposal via synthetic and real-life microarray examples. Keywords: sparse inverse covariance selection, sparsity, graphical lasso, Gaussian graphical models, graph connected components, concentration graph, large scale covariance estimation</p><p>4 0.061399013 <a title="11-tfidf-4" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>Author: Animashree Anandkumar, Vincent Y.F. Tan, Furong Huang, Alan S. Willsky</p><p>Abstract: We consider the problem of high-dimensional Gaussian graphical model selection. We identify a set of graphs for which an efﬁcient estimation algorithm exists, and this algorithm is based on thresholding of empirical conditional covariances. Under a set of transparent conditions, we establish structural consistency (or sparsistency) for the proposed algorithm, when the number of −2 samples n = Ω(Jmin log p), where p is the number of variables and Jmin is the minimum (absolute) edge potential of the graphical model. The sufﬁcient conditions for sparsistency are based on the notion of walk-summability of the model and the presence of sparse local vertex separators in the underlying graph. We also derive novel non-asymptotic necessary conditions on the number of samples required for sparsistency. Keywords: Gaussian graphical model selection, high-dimensional learning, local-separation property, walk-summability, necessary conditions for model selection</p><p>5 0.059761818 <a title="11-tfidf-5" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>Author: Michael U. Gutmann, Aapo Hyvärinen</p><p>Abstract: We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a ﬁnite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only speciﬁed up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artiﬁcially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities. Keywords: statistics unnormalized models, partition function, computation, estimation, natural image</p><p>6 0.058032382 <a title="11-tfidf-6" href="./jmlr-2012-Human_Gesture_Recognition_on_Product_Manifolds.html">50 jmlr-2012-Human Gesture Recognition on Product Manifolds</a></p>
<p>7 0.056601286 <a title="11-tfidf-7" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>8 0.053418897 <a title="11-tfidf-8" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>9 0.052327048 <a title="11-tfidf-9" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>10 0.051795695 <a title="11-tfidf-10" href="./jmlr-2012-Nonparametric_Guidance_of_Autoencoder_Representations_using_Label_Information.html">78 jmlr-2012-Nonparametric Guidance of Autoencoder Representations using Label Information</a></p>
<p>11 0.049546145 <a title="11-tfidf-11" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>12 0.046748891 <a title="11-tfidf-12" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>13 0.045106031 <a title="11-tfidf-13" href="./jmlr-2012-A_Local_Spectral_Method_for_Graphs%3A_With_Applications_to_Improving_Graph_Partitions_and_Exploring_Data_Graphs_Locally.html">5 jmlr-2012-A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally</a></p>
<p>14 0.044504303 <a title="11-tfidf-14" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>15 0.044200655 <a title="11-tfidf-15" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>16 0.042366371 <a title="11-tfidf-16" href="./jmlr-2012-Feature_Selection_via_Dependence_Maximization.html">44 jmlr-2012-Feature Selection via Dependence Maximization</a></p>
<p>17 0.04205934 <a title="11-tfidf-17" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>18 0.040457569 <a title="11-tfidf-18" href="./jmlr-2012-Robust_Kernel_Density_Estimation.html">100 jmlr-2012-Robust Kernel Density Estimation</a></p>
<p>19 0.037429478 <a title="11-tfidf-19" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>20 0.03557292 <a title="11-tfidf-20" href="./jmlr-2012-The_huge_Package_for_High-dimensional_Undirected_Graph_Estimation_in_R.html">113 jmlr-2012-The huge Package for High-dimensional Undirected Graph Estimation in R</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.182), (1, 0.077), (2, 0.155), (3, -0.022), (4, 0.005), (5, 0.036), (6, -0.051), (7, -0.011), (8, -0.033), (9, -0.095), (10, 0.1), (11, -0.003), (12, -0.192), (13, 0.02), (14, -0.041), (15, 0.03), (16, 0.052), (17, -0.004), (18, 0.148), (19, 0.104), (20, 0.022), (21, 0.059), (22, 0.01), (23, 0.003), (24, 0.002), (25, 0.02), (26, 0.001), (27, -0.054), (28, -0.152), (29, -0.004), (30, 0.06), (31, 0.025), (32, -0.122), (33, -0.07), (34, 0.051), (35, -0.187), (36, 0.116), (37, -0.106), (38, 0.01), (39, 0.079), (40, -0.071), (41, -0.258), (42, -0.119), (43, -0.064), (44, -0.084), (45, -0.126), (46, -0.006), (47, -0.055), (48, 0.086), (49, -0.198)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91982371 <a title="11-lsi-1" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random ﬁelds (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter ﬁtting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random ﬁeld via the graphical lasso.</p><p>2 0.56969976 <a title="11-lsi-2" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>Author: Dominik Schnitzer, Arthur Flexer, Markus Schedl, Gerhard Widmer</p><p>Abstract: ‘Hubness’ has recently been identiﬁed as a general problem of high dimensional data spaces, manifesting itself in the emergence of objects, so-called hubs, which tend to be among the k nearest neighbors of a large number of data items. As a consequence many nearest neighbor relations in the distance space are asymmetric, that is, object y is amongst the nearest neighbors of x but not vice versa. The work presented here discusses two classes of methods that try to symmetrize nearest neighbor relations and investigates to what extent they can mitigate the negative effects of hubs. We evaluate local distance scaling and propose a global variant which has the advantage of being easy to approximate for large data sets and of having a probabilistic interpretation. Both local and global approaches are shown to be effective especially for high-dimensional data sets, which are affected by high hubness. Both methods lead to a strong decrease of hubness in these data sets, while at the same time improving properties like classiﬁcation accuracy. We evaluate the methods on a large number of public machine learning data sets and synthetic data. Finally we present a real-world application where we are able to achieve signiﬁcantly higher retrieval quality. Keywords: local and global scaling, shared near neighbors, hubness, classiﬁcation, curse of dimensionality, nearest neighbor relation</p><p>3 0.46652415 <a title="11-lsi-3" href="./jmlr-2012-A_Local_Spectral_Method_for_Graphs%3A_With_Applications_to_Improving_Graph_Partitions_and_Exploring_Data_Graphs_Locally.html">5 jmlr-2012-A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally</a></p>
<p>Author: Michael W. Mahoney, Lorenzo Orecchia, Nisheeth K. Vishnoi</p><p>Abstract: The second eigenvalue of the Laplacian matrix and its associated eigenvector are fundamental features of an undirected graph, and as such they have found widespread use in scientiﬁc computing, machine learning, and data analysis. In many applications, however, graphs that arise have several local regions of interest, and the second eigenvector will typically fail to provide information ﬁne-tuned to each local region. In this paper, we introduce a locally-biased analogue of the second eigenvector, and we demonstrate its usefulness at highlighting local properties of data graphs in a semi-supervised manner. To do so, we ﬁrst view the second eigenvector as the solution to a constrained optimization problem, and we incorporate the local information as an additional constraint; we then characterize the optimal solution to this new problem and show that it can be interpreted as a generalization of a Personalized PageRank vector; and ﬁnally, as a consequence, we show that the solution can be computed in nearly-linear time. In addition, we show that this locally-biased vector can be used to compute an approximation to the best partition near an input seed set in a manner analogous to the way in which the second eigenvector of the Laplacian can be used to obtain an approximation to the best partition in the entire input graph. Such a primitive is useful for identifying and reﬁning clusters locally, as it allows us to focus on a local region of interest in a semi-supervised manner. Finally, we provide a detailed empirical evaluation of our method by showing how it can applied to ﬁnding locally-biased sparse cuts around an input vertex seed set in social and information networks. Keywords: spectral graph partitioning, local spectral algorithms, Laplacian matrix, semi-supervised learning, personalized pagerank c 2012 Michael W. Mahoney and Lorenzo Orecchia and Nisheeth K. Vishnoi. M AHONEY, O RECCHIA , AND V ISHNOI</p><p>4 0.44999665 <a title="11-lsi-4" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>Author: Prateek Jain, Brian Kulis, Jason V. Davis, Inderjit S. Dhillon</p><p>Abstract: Metric and kernel learning arise in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem. In particular, we propose a general optimization framework for learning metrics via linear transformations, and analyze in detail a special case of our framework—that of minimizing the LogDet divergence subject to linear constraints. We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework. Our theoretical connections between metric and kernel learning have two main consequences: 1) the learned kernel matrix parameterizes a linear transformation kernel function and can be applied inductively to new data points, 2) our result yields a constructive method for kernelizing most existing Mahalanobis metric learning formulations. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction. Keywords: divergence metric learning, kernel learning, linear transformation, matrix divergences, logdet</p><p>5 0.36549005 <a title="11-lsi-5" href="./jmlr-2012-Nonparametric_Guidance_of_Autoencoder_Representations_using_Label_Information.html">78 jmlr-2012-Nonparametric Guidance of Autoencoder Representations using Label Information</a></p>
<p>Author: Jasper Snoek, Ryan P. Adams, Hugo Larochelle</p><p>Abstract: While unsupervised learning has long been useful for density modeling, exploratory data analysis and visualization, it has become increasingly important for discovering features that will later be used for discriminative tasks. Discriminative algorithms often work best with highly-informative features; remarkably, such features can often be learned without the labels. One particularly effective way to perform such unsupervised learning has been to use autoencoder neural networks, which ﬁnd latent representations that are constrained but nevertheless informative for reconstruction. However, pure unsupervised learning with autoencoders can ﬁnd representations that may or may not be useful for the ultimate discriminative task. It is a continuing challenge to guide the training of an autoencoder so that it ﬁnds features which will be useful for predicting labels. Similarly, we often have a priori information regarding what statistical variation will be irrelevant to the ultimate discriminative task, and we would like to be able to use this for guidance as well. Although a typical strategy would be to include a parametric discriminative model as part of the autoencoder training, here we propose a nonparametric approach that uses a Gaussian process to guide the representation. By using a nonparametric model, we can ensure that a useful discriminative function exists for a given set of features, without explicitly instantiating it. We demonstrate the superiority of this guidance mechanism on four data sets, including a real-world application to rehabilitation research. We also show how our proposed approach can learn to explicitly ignore statistically signiﬁcant covariate information that is label-irrelevant, by evaluating on the small NORB image recognition problem in which pose and lighting labels are available. Keywords: autoencoder, gaussian process, gaussian process latent variable model, representation learning, unsupervised learning</p><p>6 0.35569152 <a title="11-lsi-6" href="./jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</a></p>
<p>7 0.35334951 <a title="11-lsi-7" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>8 0.32117683 <a title="11-lsi-8" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>9 0.31943056 <a title="11-lsi-9" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>10 0.31075788 <a title="11-lsi-10" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>11 0.28838837 <a title="11-lsi-11" href="./jmlr-2012-Human_Gesture_Recognition_on_Product_Manifolds.html">50 jmlr-2012-Human Gesture Recognition on Product Manifolds</a></p>
<p>12 0.28671473 <a title="11-lsi-12" href="./jmlr-2012-Feature_Selection_via_Dependence_Maximization.html">44 jmlr-2012-Feature Selection via Dependence Maximization</a></p>
<p>13 0.27615002 <a title="11-lsi-13" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>14 0.26484606 <a title="11-lsi-14" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>15 0.26181883 <a title="11-lsi-15" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>16 0.25158063 <a title="11-lsi-16" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>17 0.25097209 <a title="11-lsi-17" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>18 0.24162795 <a title="11-lsi-18" href="./jmlr-2012-Robust_Kernel_Density_Estimation.html">100 jmlr-2012-Robust Kernel Density Estimation</a></p>
<p>19 0.22409007 <a title="11-lsi-19" href="./jmlr-2012-EP-GIG_Priors_and_Applications_in_Bayesian_Sparse_Learning.html">35 jmlr-2012-EP-GIG Priors and Applications in Bayesian Sparse Learning</a></p>
<p>20 0.22158691 <a title="11-lsi-20" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.012), (16, 0.324), (21, 0.034), (26, 0.074), (27, 0.013), (29, 0.043), (35, 0.027), (49, 0.026), (56, 0.015), (57, 0.02), (64, 0.021), (69, 0.034), (75, 0.06), (77, 0.012), (79, 0.017), (81, 0.015), (92, 0.076), (96, 0.112)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7179656 <a title="11-lda-1" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random ﬁelds (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter ﬁtting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random ﬁeld via the graphical lasso.</p><p>2 0.56973743 <a title="11-lda-2" href="./jmlr-2012-Finite-Sample_Analysis_of_Least-Squares_Policy_Iteration.html">46 jmlr-2012-Finite-Sample Analysis of Least-Squares Policy Iteration</a></p>
<p>Author: Alessandro Lazaric, Mohammad Ghavamzadeh, Rémi Munos</p><p>Abstract: In this paper, we report a performance bound for the widely used least-squares policy iteration (LSPI) algorithm. We ﬁrst consider the problem of policy evaluation in reinforcement learning, that is, learning the value function of a ﬁxed policy, using the least-squares temporal-difference (LSTD) learning method, and report ﬁnite-sample analysis for this algorithm. To do so, we ﬁrst derive a bound on the performance of the LSTD solution evaluated at the states generated by the Markov chain and used by the algorithm to learn an estimate of the value function. This result is general in the sense that no assumption is made on the existence of a stationary distribution for the Markov chain. We then derive generalization bounds in the case when the Markov chain possesses a stationary distribution and is β-mixing. Finally, we analyze how the error at each policy evaluation step is propagated through the iterations of a policy iteration method, and derive a performance bound for the LSPI algorithm. Keywords: Markov decision processes, reinforcement learning, least-squares temporal-difference, least-squares policy iteration, generalization bounds, ﬁnite-sample analysis</p><p>3 0.46620095 <a title="11-lda-3" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel</p><p>Abstract: The success of many machine learning and pattern recognition methods relies heavily upon the identiﬁcation of an appropriate distance metric on the input data. It is often beneﬁcial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semideﬁnite. Semideﬁnite programming is often used to enforce this constraint, but does not scale well and is not easy to implement. B OOST M ETRIC is instead based on the observation that any positive semideﬁnite matrix can be decomposed into a linear combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting methods are easy to implement, efﬁcient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semideﬁnite matrix with trace and rank being one rather than a classiﬁer or regressor. Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. Keywords: Mahalanobis distance, semideﬁnite programming, column generation, boosting, Lagrange duality, large margin nearest neighbor</p><p>4 0.45895547 <a title="11-lda-4" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>Author: Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, Lin Xiao</p><p>Abstract: Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment. We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem. Keywords: distributed computing, online learning, stochastic optimization, regret bounds, convex optimization</p><p>5 0.44977155 <a title="11-lda-5" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>Author: Sangkyun Lee, Stephen J. Wright</p><p>Abstract: Iterative methods that calculate their steps from approximate subgradient directions have proved to be useful for stochastic learning problems over large and streaming data sets. When the objective consists of a loss function plus a nonsmooth regularization term, the solution often lies on a lowdimensional manifold of parameter space along which the regularizer is smooth. (When an ℓ1 regularizer is used to induce sparsity in the solution, for example, this manifold is deﬁned by the set of nonzero components of the parameter vector.) This paper shows that a regularized dual averaging algorithm can identify this manifold, with high probability, before reaching the solution. This observation motivates an algorithmic strategy in which, once an iterate is suspected of lying on an optimal or near-optimal manifold, we switch to a “local phase” that searches in this manifold, thus converging rapidly to a near-optimal point. Computational results are presented to verify the identiﬁcation property and to illustrate the effectiveness of this approach. Keywords: regularization, dual averaging, partly smooth manifold, manifold identiﬁcation</p><p>6 0.44884592 <a title="11-lda-6" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>7 0.4481594 <a title="11-lda-7" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>8 0.4448812 <a title="11-lda-8" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>9 0.44374734 <a title="11-lda-9" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>10 0.44307214 <a title="11-lda-10" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>11 0.44241613 <a title="11-lda-11" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>12 0.44069162 <a title="11-lda-12" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>13 0.43958539 <a title="11-lda-13" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>14 0.43911445 <a title="11-lda-14" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>15 0.43900126 <a title="11-lda-15" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>16 0.43863025 <a title="11-lda-16" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>17 0.4383567 <a title="11-lda-17" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>18 0.43798554 <a title="11-lda-18" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>19 0.43672165 <a title="11-lda-19" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>20 0.43570006 <a title="11-lda-20" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
