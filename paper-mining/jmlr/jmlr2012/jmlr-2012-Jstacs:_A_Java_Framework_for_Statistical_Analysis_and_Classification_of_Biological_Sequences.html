<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 jmlr-2012-Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-53" href="#">jmlr2012-53</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>53 jmlr-2012-Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences</h1>
<br/><p>Source: <a title="jmlr-2012-53-pdf" href="http://jmlr.org/papers/volume13/grau12a/grau12a.pdf">pdf</a></p><p>Author: Jan Grau, Jens Keilwagen, André Gohr, Berit Haldemann, Stefan Posch, Ivo Grosse</p><p>Abstract: Jstacs is an object-oriented Java library for analysing and classifying sequence data, which emerged from the need for a standardized implementation of statistical models, learning principles, classiﬁers, and performance measures. In Jstacs, these components can be used, combined, and extended easily, which allows for a direct comparison of different approaches and fosters the development of new components. Jstacs is especially tailored to biological sequence data, but is also applicable to general discrete and continuous data. Jstacs is freely available at http://www.jstacs.de under the GNU GPL license including an API documentation, a cookbook, and code examples. Keywords: machine learning, statistical models, Java, bioinformatics, classiﬁcation</p><p>Reference: <a title="jmlr-2012-53-reference" href="../jmlr2012_reference/jmlr-2012-Jstacs%3A_A_Java_Framework_for_Statistical_Analysis_and_Classification_of_Biological_Sequences_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Jstacs is especially tailored to biological sequence data, but is also applicable to general discrete and continuous data. [sent-20, score-0.065]
</p><p>2 Introduction During the last years, machine learning techniques have gained an increasing importance in many ﬁelds of science including bioinformatics and computational biology. [sent-25, score-0.033]
</p><p>3 e  G RAU , K EILWAGEN , G OHR , H ALDEMANN , P OSCH AND G ROSSE  One critical step in assessing their relevance is the comparison to existing methods. [sent-30, score-0.046]
</p><p>4 Similar to JavaML, Jstacs is mainly targeted at developers who want to use the library in their own code. [sent-37, score-0.025]
</p><p>5 One then combines these models to a classiﬁer, chooses a learning principle for learning the parameters of this classiﬁer, and learns this classiﬁer on training data. [sent-39, score-0.044]
</p><p>6 For assessing the performance of the classiﬁer, one can choose an evaluation schema like cross-validation and choose different performance measures. [sent-41, score-0.09]
</p><p>7 At each level, Jstacs deﬁnes interfaces and abstract classes to standardize and ease development, and to achieve modularity. [sent-43, score-0.117]
</p><p>8 Jstacs has been applied to diverse biological problems such as prediction of transcription factor binding sites and splice sites, de-novo motif discovery, analysis of gene expression and Array-CGH data, and classiﬁcation based on ﬂow cytometry data. [sent-45, score-0.428]
</p><p>9 In the following section, we describe the general structure, essential interfaces, and abstract classes of Jstacs. [sent-46, score-0.029]
</p><p>10 The most prevalent alphabet in Jstacs is the DNAAlphabet, while more general implementations can be used for instance to deﬁne a three-letter amino acid alphabet. [sent-50, score-0.033]
</p><p>11 Sequences are deﬁned using such alphabets, while DataSets comprise a collection of sequences over the same alphabet. [sent-51, score-0.031]
</p><p>12 DataSets are constructed either from an existing array of sequences or from a ﬁle. [sent-52, score-0.031]
</p><p>13 For these interfaces, Jstacs provides abstract classes with standard implementations of many of the speciﬁed methods to reduce implementation effort as well as factory classes enabling user-friendly creation of many standard models. [sent-56, score-0.091]
</p><p>14 Current implementations include inhomogeneous and homogeneous Markov models, Bayesian networks, hidden Markov models, and mixture models accepting any TrainableStatisticalModel as mixture components. [sent-59, score-0.131]
</p><p>15 The interfaces are colored red, abstract classes blue, enums orange, and concrete classes green without preceeding modiﬁer. [sent-61, score-0.146]
</p><p>16 Continuous transitions represent inheritance, where arrows indicate the direction of inheritance. [sent-62, score-0.033]
</p><p>17 Arrows with diamond heads represent usage of a type in the class at the arrow head. [sent-63, score-0.024]
</p><p>18 In contrast, DifferentiableStatisticalModels provide methods tailored to numerical optimization like the computation of gradients with respect to their parameters. [sent-64, score-0.028]
</p><p>19 In addition, a ZOOPS1 model for de-novo motif discovery is implemented in ExtendedZOOPSDiffSM, which will be the topic of the case study. [sent-66, score-0.229]
</p><p>20 AbstractClassifiers provide methods for learning internal StatisticalModels on training data from different classes and for classifying new input sequences. [sent-67, score-0.057]
</p><p>21 The GenDisMixClassifier performs a simultaneous numerical parameter estimation for the enclosed DifferentiableStatisticalModel, for instance by maximum supervised posterior (MSP) (Gr¨ nwald et al. [sent-69, score-0.076]
</p><p>22 , 2002; Cerquides and de M´ ntaras, 2005), or a u a uniﬁed learning principle (GenDisMix) (Keilwagen et al. [sent-70, score-0.044]
</p><p>23 ClassifierAssessments can be used for assessing the performance of any AbstractClassifier, for example by k-fold cross validation or repeated holdout sampling. [sent-72, score-0.046]
</p><p>24 Case Study In this section, we describe how we used Jstacs for developing Dispom, a new application for denovo motif discovery (Keilwagen et al. [sent-75, score-0.256]
</p><p>25 The right side shows the concrete classes used in the application. [sent-81, score-0.029]
</p><p>26 differ in the employed learning principle and in the capability of learning the positional preference of motif occurrences. [sent-82, score-0.337]
</p><p>27 However, prior to Dispom, no approach existed for learning the motif and the positional preference simultaneously using a discriminative learning principle. [sent-83, score-0.364]
</p><p>28 The general structure of Dispom in Jstacs is depicted on the left side of Figure 2, where each white piece represents a slot that can be ﬁlled with implementations of interfaces deﬁned in Jstacs. [sent-84, score-0.121]
</p><p>29 The motif, ﬂanking, and background model are DifferentiableStatisticalModels, the position distribution is a DurationDiffSM, and the learning principle is a value from an enum type. [sent-85, score-0.117]
</p><p>30 In the Dispom application illustrated on the right side of Figure 2, we use an inhomogeneous Markov model of order 0 with a mixture over the DNA-strands as motif model. [sent-86, score-0.274]
</p><p>31 We use homogeneous Markov models of order 0 for both the ﬂanking and background model. [sent-87, score-0.025]
</p><p>32 All of these models existed before we started developing Dispom. [sent-88, score-0.067]
</p><p>33 We use a mixture of a skew normal and a uniform distribution as position distribution, and the discriminative MSP learning principle. [sent-89, score-0.096]
</p><p>34 This modular structure allowed for an easy adaption to other problems like challenge 2 of DREAM52 on protein binding microarray data, where we simply increased the orders of the motif, ﬂanking, and background model, and extended the learning principle to a weighted variant of the MSP principle. [sent-90, score-0.194]
</p><p>35 These minimal changes were all that was needed for developing a novel application for the analysis of protein binding microarrays and a successful performance in the challenge. [sent-91, score-0.152]
</p><p>36 Later, JK joined as a main developer, and AG contributed occasionally. [sent-94, score-0.071]
</p><p>37 BH contributed to parts of Jstacs and to documentation. [sent-95, score-0.071]
</p><p>38 All authors contributed to writing and approved the ﬁnal manuscript. [sent-96, score-0.071]
</p><p>39 De-novo discovery of differentially abundant transcription factor binding sites including their positional preference. [sent-124, score-0.283]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('jstacs', 0.621), ('halle', 0.282), ('motif', 0.201), ('keilwagen', 0.198), ('diffsm', 0.141), ('dispom', 0.141), ('grau', 0.141), ('ivo', 0.141), ('jens', 0.141), ('posch', 0.141), ('extendedzoopsdiffsm', 0.113), ('informatik', 0.1), ('binding', 0.097), ('stefan', 0.094), ('interfaces', 0.088), ('anking', 0.087), ('differentiablestatisticalmodel', 0.085), ('differentiablestatisticalmodels', 0.085), ('gendismix', 0.085), ('gohr', 0.085), ('haldemann', 0.085), ('luther', 0.085), ('msp', 0.085), ('saale', 0.085), ('strickert', 0.085), ('trainablestatisticalmodel', 0.085), ('wittenberg', 0.085), ('contributed', 0.071), ('jan', 0.07), ('positional', 0.065), ('abeel', 0.056), ('abstractclassifier', 0.056), ('aldemann', 0.056), ('alphabets', 0.056), ('berit', 0.056), ('cerquides', 0.056), ('eilwagen', 0.056), ('gatersleben', 0.056), ('grosse', 0.056), ('ipk', 0.056), ('javaml', 0.056), ('ohr', 0.056), ('osch', 0.056), ('petri', 0.056), ('rau', 0.056), ('rosse', 0.056), ('stacs', 0.056), ('statisticalmodel', 0.056), ('trainablestatisticalmodels', 0.056), ('er', 0.055), ('sites', 0.053), ('datasets', 0.053), ('java', 0.053), ('germany', 0.053), ('martin', 0.05), ('enum', 0.048), ('jg', 0.048), ('inhomogeneous', 0.048), ('nwald', 0.048), ('ag', 0.048), ('marc', 0.048), ('assessing', 0.046), ('principle', 0.044), ('schema', 0.044), ('andr', 0.044), ('plant', 0.044), ('interface', 0.042), ('skew', 0.04), ('existed', 0.04), ('transcription', 0.04), ('shogun', 0.037), ('uni', 0.037), ('biological', 0.037), ('arrows', 0.033), ('christian', 0.033), ('implementations', 0.033), ('bioinformatics', 0.033), ('markov', 0.032), ('ministry', 0.032), ('classi', 0.032), ('sequences', 0.031), ('discriminative', 0.031), ('classes', 0.029), ('posterior', 0.028), ('classifying', 0.028), ('protein', 0.028), ('tailored', 0.028), ('gr', 0.028), ('discovery', 0.028), ('berlin', 0.028), ('preference', 0.027), ('developing', 0.027), ('mixture', 0.025), ('background', 0.025), ('library', 0.025), ('unter', 0.024), ('myllym', 0.024), ('inheritance', 0.024), ('crop', 0.024), ('heads', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="53-tfidf-1" href="./jmlr-2012-Jstacs%3A_A_Java_Framework_for_Statistical_Analysis_and_Classification_of_Biological_Sequences.html">53 jmlr-2012-Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences</a></p>
<p>Author: Jan Grau, Jens Keilwagen, André Gohr, Berit Haldemann, Stefan Posch, Ivo Grosse</p><p>Abstract: Jstacs is an object-oriented Java library for analysing and classifying sequence data, which emerged from the need for a standardized implementation of statistical models, learning principles, classiﬁers, and performance measures. In Jstacs, these components can be used, combined, and extended easily, which allows for a direct comparison of different approaches and fosters the development of new components. Jstacs is especially tailored to biological sequence data, but is also applicable to general discrete and continuous data. Jstacs is freely available at http://www.jstacs.de under the GNU GPL license including an API documentation, a cookbook, and code examples. Keywords: machine learning, statistical models, Java, bioinformatics, classiﬁcation</p><p>2 0.047251321 <a title="53-tfidf-2" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>3 0.029007303 <a title="53-tfidf-3" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>Author: Stephen Gould</p><p>Abstract: We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data ﬂows. Keywords: machine learning, graphical models, computer vision, open-source software</p><p>4 0.025550274 <a title="53-tfidf-4" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>Author: Wojciech Rejchel</p><p>Abstract: The problem of ranking is to predict or to guess the ordering between objects on the basis of their observed features. In this paper we consider ranking estimators that minimize the empirical convex risk. We prove generalization bounds for the excess risk of such estimators with rates that are 1 faster than √n . We apply our results to commonly used ranking algorithms, for instance boosting or support vector machines. Moreover, we study the performance of considered estimators on real data sets. Keywords: convex risk minimization, excess risk, support vector machine, empirical process, U-process</p><p>5 0.023755703 <a title="53-tfidf-5" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>Author: Kay H. Brodersen, Christoph Mathys, Justin R. Chumbley, Jean Daunizeau, Cheng Soon Ong, Joachim M. Buhmann, Klaas E. Stephan</p><p>Abstract: Classiﬁcation algorithms are frequently used on data with a natural hierarchical structure. For instance, classiﬁers are often trained and tested on trial-wise measurements, separately for each subject within a group. One important question is how classiﬁcation outcomes observed in individual subjects can be generalized to the population from which the group was sampled. To address this question, this paper introduces novel statistical models that are guided by three desiderata. First, all models explicitly respect the hierarchical nature of the data, that is, they are mixed-effects models that simultaneously account for within-subjects (ﬁxed-effects) and across-subjects (random-effects) variance components. Second, maximum-likelihood estimation is replaced by full Bayesian inference in order to enable natural regularization of the estimation problem and to afford conclusions in terms of posterior probability statements. Third, inference on classiﬁcation accuracy is complemented by inference on the balanced accuracy, which avoids inﬂated accuracy estimates for imbalanced data sets. We introduce hierarchical models that satisfy these criteria and demonstrate their advantages over conventional methods using MCMC implementations for model inversion and model selection on both synthetic and empirical data. We envisage that our approach will improve the sensitivity and validity of statistical inference in future hierarchical classiﬁcation studies. Keywords: beta-binomial, normal-binomial, balanced accuracy, Bayesian inference, group studies</p><p>6 0.023259915 <a title="53-tfidf-6" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>7 0.021801842 <a title="53-tfidf-7" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>8 0.019282622 <a title="53-tfidf-8" href="./jmlr-2012-Sally%3A_A_Tool_for_Embedding_Strings_in_Vector_Spaces.html">102 jmlr-2012-Sally: A Tool for Embedding Strings in Vector Spaces</a></p>
<p>9 0.018319279 <a title="53-tfidf-9" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>10 0.017114023 <a title="53-tfidf-10" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>11 0.015903179 <a title="53-tfidf-11" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>12 0.015260842 <a title="53-tfidf-12" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>13 0.014563221 <a title="53-tfidf-13" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>14 0.013647195 <a title="53-tfidf-14" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>15 0.01364022 <a title="53-tfidf-15" href="./jmlr-2012-PAC-Bayes_Bounds_with_Data_Dependent_Priors.html">87 jmlr-2012-PAC-Bayes Bounds with Data Dependent Priors</a></p>
<p>16 0.013601675 <a title="53-tfidf-16" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>17 0.013591146 <a title="53-tfidf-17" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>18 0.012431335 <a title="53-tfidf-18" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>19 0.011808326 <a title="53-tfidf-19" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>20 0.011684543 <a title="53-tfidf-20" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.055), (1, 0.022), (2, 0.079), (3, -0.008), (4, 0.03), (5, 0.016), (6, 0.054), (7, 0.016), (8, -0.028), (9, 0.007), (10, -0.012), (11, -0.041), (12, 0.095), (13, -0.048), (14, -0.041), (15, 0.005), (16, 0.031), (17, -0.027), (18, -0.036), (19, -0.117), (20, 0.029), (21, 0.003), (22, -0.004), (23, 0.06), (24, -0.033), (25, -0.045), (26, 0.007), (27, -0.124), (28, 0.036), (29, 0.032), (30, 0.005), (31, -0.018), (32, 0.051), (33, 0.078), (34, 0.018), (35, -0.072), (36, -0.202), (37, -0.13), (38, -0.057), (39, -0.008), (40, -0.017), (41, -0.066), (42, -0.208), (43, 0.059), (44, 0.576), (45, 0.147), (46, -0.413), (47, 0.053), (48, 0.064), (49, -0.145)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9729442 <a title="53-lsi-1" href="./jmlr-2012-Jstacs%3A_A_Java_Framework_for_Statistical_Analysis_and_Classification_of_Biological_Sequences.html">53 jmlr-2012-Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences</a></p>
<p>Author: Jan Grau, Jens Keilwagen, André Gohr, Berit Haldemann, Stefan Posch, Ivo Grosse</p><p>Abstract: Jstacs is an object-oriented Java library for analysing and classifying sequence data, which emerged from the need for a standardized implementation of statistical models, learning principles, classiﬁers, and performance measures. In Jstacs, these components can be used, combined, and extended easily, which allows for a direct comparison of different approaches and fosters the development of new components. Jstacs is especially tailored to biological sequence data, but is also applicable to general discrete and continuous data. Jstacs is freely available at http://www.jstacs.de under the GNU GPL license including an API documentation, a cookbook, and code examples. Keywords: machine learning, statistical models, Java, bioinformatics, classiﬁcation</p><p>2 0.37200952 <a title="53-lsi-2" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>3 0.30866572 <a title="53-lsi-3" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>Author: Stephen Gould</p><p>Abstract: We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data ﬂows. Keywords: machine learning, graphical models, computer vision, open-source software</p><p>4 0.21913488 <a title="53-lsi-4" href="./jmlr-2012-Sally%3A_A_Tool_for_Embedding_Strings_in_Vector_Spaces.html">102 jmlr-2012-Sally: A Tool for Embedding Strings in Vector Spaces</a></p>
<p>Author: Konrad Rieck, Christian Wressnegger, Alexander Bikadorov</p><p>Abstract: Strings and sequences are ubiquitous in many areas of data analysis. However, only few learning methods can be directly applied to this form of data. We present Sally, a tool for embedding strings in vector spaces that allows for applying a wide range of learning methods to string data. Sally implements a generalized form of the bag-of-words model, where strings are mapped to a vector space that is spanned by a set of string features, such as words or n-grams of words. The implementation of Sally builds on efﬁcient string algorithms and enables processing millions of strings and features. The tool supports several data formats and is capable of interfacing with common learning environments, such as Weka, Shogun, Matlab, or Pylab. Sally has been successfully applied for learning with natural language text, DNA sequences and monitored program behavior. Keywords: string embedding, bag-of-words models, learning with sequential data</p><p>5 0.17174321 <a title="53-lsi-5" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>Author: Hannes Nickisch</p><p>Abstract: The glm-ie toolbox contains functionality for estimation and inference in generalised linear models over continuous-valued variables. Besides a variety of penalised least squares solvers for estimation, it offers inference based on (convex) variational bounds, on expectation propagation and on factorial mean ﬁeld. Scalable and efﬁcient inference in fully-connected undirected graphical models or Markov random ﬁelds with Gaussian and non-Gaussian potentials is achieved by casting all the computations as matrix vector multiplications. We provide a wide choice of penalty functions for estimation, potential functions for inference and matrix classes with lazy evaluation for convenient modelling. We designed the glm-ie package to be simple, generic and easily expansible. Most of the code is written in Matlab including some MEX ﬁles to be fully compatible to both Matlab 7.x and GNU Octave 3.3.x. Large scale probabilistic classiﬁcation as well as sparse linear modelling can be performed in a common algorithmical framework by the glm-ie toolkit. Keywords: sparse linear models, generalised linear models, Bayesian inference, approximate inference, probabilistic regression and classiﬁcation, penalised least squares estimation, lazy evaluation matrix class</p><p>6 0.15685076 <a title="53-lsi-6" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>7 0.14687668 <a title="53-lsi-7" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>8 0.13671893 <a title="53-lsi-8" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>9 0.12102052 <a title="53-lsi-9" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>10 0.1095788 <a title="53-lsi-10" href="./jmlr-2012-Learning_Linear_Cyclic_Causal_Models_with_Latent_Variables.html">56 jmlr-2012-Learning Linear Cyclic Causal Models with Latent Variables</a></p>
<p>11 0.10731931 <a title="53-lsi-11" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<p>12 0.10065184 <a title="53-lsi-12" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>13 0.097967096 <a title="53-lsi-13" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>14 0.087064467 <a title="53-lsi-14" href="./jmlr-2012-Exploration_in_Relational_Domains_for_Model-based_Reinforcement_Learning.html">41 jmlr-2012-Exploration in Relational Domains for Model-based Reinforcement Learning</a></p>
<p>15 0.085638605 <a title="53-lsi-15" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>16 0.083926961 <a title="53-lsi-16" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>17 0.079793438 <a title="53-lsi-17" href="./jmlr-2012-Characterization_and_Greedy_Learning_of_Interventional_Markov_Equivalence_Classes_of_Directed_Acyclic_Graphs.html">25 jmlr-2012-Characterization and Greedy Learning of Interventional Markov Equivalence Classes of Directed Acyclic Graphs</a></p>
<p>18 0.078364745 <a title="53-lsi-18" href="./jmlr-2012-Eliminating_Spammers_and_Ranking_Annotators_for_Crowdsourced_Labeling_Tasks.html">37 jmlr-2012-Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks</a></p>
<p>19 0.077234037 <a title="53-lsi-19" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>20 0.072235771 <a title="53-lsi-20" href="./jmlr-2012-Query_Strategies_for_Evading_Convex-Inducing_Classifiers.html">94 jmlr-2012-Query Strategies for Evading Convex-Inducing Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(21, 0.031), (26, 0.027), (27, 0.025), (29, 0.022), (35, 0.019), (49, 0.029), (56, 0.049), (57, 0.01), (68, 0.572), (75, 0.029), (79, 0.017), (92, 0.024), (96, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84170288 <a title="53-lda-1" href="./jmlr-2012-Jstacs%3A_A_Java_Framework_for_Statistical_Analysis_and_Classification_of_Biological_Sequences.html">53 jmlr-2012-Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences</a></p>
<p>Author: Jan Grau, Jens Keilwagen, André Gohr, Berit Haldemann, Stefan Posch, Ivo Grosse</p><p>Abstract: Jstacs is an object-oriented Java library for analysing and classifying sequence data, which emerged from the need for a standardized implementation of statistical models, learning principles, classiﬁers, and performance measures. In Jstacs, these components can be used, combined, and extended easily, which allows for a direct comparison of different approaches and fosters the development of new components. Jstacs is especially tailored to biological sequence data, but is also applicable to general discrete and continuous data. Jstacs is freely available at http://www.jstacs.de under the GNU GPL license including an API documentation, a cookbook, and code examples. Keywords: machine learning, statistical models, Java, bioinformatics, classiﬁcation</p><p>2 0.32065928 <a title="53-lda-2" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>Author: Stephen Gould</p><p>Abstract: We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data ﬂows. Keywords: machine learning, graphical models, computer vision, open-source software</p><p>3 0.13052079 <a title="53-lda-3" href="./jmlr-2012-Fast_Approximation_of_Matrix_Coherence_and_Statistical_Leverage.html">43 jmlr-2012-Fast Approximation of Matrix Coherence and Statistical Leverage</a></p>
<p>Author: Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, David P. Woodruff</p><p>Abstract: The statistical leverage scores of a matrix A are the squared row-norms of the matrix containing its (top) left singular vectors and the coherence is the largest leverage score. These quantities are of interest in recently-popular problems such as matrix completion and Nystr¨ m-based low-rank o matrix approximation as well as in large-scale statistical data analysis applications more generally; moreover, they are of interest since they deﬁne the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms. Our main result is a randomized algorithm that takes as input an arbitrary n × d matrix A, with n ≫ d, and that returns as output relative-error approximations to all n of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of n and d) in O(nd log n) time, as opposed to the O(nd 2 ) time required by the na¨ve algorithm that involves computing an orthogonal basis for the ı range of A. Our analysis may be viewed in terms of computing a relative-error approximation to an underconstrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with n ≈ d, and the extension to streaming environments. Keywords: matrix coherence, statistical leverage, randomized algorithm</p><p>4 0.12594202 <a title="53-lda-4" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>5 0.12580144 <a title="53-lda-5" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>Author: Trinh Minh Tri Do, Thierry Artières</p><p>Abstract: Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efﬁcient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efﬁcient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random ﬁelds, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy ε with a rate O(1/λε) where λ is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difﬁcult and requires a stronger and more disputable assumption. Yet we provide experimental results on artiﬁcial test problems, and on ﬁve standard and difﬁcult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms. Keywords: optimization, non-convex, non-smooth, cutting plane, bundle method, regularized risk</p><p>6 0.12076239 <a title="53-lda-6" href="./jmlr-2012-On_the_Convergence_Rate_oflp-Norm_Multiple_Kernel_Learning.html">81 jmlr-2012-On the Convergence Rate oflp-Norm Multiple Kernel Learning</a></p>
<p>7 0.1176281 <a title="53-lda-7" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>8 0.11745609 <a title="53-lda-8" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>9 0.1166226 <a title="53-lda-9" href="./jmlr-2012-EP-GIG_Priors_and_Applications_in_Bayesian_Sparse_Learning.html">35 jmlr-2012-EP-GIG Priors and Applications in Bayesian Sparse Learning</a></p>
<p>10 0.11466096 <a title="53-lda-10" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>11 0.11436087 <a title="53-lda-11" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>12 0.11411534 <a title="53-lda-12" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>13 0.11407913 <a title="53-lda-13" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>14 0.11379743 <a title="53-lda-14" href="./jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</a></p>
<p>15 0.11374768 <a title="53-lda-15" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<p>16 0.1135021 <a title="53-lda-16" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>17 0.11271676 <a title="53-lda-17" href="./jmlr-2012-Security_Analysis_of_Online_Centroid_Anomaly_Detection.html">104 jmlr-2012-Security Analysis of Online Centroid Anomaly Detection</a></p>
<p>18 0.11268148 <a title="53-lda-18" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>19 0.11255479 <a title="53-lda-19" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>20 0.11244574 <a title="53-lda-20" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
