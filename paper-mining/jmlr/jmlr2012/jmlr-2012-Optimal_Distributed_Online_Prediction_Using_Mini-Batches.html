<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-85" href="#">jmlr2012-85</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</h1>
<br/><p>Source: <a title="jmlr-2012-85-pdf" href="http://jmlr.org/papers/volume13/dekel12a/dekel12a.pdf">pdf</a></p><p>Author: Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, Lin Xiao</p><p>Abstract: Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment. We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem. Keywords: distributed computing, online learning, stochastic optimization, regret bounds, convex optimization</p><p>Reference: <a title="jmlr-2012-85-reference" href="../jmlr2012_reference/jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. [sent-8, score-0.731]
</p><p>2 We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. [sent-9, score-0.646]
</p><p>3 Keywords: distributed computing, online learning, stochastic optimization, regret bounds, convex optimization  1. [sent-13, score-0.688]
</p><p>4 After making the prediction wi , we observe zi and suffer the loss f (wi , zi ), where f is a predeﬁned loss function. [sent-26, score-0.702]
</p><p>5 Since regret relies on the stochastic inputs zi , it is a random variable. [sent-33, score-0.7]
</p><p>6 It is well-known that the optimal regret bound √ that can be achieved by a gradient-based serial algorithm on an arbitrary convex loss is O( m) (e. [sent-57, score-0.911]
</p><p>7 At the other extreme, a trivial solution to our problem is to have each node operate in isolation of the other k−1 nodes, running an independent copy of a serial algorithm, without any communication over 166  O PTIMAL D ISTRIBUTED O NLINE P REDICTION  the network. [sent-61, score-0.554]
</p><p>8 More speciﬁcally, assuming that each node processes m/k inputs, the expected regret per node is √ √ O( m/k). [sent-64, score-0.515]
</p><p>9 ı In this paper, we present the distributed mini-batch (DMB) algorithm, a method of converting any serial gradient-based online prediction algorithm into a parallel or distributed algorithm. [sent-67, score-0.78]
</p><p>10 This method has two important properties: • It can use any gradient-based update rule for serial online prediction as a black box, and convert it into a parallel or distributed online prediction algorithm. [sent-68, score-0.965]
</p><p>11 Moreover, the coefﬁcient √ of the dominant term m is the same as in the serial bound, and independent of k and of the network topology. [sent-70, score-0.531]
</p><p>12 The idea of using mini-batches in stochastic and online learning is not new, and has been previously explored in both the serial and parallel settings (see, e. [sent-71, score-0.718]
</p><p>13 Our results build on the fact that the optimal regret bound for serial stochastic gradient-based prediction algorithms can be reﬁned if the loss function is smooth. [sent-77, score-1.047]
</p><p>14 In particular, it can be shown √ that the hidden coefﬁcient in the O( m) notation is proportional to the standard deviation of the stochastic gradients evaluated at each predictor wi (Juditsky et al. [sent-78, score-0.557]
</p><p>15 However, the non-negligible communication latencies prevent a straightforward parallel implementation from obtaining the optimal serial regret bound. [sent-81, score-0.896]
</p><p>16 In Section 2, we present a template for stochastic gradientbased serial prediction algorithms, and state reﬁned variance-based regret bounds for smooth loss functions. [sent-84, score-1.08]
</p><p>17 In Section 3, we analyze the effect of using mini-batches in the serial setting, and show that it does not signiﬁcantly affect the regret bounds. [sent-85, score-0.765]
</p><p>18 In Section 4, we present the DMB algorithm, and show that it achieves an asymptotically optimal serial regret bound for smooth loss functions. [sent-86, score-0.895]
</p><p>19 For example, if the network communication operates over a minimum-depth spanning tree and the diameter of the network scales as log(k), then we can show that a straightforward implementation of the idea of parallel variance reduction leads to an O m log(k) regret bound. [sent-90, score-0.637]
</p><p>20 167  D EKEL , G ILAD -BACHRACH , S HAMIR AND X IAO  Algorithm 1: Template for a serial ﬁrst-order stochastic online prediction algorithm. [sent-92, score-0.719]
</p><p>21 Variance Bounds for Serial Algorithms Before discussing distributed algorithms, we must fully understand the serial algorithms on which they are based. [sent-106, score-0.509]
</p><p>22 α j i=1  (4)  For stochastic online prediction problems with convex loss functions, both of these update rules √ √ have expected regret bound of O( m). [sent-125, score-0.879]
</p><p>23 In particular, for the projected stochastic gradient method deﬁned in Equation (2), we have the following result: Theorem 1 Let f (w, z) be an L-smooth convex loss function in w for each z ∈ Z and assume that the stochastic gradient ∇w f (w, z) has σ2 -bounded variance for all w ∈ W . [sent-140, score-0.52]
</p><p>24 Although we focus on expected regret bounds here, our results can equally be stated as high-probability bounds on the actual regret (see Appendix B for details). [sent-149, score-0.747]
</p><p>25 For each of these update rules, we get an expected regret bound that closely resembles the bound in Equation (6). [sent-157, score-0.531]
</p><p>26 Therefore, each node suffers an expected regret of at most ψ(σ2 , ⌈m/k⌉) on its portion of the input stream, and the total regret bound is obtain by simply summing over the k nodes, that is, E[R(m)] ≤ k ψ σ2 ,  m k  . [sent-185, score-0.819]
</p><p>27 k  √ Comparing this bound to 2D2 L + 2Dσ m in the ideal serial solution, we see that it is approximately √ k times worse in its leading term. [sent-187, score-0.506]
</p><p>28 Serial Online Prediction using Mini-Batches The expected regret bounds presented in the previous section depend on the variance of the stochastic gradients. [sent-191, score-0.572]
</p><p>29 Before we present the distributed mini-batch algorithm in the next section, we ﬁrst analyze a serial mini-batch algorithm. [sent-193, score-0.509]
</p><p>30 We deﬁne the serial mini-batch algorithm as follows: Our prediction remains constant for the duration of each batch, and is updated only when a batch ends. [sent-197, score-0.634]
</p><p>31 Once a batch ends, the serial mini-batch algorithm feeds g j to the update rule φ as the jth gradient and obtains the new prediction ¯ for the next batch and the new state. [sent-200, score-0.947]
</p><p>32 The appeal of the serial mini-batch setting is that the update rule is used less frequently, which may have computational beneﬁts. [sent-202, score-0.546]
</p><p>33 If the update rule φ has the serial regret bound ψ(σ2 , m), then the expected regret of Algorithm 2 over m inputs is at most bψ  σ2 m , b b  . [sent-204, score-1.399]
</p><p>34 Proof Assume without loss of generality that b divides m, and that the serial mini-batch algorithm processes exactly m/b complete batches. [sent-206, score-0.526]
</p><p>35 Therefore the serial ¯ ¯ mini-batch algorithm is equivalent to using the update rule φ with the loss function f¯. [sent-226, score-0.59]
</p><p>36 If the ¯ update rule φ has a regret bound ψ(σ2 , m) for the loss function f over m inputs, then its regret for f¯ over m/b batches is bounded as m/b  E  ∑  j=1  f¯(w j , z j ) − f¯(w⋆ , z j ) ¯ ¯  ≤ ψ  σ2 m . [sent-237, score-0.905]
</p><p>37 √ The bound in Theorem 3 is asymptotically equivalent to the 2D2 L + 2Dσ m regret bound for the basic serial algorithms presented in Section 2. [sent-241, score-0.903]
</p><p>38 In other words, performing the mini-batch update in the serial setting does not signiﬁcantly hurt the performance of the update rule. [sent-242, score-0.571]
</p><p>39 On the other hand, it is also not surprising that using mini-batches in the serial setting does not improve the regret bound. [sent-243, score-0.765]
</p><p>40 Nevertheless, our experiments demonstrate that in real-world scenarios, mini-batching can in fact have a very substantial positive effect on the transient performance of the online prediction algorithm, even in the serial setting (see Section 6 for details). [sent-246, score-0.583]
</p><p>41 This technique resembles the serial mini-batch technique described earlier, and is therefore called the distributed mini-batch algorithm, or DMB for short. [sent-271, score-0.509]
</p><p>42 Once all of the nodes have the overall average g j , each node updates the predictor using the same deterministic serial ¯ algorithm. [sent-306, score-0.63]
</p><p>43 If the update rule φ has the serial regret bound ψ(σ2 , m), then the expected regret of Algorithm 3 over m samples is at most (b + µ) ψ  σ2 m , b b+µ  . [sent-316, score-1.287]
</p><p>44 √ Speciﬁcally, if ψ(σ2 , m) = 2D2 L + 2Dσ m, then setting the batch size b = m1/3 gives the expected regret bound √ √ 1 1 1 2Dσ m + 2Dm /3 (LD + σ µ) + 2Dσm /6 + 2Dσµm− /6 + 2µD2 L. [sent-317, score-0.56]
</p><p>45 To appreciate the power of this result, we compare the speciﬁc bound in Equation (8) with the ideal serial solution and the na¨ve no-communication solution discussed in the introduction. [sent-319, score-0.506]
</p><p>46 It ı is clear that our bound is asymptotically equivalent to the ideal serial bound ψ(σ2 , m)—even the constants in the dominant terms are identical. [sent-320, score-0.615]
</p><p>47 2 Improving Performance on Short Input Streams Theorem 4 presents an optimal way of choosing the batch size b, which results in an asymptotically optimal regret bound. [sent-352, score-0.513]
</p><p>48 The batch size should indeed be set such that the generated trafﬁc does not exceed the network bandwidth limit, but the latency of each sum operation should not be affected by the fact that multiple sum operations take place at once. [sent-367, score-0.499]
</p><p>49 Each instance of the algorithm handles m/c inputs and suffers a regret of at most bψ  σ2 m ,1+ b bc  ,  and, using Jensen’s inequality, the overall regret using the average prediction is upper bounded by bc ψ  σ2 m ,1+ b bc  . [sent-372, score-0.822]
</p><p>50 m Therefore, a bound on the expected optimality gap can be readily obtained from a bound on the expected regret of the same algorithm. [sent-403, score-0.565]
</p><p>51 In particular, if f is an L-smooth convex loss function and ∇w f (w, z) has σ2 -bounded variance, and our algorithm has a regret bound of ψ(σ2 , m), then it also has an expected optimality gap of at most E[G(m)] ≤  1 ψ(σ2 , m) . [sent-404, score-0.578]
</p><p>52 m √ For the speciﬁc regret bound ψ(σ2 , m) = 2D2 L + 2Dσ m, which holds for the serial algorithms presented in Section 2, we have ¯ ψ(σ2 , m) =  ¯ E[G(m)] ≤ ψ(σ2 , m) =  2D2 L 2Dσ +√ . [sent-405, score-0.817]
</p><p>53 If the update rule φ used ¯ in a serial setting has an expected optimality gap bounded by ψ(σ2 , m), then the expected optimality gap of Algorithm 4 after processing m samples is at most ¯ ψ ¯ If ψ(σ2 , m) =  2D2 L m  σ2 m , b b  . [sent-422, score-0.75]
</p><p>54 Since we can run the workload associated with a single batch in parallel, this theorem shows that the mini-batch technique is capable of turning many serial optimization algorithms into parallel ones. [sent-433, score-0.67]
</p><p>55 Suppose the update rule φ 2 √ ¯ used in the serial setting has an expected optimality gap bounded by ψ(σ2 , m) = 2D L + 2Dσ . [sent-458, score-0.648]
</p><p>56 ε→0  Proof By solving the equation 2D2 L 2Dσ + √ =ε, m m we see that the following number of samples is sufﬁcient for the serial algorithm to reach εoptimality: D2 σ2 msrl (ε) = ε2  2Lε 1+ 2 σ  1+  2  . [sent-462, score-0.511]
</p><p>57 6 5 10  6  10  7  10  8  10  9  10  number of inputs Figure 2: The effects of of the batch size when serial mini-batching on average loss. [sent-518, score-0.696]
</p><p>58 1 Serial Mini-Batching As a warm-up, we investigated the effects of modifying the mini-batch size b in a standard serial Euclidean dual averaging algorithm. [sent-533, score-0.592]
</p><p>59 5 no−comm batch no−comm serial DMB  2  average loss  average loss  2  1. [sent-545, score-0.672]
</p><p>60 5 5 10  6  10  7  10  8  10  9  10  number of inputs  Figure 3: Comparing DBM with the serial algorithm and the no-communication distributed algorithm. [sent-549, score-0.621]
</p><p>61 2 Evaluating DBM Next, we compared the average loss of the DBM algorithm with the average loss of the serial algorithm and the no-communication algorithm (where each cluster node works independently). [sent-560, score-0.635]
</p><p>62 We have already seen that larger batch sizes accelerate the initial learning phase, even in a serial setting. [sent-630, score-0.584]
</p><p>63 As noted in the serial case, larger batch sizes (b = 512) are beneﬁcial at ﬁrst (m = 107 ), while smaller batch sizes (b = 128) are better in the end (m = 109 ). [sent-637, score-0.733]
</p><p>64 5 Discussion We presented an empirical evaluation of the serial mini-batch algorithm and its distributed version, the DMB algorithm, on a realistic web-scale online prediction problem. [sent-639, score-0.657]
</p><p>65 Instead, our novel use of the variancebased regret bounds can exploit parallel/distributed computing to obtain the asymptotic optimal regret bound. [sent-678, score-0.689]
</p><p>66 As far as we know, we are the ﬁrst to propose a general principled framework for distributing many gradient-based update rule, with a concrete regret analysis for the large family of mirror descent and dual averaging update rules. [sent-680, score-0.711]
</p><p>67 Additionally, our work is the ﬁrst to explicitly include network latency in our regret analysis, and to theoretically guarantee that a large latency can be overcome by setting parameters appropriately. [sent-681, score-0.813]
</p><p>68 In this work we studied the problems of distributed stochastic online prediction and distributed stochastic optimization. [sent-685, score-0.568]
</p><p>69 We presented a family of distributed online algorithms with asymptotically optimal regret and optimality gap guarantees. [sent-686, score-0.609]
</p><p>70 Our analysis shows that asymptotically, a distributed computing system can 188  O PTIMAL D ISTRIBUTED O NLINE P REDICTION  perform as well as a hypothetical fast serial computer. [sent-688, score-0.509]
</p><p>71 Our formal analysis hinges on the fact that the regret bounds of many stochastic online update rules scale with the variance of the stochastic gradients when the loss function is smooth. [sent-693, score-0.989]
</p><p>72 For any serial update rule φ with a regret bound √ √ of ψ(σ2 , m) = Cσ m + o ( m), the DMB algorithm and its variants have the optimal regret bound √ √ of Cσ m + o ( m), provided that the bound ψ(σ2 , m) applies equally to the function f and to the function 1 b f¯ (w, (z1 , . [sent-696, score-1.362]
</p><p>73 Smooth Stochastic Online Prediction in the Serial Setting In this appendix, we prove expected regret bounds for stochastic dual averaging and stochastic mirror descent applied to smooth loss functions. [sent-726, score-0.949]
</p><p>74 Before observing each zi we predict wi ∈ W , and suffer a loss f (wi , zi ). [sent-733, score-0.608]
</p><p>75 Under the above assumptions, we are concerned with bounding the expected regret E[R(m)], where regret is deﬁned as m  R(m) =  ∑ ( f (wi , zi ) − f (w⋆ , zi )) . [sent-743, score-0.933]
</p><p>76 In the stochastic dual averaging method, we predict each wi by i  wi+1 = arg min w∈W  ∑ g j, w  + (L + βi+1 )h(w)  ,  (13)  j=1  where g j denotes the stochastic gradient ∇w f (w j , z j ), and (βi )i≥1 is a sequence of positive and nondecreasing parameters (i. [sent-760, score-0.806]
</p><p>77 (14)  w∈W  We are now ready to state a bound on the expected regret of the dual averaging method, in the smooth stochastic case. [sent-764, score-0.704]
</p><p>78 Theorem 7 The expected regret of the stochastic dual averaging method is bounded as ∀m,  E[R(m)] ≤ (F(w1 ) − F(w⋆ )) + (L + βm )h(w⋆ ) +  The optimal choice of βi is exactly of order positive parameter. [sent-765, score-0.652]
</p><p>79 Proof First, we deﬁne the linear functions ℓi (w) = F(wi ) + ∇F(wi ), w − wi ,  and (using the notation gi = ∇ f (wi , zi ))  ∀ i ≥ 1,  ˆ ℓi (w) = F(wi ) + gi , w − wi = ℓi (w) + qi , w − wi , where qi = gi − ∇F(wi ). [sent-776, score-1.447]
</p><p>80 Therefore, the stochastic dual averaging method speciﬁed in Equation (13) is equivalent to i−1  wi = arg min w∈W  ∑ ℓˆj (w) + (L + βi )h(w)  . [sent-777, score-0.617]
</p><p>81 2βi i=1  Therefore, m  ∑  i=2  F(wi ) − F(w⋆ ) ≤ (L + βm )h(w⋆ ) +  m−1  ∑  i=1  qi 2 m−1 ∗ + ∑ qi , w⋆ − wi . [sent-785, score-0.626]
</p><p>82 2βi  Theorem 7 is proved by further noticing E f (wi , zi ) = E F(wi ),  E f (w⋆ , zi ) = F(w⋆ ),  ∀ i ≥ 1,  which are due to the fact that wi is a deterministic function of z0 , . [sent-797, score-0.544]
</p><p>83 In the stochastic mirror descent method, we use the same initialization as in the dual averaging method (see Equation (14)) and then we set wi+1 = arg min  gi , w + (L + βi )d(w, wi ) ,  i ≥ 1. [sent-807, score-0.738]
</p><p>84 u,v∈W  Then the expected regret of the stochastic mirror descent method is bounded as E[R(m)] ≤ (F(w1 ) − F(w⋆ )) + (L + βm )D2 +  σ2 m−1 1 ∑ . [sent-811, score-0.583]
</p><p>85 2 i=1 βi  √ Similar to the dual averaging case, using the sequence of parameters βi = (σ/D) i gives the expected regret bound √ E[R(m)] ≤ (F(w1 ) − F(w⋆ )) + LD2 + (2σD) m. [sent-812, score-0.568]
</p><p>86 2βi  (17)  ˆ Now using Lemma 11 with ϕ(w) = ℓi (w) yields ˆ ˆ ℓi (wi+1 ) + (L + βi )d(wi+1 , wi ) ≤ ℓi (w⋆ ) + (L + βi )d(w⋆ , wi ) − (L + βi )d(w⋆ , wi+1 ). [sent-830, score-0.6]
</p><p>87 Summing the above inequality from i = 1 to i = m − 1, we have m  ∑ F(wi )  i=2  ≤ (m − 1)F(w⋆ ) + (L + β1 )d(w⋆ , w1 ) − (L + βm )d(w⋆ , wm ) + (βm − β1 )D2 m−1  +  ∑  i=1  qi 2 m−1 ∗ + ∑ qi , w⋆ − wi . [sent-832, score-0.657]
</p><p>88 2βi i=1  Notice that d(w⋆ , wi ) ≥ 0 and d(w⋆ , w1 ) ≤ D2 , so we have m  m−1  i=2  i=1  ∑ F(wi ) ≤ (m − 1)F(w⋆ ) + (L + βm )D2 + ∑ 195  qi 2 m−1 ∗ + ∑ qi , w⋆ − wi . [sent-833, score-0.926]
</p><p>89 First, we need to justify that the expected regret bounds for the online prediction rules discussed in Appendix A have high-probability versions. [sent-847, score-0.558]
</p><p>90 For simplicity, we will focus on a high-probability version of the regret bound for dual averaging (Theorem 7), but exactly the same technique will work for stochastic mirror descent (Theorem 9 and Theorem 10). [sent-848, score-0.763]
</p><p>91 Theorem 12 For any m and any δ ∈ (0, 1], the regret of the stochastic dual averaging method is bounded with probability at least 1 − δ over the sampling of z1 , . [sent-856, score-0.623]
</p><p>92 log(2/δ) 196  1 G2 σ2 ∑m β2 + D2 σ2 m i=1 i  log(2/δ)  O PTIMAL D ISTRIBUTED O NLINE P REDICTION  Proof The proof of the theorem is identical to the one of Theorem 7, up to Equation (16): m  ∑  i=2  F(wi ) − F(w⋆ ) ≤ (L + βm )h(w⋆ ) +  m−1  ∑  i=1  qi 2 m−1 + ∑ qi , w⋆ − wi . [sent-860, score-0.663]
</p><p>93 We i i will ﬁrst use this result for the sequence xi =  qi  2 − σ2 i  2βi  + qi , w⋆ − wi . [sent-882, score-0.626]
</p><p>94 Moreover, | qi , w⋆ − wi | ≤ D qi ≤ 2DG, qi 2 ≤ 4G2 . [sent-893, score-0.789]
</p><p>95 Then Varzi (xi ) ≤ 2Varzi 1 ≤ Ezi 2 ≤ 2G2 Ezi ≤ 2G2  qi  2 − σ2 i  2βi qi 4 β2 i  + 2Varzi ( qi , w⋆ − wi )  + 2Ezi [( qi , w⋆ − wi )2 ]  qi 2 β2 i  + 2 w⋆ − wi 2 Ezi [ qi 2 ]  σ2 σ2 i + 2D2 σ2 ≤ 2G2 2 + 2D2 σ2 . [sent-899, score-1.878]
</p><p>96 i β2 βi i  Combining these observations with Equation (19), we get that with probability at least 1 − δ, m−1  ∑  i=1  qi  2 − σ2  βi  4G2 + qi , w − wi ≤ 2DG + β1 ⋆  log(1/δ)  1 + 36  1 G2 σ2 ∑m β2 + D2 σ2 m i=1 i  log(1/δ)  . [sent-900, score-0.626]
</p><p>97 197  D EKEL , G ILAD -BACHRACH , S HAMIR AND X IAO  Also, |( f (wi , zi ) − f (w⋆ , zi )) − (F(wi ) − F(w⋆ ))| ≤ 4B, and Varzi  f (wi , zi ) − f (w⋆ , zi ) − F(wi ) − F(w⋆ )  = Varzi f (wi , zi ) − f (w⋆ , zi )  ˆ ≤ σ2 . [sent-912, score-0.732]
</p><p>98 ˆ now, the regret bound also depends on the function variance σ Theorem 13 Let f is an L-smooth convex loss function. [sent-920, score-0.524]
</p><p>99 Assume that the stochastic gradient ∇w f (w, zi ) is bounded by a constant and has σ2 -bounded variance for all i and all w, and that ˆ f (w, zi ) is bounded by a constant and has σ2 -bounded variance for all i and for all w. [sent-921, score-0.529]
</p><p>100 If the update ˆ rule φ has a serial high-probability regret bound ψ(σ2 , σ2 , δ, m). [sent-922, score-0.928]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('serial', 0.435), ('dmb', 0.352), ('regret', 0.33), ('wi', 0.3), ('latency', 0.205), ('qi', 0.163), ('hamir', 0.156), ('ilad', 0.156), ('batch', 0.149), ('istributed', 0.147), ('stochastic', 0.136), ('iao', 0.133), ('ekel', 0.133), ('zi', 0.122), ('rediction', 0.114), ('inputs', 0.112), ('ptimal', 0.105), ('online', 0.098), ('averaging', 0.092), ('xiao', 0.091), ('zs', 0.087), ('nline', 0.083), ('node', 0.078), ('gradients', 0.078), ('distributed', 0.074), ('nodes', 0.074), ('mdmb', 0.074), ('network', 0.073), ('lan', 0.071), ('update', 0.068), ('dual', 0.065), ('mirror', 0.059), ('template', 0.056), ('gradient', 0.053), ('bound', 0.052), ('ez', 0.051), ('convex', 0.05), ('prediction', 0.05), ('operation', 0.05), ('monetizable', 0.049), ('msrl', 0.049), ('parallel', 0.049), ('variance', 0.048), ('divides', 0.047), ('loss', 0.044), ('queries', 0.043), ('predictor', 0.043), ('rule', 0.043), ('ezi', 0.042), ('communication', 0.041), ('latencies', 0.041), ('accelerated', 0.041), ('asynchronous', 0.041), ('gap', 0.039), ('minw', 0.039), ('batches', 0.038), ('theorem', 0.037), ('nemirovski', 0.036), ('dbm', 0.035), ('lh', 0.035), ('zm', 0.034), ('cluster', 0.034), ('asymptotically', 0.034), ('optimality', 0.034), ('gi', 0.033), ('comm', 0.033), ('nedi', 0.033), ('varzi', 0.033), ('waste', 0.033), ('juditsky', 0.032), ('wm', 0.031), ('query', 0.03), ('dekel', 0.03), ('descent', 0.029), ('issued', 0.029), ('expected', 0.029), ('bounds', 0.029), ('equation', 0.027), ('nesterov', 0.025), ('bregman', 0.025), ('ghadimi', 0.025), ('synchronized', 0.025), ('arg', 0.024), ('lim', 0.024), ('diameter', 0.023), ('engine', 0.023), ('dominant', 0.023), ('operations', 0.022), ('rules', 0.022), ('duchi', 0.022), ('smoothness', 0.021), ('ethernet', 0.021), ('incoming', 0.02), ('zb', 0.02), ('tseng', 0.02), ('stream', 0.02), ('suffer', 0.02), ('ideal', 0.019), ('receive', 0.019), ('mimics', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="85-tfidf-1" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>Author: Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, Lin Xiao</p><p>Abstract: Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment. We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem. Keywords: distributed computing, online learning, stochastic optimization, regret bounds, convex optimization</p><p>2 0.14594792 <a title="85-tfidf-2" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>Author: Sham M. Kakade, Shai Shalev-Shwartz, Ambuj Tewari</p><p>Abstract: There is growing body of learning problems for which it is natural to organize the parameters into a matrix. As a result, it becomes easy to impose sophisticated prior knowledge by appropriately regularizing the parameters under some matrix norm. This work describes and analyzes a systematic method for constructing such matrix-based regularization techniques. In particular, we focus on how the underlying statistical properties of a given problem can help us decide which regularization function is appropriate. Our methodology is based on a known duality phenomenon: a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. We demonstrate the potential of this framework by deriving novel generalization and regret bounds for multi-task learning, multi-class learning, and multiple kernel learning. Keywords: regularization, strong convexity, regret bounds, generalization bounds, multi-task learning, multi-class learning, multiple kernel learning</p><p>3 0.13269724 <a title="85-tfidf-3" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>Author: Mehrdad Mahdavi, Rong Jin, Tianbao Yang</p><p>Abstract: In this paper we propose efﬁcient algorithms for solving constrained online convex optimization problems. Our motivation stems from the observation that most algorithms proposed for online convex optimization require a projection onto the convex set K from which the decisions are made. While the projection is straightforward for simple shapes (e.g., Euclidean ball), for arbitrary complex sets it is the main computational challenge and may be inefﬁcient in practice. In this paper, we consider an alternative online convex optimization problem. Instead of requiring that decisions belong to K for all rounds, we only require that the constraints, which deﬁne the set K , be satisﬁed in the long run. By turning the problem into an online convex-concave optimization problem, √ we propose an efﬁcient algorithm which achieves O( T ) regret bound and O(T 3/4 ) bound on the violation of constraints. Then, we modify the algorithm in order to guarantee that the constraints are satisﬁed in the long run. This gain is achieved at the price of getting O(T 3/4 ) regret bound. Our second algorithm is based on the mirror prox method (Nemirovski, 2005) to solve variational inequalities which achieves O(T 2/3 ) bound for both regret and the violation of constraints when the domain K can be described by a ﬁnite number of linear constraints. Finally, we extend the results to the setting where we only have partial access to the convex set K and propose a multipoint bandit feedback algorithm with the same bounds in expectation as our ﬁrst algorithm. Keywords: online convex optimization, convex-concave optimization, bandit feedback, variational inequality</p><p>4 0.1105826 <a title="85-tfidf-4" href="./jmlr-2012-Online_Submodular_Minimization.html">84 jmlr-2012-Online Submodular Minimization</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We consider an online decision problem over a discrete space in which the loss function is submodular. We give algorithms which are computationally efﬁcient and are Hannan-consistent in both the full information and partial feedback settings. Keywords: submodular optimization, online learning, regret minimization</p><p>5 0.09916544 <a title="85-tfidf-5" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>Author: Garvesh Raskutti, Martin J. Wainwright, Bin Yu</p><p>Abstract: Sparse additive models are families of d-variate functions with the additive decomposition f ∗ = ∑ j∈S f j∗ , where S is an unknown subset of cardinality s ≪ d. In this paper, we consider the case where each univariate component function f j∗ lies in a reproducing kernel Hilbert space (RKHS), and analyze a method for estimating the unknown function f ∗ based on kernels combined with ℓ1 -type convex regularization. Working within a high-dimensional framework that allows both the dimension d and sparsity s to increase with n, we derive convergence rates in the L2 (P) and L2 (Pn ) norms over the class Fd,s,H of sparse additive models with each univariate function f j∗ in the unit ball of a univariate RKHS with bounded kernel function. We complement our upper bounds by deriving minimax lower bounds on the L2 (P) error, thereby showing the optimality of our method. Thus, we obtain optimal minimax rates for many interesting classes of sparse additive models, including polynomials, splines, and Sobolev classes. We also show that if, in contrast to our univariate conditions, the d-variate function class is assumed to be globally bounded, then much √ faster estimation rates are possible for any sparsity s = Ω( n), showing that global boundedness is a signiﬁcant restriction in the high-dimensional setting. Keywords: sparsity, kernel, non-parametric, convex, minimax</p><p>6 0.08461637 <a title="85-tfidf-6" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>7 0.074457951 <a title="85-tfidf-7" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>8 0.072077416 <a title="85-tfidf-8" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>9 0.057316627 <a title="85-tfidf-9" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>10 0.054467443 <a title="85-tfidf-10" href="./jmlr-2012-Optimistic_Bayesian_Sampling_in_Contextual-Bandit_Problems.html">86 jmlr-2012-Optimistic Bayesian Sampling in Contextual-Bandit Problems</a></p>
<p>11 0.053751748 <a title="85-tfidf-11" href="./jmlr-2012-An_Active_Learning_Algorithm_for_Ranking_from_Pairwise_Preferences_with_an_Almost_Optimal_Query_Complexity.html">17 jmlr-2012-An Active Learning Algorithm for Ranking from Pairwise Preferences with an Almost Optimal Query Complexity</a></p>
<p>12 0.052508023 <a title="85-tfidf-12" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>13 0.046656035 <a title="85-tfidf-13" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>14 0.044406738 <a title="85-tfidf-14" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>15 0.043699265 <a title="85-tfidf-15" href="./jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</a></p>
<p>16 0.04329244 <a title="85-tfidf-16" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>17 0.040640805 <a title="85-tfidf-17" href="./jmlr-2012-Query_Strategies_for_Evading_Convex-Inducing_Classifiers.html">94 jmlr-2012-Query Strategies for Evading Convex-Inducing Classifiers</a></p>
<p>18 0.039126609 <a title="85-tfidf-18" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>19 0.038706567 <a title="85-tfidf-19" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>20 0.037261341 <a title="85-tfidf-20" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.222), (1, -0.201), (2, -0.079), (3, -0.084), (4, 0.001), (5, 0.055), (6, 0.072), (7, 0.053), (8, -0.03), (9, -0.077), (10, 0.024), (11, 0.0), (12, -0.015), (13, -0.004), (14, -0.049), (15, -0.002), (16, 0.103), (17, -0.194), (18, -0.161), (19, 0.024), (20, 0.063), (21, -0.013), (22, 0.112), (23, 0.125), (24, 0.101), (25, -0.074), (26, -0.113), (27, 0.071), (28, -0.172), (29, -0.055), (30, 0.004), (31, 0.164), (32, -0.03), (33, 0.04), (34, 0.022), (35, 0.137), (36, -0.007), (37, 0.006), (38, 0.028), (39, 0.005), (40, -0.062), (41, 0.028), (42, -0.115), (43, -0.122), (44, -0.133), (45, -0.094), (46, -0.017), (47, 0.058), (48, 0.035), (49, -0.123)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95041084 <a title="85-lsi-1" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>Author: Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, Lin Xiao</p><p>Abstract: Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment. We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem. Keywords: distributed computing, online learning, stochastic optimization, regret bounds, convex optimization</p><p>2 0.49527061 <a title="85-lsi-2" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>Author: Mehrdad Mahdavi, Rong Jin, Tianbao Yang</p><p>Abstract: In this paper we propose efﬁcient algorithms for solving constrained online convex optimization problems. Our motivation stems from the observation that most algorithms proposed for online convex optimization require a projection onto the convex set K from which the decisions are made. While the projection is straightforward for simple shapes (e.g., Euclidean ball), for arbitrary complex sets it is the main computational challenge and may be inefﬁcient in practice. In this paper, we consider an alternative online convex optimization problem. Instead of requiring that decisions belong to K for all rounds, we only require that the constraints, which deﬁne the set K , be satisﬁed in the long run. By turning the problem into an online convex-concave optimization problem, √ we propose an efﬁcient algorithm which achieves O( T ) regret bound and O(T 3/4 ) bound on the violation of constraints. Then, we modify the algorithm in order to guarantee that the constraints are satisﬁed in the long run. This gain is achieved at the price of getting O(T 3/4 ) regret bound. Our second algorithm is based on the mirror prox method (Nemirovski, 2005) to solve variational inequalities which achieves O(T 2/3 ) bound for both regret and the violation of constraints when the domain K can be described by a ﬁnite number of linear constraints. Finally, we extend the results to the setting where we only have partial access to the convex set K and propose a multipoint bandit feedback algorithm with the same bounds in expectation as our ﬁrst algorithm. Keywords: online convex optimization, convex-concave optimization, bandit feedback, variational inequality</p><p>3 0.49314752 <a title="85-lsi-3" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>Author: Sham M. Kakade, Shai Shalev-Shwartz, Ambuj Tewari</p><p>Abstract: There is growing body of learning problems for which it is natural to organize the parameters into a matrix. As a result, it becomes easy to impose sophisticated prior knowledge by appropriately regularizing the parameters under some matrix norm. This work describes and analyzes a systematic method for constructing such matrix-based regularization techniques. In particular, we focus on how the underlying statistical properties of a given problem can help us decide which regularization function is appropriate. Our methodology is based on a known duality phenomenon: a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. We demonstrate the potential of this framework by deriving novel generalization and regret bounds for multi-task learning, multi-class learning, and multiple kernel learning. Keywords: regularization, strong convexity, regret bounds, generalization bounds, multi-task learning, multi-class learning, multiple kernel learning</p><p>4 0.46606815 <a title="85-lsi-4" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>Author: Sangkyun Lee, Stephen J. Wright</p><p>Abstract: Iterative methods that calculate their steps from approximate subgradient directions have proved to be useful for stochastic learning problems over large and streaming data sets. When the objective consists of a loss function plus a nonsmooth regularization term, the solution often lies on a lowdimensional manifold of parameter space along which the regularizer is smooth. (When an ℓ1 regularizer is used to induce sparsity in the solution, for example, this manifold is deﬁned by the set of nonzero components of the parameter vector.) This paper shows that a regularized dual averaging algorithm can identify this manifold, with high probability, before reaching the solution. This observation motivates an algorithmic strategy in which, once an iterate is suspected of lying on an optimal or near-optimal manifold, we switch to a “local phase” that searches in this manifold, thus converging rapidly to a near-optimal point. Computational results are presented to verify the identiﬁcation property and to illustrate the effectiveness of this approach. Keywords: regularization, dual averaging, partly smooth manifold, manifold identiﬁcation</p><p>5 0.41030648 <a title="85-lsi-5" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>Author: Philipp Hennig, Christian J. Schuler</p><p>Abstract: Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly addresses the decision problem of maximizing information gain from each evaluation. Keywords: optimization, probability, information, Gaussian processes, expectation propagation</p><p>6 0.40772405 <a title="85-lsi-6" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>7 0.38863859 <a title="85-lsi-7" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>8 0.38403955 <a title="85-lsi-8" href="./jmlr-2012-An_Active_Learning_Algorithm_for_Ranking_from_Pairwise_Preferences_with_an_Almost_Optimal_Query_Complexity.html">17 jmlr-2012-An Active Learning Algorithm for Ranking from Pairwise Preferences with an Almost Optimal Query Complexity</a></p>
<p>9 0.3829647 <a title="85-lsi-9" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>10 0.36584619 <a title="85-lsi-10" href="./jmlr-2012-Online_Submodular_Minimization.html">84 jmlr-2012-Online Submodular Minimization</a></p>
<p>11 0.31716752 <a title="85-lsi-11" href="./jmlr-2012-Hope_and_Fear_for_Discriminative_Training_of_Statistical_Translation_Models.html">49 jmlr-2012-Hope and Fear for Discriminative Training of Statistical Translation Models</a></p>
<p>12 0.31336483 <a title="85-lsi-12" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>13 0.28395286 <a title="85-lsi-13" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>14 0.27325276 <a title="85-lsi-14" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>15 0.26739821 <a title="85-lsi-15" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>16 0.26711446 <a title="85-lsi-16" href="./jmlr-2012-Optimistic_Bayesian_Sampling_in_Contextual-Bandit_Problems.html">86 jmlr-2012-Optimistic Bayesian Sampling in Contextual-Bandit Problems</a></p>
<p>17 0.26543558 <a title="85-lsi-17" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>18 0.26469323 <a title="85-lsi-18" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>19 0.24812652 <a title="85-lsi-19" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>20 0.22982682 <a title="85-lsi-20" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.062), (12, 0.254), (21, 0.04), (26, 0.069), (27, 0.013), (29, 0.032), (35, 0.019), (49, 0.016), (56, 0.029), (57, 0.014), (64, 0.016), (69, 0.015), (75, 0.089), (77, 0.016), (79, 0.019), (92, 0.103), (96, 0.124)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76288736 <a title="85-lda-1" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>Author: Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, Lin Xiao</p><p>Abstract: Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment. We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem. Keywords: distributed computing, online learning, stochastic optimization, regret bounds, convex optimization</p><p>2 0.61640728 <a title="85-lda-2" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>Author: Sham M. Kakade, Shai Shalev-Shwartz, Ambuj Tewari</p><p>Abstract: There is growing body of learning problems for which it is natural to organize the parameters into a matrix. As a result, it becomes easy to impose sophisticated prior knowledge by appropriately regularizing the parameters under some matrix norm. This work describes and analyzes a systematic method for constructing such matrix-based regularization techniques. In particular, we focus on how the underlying statistical properties of a given problem can help us decide which regularization function is appropriate. Our methodology is based on a known duality phenomenon: a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. We demonstrate the potential of this framework by deriving novel generalization and regret bounds for multi-task learning, multi-class learning, and multiple kernel learning. Keywords: regularization, strong convexity, regret bounds, generalization bounds, multi-task learning, multi-class learning, multiple kernel learning</p><p>3 0.61580735 <a title="85-lda-3" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>Author: Mehrdad Mahdavi, Rong Jin, Tianbao Yang</p><p>Abstract: In this paper we propose efﬁcient algorithms for solving constrained online convex optimization problems. Our motivation stems from the observation that most algorithms proposed for online convex optimization require a projection onto the convex set K from which the decisions are made. While the projection is straightforward for simple shapes (e.g., Euclidean ball), for arbitrary complex sets it is the main computational challenge and may be inefﬁcient in practice. In this paper, we consider an alternative online convex optimization problem. Instead of requiring that decisions belong to K for all rounds, we only require that the constraints, which deﬁne the set K , be satisﬁed in the long run. By turning the problem into an online convex-concave optimization problem, √ we propose an efﬁcient algorithm which achieves O( T ) regret bound and O(T 3/4 ) bound on the violation of constraints. Then, we modify the algorithm in order to guarantee that the constraints are satisﬁed in the long run. This gain is achieved at the price of getting O(T 3/4 ) regret bound. Our second algorithm is based on the mirror prox method (Nemirovski, 2005) to solve variational inequalities which achieves O(T 2/3 ) bound for both regret and the violation of constraints when the domain K can be described by a ﬁnite number of linear constraints. Finally, we extend the results to the setting where we only have partial access to the convex set K and propose a multipoint bandit feedback algorithm with the same bounds in expectation as our ﬁrst algorithm. Keywords: online convex optimization, convex-concave optimization, bandit feedback, variational inequality</p><p>4 0.61300701 <a title="85-lda-4" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>Author: Matus Telgarsky</p><p>Abstract: Boosting combines weak learners into a predictor with low empirical risk. Its dual constructs a high entropy distribution upon which weak learners and training labels are uncorrelated. This manuscript studies this primal-dual relationship under a broad family of losses, including the exponential loss of AdaBoost and the logistic loss, revealing: • Weak learnability aids the whole loss family: for any ε > 0, O (ln(1/ε)) iterations sufﬁce to produce a predictor with empirical risk ε-close to the inﬁmum; • The circumstances granting the existence of an empirical risk minimizer may be characterized in terms of the primal and dual problems, yielding a new proof of the known rate O (ln(1/ε)); • Arbitrary instances may be decomposed into the above two, granting rate O (1/ε), with a matching lower bound provided for the logistic loss. Keywords: boosting, convex analysis, weak learnability, coordinate descent, maximum entropy</p><p>5 0.60909748 <a title="85-lda-5" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>Author: Sangkyun Lee, Stephen J. Wright</p><p>Abstract: Iterative methods that calculate their steps from approximate subgradient directions have proved to be useful for stochastic learning problems over large and streaming data sets. When the objective consists of a loss function plus a nonsmooth regularization term, the solution often lies on a lowdimensional manifold of parameter space along which the regularizer is smooth. (When an ℓ1 regularizer is used to induce sparsity in the solution, for example, this manifold is deﬁned by the set of nonzero components of the parameter vector.) This paper shows that a regularized dual averaging algorithm can identify this manifold, with high probability, before reaching the solution. This observation motivates an algorithmic strategy in which, once an iterate is suspected of lying on an optimal or near-optimal manifold, we switch to a “local phase” that searches in this manifold, thus converging rapidly to a near-optimal point. Computational results are presented to verify the identiﬁcation property and to illustrate the effectiveness of this approach. Keywords: regularization, dual averaging, partly smooth manifold, manifold identiﬁcation</p><p>6 0.59993047 <a title="85-lda-6" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>7 0.59966314 <a title="85-lda-7" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>8 0.59681046 <a title="85-lda-8" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>9 0.59204888 <a title="85-lda-9" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>10 0.58921182 <a title="85-lda-10" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>11 0.58764374 <a title="85-lda-11" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>12 0.58625621 <a title="85-lda-12" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>13 0.5842495 <a title="85-lda-13" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>14 0.58135152 <a title="85-lda-14" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>15 0.58119845 <a title="85-lda-15" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>16 0.5784713 <a title="85-lda-16" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>17 0.57741857 <a title="85-lda-17" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>18 0.57636601 <a title="85-lda-18" href="./jmlr-2012-Online_Submodular_Minimization.html">84 jmlr-2012-Online Submodular Minimization</a></p>
<p>19 0.57635933 <a title="85-lda-19" href="./jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</a></p>
<p>20 0.57630467 <a title="85-lda-20" href="./jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
