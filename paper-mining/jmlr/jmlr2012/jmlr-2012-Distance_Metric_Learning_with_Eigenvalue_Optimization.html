<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-33" href="#">jmlr2012-33</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</h1>
<br/><p>Source: <a title="jmlr-2012-33-pdf" href="http://jmlr.org/papers/volume13/ying12a/ying12a.pdf">pdf</a></p><p>Author: Yiming Ying, Peng Li</p><p>Abstract: The main theme of this paper is to develop a novel eigenvalue optimization framework for learning a Mahalanobis metric. Within this context, we introduce a novel metric learning approach called DML-eig which is shown to be equivalent to a well-known eigenvalue optimization problem called minimizing the maximal eigenvalue of a symmetric matrix (Overton, 1988; Lewis and Overton, 1996). Moreover, we formulate LMNN (Weinberger et al., 2005), one of the state-of-the-art metric learning methods, as a similar eigenvalue optimization problem. This novel framework not only provides new insights into metric learning but also opens new avenues to the design of efﬁcient metric learning algorithms. Indeed, ﬁrst-order algorithms are developed for DML-eig and LMNN which only need the computation of the largest eigenvector of a matrix per iteration. Their convergence characteristics are rigorously established. Various experiments on benchmark data sets show the competitive performance of our new approaches. In addition, we report an encouraging result on a difﬁcult and challenging face veriﬁcation data set called Labeled Faces in the Wild (LFW). Keywords: metric learning, convex optimization, semi-deﬁnite programming, ﬁrst-order methods, eigenvalue optimization, matrix factorization, face veriﬁcation</p><p>Reference: <a title="jmlr-2012-33-reference" href="../jmlr2012_reference/jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Within this context, we introduce a novel metric learning approach called DML-eig which is shown to be equivalent to a well-known eigenvalue optimization problem called minimizing the maximal eigenvalue of a symmetric matrix (Overton, 1988; Lewis and Overton, 1996). [sent-6, score-0.534]
</p><p>2 , 2005), one of the state-of-the-art metric learning methods, as a similar eigenvalue optimization problem. [sent-8, score-0.372]
</p><p>3 This novel framework not only provides new insights into metric learning but also opens new avenues to the design of efﬁcient metric learning algorithms. [sent-9, score-0.404]
</p><p>4 Keywords: metric learning, convex optimization, semi-deﬁnite programming, ﬁrst-order methods, eigenvalue optimization, matrix factorization, face veriﬁcation  1. [sent-14, score-0.485]
</p><p>5 Introduction Distance metrics are fundamental concepts in machine learning since a proper choice of a metric has crucial effects on the performance of both supervised and unsupervised learning algorithms. [sent-15, score-0.202]
</p><p>6 The k-means algorithm depends on the pairwise distance measurements between examples for clustering, and most information retrieval methods rely on a distance metric to identify the data points that are most similar to a given query. [sent-17, score-0.372]
</p><p>7 Recently, learning a distance metric from data has been actively studied in machine learning (Bar-Hillel et al. [sent-18, score-0.266]
</p><p>8 Most metric learning methods attempt to learn a distance metric from side information which is often available in the form of pairwise constraints, that is, pairs of similar data points and pairs of dissimilar data points. [sent-32, score-0.771]
</p><p>9 A common theme in metric learning is to learn a distance metric such that the distance between similar examples should be relatively smaller than that between dissimilar examples. [sent-37, score-0.671]
</p><p>10 Although the distance metric can be a general function, the most prevalent one is the Mahalanobis metric deﬁned by dM (xi , x j ) = (xi − x j )⊤ M(xi − x j ) where M is a positive semi-deﬁnite (p. [sent-38, score-0.468]
</p><p>11 In this work we restrict our attention to learning a Mahalanobis metric for k-nearest neighbor (k-NN) classiﬁcation. [sent-42, score-0.202]
</p><p>12 However, the proposed methods below can easily be adapted to metric learning for semi-supervised k-means clustering. [sent-43, score-0.202]
</p><p>13 In particular, we can show our approach is equivalent to a well-known eigenvalue optimization problem called minimizing the maximal eigenvalue of a symmetric matrix (Lewis and Overton, 1996; Overton, 1988). [sent-48, score-0.332]
</p><p>14 Secondly, in contrast to the full eigen-decomposition used in many existing approaches to metric learning, we will develop novel approximate semi-deﬁnite programming (SDP) algorithms for DML-eig and LMNN which only need the computation of the largest eigenvector of a matrix per iteration. [sent-52, score-0.284]
</p><p>15 In Section 2, we propose our new approach (DML-eig) for distance metric learning and show its equivalence to the well-known eigenvalue optimization problem. [sent-58, score-0.474]
</p><p>16 In Section 3, based on eigenvalue optimization formulations of DML-eig and LMNN, we develop novel ﬁrstorder algorithms. [sent-61, score-0.17]
</p><p>17 Let S index the similar pairs and D index the dissimilar pairs. [sent-83, score-0.2]
</p><p>18 Given a set of pairwise distance constraints, the target of metric learning is to ﬁnd a distance matrix M such that the distance between the dissimilar pairs is large and the distance between the similar pairs is small. [sent-85, score-0.793]
</p><p>19 In this paper, we propose to maximize the minimal squared distances between dissimilar pairs while maintaining an upper bound for the sum of squared distances between similar pairs, that is, maxM∈Sd + s. [sent-95, score-0.2]
</p><p>20 As shown in the next subsection, this simple but important property1 plays a critical role in formulating problem (2) as an eigenvalue optimization problem. [sent-108, score-0.17]
</p><p>21 If labels are known then the learning setting is often referred to as supervised metric learning which can 2 1. [sent-111, score-0.202]
</p><p>22 3  Y ING AND L I  further be divided into two categories: the global metric learning and the local metric learning. [sent-118, score-0.404]
</p><p>23 The global approach learns the distance metric in a global sense, that is, to satisfy all the pairwise constraints simultaneously. [sent-119, score-0.342]
</p><p>24 (2002) is a global method which used all the similar pairs (same labels) and dissimilar pairs (distinct labels). [sent-121, score-0.261]
</p><p>25 The local approach is to learn a distance metric only using local pairwise constraints which usually outperforms the global methods as observed in many previous studies. [sent-122, score-0.342]
</p><p>26 This is reasonable in the case of learning a metric for the k-NN classiﬁers since k-NN classiﬁers are inﬂuenced most by the data items that are close to the test/query examples. [sent-123, score-0.202]
</p><p>27 Since we are mainly concerned with learning a metric for k-NN classiﬁer, the pairwise constraints for DML-eig are generated locally, that is, the similar/dissimilar pairs are k-nearest neighbors. [sent-124, score-0.339]
</p><p>28 Let D be the number of dissimilar pairs and the simplex is denoted by  ∑ uτ = 1}. [sent-129, score-0.2]
</p><p>29 + Now we can show problem (3) is indeed an eigenvalue optimization problem. [sent-131, score-0.17]
</p><p>30 Then, problem (4)  which can further be written as an eigenvalue optimization problem: min max u∈△ S∈P  ∑ uτ Xτ , S  τ∈D  = min λmax u∈△  ∑ uτ Xτ  . [sent-134, score-0.308]
</p><p>31 By the min-max theorem, problem (4) can further be written by a well-known eigenvalue optimization problem: min max u∈△ M∈P  ∑ uτ Xτ , M  τ∈D  = min λmax u∈△  ∑ uτ Xτ  . [sent-142, score-0.308]
</p><p>32 The problem of minimizing the maximal eigenvalue of a symmetric matrix is well-known which has important applications in engineering design, see Overton (1988); Lewis and Overton (1996). [sent-144, score-0.162]
</p><p>33 Hereafter, we refer to metric learning formulation (3) (equivalently (4) or (5)) as DML-eig. [sent-145, score-0.259]
</p><p>34 (2005) proposed the large margin nearest neighbor classiﬁcation (LMNN) which is one of the state-of-the-art metric learning methods. [sent-161, score-0.228]
</p><p>35 In analogy to the above argument for DML-eig, we can also formulate LMNN as a generalized eigenvalue optimization problem. [sent-162, score-0.17]
</p><p>36 In contrast, LMNN aims to learn a metric using the relative distance constraints which are presented in the form of triplets. [sent-164, score-0.3]
</p><p>37 With a little abuse of notation, we denote a triplet by τ = (i, j, k) which means that xi is similar to x j and x j is dissimilar to xk . [sent-165, score-0.212]
</p><p>38 Given a set S of similar pairs and a set T of triplets, the target of LMNN is to learn a distance metric such that k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. [sent-169, score-0.367]
</p><p>39 (1 − γ) ∑τ∈T ξτ + γTr(XS M)  max M,ξ  (13)  Using exactly the same argument of proving the equivalence between (11) and (12), one can show that the above problem (13) is equivalent to max M,ξ  min  τ=(i, j,k)∈T  Cτ , M + ξτ : (1 − γ)  ∑ ξτ + γTr(XS M) = 1, (M, ξ) ∈ Ω  . [sent-202, score-0.176]
</p><p>40 Using the above min-max representation for LMNN, it is now easy to reformulate LMNN as a generalized eigenvalue optimization as we will do below. [sent-209, score-0.17]
</p><p>41 Moreover, it can further be written as a generalized eigenvalue optimization problem: min max u∈△  1 1 umax , λmax 1−γ γ  ∑ uτCτ  ,  (17)  τ∈T  where umax is the maximum element of the vector (uτ : τ ∈ T ). [sent-214, score-0.352]
</p><p>42 Since we have formulated LMNN as an eigenvalue optimization problem in the above theorem, hereafter we refer to formulation (16) (equivalently (17)) as LMNN-eig. [sent-225, score-0.227]
</p><p>43 The above eigenvalue optimization formulation is not restricted to metric learning problems. [sent-226, score-0.429]
</p><p>44 Its eigenvalue optimization formulation can be found in Appendix A. [sent-230, score-0.227]
</p><p>45 We can directly employ the entropy smoothing techniques (Nesterov, 2007; Baes and B¨ rgisser, 2009) for u eigenvalue optimization which, however, needs the computation of the full eigen-decomposition per iteration. [sent-233, score-0.218]
</p><p>46 To this end, for a smoothing parameter µ > 0, deﬁne fµ (S) = min u∈△  ∑ uτ  τ∈D  Xτ , S + µ  ∑ uτ ln uτ . [sent-237, score-0.166]
</p><p>47 t  Furthermore, µ  max f (S) − f (St ) ≤ 2µ ln D + S∈P  10  8 maxτ∈D Xτ µt  2  +  8 ln D . [sent-279, score-0.19]
</p><p>48 It is easy to see, for any S ∈ P that | f (S) − fµ (S)| ≤ µ max u∈△  ∑ (−uτ ln uτ ) ≤ µ ln D. [sent-281, score-0.19]
</p><p>49 2 Approximate Frank-Wolfe Algorithm for LMNN-eig We can easily extend the above approximate Frank-Wolfe algorithm to solve the eigenvalue optimization formulation of LMNN-eig (formulation (16) or (17)). [sent-292, score-0.227]
</p><p>50 If max >= max , then Z ∗ = v (v ) γ 1−γ γ where v∗ is the largest eigenvector of matrix A and ξ∗ = 0. [sent-312, score-0.174]
</p><p>51 Related Work and Discussion There is a large amount of work on metric learning including distance metric learning for k-means clustering (Xing et al. [sent-316, score-0.468]
</p><p>52 , 2005), maximally collapsing metric learning (MCML) (Goldberger et al. [sent-318, score-0.202]
</p><p>53 , 2004) and an information-theoretic approach to metric learning (ITML) (Davis et al. [sent-320, score-0.202]
</p><p>54 We refer the readers to Yang and Jin (2007) for a nice survey on metric learning. [sent-322, score-0.202]
</p><p>55 Below we discuss some speciﬁc metric learning models which are closely related to our work. [sent-323, score-0.202]
</p><p>56 (2002) developed the metric learning model (2) to learn a Mahalanobis metric for k-means clustering. [sent-325, score-0.404]
</p><p>57 The main idea is to maximize the distance between points in the dissimilarity set under the constraint that the distance between points in the similarity set is upper-bounded. [sent-326, score-0.163]
</p><p>58 It is worth mentioning that the metric learning model proposed in Xing et al. [sent-337, score-0.202]
</p><p>59 (2002), DML-eig aims to maximize the minimal distance between dissimilar pairs instead of maximizing the summation of their distances. [sent-340, score-0.264]
</p><p>60 (2005) developed a large margin framework to learn a Mahalanobis distance metric for k-nearest neighbor (k-NN) classiﬁcation (LMNN). [sent-344, score-0.266]
</p><p>61 Our method DML-eig is a local method which only uses the similar pairs and dissimilar pairs from k-nearest neighbors. [sent-348, score-0.261]
</p><p>62 Rosales and Fung (2006) proposed the following element-sparse metric learning for high-dimensional data sets min  ∑  M∈Sd t=(i, j,k)∈T +  ⊤ (1 + xi⊤ Mxi j − xk j Mxk j )+ + γ j  ∑  |Mℓk |. [sent-366, score-0.248]
</p><p>63 However, since the pairs of similarly labeled and differently labeled examples are usually of order O (n2 ), the online learning procedure takes many rank-one matrix updates. [sent-375, score-0.153]
</p><p>64 (2009) established generalization bounds for large margin metric learning and proposed an adaptive way to adjust the step sizes of the online metric learning method in order to guarantee the output matrix in each step is positive semi-deﬁnite. [sent-377, score-0.436]
</p><p>65 (2009) recently employed the exponential loss for metric learning which can be written by min ∑ e Cτ ,M + Tr(M), M∈Sd τ=(i, j,k)∈T +  where T is the triplet set and Cτ = (xi − x j )(xi − x j )⊤ − (x j − xk )(x j − xk )⊤ for any τ = (i, j, k) ∈ T . [sent-380, score-0.283]
</p><p>66 (2009) proposed a metric learning model with logistic regression loss which is referred to as LDML. [sent-392, score-0.202]
</p><p>67 Brieﬂy speaking, for each training point xi , k nearest neighbors that have same labels as yi (targets) as well as k nearest neighbors that have different labels from yi (imposers) are found. [sent-414, score-0.17]
</p><p>68 From xi and its corresponding targets and imposers, we then construct the set of similar pairs S (same labels) and the set of dissimilar pairs D (distinct labels), and the set of triplets T . [sent-415, score-0.351]
</p><p>69 Furthermore, the data is challenging and difﬁcult due to face variations in scale, pose, lighting, background, expression, hairstyle, and glasses, as the faces are detected in images in the wild, taken from Yahoo! [sent-421, score-0.226]
</p><p>70 13  Table 4: Average test error (%) of different metric learning methods (standard deviation are in parentheses). [sent-555, score-0.202]
</p><p>71 Hence, learning a Mahalanobis metric from training data does lead to improvements in k-NN classiﬁcation. [sent-624, score-0.202]
</p><p>72 The task of face veriﬁcation is to determine whether two face images are from the same identity or not. [sent-650, score-0.3]
</p><p>73 of the face images poses great challenges to the face veriﬁcation algorithms. [sent-653, score-0.3]
</p><p>74 Metric learning provides a viable solution by comparing the image pairs based on the metric learnt from the face data. [sent-655, score-0.384]
</p><p>75 Here we evaluate our new metric learning method using a large scale face database—Labeled Faces in the Wild (LFW) (Huang et al. [sent-656, score-0.323]
</p><p>76 These face images are automatically captured from news articles on the web. [sent-659, score-0.179]
</p><p>77 In each fold, there are 300 pairs of images from the same identity and another 300 pairs of images from different identities. [sent-671, score-0.238]
</p><p>78 We investigated several descriptors (features) from face images in this experiment. [sent-675, score-0.25]
</p><p>79 For each of the ten-fold cross-validation test, we use the data from 2700 pairs of images from the same identities and another 2700 pairs of images from the different identities to learn a metric. [sent-707, score-0.238]
</p><p>80 It shows that the DML-eig metric is less prone to overﬁtting than both LDML and ITML. [sent-724, score-0.202]
</p><p>81 “Square Root” means the features preprocessed by taking square root before fed into metric learning method. [sent-767, score-0.202]
</p><p>82 7  20  40 60 80 100 120 140 Dimension of principal components  160  Figure 2: Performance of DML-eig, ITML and LDML metric by varying the dimension of the principal components using SIFT descriptor. [sent-797, score-0.202]
</p><p>83 Finally, the performance of DML-eig metric may be further improved by exploring different number of nearest neighbors and different types of descriptors such as those used in Pinto et al. [sent-816, score-0.339]
</p><p>84 2  High−Throughput Brain−Inspired Features, aligned DML−eig + combined DML−eig + SIFT, funneled LDML + combined, funneled ITML + SIFT, funneled  0. [sent-826, score-0.214]
</p><p>85 Conclusion The main theme of this paper is to develop a new eigenvalue-optimization framework for metric learning. [sent-833, score-0.202]
</p><p>86 Within this context, we ﬁrst proposed a novel metric learning model which was shown to be equivalent to a well-known eigenvalue optimization problem (Overton, 1988; Lewis and Overton, 1996). [sent-834, score-0.372]
</p><p>87 Then, we developed efﬁcient ﬁrst-order algorithms for metric learning which only involve the computation of the largest eigenvector of a matrix. [sent-838, score-0.252]
</p><p>88 Finally, experiments on various data sets have shown that our proposed approach is competitive with state-of-the-art metric learning methods. [sent-840, score-0.202]
</p><p>89 In future we will exploit the extension of the above eigenvalue optimization framework to other machine learning tasks such as spectral graph cuts and semi-deﬁnite embedding (Weinberger et al. [sent-842, score-0.17]
</p><p>90 Similar eigenvalue optimization formulation can be developed for maximum-margin matrix factorization (MMMF) for collaborative ﬁltering (Srebro et al. [sent-862, score-0.291]
</p><p>91 Given a partially labeled Yia ∈ {±1} with ia ∈ S, the target of MMMF is to learn a large matrix X ∈ Rm×n where each entry Xia indicates the preference of the customer i for product a. [sent-864, score-0.17]
</p><p>92 Using exact arguments for proving Theorem 3, we can formulate MMMF as an eigenvalue optimization problem. [sent-872, score-0.17]
</p><p>93 MMMF formulation (27) is equivalent to max min  ∑ uia  u∈△ ia∈S  ξia + YiaCia , M  (m+n)  : ξ⊤ 1 + γTr(M) = 1, M ∈ S+ 22  , ξ≥0 . [sent-874, score-0.149]
</p><p>94 D ISTANCE M ETRIC L EARNING WITH E IGENVALUE O PTIMIZATION  In particular it is equivalent to the following eigenvalue optimization problem: 1 min max umax , λmax u∈△ γ  ∑ uiaYiaCia  . [sent-875, score-0.307]
</p><p>95 Since the paper mainly focuses on metric learning, we leave its empirical implementation for future study. [sent-883, score-0.202]
</p><p>96 Learning a similarity metric discriminatively with application to face veriﬁcation. [sent-952, score-0.323]
</p><p>97 Distance metric learning for large margin nearest neighbour classiﬁcation. [sent-1130, score-0.228]
</p><p>98 Fast solvers and efﬁcient implementations for distance metric learning. [sent-1137, score-0.266]
</p><p>99 Distance metric learning with application to clustering with side information. [sent-1164, score-0.202]
</p><p>100 A boosting framework for visuality-preserving distance metric learning and its application to medical image retrieval. [sent-1180, score-0.266]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lmnn', 0.382), ('sd', 0.299), ('itml', 0.257), ('xs', 0.24), ('ldml', 0.223), ('st', 0.214), ('metric', 0.202), ('guillaumin', 0.171), ('igenvalue', 0.157), ('tr', 0.154), ('dissimilar', 0.139), ('eigenvalue', 0.13), ('boostmetric', 0.123), ('overton', 0.123), ('face', 0.121), ('istance', 0.121), ('weinberger', 0.113), ('etric', 0.111), ('maxs', 0.111), ('ia', 0.108), ('xing', 0.101), ('ptimization', 0.089), ('wolf', 0.082), ('pinto', 0.08), ('eig', 0.078), ('mlmnn', 0.078), ('mahalanobis', 0.078), ('sift', 0.074), ('ln', 0.072), ('descriptors', 0.071), ('lfw', 0.07), ('veri', 0.07), ('pca', 0.07), ('ing', 0.067), ('minu', 0.067), ('ying', 0.065), ('dml', 0.065), ('mmmf', 0.065), ('distance', 0.064), ('pairs', 0.061), ('maxm', 0.06), ('funneled', 0.06), ('sdp', 0.059), ('srebro', 0.059), ('images', 0.058), ('formulation', 0.057), ('minm', 0.056), ('zt', 0.054), ('wild', 0.054), ('yia', 0.052), ('triplets', 0.052), ('earning', 0.051), ('eigenvector', 0.05), ('dm', 0.048), ('smoothing', 0.048), ('faces', 0.047), ('max', 0.046), ('min', 0.046), ('umax', 0.045), ('righthand', 0.045), ('taigman', 0.045), ('tol', 0.045), ('guration', 0.043), ('pairwise', 0.042), ('optimization', 0.04), ('neighbors', 0.04), ('baes', 0.039), ('burer', 0.039), ('exeter', 0.039), ('davis', 0.039), ('xi', 0.038), ('equivalence', 0.038), ('lewis', 0.038), ('xia', 0.037), ('descriptor', 0.037), ('rt', 0.035), ('triplet', 0.035), ('dissimilarity', 0.035), ('constraints', 0.034), ('aligned', 0.034), ('customers', 0.034), ('goldberger', 0.033), ('factorization', 0.032), ('matrix', 0.032), ('huang', 0.031), ('labeled', 0.03), ('shen', 0.03), ('jin', 0.028), ('hoi', 0.028), ('rosales', 0.028), ('people', 0.027), ('nesterov', 0.027), ('kato', 0.026), ('monteiro', 0.026), ('mxs', 0.026), ('rgisser', 0.026), ('spectrahedron', 0.026), ('torresani', 0.026), ('nearest', 0.026), ('tk', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="33-tfidf-1" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>Author: Yiming Ying, Peng Li</p><p>Abstract: The main theme of this paper is to develop a novel eigenvalue optimization framework for learning a Mahalanobis metric. Within this context, we introduce a novel metric learning approach called DML-eig which is shown to be equivalent to a well-known eigenvalue optimization problem called minimizing the maximal eigenvalue of a symmetric matrix (Overton, 1988; Lewis and Overton, 1996). Moreover, we formulate LMNN (Weinberger et al., 2005), one of the state-of-the-art metric learning methods, as a similar eigenvalue optimization problem. This novel framework not only provides new insights into metric learning but also opens new avenues to the design of efﬁcient metric learning algorithms. Indeed, ﬁrst-order algorithms are developed for DML-eig and LMNN which only need the computation of the largest eigenvector of a matrix per iteration. Their convergence characteristics are rigorously established. Various experiments on benchmark data sets show the competitive performance of our new approaches. In addition, we report an encouraging result on a difﬁcult and challenging face veriﬁcation data set called Labeled Faces in the Wild (LFW). Keywords: metric learning, convex optimization, semi-deﬁnite programming, ﬁrst-order methods, eigenvalue optimization, matrix factorization, face veriﬁcation</p><p>2 0.31421939 <a title="33-tfidf-2" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel</p><p>Abstract: The success of many machine learning and pattern recognition methods relies heavily upon the identiﬁcation of an appropriate distance metric on the input data. It is often beneﬁcial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semideﬁnite. Semideﬁnite programming is often used to enforce this constraint, but does not scale well and is not easy to implement. B OOST M ETRIC is instead based on the observation that any positive semideﬁnite matrix can be decomposed into a linear combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting methods are easy to implement, efﬁcient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semideﬁnite matrix with trace and rank being one rather than a classiﬁer or regressor. Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. Keywords: Mahalanobis distance, semideﬁnite programming, column generation, boosting, Lagrange duality, large margin nearest neighbor</p><p>3 0.17423247 <a title="33-tfidf-3" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>Author: Prateek Jain, Brian Kulis, Jason V. Davis, Inderjit S. Dhillon</p><p>Abstract: Metric and kernel learning arise in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem. In particular, we propose a general optimization framework for learning metrics via linear transformations, and analyze in detail a special case of our framework—that of minimizing the LogDet divergence subject to linear constraints. We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework. Our theoretical connections between metric and kernel learning have two main consequences: 1) the learned kernel matrix parameterizes a linear transformation kernel function and can be applied inductively to new data points, 2) our result yields a constructive method for kernelizing most existing Mahalanobis metric learning formulations. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction. Keywords: divergence metric learning, kernel learning, linear transformation, matrix divergences, logdet</p><p>4 0.13423945 <a title="33-tfidf-4" href="./jmlr-2012-Linear_Fitted-Q_Iteration_with_Multiple_Reward_Functions.html">58 jmlr-2012-Linear Fitted-Q Iteration with Multiple Reward Functions</a></p>
<p>Author: Daniel J. Lizotte, Michael Bowling, Susan A. Murphy</p><p>Abstract: We present a general and detailed development of an algorithm for ﬁnite-horizon ﬁtted-Q iteration with an arbitrary number of reward signals and linear value function approximation using an arbitrary number of state features. This includes a detailed treatment of the 3-reward function case using triangulation primitives from computational geometry and a method for identifying globally dominated actions. We also present an example of how our methods can be used to construct a realworld decision aid by considering symptom reduction, weight gain, and quality of life in sequential treatments for schizophrenia. Finally, we discuss future directions in which to take this work that will further enable our methods to make a positive impact on the ﬁeld of evidence-based clinical decision support. Keywords: reinforcement learning, dynamic programming, decision making, linear regression, preference elicitation</p><p>5 0.091522247 <a title="33-tfidf-5" href="./jmlr-2012-Online_Submodular_Minimization.html">84 jmlr-2012-Online Submodular Minimization</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We consider an online decision problem over a discrete space in which the loss function is submodular. We give algorithms which are computationally efﬁcient and are Hannan-consistent in both the full information and partial feedback settings. Keywords: submodular optimization, online learning, regret minimization</p><p>6 0.082008779 <a title="33-tfidf-6" href="./jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems.html">89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</a></p>
<p>7 0.076596051 <a title="33-tfidf-7" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>8 0.066379517 <a title="33-tfidf-8" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>9 0.06472297 <a title="33-tfidf-9" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>10 0.053418897 <a title="33-tfidf-10" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>11 0.053179864 <a title="33-tfidf-11" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>12 0.050252274 <a title="33-tfidf-12" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>13 0.047838796 <a title="33-tfidf-13" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>14 0.045717701 <a title="33-tfidf-14" href="./jmlr-2012-A_Model_of_the_Perception_of_Facial_Expressions_of_Emotion_by_Humans%3A_Research_Overview_and_Perspectives.html">6 jmlr-2012-A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives</a></p>
<p>15 0.044950988 <a title="33-tfidf-15" href="./jmlr-2012-A_Local_Spectral_Method_for_Graphs%3A_With_Applications_to_Improving_Graph_Partitions_and_Exploring_Data_Graphs_Locally.html">5 jmlr-2012-A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally</a></p>
<p>16 0.043986719 <a title="33-tfidf-16" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>17 0.043250415 <a title="33-tfidf-17" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<p>18 0.043081094 <a title="33-tfidf-18" href="./jmlr-2012-Optimistic_Bayesian_Sampling_in_Contextual-Bandit_Problems.html">86 jmlr-2012-Optimistic Bayesian Sampling in Contextual-Bandit Problems</a></p>
<p>19 0.042241141 <a title="33-tfidf-19" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>20 0.040246062 <a title="33-tfidf-20" href="./jmlr-2012-Iterative_Reweighted_Algorithms_for_Matrix_Rank_Minimization.html">52 jmlr-2012-Iterative Reweighted Algorithms for Matrix Rank Minimization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.25), (1, -0.088), (2, 0.2), (3, 0.123), (4, -0.387), (5, -0.029), (6, -0.241), (7, 0.13), (8, -0.032), (9, -0.055), (10, 0.057), (11, 0.042), (12, -0.082), (13, -0.081), (14, 0.048), (15, -0.019), (16, -0.065), (17, 0.138), (18, 0.132), (19, -0.07), (20, -0.033), (21, 0.082), (22, -0.013), (23, -0.232), (24, 0.014), (25, -0.073), (26, -0.05), (27, -0.161), (28, -0.041), (29, -0.144), (30, 0.001), (31, 0.048), (32, 0.008), (33, 0.026), (34, -0.041), (35, 0.048), (36, -0.117), (37, -0.076), (38, -0.062), (39, -0.022), (40, 0.072), (41, 0.02), (42, 0.01), (43, -0.031), (44, 0.005), (45, 0.058), (46, 0.063), (47, 0.06), (48, 0.102), (49, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95226622 <a title="33-lsi-1" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>Author: Yiming Ying, Peng Li</p><p>Abstract: The main theme of this paper is to develop a novel eigenvalue optimization framework for learning a Mahalanobis metric. Within this context, we introduce a novel metric learning approach called DML-eig which is shown to be equivalent to a well-known eigenvalue optimization problem called minimizing the maximal eigenvalue of a symmetric matrix (Overton, 1988; Lewis and Overton, 1996). Moreover, we formulate LMNN (Weinberger et al., 2005), one of the state-of-the-art metric learning methods, as a similar eigenvalue optimization problem. This novel framework not only provides new insights into metric learning but also opens new avenues to the design of efﬁcient metric learning algorithms. Indeed, ﬁrst-order algorithms are developed for DML-eig and LMNN which only need the computation of the largest eigenvector of a matrix per iteration. Their convergence characteristics are rigorously established. Various experiments on benchmark data sets show the competitive performance of our new approaches. In addition, we report an encouraging result on a difﬁcult and challenging face veriﬁcation data set called Labeled Faces in the Wild (LFW). Keywords: metric learning, convex optimization, semi-deﬁnite programming, ﬁrst-order methods, eigenvalue optimization, matrix factorization, face veriﬁcation</p><p>2 0.67954475 <a title="33-lsi-2" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel</p><p>Abstract: The success of many machine learning and pattern recognition methods relies heavily upon the identiﬁcation of an appropriate distance metric on the input data. It is often beneﬁcial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semideﬁnite. Semideﬁnite programming is often used to enforce this constraint, but does not scale well and is not easy to implement. B OOST M ETRIC is instead based on the observation that any positive semideﬁnite matrix can be decomposed into a linear combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting methods are easy to implement, efﬁcient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semideﬁnite matrix with trace and rank being one rather than a classiﬁer or regressor. Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. Keywords: Mahalanobis distance, semideﬁnite programming, column generation, boosting, Lagrange duality, large margin nearest neighbor</p><p>3 0.67059988 <a title="33-lsi-3" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>Author: Prateek Jain, Brian Kulis, Jason V. Davis, Inderjit S. Dhillon</p><p>Abstract: Metric and kernel learning arise in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem. In particular, we propose a general optimization framework for learning metrics via linear transformations, and analyze in detail a special case of our framework—that of minimizing the LogDet divergence subject to linear constraints. We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework. Our theoretical connections between metric and kernel learning have two main consequences: 1) the learned kernel matrix parameterizes a linear transformation kernel function and can be applied inductively to new data points, 2) our result yields a constructive method for kernelizing most existing Mahalanobis metric learning formulations. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction. Keywords: divergence metric learning, kernel learning, linear transformation, matrix divergences, logdet</p><p>4 0.51862615 <a title="33-lsi-4" href="./jmlr-2012-Linear_Fitted-Q_Iteration_with_Multiple_Reward_Functions.html">58 jmlr-2012-Linear Fitted-Q Iteration with Multiple Reward Functions</a></p>
<p>Author: Daniel J. Lizotte, Michael Bowling, Susan A. Murphy</p><p>Abstract: We present a general and detailed development of an algorithm for ﬁnite-horizon ﬁtted-Q iteration with an arbitrary number of reward signals and linear value function approximation using an arbitrary number of state features. This includes a detailed treatment of the 3-reward function case using triangulation primitives from computational geometry and a method for identifying globally dominated actions. We also present an example of how our methods can be used to construct a realworld decision aid by considering symptom reduction, weight gain, and quality of life in sequential treatments for schizophrenia. Finally, we discuss future directions in which to take this work that will further enable our methods to make a positive impact on the ﬁeld of evidence-based clinical decision support. Keywords: reinforcement learning, dynamic programming, decision making, linear regression, preference elicitation</p><p>5 0.35579741 <a title="33-lsi-5" href="./jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems.html">89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</a></p>
<p>Author: Carl Brunner, Andreas Fischer, Klaus Luig, Thorsten Thies</p><p>Abstract: Pairwise classiﬁcation is the task to predict whether the examples a, b of a pair (a, b) belong to the same class or to different classes. In particular, interclass generalization problems can be treated in this way. In pairwise classiﬁcation, the order of the two input examples should not affect the classiﬁcation result. To achieve this, particular kernels as well as the use of symmetric training sets in the framework of support vector machines were suggested. The paper discusses both approaches in a general way and establishes a strong connection between them. In addition, an efﬁcient implementation is discussed which allows the training of several millions of pairs. The value of these contributions is conﬁrmed by excellent results on the labeled faces in the wild benchmark. Keywords: pairwise support vector machines, interclass generalization, pairwise kernels, large scale problems</p><p>6 0.29421461 <a title="33-lsi-6" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>7 0.28283924 <a title="33-lsi-7" href="./jmlr-2012-Online_Submodular_Minimization.html">84 jmlr-2012-Online Submodular Minimization</a></p>
<p>8 0.27382821 <a title="33-lsi-8" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>9 0.24819493 <a title="33-lsi-9" href="./jmlr-2012-An_Active_Learning_Algorithm_for_Ranking_from_Pairwise_Preferences_with_an_Almost_Optimal_Query_Complexity.html">17 jmlr-2012-An Active Learning Algorithm for Ranking from Pairwise Preferences with an Almost Optimal Query Complexity</a></p>
<p>10 0.23944266 <a title="33-lsi-10" href="./jmlr-2012-Iterative_Reweighted_Algorithms_for_Matrix_Rank_Minimization.html">52 jmlr-2012-Iterative Reweighted Algorithms for Matrix Rank Minimization</a></p>
<p>11 0.235245 <a title="33-lsi-11" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>12 0.23076503 <a title="33-lsi-12" href="./jmlr-2012-Active_Clustering_of_Biological_Sequences.html">12 jmlr-2012-Active Clustering of Biological Sequences</a></p>
<p>13 0.22802302 <a title="33-lsi-13" href="./jmlr-2012-A_Model_of_the_Perception_of_Facial_Expressions_of_Emotion_by_Humans%3A_Research_Overview_and_Perspectives.html">6 jmlr-2012-A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives</a></p>
<p>14 0.2234834 <a title="33-lsi-14" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>15 0.22283359 <a title="33-lsi-15" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>16 0.20652789 <a title="33-lsi-16" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>17 0.20613897 <a title="33-lsi-17" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>18 0.20133473 <a title="33-lsi-18" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>19 0.20095505 <a title="33-lsi-19" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>20 0.19993097 <a title="33-lsi-20" href="./jmlr-2012-A_Local_Spectral_Method_for_Graphs%3A_With_Applications_to_Improving_Graph_Partitions_and_Exploring_Data_Graphs_Locally.html">5 jmlr-2012-A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.013), (21, 0.032), (26, 0.088), (29, 0.018), (35, 0.016), (49, 0.011), (56, 0.016), (59, 0.013), (64, 0.431), (69, 0.031), (75, 0.056), (77, 0.014), (81, 0.01), (92, 0.058), (96, 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95711184 <a title="33-lda-1" href="./jmlr-2012-Smoothing_Multivariate_Performance_Measures.html">107 jmlr-2012-Smoothing Multivariate Performance Measures</a></p>
<p>Author: Xinhua Zhang, Ankan Saha, S.V.N. Vishwanathan</p><p>Abstract: Optimizing multivariate performance measure is an important task in Machine Learning. Joachims (2005) introduced a Support Vector Method whose underlying optimization problem is commonly solved by cutting plane methods (CPMs) such as SVM-Perf and BMRM. It can be shown that CPMs 1 converge to an ε accurate solution in O λε iterations, where λ is the trade-off parameter between the regularizer and the loss function. Motivated by the impressive convergence rate of CPM on a number of practical problems, it was conjectured that these rates can be further improved. We disprove this conjecture in this paper by constructing counter examples. However, surprisingly, we further discover that these problems are not inherently hard, and we develop a novel smoothing strategy, which in conjunction with Nesterov’s accelerated gradient method, can ﬁnd an ε accuiterations. Computationally, our smoothing technique is also rate solution in O∗ min 1 , √1 ε λε particularly advantageous for optimizing multivariate performance scores such as precision/recall break-even point and ROCArea; the cost per iteration remains the same as that of CPMs. Empirical evaluation on some of the largest publicly available data sets shows that our method converges signiﬁcantly faster than CPMs without sacriﬁcing generalization ability. Keywords: non-smooth optimization, max-margin methods, multivariate performance measures, Support Vector Machines, smoothing</p><p>2 0.78867948 <a title="33-lda-2" href="./jmlr-2012-Query_Strategies_for_Evading_Convex-Inducing_Classifiers.html">94 jmlr-2012-Query Strategies for Evading Convex-Inducing Classifiers</a></p>
<p>Author: Blaine Nelson, Benjamin I. P. Rubinstein, Ling Huang, Anthony D. Joseph, Steven J. Lee, Satish Rao, J. D. Tygar</p><p>Abstract: Classiﬁers are often used to detect miscreant activities. We study how an adversary can systematically query a classiﬁer to elicit information that allows the attacker to evade detection while incurring a near-minimal cost of modifying their intended malfeasance. We generalize the theory of Lowd and Meek (2005) to the family of convex-inducing classiﬁers that partition their feature space into two sets, one of which is convex. We present query algorithms for this family that construct undetected instances of approximately minimal cost using only polynomially-many queries in the dimension of the space and in the level of approximation. Our results demonstrate that nearoptimal evasion can be accomplished for this family without reverse engineering the classiﬁer’s decision boundary. We also consider general ℓ p costs and show that near-optimal evasion on the family of convex-inducing classiﬁers is generally efﬁcient for both positive and negative convexity for all levels of approximation if p = 1. Keywords: query algorithms, evasion, reverse engineering, adversarial learning</p><p>same-paper 3 0.7498607 <a title="33-lda-3" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>Author: Yiming Ying, Peng Li</p><p>Abstract: The main theme of this paper is to develop a novel eigenvalue optimization framework for learning a Mahalanobis metric. Within this context, we introduce a novel metric learning approach called DML-eig which is shown to be equivalent to a well-known eigenvalue optimization problem called minimizing the maximal eigenvalue of a symmetric matrix (Overton, 1988; Lewis and Overton, 1996). Moreover, we formulate LMNN (Weinberger et al., 2005), one of the state-of-the-art metric learning methods, as a similar eigenvalue optimization problem. This novel framework not only provides new insights into metric learning but also opens new avenues to the design of efﬁcient metric learning algorithms. Indeed, ﬁrst-order algorithms are developed for DML-eig and LMNN which only need the computation of the largest eigenvector of a matrix per iteration. Their convergence characteristics are rigorously established. Various experiments on benchmark data sets show the competitive performance of our new approaches. In addition, we report an encouraging result on a difﬁcult and challenging face veriﬁcation data set called Labeled Faces in the Wild (LFW). Keywords: metric learning, convex optimization, semi-deﬁnite programming, ﬁrst-order methods, eigenvalue optimization, matrix factorization, face veriﬁcation</p><p>4 0.74137819 <a title="33-lda-4" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>Author: Stephen Gould</p><p>Abstract: We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data ﬂows. Keywords: machine learning, graphical models, computer vision, open-source software</p><p>5 0.46315354 <a title="33-lda-5" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel</p><p>Abstract: The success of many machine learning and pattern recognition methods relies heavily upon the identiﬁcation of an appropriate distance metric on the input data. It is often beneﬁcial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semideﬁnite. Semideﬁnite programming is often used to enforce this constraint, but does not scale well and is not easy to implement. B OOST M ETRIC is instead based on the observation that any positive semideﬁnite matrix can be decomposed into a linear combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting methods are easy to implement, efﬁcient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semideﬁnite matrix with trace and rank being one rather than a classiﬁer or regressor. Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. Keywords: Mahalanobis distance, semideﬁnite programming, column generation, boosting, Lagrange duality, large margin nearest neighbor</p><p>6 0.37981817 <a title="33-lda-6" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>7 0.37527257 <a title="33-lda-7" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>8 0.37183517 <a title="33-lda-8" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>9 0.3714188 <a title="33-lda-9" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<p>10 0.37052545 <a title="33-lda-10" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>11 0.36531293 <a title="33-lda-11" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>12 0.36477798 <a title="33-lda-12" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>13 0.3629207 <a title="33-lda-13" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>14 0.36280364 <a title="33-lda-14" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>15 0.35999894 <a title="33-lda-15" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>16 0.35724357 <a title="33-lda-16" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>17 0.35317141 <a title="33-lda-17" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>18 0.35250708 <a title="33-lda-18" href="./jmlr-2012-Online_Submodular_Minimization.html">84 jmlr-2012-Online Submodular Minimization</a></p>
<p>19 0.35028142 <a title="33-lda-19" href="./jmlr-2012-Security_Analysis_of_Online_Centroid_Anomaly_Detection.html">104 jmlr-2012-Security Analysis of Online Centroid Anomaly Detection</a></p>
<p>20 0.34808514 <a title="33-lda-20" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
