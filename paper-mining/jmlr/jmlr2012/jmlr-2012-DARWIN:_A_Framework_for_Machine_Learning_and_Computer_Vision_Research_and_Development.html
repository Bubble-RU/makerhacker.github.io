<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-30" href="#">jmlr2012-30</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</h1>
<br/><p>Source: <a title="jmlr-2012-30-pdf" href="http://jmlr.org/papers/volume13/gould12a/gould12a.pdf">pdf</a></p><p>Author: Stephen Gould</p><p>Abstract: We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data ﬂows. Keywords: machine learning, graphical models, computer vision, open-source software</p><p>Reference: <a title="jmlr-2012-30-reference" href="../jmlr2012_reference/jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 AU  Research School of Computer Science The Australian National University Canberra, ACT 0200, Australia  Editor: Antti Honkela  Abstract We present an open-source platform-independent C++ framework for machine learning and computer vision research. [sent-4, score-0.116]
</p><p>2 The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. [sent-5, score-0.321]
</p><p>3 The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data ﬂows. [sent-6, score-0.529]
</p><p>4 Keywords: machine learning, graphical models, computer vision, open-source software  1. [sent-7, score-0.145]
</p><p>5 Introduction Machine learning and computer vision researchers have beneﬁted over the last few years with the increased availability of numerous high quality open-source toolkits, (e. [sent-8, score-0.151]
</p><p>6 These toolkits focus on delivering efﬁcient implementations of mature algorithms but often omit infrastructure necessary for building end-to-end applications or experimenting with new algorithms. [sent-12, score-0.343]
</p><p>7 Furthermore, domain speciﬁc applications, such as those in computer vision, often require functionality from multiple packages complicating integration and maintenance. [sent-13, score-0.204]
</p><p>8 We have developed the DARWIN framework for machine learning and computer vision that aims to alleviate these limitations. [sent-14, score-0.116]
</p><p>9 The goal of the DARWIN framework is two-fold: First, to provide a stable, robust and efﬁcient library for machine learning practitioners, and second, to provide infrastructure for students and researchers to experiment with and extend state-of-the-art methods. [sent-15, score-0.427]
</p><p>10 To this end, we provide infrastructure for data management, logging and conﬁguration; a consistent interface to standard machine learning and probabilistic graphical models algorithms; and well documented and easy to extend source code. [sent-16, score-0.501]
</p><p>11 The code is divided into ﬁve main libraries, which we describe below. [sent-22, score-0.046]
</p><p>12 Sample applications wrap these libraries to provide out-of-thebox solutions, such classiﬁer training and evaluation or probabilistic inference in graphical models. [sent-23, score-0.39]
</p><p>13 G OULD  The libraries have minimal dependence on external codebases, making DARWIN easy to install and maintain. [sent-26, score-0.13]
</p><p>14 Speciﬁcally, we require RapidXML1 for XML parsing and Eigen2 for linear algebra. [sent-27, score-0.047]
</p><p>15 OpenCV (Bradski and Kaehler, 2008) is required only for the (optional) computer vision component of the framework. [sent-28, score-0.116]
</p><p>16 Applications may link to other libraries to provide additional functionality, for example, GUI interfaces. [sent-29, score-0.13]
</p><p>17 An illustration of the library components and dependencies is shown in Figure 1(a). [sent-30, score-0.242]
</p><p>18 (a)  (b)  Figure 1: (a) Illustration of the library components and dependencies of the DARWIN framework. [sent-31, score-0.242]
</p><p>19 (b) Screenshot of the experimental GUI for developing machine learning data ﬂows. [sent-33, score-0.04]
</p><p>20 Common input/output functionality is provided by the drwnIO library. [sent-37, score-0.165]
</p><p>21 The main component of this library is a persistent data store for managing ﬁlesystem storage and in-memory caching of multiple data records. [sent-38, score-0.243]
</p><p>22 The machine learning library, drwnML, contains implementations of various algorithms for supervised and unsupervised machine learning. [sent-40, score-0.06]
</p><p>23 The library also contains utilities for dimensionality reduction (such as PCA and Fisher’s LDA), unconstrained optimization, linear programming, and maintaining sufﬁcient statistics and collating classiﬁcation results. [sent-42, score-0.268]
</p><p>24 The container can be used to efﬁciently store sparse vectors with minimal changes to existing code. [sent-44, score-0.07]
</p><p>25 The probabilistic graphical models library, drwnPGM, provides functionality for (approximate) inference in structured probability spaces over discrete random variables. [sent-56, score-0.425]
</p><p>26 A factor graph data structure is used to specify the joint distribution in terms of functions, or factors, on overlapping subsets of the variables. [sent-57, score-0.033]
</p><p>27 The library provides classes for specifying factors and performing common factor operations, such as marginalization. [sent-58, score-0.209]
</p><p>28 Reference implementations for many inference algorithms are also provided. [sent-60, score-0.138]
</p><p>29 , 2008; Meshi and Globerson, 2011) Our implementations perform favorably against other graphical models inference packages, (e. [sent-66, score-0.283]
</p><p>30 The drwnVision library builds on the OpenCV computer vision library (Bradski and Kaehler, 2008) to provide additional functionality for scene understanding. [sent-75, score-0.782]
</p><p>31 For example, the library provides infrastructure for multi-class semantic segmentation. [sent-76, score-0.396]
</p><p>32 One approach is to construct a conditional Markov random ﬁeld (CRF) over pixels in the image with learned unary and pairwise terms (He et al. [sent-78, score-0.161]
</p><p>33 Another approach is to perform non-parametric label transfer by matching into a corpus of annotated images, (e. [sent-81, score-0.056]
</p><p>34 The vision library provides instances of both approaches and XML conﬁgurable applications make it easy to train and evaluate on new data sets. [sent-85, score-0.325]
</p><p>35 Moreover, documentation and scripts for generating results on standard data sets are distributed with the library. [sent-86, score-0.077]
</p><p>36 It includes learned boosted decision tree classiﬁers for the unary terms and cross-validated contrast sensitive pairwise terms. [sent-89, score-0.128]
</p><p>37 6%) accuracy on the semantic (geometric) labeling task for the unary and CRF model, respectively. [sent-97, score-0.168]
</p><p>38 Our label transfer implementation makes use of the PatchMatch algorithm (Barnes et al. [sent-98, score-0.056]
</p><p>39 , 2009) to construct a graph of dense patch correspondences between images. [sent-99, score-0.187]
</p><p>40 Projects and Applications The DARWIN framework is distributed with source code for a number of applications and projects that expose the functionality of the library, for example, training and evaluating classiﬁers or performing inference in probabilistic graphical models. [sent-110, score-0.584]
</p><p>41 Matlab mex wrappers are also provided for accessing core algorithms within the framework from the Matlab environment. [sent-111, score-0.12]
</p><p>42 For non-expect machine learning practitioners it is often helpful to provide a graphical environment for designing machine learning pipelines. [sent-112, score-0.194]
</p><p>43 To this end, we have developed an experimental graphical user interface (GUI) for constructing machine learning data ﬂows. [sent-113, score-0.192]
</p><p>44 We intend to continue developing the GUI in future releases. [sent-116, score-0.04]
</p><p>45 Documentation Comprehensive documentation, including download and installation instructions, API reference, and short tutorial is available online at http://drwn. [sent-118, score-0.045]
</p><p>46 The documentation can also be generated locally from the source code using Doxygen. [sent-122, score-0.165]
</p><p>47 PatchMatch: A randomized correspondence algorithm for structural image editing. [sent-132, score-0.033]
</p><p>48 Microsoft research cambridge (MSRC) object recognition pixel-wise labeled image database (version 2), 2004. [sent-154, score-0.033]
</p><p>49 Available for download under license from MSR at http://research. [sent-155, score-0.045]
</p><p>50 Fixing max-product: Convergent message passing algorithms for MAP LP-relaxations. [sent-178, score-0.165]
</p><p>51 PatchMatchGraph: Building a graph of dense patch correspondences for label transfer. [sent-183, score-0.187]
</p><p>52 Decomposing a scene into geometric and semantically consistent regions. [sent-189, score-0.083]
</p><p>53 Nonparametric scene parsing: Label transfer via dense scene alignment. [sent-229, score-0.26]
</p><p>54 libDAI: A free and open source C++ library for discrete approximate inference in graphical models. [sent-239, score-0.474]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('darwin', 0.462), ('gould', 0.291), ('gui', 0.249), ('library', 0.209), ('opencv', 0.166), ('functionality', 0.165), ('infrastructure', 0.147), ('graphical', 0.145), ('libraries', 0.13), ('crf', 0.128), ('unary', 0.128), ('xml', 0.128), ('bradski', 0.125), ('meshi', 0.125), ('sontag', 0.125), ('vision', 0.116), ('msrc', 0.107), ('globerson', 0.099), ('message', 0.099), ('jmlr', 0.099), ('barnes', 0.083), ('boykov', 0.083), ('jaimovich', 0.083), ('kaehler', 0.083), ('komodakis', 0.083), ('lesystem', 0.083), ('logging', 0.083), ('meltzer', 0.083), ('ould', 0.083), ('patchmatch', 0.083), ('screenshot', 0.083), ('shotton', 0.083), ('stair', 0.083), ('yanover', 0.083), ('scene', 0.083), ('relaxations', 0.078), ('inference', 0.078), ('documentation', 0.077), ('correspondences', 0.071), ('iccv', 0.067), ('passing', 0.066), ('stanford', 0.065), ('eccv', 0.064), ('toolkits', 0.064), ('implementations', 0.06), ('utilities', 0.059), ('transfer', 0.056), ('wrappers', 0.055), ('jaakkola', 0.052), ('convergent', 0.052), ('lp', 0.052), ('routines', 0.052), ('practitioners', 0.049), ('koller', 0.049), ('stephen', 0.048), ('cvpr', 0.047), ('parsing', 0.047), ('interface', 0.047), ('code', 0.046), ('download', 0.045), ('patch', 0.045), ('optional', 0.045), ('source', 0.042), ('energy', 0.041), ('semantic', 0.04), ('developing', 0.04), ('packages', 0.039), ('dense', 0.038), ('probabilistic', 0.037), ('management', 0.036), ('expose', 0.036), ('bsd', 0.036), ('container', 0.036), ('contributors', 0.036), ('criminisi', 0.036), ('delivering', 0.036), ('eigen', 0.036), ('fulton', 0.036), ('gashler', 0.036), ('icm', 0.036), ('mature', 0.036), ('mcgraw', 0.036), ('pami', 0.036), ('reilly', 0.036), ('shechtman', 0.036), ('students', 0.036), ('tightening', 0.036), ('waf', 0.036), ('matlab', 0.035), ('researchers', 0.035), ('projects', 0.035), ('store', 0.034), ('core', 0.033), ('image', 0.033), ('graph', 0.033), ('dependencies', 0.033), ('fixing', 0.032), ('anu', 0.032), ('gurable', 0.032), ('accessing', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="30-tfidf-1" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>Author: Stephen Gould</p><p>Abstract: We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data ﬂows. Keywords: machine learning, graphical models, computer vision, open-source software</p><p>2 0.061070994 <a title="30-tfidf-2" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>Author: Tom De Smedt, Walter Daelemans</p><p>Abstract: Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classiﬁers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern. Keywords: Python, data mining, natural language processing, machine learning, graph networks</p><p>3 0.051603727 <a title="30-tfidf-3" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>4 0.05050993 <a title="30-tfidf-4" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>Author: Hannes Nickisch</p><p>Abstract: The glm-ie toolbox contains functionality for estimation and inference in generalised linear models over continuous-valued variables. Besides a variety of penalised least squares solvers for estimation, it offers inference based on (convex) variational bounds, on expectation propagation and on factorial mean ﬁeld. Scalable and efﬁcient inference in fully-connected undirected graphical models or Markov random ﬁelds with Gaussian and non-Gaussian potentials is achieved by casting all the computations as matrix vector multiplications. We provide a wide choice of penalty functions for estimation, potential functions for inference and matrix classes with lazy evaluation for convenient modelling. We designed the glm-ie package to be simple, generic and easily expansible. Most of the code is written in Matlab including some MEX ﬁles to be fully compatible to both Matlab 7.x and GNU Octave 3.3.x. Large scale probabilistic classiﬁcation as well as sparse linear modelling can be performed in a common algorithmical framework by the glm-ie toolkit. Keywords: sparse linear models, generalised linear models, Bayesian inference, approximate inference, probabilistic regression and classiﬁcation, penalised least squares estimation, lazy evaluation matrix class</p><p>5 0.04651754 <a title="30-tfidf-5" href="./jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</a></p>
<p>Author: Rahul Mazumder,  Trevor Hastie</p><p>Abstract: We consider the sparse inverse covariance regularization problem or graphical lasso with regularization parameter λ. Suppose the sample covariance graph formed by thresholding the entries of the sample covariance matrix at λ is decomposed into connected components. We show that the vertex-partition induced by the connected components of the thresholded sample covariance graph (at λ) is exactly equal to that induced by the connected components of the estimated concentration graph, obtained by solving the graphical lasso problem for the same λ. This characterizes a very interesting property of a path of graphical lasso solutions. Furthermore, this simple rule, when used as a wrapper around existing algorithms for the graphical lasso, leads to enormous performance gains. For a range of values of λ, our proposal splits a large graphical lasso problem into smaller tractable problems, making it possible to solve an otherwise infeasible large-scale problem. We illustrate the graceful scalability of our proposal via synthetic and real-life microarray examples. Keywords: sparse inverse covariance selection, sparsity, graphical lasso, Gaussian graphical models, graph connected components, concentration graph, large scale covariance estimation</p><p>6 0.044182684 <a title="30-tfidf-6" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>7 0.044163816 <a title="30-tfidf-7" href="./jmlr-2012-The_huge_Package_for_High-dimensional_Undirected_Graph_Estimation_in_R.html">113 jmlr-2012-The huge Package for High-dimensional Undirected Graph Estimation in R</a></p>
<p>8 0.039852291 <a title="30-tfidf-8" href="./jmlr-2012-Discriminative_Hierarchical_Part-based_Models_for_Human_Parsing_and_Action_Recognition.html">32 jmlr-2012-Discriminative Hierarchical Part-based Models for Human Parsing and Action Recognition</a></p>
<p>9 0.037380721 <a title="30-tfidf-9" href="./jmlr-2012-A_Topic_Modeling_Toolbox_Using_Belief_Propagation.html">9 jmlr-2012-A Topic Modeling Toolbox Using Belief Propagation</a></p>
<p>10 0.035063129 <a title="30-tfidf-10" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>11 0.029350471 <a title="30-tfidf-11" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>12 0.029007303 <a title="30-tfidf-12" href="./jmlr-2012-Jstacs%3A_A_Java_Framework_for_Statistical_Analysis_and_Classification_of_Biological_Sequences.html">53 jmlr-2012-Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences</a></p>
<p>13 0.028173175 <a title="30-tfidf-13" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>14 0.027362011 <a title="30-tfidf-14" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>15 0.025234047 <a title="30-tfidf-15" href="./jmlr-2012-A_Model_of_the_Perception_of_Facial_Expressions_of_Emotion_by_Humans%3A_Research_Overview_and_Perspectives.html">6 jmlr-2012-A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives</a></p>
<p>16 0.025158498 <a title="30-tfidf-16" href="./jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems.html">89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</a></p>
<p>17 0.02466001 <a title="30-tfidf-17" href="./jmlr-2012-Transfer_in_Reinforcement_Learning_via_Shared_Features.html">116 jmlr-2012-Transfer in Reinforcement Learning via Shared Features</a></p>
<p>18 0.02392526 <a title="30-tfidf-18" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>19 0.023883166 <a title="30-tfidf-19" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>20 0.023408469 <a title="30-tfidf-20" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.096), (1, 0.039), (2, 0.183), (3, -0.041), (4, 0.025), (5, 0.097), (6, 0.071), (7, -0.016), (8, -0.033), (9, -0.026), (10, 0.031), (11, -0.004), (12, 0.108), (13, -0.146), (14, -0.09), (15, -0.07), (16, -0.029), (17, -0.038), (18, -0.018), (19, -0.106), (20, 0.052), (21, -0.008), (22, -0.01), (23, 0.053), (24, -0.01), (25, -0.105), (26, 0.051), (27, -0.078), (28, 0.039), (29, 0.119), (30, 0.029), (31, 0.035), (32, -0.045), (33, 0.176), (34, -0.091), (35, 0.008), (36, -0.111), (37, 0.111), (38, 0.057), (39, -0.004), (40, -0.009), (41, 0.027), (42, -0.135), (43, 0.096), (44, 0.117), (45, -0.145), (46, -0.036), (47, -0.211), (48, 0.12), (49, 0.162)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96532977 <a title="30-lsi-1" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>Author: Stephen Gould</p><p>Abstract: We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data ﬂows. Keywords: machine learning, graphical models, computer vision, open-source software</p><p>2 0.48153311 <a title="30-lsi-2" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>Author: Tom De Smedt, Walter Daelemans</p><p>Abstract: Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classiﬁers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern. Keywords: Python, data mining, natural language processing, machine learning, graph networks</p><p>3 0.45565182 <a title="30-lsi-3" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>4 0.42028368 <a title="30-lsi-4" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>Author: Hannes Nickisch</p><p>Abstract: The glm-ie toolbox contains functionality for estimation and inference in generalised linear models over continuous-valued variables. Besides a variety of penalised least squares solvers for estimation, it offers inference based on (convex) variational bounds, on expectation propagation and on factorial mean ﬁeld. Scalable and efﬁcient inference in fully-connected undirected graphical models or Markov random ﬁelds with Gaussian and non-Gaussian potentials is achieved by casting all the computations as matrix vector multiplications. We provide a wide choice of penalty functions for estimation, potential functions for inference and matrix classes with lazy evaluation for convenient modelling. We designed the glm-ie package to be simple, generic and easily expansible. Most of the code is written in Matlab including some MEX ﬁles to be fully compatible to both Matlab 7.x and GNU Octave 3.3.x. Large scale probabilistic classiﬁcation as well as sparse linear modelling can be performed in a common algorithmical framework by the glm-ie toolkit. Keywords: sparse linear models, generalised linear models, Bayesian inference, approximate inference, probabilistic regression and classiﬁcation, penalised least squares estimation, lazy evaluation matrix class</p><p>5 0.4011423 <a title="30-lsi-5" href="./jmlr-2012-Discriminative_Hierarchical_Part-based_Models_for_Human_Parsing_and_Action_Recognition.html">32 jmlr-2012-Discriminative Hierarchical Part-based Models for Human Parsing and Action Recognition</a></p>
<p>Author: Yang Wang, Duan Tran, Zicheng Liao, David Forsyth</p><p>Abstract: We consider the problem of parsing human poses and recognizing their actions in static images with part-based models. Most previous work in part-based models only considers rigid parts (e.g., torso, head, half limbs) guided by human anatomy. We argue that this representation of parts is not necessarily appropriate. In this paper, we introduce hierarchical poselets—a new representation for modeling the pose conﬁguration of human bodies. Hierarchical poselets can be rigid parts, but they can also be parts that cover large portions of human bodies (e.g., torso + left arm). In the extreme case, they can be the whole bodies. The hierarchical poselets are organized in a hierarchical way via a structured model. Human parsing can be achieved by inferring the optimal labeling of this hierarchical model. The pose information captured by this hierarchical model can also be used as a intermediate representation for other high-level tasks. We demonstrate it in action recognition from static images. Keywords: human parsing, action recognition, part-based models, hierarchical poselets, maxmargin structured learning</p><p>6 0.36297327 <a title="30-lsi-6" href="./jmlr-2012-Jstacs%3A_A_Java_Framework_for_Statistical_Analysis_and_Classification_of_Biological_Sequences.html">53 jmlr-2012-Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences</a></p>
<p>7 0.27266935 <a title="30-lsi-7" href="./jmlr-2012-A_Model_of_the_Perception_of_Facial_Expressions_of_Emotion_by_Humans%3A_Research_Overview_and_Perspectives.html">6 jmlr-2012-A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives</a></p>
<p>8 0.26399994 <a title="30-lsi-8" href="./jmlr-2012-The_huge_Package_for_High-dimensional_Undirected_Graph_Estimation_in_R.html">113 jmlr-2012-The huge Package for High-dimensional Undirected Graph Estimation in R</a></p>
<p>9 0.25187451 <a title="30-lsi-9" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>10 0.22253482 <a title="30-lsi-10" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>11 0.21576709 <a title="30-lsi-11" href="./jmlr-2012-A_Topic_Modeling_Toolbox_Using_Belief_Propagation.html">9 jmlr-2012-A Topic Modeling Toolbox Using Belief Propagation</a></p>
<p>12 0.2088345 <a title="30-lsi-12" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>13 0.20708762 <a title="30-lsi-13" href="./jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</a></p>
<p>14 0.20289534 <a title="30-lsi-14" href="./jmlr-2012-Transfer_in_Reinforcement_Learning_via_Shared_Features.html">116 jmlr-2012-Transfer in Reinforcement Learning via Shared Features</a></p>
<p>15 0.19966483 <a title="30-lsi-15" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>16 0.18668348 <a title="30-lsi-16" href="./jmlr-2012-Characterization_and_Greedy_Learning_of_Interventional_Markov_Equivalence_Classes_of_Directed_Acyclic_Graphs.html">25 jmlr-2012-Characterization and Greedy Learning of Interventional Markov Equivalence Classes of Directed Acyclic Graphs</a></p>
<p>17 0.18449859 <a title="30-lsi-17" href="./jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</a></p>
<p>18 0.18182902 <a title="30-lsi-18" href="./jmlr-2012-Quantum_Set_Intersection_and_its_Application_to_Associative_Memory.html">93 jmlr-2012-Quantum Set Intersection and its Application to Associative Memory</a></p>
<p>19 0.17377083 <a title="30-lsi-19" href="./jmlr-2012-Iterative_Reweighted_Algorithms_for_Matrix_Rank_Minimization.html">52 jmlr-2012-Iterative Reweighted Algorithms for Matrix Rank Minimization</a></p>
<p>20 0.17197494 <a title="30-lsi-20" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.014), (21, 0.031), (26, 0.042), (27, 0.062), (29, 0.028), (56, 0.041), (64, 0.307), (68, 0.191), (69, 0.03), (75, 0.032), (79, 0.01), (92, 0.018), (96, 0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81722456 <a title="30-lda-1" href="./jmlr-2012-Smoothing_Multivariate_Performance_Measures.html">107 jmlr-2012-Smoothing Multivariate Performance Measures</a></p>
<p>Author: Xinhua Zhang, Ankan Saha, S.V.N. Vishwanathan</p><p>Abstract: Optimizing multivariate performance measure is an important task in Machine Learning. Joachims (2005) introduced a Support Vector Method whose underlying optimization problem is commonly solved by cutting plane methods (CPMs) such as SVM-Perf and BMRM. It can be shown that CPMs 1 converge to an ε accurate solution in O λε iterations, where λ is the trade-off parameter between the regularizer and the loss function. Motivated by the impressive convergence rate of CPM on a number of practical problems, it was conjectured that these rates can be further improved. We disprove this conjecture in this paper by constructing counter examples. However, surprisingly, we further discover that these problems are not inherently hard, and we develop a novel smoothing strategy, which in conjunction with Nesterov’s accelerated gradient method, can ﬁnd an ε accuiterations. Computationally, our smoothing technique is also rate solution in O∗ min 1 , √1 ε λε particularly advantageous for optimizing multivariate performance scores such as precision/recall break-even point and ROCArea; the cost per iteration remains the same as that of CPMs. Empirical evaluation on some of the largest publicly available data sets shows that our method converges signiﬁcantly faster than CPMs without sacriﬁcing generalization ability. Keywords: non-smooth optimization, max-margin methods, multivariate performance measures, Support Vector Machines, smoothing</p><p>same-paper 2 0.7857098 <a title="30-lda-2" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>Author: Stephen Gould</p><p>Abstract: We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data ﬂows. Keywords: machine learning, graphical models, computer vision, open-source software</p><p>3 0.66849369 <a title="30-lda-3" href="./jmlr-2012-Query_Strategies_for_Evading_Convex-Inducing_Classifiers.html">94 jmlr-2012-Query Strategies for Evading Convex-Inducing Classifiers</a></p>
<p>Author: Blaine Nelson, Benjamin I. P. Rubinstein, Ling Huang, Anthony D. Joseph, Steven J. Lee, Satish Rao, J. D. Tygar</p><p>Abstract: Classiﬁers are often used to detect miscreant activities. We study how an adversary can systematically query a classiﬁer to elicit information that allows the attacker to evade detection while incurring a near-minimal cost of modifying their intended malfeasance. We generalize the theory of Lowd and Meek (2005) to the family of convex-inducing classiﬁers that partition their feature space into two sets, one of which is convex. We present query algorithms for this family that construct undetected instances of approximately minimal cost using only polynomially-many queries in the dimension of the space and in the level of approximation. Our results demonstrate that nearoptimal evasion can be accomplished for this family without reverse engineering the classiﬁer’s decision boundary. We also consider general ℓ p costs and show that near-optimal evasion on the family of convex-inducing classiﬁers is generally efﬁcient for both positive and negative convexity for all levels of approximation if p = 1. Keywords: query algorithms, evasion, reverse engineering, adversarial learning</p><p>4 0.63223577 <a title="30-lda-4" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>Author: Yiming Ying, Peng Li</p><p>Abstract: The main theme of this paper is to develop a novel eigenvalue optimization framework for learning a Mahalanobis metric. Within this context, we introduce a novel metric learning approach called DML-eig which is shown to be equivalent to a well-known eigenvalue optimization problem called minimizing the maximal eigenvalue of a symmetric matrix (Overton, 1988; Lewis and Overton, 1996). Moreover, we formulate LMNN (Weinberger et al., 2005), one of the state-of-the-art metric learning methods, as a similar eigenvalue optimization problem. This novel framework not only provides new insights into metric learning but also opens new avenues to the design of efﬁcient metric learning algorithms. Indeed, ﬁrst-order algorithms are developed for DML-eig and LMNN which only need the computation of the largest eigenvector of a matrix per iteration. Their convergence characteristics are rigorously established. Various experiments on benchmark data sets show the competitive performance of our new approaches. In addition, we report an encouraging result on a difﬁcult and challenging face veriﬁcation data set called Labeled Faces in the Wild (LFW). Keywords: metric learning, convex optimization, semi-deﬁnite programming, ﬁrst-order methods, eigenvalue optimization, matrix factorization, face veriﬁcation</p><p>5 0.61415458 <a title="30-lda-5" href="./jmlr-2012-Jstacs%3A_A_Java_Framework_for_Statistical_Analysis_and_Classification_of_Biological_Sequences.html">53 jmlr-2012-Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences</a></p>
<p>Author: Jan Grau, Jens Keilwagen, André Gohr, Berit Haldemann, Stefan Posch, Ivo Grosse</p><p>Abstract: Jstacs is an object-oriented Java library for analysing and classifying sequence data, which emerged from the need for a standardized implementation of statistical models, learning principles, classiﬁers, and performance measures. In Jstacs, these components can be used, combined, and extended easily, which allows for a direct comparison of different approaches and fosters the development of new components. Jstacs is especially tailored to biological sequence data, but is also applicable to general discrete and continuous data. Jstacs is freely available at http://www.jstacs.de under the GNU GPL license including an API documentation, a cookbook, and code examples. Keywords: machine learning, statistical models, Java, bioinformatics, classiﬁcation</p><p>6 0.38792348 <a title="30-lda-6" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>7 0.35161668 <a title="30-lda-7" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>8 0.31593782 <a title="30-lda-8" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>9 0.30957335 <a title="30-lda-9" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>10 0.30397606 <a title="30-lda-10" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>11 0.3013311 <a title="30-lda-11" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>12 0.29853487 <a title="30-lda-12" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>13 0.2973457 <a title="30-lda-13" href="./jmlr-2012-A_Geometric_Approach_to_Sample_Compression.html">3 jmlr-2012-A Geometric Approach to Sample Compression</a></p>
<p>14 0.2965841 <a title="30-lda-14" href="./jmlr-2012-A_Local_Spectral_Method_for_Graphs%3A_With_Applications_to_Improving_Graph_Partitions_and_Exploring_Data_Graphs_Locally.html">5 jmlr-2012-A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally</a></p>
<p>15 0.2945531 <a title="30-lda-15" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>16 0.29323816 <a title="30-lda-16" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>17 0.2924065 <a title="30-lda-17" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>18 0.29216316 <a title="30-lda-18" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>19 0.29208225 <a title="30-lda-19" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>20 0.28963745 <a title="30-lda-20" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
