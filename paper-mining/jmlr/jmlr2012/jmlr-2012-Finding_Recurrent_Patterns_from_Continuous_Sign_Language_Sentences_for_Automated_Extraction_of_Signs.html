<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-45" href="#">jmlr2012-45</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</h1>
<br/><p>Source: <a title="jmlr-2012-45-pdf" href="http://jmlr.org/papers/volume13/nayak12a/nayak12a.pdf">pdf</a></p><p>Author: Sunita Nayak, Kester Duncan, Sudeep Sarkar, Barbara Loeding</p><p>Abstract: We present a probabilistic framework to automatically learn models of recurring signs from multiple sign language video sequences containing the vocabulary of interest. We extract the parts of the signs that are present in most occurrences of the sign in context and are robust to the variations produced by adjacent signs. Each sentence video is ﬁrst transformed into a multidimensional time series representation, capturing the motion and shape aspects of the sign. Skin color blobs are extracted from frames of color video sequences, and a probabilistic relational distribution is formed for each frame using the contour and edge pixels from the skin blobs. Each sentence is represented as a trajectory in a low dimensional space called the space of relational distributions. Given these time series trajectories, we extract signemes from multiple sentences concurrently using iterated conditional modes (ICM). We show results by learning single signs from a collection of sentences with one common pervading sign, multiple signs from a collection of sentences with more than one common sign, and single signs from a mixed collection of sentences. The extracted signemes demonstrate that our approach is robust to some extent to the variations produced within a sign due to different contexts. We also show results whereby these learned sign models are used for spotting signs in test sequences. Keywords: pattern extraction, sign language recognition, signeme extraction, sign modeling, iterated conditional modes</p><p>Reference: <a title="jmlr-2012-45-reference" href="../jmlr2012_reference/jmlr-2012-Finding_Recurrent_Patterns_from_Continuous_Sign_Language_Sentences_for_Automated_Extraction_of_Signs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Department of Special Education University of South Florida Lakeland, FL 33803, USA  Editor: Isabelle Guyon  Abstract We present a probabilistic framework to automatically learn models of recurring signs from multiple sign language video sequences containing the vocabulary of interest. [sent-8, score-0.701]
</p><p>2 We extract the parts of the signs that are present in most occurrences of the sign in context and are robust to the variations produced by adjacent signs. [sent-9, score-0.562]
</p><p>3 Each sentence video is ﬁrst transformed into a multidimensional time series representation, capturing the motion and shape aspects of the sign. [sent-10, score-0.379]
</p><p>4 Skin color blobs are extracted from frames of color video sequences, and a probabilistic relational distribution is formed for each frame using the contour and edge pixels from the skin blobs. [sent-11, score-0.698]
</p><p>5 Given these time series trajectories, we extract signemes from multiple sentences concurrently using iterated conditional modes (ICM). [sent-13, score-0.943]
</p><p>6 We show results by learning single signs from a collection of sentences with one common pervading sign, multiple signs from a collection of sentences with more than one common sign, and single signs from a mixed collection of sentences. [sent-14, score-2.083]
</p><p>7 The extracted signemes demonstrate that our approach is robust to some extent to the variations produced within a sign due to different contexts. [sent-15, score-0.519]
</p><p>8 We also show results whereby these learned sign models are used for spotting signs in test sequences. [sent-16, score-0.485]
</p><p>9 Keywords: pattern extraction, sign language recognition, signeme extraction, sign modeling, iterated conditional modes  1. [sent-17, score-0.813]
</p><p>10 Introduction Sign language research in the computer vision community has primarily focused on improving recognition rates of signs either by improving the motion representation and similarity measures (Yang et al. [sent-18, score-0.493]
</p><p>11 Ong and Ranganath (2005) presented a review of the automated sign language research and also highlighted one important issue in continuous sign language recognition. [sent-25, score-0.643]
</p><p>12 Most of the existing work in sign language assumes that the training signs are already available and often signs used in the training set are the isolated signs with the boundaries chopped off, or manually selected frames from continuous sentences. [sent-29, score-1.23]
</p><p>13 The ability to recognize isolated signs does not guarantee the recognition of signs in continuous sentences. [sent-30, score-0.601]
</p><p>14 Unlike isolated signs, a sign in a continuous sentence is strongly affected by its context in the sentence. [sent-31, score-0.466]
</p><p>15 Figure 1 shows two sentences ‘I BUY TICKET WHERE? [sent-32, score-0.589]
</p><p>16 The frames representing the sign ‘BUY’ and the neighboring signs are marked. [sent-34, score-0.621]
</p><p>17 The unmarked frames between the signs indicate the frames corresponding to movement epenthesis. [sent-35, score-0.513]
</p><p>18 It can be observed that the same sign ‘BUY’ is preceded and succeeded by movement epenthesis that depends on the end and start of the preceding and succeeding sign respectively. [sent-36, score-0.558]
</p><p>19 This effect makes the automated extraction, modeling and recognition of signs from continuous sentences more difﬁcult when compared to just plain gestures, isolated signs, or ﬁnger spelling. [sent-38, score-0.928]
</p><p>20 These common parts can be used for spotting or recognition of signs in continuous sign language sentences. [sent-40, score-0.691]
</p><p>21 They can also be used by sign language experts for teaching or studying variations between instances of signs in continuous sign language sentences, or in automated sign language tutoring systems. [sent-41, score-1.246]
</p><p>22 In a related work inspired by the success of the use of phonemes in speech recognition, the authors sought to extract common parts in different instances of a sign and thus arrive at a phonemeanalogue for signs (Bauer and Kraiss, 2002). [sent-43, score-0.573]
</p><p>23 Note that the sign itself is also affected by having different signs preceding or following it. [sent-62, score-0.485]
</p><p>24 It should also be noted that, unlike many of the previous works in sign language that perform tracking of the hands using 3D magnetic trackers or color gloves (Fang et al. [sent-91, score-0.471]
</p><p>25 We present a Bayesian framework to extract the common subsequences or signemes from all the given sentences simultaneously. [sent-95, score-0.953]
</p><p>26 With this framework, we can extract the ﬁrst most common sign, the second most common sign, the third most common sign and so on. [sent-97, score-0.403]
</p><p>27 Skin color blobs are extracted from frames of color video, and a relational distribution is formed for each frame using the edge pixels in the skin blobs. [sent-99, score-0.606]
</p><p>28 Each sentence is then represented as a trajectory in a low dimensional space called the space of relational distributions, which is arrived at by performing principal component analysis (PCA) on the relational distributions. [sent-100, score-0.474]
</p><p>29 wn ) of the candidate signemes in all the n sentences are together represented by a parameter vector. [sent-110, score-0.837]
</p><p>30 The parameter vector is updated sequentially by sampling the starting point and width of the possible signeme in each sentence from a joint conditional distribution that is based on the locations and widths of the target possible signeme in all other sentences. [sent-112, score-0.713]
</p><p>31 Each of the n sentences is represented as a sequence in the Space of Relational Distributions, and common patterns are extracted using iterated conditional modes (ICM). [sent-122, score-0.903]
</p><p>32 We propose a system that is generalized to extract more than one common sign from a collection of sentences (ﬁrst most common sign, second most common sign and so on), whereas 2593  NAYAK , D UNCAN , S ARKAR AND L OEDING  in the previous work, only single signs were extracted. [sent-129, score-1.477]
</p><p>33 We also extract single signs from a mixed collection of sentences where there are more than one common sign in context. [sent-130, score-1.189]
</p><p>34 In Section 3, we present the deﬁnition of signeme and then formulate the problem of ﬁnding signemes from a given set of sequences in a probabilistic framework. [sent-135, score-0.381]
</p><p>35 They have also been used before for representing sign language sentences without the use of color gloves or magnetic trackers (Nayak et al. [sent-143, score-1.061]
</p><p>36 We then reduce the dimensionality of the relational distributions by performing PCA on the set of relational distributions from all the input sentences and retain the number of dimensions required to keep a certain percentage 2595  NAYAK , D UNCAN , S ARKAR AND L OEDING  of energy, typically 95%. [sent-180, score-0.851]
</p><p>37 Problem Formulation Sign language sentences are series of signs. [sent-189, score-0.672]
</p><p>38 second dimension in the feature space, of three sentences S1 , S2 and S3 with only one common sign, R, among them. [sent-191, score-0.635]
</p><p>39 The signeme represents the portion of the sign that is most similar across the sentences. [sent-192, score-0.364]
</p><p>40 second dimensions of sentences S1 with signs R11 , R, R12 in order, S2 with signs R21 , R, R22 and S3 with signs R31 , R, R32 . [sent-196, score-1.375]
</p><p>41 The portion of R that is most similar across sentences is the signeme representative of R. [sent-198, score-0.73]
</p><p>42 problem as ﬁnding the most recurring patterns among a set of n sentences {S1 , · · · , Sn }, that have at least one common sign present in all the sentences. [sent-199, score-0.951]
</p><p>43 In the generalized case where C most common signs are sought, the 2596  F INDING R ECURRENT PATTERNS FROM C ONTINUOUS S IGN L ANGUAGE S ENTENCES  {S1 , · · · , Sn } Li w sa jj  A, B θ θ(ai ) d(x, y)  Set of n sentences with at least one common sign present in all the sentences. [sent-203, score-1.166]
</p><p>44 The index within a sentence could represent time or arc length in conﬁguration shape space Length of sentence Si Subsequence of sentence S j starting from index a j to a j + w j − 1. [sent-204, score-0.664]
</p><p>45 Possible choices of width for signemes of a sign include all integers from A to B. [sent-206, score-0.474]
</p><p>46 Set of parameters {a1 , w1 , · · · , an , wn } deﬁning a set of substrings of the given sentences Set of all parameters excluding the parameter ai . [sent-208, score-0.759]
</p><p>47 2 Parameter Estimation In order to extract the common signs from a given set of sign language sentences, we need to compute θi for each of the sentences sequentially. [sent-244, score-1.245]
</p><p>48 Since we expect the starting location and width of a subsequence representing the common sign to be strongly correlated, we estimate ai and wi jointly. [sent-256, score-0.514]
</p><p>49 Note the conditional and sequential nature of sampling from various sentences within the single iteration. [sent-262, score-0.654]
</p><p>50 In Figure 6, we show an example of how the conditional probability f (θai ,wi |θ(ai ,wi ) ) changes for the ﬁrst seven sentences from a given set of fourteen video sentences containing a common sign ‘DEPART’. [sent-263, score-1.573]
</p><p>51 In the rth iteration, the parameters of the common sign in ith sentence is computed based on the parameter values of the previous (i − 1) sentences obtained in the same iteration, and those of the (i + 1)th to nth sentences obtained in the previous, that is, the (r − 1)th iteration. [sent-266, score-1.659]
</p><p>52 i Different initial parameter vectors are obtained by independently sampling the sentences multiple times. [sent-297, score-0.62]
</p><p>53 The uniform sampling of the frames in the sentences for selecting the starting locations ensures the whole parameter space is covered uniformly. [sent-299, score-0.784]
</p><p>54 We run it the number of times equal to the average number of frames in each sentence from the given set of sentences for extracting the sign. [sent-301, score-0.949]
</p><p>55 do wi = MODE(w j ) i   j ai = MODE(ai ) For extracting the sign ‘DEPART’ from 14 sentences, we had 89 frames per sentence on an average. [sent-306, score-0.691]
</p><p>56 Figure 8 shows the plots of histograms of start and end location of the sign in each of the 14 sentences from the 89 runs. [sent-308, score-0.837]
</p><p>57 Experiments And Results In this section, we present visual and quantitative results of our approach for extracting signemes from video sequences representing sentences from American Sign Language. [sent-311, score-0.991]
</p><p>58 1 Data Set Our data set consists of 155 American Sign Language (ASL) video sequences organized into 12 groups (collections) based on the vocabulary (word that pervades the sentences of the group). [sent-314, score-0.722]
</p><p>59 For instance, the ‘DEPART’ group is comprised of all the sentences containing the word ‘DEPART’, the ‘PASSPORT’ group is comprised of all the sentences containing the word ‘PASSPORT’ and so on. [sent-315, score-1.27]
</p><p>60 The breakdown of these ‘pure’ groups and the number of sentences (sequences) in each are as follows. [sent-316, score-0.589]
</p><p>61 The legend shown in the plot for the ﬁrst sentence, S1 , holds for other sentences as well. [sent-320, score-0.589]
</p><p>62 We also organized the video sequences into 10 groups by combining two ‘pure’ groups of sentences as described above. [sent-322, score-0.722]
</p><p>63 2 Common Pattern Extraction Results In this section, we present the results of our method for extracting common patterns from sign language sentences. [sent-327, score-0.486]
</p><p>64 We ﬁrst present results for extracting the single most common sign and multiple common signs from the ‘pure’ sentence groups, followed by results for the most common patterns from the ‘mixed’ groups. [sent-328, score-0.969]
</p><p>65 1 E XTRACTING T HE M OST C OMMON PATTERN We perform extraction of the most common patterns from the ‘pure’ sentence groups. [sent-331, score-0.41]
</p><p>66 As an example, Figure 9 depicts the result of extraction of the sign ‘DEPART’ from 14 video sequences. [sent-334, score-0.374]
</p><p>67 5 0  50  100  Figure 9: The ﬁrst dimension of the video sequences containing a common sign ‘DEPART’. [sent-393, score-0.402]
</p><p>68 As can be seen, the extracted patterns and the corresponding ground truth patterns are quite similar, except for a few frames at the beginning and end of the some of the patterns. [sent-398, score-0.433]
</p><p>69 Note that since we deal with continuous video sequences, a difference of one or two frames between the ground truth and the extracted pattern is not considered a problem. [sent-399, score-0.4]
</p><p>70 the estimated start positions of the pattern extracted from each of the 155 sentences in the video data set. [sent-403, score-0.798]
</p><p>71 2 E XTRACTING M ULTIPLE C OMMON S IGNS In this section we present some visual results for the extraction of the two most common signs from the ‘pure’ groups of sentences. [sent-411, score-0.367]
</p><p>72 We focused on extracting only two signs because the shortest ASL sentence contained two signs. [sent-412, score-0.515]
</p><p>73 Figure 13 shows the results for the two most common signs extracted from the sentence ‘BAGGAGE THERE NOT MINE THERE’. [sent-413, score-0.582]
</p><p>74 Consequently, the word ‘BAGGAGE’ appears in all the 14 sentences of the group, whereas the word ‘MINE’ (or ‘MY’) shows up in 11 sentences coinciding with what was expected. [sent-415, score-1.27]
</p><p>75 Similarly, Figure 14 shows the results for the two most common signs extracted from the sentence ‘MY PASSPORT THERE STILL GOOD THERE’. [sent-416, score-0.582]
</p><p>76 The word ‘MY’ appears in all the 11 sentences of the group, whereas the word ‘PASSPORT’ appears in all 14 sentences. [sent-418, score-0.681]
</p><p>77 3 E XTRACTING T HE M OST C OMMON PATTERNS F ROM M IXED S ENTENCES We perform extraction of the most common patterns from the collection of ‘mixed’ sentences as outlined in Section 4. [sent-422, score-0.787]
</p><p>78 Similarly, Figure 15(b) 2607  NAYAK , D UNCAN , S ARKAR AND L OEDING  (a) BUY  (b) CANT  (c) DEPART  (d) FUTURE  (e) MOVE Figure 11: Signemes extracted from sentences shows the corresponding scatter plot for the end position of the patterns in the sentences. [sent-426, score-0.769]
</p><p>79 As can be seen, the points are more scattered as compared to the results shown in Figure 10 where the sentences used were known to contain common words. [sent-427, score-0.635]
</p><p>80 We believe that the incorrect patterns extracted are due to the differences in the frame width ranges for the mixed sentence sets. [sent-431, score-0.543]
</p><p>81 For example, sentences containing the word ‘MOVE’ were combined with sentences containing the word ‘HAVE’. [sent-432, score-1.27]
</p><p>82 The frame width range for the sign ‘HAVE’ is between 4 and 6 frames with 4 being the minimum width and 6 being the maximum width. [sent-433, score-0.531]
</p><p>83 On the other hand, the frame width range for the sign ‘MOVE’ is between 19 and 27 frames. [sent-434, score-0.372]
</p><p>84 2608  F INDING R ECURRENT PATTERNS FROM C ONTINUOUS S IGN L ANGUAGE S ENTENCES  (f) PASSPORT  (g) SECURITY  (h) TICKET  (i) TIME  (j) TABLE Figure 12: Signemes extracted from sentences  4. [sent-438, score-0.651]
</p><p>85 3 Sign Localization We used the extracted signemes to localize or spot signs in test sentences. [sent-439, score-0.523]
</p><p>86 The same process that is used for training sign models is used for sign localization. [sent-440, score-0.446]
</p><p>87 We tested with 12 test sentences from the ‘pure’ group speciﬁed in Section 4. [sent-442, score-0.589]
</p><p>88 The set of points representing the signeme were matched with the segments of the SoRD points from the test sentences to ﬁnd the segment with the minimum matching score, which would represent the sign in the test sentence. [sent-445, score-0.982]
</p><p>89 The SoRD points of the signeme retrieved from the test sentence are mapped to their nearest frames and compared with the ground truth frame series representing the sign in the sentence. [sent-446, score-0.887]
</p><p>90 (a) Frames corresponding to the word ‘MY’  (b) Frames corresponding to the word ‘PASSPORT’  Figure 14: Extraction of the two most common patterns or signemes from the sentence ‘MY PASSPORT THERE STILL GOOD THERE’. [sent-449, score-0.642]
</p><p>91 A signeme is a part of the sign that is robust to the variations of the adjacent signs and the associated movement epenthesis. [sent-483, score-0.698]
</p><p>92 We use iterative conditional modes (ICM) to sample the parameters, that is, the starting location and width of the signemes in each sentence in a sequential manner. [sent-485, score-0.561]
</p><p>93 The extracted signemes demonstrate that our approach is robust to some extent to the variations produced within a sign due to different contexts. [sent-488, score-0.519]
</p><p>94 Rather than manually demarcating signs in continuous sentences, which for our work took an expert approximately 5 minutes, we would just need instances of sentences containing the sign whose model is sought and based on our experiments this can be generated in approximately 2 minutes. [sent-490, score-1.105]
</p><p>95 Another contribution of this work is an empirically derived robust representation of the sign that is stable with respect to the variations due to neighboring signs and sentence context. [sent-491, score-0.732]
</p><p>96 Learning signs from subtitles: A weakly supervised approach to sign language recognition. [sent-566, score-0.568]
</p><p>97 Modelling and segmenting subunits for sign language recognition based on hand motion analysis. [sent-618, score-0.427]
</p><p>98 Automated extraction of signs from continuous sign language sentences using iterated conditional modes. [sent-667, score-1.324]
</p><p>99 Determining subunits for sign language recognition by evolutionary cluster-based segmentation of time series. [sent-689, score-0.379]
</p><p>100 Handling movement epenthesis and hand segmentation ambiguities in continuous sign language recognition using nested dynamic programming. [sent-772, score-0.497]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sentences', 0.589), ('signs', 0.262), ('nayak', 0.224), ('sign', 0.223), ('sentence', 0.212), ('signemes', 0.199), ('icm', 0.149), ('signeme', 0.141), ('relational', 0.131), ('buy', 0.116), ('entences', 0.116), ('arkar', 0.108), ('ecurrent', 0.108), ('oeding', 0.108), ('ontinuous', 0.108), ('uncan', 0.108), ('frames', 0.107), ('passport', 0.099), ('frame', 0.097), ('patterns', 0.093), ('anguage', 0.092), ('ign', 0.092), ('inding', 0.092), ('video', 0.092), ('language', 0.083), ('depart', 0.077), ('subsequences', 0.077), ('motion', 0.075), ('baggage', 0.075), ('sarkar', 0.075), ('ai', 0.064), ('extracted', 0.062), ('extraction', 0.059), ('ticket', 0.058), ('substrings', 0.057), ('warping', 0.057), ('width', 0.052), ('epenthesis', 0.05), ('swii', 0.05), ('asl', 0.05), ('wn', 0.049), ('skin', 0.047), ('common', 0.046), ('word', 0.046), ('recognition', 0.046), ('widths', 0.045), ('wi', 0.044), ('iterated', 0.043), ('extract', 0.042), ('bauer', 0.041), ('sord', 0.041), ('sequences', 0.041), ('extracting', 0.041), ('ground', 0.04), ('gesture', 0.039), ('truth', 0.038), ('movement', 0.037), ('color', 0.037), ('modes', 0.036), ('runs', 0.036), ('gloves', 0.035), ('variations', 0.035), ('conditional', 0.034), ('blobs', 0.033), ('swkk', 0.033), ('trackers', 0.033), ('offset', 0.033), ('magnetic', 0.032), ('mine', 0.032), ('gibbs', 0.032), ('sampling', 0.031), ('horizontal', 0.031), ('continuous', 0.031), ('pattern', 0.03), ('pure', 0.03), ('pixels', 0.03), ('locations', 0.029), ('localization', 0.029), ('representing', 0.029), ('axis', 0.029), ('bowden', 0.028), ('hands', 0.028), ('starner', 0.028), ('vogler', 0.028), ('subsequence', 0.028), ('starting', 0.028), ('casella', 0.027), ('recurrent', 0.027), ('vision', 0.027), ('mixed', 0.027), ('segmentation', 0.027), ('gao', 0.026), ('edge', 0.025), ('chinese', 0.025), ('gilks', 0.025), ('ommon', 0.025), ('vega', 0.025), ('xtracting', 0.025), ('scatter', 0.025), ('start', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="45-tfidf-1" href="./jmlr-2012-Finding_Recurrent_Patterns_from_Continuous_Sign_Language_Sentences_for_Automated_Extraction_of_Signs.html">45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</a></p>
<p>Author: Sunita Nayak, Kester Duncan, Sudeep Sarkar, Barbara Loeding</p><p>Abstract: We present a probabilistic framework to automatically learn models of recurring signs from multiple sign language video sequences containing the vocabulary of interest. We extract the parts of the signs that are present in most occurrences of the sign in context and are robust to the variations produced by adjacent signs. Each sentence video is ﬁrst transformed into a multidimensional time series representation, capturing the motion and shape aspects of the sign. Skin color blobs are extracted from frames of color video sequences, and a probabilistic relational distribution is formed for each frame using the contour and edge pixels from the skin blobs. Each sentence is represented as a trajectory in a low dimensional space called the space of relational distributions. Given these time series trajectories, we extract signemes from multiple sentences concurrently using iterated conditional modes (ICM). We show results by learning single signs from a collection of sentences with one common pervading sign, multiple signs from a collection of sentences with more than one common sign, and single signs from a mixed collection of sentences. The extracted signemes demonstrate that our approach is robust to some extent to the variations produced within a sign due to different contexts. We also show results whereby these learned sign models are used for spotting signs in test sequences. Keywords: pattern extraction, sign language recognition, signeme extraction, sign modeling, iterated conditional modes</p><p>2 0.25073087 <a title="45-tfidf-2" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>Author: Helen Cooper, Eng-Jon Ong, Nicolas Pugeault, Richard Bowden</p><p>Abstract: This paper discusses sign language recognition using linguistic sub-units. It presents three types of sub-units for consideration; those learnt from appearance data as well as those inferred from both 2D or 3D tracking data. These sub-units are then combined using a sign level classiﬁer; here, two options are presented. The ﬁrst uses Markov Models to encode the temporal changes between sub-units. The second makes use of Sequential Pattern Boosting to apply discriminative feature selection at the same time as encoding temporal information. This approach is more robust to noise and performs well in signer independent tests, improving results from the 54% achieved by the Markov Chains to 76%. Keywords: sign language recognition, sequential pattern boosting, depth cameras, sub-units, signer independence, data set</p><p>3 0.092006877 <a title="45-tfidf-3" href="./jmlr-2012-Exploration_in_Relational_Domains_for_Model-based_Reinforcement_Learning.html">41 jmlr-2012-Exploration in Relational Domains for Model-based Reinforcement Learning</a></p>
<p>Author: Tobias Lang, Marc Toussaint, Kristian Kersting</p><p>Abstract: A fundamental problem in reinforcement learning is balancing exploration and exploitation. We address this problem in the context of model-based reinforcement learning in large stochastic relational domains by developing relational extensions of the concepts of the E 3 and R- MAX algorithms. Efﬁcient exploration in exponentially large state spaces needs to exploit the generalization of the learned model: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be a well-known context in which exploitation is promising. To address this we introduce relational count functions which generalize the classical notion of state and action visitation counts. We provide guarantees on the exploration efﬁciency of our framework using count functions under the assumption that we had a relational KWIK learner and a near-optimal planner. We propose a concrete exploration algorithm which integrates a practically efﬁcient probabilistic rule learner and a relational planner (for which there are no guarantees, however) and employs the contexts of learned relational rules as features to model the novelty of states and actions. Our results in noisy 3D simulated robot manipulation problems and in domains of the international planning competition demonstrate that our approach is more effective than existing propositional and factored exploration techniques. Keywords: reinforcement learning, statistical relational learning, exploration, relational transition models, robotics</p><p>4 0.078930415 <a title="45-tfidf-4" href="./jmlr-2012-Human_Gesture_Recognition_on_Product_Manifolds.html">50 jmlr-2012-Human Gesture Recognition on Product Manifolds</a></p>
<p>Author: Yui Man Lui</p><p>Abstract: Action videos are multidimensional data and can be naturally represented as data tensors. While tensor computing is widely used in computer vision, the geometry of tensor space is often ignored. The aim of this paper is to demonstrate the importance of the intrinsic geometry of tensor space which yields a very discriminating structure for action recognition. We characterize data tensors as points on a product manifold and model it statistically using least squares regression. To this aim, we factorize a data tensor relating to each order of the tensor using Higher Order Singular Value Decomposition (HOSVD) and then impose each factorized element on a Grassmann manifold. Furthermore, we account for underlying geometry on manifolds and formulate least squares regression as a composite function. This gives a natural extension from Euclidean space to manifolds. Consequently, classiﬁcation is performed using geodesic distance on a product manifold where each factor manifold is Grassmannian. Our method exploits appearance and motion without explicitly modeling the shapes and dynamics. We assess the proposed method using three gesture databases, namely the Cambridge hand-gesture, the UMD Keck body-gesture, and the CHALEARN gesture challenge data sets. Experimental results reveal that not only does the proposed method perform well on the standard benchmark data sets, but also it generalizes well on the one-shot-learning gesture challenge. Furthermore, it is based on a simple statistical model and the intrinsic geometry of tensor space. Keywords: gesture recognition, action recognition, Grassmann manifolds, product manifolds, one-shot-learning, kinect data</p><p>5 0.050492022 <a title="45-tfidf-5" href="./jmlr-2012-Hope_and_Fear_for_Discriminative_Training_of_Statistical_Translation_Models.html">49 jmlr-2012-Hope and Fear for Discriminative Training of Statistical Translation Models</a></p>
<p>Author: David Chiang</p><p>Abstract: In machine translation, discriminative models have almost entirely supplanted the classical noisychannel model, but are standardly trained using a method that is reliable only in low-dimensional spaces. Two strands of research have tried to adapt more scalable discriminative training methods to machine translation: the ﬁrst uses log-linear probability models and either maximum likelihood or minimum risk, and the other uses linear models and large-margin methods. Here, we provide an overview of the latter. We compare several learning algorithms and describe in detail some novel extensions suited to properties of the translation task: no single correct output, a large space of structured outputs, and slow inference. We present experimental results on a large-scale ArabicEnglish translation task, demonstrating large gains in translation accuracy. Keywords: machine translation, structured prediction, large-margin methods, online learning, distributed computing</p><p>6 0.043376967 <a title="45-tfidf-6" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>7 0.038489223 <a title="45-tfidf-7" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<p>8 0.029201258 <a title="45-tfidf-8" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>9 0.027987015 <a title="45-tfidf-9" href="./jmlr-2012-Discriminative_Hierarchical_Part-based_Models_for_Human_Parsing_and_Action_Recognition.html">32 jmlr-2012-Discriminative Hierarchical Part-based Models for Human Parsing and Action Recognition</a></p>
<p>10 0.027387902 <a title="45-tfidf-10" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>11 0.025656123 <a title="45-tfidf-11" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>12 0.025278369 <a title="45-tfidf-12" href="./jmlr-2012-A_Model_of_the_Perception_of_Facial_Expressions_of_Emotion_by_Humans%3A_Research_Overview_and_Perspectives.html">6 jmlr-2012-A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives</a></p>
<p>13 0.023567609 <a title="45-tfidf-13" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>14 0.022999523 <a title="45-tfidf-14" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>15 0.022608785 <a title="45-tfidf-15" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>16 0.020403164 <a title="45-tfidf-16" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>17 0.020371012 <a title="45-tfidf-17" href="./jmlr-2012-A_Topic_Modeling_Toolbox_Using_Belief_Propagation.html">9 jmlr-2012-A Topic Modeling Toolbox Using Belief Propagation</a></p>
<p>18 0.019704953 <a title="45-tfidf-18" href="./jmlr-2012-Sally%3A_A_Tool_for_Embedding_Strings_in_Vector_Spaces.html">102 jmlr-2012-Sally: A Tool for Embedding Strings in Vector Spaces</a></p>
<p>19 0.019703854 <a title="45-tfidf-19" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>20 0.019336615 <a title="45-tfidf-20" href="./jmlr-2012-Active_Clustering_of_Biological_Sequences.html">12 jmlr-2012-Active Clustering of Biological Sequences</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.109), (1, 0.014), (2, 0.198), (3, -0.078), (4, -0.012), (5, -0.173), (6, 0.18), (7, -0.166), (8, 0.133), (9, 0.08), (10, 0.303), (11, -0.224), (12, -0.03), (13, 0.048), (14, -0.071), (15, -0.108), (16, 0.087), (17, -0.087), (18, -0.092), (19, 0.163), (20, -0.266), (21, -0.026), (22, -0.096), (23, -0.148), (24, -0.064), (25, 0.064), (26, 0.039), (27, -0.009), (28, -0.1), (29, 0.071), (30, -0.101), (31, 0.016), (32, 0.061), (33, -0.083), (34, 0.038), (35, -0.037), (36, -0.026), (37, -0.135), (38, 0.115), (39, -0.022), (40, 0.108), (41, 0.072), (42, -0.072), (43, -0.062), (44, -0.031), (45, 0.119), (46, 0.001), (47, 0.007), (48, -0.032), (49, -0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97986853 <a title="45-lsi-1" href="./jmlr-2012-Finding_Recurrent_Patterns_from_Continuous_Sign_Language_Sentences_for_Automated_Extraction_of_Signs.html">45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</a></p>
<p>Author: Sunita Nayak, Kester Duncan, Sudeep Sarkar, Barbara Loeding</p><p>Abstract: We present a probabilistic framework to automatically learn models of recurring signs from multiple sign language video sequences containing the vocabulary of interest. We extract the parts of the signs that are present in most occurrences of the sign in context and are robust to the variations produced by adjacent signs. Each sentence video is ﬁrst transformed into a multidimensional time series representation, capturing the motion and shape aspects of the sign. Skin color blobs are extracted from frames of color video sequences, and a probabilistic relational distribution is formed for each frame using the contour and edge pixels from the skin blobs. Each sentence is represented as a trajectory in a low dimensional space called the space of relational distributions. Given these time series trajectories, we extract signemes from multiple sentences concurrently using iterated conditional modes (ICM). We show results by learning single signs from a collection of sentences with one common pervading sign, multiple signs from a collection of sentences with more than one common sign, and single signs from a mixed collection of sentences. The extracted signemes demonstrate that our approach is robust to some extent to the variations produced within a sign due to different contexts. We also show results whereby these learned sign models are used for spotting signs in test sequences. Keywords: pattern extraction, sign language recognition, signeme extraction, sign modeling, iterated conditional modes</p><p>2 0.83947271 <a title="45-lsi-2" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>Author: Helen Cooper, Eng-Jon Ong, Nicolas Pugeault, Richard Bowden</p><p>Abstract: This paper discusses sign language recognition using linguistic sub-units. It presents three types of sub-units for consideration; those learnt from appearance data as well as those inferred from both 2D or 3D tracking data. These sub-units are then combined using a sign level classiﬁer; here, two options are presented. The ﬁrst uses Markov Models to encode the temporal changes between sub-units. The second makes use of Sequential Pattern Boosting to apply discriminative feature selection at the same time as encoding temporal information. This approach is more robust to noise and performs well in signer independent tests, improving results from the 54% achieved by the Markov Chains to 76%. Keywords: sign language recognition, sequential pattern boosting, depth cameras, sub-units, signer independence, data set</p><p>3 0.29515725 <a title="45-lsi-3" href="./jmlr-2012-Exploration_in_Relational_Domains_for_Model-based_Reinforcement_Learning.html">41 jmlr-2012-Exploration in Relational Domains for Model-based Reinforcement Learning</a></p>
<p>Author: Tobias Lang, Marc Toussaint, Kristian Kersting</p><p>Abstract: A fundamental problem in reinforcement learning is balancing exploration and exploitation. We address this problem in the context of model-based reinforcement learning in large stochastic relational domains by developing relational extensions of the concepts of the E 3 and R- MAX algorithms. Efﬁcient exploration in exponentially large state spaces needs to exploit the generalization of the learned model: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be a well-known context in which exploitation is promising. To address this we introduce relational count functions which generalize the classical notion of state and action visitation counts. We provide guarantees on the exploration efﬁciency of our framework using count functions under the assumption that we had a relational KWIK learner and a near-optimal planner. We propose a concrete exploration algorithm which integrates a practically efﬁcient probabilistic rule learner and a relational planner (for which there are no guarantees, however) and employs the contexts of learned relational rules as features to model the novelty of states and actions. Our results in noisy 3D simulated robot manipulation problems and in domains of the international planning competition demonstrate that our approach is more effective than existing propositional and factored exploration techniques. Keywords: reinforcement learning, statistical relational learning, exploration, relational transition models, robotics</p><p>4 0.27126992 <a title="45-lsi-4" href="./jmlr-2012-Human_Gesture_Recognition_on_Product_Manifolds.html">50 jmlr-2012-Human Gesture Recognition on Product Manifolds</a></p>
<p>Author: Yui Man Lui</p><p>Abstract: Action videos are multidimensional data and can be naturally represented as data tensors. While tensor computing is widely used in computer vision, the geometry of tensor space is often ignored. The aim of this paper is to demonstrate the importance of the intrinsic geometry of tensor space which yields a very discriminating structure for action recognition. We characterize data tensors as points on a product manifold and model it statistically using least squares regression. To this aim, we factorize a data tensor relating to each order of the tensor using Higher Order Singular Value Decomposition (HOSVD) and then impose each factorized element on a Grassmann manifold. Furthermore, we account for underlying geometry on manifolds and formulate least squares regression as a composite function. This gives a natural extension from Euclidean space to manifolds. Consequently, classiﬁcation is performed using geodesic distance on a product manifold where each factor manifold is Grassmannian. Our method exploits appearance and motion without explicitly modeling the shapes and dynamics. We assess the proposed method using three gesture databases, namely the Cambridge hand-gesture, the UMD Keck body-gesture, and the CHALEARN gesture challenge data sets. Experimental results reveal that not only does the proposed method perform well on the standard benchmark data sets, but also it generalizes well on the one-shot-learning gesture challenge. Furthermore, it is based on a simple statistical model and the intrinsic geometry of tensor space. Keywords: gesture recognition, action recognition, Grassmann manifolds, product manifolds, one-shot-learning, kinect data</p><p>5 0.20432295 <a title="45-lsi-5" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>Author: Tom De Smedt, Walter Daelemans</p><p>Abstract: Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classiﬁers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern. Keywords: Python, data mining, natural language processing, machine learning, graph networks</p><p>6 0.17061479 <a title="45-lsi-6" href="./jmlr-2012-Hope_and_Fear_for_Discriminative_Training_of_Statistical_Translation_Models.html">49 jmlr-2012-Hope and Fear for Discriminative Training of Statistical Translation Models</a></p>
<p>7 0.15909247 <a title="45-lsi-7" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<p>8 0.15762284 <a title="45-lsi-8" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>9 0.14577997 <a title="45-lsi-9" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>10 0.14367355 <a title="45-lsi-10" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>11 0.12849271 <a title="45-lsi-11" href="./jmlr-2012-Active_Clustering_of_Biological_Sequences.html">12 jmlr-2012-Active Clustering of Biological Sequences</a></p>
<p>12 0.11998964 <a title="45-lsi-12" href="./jmlr-2012-Towards_Integrative_Causal_Analysis_of_Heterogeneous_Data_Sets_and_Studies.html">114 jmlr-2012-Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies</a></p>
<p>13 0.11646408 <a title="45-lsi-13" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>14 0.11411445 <a title="45-lsi-14" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>15 0.11090107 <a title="45-lsi-15" href="./jmlr-2012-Transfer_in_Reinforcement_Learning_via_Shared_Features.html">116 jmlr-2012-Transfer in Reinforcement Learning via Shared Features</a></p>
<p>16 0.10403284 <a title="45-lsi-16" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>17 0.098803163 <a title="45-lsi-17" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>18 0.09576147 <a title="45-lsi-18" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>19 0.095491119 <a title="45-lsi-19" href="./jmlr-2012-A_Topic_Modeling_Toolbox_Using_Belief_Propagation.html">9 jmlr-2012-A Topic Modeling Toolbox Using Belief Propagation</a></p>
<p>20 0.093874283 <a title="45-lsi-20" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.042), (21, 0.015), (26, 0.034), (27, 0.037), (29, 0.035), (35, 0.033), (49, 0.017), (52, 0.388), (56, 0.014), (57, 0.02), (69, 0.056), (75, 0.031), (77, 0.012), (79, 0.013), (81, 0.018), (92, 0.046), (96, 0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73283696 <a title="45-lda-1" href="./jmlr-2012-Finding_Recurrent_Patterns_from_Continuous_Sign_Language_Sentences_for_Automated_Extraction_of_Signs.html">45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</a></p>
<p>Author: Sunita Nayak, Kester Duncan, Sudeep Sarkar, Barbara Loeding</p><p>Abstract: We present a probabilistic framework to automatically learn models of recurring signs from multiple sign language video sequences containing the vocabulary of interest. We extract the parts of the signs that are present in most occurrences of the sign in context and are robust to the variations produced by adjacent signs. Each sentence video is ﬁrst transformed into a multidimensional time series representation, capturing the motion and shape aspects of the sign. Skin color blobs are extracted from frames of color video sequences, and a probabilistic relational distribution is formed for each frame using the contour and edge pixels from the skin blobs. Each sentence is represented as a trajectory in a low dimensional space called the space of relational distributions. Given these time series trajectories, we extract signemes from multiple sentences concurrently using iterated conditional modes (ICM). We show results by learning single signs from a collection of sentences with one common pervading sign, multiple signs from a collection of sentences with more than one common sign, and single signs from a mixed collection of sentences. The extracted signemes demonstrate that our approach is robust to some extent to the variations produced within a sign due to different contexts. We also show results whereby these learned sign models are used for spotting signs in test sequences. Keywords: pattern extraction, sign language recognition, signeme extraction, sign modeling, iterated conditional modes</p><p>2 0.34423292 <a title="45-lda-2" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>Author: Helen Cooper, Eng-Jon Ong, Nicolas Pugeault, Richard Bowden</p><p>Abstract: This paper discusses sign language recognition using linguistic sub-units. It presents three types of sub-units for consideration; those learnt from appearance data as well as those inferred from both 2D or 3D tracking data. These sub-units are then combined using a sign level classiﬁer; here, two options are presented. The ﬁrst uses Markov Models to encode the temporal changes between sub-units. The second makes use of Sequential Pattern Boosting to apply discriminative feature selection at the same time as encoding temporal information. This approach is more robust to noise and performs well in signer independent tests, improving results from the 54% achieved by the Markov Chains to 76%. Keywords: sign language recognition, sequential pattern boosting, depth cameras, sub-units, signer independence, data set</p><p>3 0.3104158 <a title="45-lda-3" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>Author: Trinh Minh Tri Do, Thierry Artières</p><p>Abstract: Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efﬁcient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efﬁcient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random ﬁelds, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy ε with a rate O(1/λε) where λ is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difﬁcult and requires a stronger and more disputable assumption. Yet we provide experimental results on artiﬁcial test problems, and on ﬁve standard and difﬁcult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms. Keywords: optimization, non-convex, non-smooth, cutting plane, bundle method, regularized risk</p><p>4 0.30116096 <a title="45-lda-4" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random ﬁelds (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter ﬁtting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random ﬁeld via the graphical lasso.</p><p>5 0.30014384 <a title="45-lda-5" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel</p><p>Abstract: The success of many machine learning and pattern recognition methods relies heavily upon the identiﬁcation of an appropriate distance metric on the input data. It is often beneﬁcial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semideﬁnite. Semideﬁnite programming is often used to enforce this constraint, but does not scale well and is not easy to implement. B OOST M ETRIC is instead based on the observation that any positive semideﬁnite matrix can be decomposed into a linear combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting methods are easy to implement, efﬁcient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semideﬁnite matrix with trace and rank being one rather than a classiﬁer or regressor. Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. Keywords: Mahalanobis distance, semideﬁnite programming, column generation, boosting, Lagrange duality, large margin nearest neighbor</p><p>6 0.30000627 <a title="45-lda-6" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>7 0.29857427 <a title="45-lda-7" href="./jmlr-2012-Human_Gesture_Recognition_on_Product_Manifolds.html">50 jmlr-2012-Human Gesture Recognition on Product Manifolds</a></p>
<p>8 0.29692662 <a title="45-lda-8" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>9 0.2945765 <a title="45-lda-9" href="./jmlr-2012-A_Model_of_the_Perception_of_Facial_Expressions_of_Emotion_by_Humans%3A_Research_Overview_and_Perspectives.html">6 jmlr-2012-A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives</a></p>
<p>10 0.2913397 <a title="45-lda-10" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>11 0.29058838 <a title="45-lda-11" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>12 0.29005742 <a title="45-lda-12" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>13 0.28948912 <a title="45-lda-13" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>14 0.28899229 <a title="45-lda-14" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>15 0.28864938 <a title="45-lda-15" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<p>16 0.28857312 <a title="45-lda-16" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>17 0.28851286 <a title="45-lda-17" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>18 0.28801745 <a title="45-lda-18" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>19 0.28786775 <a title="45-lda-19" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>20 0.28778493 <a title="45-lda-20" href="./jmlr-2012-Robust_Kernel_Density_Estimation.html">100 jmlr-2012-Robust Kernel Density Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
