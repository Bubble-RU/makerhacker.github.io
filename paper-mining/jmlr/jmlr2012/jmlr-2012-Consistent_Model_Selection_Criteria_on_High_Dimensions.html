<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-29" href="#">jmlr2012-29</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</h1>
<br/><p>Source: <a title="jmlr-2012-29-pdf" href="http://jmlr.org/papers/volume13/kim12a/kim12a.pdf">pdf</a></p><p>Author: Yongdai Kim, Sunghoon Kwon, Hosik Choi</p><p>Abstract: Asymptotic properties of model selection criteria for high-dimensional regression models are studied where the dimension of covariates is much larger than the sample size. Several sufﬁcient conditions for model selection consistency are provided. Non-Gaussian error distributions are considered and it is shown that the maximal number of covariates for model selection consistency depends on the tail behavior of the error distribution. Also, sufﬁcient conditions for model selection consistency are given when the variance of the noise is neither known nor estimated consistently. Results of simulation studies as well as real data analysis are given to illustrate that ﬁnite sample performances of consistent model selection criteria can be quite different. Keywords: model selection consistency, general information criteria, high dimension, regression</p><p>Reference: <a title="jmlr-2012-29-reference" href="../jmlr2012_reference/jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 COM  Department of Informational Statistics Hoseo University Chungnam 336-795, Korea  Editor: Xiaotong Shen  Abstract Asymptotic properties of model selection criteria for high-dimensional regression models are studied where the dimension of covariates is much larger than the sample size. [sent-7, score-0.207]
</p><p>2 Non-Gaussian error distributions are considered and it is shown that the maximal number of covariates for model selection consistency depends on the tail behavior of the error distribution. [sent-9, score-0.295]
</p><p>3 Results of simulation studies as well as real data analysis are given to illustrate that ﬁnite sample performances of consistent model selection criteria can be quite different. [sent-11, score-0.256]
</p><p>4 (2009) proposed a modiﬁed BIC which is consistent when the dimension of covariates is diverging slower than the sample size. [sent-20, score-0.181]
</p><p>5 Here, the consistency of a model selection criterion means that the probability of the selected model being equal to the true model converges to 1. [sent-21, score-0.194]
</p><p>6 The extended BIC by Chen and Chen (2008) and corrected RIC by Zhang and Shen (2010) are shown to be consistent even when the dimension of covariates is larger than the c 2012 Yongdai Kim, Sunghoon Kwon and Hosik Choi. [sent-23, score-0.207]
</p><p>7 In this paper, we study asymptotic properties of a large class of model selection criteria based on the generalized information criterion (GIC) considered by Shao (1997). [sent-28, score-0.177]
</p><p>8 For a case of the Gaussian error distribution with consistent estimator of the variance, our sufﬁcient conditions include most of the previously proposed consistent model selection criteria such as the modiﬁed BIC (Wang et al. [sent-34, score-0.253]
</p><p>9 For high-dimensional models, it is not practically feasible to ﬁnd the best model among all possible submodels since the number of submodels are too large. [sent-36, score-0.355]
</p><p>10 A simple remedy is to ﬁnd a sequence of submodels with increasing complexities (e. [sent-37, score-0.168]
</p><p>11 Examples of constructing a sequence of submodels are the forward selection procedure and solution paths of penalized regression approaches. [sent-40, score-0.287]
</p><p>12 Our sufﬁcient conditions are still valid as long as the sequence of submodels includes the true model with probability converging to 1. [sent-41, score-0.209]
</p><p>13 , (yn , xn )} be a given data set of independent pairs of response and covariates, where yi ∈ R and xi ∈ R pn . [sent-53, score-0.568]
</p><p>14 Suppose the true regression model for (y, x) is given as ′  y = x β∗ + ε, where β∗ ∈ R pn , E(ε) = 0 and Var(ε) = σ2 . [sent-54, score-0.477]
</p><p>15 , yn ) and Xn be the n × pn dimensional design matrix whose jth column is ′ j Xn = (x1 j , . [sent-61, score-0.585]
</p><p>16 For given β ∈ R pn , let Rn (β) = Yn − Xn β 2 , where · is the Euclidean norm. [sent-65, score-0.458]
</p><p>17 The AIC corresponds to λn = 2, the BIC to λn = log n, the RIC of Foster and George (1994) to λn = 2 log pn , the RIC of Zhang and Shen (2010) to λn = 2(log pn + log log pn ). [sent-77, score-1.594]
</p><p>18 When pn is large, it would not be wise to search all possible subsets of {1, . [sent-79, score-0.458]
</p><p>19 Instead, we set an upper bound on the cardinality of π, say sn and search the optimal model among submodels whose cardinalities are smaller than sn . [sent-83, score-0.499]
</p><p>20 We deﬁne the restricted GICλn as ˆ ˆ πλn = argminπ∈M sn Rn (βπ ) + λn |π|σ2 . [sent-89, score-0.156]
</p><p>21 (1)  The restricted GIC is the same as the GIC if sn = pn . [sent-90, score-0.614]
</p><p>22 , pn }, let Xπ = (Xn , j ∈ π) be the n × |π| matrix whose columns j consist of Xn , j ∈ π. [sent-99, score-0.458]
</p><p>23 j ∗ j∈πn  • A5 : qn = O(nc3 ) for some 0 ≤ c3 < c2 , and qn ≤ sn , where qn = |π∗ |. [sent-110, score-0.678]
</p><p>24 If λn = o(nc2 −c1 ) and pn /(λn ρn )k → 0, then the GICλn is consistent. [sent-120, score-0.458]
</p><p>25 In Theorem 2, pn can diverge only polynomially fast in n since pn = o(λk ) = o(nkc2 ). [sent-121, score-0.938]
</p><p>26 Since k n can be considered as a degree of tail lightness of the error distribution, we can conclude that the lighter the tail of the error distribution is, the more covariates the GICλn is consistent with. [sent-122, score-0.304]
</p><p>27 When ε is Gaussian, the following theorem proves that the GICλn can be consistent when pn diverges exponentially fast. [sent-123, score-0.542]
</p><p>28 If λn = o(nc2 −c1 ), sn log pn = o(nc2 −c1 ) and λn − 2 log pn − log log pn → ∞, then the GICλn is consistent. [sent-125, score-1.75]
</p><p>29 In the following, we give three examples for (i) ﬁxed pn , (ii) polynomially diverging pn and (iii) exponentially diverging pn . [sent-126, score-1.454]
</p><p>30 j n Example 1 Consider a standard case where pn is ﬁxed and n goes to inﬁnity. [sent-135, score-0.458]
</p><p>31 Any GIC with λn = nc , 0 < c < 1 is consistent, which suggests that the class of consistent model selection criteria is quite large. [sent-141, score-0.2]
</p><p>32 That is, for larger pn , we need larger λn for consistency, which is reasonable because we need to be more careful not to overﬁt when pn is large. [sent-145, score-0.916]
</p><p>33 (2009), which is a GIC with λn = log log pn log n, is consistent. [sent-151, score-0.623]
</p><p>34 Chen and Chen (2008) proposed a model selection criterion called the extended BIC given by pn ˆ ˆ πeBIC = argminπ⊂{1,. [sent-152, score-0.563]
</p><p>35 ,pn },|π|≤K Rn (βπ ) + |π|σ2 log n + 2κσ2 log |π|  for some K > 0 and 0 ≤ κ ≤ 1, and proved that the extended BIC is consistent when κ > 1 − 1/(2γ). [sent-155, score-0.163]
</p><p>36 pn Since log |π| ≍ |π| log pn for |π| ≤ K, we have |π|σ2 log n + 2γσ2 log  pn ≍ (log n + 2κ log pn )|π|σ2 . [sent-156, score-2.107]
</p><p>37 Example 3 When the error distribution is Gaussian, the GIC can be consistent for exponentially increasing pn (i. [sent-158, score-0.539]
</p><p>38 The GIC with λn = nξ , 0 < ξ < 1 is consistent when pn = O(exp(αnγ )) for 0 < γ < ξ and α > 0. [sent-161, score-0.511]
</p><p>39 Also, it can be shown by Theorem 3 that the extended BIC with γ = 1 is consistent with pn = O(exp(αnγ )) for 0 < γ < 1/2. [sent-162, score-0.511]
</p><p>40 The consistency of the corrected RIC of Zhang and Shen (2010) can be conﬁrmed by Theorem 3, but the regularity conditions for Theorem 3 are more general than those of Zhang and Shen (2010). [sent-163, score-0.156]
</p><p>41 A simple remedy is to construct a sequence of submodels and select the optimal model among the sequence of submodels. [sent-168, score-0.187]
</p><p>42 Examples of constructing a sequence of submodels are the forward selection (Wang, 2009) and the solution path of a sparse penalized estimator obtained by, for example, the Lars algorithm (Efron et al. [sent-169, score-0.356]
</p><p>43 • For a given sparse penalty Jη (t) indexed by η ≥ 0, ﬁnd the solution path of a penalized ˆ estimator {β(η) : η > 0}, where p  ˆ β(η) = argminβ Rn (β) + ∑ Jη (|β j |) . [sent-172, score-0.181]
</p><p>44 ˆ • Let S(η) = { j : β(η) j = 0} and ϒ = {η : S(η) = S(η−), |S(η)| ≤ sn }. [sent-174, score-0.156]
</p><p>45 1041  K IM , K WON AND C HOI  It is easy to see that a consistent GIC is still consistent with a sequence of sub-models as long as the sequence of submodels includes the true model with probability converging to 1. [sent-177, score-0.315]
</p><p>46 The consistency of the solution path of a nonconvex penalized estimator with either the SCAD penalty or minimax concave penalty is proved by Zhang (2010) and Kim and Kwon (2012). [sent-180, score-0.354]
</p><p>47 By combining Theorem 4 of Kim and Kwon (2012) and Theorem 2 of the current paper, we can prove the consistency of the GIC with the solution path of the SCAD penalty or minimax concave penalty, which is formally stated in the following theorem. [sent-181, score-0.261]
</p><p>48 Remark 6 Theorem 3 can be modiﬁed similarly for the GIC with the solution path of the SCAD or minimax concave penalty, since Theorem 4 of Kim and Kwon (2012) can be modiﬁed accordingly for the Gaussian error distribution. [sent-187, score-0.198]
</p><p>49 When pn is ﬁxed, we can estimate σ2 consistently by the mean squared error of the full model. [sent-192, score-0.486]
</p><p>50 If λn = o(nc2 −c1 ) and λn − 2M1 log pn /ρn rin f → ∞, then the GICλn with the estimated variance is consistent. [sent-202, score-0.578]
</p><p>51 The corrected RIC, the GIC with λn = 2(log pn + log log pn ), does not satisfy the condition in Theorem 7, and hence may not be consistent with an estimated variance. [sent-203, score-1.169]
</p><p>52 On the other hand, the GIC with λn = αn log pn is consistent as long as αn → ∞. [sent-204, score-0.566]
</p><p>53 3 The Size of sn For condition A5, sn should be large enough so that qn ≤ sn . [sent-206, score-0.666]
</p><p>54 In many cases, sn can be sufﬁciently large for practical purposes. [sent-207, score-0.156]
</p><p>55 For example, suppose {xi , i ≤ n} are independent and identically distributed pn dimensional random vectors such that E(x1 ) = 0 and Var(x1 ) = Σ = [σ jk ]. [sent-208, score-0.494]
</p><p>56 By the inequality (2) in Greenshtein and Ritov (2004), we have n  sup  ∑ xi j xik /n − σ jk  = Op  j,k i=1  log n n  ,  and hence sup jk |a jk | = O p ( log n/n), where a jk is the ( j, k) entry of A. [sent-218, score-0.254]
</p><p>57 We consider the ﬁve GICs whose corresponding λn s are given as (1)  • GIC1 (=BIC) : λn = log n, (2)  1/3  • GIC2 : λn = pn , (3)  • GIC3 : λn = 2 log pn , (4)  • GIC4 : λn = 2(log pn + log log pn ), (5)  • GIC5 : λn = log log n log pn , (6)  • GIC6 : λn = log n log pn . [sent-223, score-3.243]
</p><p>58 First, we compare performances of the GICs applied to all possible submodels with those applied to submodels constructed by the solution path of a sparse penalized approach. [sent-240, score-0.514]
</p><p>59 From Table 1, we can see that the results based on the SCAD solution path are almost identical to those based on the all possible search, which suggests that the model selection with the SCAD solution path is a promising alternative to all possible search. [sent-246, score-0.242]
</p><p>60 However, the relative performances of the model selection criteria are similar. [sent-307, score-0.156]
</p><p>61 However, when n = 100, the GIC1 is better in terms of prediction accuracy than some other GICs which are selection consistent, which is an example of the conﬂict between selection consistency and prediction optimality. [sent-430, score-0.185]
</p><p>62 We compare prediction accuracies of the 6 GICs with the submodels obtained from the SCAD solution path. [sent-548, score-0.207]
</p><p>63 nonzero coefﬁcients is equal to pmax , and to estimate the error variance by the mean squared error of the selected model. [sent-665, score-0.225]
</p><p>64 Table 6 compares the 6 GICs with the number of pre-screened genes being p = 500 and p = 3000, when the error variance is estimated with pmax being 20, 30 and 40, respectively. [sent-671, score-0.194]
</p><p>65 For p = 500, the lowest prediction error is achieved by the GIC2 and the GIC3 , GIC4 and GIC5 perform reasonably well with pmax = 20. [sent-676, score-0.157]
</p><p>66 For p = 3000, the lowest prediction error is achieved by the GIC5 with pmax = 20. [sent-677, score-0.157]
</p><p>67 Concluding Remarks The range of consistent model selection criteria is rather large, and it is not clear which one is better with ﬁnite samples. [sent-802, score-0.172]
</p><p>68 It would be interesting to rewrite the class of GICs as {λn = αn log pn : αn > 0}. [sent-803, score-0.513]
</p><p>69 The GIC3 , GIC5 and GIC6 correspond to αn = 2, αn = log log n and αn = log n, respectively. [sent-804, score-0.165]
</p><p>70 , αn = 2 or αn = log log n) would be better when many signal covariates with small regression coefﬁcients are expected to exist. [sent-809, score-0.23]
</p><p>71 n n n ′  (i, j)  We let β∗ = (β(1)∗ , β(2)∗ ), where β(1)∗ ∈ Rqn and β(2)∗ ∈ R pn −qn . [sent-964, score-0.458]
</p><p>72 Since ′ (1,1) (1) H(1) H(1) = (Cn )−1 , A2 of the regularity conditions implies h j 2 ≤ 1/M2 for all j ≤ qn . [sent-981, score-0.213]
</p><p>73 Hence, 2 E(z j )2k < ∞ for all j ≤ qn since E(εi )2k < ∞. [sent-982, score-0.174]
</p><p>74 , qn ) ≤ ≤ =  qn  ∑ Pr(|z j | > ηnc /2 ) 2  j=1 qn  1  ∑ η n−c k 2  j=1  1 1 qn n−c2 k ≤ n−(c2 −c3 )k → 0, η η  which completes the proof. [sent-987, score-0.696]
</p><p>75 , pn ) ′ (2) (1) ˆ = Xn Yn − Xn β∗(1) (2)′  = Xn  (2)′  = Xn  (2)′  = Xn Hence, we have  (1) 1  Yn − Xn  n  (1)′  (1,1) −1  (Cn  ) Xn Yn (1) 1  (1)  Xn β∗(1) + εn − Xn  n  (1,1) −1  (Cn  (1)′  (1)  ) Xn (Xn β∗(1) + εn )  1 (1) (1,1) (1)′ I − Xn (Cn )−1 Xn εn . [sent-993, score-0.458]
</p><p>76 n  √ (2)′ ˆ < Yn − Yn∗ , Xnj > / n = h j εn for j = qn + 1, . [sent-994, score-0.174]
</p><p>77 , pn ,  (2)  where h j is the j − qn column vector of H(2) and ′  (2,1)  H(2) = Cn Note that  (1,1) −1  (Cn  )  1 (2)′ 1 (1)′ √ Xn − √ Xn . [sent-997, score-0.632]
</p><p>78 n  1051  (3)  K IM , K WON AND C HOI  (1)′  (1)  (1)′  (1)  (2) 2 2  Since the all eigenvalues of I − Xn (Xn Xn )−1 Xn  are between 0 and 1, we have h j 2k < ∞, where ξ =< Y − Y ∗ , X j > /√n, and so ˆn n for all j = qn + 1, . [sent-999, score-0.174]
</p><p>79 Finally, for any η > 0, ˆ Pr | < Yn − Yn∗ , Xnj > | > η  nλn ρn for some j = qn + 1, . [sent-1004, score-0.174]
</p><p>80 , pn  = Pr |ξ j | > η λn ρn for some j = qn + 1, . [sent-1007, score-0.632]
</p><p>81 , pn pn  ≤  ∑  j=qn +1  Pr |ξ j | > η 1 (λn ρn )k  = (pn − qn )O  λn ρn =O  pn (λn ρn )k  → 0,  which completes the proof. [sent-1010, score-1.548]
</p><p>82 For any π, we can write  =  ˆ −2 ∑ pn n +1 βπ, j j=q  By Condition A3,  ˆ ˆ Rn (βπ ) + λn |π|σ2 − Rn (β∗ ) − λn |π∗ |σ2 n ˆ ˆ ˆ ˆ ˆ ∗ , X j > +(βπ − β∗ )′ (X′ Xn )(βπ − β∗ ) + λn (|π| − |π∗ |)σ2 . [sent-1012, score-0.458]
</p><p>83 j  Hence, we have for any π ∈ M sn , ˆ ˆ Rn (βπ ) + λn |π|σ2 − Rn (β∗ ) − λn |π∗ |σ2 ≥ n  ∑  w j,  j∈π∪π∗ n  where ˆ ˆ ˆ ˆ w j = −2βπ, j < Yn − Yn∗ , Xnj > I( j ∈ π∗ ) + nρn (βπ, j − β∗ )2 + λn (I( j ∈ π − π∗ ) − I( j ∈ π∗ − π))σ2 . [sent-1014, score-0.156]
</p><p>84 ≥ − < Yn − Y Let ˆ Bn = {− < Yn − Yn∗ , Xnj >2 /(nρn ) + λn σ2 > 0, j = qn + 1, . [sent-1022, score-0.174]
</p><p>85 , pn }, let Mπ be the projection operator onto the space spanned by (X ( j) , j ∈ π). [sent-1034, score-0.458]
</p><p>86 Lemma 10 There exists η > 0 such that for any π ∈ M sn with π∗ n  π,  ′  µn (I − Mπ )µn ≥ η|π− |nc2 −c1 , where π− = π∗ − π. [sent-1038, score-0.156]
</p><p>87 For given π ∈ M sn with π∗ n  π, we have  ′  = =  µn (I − Mπ )µn inf  α∈R|π|  Xπ− β∗ − − Xπ α π ′  2 ′  ′  ′  ′ ′  inf (β∗ − , α )(Xπ− , Xπ ) (Xπ− , Xπ )(β∗ − , α ) π π  α∈R|π|  ≥ n β∗ − π  2  ρn  −  ≥ M3 M4 |π |nc2 −c1 , where β∗ − = (β∗ , j ∈ π− ) and the last inequality is due to Condition A4. [sent-1040, score-0.156]
</p><p>88 , pn }, let ′  Zπ =  µn (I − Mπ )εn  µ′n (I − Mπ )µn  . [sent-1044, score-0.458]
</p><p>89 Since Pr(|Zπ | > t) ≤ C exp(−t 2 /2) 1053  (6)  K IM , K WON AND C HOI  for some C > 0, we have Pr  Hence, if we let t =  √  max |Zπ | > t  π∈M sn  ∑  ≤  C exp(−t 2 /2)  π∈M sn Cpsn exp(−t 2 /2). [sent-1048, score-0.312]
</p><p>90 n  ≤ wsn log pn , max |Zπ | > t  Pr  ≤ C exp((−w/2 + 1)sn log pn ) → 0  π∈M sn  as w → ∞. [sent-1049, score-1.234]
</p><p>91 (7) 2 r(π) Hence Pr  ′  max εn Mπ εn ≥ t  π∈M sn  sn  ∑  ≤  k=1  pn Pr(Wk ≤ t), k  where Wk ∼ χ2 (k). [sent-1058, score-0.77]
</p><p>92 Since Pr(Wk ≥ t) ≤ Pr(Wsn ≥ t), we have Pr  sn  ′  max en Mπ en ≥ t  π∈M sn  ≤ Pr(Wsn ≥ t) ∑  k=1  pn k  ≤ Pr(Wsn ≥ t)psn . [sent-1059, score-0.77]
</p><p>93 For π π∗ , Lemmas 10, 11 and 12 imply n Rn (π) − Rn (π∗ ) + λn (|π| − |π∗ |)σ2 n n ′  ′  ′  = µn (I − Mπ )µn + 2µn (I − Mπ )εn + εn (Mπ∗ − Mπ )εn + λn (|π| − |π∗ |)σ2 n  ≥ η|π− |nc2 −c1 − 2 η|π− |nc2 −c1 O p ( sn log pn ) − O p (sn log pn ) − |π− |λn , where π− = π∗ − π. [sent-1068, score-1.182]
</p><p>94 Since sn log pn ≤ o(nc2 −c1 ) and λn = o(nc2 −c1 ), the proof is done. [sent-1069, score-0.669]
</p><p>95 By Theorem 1 of Zhang and Shen (2010), the probability of (9) is larger than 2 − 1 + e1/2 exp −  λn − log λn 2  pn −qn  ,  which converges to 1 when 2 log pn − λn + log λn → −∞. [sent-1071, score-1.081]
</p><p>96 The equivalent condition with 2 log pn − λn + log λn → −∞ is λn − 2 log pn − log log pn → ∞. [sent-1072, score-1.673]
</p><p>97 Proof of Theorem 4 By Theorem 4 of Kim and Kwon (2012), the solution path of the SCAD or minimax concave penalty include the true model with probability converging to 1. [sent-1074, score-0.251]
</p><p>98 Since condition A3’ is stronger than condition A3, the GICλn with λn = o(nc2 −c1 ) is consistent, and so is with the solution path of the SCAD or minimax concave penalty. [sent-1075, score-0.218]
</p><p>99 By (6), we have j ˜n ˆ ˆ Pr(Bc ) ≤ Pr(< Yn − Yn∗ , Xnj >2 > nρn λn σ2 for some j = qn + 1, . [sent-1081, score-0.174]
</p><p>100 ˜n Hence, as long as 2M1 log pn /(ρn rin f ) − λn → −∞, Pr(Bc ) → 0 and the proof is done. [sent-1085, score-0.557]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gic', 0.504), ('pn', 0.458), ('gics', 0.258), ('qn', 0.174), ('scad', 0.169), ('submodels', 0.168), ('bic', 0.163), ('sn', 0.156), ('imensions', 0.129), ('onsistent', 0.129), ('riteria', 0.129), ('xnj', 0.129), ('yn', 0.127), ('ric', 0.11), ('xn', 0.11), ('pmax', 0.109), ('pr', 0.107), ('kwon', 0.103), ('hoi', 0.092), ('won', 0.092), ('covariates', 0.088), ('igh', 0.086), ('aic', 0.08), ('election', 0.077), ('odel', 0.07), ('path', 0.069), ('kim', 0.068), ('ptm', 0.066), ('corrected', 0.066), ('shen', 0.066), ('foster', 0.065), ('im', 0.064), ('chen', 0.063), ('log', 0.055), ('cn', 0.054), ('consistent', 0.053), ('criteria', 0.053), ('penalized', 0.053), ('scheetz', 0.052), ('wsn', 0.052), ('consistency', 0.051), ('rn', 0.048), ('simulation', 0.047), ('selection', 0.047), ('rin', 0.044), ('concave', 0.041), ('minimax', 0.041), ('bn', 0.041), ('penalty', 0.04), ('diverging', 0.04), ('criterion', 0.039), ('nonzero', 0.039), ('hosik', 0.039), ('lightness', 0.039), ('shao', 0.039), ('regularity', 0.039), ('performances', 0.037), ('zou', 0.037), ('genes', 0.036), ('jk', 0.036), ('argmin', 0.035), ('tail', 0.034), ('zhang', 0.033), ('ation', 0.033), ('clipped', 0.033), ('signal', 0.032), ('theorem', 0.031), ('george', 0.031), ('korea', 0.031), ('remarks', 0.029), ('gene', 0.028), ('error', 0.028), ('sinica', 0.028), ('statistica', 0.028), ('choi', 0.028), ('nc', 0.028), ('lasso', 0.027), ('broman', 0.026), ('casavant', 0.026), ('craven', 0.026), ('selectivity', 0.026), ('sunghoon', 0.026), ('swiderski', 0.026), ('yongdai', 0.026), ('huang', 0.026), ('condition', 0.024), ('smoothly', 0.023), ('shef', 0.022), ('diverge', 0.022), ('greenshtein', 0.022), ('converging', 0.022), ('variance', 0.021), ('eigenvalue', 0.021), ('prediction', 0.02), ('eye', 0.02), ('seoul', 0.02), ('asymptotic', 0.019), ('model', 0.019), ('solution', 0.019), ('wang', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="29-tfidf-1" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>Author: Yongdai Kim, Sunghoon Kwon, Hosik Choi</p><p>Abstract: Asymptotic properties of model selection criteria for high-dimensional regression models are studied where the dimension of covariates is much larger than the sample size. Several sufﬁcient conditions for model selection consistency are provided. Non-Gaussian error distributions are considered and it is shown that the maximal number of covariates for model selection consistency depends on the tail behavior of the error distribution. Also, sufﬁcient conditions for model selection consistency are given when the variance of the noise is neither known nor estimated consistently. Results of simulation studies as well as real data analysis are given to illustrate that ﬁnite sample performances of consistent model selection criteria can be quite different. Keywords: model selection consistency, general information criteria, high dimension, regression</p><p>2 0.14924984 <a title="29-tfidf-2" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>Author: Lan Xue, Annie Qu</p><p>Abstract: The varying-coefﬁcient model is ﬂexible and powerful for modeling the dynamic changes of regression coefﬁcients. It is important to identify signiﬁcant covariates associated with response variables, especially for high-dimensional settings where the number of covariates can be larger than the sample size. We consider model selection in the high-dimensional setting and adopt difference convex programming to approximate the L0 penalty, and we investigate the global optimality properties of the varying-coefﬁcient estimator. The challenge of the variable selection problem here is that the dimension of the nonparametric form for the varying-coefﬁcient modeling could be inﬁnite, in addition to dealing with the high-dimensional linear covariates. We show that the proposed varying-coefﬁcient estimator is consistent, enjoys the oracle property and achieves an optimal convergence rate for the non-zero nonparametric components for high-dimensional data. Our simulations and numerical examples indicate that the difference convex algorithm is efﬁcient using the coordinate decent algorithm, and is able to select the true model at a higher frequency than the least absolute shrinkage and selection operator (LASSO), the adaptive LASSO and the smoothly clipped absolute deviation (SCAD) approaches. Keywords: coordinate decent algorithm, difference convex programming, L0 - regularization, large-p small-n, model selection, nonparametric function, oracle property, truncated L1 penalty</p><p>3 0.14063405 <a title="29-tfidf-3" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>Author: Wojciech Rejchel</p><p>Abstract: The problem of ranking is to predict or to guess the ordering between objects on the basis of their observed features. In this paper we consider ranking estimators that minimize the empirical convex risk. We prove generalization bounds for the excess risk of such estimators with rates that are 1 faster than √n . We apply our results to commonly used ranking algorithms, for instance boosting or support vector machines. Moreover, we study the performance of considered estimators on real data sets. Keywords: convex risk minimization, excess risk, support vector machine, empirical process, U-process</p><p>4 0.13972321 <a title="29-tfidf-4" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>Author: Michael U. Gutmann, Aapo Hyvärinen</p><p>Abstract: We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a ﬁnite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only speciﬁed up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artiﬁcially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities. Keywords: statistics unnormalized models, partition function, computation, estimation, natural image</p><p>5 0.12875324 <a title="29-tfidf-5" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>Author: Gérard Biau</p><p>Abstract: Random forests are a scheme proposed by Leo Breiman in the 2000’s for building a predictor ensemble with a set of decision trees that grow in randomly selected subspaces of data. Despite growing interest and practical use, there has been little exploration of the statistical properties of random forests, and little is known about the mathematical forces driving the algorithm. In this paper, we offer an in-depth analysis of a random forests model suggested by Breiman (2004), which is very close to the original algorithm. We show in particular that the procedure is consistent and adapts to sparsity, in the sense that its rate of convergence depends only on the number of strong features and not on how many noise variables are present. Keywords: random forests, randomization, sparsity, dimension reduction, consistency, rate of convergence</p><p>6 0.10306484 <a title="29-tfidf-6" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>7 0.08437907 <a title="29-tfidf-7" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>8 0.081507102 <a title="29-tfidf-8" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>9 0.077379242 <a title="29-tfidf-9" href="./jmlr-2012-Minimax_Manifold_Estimation.html">68 jmlr-2012-Minimax Manifold Estimation</a></p>
<p>10 0.070890814 <a title="29-tfidf-10" href="./jmlr-2012-Integrating_a_Partial_Model_into_Model_Free_Reinforcement_Learning.html">51 jmlr-2012-Integrating a Partial Model into Model Free Reinforcement Learning</a></p>
<p>11 0.060483392 <a title="29-tfidf-11" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>12 0.058113161 <a title="29-tfidf-12" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>13 0.049229171 <a title="29-tfidf-13" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>14 0.047915645 <a title="29-tfidf-14" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<p>15 0.040734604 <a title="29-tfidf-15" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>16 0.038548499 <a title="29-tfidf-16" href="./jmlr-2012-EP-GIG_Priors_and_Applications_in_Bayesian_Sparse_Learning.html">35 jmlr-2012-EP-GIG Priors and Applications in Bayesian Sparse Learning</a></p>
<p>17 0.036868677 <a title="29-tfidf-17" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>18 0.036164224 <a title="29-tfidf-18" href="./jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</a></p>
<p>19 0.034261525 <a title="29-tfidf-19" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>20 0.032844275 <a title="29-tfidf-20" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.19), (1, 0.18), (2, -0.195), (3, -0.093), (4, -0.022), (5, -0.036), (6, -0.106), (7, -0.03), (8, 0.033), (9, -0.08), (10, 0.232), (11, 0.157), (12, 0.258), (13, 0.052), (14, -0.123), (15, 0.253), (16, -0.102), (17, -0.009), (18, 0.08), (19, 0.044), (20, 0.012), (21, -0.095), (22, -0.019), (23, -0.086), (24, 0.018), (25, 0.024), (26, -0.151), (27, -0.113), (28, 0.058), (29, 0.054), (30, -0.085), (31, 0.071), (32, 0.003), (33, 0.115), (34, 0.071), (35, -0.121), (36, 0.07), (37, 0.021), (38, 0.098), (39, -0.021), (40, -0.002), (41, 0.017), (42, 0.074), (43, -0.002), (44, -0.027), (45, 0.079), (46, -0.093), (47, 0.161), (48, 0.034), (49, 0.09)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9497543 <a title="29-lsi-1" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>Author: Yongdai Kim, Sunghoon Kwon, Hosik Choi</p><p>Abstract: Asymptotic properties of model selection criteria for high-dimensional regression models are studied where the dimension of covariates is much larger than the sample size. Several sufﬁcient conditions for model selection consistency are provided. Non-Gaussian error distributions are considered and it is shown that the maximal number of covariates for model selection consistency depends on the tail behavior of the error distribution. Also, sufﬁcient conditions for model selection consistency are given when the variance of the noise is neither known nor estimated consistently. Results of simulation studies as well as real data analysis are given to illustrate that ﬁnite sample performances of consistent model selection criteria can be quite different. Keywords: model selection consistency, general information criteria, high dimension, regression</p><p>2 0.53062099 <a title="29-lsi-2" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>Author: Michael U. Gutmann, Aapo Hyvärinen</p><p>Abstract: We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a ﬁnite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only speciﬁed up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artiﬁcially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities. Keywords: statistics unnormalized models, partition function, computation, estimation, natural image</p><p>3 0.52185589 <a title="29-lsi-3" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>Author: Gérard Biau</p><p>Abstract: Random forests are a scheme proposed by Leo Breiman in the 2000’s for building a predictor ensemble with a set of decision trees that grow in randomly selected subspaces of data. Despite growing interest and practical use, there has been little exploration of the statistical properties of random forests, and little is known about the mathematical forces driving the algorithm. In this paper, we offer an in-depth analysis of a random forests model suggested by Breiman (2004), which is very close to the original algorithm. We show in particular that the procedure is consistent and adapts to sparsity, in the sense that its rate of convergence depends only on the number of strong features and not on how many noise variables are present. Keywords: random forests, randomization, sparsity, dimension reduction, consistency, rate of convergence</p><p>4 0.48782659 <a title="29-lsi-4" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>Author: Lan Xue, Annie Qu</p><p>Abstract: The varying-coefﬁcient model is ﬂexible and powerful for modeling the dynamic changes of regression coefﬁcients. It is important to identify signiﬁcant covariates associated with response variables, especially for high-dimensional settings where the number of covariates can be larger than the sample size. We consider model selection in the high-dimensional setting and adopt difference convex programming to approximate the L0 penalty, and we investigate the global optimality properties of the varying-coefﬁcient estimator. The challenge of the variable selection problem here is that the dimension of the nonparametric form for the varying-coefﬁcient modeling could be inﬁnite, in addition to dealing with the high-dimensional linear covariates. We show that the proposed varying-coefﬁcient estimator is consistent, enjoys the oracle property and achieves an optimal convergence rate for the non-zero nonparametric components for high-dimensional data. Our simulations and numerical examples indicate that the difference convex algorithm is efﬁcient using the coordinate decent algorithm, and is able to select the true model at a higher frequency than the least absolute shrinkage and selection operator (LASSO), the adaptive LASSO and the smoothly clipped absolute deviation (SCAD) approaches. Keywords: coordinate decent algorithm, difference convex programming, L0 - regularization, large-p small-n, model selection, nonparametric function, oracle property, truncated L1 penalty</p><p>5 0.4635902 <a title="29-lsi-5" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>Author: Tim van Erven, Mark D. Reid, Robert C. Williamson</p><p>Abstract: Mixability of a loss characterizes fast rates in the online learning setting of prediction with expert advice. The determination of the mixability constant for binary losses is straightforward but opaque. In the binary case we make this transparent and simpler by characterising mixability in terms of the second derivative of the Bayes risk of proper losses. We then extend this result to multiclass proper losses where there are few existing results. We show that mixability is governed by the maximum eigenvalue of the Hessian of the Bayes risk, relative to the Hessian of the Bayes risk for log loss. We conclude by comparing our result to other work that bounds prediction performance in terms of the geometry of the Bayes risk. Although all calculations are for proper losses, we also show how to carry the results across to improper losses. Keywords: mixability, multiclass, prediction with expert advice, proper loss, learning rates</p><p>6 0.44029915 <a title="29-lsi-6" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>7 0.34029603 <a title="29-lsi-7" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>8 0.31735262 <a title="29-lsi-8" href="./jmlr-2012-Integrating_a_Partial_Model_into_Model_Free_Reinforcement_Learning.html">51 jmlr-2012-Integrating a Partial Model into Model Free Reinforcement Learning</a></p>
<p>9 0.30171892 <a title="29-lsi-9" href="./jmlr-2012-Minimax_Manifold_Estimation.html">68 jmlr-2012-Minimax Manifold Estimation</a></p>
<p>10 0.2652649 <a title="29-lsi-10" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>11 0.2651898 <a title="29-lsi-11" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>12 0.20692676 <a title="29-lsi-12" href="./jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</a></p>
<p>13 0.20628056 <a title="29-lsi-13" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>14 0.20242989 <a title="29-lsi-14" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>15 0.18505532 <a title="29-lsi-15" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>16 0.18175946 <a title="29-lsi-16" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<p>17 0.17264716 <a title="29-lsi-17" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>18 0.16801068 <a title="29-lsi-18" href="./jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</a></p>
<p>19 0.16473788 <a title="29-lsi-19" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<p>20 0.16250335 <a title="29-lsi-20" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.421), (7, 0.017), (21, 0.076), (26, 0.033), (29, 0.025), (35, 0.011), (49, 0.02), (75, 0.066), (77, 0.036), (79, 0.019), (92, 0.098), (96, 0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85248333 <a title="29-lda-1" href="./jmlr-2012-A_Topic_Modeling_Toolbox_Using_Belief_Propagation.html">9 jmlr-2012-A Topic Modeling Toolbox Using Belief Propagation</a></p>
<p>Author: Jia Zeng</p><p>Abstract: Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology. This paper introduces a topic modeling toolbox (TMBP) based on the belief propagation (BP) algorithms. TMBP toolbox is implemented by MEX C++/Matlab/Octave for either Windows 7 or Linux. Compared with existing topic modeling packages, the novelty of this toolbox lies in the BP algorithms for learning LDA-based topic models. The current version includes BP algorithms for latent Dirichlet allocation (LDA), authortopic models (ATM), relational topic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project and more BP-based algorithms for various topic models will be added in the near future. Interested users may also extend BP algorithms for learning more complicated topic models. The source codes are freely available under the GNU General Public Licence, Version 1.0 at https://mloss.org/software/view/399/. Keywords: topic models, belief propagation, variational Bayes, Gibbs sampling</p><p>same-paper 2 0.64855552 <a title="29-lda-2" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>Author: Yongdai Kim, Sunghoon Kwon, Hosik Choi</p><p>Abstract: Asymptotic properties of model selection criteria for high-dimensional regression models are studied where the dimension of covariates is much larger than the sample size. Several sufﬁcient conditions for model selection consistency are provided. Non-Gaussian error distributions are considered and it is shown that the maximal number of covariates for model selection consistency depends on the tail behavior of the error distribution. Also, sufﬁcient conditions for model selection consistency are given when the variance of the noise is neither known nor estimated consistently. Results of simulation studies as well as real data analysis are given to illustrate that ﬁnite sample performances of consistent model selection criteria can be quite different. Keywords: model selection consistency, general information criteria, high dimension, regression</p><p>3 0.34125453 <a title="29-lda-3" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>Author: Lan Xue, Annie Qu</p><p>Abstract: The varying-coefﬁcient model is ﬂexible and powerful for modeling the dynamic changes of regression coefﬁcients. It is important to identify signiﬁcant covariates associated with response variables, especially for high-dimensional settings where the number of covariates can be larger than the sample size. We consider model selection in the high-dimensional setting and adopt difference convex programming to approximate the L0 penalty, and we investigate the global optimality properties of the varying-coefﬁcient estimator. The challenge of the variable selection problem here is that the dimension of the nonparametric form for the varying-coefﬁcient modeling could be inﬁnite, in addition to dealing with the high-dimensional linear covariates. We show that the proposed varying-coefﬁcient estimator is consistent, enjoys the oracle property and achieves an optimal convergence rate for the non-zero nonparametric components for high-dimensional data. Our simulations and numerical examples indicate that the difference convex algorithm is efﬁcient using the coordinate decent algorithm, and is able to select the true model at a higher frequency than the least absolute shrinkage and selection operator (LASSO), the adaptive LASSO and the smoothly clipped absolute deviation (SCAD) approaches. Keywords: coordinate decent algorithm, difference convex programming, L0 - regularization, large-p small-n, model selection, nonparametric function, oracle property, truncated L1 penalty</p><p>4 0.34095973 <a title="29-lda-4" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>Author: Matus Telgarsky</p><p>Abstract: Boosting combines weak learners into a predictor with low empirical risk. Its dual constructs a high entropy distribution upon which weak learners and training labels are uncorrelated. This manuscript studies this primal-dual relationship under a broad family of losses, including the exponential loss of AdaBoost and the logistic loss, revealing: • Weak learnability aids the whole loss family: for any ε > 0, O (ln(1/ε)) iterations sufﬁce to produce a predictor with empirical risk ε-close to the inﬁmum; • The circumstances granting the existence of an empirical risk minimizer may be characterized in terms of the primal and dual problems, yielding a new proof of the known rate O (ln(1/ε)); • Arbitrary instances may be decomposed into the above two, granting rate O (1/ε), with a matching lower bound provided for the logistic loss. Keywords: boosting, convex analysis, weak learnability, coordinate descent, maximum entropy</p><p>5 0.3408969 <a title="29-lda-5" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>Author: Gérard Biau</p><p>Abstract: Random forests are a scheme proposed by Leo Breiman in the 2000’s for building a predictor ensemble with a set of decision trees that grow in randomly selected subspaces of data. Despite growing interest and practical use, there has been little exploration of the statistical properties of random forests, and little is known about the mathematical forces driving the algorithm. In this paper, we offer an in-depth analysis of a random forests model suggested by Breiman (2004), which is very close to the original algorithm. We show in particular that the procedure is consistent and adapts to sparsity, in the sense that its rate of convergence depends only on the number of strong features and not on how many noise variables are present. Keywords: random forests, randomization, sparsity, dimension reduction, consistency, rate of convergence</p><p>6 0.33999008 <a title="29-lda-6" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>7 0.33943975 <a title="29-lda-7" href="./jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</a></p>
<p>8 0.33441135 <a title="29-lda-8" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>9 0.33308795 <a title="29-lda-9" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>10 0.33280712 <a title="29-lda-10" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>11 0.32917097 <a title="29-lda-11" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>12 0.32790789 <a title="29-lda-12" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>13 0.32723236 <a title="29-lda-13" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>14 0.32576698 <a title="29-lda-14" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>15 0.32548434 <a title="29-lda-15" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>16 0.3239457 <a title="29-lda-16" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>17 0.32361135 <a title="29-lda-17" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>18 0.32309884 <a title="29-lda-18" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>19 0.32112634 <a title="29-lda-19" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>20 0.32076544 <a title="29-lda-20" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
