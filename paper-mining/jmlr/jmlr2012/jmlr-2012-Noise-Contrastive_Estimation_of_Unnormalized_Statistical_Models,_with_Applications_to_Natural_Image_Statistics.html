<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-76" href="#">jmlr2012-76</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</h1>
<br/><p>Source: <a title="jmlr-2012-76-pdf" href="http://jmlr.org/papers/volume13/gutmann12a/gutmann12a.pdf">pdf</a></p><p>Author: Michael U. Gutmann, Aapo Hyvärinen</p><p>Abstract: We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a ﬁnite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only speciﬁed up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artiﬁcially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities. Keywords: statistics unnormalized models, partition function, computation, estimation, natural image</p><p>Reference: <a title="jmlr-2012-76-reference" href="../jmlr2012_reference/jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 , xTd ) of a random vector x ∈ Rn is observed which follows an unknown probability density function (pdf) pd . [sent-26, score-0.397]
</p><p>2 The data-pdf pd is modeled by a parameterized family of functions {pm (. [sent-27, score-0.397]
</p><p>3 It is commonly assumed that pd belongs to this family. [sent-29, score-0.397]
</p><p>4 ; α) to include a normalizing parameter c m and estimate ln pm (. [sent-67, score-0.436]
</p><p>5 A relative description of the data X is then given by the ratio pd /pn of the two density functions. [sent-114, score-0.432]
</p><p>6 If the reference distribution pn is known, one can, of course, obtain pd from the ratio pd /pn . [sent-115, score-1.104]
</p><p>7 In the following, we show that training a classiﬁer based on logistic regression provides a relative description of X in the form of an estimate of the ratio pd /pn . [sent-118, score-0.459]
</p><p>8 As the pdf pd of the data x is unknown, we model the class-conditional probability p(. [sent-124, score-0.438]
</p><p>9 2 The classconditional probability densities are thus p(u|C = 1; θ) = pm (u; θ),  p(u|C = 0) = pn (u). [sent-127, score-0.45]
</p><p>10 The posterior probabilities for the classes are therefore P (C = 1|u; θ) =  pm (u; θ) , pm (u; θ) + νpn (u)  P (C = 0|u; θ) =  νpn (u) , pm (u; θ) + νpn (u)  (3)  where ν is the ratio P (C = 0)/P (C = 1) = Tn /Td . [sent-129, score-0.56]
</p><p>11 ; θ) and pn , G(u; θ) = ln pm (u; θ) − ln pn (u),  (4)  h(u; θ) = rν (G(u; θ)) ,  (5)  1 1 + ν exp(−u)  (6)  h(u; θ) can be written as  where rν (u) =  is the logistic function parameterized by ν. [sent-133, score-1.09]
</p><p>12 The conditional loglikelihood is given by Td +Tn  ℓ(θ) = t=1 Td  Ct ln P (Ct = 1|ut ; θ) + (1 − Ct ) ln P (Ct = 0|ut ; θ) Tn  ln [h(xt ; θ)] +  = t=1  t=1  ln [1 − h(yt ; θ)] . [sent-135, score-0.676]
</p><p>13 The estimator is deﬁned to be the argument θ T which maximizes     T  Tn  1  d JT (θ) = ln [h(xt ; θ)] + ln [1 − h(yt ; θ)] ,  Td t=1 t=1  (8)  where the nonlinearity h(. [sent-168, score-0.505]
</p><p>14 ; θ), ˜ J(fm ) = E {ln [rν (fm (x) − ln pn (x))]} + ν E {ln [1 − rν (fm (y) − ln pn (y))]} . [sent-185, score-0.888]
</p><p>15 ˜ The following theorem shows that the data-pdf pd can be found by maximization of J, that is by learning a nonparametric classiﬁer under the ideal situation of an inﬁnite amount of data. [sent-187, score-0.397]
</p><p>16 ˜ Theorem 1 (Nonparametric estimation) J attains a maximum at fm = ln pd . [sent-188, score-0.642]
</p><p>17 There are no other extrema if the noise density pn is chosen such that it is nonzero whenever pd is nonzero. [sent-189, score-0.733]
</p><p>18 The positivity condition for pn in the theorem tells us that the data-pdf pd cannot be inferred at regions in the data space where there are no contrastive noise samples. [sent-196, score-0.943]
</p><p>19 For example, the estimation of a pdf pd which is nonzero only on the positive real line by means of a noise distribution pn that has its support on the negative real line is impossible. [sent-197, score-0.862]
</p><p>20 This has two consequences for any estimation method that is based on optimization: First, it restricts the space where the data-pdf pd is searched for. [sent-201, score-0.485]
</p><p>21 For the characterization of the estimator in this situation, it is normally assumed that pd follows the model, so that there is a θ ⋆ with pd (. [sent-203, score-0.794]
</p><p>22 The correct estimate of pd is thus obtained as the sample size Td increases. [sent-208, score-0.397]
</p><p>23 (a) pn is nonzero whenever pd is nonzero P  (b) supθ |JT (θ) − J(θ)| → 0 (c) The matrix I ν = g(u)g(u)T Pν (u)pd (u)du has full rank, where g(u) = ∇θ ln pm (u; θ)|θ⋆ ,  Pν (u) =  νpn (u) . [sent-212, score-1.016]
</p><p>24 pd (u) + νpn (u)  The proof is given in Appendix A. [sent-213, score-0.397]
</p><p>25 Corollary 5 For ν → ∞, Σ is independent of the choice of pn and equals Σ = I −1 − I −1 E(g) E(g)T I −1 , where E(g) = g(u)pd (u)du and I = g(u)g(u)T pd (u)du. [sent-236, score-0.672]
</p><p>26 The asymptotic distribution of the estimation error becomes thus independent from pn . [sent-237, score-0.413]
</p><p>27 The corollaries above give one answer to the question on how to choose the noise distribution pn and the ratio ν: If ν is made large enough, the actual choice of pn is not of great importance. [sent-244, score-0.646]
</p><p>28 Intuitively, one could think that a good candidate for the noise distribution pn is a distribution which is close to the data distribution pd . [sent-249, score-0.733]
</p><p>29 If pn is too different from pd , the classiﬁcation problem might be too easy and would not require the system to learn much about the structure of the data. [sent-250, score-0.672]
</p><p>30 This intuition is partly justiﬁed by the following theoretical result: 313  ¨ G UTMANN AND H YV ARINEN  1 Corollary 7 If pn = pd then Σ = 1 + ν  I −1 − I −1 E(g) E(g)T I −1 . [sent-251, score-0.672]
</p><p>31 Proof The corollary follows from Theorem 3 and the fact that Pν equals ν/(1 + ν) for pn = pd . [sent-252, score-0.672]
</p><p>32 Choose noise for which an analytical expression for ln pn is available. [sent-257, score-0.505]
</p><p>33 Its log-pdf is 1 n c⋆ = − ln | det Λ⋆ | − ln(2π) , 2 2  1 ln pd (x) = − xT Λ⋆ x + c⋆ , 2  (12)  where c⋆ does not depend on x and normalizes pd to integrate to one. [sent-276, score-1.132]
</p><p>34 The model that we estimate is thus ln pm (x; θ) = ln p0 (x; α) + c. [sent-286, score-0.513]
</p><p>35 With a validation set of size Tv , a sample version DKL of the Kullback-Leibler divergence is given by the difference 1 Tv 1 Tv ˆ ˆ DKL = ln pd (xt ) − ln p0 (xt ; α) + ln 1/Z(α) . [sent-311, score-0.951]
</p><p>36 The sources in the vector s ∈ R4 are identically distributed and independent from each other so that the data log-pdf ln pd is n  f (b⋆ x) + c⋆ . [sent-343, score-0.619]
</p><p>37 i  ln pd (x) =  (15)  i=1  The i-th row of the matrix B⋆ = A−1 is denoted by b⋆ . [sent-344, score-0.566]
</p><p>38 The nonlinearity f and the constant c⋆ , which normalizes pd to integrate to one, are in this case given by √ n f (u) = − 2|u|, c⋆ = ln |detB⋆ | − ln 2. [sent-346, score-0.902]
</p><p>39 For noise-contrastive estimation, we add an additional normalizing parameter c and estimate the model ln pm (x; θ) = ln p0 (x; α) + c, m with θ = (α, c). [sent-369, score-0.605]
</p><p>40 In Corollary 7, we have considered the hypothetical case where the noise distribution pn is the same as the data distribution pd . [sent-404, score-0.733]
</p><p>41 The asymptotic variance for pn = pd is, for a given value of ν, always smaller than the asymptotic variance for the case where the noise is Gaussian. [sent-408, score-0.881]
</p><p>42 However, by choosing ν large enough for the case of Gaussian noise, it is possible to get estimates which are as accurate as those obtained in the hypothetical situation where pn = pd . [sent-409, score-0.672]
</p><p>43 Figure (d) compares noisecontrastive estimation with Gaussian noise to the hypothetical case where pn equals the data distribution pd . [sent-467, score-0.866]
</p><p>44 1 Data Used in the Comparison For the comparison, we use artiﬁcial data which follows the ICA model in Equation (14) with the data log-pdf ln pd being given by Equation (15). [sent-518, score-0.566]
</p><p>45 The log-pdf ln pd is then speciﬁed with Equation (16). [sent-521, score-0.566]
</p><p>46 The nonlinearity f and the log normalizing constant c∗ in Equation (15) are in that case f (u) = −2 ln cosh  π √ u , 2 3  c∗ = ln |detB⋆ | + n ln  π √ , 4 3  respectively. [sent-524, score-0.766]
</p><p>47 4 P ERSISTENT C ONTRASTIVE D IVERGENCE As contrastive divergence, persistent contrastive divergence (Younes, 1989; Tieleman, 2008) uses the update rule in Equation (19) together with an approximative evaluation of the integral in Equation (20) to learn the parameters α. [sent-591, score-0.536]
</p><p>48 Since persistent contrastive divergence differs from contrastive divergence only by the initialization of the Markov chains, it has the same tuning parameters. [sent-597, score-0.583]
</p><p>49 3 Limitations of the Comparison For all considered methods but contrastive and persistent contrastive divergence, the algorithm which is used to optimize the given objectives can be rather freely chosen. [sent-617, score-0.489]
</p><p>50 Then, we compare it with contrastive and persistent contrastive divergence. [sent-631, score-0.489]
</p><p>51 2 C OMPARISON  WITH  C ONTRASTIVE AND P ERSISTENT C ONTRASTIVE D IVERGENCE  Since contrastive and persistent contrastive divergence do not have an objective function and given the randomness that is introduced by the minibatches, it is difﬁcult to choose a reliable stopping criterion. [sent-657, score-0.536]
</p><p>52 For noise-contrastive estimation, the parameter ν controls the trade-off between computational and statistical performance; for contrastive and persistent contrastive divergence, it is the number of leapfrog steps and the number of Markov steps taken in each update. [sent-662, score-0.567]
</p><p>53 In case of thin tails, noise-contrastive estimation performs similarly to Monte Carlo maximum likelihood, and contrastive or persistent contrastive divergence has a better trade-off. [sent-673, score-0.624]
</p><p>54 5  Time [log10 s]  Time [log10 s]  (a) Noise-contrastive estimation  (b) Contrastive divergence  Figure 7: Example of a trade-off curve for noise-contrastive estimation and contrastive divergence. [sent-767, score-0.474]
</p><p>55 5  4  (b) Logistic sources  Figure 8: Distribution of the trade-off curves for contrastive divergence (CD, green), persistent contrastive divergence (PCD, cyan), and noise-contrastive estimation (NCE, red). [sent-814, score-0.755]
</p><p>56 For the contrastive noise distribution pn , we take a uniform distribution on the surface of the n − 1 dimensional sphere S on which x is deﬁned. [sent-832, score-0.546]
</p><p>57 ln pn = − ln(2) − n−1 ln(π) − (n − 2) ln(r) + ln Γ 2  n−1 2  330  with r =  √  n − 1. [sent-837, score-0.613]
</p><p>58 3 Two-Layer Model with Thresholding Nonlinearities The ﬁrst model that we consider is n  ln pm (x; θ) =  n  f (yk ; ak , bk ) + c,  T Qki (wi x)2 ,  yk =  (24)  i=1  k=1  where f is a smooth, compressive thresholding function that is parameterized by ak and bk . [sent-859, score-0.503]
</p><p>59 The compression of large values of yk leads to numerical robustness in the computation of ln pm . [sent-877, score-0.43]
</p><p>60 In this regime, the squashing nonlinearities map thus more often the noise input to small values than natural images ˆ so that ln pm (u; θ T ) tends to be larger when input u is a natural image than when it is noise (see Section 5. [sent-929, score-0.797]
</p><p>61 One could, however, think that the thresholding nonlinearities are suboptimal because they ignore the fact that natural images lead, compared to the noise, rather often to yk which are close to zero, see Figure 14(b). [sent-931, score-0.415]
</p><p>62 An optimal nonlinearity should, unlike the thresholding nonlinearities, assign a large value to both large and small yk while mapping intermediate values of yk to small numbers. [sent-932, score-0.412]
</p><p>63 5 0  1  2  3  4  5 y  6  7  8  9  −8  10  0  1  2  3  4  5 y  6  7  8  9  10  (b) Distribution of second-layer outputs yk  (a) Learned nonlinearities  Figure 14: Two-layer model with thresholding nonlinearities: Learned nonlinearities and interpretation. [sent-963, score-0.546]
</p><p>64 In Figure (a), the nonlinearities acting on pooled low-frequency feature detectors are shown in green (dashed lines), those for medium and high frequency feature detectors in black (solid lines). [sent-966, score-0.404]
</p><p>65 Compared to the thresholding nonlinearities from the previous subsection, the learned nonlinearity has also for small inputs large outputs. [sent-1007, score-0.463]
</p><p>66 In order to get the likely points, we drew random samples that followed the noise distribution pn (uniform on the sphere), and used them as initial points in ˆ the optimization of the various log-densities ln pm (x; θ T ) with respect to x under the constraint of ˆ Equation (22). [sent-1041, score-0.68]
</p><p>67 We have further derived the asymptotic distribution of the estimation error which shows that, in the limit of arbitrarily many contrastive noise samples, the estimator performs like the maximum likelihood estimator. [sent-1111, score-0.443]
</p><p>68 It follows that νpn (u)pm (u; θ) ,, pm (u; θ) + νpn (u) νpn (u)pd (u) , pd (u)r 1 (−G(u; θ)) = ν pm (u; θ) + νpn (u) νpn (u)rν (G(u; θ)) =  which are key properties for the proofs below. [sent-1137, score-0.747]
</p><p>69 Lemma 8 For ǫ > 0 and φ(x) a perturbation of the log-pdf fm (x) = ln pm (x), ˜ ˜ J(fm + ǫφ) = J(fm ) + ǫ  [pd (u)r 1 (−fm (u) + ln pn (u)) − ν  νpn (u)rν (fm (u) − ln pn (u))]φ(u)du − ǫ2 r 1 (−fm (u) + ln pn (u))rν (fm (u) − ln pn (u)) ν 2 (pd (u) + νpn (u))φ(u)2 du + O(ǫ3 ). [sent-1144, score-2.271]
</p><p>70 ˜ Proof The proof is obtained by evaluating the objective function J in Equation (11) at fm + ǫφ, and making then use of the Taylor expansions in Equation (30) and Equation (31) with u = fm (x) − ln pn (x), u1 = φ(x) and u2 = 0. [sent-1145, score-0.596]
</p><p>71 This happens if and only if pd (u)r 1 (−fm (u) + ln pn (u)) = νpn (u)rν (fm (u) − ln pn (u)). [sent-1149, score-1.285]
</p><p>72 pm (u) + νpn (u) pm (u) + νpn (u) That is, as ν > 0, pm (u) = pd (u) at all points u where pn (u) = 0. [sent-1151, score-1.197]
</p><p>73 Hence, pm = pd , or fm = ln pd , leads to an extremum of J. [sent-1153, score-1.214]
</p><p>74 342  N OISE -C ONTRASTIVE E STIMATION  ˜ Inserting fm = ln pd into J in Lemma 8 leads to 2  ǫ ˜ ˜ J(ln pd + ǫφ) = J(ln pd ) − 2  νpn (u)pd (u) φ(u)2 du + O(ǫ3 ). [sent-1154, score-1.511]
</p><p>75 pd (u) + νpn (u)  Since the term of order ǫ2 is negative for all choices of φ, the extremum is a maximum. [sent-1155, score-0.397]
</p><p>76 The assumption that pn (u) = 0 whenever pd (u) = 0 shows that fm = ln pd is the only extremum and completes the proof. [sent-1156, score-1.314]
</p><p>77 Proof With the deﬁnition of J in Equation (10), we have J(θ + ǫϕ) =  ln [rν (G(u; θ + ǫϕ))] pd (u)du + ν  ln [1 − rν (G(u; θ + ǫϕ))] pn (u)du. [sent-1164, score-1.01]
</p><p>78 ν  Lemma 10 If pn (u) = 0 whenever pd (u) = 0 and if Iν =  g(u)g(u)T Pν (u)pd (u)du  is full rank, where νpn (u) , pd (u) + νpn (u) g(u) = ∇θ ln pm (u; θ)|θ=θ⋆ ,  Pν (u) =  then J(θ ⋆ ) > J(θ ⋆ + ϕ) ∀ϕ = 0. [sent-1168, score-1.413]
</p><p>79 This happens if pd (u)(1 − h(u; θ)) = νpn (u)h(u; θ), that is, if νpn (u)pd (u) νpn (u)pm (u; θ) = , pm (u; θ) + νpn (u) pm (u; θ) + νpn (u) where we have used Equation (28) and Equation (29) as in the proof for Lemma 8. [sent-1170, score-0.747]
</p><p>80 J(θ ⋆ + ǫϕ) = J(θ ⋆ ) −  The terms h(u; θ ⋆ ) and 1 − h(u; θ ⋆ ) are with Equation (27) h(u; θ ⋆ ) =  pd (u) , pd (u) + νpn (u)  1 − h(u; θ ⋆ ) =  νpn (u) . [sent-1175, score-0.794]
</p><p>81 pd (u) + νpn (u)  The expression for J(θ ⋆ + ǫϕ) becomes then ǫ2 T ϕ g(u)g(u)T Pν (u)pd (u)du ϕ + O(ǫ3 ) 2 by inserting the deﬁnition of u1 evaluated at θ ⋆ , and making use of the deﬁnitions for Pν (u) and g(u) in the statement of the lemma. [sent-1176, score-0.397]
</p><p>82 (1 − h(u; θ ⋆ ))h(u; θ ⋆ )(pd (u) + νpn (u)) = pd (u) + νpn (u) Hence, P  lim HJ (θ ⋆ ) → −  Td →∞  νpn (u)pd (u) g(u)g(u)T du, pd (u) + νpn (u)  which is −I ν . [sent-1224, score-0.794]
</p><p>83 Lemma 14 The variance Var ∇θ JT (θ ⋆ ) is 1 1 Iν − 1 + E(Pν g) E(Pν g)T , Td ν where I ν , Pν and g were deﬁned in Lemma 10, and the expectation is taken over the data-pdf pd . [sent-1231, score-0.421]
</p><p>84 Denoting by A the sum of the ﬁrst and last line of Equation (34), we have A =  1 Td  g(u)g(u)T (1 − h(u; θ ⋆ ))2 pd (u) + h(u; θ ⋆ )2 νpn (u) du  since Tn = νTd . [sent-1236, score-0.472]
</p><p>85 Now, Equation (27) and pm (u; θ ⋆ ) = pd (u) imply that νpn (u)pd (u) pd (u) + νpn (u) = Pν pd (u),  (1 − h(u; θ ⋆ ))2 pd (u) + h(u; θ ⋆ )2 νpn (u) =  349  (34)  ¨ G UTMANN AND H YV ARINEN  so that A = =  1 g(u)g(u)T Pν pd (u)du Td 1 Iν. [sent-1237, score-2.16]
</p><p>86 Td  (35)  Again, Equation (27) and pm (u; θ ⋆ ) = pd (u) imply that (1 − h(u; θ ⋆ ))pd (u) = h(u; θ ⋆ )νpn (u) νpn (u)pd (u) = pd (u) + νpn (u) = Pν pd (u), so that the ﬁrst line in Equation (35) is zero and mx =  Pν g(u)pd (u)du. [sent-1240, score-1.407]
</p><p>87 The term B is thus B = −  1 Td  Pν g(u)pd (u)du  Pν g(u)T pd (u)du. [sent-1241, score-0.397]
</p><p>88 Td The term νmy is with Equation (27) and pm (u; θ ⋆ ) = pd (u) νmy =  Pν g(u)pd (u)du,  C = −  1 T (νmy )(νmy ) νTd  so that νmy = mx , and hence  1 B. [sent-1244, score-0.613]
</p><p>89 an ) has the distribution 1 n T pd (x) = f (ai x), Z i=1 where Z is the partition function. [sent-1266, score-0.397]
</p><p>90 By orthogonality of A, pd (Ax) =  1 n f (xi ), Z i=1  which equals ps (x) where ps is the distribution of the sources s of the ICA model. [sent-1267, score-0.474]
</p><p>91 With the above data and noise distribution, Pν (u) has the property that Pν (Au) = =  νpn (Au) pd (Au) + νpn (Au) νpn (u) . [sent-1271, score-0.458]
</p><p>92 , gn (u), gc (u))T T where gi (u) = ∇ai ln pm (u) = f ′ (ai u)u and gc (u) = ∂c ln pm (u) = 1. [sent-1277, score-0.688]
</p><p>93 ν The same reasoning shows that pd (u)Pν (u)g(u)du = A  ˜ ps (v)˜ (v)Pν (v)dv, g  ˜ ˜ which we will denote below by Am. [sent-1287, score-0.397]
</p><p>94 1 Trade-Off, Section 4: Comparison of the Different Settings of Contrastive and Persistent Contrastive Divergence We compare here the different settings of contrastive and persistent contrastive divergence. [sent-1346, score-0.489]
</p><p>95 ) + c,  ln pm (x; θ) = k=1  where the nonlinearity f is a cubic spline. [sent-1433, score-0.511]
</p><p>96 5: Reﬁnement of the Thresholding Model We are taking here a simple approach to the estimation of a two-layer model with spline nonlinearity f : We leave the feature extraction layers that were obtained for the thresholding model in Section 5. [sent-1468, score-0.401]
</p><p>97 Figure 21(b) shows the effective nonlinearities fk when the different scales of the second layer outputs yk and the normalizing parameter c are taken into account, as we have done in Figure 14(a). [sent-1485, score-0.448]
</p><p>98 5 0  2  4  6  8  10 y  12  14  16  18  20  0  (a) Learned nonlinearity  1  2  3 y  4  5  6  (b) Learned effective nonlinearities  Figure 21: Reﬁnement of the thresholding model of Section 5. [sent-1500, score-0.418]
</p><p>99 The dashed vertical line indicates the border of validity of the learned nonlinearity since 99% of the yk fall, for natural image input, to the left of it. [sent-1505, score-0.398]
</p><p>100 Inspection of Figure 14(b) shows that the optimal nonlinearities fk take, unlike the thresholding nonlinearities, the distribution of the second-layer outputs yk fully into account. [sent-1509, score-0.393]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pd', 0.397), ('td', 0.312), ('pn', 0.275), ('contrastive', 0.21), ('jt', 0.185), ('tn', 0.183), ('nonlinearities', 0.178), ('ontrastive', 0.178), ('pm', 0.175), ('ln', 0.169), ('nonlinearity', 0.167), ('nce', 0.156), ('arinen', 0.15), ('utmann', 0.15), ('oise', 0.133), ('yv', 0.129), ('unnormalized', 0.121), ('qki', 0.111), ('mle', 0.106), ('stimation', 0.101), ('hyv', 0.1), ('xt', 0.096), ('normalizing', 0.092), ('estimation', 0.088), ('yk', 0.086), ('ica', 0.084), ('yt', 0.082), ('detectors', 0.079), ('leapfrog', 0.078), ('sqerror', 0.078), ('fm', 0.076), ('du', 0.075), ('rinen', 0.074), ('thresholding', 0.073), ('laplacian', 0.073), ('spline', 0.073), ('persistent', 0.069), ('monte', 0.063), ('noise', 0.061), ('carlo', 0.059), ('mixing', 0.059), ('wi', 0.058), ('images', 0.054), ('mse', 0.054), ('sources', 0.053), ('image', 0.051), ('asymptotic', 0.05), ('gutmann', 0.048), ('divergence', 0.047), ('learned', 0.045), ('noisecontrastive', 0.045), ('patches', 0.043), ('hj', 0.043), ('pcd', 0.043), ('blue', 0.042), ('curve', 0.041), ('equation', 0.041), ('mx', 0.041), ('pdf', 0.041), ('black', 0.04), ('hg', 0.039), ('sim', 0.039), ('layer', 0.036), ('ratio', 0.035), ('dim', 0.035), ('score', 0.034), ('minibatches', 0.034), ('likelihood', 0.034), ('pred', 0.033), ('stimuli', 0.033), ('outputs', 0.031), ('curves', 0.031), ('red', 0.03), ('green', 0.028), ('icons', 0.028), ('rasmussen', 0.028), ('chains', 0.027), ('circles', 0.027), ('tv', 0.027), ('logistic', 0.027), ('triangles', 0.026), ('au', 0.026), ('conjugate', 0.026), ('dashed', 0.025), ('gk', 0.025), ('fk', 0.025), ('esults', 0.025), ('variance', 0.024), ('covariance', 0.024), ('nt', 0.024), ('asterisks', 0.024), ('orthogonality', 0.024), ('osindero', 0.024), ('natural', 0.024), ('cd', 0.023), ('squared', 0.023), ('models', 0.022), ('hamiltonian', 0.022), ('jis', 0.022), ('jmle', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="76-tfidf-1" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>Author: Michael U. Gutmann, Aapo Hyvärinen</p><p>Abstract: We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a ﬁnite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only speciﬁed up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artiﬁcially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities. Keywords: statistics unnormalized models, partition function, computation, estimation, natural image</p><p>2 0.13972321 <a title="76-tfidf-2" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>Author: Yongdai Kim, Sunghoon Kwon, Hosik Choi</p><p>Abstract: Asymptotic properties of model selection criteria for high-dimensional regression models are studied where the dimension of covariates is much larger than the sample size. Several sufﬁcient conditions for model selection consistency are provided. Non-Gaussian error distributions are considered and it is shown that the maximal number of covariates for model selection consistency depends on the tail behavior of the error distribution. Also, sufﬁcient conditions for model selection consistency are given when the variance of the noise is neither known nor estimated consistently. Results of simulation studies as well as real data analysis are given to illustrate that ﬁnite sample performances of consistent model selection criteria can be quite different. Keywords: model selection consistency, general information criteria, high dimension, regression</p><p>3 0.11340851 <a title="76-tfidf-3" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>Author: Matthieu Solnon, Sylvain Arlot, Francis Bach</p><p>Abstract: In this paper we study the kernel multiple ridge regression framework, which we refer to as multitask regression, using penalization techniques. The theoretical analysis of this problem shows that the key element appearing for an optimal calibration is the covariance matrix of the noise between the different tasks. We present a new algorithm to estimate this covariance matrix, based on the concept of minimal penalty, which was previously used in the single-task regression framework to estimate the variance of the noise. We show, in a non-asymptotic setting and under mild assumptions on the target function, that this estimator converges towards the covariance matrix. Then plugging this estimator into the corresponding ideal penalty leads to an oracle inequality. We illustrate the behavior of our algorithm on synthetic examples. Keywords: multi-task, oracle inequality, learning theory</p><p>4 0.10993078 <a title="76-tfidf-4" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>Author: Wojciech Rejchel</p><p>Abstract: The problem of ranking is to predict or to guess the ordering between objects on the basis of their observed features. In this paper we consider ranking estimators that minimize the empirical convex risk. We prove generalization bounds for the excess risk of such estimators with rates that are 1 faster than √n . We apply our results to commonly used ranking algorithms, for instance boosting or support vector machines. Moreover, we study the performance of considered estimators on real data sets. Keywords: convex risk minimization, excess risk, support vector machine, empirical process, U-process</p><p>5 0.09198723 <a title="76-tfidf-5" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>Author: David P. Helmbold, Philip M. Long</p><p>Abstract: This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classiﬁers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high conﬁdence whether or not any individual variable is relevant. Keywords: feature selection, generalization, learning theory</p><p>6 0.084054999 <a title="76-tfidf-6" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>7 0.083059117 <a title="76-tfidf-7" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>8 0.082967922 <a title="76-tfidf-8" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>9 0.080154017 <a title="76-tfidf-9" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>10 0.073245361 <a title="76-tfidf-10" href="./jmlr-2012-Integrating_a_Partial_Model_into_Model_Free_Reinforcement_Learning.html">51 jmlr-2012-Integrating a Partial Model into Model Free Reinforcement Learning</a></p>
<p>11 0.072848372 <a title="76-tfidf-11" href="./jmlr-2012-Algebraic_Geometric_Comparison_of_Probability_Distributions.html">15 jmlr-2012-Algebraic Geometric Comparison of Probability Distributions</a></p>
<p>12 0.072590157 <a title="76-tfidf-12" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>13 0.070747681 <a title="76-tfidf-13" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>14 0.066995539 <a title="76-tfidf-14" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>15 0.064870156 <a title="76-tfidf-15" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<p>16 0.063767232 <a title="76-tfidf-16" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>17 0.060354907 <a title="76-tfidf-17" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>18 0.059761818 <a title="76-tfidf-18" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>19 0.053930111 <a title="76-tfidf-19" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>20 0.051381387 <a title="76-tfidf-20" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.255), (1, 0.055), (2, -0.149), (3, -0.09), (4, -0.025), (5, -0.099), (6, 0.026), (7, 0.07), (8, -0.046), (9, -0.072), (10, 0.139), (11, 0.111), (12, 0.189), (13, 0.071), (14, -0.198), (15, 0.161), (16, -0.042), (17, 0.067), (18, 0.291), (19, 0.08), (20, -0.079), (21, 0.078), (22, 0.047), (23, 0.062), (24, 0.017), (25, -0.053), (26, 0.074), (27, -0.018), (28, 0.031), (29, 0.137), (30, 0.027), (31, -0.038), (32, -0.101), (33, 0.001), (34, -0.016), (35, -0.048), (36, 0.093), (37, 0.024), (38, 0.071), (39, 0.015), (40, -0.019), (41, -0.057), (42, -0.125), (43, -0.061), (44, -0.009), (45, 0.045), (46, 0.037), (47, -0.015), (48, 0.024), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94654363 <a title="76-lsi-1" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>Author: Michael U. Gutmann, Aapo Hyvärinen</p><p>Abstract: We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a ﬁnite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only speciﬁed up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artiﬁcially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities. Keywords: statistics unnormalized models, partition function, computation, estimation, natural image</p><p>2 0.57579249 <a title="76-lsi-2" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>Author: Yongdai Kim, Sunghoon Kwon, Hosik Choi</p><p>Abstract: Asymptotic properties of model selection criteria for high-dimensional regression models are studied where the dimension of covariates is much larger than the sample size. Several sufﬁcient conditions for model selection consistency are provided. Non-Gaussian error distributions are considered and it is shown that the maximal number of covariates for model selection consistency depends on the tail behavior of the error distribution. Also, sufﬁcient conditions for model selection consistency are given when the variance of the noise is neither known nor estimated consistently. Results of simulation studies as well as real data analysis are given to illustrate that ﬁnite sample performances of consistent model selection criteria can be quite different. Keywords: model selection consistency, general information criteria, high dimension, regression</p><p>3 0.52496672 <a title="76-lsi-3" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>Author: Matthieu Solnon, Sylvain Arlot, Francis Bach</p><p>Abstract: In this paper we study the kernel multiple ridge regression framework, which we refer to as multitask regression, using penalization techniques. The theoretical analysis of this problem shows that the key element appearing for an optimal calibration is the covariance matrix of the noise between the different tasks. We present a new algorithm to estimate this covariance matrix, based on the concept of minimal penalty, which was previously used in the single-task regression framework to estimate the variance of the noise. We show, in a non-asymptotic setting and under mild assumptions on the target function, that this estimator converges towards the covariance matrix. Then plugging this estimator into the corresponding ideal penalty leads to an oracle inequality. We illustrate the behavior of our algorithm on synthetic examples. Keywords: multi-task, oracle inequality, learning theory</p><p>4 0.47438198 <a title="76-lsi-4" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>Author: Tim van Erven, Mark D. Reid, Robert C. Williamson</p><p>Abstract: Mixability of a loss characterizes fast rates in the online learning setting of prediction with expert advice. The determination of the mixability constant for binary losses is straightforward but opaque. In the binary case we make this transparent and simpler by characterising mixability in terms of the second derivative of the Bayes risk of proper losses. We then extend this result to multiclass proper losses where there are few existing results. We show that mixability is governed by the maximum eigenvalue of the Hessian of the Bayes risk, relative to the Hessian of the Bayes risk for log loss. We conclude by comparing our result to other work that bounds prediction performance in terms of the geometry of the Bayes risk. Although all calculations are for proper losses, we also show how to carry the results across to improper losses. Keywords: mixability, multiclass, prediction with expert advice, proper loss, learning rates</p><p>5 0.39482161 <a title="76-lsi-5" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>Author: Hugo Larochelle, Michael Mandel, Razvan Pascanu, Yoshua Bengio</p><p>Abstract: Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artiﬁcial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classiﬁers. We study the Classiﬁcation RBM (ClassRBM), a variant on the RBM adapted to the classiﬁcation setting. We study different strategies for training the ClassRBM and show that competitive classiﬁcation performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classiﬁcation problems, namely semi-supervised and multitask learning. Keywords: restricted Boltzmann machine, classiﬁcation, discriminative learning, generative learning</p><p>6 0.39306849 <a title="76-lsi-6" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>7 0.38008481 <a title="76-lsi-7" href="./jmlr-2012-Algebraic_Geometric_Comparison_of_Probability_Distributions.html">15 jmlr-2012-Algebraic Geometric Comparison of Probability Distributions</a></p>
<p>8 0.37489453 <a title="76-lsi-8" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>9 0.37418559 <a title="76-lsi-9" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>10 0.36145803 <a title="76-lsi-10" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>11 0.34212628 <a title="76-lsi-11" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>12 0.3281427 <a title="76-lsi-12" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>13 0.29967409 <a title="76-lsi-13" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>14 0.26591441 <a title="76-lsi-14" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<p>15 0.2642509 <a title="76-lsi-15" href="./jmlr-2012-Optimistic_Bayesian_Sampling_in_Contextual-Bandit_Problems.html">86 jmlr-2012-Optimistic Bayesian Sampling in Contextual-Bandit Problems</a></p>
<p>16 0.26263195 <a title="76-lsi-16" href="./jmlr-2012-Integrating_a_Partial_Model_into_Model_Free_Reinforcement_Learning.html">51 jmlr-2012-Integrating a Partial Model into Model Free Reinforcement Learning</a></p>
<p>17 0.25452578 <a title="76-lsi-17" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>18 0.25140962 <a title="76-lsi-18" href="./jmlr-2012-Finite-Sample_Analysis_of_Least-Squares_Policy_Iteration.html">46 jmlr-2012-Finite-Sample Analysis of Least-Squares Policy Iteration</a></p>
<p>19 0.24742042 <a title="76-lsi-19" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<p>20 0.2425451 <a title="76-lsi-20" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.01), (21, 0.024), (26, 0.537), (29, 0.035), (35, 0.017), (49, 0.017), (56, 0.01), (57, 0.012), (69, 0.011), (75, 0.04), (77, 0.014), (92, 0.082), (96, 0.094)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97783637 <a title="76-lda-1" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>Author: David Verstraeten, Benjamin Schrauwen, Sander Dieleman, Philemon Brakel, Pieter Buteneers, Dejan Pecevski</p><p>Abstract: Oger (OrGanic Environment for Reservoir computing) is a Python toolbox for building, training and evaluating modular learning architectures on large data sets. It builds on MDP for its modularity, and adds processing of sequential data sets, gradient descent training, several crossvalidation schemes and parallel parameter optimization methods. Additionally, several learning algorithms are implemented, such as different reservoir implementations (both sigmoid and spiking), ridge regression, conditional restricted Boltzmann machine (CRBM) and others, including GPU accelerated versions. Oger is released under the GNU LGPL, and is available from http: //organic.elis.ugent.be/oger. Keywords: Python, modular architectures, sequential processing</p><p>same-paper 2 0.89759696 <a title="76-lda-2" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>Author: Michael U. Gutmann, Aapo Hyvärinen</p><p>Abstract: We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a ﬁnite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only speciﬁed up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artiﬁcially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities. Keywords: statistics unnormalized models, partition function, computation, estimation, natural image</p><p>3 0.89477509 <a title="76-lda-3" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<p>Author: Zhiwei Qin, Donald Goldfarb</p><p>Abstract: We consider a class of sparse learning problems in high dimensional feature space regularized by a structured sparsity-inducing norm that incorporates prior knowledge of the group structure of the features. Such problems often pose a considerable challenge to optimization algorithms due to the non-smoothness and non-separability of the regularization term. In this paper, we focus on two commonly adopted sparsity-inducing regularization terms, the overlapping Group Lasso penalty l1 /l2 -norm and the l1 /l∞ -norm. We propose a uniﬁed framework based on the augmented Lagrangian method, under which problems with both types of regularization and their variants can be efﬁciently solved. As one of the core building-blocks of this framework, we develop new algorithms using a partial-linearization/splitting technique and prove that the accelerated versions 1 of these algorithms require O( √ε ) iterations to obtain an ε-optimal solution. We compare the performance of these algorithms against that of the alternating direction augmented Lagrangian and FISTA methods on a collection of data sets and apply them to two real-world problems to compare the relative merits of the two norms. Keywords: structured sparsity, overlapping Group Lasso, alternating direction methods, variable splitting, augmented Lagrangian</p><p>4 0.87684637 <a title="76-lda-4" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>Author: Prateek Jain, Brian Kulis, Jason V. Davis, Inderjit S. Dhillon</p><p>Abstract: Metric and kernel learning arise in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem. In particular, we propose a general optimization framework for learning metrics via linear transformations, and analyze in detail a special case of our framework—that of minimizing the LogDet divergence subject to linear constraints. We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework. Our theoretical connections between metric and kernel learning have two main consequences: 1) the learned kernel matrix parameterizes a linear transformation kernel function and can be applied inductively to new data points, 2) our result yields a constructive method for kernelizing most existing Mahalanobis metric learning formulations. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction. Keywords: divergence metric learning, kernel learning, linear transformation, matrix divergences, logdet</p><p>5 0.58095485 <a title="76-lda-5" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>Author: Garvesh Raskutti, Martin J. Wainwright, Bin Yu</p><p>Abstract: Sparse additive models are families of d-variate functions with the additive decomposition f ∗ = ∑ j∈S f j∗ , where S is an unknown subset of cardinality s ≪ d. In this paper, we consider the case where each univariate component function f j∗ lies in a reproducing kernel Hilbert space (RKHS), and analyze a method for estimating the unknown function f ∗ based on kernels combined with ℓ1 -type convex regularization. Working within a high-dimensional framework that allows both the dimension d and sparsity s to increase with n, we derive convergence rates in the L2 (P) and L2 (Pn ) norms over the class Fd,s,H of sparse additive models with each univariate function f j∗ in the unit ball of a univariate RKHS with bounded kernel function. We complement our upper bounds by deriving minimax lower bounds on the L2 (P) error, thereby showing the optimality of our method. Thus, we obtain optimal minimax rates for many interesting classes of sparse additive models, including polynomials, splines, and Sobolev classes. We also show that if, in contrast to our univariate conditions, the d-variate function class is assumed to be globally bounded, then much √ faster estimation rates are possible for any sparsity s = Ω( n), showing that global boundedness is a signiﬁcant restriction in the high-dimensional setting. Keywords: sparsity, kernel, non-parametric, convex, minimax</p><p>6 0.57721949 <a title="76-lda-6" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>7 0.55462486 <a title="76-lda-7" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>8 0.53506023 <a title="76-lda-8" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>9 0.51646703 <a title="76-lda-9" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>10 0.51617318 <a title="76-lda-10" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>11 0.51013064 <a title="76-lda-11" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>12 0.50707054 <a title="76-lda-12" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>13 0.50441843 <a title="76-lda-13" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>14 0.50069433 <a title="76-lda-14" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>15 0.49647579 <a title="76-lda-15" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>16 0.49646568 <a title="76-lda-16" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>17 0.4918541 <a title="76-lda-17" href="./jmlr-2012-Large-scale_Linear_Support_Vector_Regression.html">54 jmlr-2012-Large-scale Linear Support Vector Regression</a></p>
<p>18 0.48816133 <a title="76-lda-18" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>19 0.46165448 <a title="76-lda-19" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>20 0.45980883 <a title="76-lda-20" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
