<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-39" href="#">jmlr2012-39</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</h1>
<br/><p>Source: <a title="jmlr-2012-39-pdf" href="http://jmlr.org/papers/volume13/huang12b/huang12b.pdf">pdf</a></p><p>Author: Jian Huang, Cun-Hui Zhang</p><p>Abstract: The ℓ1 -penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ1 -penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1 -penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results. Keywords: variable selection, penalized estimation, oracle inequality, generalized linear models, selection consistency, sparsity</p><p>Reference: <a title="jmlr-2012-39-reference" href="../jmlr2012_reference/jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this article, we consider a class of weighted ℓ1 -penalized estimators for convex loss functions of a general form, including the generalized linear models. [sent-6, score-0.191]
</p><p>2 We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1 -penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. [sent-7, score-0.155]
</p><p>3 A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. [sent-9, score-0.235]
</p><p>4 We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. [sent-10, score-0.306]
</p><p>5 Keywords: variable selection, penalized estimation, oracle inequality, generalized linear models, selection consistency, sparsity  1. [sent-12, score-0.253]
</p><p>6 Meinshausen and B¨ hlmann (2006) showed u that, for neighborhood selection in the Gaussian graphical models, under a neighborhood stability condition on the design matrix and certain additional regularity conditions, the Lasso is selection consistent even when p → ∞ at a rate faster than n. [sent-23, score-0.271]
</p><p>7 Zhao and Yu (2006) formalized the neighborhood stability condition in the context of linear regression as a strong irrepresentable condition. [sent-24, score-0.19]
</p><p>8 Candes and Tao (2007) derived an upper bound for the ℓ2 loss of a closely related Dantzig selector in the estimation of regression coefﬁcients under a condition on the number of nonzero coefﬁcients and a uniform uncertainty principle on the design matrix. [sent-25, score-0.198]
</p><p>9 For convex minimization methods beyond linear regression, van de Geer (2008) studied the Lasso in high-dimensional generalized linear models (GLM) and obtained prediction and ℓ1 estimation error bounds. [sent-30, score-0.127]
</p><p>10 (2010) studied penalized M-estimators with a general class of regularizers, including an ℓ2 error bound for the Lasso in GLM under a restricted strong convexity and other regularity conditions. [sent-32, score-0.247]
</p><p>11 As a remedy, a number of proposals have been introduced in the literature and proven to be variable selection consistent under regularity conditions of milder forms, including concave penalized LSE (Fan and Li, 2001; Zhang, 2010a), adaptive Lasso (Zou, 2006; Meier and B¨ hlmann, 2007; Huang et al. [sent-35, score-0.311]
</p><p>12 In this article, we study a class of weighted ℓ1 -penalized estimators with a convex loss function. [sent-37, score-0.191]
</p><p>13 This class includes the Lasso, adaptive Lasso and multistage recursive application of adaptive Lasso in generalized linear models as special cases. [sent-38, score-0.242]
</p><p>14 We study prediction, estimation, selection and sparsity properties of the weighted ℓ1 -penalized estimator based on a convex loss in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. [sent-39, score-0.247]
</p><p>15 • We extend the existing theory for the unweighted Lasso from linear regression to more general convex loss function. [sent-41, score-0.16]
</p><p>16 • We develop a multistage method to approximate concave regularized convex minimization with recursive application of adaptive Lasso, and provide sharper risk bounds for this concave regularization approach in the general setting. [sent-42, score-0.522]
</p><p>17 In Section 2 we describe a general formulation of the absolute penalized minimization problem with a convex loss, along with two basic inequalities and a number of examples. [sent-45, score-0.297]
</p><p>18 In Section 3 we develop oracle inequalities for the weighted Lasso estimator for general quasi star-shaped loss functions and an ℓ2 bound on the prediction error. [sent-46, score-0.613]
</p><p>19 In 1840  A BSOLUTE P ENALIZED C ONVEX M INIMIZATION  Section 4 we develop multistage recursive applications of adaptive Lasso as an approximate concave regularization method and provide sharper oracle inequalities for this approach. [sent-47, score-0.624]
</p><p>20 Absolute Penalized Convex Minimization In this section, we deﬁne the weighted Lasso for a convex loss function and characterize its solutions via the KKT conditions. [sent-53, score-0.159]
</p><p>21 We then derive some basic inequalities for the weighted Lasso solutions in terms of the symmetrized Bregman divergence (Bregman, 1967; Nielsen and Nock, 2007). [sent-54, score-0.482]
</p><p>22 We also illustrate the applications of the basic inequalities in several important examples. [sent-55, score-0.11]
</p><p>23 1 Deﬁnition and the KKT Conditions We consider a general convex loss function of the form ℓ(β) = ψ(β) − β, z ,  (1)  where ψ(β) is a known convex function, z is observed, and β is unknown. [sent-57, score-0.146]
</p><p>24 The weighted absolute penalized estimator, or weighted Lasso, is deﬁned as β = arg min ℓ(β) + λ|W β|1 . [sent-66, score-0.23]
</p><p>25 In Zou ˙ and Li (2008) and Zhang (2010b), the weights w j are computed iteratively with w j = ρλ (β j ), where ˙ ρλ (t) = (d/dt)ρλ (t) with a suitable concave penalty function ρλ (t). [sent-76, score-0.197]
</p><p>26 However, in other convex minimization problems, such weights need to be computed iteratively. [sent-88, score-0.135]
</p><p>27 In high-dimensional models, the performance of an estimator β is typically measured by its proximity to a target under conditions on the sparsity of β∗ and the ˙ ˙ size of the negative gradient −ℓ(β∗ ) = z − ψ(β∗ ). [sent-93, score-0.136]
</p><p>28 Let D(β, β∗ ) = ℓ(β) − ℓ(β∗ ) − ℓ(β∗ ), β − β∗ be the Bregman divergence (Bregman, 1967) and consider its symmetrized version (Nielsen and Nock, 2007) ˙ ˙ ∆(β, β∗ ) = D(β, β∗ ) + D(β∗ , β) = β − β∗ , ψ(β) − ψ(β∗ ) . [sent-95, score-0.305]
</p><p>29 Two basic inequalities below provide upper bounds for the symmetrized Bregman divergence ∆(β, β∗ ). [sent-97, score-0.415]
</p><p>30 Consequently, in Ω0 ∩{(|wS |∞ λ+ ˙ z∗ )/(λ − z∗ ) ≤ ξ}, h = 0 belongs to the sign-restricted cone C− (ξ, S) = {b ∈ C (ξ, S) : b j (ψ(β + b) − 0 1 c }, where ˙ ψ(β)) j ≤ 0 ∀ j ∈ S C (ξ, S) = b ∈ R p : |WSc bSc |1 ≤ ξ|bS |1 = 0 . [sent-110, score-0.122]
</p><p>31 By Lemma 1 (ii), it sufﬁces to study the analytical properties of the penalized criterion with the error h = β − β∗ in the sign-restricted cone, provided that the event (|wS |∞ λ + z∗ )/(λ − z∗ ) ≤ ξ has large probability. [sent-115, score-0.149]
</p><p>32 However, unless C− (ξ, S) is speciﬁed, we will consider the 1 0 larger cone in (9) in order to simplify the analysis. [sent-116, score-0.122]
</p><p>33 In linear regression or generalized linear models, we may conveniently 0 1 consider β∗ as the vector of true regression coefﬁcients under a probability measure Pβ∗ . [sent-119, score-0.136]
</p><p>34 In Section 3, we provide “fast rate” of convergence for the Bregman divergence via oracle inequalities for |hS |1 in (8). [sent-122, score-0.385]
</p><p>35 Example 1 (Linear regression) Consider the linear regression model p  yi =  ∑ xi j β j + εi ,  i = 1, . [sent-125, score-0.223]
</p><p>36 , n,  j=1  1843  (10)  H UANG AND Z HANG  where yi is the response variable, xi j are predictors or design variables, and εi is the error term. [sent-128, score-0.186]
</p><p>37 The estimator (2) can be written as a weighted Lasso with ψ(β) = |Xβ|2 /(2n) and z = X ′ y/n in (1). [sent-136, score-0.124]
</p><p>38 Example 2 (Logistic regression) We observe (X, y) ∈ Rn×(p+1) with independent rows (xi , yi ), where yi ∈ {0, 1} are binary response variables with Pβ (yi = 1|xi ) = πi (β) = exp(xi β)/(1 + exp(xi β)), 1 ≤ i ≤ n. [sent-139, score-0.134]
</p><p>39 n i=1  ℓ(β) = ψ(β) − z′ β with ψ(β) = ∑  (12)  Thus, (2) is a weighted ℓ1 penalized MLE. [sent-141, score-0.163]
</p><p>40 Since ψ(β) = ∑n xi πi (β)/n and i=1 ∗ )) − logit(π (β)) = xi (β∗ − β), (4) gives logit(πi (β i ∆(β, β∗ ) =  1 n ∑ K πi (β∗ ), πi (β) + K πi (β), πi (β∗ ) n i=1  . [sent-143, score-0.176]
</p><p>41 Thus, the symmetrized Bregman divergence ∆(β∗ , β) is the symmetrised KL-divergence. [sent-144, score-0.375]
</p><p>42 Suppose that conditionally on X, yi are independent under Pβ with yi ∼ f (yi |θi ) = exp  θi yi − ψ0 (θi ) c(yi , σ) + , θi = xi β. [sent-148, score-0.289]
</p><p>43 The i=1 KL divergence is D fn (·|X, β∗ ) fn (·|X, β) = Eβ∗ log  f(n) (y|X, β∗ ) . [sent-151, score-0.15]
</p><p>44 f(n) (y|X, β)  The symmetrized Bregman divergence can be written as ∆(β, β∗ ) =  σ2 D f(n) (·|X, β∗ ) f(n) (·|X, β) + D f(n) (·|X, β) f(n) (·|X, β∗ ) n 1844  . [sent-152, score-0.305]
</p><p>45 A BSOLUTE P ENALIZED C ONVEX M INIMIZATION  Example 4 (Nonparametric density estimation) Although the focus of this paper is on regression models, here we illustrate that ∆(β, β∗ ) is the symmetrised KL divergence in the context of nonparametric density estimation. [sent-153, score-0.256]
</p><p>46 Since Eβ T (yi ) = ψ(β), the KL divergence is i=1 D f (·|β∗ ) f (·|β) = Eβ∗ log  f (yi |β∗ ) ˙ = ψ(β) − ψ(β∗ ) − β − β∗ , ψ(β∗ ) . [sent-159, score-0.15]
</p><p>47 f (yi |β)  Again, the symmetrized Bregman divergence is the symmetrised KL divergence between the target density f (·|β∗ ) and the estimated density f (·|β): ∆(β, β∗ ) = D f (·|β∗ ) f (·|β) + D f (·|β) f (·|β∗ ) . [sent-160, score-0.53]
</p><p>48 Consider w jk = I{ j = k}, so that the penalty for the off˙ diagonal elements are uniformly weighted. [sent-169, score-0.144]
</p><p>49 Since Lemma 1 requires |(z − ψ(β∗ )) jk | ≤ w jk λ, β∗ is ′ X/n on the diagonal and the true β in correlations. [sent-170, score-0.13]
</p><p>50 taken to match X jk In the event max j=k |z jk − β∗ | ≤ λ, Lemma 1 (i) gives jk |S|λ max |β∗ | = o(1) ⇒ (β∗ )−1/2 β(β∗ )−1/2 − I p×p jk j=k  2  = o(1)  where · 2 is the spectrum norm. [sent-172, score-0.313]
</p><p>51 Since (8) is monotone in the weights, the oracle inequalities are sharper when the weights w j are smaller in S ⊇ { j : β∗ = 0} and larger in Sc . [sent-177, score-0.404]
</p><p>52 j We say that a function φ(b) deﬁned in R p is quasi star-shaped if φ(tb) is continuous and nondecreasing in t ∈ [0, ∞) for all b ∈ R p and limb→0 φ(b) = 0. [sent-178, score-0.145]
</p><p>53 The sublevel sets {b : φ(b) ≤ t} of a quasi star-shaped function are all star-shaped. [sent-180, score-0.145]
</p><p>54 The GIF extends the squared compatibility constant (van de Geer and B¨ hlmann, 2009) and the u weak and sign-restricted cone invertibility factors (Ye and Zhang, 2010) from the linear regression model with φ0 (·) = 0 to the general model (1) and from ℓq norms to general φ(·). [sent-183, score-0.361]
</p><p>55 The basic inequality (8) implies that the symmetrized Bregman divergence ∆(β, β∗ ) is no greater than a linear function of |hS |1 , where h = β − β∗ . [sent-187, score-0.305]
</p><p>56 Since the symmetrized Bregman ¨ divergence (4) is approximately quadratic, ∆(β, β∗ ) ≈ h, ψ(β∗ )h , in a neighborhood of β∗ , this is ¨ reasonable when h = β − β∗ is not too large and ψ(β∗ ) is invertible in the cone. [sent-189, score-0.341]
</p><p>57 We ﬁrst provide a set of general oracle inequalities. [sent-191, score-0.157]
</p><p>58 Theorem 4 Let {z∗ , z∗ } be as in (5) with S ⊇ { j : β∗ = 0}, Ω0 in (6), 0 ≤ η ≤ η∗ ≤ 1, and j 0 1 {φ0 (b), φ(b)} be a pair of quasi star-shaped functions. [sent-192, score-0.145]
</p><p>59 The oracle inequalities in Theorem 4 control both the estimation error in terms of φ(β − β∗ ) and the prediction error in terms of the symmetrized Bregman divergence ∆(β, β∗ ) discussed in Section 2. [sent-197, score-0.572]
</p><p>60 Since they are based on the GIF (14) in the intersection of the cone and the unit ball {b : φ0 (b) ≤ 1/e}, they are different from typical results in a small-ball analysis based on the Taylor expansion of ψ(β) at β = β∗ . [sent-198, score-0.122]
</p><p>61 An important feature of Theorem 4 is that its regularity condition is imposed only on the GIF (14) evaluated at the target β∗ ; The uniformity of the order of ∆(β + b, β) in β is not required. [sent-199, score-0.137]
</p><p>62 1 The Hessian and Related Quantities In this subsection we describe the relationship between the GIF (14) and the Hessian of the convex function ψ(·) in (1) and examine cases where the quasi star-shaped functions φ0 (·) and φ(·) are ¨ familiar seminorms. [sent-202, score-0.231]
</p><p>63 Deﬁnition 6 Given a nonnegative-deﬁnite matrix Σ and constant η∗ > 0, the symmetriized Bregman divergence ∆(β, β∗ ) satisﬁes the φ0 -relaxed convexity (φ0 -RC) condition if ∆(β∗ + b, β∗ )eφ0 (b) ≥ b, Σb , ∀ b ∈ C (ξ, S), φ0 (b) ≤ η∗ . [sent-206, score-0.201]
</p><p>64 (18)  The φ0 -RC condition is related to the restricted strong convexity condition for the Bregman di˙ vergence (Negahban et al. [sent-207, score-0.171]
</p><p>65 It actually implies the restricted strong convexity of the ∗ symmetrized Bregman divergence with κ = e−η and loss b ∗ = b, Σb 1/2 . [sent-209, score-0.408]
</p><p>66 In fact, in our examples, we ﬁnd quasi star-shaped functions φ0 for which (18) holds for unrestricted b (η∗ = ξ = ∞). [sent-211, score-0.181]
</p><p>67 In such ¨ ¨ cases, the φ0 -RC condition is a smoothness condition on the Hessian operator ψ(β) = ℓ(β), since ∗ + h, β∗ ) = 1 h, ψ(β∗ + th)h dt by (4). [sent-212, score-0.145]
</p><p>68 b∈C (ξ,S) |bS |1 φ(b) inf  (19)  In linear regression, F0 (ξ, S; φ) is the square of the compatibility factor for φ(b) = φ1,S (b) = |bS |1 /|S| (van de Geer, 2007) and the weak cone invertibility factor for φ(b) = φq (b) = |b|q /|S|1/q (Ye and Zhang, 2010). [sent-215, score-0.358]
</p><p>69 They are both closely related to the restricted isometry property (RIP) (Candes and Tao, 2005), the sparse Rieze condition (SRC) (Zhang and Huang, 2008), and the restricted eigenvalue (Bickel et al. [sent-216, score-0.123]
</p><p>70 The following corollary u is an extension of an oracle inequality of Ye and Zhang (2010) from linear regression to the general convex minimization problem (1). [sent-220, score-0.316]
</p><p>71 Then, in the event Ω0 ∩ |wS |∞ λ + z∗ ≤ min ξ(λ − z∗ ), ηe−η F0 (ξ, S; φ0 ) 0 1  ,  the oracle inequalities (16) and (17) in Theorem 4 hold with the GIF F(ξ, S; φ0 , φ) replaced by the simple GIF F0 (ξ, S; φ) in (19). [sent-223, score-0.32]
</p><p>72 1 F0 (ξ, S; φ1,S ) 1847  H UANG AND Z HANG  Here the only differences between the general model (1) and linear regression (φ0 (b) = 0) are the extra factor eη with η ≤ 1, the extra constraint |wS |∞ λ + z∗ ≤ ηe−η F0 (ξ, S; φ0 ), and the extra φ0 0 RC condition (18). [sent-225, score-0.121]
</p><p>73 For ψ(β) = |Xb|2 /(2n) and Σ = X ′ X/n, 2 F0 (ξ, S; φq ) is the weak cone invertibility factor for q ∈ [1, ∞] (Ye and Zhang, 2010), where a sharper version is deﬁned as the sign restricted invertibility factor (SCIF): SCIFq (ξ, S) =  inf  b∈C− (ξ,S)  |Σb|∞ /φq (b), φq = |b|q /|S|1/q . [sent-228, score-0.491]
</p><p>74 1/2  For q = 1, F0 (ξ, S; φ1,S ) is the compatibility constant (van de Geer, 2007) κ∗ (ξ, S) =  |S|1/2 |Xb|2 b′ Σb = inf b∈C (ξ,S) |bS |1 n1/2 b∈C (ξ,S) |bS |2 /|S| 1  1/2  inf  . [sent-229, score-0.213]
</p><p>75 (20)  They are all closely related to the ℓ2 restricted eigenvalues RE2 (ξ, S) =  b′ Σb |Xb|2 = inf b∈C (ξ,S) |b|2 b∈C (ξ,S) |b|2 n1/2 2  1/2  inf  (Bickel et al. [sent-230, score-0.165]
</p><p>76 Thus, cone and general invertibility factors 0 1 yield sharper ℓ2 oracle inequalities. [sent-234, score-0.46]
</p><p>77 The factors in the oracle inequalities in (21) do not always have the same order for large |S|. [sent-235, score-0.267]
</p><p>78 Although the oracle inequality based on SCIF2 (ξ, S) is the sharpest among them, it seems not to lead to a simple extension to the general convex minimization in (1). [sent-236, score-0.284]
</p><p>79 Thus, we settle with extensions of the second sharpest oracle inequality in (21) with F0 (ξ, S; ·). [sent-237, score-0.193]
</p><p>80 2, where we set up the notation in (13) and gave the KL divergence interpretation to (4). [sent-241, score-0.118]
</p><p>81 A crucial condition in our analysis of the Lasso in GLM is the Lipschitz condition ¨ ¨ max log ψ0 (xi β∗ + t) − log ψ0 (xi β∗ ) i≤n  ≤ M1 |t|, ∀M1 |t| ≤ η∗ ,  (24)  where M1 and η∗ are constants determined by ψ0 . [sent-246, score-0.17]
</p><p>82 Thus, for φ0 (b) = M2 |b|2 and seminorms φ, this lower bound is F ∗ (ξ, S; φ) =  ¨ M2 ψ0 (xi β∗ ) |xi b| (xi b)2 min , , ∑ M1 M2 b∈C (ξ,S),|b|2 =1 i=1 n|bS |1 φ(b) n  inf  (25)  due to (xi b)2 01 I{tM1 |xi b| ≤ M2 }dt = M2 min{|xi b|/M1 , (xi b)2 /M2 }. [sent-249, score-0.198]
</p><p>83 (28)  Under the Lipschitz condition (24), we may also use the following large deviation inequalities to ﬁnd explicit penalty levels to guarantee the noise bound (15). [sent-253, score-0.281]
</p><p>84 Theorem 9 (i) Let β be the weighted Lasso estimator in (2) with GLM loss function in (22). [sent-262, score-0.162]
</p><p>85 Moreover, if either (29) or (30) holds for the penalty level {λ0 , λ1 } and the weight bounds w j in (6) are deterministic, then Pβ∗ (32) holds for all seminorms φ ≥ Pβ∗ (Ω0 ) − ε0 . [sent-269, score-0.245]
</p><p>86 ∗ Remark 10 If either (29) or (30) holds for the penalty levels {λ0 , λ1 } and the bounds w j in (6) are deterministic, then (32) implies Pβ∗ {the noise bound (15) holds} ≥ Pβ∗ (Ω0 ) − ε0 . [sent-274, score-0.154]
</p><p>87 Then, (29) holds with the penalty level λ0 = λ1 = 1/2 aσ (2/n) log(p/ε0 ) for certain a ≤ (1 + o(1)) max j (Σ∗ ) j j /w j , due to max{λ0 , η, η0 } → 0+. [sent-276, score-0.115]
</p><p>88 Again, the conditions and conclusions of Theorem 9 “converge” to those for the linear regression as if the Gram matrix is Σ∗ . [sent-277, score-0.11]
</p><p>89 For Σ = Σ∗ and M1 = M2 ≤ M3 /(1 + ξ), κ2 (ξ, S)/(M3 |S|) ≤ min F∗ (ξ, S), F ∗ (ξ, S; M2 | · |2 ) , ∗ since n−1 ∑n ψ0 (xi β∗ )|xi b|3 / b, Σ∗ b ≤ |Xb|∞ ≤ |bS |1 M3 /M1 as in the derivation of (28) and |b|2 ≤ i=1 ¨ |b|1 ≤ (1 + ξ)|bS |1 in the cone (9). [sent-279, score-0.122]
</p><p>90 The sharper Theorem 9 (i) and (ii) provides conditions to relax the requirement to a small |S|(log p)/n. [sent-281, score-0.135]
</p><p>91 (2010) considered M-estimators under the restricted strong convexity condition discussed below Deﬁnition 6. [sent-283, score-0.118]
</p><p>92 For the GLM, they considered iid subGaussian xi and used empirical process theory to bound the ratio ∆(β∗ + b, β∗ )/{|b|2 (|b|2 − c0 |b|1 } from below over the cone (9) with a small c0 . [sent-284, score-0.291]
</p><p>93 (2010), for iid sub-Gaussian xi , empirical process theory can be applied to the lower bound (25) for the GIF to verify the key condition (31) with F ∗ (ξ, S; M2 | · |2 ) |S|−1/2 , provided that |S|(log p)/n is small. [sent-289, score-0.222]
</p><p>94 Example 7 (Linear regression: oracle inequalities, continuation) For the linear regression model (10) with quadratic loss, ψ0 (θ) = θ2 /2, so that (24) holds with M1 = 0 and η∗ = ∞. [sent-290, score-0.261]
</p><p>95 Thus, the conditions and conclusions of Theorem 9 “converge” to the case of linear regression as M1 → 0+. [sent-293, score-0.11]
</p><p>96 Example 8 (Logistic regression: oracle inequalities) The model and loss function are given in (11) and (12) respectively. [sent-300, score-0.195]
</p><p>97 For such deterministic W and X, an adaptive choice of the penalty level is λ = σ (2/n) log p with σ2 = ∑n πi (β){1 − πi (β)}/n, where πi (β) is as in Example 2. [sent-305, score-0.194]
</p><p>98 i=1 Example 9 (Log-linear models: oracle inequalities) Consider counting data with yi ∈ {0, 1, 2, . [sent-306, score-0.224]
</p><p>99 Adaptive and Multistage Methods We consider in this section an adaptive Lasso and its repeated applications, with weights recursively generated from a concave penalty function. [sent-316, score-0.249]
</p><p>100 Let ρλ (t) be a concave penalty function with ρλ (0+) = λ, where ρλ (t) = (∂/∂t)ρλ (t). [sent-320, score-0.153]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lasso', 0.388), ('bs', 0.317), ('gif', 0.271), ('ws', 0.25), ('glm', 0.221), ('symmetrized', 0.187), ('bregman', 0.183), ('oracle', 0.157), ('quasi', 0.145), ('bsolute', 0.141), ('enalized', 0.141), ('cone', 0.122), ('divergence', 0.118), ('inequalities', 0.11), ('uang', 0.109), ('multistage', 0.109), ('inimization', 0.109), ('ye', 0.103), ('hang', 0.103), ('geer', 0.103), ('wsc', 0.1), ('penalized', 0.096), ('seminorms', 0.094), ('sharper', 0.093), ('invertibility', 0.088), ('onvex', 0.088), ('xi', 0.088), ('kkt', 0.085), ('zhang', 0.084), ('xb', 0.084), ('compatibility', 0.083), ('penalty', 0.079), ('concave', 0.074), ('symmetrised', 0.07), ('regression', 0.068), ('yi', 0.067), ('negahban', 0.067), ('weighted', 0.067), ('hs', 0.066), ('bickel', 0.065), ('jk', 0.065), ('inf', 0.065), ('hlmann', 0.061), ('hessian', 0.059), ('sc', 0.059), ('estimator', 0.057), ('huang', 0.056), ('jian', 0.054), ('kl', 0.054), ('convex', 0.054), ('zou', 0.053), ('condition', 0.053), ('event', 0.053), ('adaptive', 0.052), ('bsc', 0.047), ('nock', 0.047), ('xsc', 0.047), ('regularity', 0.047), ('weights', 0.044), ('conditions', 0.042), ('xs', 0.042), ('iid', 0.042), ('iowa', 0.04), ('greenshtein', 0.04), ('hsc', 0.04), ('maxt', 0.04), ('unequal', 0.04), ('dt', 0.039), ('bound', 0.039), ('graphical', 0.038), ('loss', 0.038), ('target', 0.037), ('minimization', 0.037), ('neighborhood', 0.036), ('holds', 0.036), ('sharpest', 0.036), ('van', 0.036), ('lipschitz', 0.036), ('restricted', 0.035), ('remark', 0.034), ('suppose', 0.034), ('irrepresentable', 0.033), ('nielsen', 0.033), ('tb', 0.033), ('ii', 0.033), ('log', 0.032), ('remarks', 0.032), ('subsection', 0.032), ('estimators', 0.032), ('logistic', 0.032), ('ritov', 0.031), ('deterministic', 0.031), ('predictors', 0.031), ('convexity', 0.03), ('logit', 0.029), ('candes', 0.029), ('concavity', 0.029), ('recursive', 0.029), ('theorem', 0.028), ('rutgers', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="39-tfidf-1" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>Author: Jian Huang, Cun-Hui Zhang</p><p>Abstract: The ℓ1 -penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ1 -penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1 -penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results. Keywords: variable selection, penalized estimation, oracle inequality, generalized linear models, selection consistency, sparsity</p><p>2 0.17344403 <a title="39-tfidf-2" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>Author: Christopher R. Genovese, Jiashun Jin, Larry Wasserman, Zhigang Yao</p><p>Abstract: The lasso is an important method for sparse, high-dimensional regression problems, with efﬁcient algorithms available, a long history of practical success, and a large body of theoretical results supporting and explaining its performance. But even with the best available algorithms, ﬁnding the lasso solutions remains a computationally challenging task in cases where the number of covariates vastly exceeds the number of data points. Marginal regression, where each dependent variable is regressed separately on each covariate, offers a promising alternative in this case because the estimates can be computed roughly two orders faster than the lasso solutions. The question that remains is how the statistical performance of the method compares to that of the lasso in these cases. In this paper, we study the relative statistical performance of the lasso and marginal regression for sparse, high-dimensional regression problems. We consider the problem of learning which coefﬁcients are non-zero. Our main results are as follows: (i) we compare the conditions under which the lasso and marginal regression guarantee exact recovery in the ﬁxed design, noise free case; (ii) we establish conditions under which marginal regression provides exact recovery with high probability in the ﬁxed design, noise free, random coefﬁcients case; and (iii) we derive rates of convergence for both procedures, where performance is measured by the number of coefﬁcients with incorrect sign, and characterize the regions in the parameter space recovery is and is not possible under this metric. In light of the computational advantages of marginal regression in very high dimensional problems, our theoretical and simulations results suggest that the procedure merits further study. Keywords: high-dimensional regression, lasso, phase diagram, regularization</p><p>3 0.15755774 <a title="39-tfidf-3" href="./jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</a></p>
<p>Author: Ji Liu, Peter Wonka, Jieping Ye</p><p>Abstract: We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ Rn×m (m ≫ n) and a noisy observation vector y ∈ Rn satisfying y = Xβ∗ + ε where ε is the noise vector following a Gaussian distribution N(0, σ2 I), how to recover the signal (or parameter vector) β∗ when the signal is sparse? The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively reﬁnes the target signal β∗ . We show that if X obeys a certain condition, then with a large probability ˆ the difference between the solution β estimated by the proposed method and the true solution β∗ measured in terms of the ℓ p norm (p ≥ 1) is bounded as ˆ β − β∗ p ≤ C(s − N)1/p log m + ∆ σ, where C is a constant, s is the number of nonzero entries in β∗ , the risk of the oracle estimator ∆ is independent of m and is much smaller than the ﬁrst term, and N is the number of entries of β∗ larger √ than a certain value in the order of O (σ log m). The proposed √ method improves the estimation √ bound of the standard Dantzig selector approximately from Cs1/p log mσ to C(s − N)1/p log mσ where the value N depends on the number of large entries in β∗ . When N = s, the proposed algorithm achieves the oracle solution with a high probability, where the oracle solution is the projection of the observation vector y onto true features. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector. Finally, we extend this multi-stage procedure to the LASSO case. Keywords: multi-stage, Dantzig selector, LASSO, sparse signal recovery</p><p>4 0.13183226 <a title="39-tfidf-4" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>Author: Sham M. Kakade, Shai Shalev-Shwartz, Ambuj Tewari</p><p>Abstract: There is growing body of learning problems for which it is natural to organize the parameters into a matrix. As a result, it becomes easy to impose sophisticated prior knowledge by appropriately regularizing the parameters under some matrix norm. This work describes and analyzes a systematic method for constructing such matrix-based regularization techniques. In particular, we focus on how the underlying statistical properties of a given problem can help us decide which regularization function is appropriate. Our methodology is based on a known duality phenomenon: a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. We demonstrate the potential of this framework by deriving novel generalization and regret bounds for multi-task learning, multi-class learning, and multiple kernel learning. Keywords: regularization, strong convexity, regret bounds, generalization bounds, multi-task learning, multi-class learning, multiple kernel learning</p><p>5 0.1314522 <a title="39-tfidf-5" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>Author: Andreas Maurer, Massimiliano Pontil</p><p>Abstract: We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an inﬁnite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels. Keywords: empirical processes, Rademacher average, sparse estimation.</p><p>6 0.13018051 <a title="39-tfidf-6" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>7 0.097818851 <a title="39-tfidf-7" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>8 0.093859904 <a title="39-tfidf-8" href="./jmlr-2012-EP-GIG_Priors_and_Applications_in_Bayesian_Sparse_Learning.html">35 jmlr-2012-EP-GIG Priors and Applications in Bayesian Sparse Learning</a></p>
<p>9 0.087644152 <a title="39-tfidf-9" href="./jmlr-2012-The_huge_Package_for_High-dimensional_Undirected_Graph_Estimation_in_R.html">113 jmlr-2012-The huge Package for High-dimensional Undirected Graph Estimation in R</a></p>
<p>10 0.080521472 <a title="39-tfidf-10" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>11 0.07155101 <a title="39-tfidf-11" href="./jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</a></p>
<p>12 0.062549978 <a title="39-tfidf-12" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>13 0.061401904 <a title="39-tfidf-13" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<p>14 0.061190743 <a title="39-tfidf-14" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>15 0.060483392 <a title="39-tfidf-15" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>16 0.053458367 <a title="39-tfidf-16" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>17 0.053289581 <a title="39-tfidf-17" href="./jmlr-2012-Learning_Linear_Cyclic_Causal_Models_with_Latent_Variables.html">56 jmlr-2012-Learning Linear Cyclic Causal Models with Latent Variables</a></p>
<p>18 0.052258749 <a title="39-tfidf-18" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<p>19 0.051490612 <a title="39-tfidf-19" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>20 0.051420923 <a title="39-tfidf-20" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.263), (1, 0.127), (2, -0.201), (3, -0.028), (4, -0.054), (5, 0.288), (6, -0.068), (7, -0.225), (8, -0.094), (9, 0.03), (10, 0.13), (11, -0.043), (12, 0.029), (13, 0.004), (14, 0.154), (15, -0.09), (16, 0.015), (17, 0.141), (18, -0.095), (19, -0.052), (20, -0.056), (21, 0.117), (22, 0.139), (23, -0.122), (24, 0.1), (25, -0.073), (26, 0.012), (27, 0.074), (28, -0.039), (29, -0.067), (30, -0.047), (31, -0.005), (32, 0.143), (33, 0.003), (34, -0.017), (35, -0.01), (36, 0.002), (37, -0.006), (38, 0.029), (39, 0.081), (40, -0.06), (41, -0.012), (42, 0.005), (43, 0.001), (44, 0.038), (45, 0.04), (46, -0.011), (47, -0.069), (48, -0.084), (49, -0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.949992 <a title="39-lsi-1" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>Author: Jian Huang, Cun-Hui Zhang</p><p>Abstract: The ℓ1 -penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ1 -penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1 -penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results. Keywords: variable selection, penalized estimation, oracle inequality, generalized linear models, selection consistency, sparsity</p><p>2 0.81018627 <a title="39-lsi-2" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>Author: Christopher R. Genovese, Jiashun Jin, Larry Wasserman, Zhigang Yao</p><p>Abstract: The lasso is an important method for sparse, high-dimensional regression problems, with efﬁcient algorithms available, a long history of practical success, and a large body of theoretical results supporting and explaining its performance. But even with the best available algorithms, ﬁnding the lasso solutions remains a computationally challenging task in cases where the number of covariates vastly exceeds the number of data points. Marginal regression, where each dependent variable is regressed separately on each covariate, offers a promising alternative in this case because the estimates can be computed roughly two orders faster than the lasso solutions. The question that remains is how the statistical performance of the method compares to that of the lasso in these cases. In this paper, we study the relative statistical performance of the lasso and marginal regression for sparse, high-dimensional regression problems. We consider the problem of learning which coefﬁcients are non-zero. Our main results are as follows: (i) we compare the conditions under which the lasso and marginal regression guarantee exact recovery in the ﬁxed design, noise free case; (ii) we establish conditions under which marginal regression provides exact recovery with high probability in the ﬁxed design, noise free, random coefﬁcients case; and (iii) we derive rates of convergence for both procedures, where performance is measured by the number of coefﬁcients with incorrect sign, and characterize the regions in the parameter space recovery is and is not possible under this metric. In light of the computational advantages of marginal regression in very high dimensional problems, our theoretical and simulations results suggest that the procedure merits further study. Keywords: high-dimensional regression, lasso, phase diagram, regularization</p><p>3 0.60213566 <a title="39-lsi-3" href="./jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</a></p>
<p>Author: Ji Liu, Peter Wonka, Jieping Ye</p><p>Abstract: We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ Rn×m (m ≫ n) and a noisy observation vector y ∈ Rn satisfying y = Xβ∗ + ε where ε is the noise vector following a Gaussian distribution N(0, σ2 I), how to recover the signal (or parameter vector) β∗ when the signal is sparse? The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively reﬁnes the target signal β∗ . We show that if X obeys a certain condition, then with a large probability ˆ the difference between the solution β estimated by the proposed method and the true solution β∗ measured in terms of the ℓ p norm (p ≥ 1) is bounded as ˆ β − β∗ p ≤ C(s − N)1/p log m + ∆ σ, where C is a constant, s is the number of nonzero entries in β∗ , the risk of the oracle estimator ∆ is independent of m and is much smaller than the ﬁrst term, and N is the number of entries of β∗ larger √ than a certain value in the order of O (σ log m). The proposed √ method improves the estimation √ bound of the standard Dantzig selector approximately from Cs1/p log mσ to C(s − N)1/p log mσ where the value N depends on the number of large entries in β∗ . When N = s, the proposed algorithm achieves the oracle solution with a high probability, where the oracle solution is the projection of the observation vector y onto true features. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector. Finally, we extend this multi-stage procedure to the LASSO case. Keywords: multi-stage, Dantzig selector, LASSO, sparse signal recovery</p><p>4 0.57514906 <a title="39-lsi-4" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>Author: Andreas Maurer, Massimiliano Pontil</p><p>Abstract: We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an inﬁnite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels. Keywords: empirical processes, Rademacher average, sparse estimation.</p><p>5 0.56088066 <a title="39-lsi-5" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>Author: Lan Xue, Annie Qu</p><p>Abstract: The varying-coefﬁcient model is ﬂexible and powerful for modeling the dynamic changes of regression coefﬁcients. It is important to identify signiﬁcant covariates associated with response variables, especially for high-dimensional settings where the number of covariates can be larger than the sample size. We consider model selection in the high-dimensional setting and adopt difference convex programming to approximate the L0 penalty, and we investigate the global optimality properties of the varying-coefﬁcient estimator. The challenge of the variable selection problem here is that the dimension of the nonparametric form for the varying-coefﬁcient modeling could be inﬁnite, in addition to dealing with the high-dimensional linear covariates. We show that the proposed varying-coefﬁcient estimator is consistent, enjoys the oracle property and achieves an optimal convergence rate for the non-zero nonparametric components for high-dimensional data. Our simulations and numerical examples indicate that the difference convex algorithm is efﬁcient using the coordinate decent algorithm, and is able to select the true model at a higher frequency than the least absolute shrinkage and selection operator (LASSO), the adaptive LASSO and the smoothly clipped absolute deviation (SCAD) approaches. Keywords: coordinate decent algorithm, difference convex programming, L0 - regularization, large-p small-n, model selection, nonparametric function, oracle property, truncated L1 penalty</p><p>6 0.38963246 <a title="39-lsi-6" href="./jmlr-2012-EP-GIG_Priors_and_Applications_in_Bayesian_Sparse_Learning.html">35 jmlr-2012-EP-GIG Priors and Applications in Bayesian Sparse Learning</a></p>
<p>7 0.37469384 <a title="39-lsi-7" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>8 0.37225315 <a title="39-lsi-8" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>9 0.36972657 <a title="39-lsi-9" href="./jmlr-2012-The_huge_Package_for_High-dimensional_Undirected_Graph_Estimation_in_R.html">113 jmlr-2012-The huge Package for High-dimensional Undirected Graph Estimation in R</a></p>
<p>10 0.34749344 <a title="39-lsi-10" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<p>11 0.2989898 <a title="39-lsi-11" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>12 0.2962648 <a title="39-lsi-12" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>13 0.27391621 <a title="39-lsi-13" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<p>14 0.2665424 <a title="39-lsi-14" href="./jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</a></p>
<p>15 0.25419632 <a title="39-lsi-15" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>16 0.25380707 <a title="39-lsi-16" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>17 0.25375876 <a title="39-lsi-17" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>18 0.24864106 <a title="39-lsi-18" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>19 0.2385156 <a title="39-lsi-19" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>20 0.23540327 <a title="39-lsi-20" href="./jmlr-2012-On_the_Convergence_Rate_oflp-Norm_Multiple_Kernel_Learning.html">81 jmlr-2012-On the Convergence Rate oflp-Norm Multiple Kernel Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.018), (21, 0.569), (26, 0.032), (29, 0.026), (49, 0.019), (56, 0.012), (75, 0.029), (92, 0.081), (96, 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98058403 <a title="39-lda-1" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>Author: Félix-Antoine Fortin, François-Michel De Rainville, Marc-André Gardner, Marc Parizeau, Christian Gagné</p><p>Abstract: DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black-box frameworks. Freely available with extensive documentation at http://deap.gel.ulaval.ca, DEAP is an open source project under an LGPL license. Keywords: distributed evolutionary algorithms, software tools</p><p>same-paper 2 0.90704006 <a title="39-lda-2" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>Author: Jian Huang, Cun-Hui Zhang</p><p>Abstract: The ℓ1 -penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ1 -penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1 -penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results. Keywords: variable selection, penalized estimation, oracle inequality, generalized linear models, selection consistency, sparsity</p><p>3 0.86348903 <a title="39-lda-3" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>Author: Gérard Biau</p><p>Abstract: Random forests are a scheme proposed by Leo Breiman in the 2000’s for building a predictor ensemble with a set of decision trees that grow in randomly selected subspaces of data. Despite growing interest and practical use, there has been little exploration of the statistical properties of random forests, and little is known about the mathematical forces driving the algorithm. In this paper, we offer an in-depth analysis of a random forests model suggested by Breiman (2004), which is very close to the original algorithm. We show in particular that the procedure is consistent and adapts to sparsity, in the sense that its rate of convergence depends only on the number of strong features and not on how many noise variables are present. Keywords: random forests, randomization, sparsity, dimension reduction, consistency, rate of convergence</p><p>4 0.52322704 <a title="39-lda-4" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>Author: Lan Xue, Annie Qu</p><p>Abstract: The varying-coefﬁcient model is ﬂexible and powerful for modeling the dynamic changes of regression coefﬁcients. It is important to identify signiﬁcant covariates associated with response variables, especially for high-dimensional settings where the number of covariates can be larger than the sample size. We consider model selection in the high-dimensional setting and adopt difference convex programming to approximate the L0 penalty, and we investigate the global optimality properties of the varying-coefﬁcient estimator. The challenge of the variable selection problem here is that the dimension of the nonparametric form for the varying-coefﬁcient modeling could be inﬁnite, in addition to dealing with the high-dimensional linear covariates. We show that the proposed varying-coefﬁcient estimator is consistent, enjoys the oracle property and achieves an optimal convergence rate for the non-zero nonparametric components for high-dimensional data. Our simulations and numerical examples indicate that the difference convex algorithm is efﬁcient using the coordinate decent algorithm, and is able to select the true model at a higher frequency than the least absolute shrinkage and selection operator (LASSO), the adaptive LASSO and the smoothly clipped absolute deviation (SCAD) approaches. Keywords: coordinate decent algorithm, difference convex programming, L0 - regularization, large-p small-n, model selection, nonparametric function, oracle property, truncated L1 penalty</p><p>5 0.48467755 <a title="39-lda-5" href="./jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</a></p>
<p>Author: Ji Liu, Peter Wonka, Jieping Ye</p><p>Abstract: We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ Rn×m (m ≫ n) and a noisy observation vector y ∈ Rn satisfying y = Xβ∗ + ε where ε is the noise vector following a Gaussian distribution N(0, σ2 I), how to recover the signal (or parameter vector) β∗ when the signal is sparse? The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively reﬁnes the target signal β∗ . We show that if X obeys a certain condition, then with a large probability ˆ the difference between the solution β estimated by the proposed method and the true solution β∗ measured in terms of the ℓ p norm (p ≥ 1) is bounded as ˆ β − β∗ p ≤ C(s − N)1/p log m + ∆ σ, where C is a constant, s is the number of nonzero entries in β∗ , the risk of the oracle estimator ∆ is independent of m and is much smaller than the ﬁrst term, and N is the number of entries of β∗ larger √ than a certain value in the order of O (σ log m). The proposed √ method improves the estimation √ bound of the standard Dantzig selector approximately from Cs1/p log mσ to C(s − N)1/p log mσ where the value N depends on the number of large entries in β∗ . When N = s, the proposed algorithm achieves the oracle solution with a high probability, where the oracle solution is the projection of the observation vector y onto true features. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector. Finally, we extend this multi-stage procedure to the LASSO case. Keywords: multi-stage, Dantzig selector, LASSO, sparse signal recovery</p><p>6 0.46900243 <a title="39-lda-6" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>7 0.46527028 <a title="39-lda-7" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>8 0.46426308 <a title="39-lda-8" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>9 0.43181449 <a title="39-lda-9" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>10 0.42294392 <a title="39-lda-10" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>11 0.41436207 <a title="39-lda-11" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>12 0.40318441 <a title="39-lda-12" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>13 0.39686695 <a title="39-lda-13" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>14 0.38624379 <a title="39-lda-14" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>15 0.38339037 <a title="39-lda-15" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>16 0.38319528 <a title="39-lda-16" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>17 0.37880161 <a title="39-lda-17" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>18 0.37572271 <a title="39-lda-18" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>19 0.36697701 <a title="39-lda-19" href="./jmlr-2012-Refinement_of_Operator-valued_Reproducing_Kernels.html">96 jmlr-2012-Refinement of Operator-valued Reproducing Kernels</a></p>
<p>20 0.3659631 <a title="39-lda-20" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
