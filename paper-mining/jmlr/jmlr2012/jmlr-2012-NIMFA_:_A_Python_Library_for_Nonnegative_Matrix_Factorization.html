<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-75" href="#">jmlr2012-75</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</h1>
<br/><p>Source: <a title="jmlr-2012-75-pdf" href="http://jmlr.org/papers/volume13/zitnik12a/zitnik12a.pdf">pdf</a></p><p>Author: Marinka Žitnik, Blaž Zupan</p><p>Abstract: NIMFA is an open-source Python library that provides a uniﬁed interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA’s component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks. Keywords: nonnegative matrix factorization, initialization methods, quality measures, scripting, Python</p><p>Reference: <a title="jmlr-2012-75-reference" href="../jmlr2012_reference/jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 SI  Faculty of Computer and Information Science University of Ljubljana SI-1000 Ljubljana, Trˇ aˇka 25, Slovenia z s  Editor: Mikio Braun  Abstract NIMFA is an open-source Python library that provides a uniﬁed interface to nonnegative matrix factorization algorithms. [sent-7, score-0.604]
</p><p>2 It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. [sent-8, score-0.535]
</p><p>3 NIMFA’s component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks. [sent-10, score-0.432]
</p><p>4 Keywords: nonnegative matrix factorization, initialization methods, quality measures, scripting, Python  1. [sent-11, score-0.332]
</p><p>5 Introduction As a method to learn parts-based representation, a nonnegative matrix factorization (NMF) has become a popular approach for gaining new insights about complex latent relationships in highdimensional data through feature construction, selection and clustering. [sent-12, score-0.564]
</p><p>6 NMF’s distinguishing feature is imposition of nonnegativity constraints, where only non-subtractive combinations of vectors in original space are allowed (Lee and Seung, 1999, 2001). [sent-15, score-0.047]
</p><p>7 We have developed a Python-based NMF library called NIMFA which implements a wide variety of useful NMF operations and its components at a granular level. [sent-17, score-0.107]
</p><p>8 Our aim was both to provide access to already published variants of NMF and ease the innovative use of its components in crafting new algorithms. [sent-18, score-0.025]
</p><p>9 The library intentionally focuses on nonnegative variant of matrix factorization, and in terms of variety of different approaches compares favourably to several popular matrix factorization packages that are broader in scope (PyMF, (http://code. [sent-19, score-0.682]
</p><p>10 Symbol + denotes full support, (+) partial support and symbol − no support. [sent-30, score-0.03]
</p><p>11 Supported Factorization Methods and Approaches In a standard model of NMF (Lee and Seung, 2001), a data matrix V is factorized to V ≡ W H by solving a related optimization problem. [sent-33, score-0.083]
</p><p>12 Nonnegative matrices W and H are commonly referred to as basis and mixture matrix, respectively. [sent-34, score-0.029]
</p><p>13 NIMFA implements an originally proposed optimization (Lee and Seung, 2001; Brunet et al. [sent-35, score-0.039]
</p><p>14 , 2004) with Euclidean or Kullback-Leibler cost function, along with Frobenius, divergence or connectivity costs. [sent-36, score-0.029]
</p><p>15 It also supports alternative optimization algorithms including Bayesian NMF Gibbs sampler (Schmidt et al. [sent-37, score-0.03]
</p><p>16 , 2008) and alternating least squares NMF using projected gradient method for subproblems (Lin, 2007). [sent-40, score-0.151]
</p><p>17 Sparse matrix factorization is provided either through probabilistic (Dueck and Frey, 2004) or alternating nonnegativityconstrained least squares factorization (Kim and Park, 2007). [sent-41, score-0.851]
</p><p>18 These comprise nonsmooth factorization V ≡ W S(θ) H (Pascual-Montano et al. [sent-48, score-0.39]
</p><p>19 , 2006) and multiple model factorization for simultaneous treatment of several input matrices and their factorization with the same basis matrix W (Zhang et al. [sent-49, score-0.816]
</p><p>20 All mentioned optimizations are incremental and start with initial approximation of matrices W and H. [sent-51, score-0.029]
</p><p>21 Appropriate choice of initialization can greatly speed-up the convergence and increase the overall quality of the factorization results. [sent-52, score-0.51]
</p><p>22 NIMFA contains implementations of popular initialization methods such as nonnegative double singular value decomposition (Boutsidis and Gallopoulos, 2007), random C and random Vcol algorithms (Albright et al. [sent-53, score-0.288]
</p><p>23 User can also completely 850  NIMFA : A P YTHON L IBRARY FOR N ONNEGATIVE M ATRIX FACTORIZATION  specify initial factorization by passing ﬁxed factors or choose any inexpensive method of randomly populated factors. [sent-55, score-0.386]
</p><p>24 Factorization rank, choice of optimization method, and method-speciﬁc parameters jointly deﬁne the quality of approximation of input matrix V with the factorized system. [sent-56, score-0.13]
</p><p>25 NIMFA provides a number of quality measures ranging from standard ones (e. [sent-57, score-0.047]
</p><p>26 , Euclidean distance, Kullback-Leibler divergence, and sparseness) to those more speciﬁc like feature scoring representing speciﬁcity to basis vectors (Kim and Park, 2007). [sent-59, score-0.027]
</p><p>27 Design and Implementation NIMFA has hierarchical, modular, and scalable structure which allows uniform treatment of numerous factorization models, their corresponding factorization algorithms and initialization methods. [sent-61, score-0.82]
</p><p>28 The library enables easy integration into user’s code and arbitrary combinations of its factorization algorithms and their components. [sent-62, score-0.425]
</p><p>29 seeding), supporting models for factorization, ﬁtted results, tracking and computation of quality and performance measures (nimfa. [sent-67, score-0.115]
</p><p>30 models), and linear algebra helper routines for sparse and dense matrices (nimfa. [sent-68, score-0.077]
</p><p>31 The library provides access to a set of standard data sets (nimfa. [sent-70, score-0.068]
</p><p>32 examples stores scripts that demonstrate factorization-based analysis of these data sets and provide examples for various analytic approaches like factorization of sparse matrices, multiple factorization runs, and others. [sent-73, score-0.737]
</p><p>33 Every block of the algorithms, like data preprocessing, initialization of matrix factors, overall optimization, stopping criteria and quality scoring may be selected from the library or deﬁned in a user-script, thus seamlessly enabling experimentation and construction of new approaches. [sent-75, score-0.323]
</p><p>34 Optimization process may be monitored, tracking residuals across iterations or tracking ﬁtted factorization model. [sent-76, score-0.534]
</p><p>35 NIMFA uses a popular Python matrix computation package NumPy for data management and representation. [sent-77, score-0.109]
</p><p>36 A drawback of the library is that is holds matrix factors and ﬁtted model in main memory, raising an issue with very large data sets. [sent-78, score-0.172]
</p><p>37 To address this, NIMFA fully supports computations with sparse matrices as implemented in SciPy. [sent-79, score-0.082]
</p><p>38 An Example Script The sample script below demonstrates factorization of medulloblastoma gene expression data using alternating least squares NMF with projected gradient method for subproblems (Lin, 2007) and Random Vcol (Albright et al. [sent-81, score-0.627]
</p><p>39 mf run is ﬁtted factorization model through which user can access matrix factors and estimate quality measures. [sent-84, score-0.483]
</p><p>40 mf (V , seed = ’ random_vcol ’, method = ’ lsnmf ’, rank =40 , max_iter =65) fctr_res = nimfa . [sent-89, score-0.692]
</p><p>41 evar ())  851  ˇ Z ITNIK AND Z UPAN  print ’K -L divergence : %5. [sent-96, score-0.19]
</p><p>42 distance ( metric = ’kl ’) print ’ Sparseness , W: %5. [sent-98, score-0.074]
</p><p>43 sparseness ()  Running this script produces the following output, where slight differences in reported scores across different runs can be attributed to randomness of the Random Vcol initialization method. [sent-102, score-0.232]
</p><p>44 Availability and Requirements NIMFA is a Python-based package requiring SciPy version 0. [sent-109, score-0.031]
</p><p>45 The latest version with documentation and working examples can be found at http://nimfa. [sent-113, score-0.027]
</p><p>46 Algorithms, initializations, and convergence for the nonnegative matrix factorization. [sent-120, score-0.179]
</p><p>47 SVD-based initialization: A head start for nonnegative matrix factorization. [sent-123, score-0.179]
</p><p>48 Sparse non-negative matrix factorizations via alternating nonnegativity-constrained least squares for microarray data analysis. [sent-139, score-0.173]
</p><p>49 Learning the parts of objects by non-negative matrix factorization. [sent-152, score-0.05]
</p><p>50 A novel computational framework for simultaneous integration of multiple types of genomic data to identify microRNA-gene regulatory modules. [sent-184, score-0.023]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nimfa', 0.692), ('factorization', 0.357), ('nmf', 0.333), ('nonnegative', 0.129), ('python', 0.108), ('initialization', 0.106), ('albright', 0.087), ('evar', 0.087), ('marinka', 0.087), ('pymf', 0.087), ('rss', 0.087), ('vcol', 0.087), ('zitnik', 0.087), ('print', 0.074), ('tracking', 0.068), ('library', 0.068), ('fit', 0.067), ('tted', 0.065), ('sparseness', 0.065), ('script', 0.061), ('bionmf', 0.058), ('brunet', 0.058), ('dueck', 0.058), ('fctr', 0.058), ('ibrary', 0.058), ('itnik', 0.058), ('medulloblastoma', 0.058), ('onnegative', 0.058), ('upan', 0.058), ('alternating', 0.055), ('seung', 0.051), ('matrix', 0.05), ('bla', 0.049), ('cichocki', 0.049), ('laurberg', 0.049), ('ython', 0.049), ('schmidt', 0.049), ('quality', 0.047), ('lars', 0.044), ('ljubljana', 0.044), ('zupan', 0.044), ('lj', 0.044), ('bioinformatics', 0.044), ('lee', 0.042), ('residuals', 0.041), ('boutsidis', 0.041), ('implements', 0.039), ('toronto', 0.038), ('subproblems', 0.036), ('factorizations', 0.036), ('atrix', 0.034), ('factorized', 0.033), ('nonsmooth', 0.033), ('park', 0.033), ('kim', 0.032), ('squares', 0.032), ('package', 0.031), ('sebastian', 0.03), ('symbol', 0.03), ('supports', 0.03), ('divergence', 0.029), ('factors', 0.029), ('zhang', 0.029), ('matrices', 0.029), ('projected', 0.028), ('popular', 0.028), ('fisher', 0.027), ('scoring', 0.027), ('documentation', 0.027), ('hierarchical', 0.025), ('implementations', 0.025), ('omaha', 0.025), ('huo', 0.025), ('lehmann', 0.025), ('imposition', 0.025), ('ka', 0.025), ('partsbased', 0.025), ('raising', 0.025), ('client', 0.025), ('scripting', 0.025), ('ltd', 0.025), ('andrzej', 0.025), ('slovenian', 0.025), ('christos', 0.025), ('crafting', 0.025), ('gallopoulos', 0.025), ('genomics', 0.025), ('helper', 0.025), ('kauai', 0.025), ('php', 0.025), ('plumbley', 0.025), ('seamlessly', 0.025), ('zdunek', 0.025), ('li', 0.025), ('sparse', 0.023), ('simultaneous', 0.023), ('uni', 0.023), ('nonnegativity', 0.022), ('jia', 0.022), ('crisp', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="75-tfidf-1" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>Author: Marinka Žitnik, Blaž Zupan</p><p>Abstract: NIMFA is an open-source Python library that provides a uniﬁed interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA’s component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks. Keywords: nonnegative matrix factorization, initialization methods, quality measures, scripting, Python</p><p>2 0.27084213 <a title="75-tfidf-2" href="./jmlr-2012-Sparse_and_Unique_Nonnegative_Matrix_Factorization_Through_Data_Preprocessing.html">108 jmlr-2012-Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing</a></p>
<p>Author: Nicolas Gillis</p><p>Abstract: Nonnegative matrix factorization (NMF) has become a very popular technique in machine learning because it automatically extracts meaningful features through a sparse and part-based representation. However, NMF has the drawback of being highly ill-posed, that is, there typically exist many different but equivalent factorizations. In this paper, we introduce a completely new way to obtaining more well-posed NMF problems whose solutions are sparser. Our technique is based on the preprocessing of the nonnegative input data matrix, and relies on the theory of M-matrices and the geometric interpretation of NMF. This approach provably leads to optimal and sparse solutions under the separability assumption of Donoho and Stodden (2003), and, for rank-three matrices, makes the number of exact factorizations ﬁnite. We illustrate the effectiveness of our technique on several image data sets. Keywords: nonnegative matrix factorization, data preprocessing, uniqueness, sparsity, inversepositive matrices</p><p>3 0.059609868 <a title="75-tfidf-3" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>Author: Joonseok Lee, Mingxuan Sun, Guy Lebanon</p><p>Abstract: Recommendation systems are important business applications with signiﬁcant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms. Keywords: recommender systems, collaborative ﬁltering, evaluation metrics</p><p>4 0.057863012 <a title="75-tfidf-4" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>Author: Tom De Smedt, Walter Daelemans</p><p>Abstract: Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classiﬁers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern. Keywords: Python, data mining, natural language processing, machine learning, graph networks</p><p>5 0.048862521 <a title="75-tfidf-5" href="./jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering.html">101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</a></p>
<p>Author: Tianqi Chen, Weinan Zhang, Qiuxia Lu, Kailong Chen, Zhao Zheng, Yong Yu</p><p>Abstract: In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative ﬁltering. SVDFeature is designed to efﬁciently solve the feature-based matrix factorization. The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information. The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efﬁcient training on large-scale data set. Using this toolkit, we built solutions to win KDD Cup for two consecutive years. Keywords: large-scale collaborative ﬁltering, context-aware recommendation, ranking</p><p>6 0.035063129 <a title="75-tfidf-6" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>7 0.02992022 <a title="75-tfidf-7" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>8 0.028078718 <a title="75-tfidf-8" href="./jmlr-2012-Human_Gesture_Recognition_on_Product_Manifolds.html">50 jmlr-2012-Human Gesture Recognition on Product Manifolds</a></p>
<p>9 0.027264062 <a title="75-tfidf-9" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>10 0.025452057 <a title="75-tfidf-10" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<p>11 0.025162289 <a title="75-tfidf-11" href="./jmlr-2012-Fast_Approximation_of_Matrix_Coherence_and_Statistical_Leverage.html">43 jmlr-2012-Fast Approximation of Matrix Coherence and Statistical Leverage</a></p>
<p>12 0.024947813 <a title="75-tfidf-12" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>13 0.024822213 <a title="75-tfidf-13" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>14 0.024583677 <a title="75-tfidf-14" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>15 0.02305562 <a title="75-tfidf-15" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>16 0.022648063 <a title="75-tfidf-16" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>17 0.020621095 <a title="75-tfidf-17" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>18 0.020558449 <a title="75-tfidf-18" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>19 0.019066229 <a title="75-tfidf-19" href="./jmlr-2012-The_huge_Package_for_High-dimensional_Undirected_Graph_Estimation_in_R.html">113 jmlr-2012-The huge Package for High-dimensional Undirected Graph Estimation in R</a></p>
<p>20 0.018152708 <a title="75-tfidf-20" href="./jmlr-2012-Discriminative_Hierarchical_Part-based_Models_for_Human_Parsing_and_Action_Recognition.html">32 jmlr-2012-Discriminative Hierarchical Part-based Models for Human Parsing and Action Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.086), (1, 0.04), (2, 0.164), (3, -0.023), (4, -0.036), (5, 0.067), (6, 0.079), (7, -0.12), (8, 0.038), (9, -0.376), (10, -0.254), (11, -0.256), (12, 0.228), (13, -0.027), (14, 0.049), (15, -0.006), (16, -0.229), (17, 0.119), (18, 0.021), (19, 0.351), (20, 0.068), (21, -0.048), (22, -0.011), (23, -0.038), (24, -0.024), (25, -0.142), (26, -0.063), (27, 0.097), (28, 0.018), (29, 0.034), (30, 0.013), (31, 0.003), (32, -0.028), (33, 0.048), (34, 0.012), (35, -0.018), (36, -0.042), (37, -0.039), (38, -0.005), (39, -0.033), (40, 0.018), (41, 0.007), (42, 0.023), (43, -0.004), (44, 0.003), (45, -0.007), (46, 0.014), (47, 0.038), (48, 0.012), (49, -0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97256529 <a title="75-lsi-1" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>Author: Marinka Žitnik, Blaž Zupan</p><p>Abstract: NIMFA is an open-source Python library that provides a uniﬁed interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA’s component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks. Keywords: nonnegative matrix factorization, initialization methods, quality measures, scripting, Python</p><p>2 0.86244208 <a title="75-lsi-2" href="./jmlr-2012-Sparse_and_Unique_Nonnegative_Matrix_Factorization_Through_Data_Preprocessing.html">108 jmlr-2012-Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing</a></p>
<p>Author: Nicolas Gillis</p><p>Abstract: Nonnegative matrix factorization (NMF) has become a very popular technique in machine learning because it automatically extracts meaningful features through a sparse and part-based representation. However, NMF has the drawback of being highly ill-posed, that is, there typically exist many different but equivalent factorizations. In this paper, we introduce a completely new way to obtaining more well-posed NMF problems whose solutions are sparser. Our technique is based on the preprocessing of the nonnegative input data matrix, and relies on the theory of M-matrices and the geometric interpretation of NMF. This approach provably leads to optimal and sparse solutions under the separability assumption of Donoho and Stodden (2003), and, for rank-three matrices, makes the number of exact factorizations ﬁnite. We illustrate the effectiveness of our technique on several image data sets. Keywords: nonnegative matrix factorization, data preprocessing, uniqueness, sparsity, inversepositive matrices</p><p>3 0.23704465 <a title="75-lsi-3" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>Author: Joonseok Lee, Mingxuan Sun, Guy Lebanon</p><p>Abstract: Recommendation systems are important business applications with signiﬁcant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms. Keywords: recommender systems, collaborative ﬁltering, evaluation metrics</p><p>4 0.21691608 <a title="75-lsi-4" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>Author: Tom De Smedt, Walter Daelemans</p><p>Abstract: Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classiﬁers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern. Keywords: Python, data mining, natural language processing, machine learning, graph networks</p><p>5 0.18554527 <a title="75-lsi-5" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>Author: Uri Shalit, Daphna Weinshall, Gal Chechik</p><p>Abstract: When learning models that are represented in matrix forms, enforcing a low-rank constraint can dramatically improve the memory and run time complexity, while providing a natural regularization of the model. However, naive approaches to minimizing functions over the set of low-rank matrices are either prohibitively time consuming (repeated singular value decomposition of the matrix) or numerically unstable (optimizing a factored representation of the low-rank matrix). We build on recent advances in optimization over manifolds, and describe an iterative online learning procedure, consisting of a gradient step, followed by a second-order retraction back to the manifold. While the ideal retraction is costly to compute, and so is the projection operator that approximates it, we describe another retraction that can be computed efﬁciently. It has run time and memory complexity of O ((n + m)k) for a rank-k matrix of dimension m × n, when using an online procedure with rank-one gradients. We use this algorithm, L ORETA, to learn a matrix-form similarity measure over pairs of documents represented as high dimensional vectors. L ORETA improves the mean average precision over a passive-aggressive approach in a factorized model, and also improves over a full model trained on pre-selected features using the same memory requirements. We further adapt L ORETA to learn positive semi-deﬁnite low-rank matrices, providing an online algorithm for low-rank metric learning. L ORETA also shows consistent improvement over standard weakly supervised methods in a large (1600 classes and 1 million images, using ImageNet) multi-label image classiﬁcation task. Keywords: low rank, Riemannian manifolds, metric learning, retractions, multitask learning, online learning</p><p>6 0.14326672 <a title="75-lsi-6" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>7 0.14173588 <a title="75-lsi-7" href="./jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering.html">101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</a></p>
<p>8 0.13403702 <a title="75-lsi-8" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>9 0.12214772 <a title="75-lsi-9" href="./jmlr-2012-Fast_Approximation_of_Matrix_Coherence_and_Statistical_Leverage.html">43 jmlr-2012-Fast Approximation of Matrix Coherence and Statistical Leverage</a></p>
<p>10 0.11259547 <a title="75-lsi-10" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>11 0.11209602 <a title="75-lsi-11" href="./jmlr-2012-Iterative_Reweighted_Algorithms_for_Matrix_Rank_Minimization.html">52 jmlr-2012-Iterative Reweighted Algorithms for Matrix Rank Minimization</a></p>
<p>12 0.10926753 <a title="75-lsi-12" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>13 0.10691706 <a title="75-lsi-13" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<p>14 0.10406281 <a title="75-lsi-14" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<p>15 0.095218092 <a title="75-lsi-15" href="./jmlr-2012-Sally%3A_A_Tool_for_Embedding_Strings_in_Vector_Spaces.html">102 jmlr-2012-Sally: A Tool for Embedding Strings in Vector Spaces</a></p>
<p>16 0.09500806 <a title="75-lsi-16" href="./jmlr-2012-Human_Gesture_Recognition_on_Product_Manifolds.html">50 jmlr-2012-Human Gesture Recognition on Product Manifolds</a></p>
<p>17 0.087425254 <a title="75-lsi-17" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<p>18 0.08730153 <a title="75-lsi-18" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>19 0.084538035 <a title="75-lsi-19" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>20 0.080895886 <a title="75-lsi-20" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.02), (8, 0.457), (21, 0.034), (26, 0.046), (27, 0.04), (29, 0.02), (49, 0.021), (56, 0.055), (57, 0.016), (69, 0.045), (75, 0.03), (92, 0.035), (96, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77519745 <a title="75-lda-1" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>Author: Marinka Žitnik, Blaž Zupan</p><p>Abstract: NIMFA is an open-source Python library that provides a uniﬁed interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA’s component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks. Keywords: nonnegative matrix factorization, initialization methods, quality measures, scripting, Python</p><p>2 0.2289108 <a title="75-lda-2" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>Author: Trinh Minh Tri Do, Thierry Artières</p><p>Abstract: Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efﬁcient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efﬁcient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random ﬁelds, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy ε with a rate O(1/λε) where λ is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difﬁcult and requires a stronger and more disputable assumption. Yet we provide experimental results on artiﬁcial test problems, and on ﬁve standard and difﬁcult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms. Keywords: optimization, non-convex, non-smooth, cutting plane, bundle method, regularized risk</p><p>3 0.21869186 <a title="75-lda-3" href="./jmlr-2012-Fast_Approximation_of_Matrix_Coherence_and_Statistical_Leverage.html">43 jmlr-2012-Fast Approximation of Matrix Coherence and Statistical Leverage</a></p>
<p>Author: Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, David P. Woodruff</p><p>Abstract: The statistical leverage scores of a matrix A are the squared row-norms of the matrix containing its (top) left singular vectors and the coherence is the largest leverage score. These quantities are of interest in recently-popular problems such as matrix completion and Nystr¨ m-based low-rank o matrix approximation as well as in large-scale statistical data analysis applications more generally; moreover, they are of interest since they deﬁne the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms. Our main result is a randomized algorithm that takes as input an arbitrary n × d matrix A, with n ≫ d, and that returns as output relative-error approximations to all n of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of n and d) in O(nd log n) time, as opposed to the O(nd 2 ) time required by the na¨ve algorithm that involves computing an orthogonal basis for the ı range of A. Our analysis may be viewed in terms of computing a relative-error approximation to an underconstrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with n ≈ d, and the extension to streaming environments. Keywords: matrix coherence, statistical leverage, randomized algorithm</p><p>4 0.21718197 <a title="75-lda-4" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel</p><p>Abstract: The success of many machine learning and pattern recognition methods relies heavily upon the identiﬁcation of an appropriate distance metric on the input data. It is often beneﬁcial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semideﬁnite. Semideﬁnite programming is often used to enforce this constraint, but does not scale well and is not easy to implement. B OOST M ETRIC is instead based on the observation that any positive semideﬁnite matrix can be decomposed into a linear combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting methods are easy to implement, efﬁcient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semideﬁnite matrix with trace and rank being one rather than a classiﬁer or regressor. Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. Keywords: Mahalanobis distance, semideﬁnite programming, column generation, boosting, Lagrange duality, large margin nearest neighbor</p><p>5 0.21304764 <a title="75-lda-5" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>Author: Joonseok Lee, Mingxuan Sun, Guy Lebanon</p><p>Abstract: Recommendation systems are important business applications with signiﬁcant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms. Keywords: recommender systems, collaborative ﬁltering, evaluation metrics</p><p>6 0.21143633 <a title="75-lda-6" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>7 0.21138385 <a title="75-lda-7" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>8 0.21018201 <a title="75-lda-8" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>9 0.20880002 <a title="75-lda-9" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>10 0.20792034 <a title="75-lda-10" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>11 0.20731449 <a title="75-lda-11" href="./jmlr-2012-Human_Gesture_Recognition_on_Product_Manifolds.html">50 jmlr-2012-Human Gesture Recognition on Product Manifolds</a></p>
<p>12 0.20508474 <a title="75-lda-12" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>13 0.20463583 <a title="75-lda-13" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>14 0.20378858 <a title="75-lda-14" href="./jmlr-2012-A_Model_of_the_Perception_of_Facial_Expressions_of_Emotion_by_Humans%3A_Research_Overview_and_Perspectives.html">6 jmlr-2012-A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives</a></p>
<p>15 0.20330182 <a title="75-lda-15" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>16 0.20261961 <a title="75-lda-16" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>17 0.20243454 <a title="75-lda-17" href="./jmlr-2012-Finding_Recurrent_Patterns_from_Continuous_Sign_Language_Sentences_for_Automated_Extraction_of_Signs.html">45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</a></p>
<p>18 0.20239499 <a title="75-lda-18" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>19 0.20201799 <a title="75-lda-19" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>20 0.20176807 <a title="75-lda-20" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
