<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-31" href="#">jmlr2012-31</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</h1>
<br/><p>Source: <a title="jmlr-2012-31-pdf" href="http://jmlr.org/papers/volume13/fortin12a/fortin12a.pdf">pdf</a></p><p>Author: Félix-Antoine Fortin, François-Michel De Rainville, Marc-André Gardner, Marc Parizeau, Christian Gagné</p><p>Abstract: DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black-box frameworks. Freely available with extensive documentation at http://deap.gel.ulaval.ca, DEAP is an open source project under an LGPL license. Keywords: distributed evolutionary algorithms, software tools</p><p>Reference: <a title="jmlr-2012-31-reference" href="../jmlr2012_reference/jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black-box frameworks. [sent-17, score-0.191]
</p><p>2 Introduction Evolutionary Computation (EC) is a sophisticated ﬁeld with very diverse techniques and mechanisms, where even well designed frameworks can become quite complicated under the hood. [sent-23, score-0.158]
</p><p>3 They thus strive to hide the implementation details as much as possible, by providing large libraries of high-level functionalities, often in many different ﬂavours. [sent-24, score-0.028]
</p><p>4 The more elaborate these boxes become, the more obscure they are, and the less likely the commoner is to ever take a peek under the hood to consider making changes. [sent-26, score-0.028]
</p><p>5 But using EC to solve real-world problems most often requires the customization of algorithms. [sent-27, score-0.067]
</p><p>6 The DEAP (Distributed Evolutionary Algorithms in Python) framework is built over the Python programming language that provides the essential glue for assembling sophisticated EC systems. [sent-28, score-0.028]
</p><p>7 Its aim is to provide practical tools for rapid prototyping of custom evolutionary algorithms, where every step of the process is as explicit (pseudocode like) and easy to read and understand as possible. [sent-29, score-0.453]
</p><p>8 Core Architecture DEAP’s core is composed of two simple structures: a creator and a toolbox. [sent-32, score-0.162]
</p><p>9 The creator module is a meta-factory that allows the run-time creation of classes via both inheritance and composition. [sent-33, score-0.316]
</p><p>10 Practically speaking, this allows the creation of genotypes and populations from any data structure such as lists, sets, dictionaries, trees, etc. [sent-36, score-0.038]
</p><p>11 This creator concept is key to facilitating the implementation of any type of EA, including genetic algorithms (Mitchell, 1998), genetic programming (Banzhaf et al. [sent-37, score-0.308]
</p><p>12 , 1998), evolution strategies (Beyer and Schwefel, 2002), covariance matrix adaptation evolution strategy (Hansen and Ostermeier, 2001), particle swarm optimization (Kennedy and Eberhart, 2001), and many more. [sent-38, score-0.251]
</p><p>13 The toolbox is a container for the tools (operators) that the user wants to use in his EA. [sent-39, score-0.25]
</p><p>14 The toolbox is manually populated by the user with selected tools. [sent-40, score-0.195]
</p><p>15 For instance, if the user needs a crossover in his algorithm, but has access to several crossover types, he will choose the one best suited for his current problem, for example a uniform crossover “cxUniform”, and register it into the toolbox using a generic “mate” alias. [sent-41, score-0.654]
</p><p>16 If he later decides that some other crossover is better suited, his algorithm will remain unchanged, he will only need to update the corresponding alias in the toolbox. [sent-43, score-0.342]
</p><p>17 The core functionalities of DEAP are levered by several peripheral modules. [sent-44, score-0.157]
</p><p>18 The algorithms module contains four classical EC algorithms: generational, (µ , λ), (µ + λ), and ask-and-tell (Collette et al. [sent-45, score-0.15]
</p><p>19 These serve as a starting point for users to build their own custom EAs meeting their speciﬁc needs. [sent-47, score-0.071]
</p><p>20 The tools module provides basic EC operators such as initializations, mutations, crossovers, and selections. [sent-48, score-0.269]
</p><p>21 These operators can be directly added to a toolbox in order to be used in algorithms. [sent-49, score-0.174]
</p><p>22 This module also contains a number of components that gather useful information regarding the evolution: ﬁtness statistics, genealogy, hall-of-fame for the best individuals encountered so far, and checkpointing mechanisms to allow evolution restart. [sent-50, score-0.361]
</p><p>23 A base module contains some data structures frequently used in EC, but not implemented in standard Python (e. [sent-51, score-0.183]
</p><p>24 The last module named dtm, for Distributed Task Manager, offers distributed substitutes for common Python functions such as apply and map. [sent-54, score-0.15]
</p><p>25 It even balances the workload among workers to optimize the distribution efﬁciency. [sent-56, score-0.095]
</p><p>26 The individual is represented as a bit-string where each bit corresponds to a feature that can be selected or not. [sent-58, score-0.026]
</p><p>27 On line 2, the relevant DEAP modules are ﬁrst imported. [sent-61, score-0.033]
</p><p>28 Next, an Individual class is derived from the Python list and composed with our newly created FitnessMulti object. [sent-69, score-0.029]
</p><p>29 The toolbox register method accepts a variable number of arguments. [sent-71, score-0.203]
</p><p>30 The ﬁrst is the alias name, and the second the function that we want to associate with this alias. [sent-72, score-0.2]
</p><p>31 All other arguments are passed to this function when the alias is called. [sent-73, score-0.228]
</p><p>32 randint with arguments 0 and 1, and thus generate a random bit. [sent-76, score-0.028]
</p><p>33 tion problem, this bit function is simply called n = 80 times repeatedly using the tools. [sent-79, score-0.026]
</p><p>34 initRepeat method that accepts three arguments: a container, a function, and the number of times to repeat initialization. [sent-80, score-0.042]
</p><p>35 Similarly, a population alias is deﬁned to allow population initialization, in this case with n = 100 individuals. [sent-81, score-0.392]
</p><p>36 Line 18 then proceeds with the allocation of a population of individuals that have their ﬁtness evaluated on line 19 by mapping the evaluation function to every element of the population container. [sent-82, score-0.28]
</p><p>37 Lines 20 and 21 replace the individuals ﬁtness with their newly computed values. [sent-83, score-0.117]
</p><p>38 Finally, lines 23 through 28 show the evolutionary loop that uses the µ + λ strategy, where µ = 100 parents (current population) are mixed with λ = 100 offspring (lambda line 24) for the selection process (NSGA-II) to produce the next generation of k = 100 parents (line 28). [sent-84, score-0.412]
</p><p>39 The varOr algorithm loops until it has generated λ offspring using either crossover (mate alias) with probability cxpb, mutation (mutate alias) with probability mutpb, or reproduction. [sent-85, score-0.209]
</p><p>40 Efﬁcient distribution of speciﬁc parts of user algorithms is made possible by the dtm module. [sent-86, score-0.24]
</p><p>41 The task manager distributes these sub-tasks across the worker nodes, and automatically balances the workload evenly among them. [sent-88, score-0.18]
</p><p>42 Table 1 presents a comparison of the number of lines required by some of the most popular frameworks to deﬁne different components of a OneMax example (without distribution). [sent-92, score-0.261]
</p><p>43 Columns respectively indicate how many lines are required to deﬁne the bit-string type, to conﬁgure the operators, to implement the generational algorithm and to execute the example. [sent-93, score-0.17]
</p><p>44 DEAP is the only framework that allows the complete deﬁnition of the EA in less than one hundred lines of code. [sent-94, score-0.103]
</p><p>45 Even though some frameworks may allow shorter expressions of standardized solutions, any needed customization of type or algorithm can also force the user to dig deep into the framework to modify perhaps hundreds of lines. [sent-95, score-0.265]
</p><p>46 We therefore assert that DEAP is superior to previous frameworks for rapid prototyping of new algorithms and deﬁnition of custom types. [sent-96, score-0.396]
</p><p>47 Conclusion Current major EC frameworks generally do a good job of offering generic tools to solve hard problems using EAs. [sent-98, score-0.234]
</p><p>48 Even experts can become overwhelmed when trying to implement speciﬁc features. [sent-100, score-0.028]
</p><p>49 The DEAP core is implemented in 6 program ﬁles (Python modules) with only 1653 lines of code. [sent-102, score-0.165]
</p><p>50 To these core lines, the DTM module adds another 2073 lines for enabling the easy distribution of computationally intensive algorithm parts. [sent-103, score-0.315]
</p><p>51 The framework also includes more than 35 application examples that add another 2448 lines of code, leading to an average ratio of core lines to example lines of 1. [sent-104, score-0.371]
</p><p>52 In other words, DEAP implements the 15 examples of OpenBEAGLE plus 20 more with 10 times less lines of code. [sent-111, score-0.103]
</p><p>53 Acknowledgments This work has been supported by the Fonds de recherche du Qu´ bec - Nature et technologies e (FQRNT) and by the Natural Sciences and Engineering Research Council of Canada (NSERC). [sent-112, score-0.085]
</p><p>54 On object-oriented programming of optimizers – Examples in Scilab. [sent-135, score-0.028]
</p><p>55 A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II. [sent-146, score-0.09]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('deap', 0.532), ('tness', 0.212), ('alias', 0.2), ('dtm', 0.2), ('ec', 0.177), ('parizeau', 0.166), ('ulaval', 0.166), ('evolutionary', 0.166), ('frameworks', 0.158), ('module', 0.15), ('python', 0.146), ('crossover', 0.142), ('prototyping', 0.114), ('toolbox', 0.104), ('lines', 0.103), ('creator', 0.1), ('fortin', 0.1), ('rainville', 0.1), ('population', 0.096), ('evolution', 0.092), ('genetic', 0.09), ('individuals', 0.088), ('bec', 0.085), ('gardner', 0.085), ('mate', 0.085), ('marc', 0.075), ('custom', 0.071), ('qu', 0.071), ('operators', 0.07), ('ade', 0.067), ('agn', 0.067), ('ainville', 0.067), ('ardner', 0.067), ('banzhaf', 0.067), ('beyer', 0.067), ('collette', 0.067), ('customization', 0.067), ('deb', 0.067), ('ecj', 0.067), ('fitnessmulti', 0.067), ('functionalities', 0.067), ('gagn', 0.067), ('gel', 0.067), ('generational', 0.067), ('inspyred', 0.067), ('mutate', 0.067), ('nie', 0.067), ('offspring', 0.067), ('onemax', 0.067), ('ortin', 0.067), ('pyevolve', 0.067), ('swarm', 0.067), ('volutionary', 0.067), ('ea', 0.066), ('core', 0.062), ('hansen', 0.062), ('christian', 0.059), ('container', 0.057), ('asy', 0.057), ('kennedy', 0.057), ('manager', 0.057), ('register', 0.057), ('rapid', 0.053), ('eo', 0.051), ('populated', 0.051), ('workload', 0.051), ('tools', 0.049), ('roberts', 0.047), ('transparent', 0.047), ('balances', 0.044), ('accepts', 0.042), ('user', 0.04), ('lgorithms', 0.039), ('parents', 0.038), ('creation', 0.038), ('ca', 0.035), ('modules', 0.033), ('structures', 0.033), ('mechanisms', 0.031), ('newly', 0.029), ('peripheral', 0.028), ('strive', 0.028), ('mutations', 0.028), ('overwhelmed', 0.028), ('addison', 0.028), ('fonds', 0.028), ('hood', 0.028), ('inheritance', 0.028), ('keller', 0.028), ('lambda', 0.028), ('ostermeier', 0.028), ('syst', 0.028), ('wesley', 0.028), ('worker', 0.028), ('programming', 0.028), ('arguments', 0.028), ('generic', 0.027), ('kaufmann', 0.027), ('bit', 0.026), ('dictionaries', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="31-tfidf-1" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>Author: Félix-Antoine Fortin, François-Michel De Rainville, Marc-André Gardner, Marc Parizeau, Christian Gagné</p><p>Abstract: DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black-box frameworks. Freely available with extensive documentation at http://deap.gel.ulaval.ca, DEAP is an open source project under an LGPL license. Keywords: distributed evolutionary algorithms, software tools</p><p>2 0.065119788 <a title="31-tfidf-2" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>Author: Tom De Smedt, Walter Daelemans</p><p>Abstract: Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classiﬁers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern. Keywords: Python, data mining, natural language processing, machine learning, graph networks</p><p>3 0.059492171 <a title="31-tfidf-3" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>4 0.052465536 <a title="31-tfidf-4" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>Author: Kay H. Brodersen, Christoph Mathys, Justin R. Chumbley, Jean Daunizeau, Cheng Soon Ong, Joachim M. Buhmann, Klaas E. Stephan</p><p>Abstract: Classiﬁcation algorithms are frequently used on data with a natural hierarchical structure. For instance, classiﬁers are often trained and tested on trial-wise measurements, separately for each subject within a group. One important question is how classiﬁcation outcomes observed in individual subjects can be generalized to the population from which the group was sampled. To address this question, this paper introduces novel statistical models that are guided by three desiderata. First, all models explicitly respect the hierarchical nature of the data, that is, they are mixed-effects models that simultaneously account for within-subjects (ﬁxed-effects) and across-subjects (random-effects) variance components. Second, maximum-likelihood estimation is replaced by full Bayesian inference in order to enable natural regularization of the estimation problem and to afford conclusions in terms of posterior probability statements. Third, inference on classiﬁcation accuracy is complemented by inference on the balanced accuracy, which avoids inﬂated accuracy estimates for imbalanced data sets. We introduce hierarchical models that satisfy these criteria and demonstrate their advantages over conventional methods using MCMC implementations for model inversion and model selection on both synthetic and empirical data. We envisage that our approach will improve the sensitivity and validity of statistical inference in future hierarchical classiﬁcation studies. Keywords: beta-binomial, normal-binomial, balanced accuracy, Bayesian inference, group studies</p><p>5 0.044576302 <a title="31-tfidf-5" href="./jmlr-2012-Smoothing_Multivariate_Performance_Measures.html">107 jmlr-2012-Smoothing Multivariate Performance Measures</a></p>
<p>Author: Xinhua Zhang, Ankan Saha, S.V.N. Vishwanathan</p><p>Abstract: Optimizing multivariate performance measure is an important task in Machine Learning. Joachims (2005) introduced a Support Vector Method whose underlying optimization problem is commonly solved by cutting plane methods (CPMs) such as SVM-Perf and BMRM. It can be shown that CPMs 1 converge to an ε accurate solution in O λε iterations, where λ is the trade-off parameter between the regularizer and the loss function. Motivated by the impressive convergence rate of CPM on a number of practical problems, it was conjectured that these rates can be further improved. We disprove this conjecture in this paper by constructing counter examples. However, surprisingly, we further discover that these problems are not inherently hard, and we develop a novel smoothing strategy, which in conjunction with Nesterov’s accelerated gradient method, can ﬁnd an ε accuiterations. Computationally, our smoothing technique is also rate solution in O∗ min 1 , √1 ε λε particularly advantageous for optimizing multivariate performance scores such as precision/recall break-even point and ROCArea; the cost per iteration remains the same as that of CPMs. Empirical evaluation on some of the largest publicly available data sets shows that our method converges signiﬁcantly faster than CPMs without sacriﬁcing generalization ability. Keywords: non-smooth optimization, max-margin methods, multivariate performance measures, Support Vector Machines, smoothing</p><p>6 0.042706564 <a title="31-tfidf-6" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>7 0.04252832 <a title="31-tfidf-7" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>8 0.037183661 <a title="31-tfidf-8" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>9 0.030916339 <a title="31-tfidf-9" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>10 0.02992022 <a title="31-tfidf-10" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>11 0.026874611 <a title="31-tfidf-11" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>12 0.020893637 <a title="31-tfidf-12" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>13 0.019679429 <a title="31-tfidf-13" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>14 0.018828373 <a title="31-tfidf-14" href="./jmlr-2012-Random_Search_for_Hyper-Parameter_Optimization.html">95 jmlr-2012-Random Search for Hyper-Parameter Optimization</a></p>
<p>15 0.018789282 <a title="31-tfidf-15" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>16 0.018113537 <a title="31-tfidf-16" href="./jmlr-2012-A_Topic_Modeling_Toolbox_Using_Belief_Propagation.html">9 jmlr-2012-A Topic Modeling Toolbox Using Belief Propagation</a></p>
<p>17 0.016607549 <a title="31-tfidf-17" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<p>18 0.015649531 <a title="31-tfidf-18" href="./jmlr-2012-Sally%3A_A_Tool_for_Embedding_Strings_in_Vector_Spaces.html">102 jmlr-2012-Sally: A Tool for Embedding Strings in Vector Spaces</a></p>
<p>19 0.015396557 <a title="31-tfidf-19" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>20 0.013647195 <a title="31-tfidf-20" href="./jmlr-2012-Jstacs%3A_A_Java_Framework_for_Statistical_Analysis_and_Classification_of_Biological_Sequences.html">53 jmlr-2012-Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.069), (1, 0.028), (2, 0.126), (3, -0.03), (4, 0.053), (5, 0.036), (6, 0.078), (7, 0.025), (8, -0.081), (9, -0.018), (10, -0.063), (11, -0.037), (12, 0.132), (13, -0.01), (14, -0.09), (15, 0.03), (16, 0.009), (17, -0.001), (18, -0.124), (19, -0.17), (20, 0.029), (21, 0.063), (22, -0.102), (23, 0.018), (24, -0.143), (25, -0.343), (26, -0.017), (27, -0.032), (28, -0.021), (29, -0.034), (30, -0.212), (31, 0.085), (32, -0.016), (33, -0.22), (34, -0.078), (35, -0.001), (36, 0.176), (37, -0.003), (38, -0.008), (39, -0.04), (40, 0.059), (41, 0.022), (42, 0.114), (43, -0.021), (44, -0.059), (45, -0.036), (46, 0.01), (47, 0.097), (48, 0.023), (49, 0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97295409 <a title="31-lsi-1" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>Author: Félix-Antoine Fortin, François-Michel De Rainville, Marc-André Gardner, Marc Parizeau, Christian Gagné</p><p>Abstract: DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black-box frameworks. Freely available with extensive documentation at http://deap.gel.ulaval.ca, DEAP is an open source project under an LGPL license. Keywords: distributed evolutionary algorithms, software tools</p><p>2 0.61303896 <a title="31-lsi-2" href="./jmlr-2012-Smoothing_Multivariate_Performance_Measures.html">107 jmlr-2012-Smoothing Multivariate Performance Measures</a></p>
<p>Author: Xinhua Zhang, Ankan Saha, S.V.N. Vishwanathan</p><p>Abstract: Optimizing multivariate performance measure is an important task in Machine Learning. Joachims (2005) introduced a Support Vector Method whose underlying optimization problem is commonly solved by cutting plane methods (CPMs) such as SVM-Perf and BMRM. It can be shown that CPMs 1 converge to an ε accurate solution in O λε iterations, where λ is the trade-off parameter between the regularizer and the loss function. Motivated by the impressive convergence rate of CPM on a number of practical problems, it was conjectured that these rates can be further improved. We disprove this conjecture in this paper by constructing counter examples. However, surprisingly, we further discover that these problems are not inherently hard, and we develop a novel smoothing strategy, which in conjunction with Nesterov’s accelerated gradient method, can ﬁnd an ε accuiterations. Computationally, our smoothing technique is also rate solution in O∗ min 1 , √1 ε λε particularly advantageous for optimizing multivariate performance scores such as precision/recall break-even point and ROCArea; the cost per iteration remains the same as that of CPMs. Empirical evaluation on some of the largest publicly available data sets shows that our method converges signiﬁcantly faster than CPMs without sacriﬁcing generalization ability. Keywords: non-smooth optimization, max-margin methods, multivariate performance measures, Support Vector Machines, smoothing</p><p>3 0.47613457 <a title="31-lsi-3" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>Author: Tom De Smedt, Walter Daelemans</p><p>Abstract: Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classiﬁers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern. Keywords: Python, data mining, natural language processing, machine learning, graph networks</p><p>4 0.45600981 <a title="31-lsi-4" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>Author: David Verstraeten, Benjamin Schrauwen, Sander Dieleman, Philemon Brakel, Pieter Buteneers, Dejan Pecevski</p><p>Abstract: Oger (OrGanic Environment for Reservoir computing) is a Python toolbox for building, training and evaluating modular learning architectures on large data sets. It builds on MDP for its modularity, and adds processing of sequential data sets, gradient descent training, several crossvalidation schemes and parallel parameter optimization methods. Additionally, several learning algorithms are implemented, such as different reservoir implementations (both sigmoid and spiking), ridge regression, conditional restricted Boltzmann machine (CRBM) and others, including GPU accelerated versions. Oger is released under the GNU LGPL, and is available from http: //organic.elis.ugent.be/oger. Keywords: Python, modular architectures, sequential processing</p><p>5 0.36317044 <a title="31-lsi-5" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>6 0.30434424 <a title="31-lsi-6" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>7 0.2624822 <a title="31-lsi-7" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>8 0.20818903 <a title="31-lsi-8" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>9 0.17450461 <a title="31-lsi-9" href="./jmlr-2012-Random_Search_for_Hyper-Parameter_Optimization.html">95 jmlr-2012-Random Search for Hyper-Parameter Optimization</a></p>
<p>10 0.16034786 <a title="31-lsi-10" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>11 0.15478444 <a title="31-lsi-11" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>12 0.12069041 <a title="31-lsi-12" href="./jmlr-2012-Quantum_Set_Intersection_and_its_Application_to_Associative_Memory.html">93 jmlr-2012-Quantum Set Intersection and its Application to Associative Memory</a></p>
<p>13 0.12034672 <a title="31-lsi-13" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>14 0.11081687 <a title="31-lsi-14" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>15 0.1096179 <a title="31-lsi-15" href="./jmlr-2012-Sally%3A_A_Tool_for_Embedding_Strings_in_Vector_Spaces.html">102 jmlr-2012-Sally: A Tool for Embedding Strings in Vector Spaces</a></p>
<p>16 0.10596324 <a title="31-lsi-16" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>17 0.10502796 <a title="31-lsi-17" href="./jmlr-2012-Active_Clustering_of_Biological_Sequences.html">12 jmlr-2012-Active Clustering of Biological Sequences</a></p>
<p>18 0.10476618 <a title="31-lsi-18" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>19 0.10335042 <a title="31-lsi-19" href="./jmlr-2012-A_Model_of_the_Perception_of_Facial_Expressions_of_Emotion_by_Humans%3A_Research_Overview_and_Perspectives.html">6 jmlr-2012-A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives</a></p>
<p>20 0.098547868 <a title="31-lsi-20" href="./jmlr-2012-Algorithms_for_Learning_Kernels_Based_on_Centered_Alignment.html">16 jmlr-2012-Algorithms for Learning Kernels Based on Centered Alignment</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.011), (21, 0.721), (26, 0.016), (27, 0.017), (29, 0.014), (49, 0.013), (56, 0.039), (75, 0.018), (92, 0.017), (96, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94169164 <a title="31-lda-1" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>Author: Félix-Antoine Fortin, François-Michel De Rainville, Marc-André Gardner, Marc Parizeau, Christian Gagné</p><p>Abstract: DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black-box frameworks. Freely available with extensive documentation at http://deap.gel.ulaval.ca, DEAP is an open source project under an LGPL license. Keywords: distributed evolutionary algorithms, software tools</p><p>2 0.81562167 <a title="31-lda-2" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>Author: Jian Huang, Cun-Hui Zhang</p><p>Abstract: The ℓ1 -penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ1 -penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1 -penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results. Keywords: variable selection, penalized estimation, oracle inequality, generalized linear models, selection consistency, sparsity</p><p>3 0.76576018 <a title="31-lda-3" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>Author: Gérard Biau</p><p>Abstract: Random forests are a scheme proposed by Leo Breiman in the 2000’s for building a predictor ensemble with a set of decision trees that grow in randomly selected subspaces of data. Despite growing interest and practical use, there has been little exploration of the statistical properties of random forests, and little is known about the mathematical forces driving the algorithm. In this paper, we offer an in-depth analysis of a random forests model suggested by Breiman (2004), which is very close to the original algorithm. We show in particular that the procedure is consistent and adapts to sparsity, in the sense that its rate of convergence depends only on the number of strong features and not on how many noise variables are present. Keywords: random forests, randomization, sparsity, dimension reduction, consistency, rate of convergence</p><p>4 0.37596738 <a title="31-lda-4" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>Author: Lan Xue, Annie Qu</p><p>Abstract: The varying-coefﬁcient model is ﬂexible and powerful for modeling the dynamic changes of regression coefﬁcients. It is important to identify signiﬁcant covariates associated with response variables, especially for high-dimensional settings where the number of covariates can be larger than the sample size. We consider model selection in the high-dimensional setting and adopt difference convex programming to approximate the L0 penalty, and we investigate the global optimality properties of the varying-coefﬁcient estimator. The challenge of the variable selection problem here is that the dimension of the nonparametric form for the varying-coefﬁcient modeling could be inﬁnite, in addition to dealing with the high-dimensional linear covariates. We show that the proposed varying-coefﬁcient estimator is consistent, enjoys the oracle property and achieves an optimal convergence rate for the non-zero nonparametric components for high-dimensional data. Our simulations and numerical examples indicate that the difference convex algorithm is efﬁcient using the coordinate decent algorithm, and is able to select the true model at a higher frequency than the least absolute shrinkage and selection operator (LASSO), the adaptive LASSO and the smoothly clipped absolute deviation (SCAD) approaches. Keywords: coordinate decent algorithm, difference convex programming, L0 - regularization, large-p small-n, model selection, nonparametric function, oracle property, truncated L1 penalty</p><p>5 0.32920077 <a title="31-lda-5" href="./jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</a></p>
<p>Author: Ji Liu, Peter Wonka, Jieping Ye</p><p>Abstract: We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ Rn×m (m ≫ n) and a noisy observation vector y ∈ Rn satisfying y = Xβ∗ + ε where ε is the noise vector following a Gaussian distribution N(0, σ2 I), how to recover the signal (or parameter vector) β∗ when the signal is sparse? The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively reﬁnes the target signal β∗ . We show that if X obeys a certain condition, then with a large probability ˆ the difference between the solution β estimated by the proposed method and the true solution β∗ measured in terms of the ℓ p norm (p ≥ 1) is bounded as ˆ β − β∗ p ≤ C(s − N)1/p log m + ∆ σ, where C is a constant, s is the number of nonzero entries in β∗ , the risk of the oracle estimator ∆ is independent of m and is much smaller than the ﬁrst term, and N is the number of entries of β∗ larger √ than a certain value in the order of O (σ log m). The proposed √ method improves the estimation √ bound of the standard Dantzig selector approximately from Cs1/p log mσ to C(s − N)1/p log mσ where the value N depends on the number of large entries in β∗ . When N = s, the proposed algorithm achieves the oracle solution with a high probability, where the oracle solution is the projection of the observation vector y onto true features. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector. Finally, we extend this multi-stage procedure to the LASSO case. Keywords: multi-stage, Dantzig selector, LASSO, sparse signal recovery</p><p>6 0.3276813 <a title="31-lda-6" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>7 0.32532015 <a title="31-lda-7" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>8 0.32317859 <a title="31-lda-8" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>9 0.28204903 <a title="31-lda-9" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>10 0.27293861 <a title="31-lda-10" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>11 0.26678127 <a title="31-lda-11" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>12 0.2542806 <a title="31-lda-12" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>13 0.24792717 <a title="31-lda-13" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>14 0.2462135 <a title="31-lda-14" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>15 0.23849879 <a title="31-lda-15" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>16 0.23618436 <a title="31-lda-16" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>17 0.23488881 <a title="31-lda-17" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>18 0.23208293 <a title="31-lda-18" href="./jmlr-2012-Mal-ID%3A_Automatic_Malware_Detection_Using_Common_Segment_Analysis_and_Meta-Features.html">63 jmlr-2012-Mal-ID: Automatic Malware Detection Using Common Segment Analysis and Meta-Features</a></p>
<p>19 0.22529197 <a title="31-lda-19" href="./jmlr-2012-Refinement_of_Operator-valued_Reproducing_Kernels.html">96 jmlr-2012-Refinement of Operator-valued Reproducing Kernels</a></p>
<p>20 0.22125691 <a title="31-lda-20" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
