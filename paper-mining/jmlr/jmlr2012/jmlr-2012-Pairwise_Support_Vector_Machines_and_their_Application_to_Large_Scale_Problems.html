<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-89" href="#">jmlr2012-89</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</h1>
<br/><p>Source: <a title="jmlr-2012-89-pdf" href="http://jmlr.org/papers/volume13/brunner12a/brunner12a.pdf">pdf</a></p><p>Author: Carl Brunner, Andreas Fischer, Klaus Luig, Thorsten Thies</p><p>Abstract: Pairwise classiﬁcation is the task to predict whether the examples a, b of a pair (a, b) belong to the same class or to different classes. In particular, interclass generalization problems can be treated in this way. In pairwise classiﬁcation, the order of the two input examples should not affect the classiﬁcation result. To achieve this, particular kernels as well as the use of symmetric training sets in the framework of support vector machines were suggested. The paper discusses both approaches in a general way and establishes a strong connection between them. In addition, an efﬁcient implementation is discussed which allows the training of several millions of pairs. The value of these contributions is conﬁrmed by excellent results on the labeled faces in the wild benchmark. Keywords: pairwise support vector machines, interclass generalization, pairwise kernels, large scale problems</p><p>Reference: <a title="jmlr-2012-89-reference" href="../jmlr2012_reference/jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In pairwise classiﬁcation, the order of the two input examples should not affect the classiﬁcation result. [sent-10, score-0.442]
</p><p>2 To achieve this, particular kernels as well as the use of symmetric training sets in the framework of support vector machines were suggested. [sent-11, score-0.413]
</p><p>3 Keywords: pairwise support vector machines, interclass generalization, pairwise kernels, large scale problems  1. [sent-15, score-0.917]
</p><p>4 A more recent approach used in the ﬁeld of multiclass and binary classiﬁcation is pairwise classiﬁcation (Abernethy et al. [sent-17, score-0.44]
</p><p>5 For later use, a support vector machine (SVM) that is able to handle pairwise classiﬁcation tasks is called pairwise SVM. [sent-23, score-0.83]
</p><p>6 A natural requirement for a pairwise classiﬁer is that the order of the two input examples should not inﬂuence the classiﬁcation result (symmetry). [sent-24, score-0.442]
</p><p>7 (2004a) propose the use of training sets with a symmetric structure. [sent-29, score-0.22]
</p><p>8 A typical pairwise classiﬁcation task arises in face recognition. [sent-33, score-0.415]
</p><p>9 Therefore, we discuss an efﬁcient implementation of pairwise SVMs. [sent-37, score-0.415]
</p><p>10 This enables the training of pairwise SVMs with several millions of pairs. [sent-38, score-0.509]
</p><p>11 In Section 2 we give a short introduction to pairwise classiﬁcation and discuss the symmetry of decision functions obtained by pairwise SVMs. [sent-41, score-1.003]
</p><p>12 1, we analyze the symmetry of decision functions from pairwise SVMs that rely on symmetric training sets. [sent-43, score-0.808]
</p><p>13 The efﬁcient implementation of pairwise SVMs is discussed in Section 4. [sent-46, score-0.415]
</p><p>14 The main contribution of the paper is that we show the equivalence of two approaches for obtaining a symmetric classiﬁer from pairwise SVMs and demonstrate the efﬁciency and good interclass generalization performance of pairwise SVMs on large scale problems. [sent-48, score-1.043]
</p><p>15 The class of a training example might be unknown, but we demand that we know for each pair (xi , x j ) of training examples whether its examples belong to the same class or to different classes. [sent-54, score-0.277]
</p><p>16 In pairwise classiﬁcation the aim is to decide whether the examples of a pair (a, b) ∈ X × X belong to the same class or not. [sent-57, score-0.477]
</p><p>17 In this paper, we will make use of pairwise decision functions f : X × X → . [sent-58, score-0.496]
</p><p>18 Note that neither a, b need to belong to the set of training examples nor the classes of a, b need to belong to the classes of the training examples. [sent-60, score-0.359]
</p><p>19 In pairwise classiﬁcation one often uses pairwise kernels K : (X × X) × (X × X) → paper we assume that any pairwise kernel is symmetric, that is, it holds that  Ê. [sent-64, score-1.513]
</p><p>20 We call KD direct sum pairwise kernel and KT tensor pairwise kernel (cf. [sent-67, score-1.06]
</p><p>21 o A natural and desirable property of any pairwise decision function is that it should be symmetric in the following sense f (a, b) = f (b, a)  for all a, b ∈ X. [sent-69, score-0.622]
</p><p>22 Then, the pairwise decision function f obtained by a pairwise SVM can be written as  ∑  f (a, b) ≔  αi j yi j K ((xi , x j ), (a, b)) + γ  (3)  (i, j)∈I  Ê  with bias γ ∈ and αi j ≥ 0 for all (i, j) ∈ I. [sent-71, score-0.954]
</p><p>23 This motivates us to call a kernel K balanced if K((a, b), (c, d)) = K((a, b), (d, c)) for all a, b, c, d ∈ X holds. [sent-73, score-0.264]
</p><p>24 Thus, if a balanced kernel is used, then (3) is always a symmetric decision function. [sent-74, score-0.471]
</p><p>25 For instance, the following kernels are balanced 1 (k(a, c) + k(a, d) + k(b, c) + k(b, d)), 2 1 KT L ((a, b), (c, d)) ≔ (k(a, c)k(b, d) + k(a, d)k(b, c)) , 2 1 KML ((a, b), (c, d)) ≔ (k(a, c) − k(a, d) − k(b, c) + k(b, d))2 , 4 KT M ((a, b), (c, d)) ≔ KT L ((a, b), (c, d)) + KML ((a, b), (c, d)). [sent-75, score-0.326]
</p><p>26 (2007) call KML metric learning pairwise kernel and KT L tensor learning pairwise kernel. [sent-77, score-0.957]
</p><p>27 (2004a), direct sum learning pairwise kernel and KT M tensor metric learning pairwise kernel. [sent-79, score-0.957]
</p><p>28 For representing some balanced kernels by projections see Brunner et al. [sent-80, score-0.326]
</p><p>29 As detailed above, if a balanced kernel is used within a pairwise SVM, one always obtains a symmetric decision function. [sent-84, score-0.886]
</p><p>30 For pairwise SVMs which use KD (1) as pairwise kernel, it has been claimed that any symmetric set of training pairs leads to a symmetric decision function (see Bar-Hillel et al. [sent-85, score-1.305]
</p><p>31 We call a set of training pairs symmetric, if for any training pair (a, b) the pair (b, a) also belongs to the training set. [sent-87, score-0.33]
</p><p>32 2 that under some conditions a symmetric training set leads to the same decision function as balanced kernels if we disregard the SVM bias term γ. [sent-92, score-0.627]
</p><p>33 Interestingly, the application of balanced kernels leads to signiﬁcantly shorter training times (see Section 4. [sent-93, score-0.443]
</p><p>34 1 Symmetric Training Sets In this subsection we show that the symmetry of a pairwise decision function is indeed achieved by means of symmetric training sets. [sent-96, score-0.808]
</p><p>35 Furthermore, we will make use of pairwise kernels K with K((a, b), (c, d)) = K((b, a), (d, c)) for all a, b, c, d ∈ X. [sent-98, score-0.58]
</p><p>36 (8) As any pairwise kernel is assumed to be symmetric, (8) holds for any balanced pairwise kernel. [sent-99, score-1.094]
</p><p>37 Note that there are other pairwise kernels that satisfy (8), for instance for the kernels given in Equations 1 and 2. [sent-100, score-0.745]
</p><p>38 For IR , IN ⊆ I deﬁned by IR ≔ {(i, j) ∈ I|i = j} and IN ≔ I \ IR let us consider the dual pairwise SVM min G(α) α  0 ≤ αi j ≤ C  for all (i, j) ∈ IN  0 ≤ αii ≤ 2C  s. [sent-101, score-0.415]
</p><p>39 (i, j)∈I  with G(α) ≔  1 ∑ αi j αkl yi j ykl K((xi , x j ), (xk , xl )) − ∑ αi j . [sent-104, score-0.268]
</p><p>40 2 (i, j),(k,l)∈I (i, j)∈I  ˆ Lemma 1 If I is a symmetric index set and if (8) holds, then there is a solution α of (9) with ˆ ˆ αi j = α ji for all (i, j) ∈ I. [sent-105, score-0.277]
</p><p>41 Then,  ∑  ˜ 2G(α) =  α∗ α∗ yi j ykl Ki j,kl − 2 ji lk  ∑  α∗ . [sent-109, score-0.458]
</p><p>42 ji  (i, j)∈I  (i, j),(k,l)∈I  Note that yi j = y ji holds for all (i, j) ∈ I. [sent-110, score-0.345]
</p><p>43 By (8) we further obtain ˜ 2G(α) =  ∑  α∗ α∗ y ji ylk K ji,lk − 2 ji lk  ∑  α∗ = 2G(α∗ ). [sent-111, score-0.393]
</p><p>44 ji  (i, j)∈I  (i, j),(k,l)∈I  ˜ The last equality holds since I is a symmetric training set. [sent-112, score-0.371]
</p><p>45 2282  PAIRWISE SVM S AND L ARGE S CALE P ROBLEMS  Theorem 2 If I is a symmetric index set and if (8) holds, then any solution α of the optimization problem (9) leads to a symmetric pairwise decision function f : X × X → . [sent-120, score-0.748]
</p><p>46 Symmetric Training Sets Section 2 shows that one can use balanced kernels to obtain a symmetric pairwise decision function by means of a pairwise SVM. [sent-129, score-1.363]
</p><p>47 1 this can also be achieved by symmetric training sets. [sent-131, score-0.22]
</p><p>48 Now, we show in Theorem 3 that the decision function is the same, regardless whether a symmetric training set or a certain balanced kernel is used. [sent-132, score-0.565]
</p><p>49 This result is also of practical value, since the approach with balanced kernels leads to signiﬁcantly shorter training times (see the empirical results in Section 4. [sent-133, score-0.443]
</p><p>50 0 ≤ βi j ≤ 2C  ∑  for all (i, j) ∈ J  (10)  yi j βi j = 0  (i, j)∈J  with H(β) ≔  1 ˆ ∑ βi j βkl yi j ykl Ki j,kl − ∑ βi j 2 (i, j),(k,l)∈J (i, j)∈J  and  1 ˆ Ki j,kl ≔ Ki j,kl + K ji,kl , (11) 2 ˆ where K is an arbitrary pairwise kernel. [sent-138, score-0.726]
</p><p>51 The assumed symmetry of K yields ˆ (1) then K ˆ ˆ ˆ ˆ ˆ ˆ ˆ ˆ Ki j,kl = Ki j,lk = K ji,kl = K ji,lk = Kkl,i j = Klk,i j = Kkl, ji = Klk, ji . [sent-141, score-0.394]
</p><p>52 Note that (12) holds not only for kernels given by (11) but for any balanced kernel. [sent-142, score-0.326]
</p><p>53 For JR ≔ IR and JN ≔ J \ JR we deﬁne β by ji i ¯ βi j ≔  α∗j + α∗ if (i, j) ∈ JN , ji i α∗ if (i, j) ∈ JR . [sent-149, score-0.302]
</p><p>54 Then, by (11) and by α∗j = α∗ we obtain for i ji ¯ α∗j + α∗ βi j ji i ¯ ˆ βi j Ki j,kl = Ki j,kl + K ji,kl (Ki j,kl + K ji,kl ) = 2 2 = α∗j Ki j,kl + α∗ K ji,kl , i ji ¯ ii β ¯ ˆ βii Kii,kl = (Kii,kl + Kii,kl ) = α∗ Kii,kl . [sent-151, score-0.453]
</p><p>55 kk 2  This, (12), and ykl = ylk yield 2H(β∗ ) + 2  β∗j i  ∑ (i, j)∈J  =  ∑  β∗j yi j i  ∑  (i, j)∈J  =  β∗ ykl kl  (k,l)∈JN  1 β∗ yi j 2 (i,∑ i j j)∈J  ∑  1 ˆ 1 ˆ ˆ ˆ Ki j,kl + K ji,kl + ∑ β∗ ykk Ki j,kk + K ji,kk kk 2 2 (k,k)∈J R  ¯ αkl ykl Ki j,kl + K ji,kl  . [sent-159, score-1.036]
</p><p>56 (k,l)∈I  ¯ ¯ ¯ ¯ ¯ Then, the deﬁnition of α provides β∗j = αi j + α ji for (i, j) ∈ JN and αi j = α ji . [sent-160, score-0.302]
</p><p>57 Thus, i 2H(β∗ ) + 2  ∑ (i, j)∈J  β∗j = i  ∑ (i, j)∈I  ¯ αi j yi j  ∑ (k,l)∈I  ¯ αkl ykl Ki j,kl  ¯ = 2G(α) + 2  ∑  ¯ αi j  (i, j)∈I  ¯ ¯ follows. [sent-161, score-0.268]
</p><p>58 Note that in pairwise classiﬁcation the training points are the training pairs. [sent-172, score-0.603]
</p><p>59 If all possible training pairs are used, then the number of training pairs grows quadratically with the number m of training examples. [sent-173, score-0.378]
</p><p>60 1 we discuss how the costs for evaluating pairwise kernels, which can be expressed by standard kernels, can be drastically reduced. [sent-176, score-0.415]
</p><p>61 In Section 3 we discussed that one can either use balanced kernels or symmetric training sets to enforce the symmetry of a pairwise decision function. [sent-177, score-1.161]
</p><p>62 2 compares the needed training times of the approach with balanced kernels and the approach with symmetric training sets. [sent-180, score-0.64]
</p><p>63 1 Caching the Standard Kernel In this subsection balanced kernels are used to enforce the symmetry of the pairwise decision function. [sent-182, score-0.941]
</p><p>64 Today, this seems impossible for signiﬁcantly more than 125,250 training pairs as storing the (symmetric) kernel matrix for this number of pairs in double precision needs approximately 59GB. [sent-185, score-0.338]
</p><p>65 Note that training sets with 500 training examples already result in 125,250 training pairs. [sent-186, score-0.309]
</p><p>66 In general, it is possible to cache the standard kernel values for all training examples. [sent-192, score-0.254]
</p><p>67 Table 1 compares the training times with and without caching the standard kernel values. [sent-196, score-0.269]
</p><p>68 1) are used where each class is represented by 5 examples, KT L is chosen as pairwise kernel with a linear standard kernel, a cache size of 100MB is selected for caching pairwise kernel values, and all possible pairs are used for training. [sent-199, score-1.213]
</p><p>69 Symmetric Training Sets Theorem 3 shows that pairwise SVMs which use symmetric training sets and pairwise SVMs with balanced kernels lead to the same decision function. [sent-205, score-1.457]
</p><p>70 For symmetric training sets the number of training pairs is nearly doubled compared to the number in the case of balanced kernels. [sent-206, score-0.523]
</p><p>71 Simultaneously, (11) shows that evaluating a balanced kernel is computationally more expensive compared to the corresponding non balanced kernel. [sent-207, score-0.425]
</p><p>72 1) of dimension n = 500 are used where each class is represented by 5 examples, KT and its balanced version KT L with linear standard kernels are chosen as pairwise kernel, a cache size of 100MB is selected for caching the pairwise kernel values, and all possible pairs are used for training. [sent-211, score-1.436]
</p><p>73 It turns out, that the approach with balanced kernels is three to four times faster than using symmetric training sets. [sent-212, score-0.546]
</p><p>74 Number m of examples 500 1000 1500 2000 2500  Symmetric training set Balanced kernel (t in hh:mm) 0:03 0:46 3:26 9:44 23:15  0:01 0:17 0:56 2:58 6:20  Table 2: Training time for symmetric training sets and for balanced kernels  5. [sent-215, score-0.77]
</p><p>75 Classiﬁcation Experiments In this section we will present results of applying pairwise SVMs to one synthetic data set and to lin one real world data set. [sent-216, score-0.48]
</p><p>76 Those kernels denote KT L (5) with linear standard kernel and homogenous polynomial poly poly lin lin standard kernel of degree two, respectively. [sent-220, score-0.902]
</p><p>77 , 2004) will be used to measure the performance of a pairwise classiﬁer. [sent-224, score-0.415]
</p><p>78 For our measurements we selected n = 500 and tested all kernels in (4)–(7) with a linear standard kernel and a homogenous polynomial standard kernel of degree two, respectively. [sent-239, score-0.423]
</p><p>79 Any training set was generated in such a way that the set of classes in the training set is disjoint from the 2287  B RUNNER , F ISCHER , L UIG AND T HIES  1  1  lin KML 50 Classes lin KML 100 Classes lin KML 200 Classes poly KT M 50 Classes poly KT M 100 Classes poly KT M 200 Classes  0. [sent-241, score-0.981]
</p><p>80 6 FNMR  lin KML poly KML lin KT L poly KT L lin KT M poly KT M  0. [sent-248, score-0.756]
</p><p>81 1  1  FMR  (a) Different class numbers in training  (b) Different kernels for 200 classes in training  Figure 1: DET curves for double interval task set of classes in the test set. [sent-265, score-0.512]
</p><p>82 Here, we only present results for poly lin KML and KT M . [sent-275, score-0.252]
</p><p>83 Figure 1b shows the DET curves for different kernels where the training set consists of 200 classes. [sent-276, score-0.299]
</p><p>84 In particular, any of the pairwise kernels which uses a homogeneous polynomial of degree 2 as standard kernel leads to better results than its corresponding counterpart with a linear poly standard kernel. [sent-277, score-0.87]
</p><p>85 07 KT M leads to the best results, whereas for larger poly poly poly FMRs the DET curves of KML , KT L , and KT M intersect. [sent-279, score-0.601]
</p><p>86 2 Labeled Faces in the Wild In this subsection we will present results of applying pairwise SVMs to the labeled faces in the wild (LFW) data set (Huang et al. [sent-281, score-0.513]
</p><p>87 9  lin KML poly KML lin KT L poly KT L lin KT M poly KT M  0. [sent-300, score-0.756]
</p><p>88 Using all possible pairs of this partition for training and for testing, we obtained that a penalty poly parameter C of 1,000 is suitable. [sent-331, score-0.329]
</p><p>89 Moreover, for each used feature vector, the kernel KT M leads to the best results among all used kernels and also if sums of decision function values belonging to SIFT, LBP, and TPLBP feature vectors are used. [sent-332, score-0.349]
</p><p>90 However, if all pairs were used for training, then any training set would consist of approximately 50,000,000 pairs and the training would still need too much time. [sent-335, score-0.284]
</p><p>91 Hence, whereas in any training set all positive training pairs were used, the negative training pairs were randomly selected in such a way that any training set consists of 2,000,000 pairs. [sent-336, score-0.472]
</p><p>92 In Figure 2b we present the average DET curves obtained poly for KT M and feature vectors based on SIFT, LBP, and TPLBP. [sent-338, score-0.227]
</p><p>93 With 2289  B RUNNER , F ISCHER , L UIG AND T HIES  pairwise SVMs we achieved the same EER but a slightly higher SEM 0. [sent-353, score-0.415]
</p><p>94 Final Remarks In this paper we suggested the SVM framework for handling large pairwise classiﬁcation problems. [sent-395, score-0.415]
</p><p>95 Additionally, we showed that the approach based on balanced kernels leads to shorter training times. [sent-399, score-0.443]
</p><p>96 We discussed details of the implementation of a pairwise SVM solver and presented numerical results. [sent-400, score-0.415]
</p><p>97 Those results demonstrate that pairwise SVMs are capable of successfully treating large scale pairwise classiﬁcation problems. [sent-401, score-0.83]
</p><p>98 Furthermore, we showed that pairwise SVMs compete very well for a real world data set. [sent-402, score-0.415]
</p><p>99 We would like to underline that some of the discussed techniques could be transferred to other approaches for solving pairwise classiﬁcation problems. [sent-403, score-0.415]
</p><p>100 A new pairwise kernel for biological network inference with support vector machines. [sent-596, score-0.518]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pairwise', 0.415), ('kt', 0.359), ('kml', 0.243), ('ykl', 0.225), ('poly', 0.187), ('tplbp', 0.178), ('kernels', 0.165), ('ki', 0.164), ('balanced', 0.161), ('lbp', 0.161), ('ji', 0.151), ('symmetric', 0.126), ('jn', 0.123), ('eer', 0.121), ('hies', 0.121), ('ischer', 0.121), ('runner', 0.121), ('uig', 0.121), ('jr', 0.111), ('sift', 0.108), ('fmr', 0.104), ('fnmr', 0.104), ('kernel', 0.103), ('svms', 0.099), ('training', 0.094), ('symmetry', 0.092), ('sem', 0.089), ('cale', 0.089), ('brunner', 0.087), ('dresden', 0.087), ('interclass', 0.087), ('decision', 0.081), ('arge', 0.074), ('roblems', 0.074), ('kl', 0.074), ('caching', 0.072), ('luig', 0.069), ('lfw', 0.067), ('lin', 0.065), ('ir', 0.057), ('kk', 0.057), ('cache', 0.057), ('svm', 0.057), ('cached', 0.053), ('fischer', 0.053), ('cognitec', 0.052), ('kdl', 0.052), ('ylk', 0.052), ('wild', 0.051), ('kd', 0.051), ('det', 0.051), ('pairs', 0.048), ('faces', 0.047), ('double', 0.045), ('wolf', 0.043), ('yi', 0.043), ('huang', 0.041), ('august', 0.04), ('curves', 0.04), ('lk', 0.039), ('classi', 0.038), ('classes', 0.037), ('lkopf', 0.037), ('belong', 0.035), ('fmrs', 0.035), ('gamassi', 0.035), ('kds', 0.035), ('persons', 0.035), ('tenfold', 0.035), ('thies', 0.035), ('ykk', 0.035), ('vert', 0.034), ('sch', 0.033), ('afterwards', 0.032), ('duan', 0.03), ('art', 0.029), ('mm', 0.028), ('protocol', 0.028), ('andreas', 0.028), ('machines', 0.028), ('enforce', 0.027), ('url', 0.027), ('examples', 0.027), ('homogenous', 0.027), ('guillaumin', 0.027), ('hassner', 0.027), ('ojala', 0.027), ('rifkin', 0.027), ('smola', 0.026), ('multiclass', 0.025), ('measurements', 0.025), ('klaus', 0.025), ('hill', 0.025), ('hh', 0.025), ('tensor', 0.024), ('shorter', 0.023), ('libsvm', 0.023), ('obviously', 0.023), ('abernethy', 0.023), ('hertz', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="89-tfidf-1" href="./jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems.html">89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</a></p>
<p>Author: Carl Brunner, Andreas Fischer, Klaus Luig, Thorsten Thies</p><p>Abstract: Pairwise classiﬁcation is the task to predict whether the examples a, b of a pair (a, b) belong to the same class or to different classes. In particular, interclass generalization problems can be treated in this way. In pairwise classiﬁcation, the order of the two input examples should not affect the classiﬁcation result. To achieve this, particular kernels as well as the use of symmetric training sets in the framework of support vector machines were suggested. The paper discusses both approaches in a general way and establishes a strong connection between them. In addition, an efﬁcient implementation is discussed which allows the training of several millions of pairs. The value of these contributions is conﬁrmed by excellent results on the labeled faces in the wild benchmark. Keywords: pairwise support vector machines, interclass generalization, pairwise kernels, large scale problems</p><p>2 0.085604466 <a title="89-tfidf-2" href="./jmlr-2012-An_Active_Learning_Algorithm_for_Ranking_from_Pairwise_Preferences_with_an_Almost_Optimal_Query_Complexity.html">17 jmlr-2012-An Active Learning Algorithm for Ranking from Pairwise Preferences with an Almost Optimal Query Complexity</a></p>
<p>Author: Nir Ailon</p><p>Abstract: Given a set V of n elements we wish to linearly order them given pairwise preference labels which may be non-transitive (due to irrationality or arbitrary noise). The goal is to linearly order the elements while disagreeing with as few pairwise preference labels as possible. Our performance is measured by two parameters: The number of disagreements (loss) and the query complexity (number of pairwise preference labels). Our algorithm adaptively queries at most O(ε−6 n log5 n) preference labels for a regret of ε times the optimal loss. As a function of n, this is asymptotically better than standard (non-adaptive) learning bounds achievable for the same problem. Our main result takes us a step closer toward settling an open problem posed by learning-torank (from pairwise information) theoreticians and practitioners: What is a provably correct way to sample preference labels? To further show the power and practicality of our solution, we analyze a typical test case in which a large margin linear relaxation is used for efﬁciently solving the simpler learning problems in our decomposition. Keywords: statistical learning theory, active learning, ranking, pairwise ranking, preferences</p><p>3 0.082008779 <a title="89-tfidf-3" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>Author: Yiming Ying, Peng Li</p><p>Abstract: The main theme of this paper is to develop a novel eigenvalue optimization framework for learning a Mahalanobis metric. Within this context, we introduce a novel metric learning approach called DML-eig which is shown to be equivalent to a well-known eigenvalue optimization problem called minimizing the maximal eigenvalue of a symmetric matrix (Overton, 1988; Lewis and Overton, 1996). Moreover, we formulate LMNN (Weinberger et al., 2005), one of the state-of-the-art metric learning methods, as a similar eigenvalue optimization problem. This novel framework not only provides new insights into metric learning but also opens new avenues to the design of efﬁcient metric learning algorithms. Indeed, ﬁrst-order algorithms are developed for DML-eig and LMNN which only need the computation of the largest eigenvector of a matrix per iteration. Their convergence characteristics are rigorously established. Various experiments on benchmark data sets show the competitive performance of our new approaches. In addition, we report an encouraging result on a difﬁcult and challenging face veriﬁcation data set called Labeled Faces in the Wild (LFW). Keywords: metric learning, convex optimization, semi-deﬁnite programming, ﬁrst-order methods, eigenvalue optimization, matrix factorization, face veriﬁcation</p><p>4 0.080498174 <a title="89-tfidf-4" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>Author: Fei Yan, Josef Kittler, Krystian Mikolajczyk, Atif Tahir</p><p>Abstract: Sparsity-inducing multiple kernel Fisher discriminant analysis (MK-FDA) has been studied in the literature. Building on recent advances in non-sparse multiple kernel learning (MKL), we propose a non-sparse version of MK-FDA, which imposes a general ℓ p norm regularisation on the kernel weights. We formulate the associated optimisation problem as a semi-inﬁnite program (SIP), and adapt an iterative wrapper algorithm to solve it. We then discuss, in light of latest advances in MKL optimisation techniques, several reformulations and optimisation strategies that can potentially lead to signiﬁcant improvements in the efﬁciency and scalability of MK-FDA. We carry out extensive experiments on six datasets from various application areas, and compare closely the performance of ℓ p MK-FDA, ﬁxed norm MK-FDA, and several variants of SVM-based MKL (MK-SVM). Our results demonstrate that ℓ p MK-FDA improves upon sparse MK-FDA in many practical situations. The results also show that on image categorisation problems, ℓ p MK-FDA tends to outperform its SVM counterpart. Finally, we also discuss the connection between (MK-)FDA and (MK-)SVM, under the uniﬁed framework of regularised kernel machines. Keywords: multiple kernel learning, kernel ﬁsher discriminant analysis, regularised least squares, support vector machines</p><p>5 0.074500389 <a title="89-tfidf-5" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>Author: Francesco Orabona, Luo Jie, Barbara Caputo</p><p>Abstract: In recent years there has been a lot of interest in designing principled classiﬁcation algorithms over multiple cues, based on the intuitive notion that using more features should lead to better performance. In the domain of kernel methods, a principled way to use multiple features is the Multi Kernel Learning (MKL) approach. Here we present a MKL optimization algorithm based on stochastic gradient descent that has a guaranteed convergence rate. We directly solve the MKL problem in the primal formulation. By having a p-norm formulation of MKL, we introduce a parameter that controls the level of sparsity of the solution, while leading to an easier optimization problem. We prove theoretically and experimentally that 1) our algorithm has a faster convergence rate as the number of kernels grows; 2) the training complexity is linear in the number of training examples; 3) very few iterations are sufﬁcient to reach good solutions. Experiments on standard benchmark databases support our claims. Keywords: multiple kernel learning, learning kernels, online optimization, stochastic subgradient descent, convergence bounds, large scale</p><p>6 0.066843912 <a title="89-tfidf-6" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>7 0.060739636 <a title="89-tfidf-7" href="./jmlr-2012-Algorithms_for_Learning_Kernels_Based_on_Centered_Alignment.html">16 jmlr-2012-Algorithms for Learning Kernels Based on Centered Alignment</a></p>
<p>8 0.059797443 <a title="89-tfidf-8" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>9 0.057077903 <a title="89-tfidf-9" href="./jmlr-2012-Feature_Selection_via_Dependence_Maximization.html">44 jmlr-2012-Feature Selection via Dependence Maximization</a></p>
<p>10 0.056989126 <a title="89-tfidf-10" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>11 0.051272701 <a title="89-tfidf-11" href="./jmlr-2012-PAC-Bayes_Bounds_with_Data_Dependent_Priors.html">87 jmlr-2012-PAC-Bayes Bounds with Data Dependent Priors</a></p>
<p>12 0.049219977 <a title="89-tfidf-12" href="./jmlr-2012-Refinement_of_Operator-valued_Reproducing_Kernels.html">96 jmlr-2012-Refinement of Operator-valued Reproducing Kernels</a></p>
<p>13 0.044285927 <a title="89-tfidf-13" href="./jmlr-2012-Static_Prediction_Games_for_Adversarial_Learning_Problems.html">110 jmlr-2012-Static Prediction Games for Adversarial Learning Problems</a></p>
<p>14 0.043592285 <a title="89-tfidf-14" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>15 0.041711543 <a title="89-tfidf-15" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>16 0.040860213 <a title="89-tfidf-16" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>17 0.037152503 <a title="89-tfidf-17" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>18 0.035541113 <a title="89-tfidf-18" href="./jmlr-2012-On_the_Convergence_Rate_oflp-Norm_Multiple_Kernel_Learning.html">81 jmlr-2012-On the Convergence Rate oflp-Norm Multiple Kernel Learning</a></p>
<p>19 0.03545256 <a title="89-tfidf-19" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>20 0.033345278 <a title="89-tfidf-20" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.168), (1, 0.001), (2, 0.114), (3, 0.175), (4, 0.07), (5, -0.015), (6, -0.044), (7, 0.059), (8, -0.032), (9, 0.062), (10, 0.003), (11, -0.034), (12, -0.095), (13, -0.13), (14, 0.073), (15, 0.088), (16, -0.015), (17, 0.062), (18, 0.073), (19, 0.023), (20, -0.003), (21, -0.002), (22, 0.045), (23, 0.001), (24, 0.172), (25, -0.011), (26, -0.111), (27, -0.196), (28, 0.195), (29, 0.168), (30, -0.108), (31, 0.123), (32, 0.031), (33, 0.125), (34, -0.166), (35, 0.052), (36, -0.043), (37, -0.193), (38, -0.197), (39, 0.006), (40, 0.054), (41, 0.173), (42, 0.152), (43, -0.026), (44, -0.014), (45, 0.06), (46, 0.05), (47, -0.065), (48, -0.044), (49, -0.107)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96071088 <a title="89-lsi-1" href="./jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems.html">89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</a></p>
<p>Author: Carl Brunner, Andreas Fischer, Klaus Luig, Thorsten Thies</p><p>Abstract: Pairwise classiﬁcation is the task to predict whether the examples a, b of a pair (a, b) belong to the same class or to different classes. In particular, interclass generalization problems can be treated in this way. In pairwise classiﬁcation, the order of the two input examples should not affect the classiﬁcation result. To achieve this, particular kernels as well as the use of symmetric training sets in the framework of support vector machines were suggested. The paper discusses both approaches in a general way and establishes a strong connection between them. In addition, an efﬁcient implementation is discussed which allows the training of several millions of pairs. The value of these contributions is conﬁrmed by excellent results on the labeled faces in the wild benchmark. Keywords: pairwise support vector machines, interclass generalization, pairwise kernels, large scale problems</p><p>2 0.48438537 <a title="89-lsi-2" href="./jmlr-2012-An_Active_Learning_Algorithm_for_Ranking_from_Pairwise_Preferences_with_an_Almost_Optimal_Query_Complexity.html">17 jmlr-2012-An Active Learning Algorithm for Ranking from Pairwise Preferences with an Almost Optimal Query Complexity</a></p>
<p>Author: Nir Ailon</p><p>Abstract: Given a set V of n elements we wish to linearly order them given pairwise preference labels which may be non-transitive (due to irrationality or arbitrary noise). The goal is to linearly order the elements while disagreeing with as few pairwise preference labels as possible. Our performance is measured by two parameters: The number of disagreements (loss) and the query complexity (number of pairwise preference labels). Our algorithm adaptively queries at most O(ε−6 n log5 n) preference labels for a regret of ε times the optimal loss. As a function of n, this is asymptotically better than standard (non-adaptive) learning bounds achievable for the same problem. Our main result takes us a step closer toward settling an open problem posed by learning-torank (from pairwise information) theoreticians and practitioners: What is a provably correct way to sample preference labels? To further show the power and practicality of our solution, we analyze a typical test case in which a large margin linear relaxation is used for efﬁciently solving the simpler learning problems in our decomposition. Keywords: statistical learning theory, active learning, ranking, pairwise ranking, preferences</p><p>3 0.43352842 <a title="89-lsi-3" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>Author: Grigorios Skolidis, Guido Sanguinetti</p><p>Abstract: We propose a novel model for meta-generalisation, that is, performing prediction on novel tasks based on information from multiple different but related tasks. The model is based on two coupled Gaussian processes with structured covariance function; one model performs predictions by learning a constrained covariance function encapsulating the relations between the various training tasks, while the second model determines the similarity of new tasks to previously seen tasks. We demonstrate empirically on several real and synthetic data sets both the strengths of the approach and its limitations due to the distributional assumptions underpinning it. Keywords: transfer learning, meta-generalising, multi-task learning, Gaussian processes, mixture of experts</p><p>4 0.38744462 <a title="89-lsi-4" href="./jmlr-2012-Algorithms_for_Learning_Kernels_Based_on_Centered_Alignment.html">16 jmlr-2012-Algorithms for Learning Kernels Based on Centered Alignment</a></p>
<p>Author: Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difﬁcult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classiﬁcation and regression. Our algorithms are based on the notion of centered alignment which is used as a similarity measure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efﬁcient algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization. Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classiﬁcation and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classiﬁcation and regression. Keywords: kernel methods, learning kernels, feature selection</p><p>5 0.35101378 <a title="89-lsi-5" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>Author: Yiming Ying, Peng Li</p><p>Abstract: The main theme of this paper is to develop a novel eigenvalue optimization framework for learning a Mahalanobis metric. Within this context, we introduce a novel metric learning approach called DML-eig which is shown to be equivalent to a well-known eigenvalue optimization problem called minimizing the maximal eigenvalue of a symmetric matrix (Overton, 1988; Lewis and Overton, 1996). Moreover, we formulate LMNN (Weinberger et al., 2005), one of the state-of-the-art metric learning methods, as a similar eigenvalue optimization problem. This novel framework not only provides new insights into metric learning but also opens new avenues to the design of efﬁcient metric learning algorithms. Indeed, ﬁrst-order algorithms are developed for DML-eig and LMNN which only need the computation of the largest eigenvector of a matrix per iteration. Their convergence characteristics are rigorously established. Various experiments on benchmark data sets show the competitive performance of our new approaches. In addition, we report an encouraging result on a difﬁcult and challenging face veriﬁcation data set called Labeled Faces in the Wild (LFW). Keywords: metric learning, convex optimization, semi-deﬁnite programming, ﬁrst-order methods, eigenvalue optimization, matrix factorization, face veriﬁcation</p><p>6 0.34753698 <a title="89-lsi-6" href="./jmlr-2012-Static_Prediction_Games_for_Adversarial_Learning_Problems.html">110 jmlr-2012-Static Prediction Games for Adversarial Learning Problems</a></p>
<p>7 0.34701917 <a title="89-lsi-7" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>8 0.33169872 <a title="89-lsi-8" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>9 0.31648156 <a title="89-lsi-9" href="./jmlr-2012-PAC-Bayes_Bounds_with_Data_Dependent_Priors.html">87 jmlr-2012-PAC-Bayes Bounds with Data Dependent Priors</a></p>
<p>10 0.31144968 <a title="89-lsi-10" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>11 0.30595195 <a title="89-lsi-11" href="./jmlr-2012-Feature_Selection_via_Dependence_Maximization.html">44 jmlr-2012-Feature Selection via Dependence Maximization</a></p>
<p>12 0.29741991 <a title="89-lsi-12" href="./jmlr-2012-Refinement_of_Operator-valued_Reproducing_Kernels.html">96 jmlr-2012-Refinement of Operator-valued Reproducing Kernels</a></p>
<p>13 0.29312214 <a title="89-lsi-13" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>14 0.29022169 <a title="89-lsi-14" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>15 0.28142187 <a title="89-lsi-15" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>16 0.26826739 <a title="89-lsi-16" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>17 0.2504015 <a title="89-lsi-17" href="./jmlr-2012-Large-scale_Linear_Support_Vector_Regression.html">54 jmlr-2012-Large-scale Linear Support Vector Regression</a></p>
<p>18 0.23892684 <a title="89-lsi-18" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>19 0.22846089 <a title="89-lsi-19" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>20 0.20192526 <a title="89-lsi-20" href="./jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.015), (21, 0.022), (26, 0.032), (29, 0.016), (35, 0.018), (49, 0.014), (56, 0.018), (59, 0.419), (64, 0.031), (69, 0.015), (75, 0.126), (77, 0.035), (92, 0.038), (96, 0.116)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91626757 <a title="89-lda-1" href="./jmlr-2012-MULTIBOOST%3A_A_Multi-purpose_Boosting_Package.html">62 jmlr-2012-MULTIBOOST: A Multi-purpose Boosting Package</a></p>
<p>Author: Djalel Benbouzid, Róbert Busa-Fekete, Norman Casagrande, François-David Collin, Balázs Kégl</p><p>Abstract: The M ULTI B OOST package provides a fast C++ implementation of multi-class/multi-label/multitask boosting algorithms. It is based on A DA B OOST.MH but it also implements popular cascade classiﬁers and F ILTER B OOST. The package contains common multi-class base learners (stumps, trees, products, Haar ﬁlters). Further base learners and strong learners following the boosting paradigm can be easily implemented in a ﬂexible framework. Keywords: boosting, A DA B OOST.MH, F ILTER B OOST, cascade classiﬁer</p><p>same-paper 2 0.73948717 <a title="89-lda-2" href="./jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems.html">89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</a></p>
<p>Author: Carl Brunner, Andreas Fischer, Klaus Luig, Thorsten Thies</p><p>Abstract: Pairwise classiﬁcation is the task to predict whether the examples a, b of a pair (a, b) belong to the same class or to different classes. In particular, interclass generalization problems can be treated in this way. In pairwise classiﬁcation, the order of the two input examples should not affect the classiﬁcation result. To achieve this, particular kernels as well as the use of symmetric training sets in the framework of support vector machines were suggested. The paper discusses both approaches in a general way and establishes a strong connection between them. In addition, an efﬁcient implementation is discussed which allows the training of several millions of pairs. The value of these contributions is conﬁrmed by excellent results on the labeled faces in the wild benchmark. Keywords: pairwise support vector machines, interclass generalization, pairwise kernels, large scale problems</p><p>3 0.40155137 <a title="89-lda-3" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel</p><p>Abstract: The success of many machine learning and pattern recognition methods relies heavily upon the identiﬁcation of an appropriate distance metric on the input data. It is often beneﬁcial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semideﬁnite. Semideﬁnite programming is often used to enforce this constraint, but does not scale well and is not easy to implement. B OOST M ETRIC is instead based on the observation that any positive semideﬁnite matrix can be decomposed into a linear combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting methods are easy to implement, efﬁcient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semideﬁnite matrix with trace and rank being one rather than a classiﬁer or regressor. Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. Keywords: Mahalanobis distance, semideﬁnite programming, column generation, boosting, Lagrange duality, large margin nearest neighbor</p><p>4 0.39054817 <a title="89-lda-4" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>Author: Francesco Orabona, Luo Jie, Barbara Caputo</p><p>Abstract: In recent years there has been a lot of interest in designing principled classiﬁcation algorithms over multiple cues, based on the intuitive notion that using more features should lead to better performance. In the domain of kernel methods, a principled way to use multiple features is the Multi Kernel Learning (MKL) approach. Here we present a MKL optimization algorithm based on stochastic gradient descent that has a guaranteed convergence rate. We directly solve the MKL problem in the primal formulation. By having a p-norm formulation of MKL, we introduce a parameter that controls the level of sparsity of the solution, while leading to an easier optimization problem. We prove theoretically and experimentally that 1) our algorithm has a faster convergence rate as the number of kernels grows; 2) the training complexity is linear in the number of training examples; 3) very few iterations are sufﬁcient to reach good solutions. Experiments on standard benchmark databases support our claims. Keywords: multiple kernel learning, learning kernels, online optimization, stochastic subgradient descent, convergence bounds, large scale</p><p>5 0.38188297 <a title="89-lda-5" href="./jmlr-2012-Feature_Selection_via_Dependence_Maximization.html">44 jmlr-2012-Feature Selection via Dependence Maximization</a></p>
<p>Author: Le Song, Alex Smola, Arthur Gretton, Justin Bedo, Karsten Borgwardt</p><p>Abstract: We introduce a framework for feature selection based on dependence maximization between the selected features and the labels of an estimation problem, using the Hilbert-Schmidt Independence Criterion. The key idea is that good features should be highly dependent on the labels. Our approach leads to a greedy procedure for feature selection. We show that a number of existing feature selectors are special cases of this framework. Experiments on both artiﬁcial and real-world data show that our feature selector works well in practice. Keywords: kernel methods, feature selection, independence measure, Hilbert-Schmidt independence criterion, Hilbert space embedding of distribution</p><p>6 0.38161939 <a title="89-lda-6" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>7 0.37478203 <a title="89-lda-7" href="./jmlr-2012-Static_Prediction_Games_for_Adversarial_Learning_Problems.html">110 jmlr-2012-Static Prediction Games for Adversarial Learning Problems</a></p>
<p>8 0.37005001 <a title="89-lda-8" href="./jmlr-2012-Activized_Learning%3A_Transforming_Passive_to_Active_with_Improved_Label_Complexity.html">14 jmlr-2012-Activized Learning: Transforming Passive to Active with Improved Label Complexity</a></p>
<p>9 0.3670333 <a title="89-lda-9" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>10 0.36002529 <a title="89-lda-10" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>11 0.3509956 <a title="89-lda-11" href="./jmlr-2012-Breaking_the_Curse_of_Kernelization%3A_Budgeted_Stochastic_Gradient_Descent_for_Large-Scale_SVM_Training.html">23 jmlr-2012-Breaking the Curse of Kernelization: Budgeted Stochastic Gradient Descent for Large-Scale SVM Training</a></p>
<p>12 0.35098869 <a title="89-lda-12" href="./jmlr-2012-Large-scale_Linear_Support_Vector_Regression.html">54 jmlr-2012-Large-scale Linear Support Vector Regression</a></p>
<p>13 0.35020119 <a title="89-lda-13" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>14 0.34875661 <a title="89-lda-14" href="./jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</a></p>
<p>15 0.34797591 <a title="89-lda-15" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>16 0.34708947 <a title="89-lda-16" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>17 0.34686881 <a title="89-lda-17" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>18 0.34680712 <a title="89-lda-18" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>19 0.34023419 <a title="89-lda-19" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>20 0.34014171 <a title="89-lda-20" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
