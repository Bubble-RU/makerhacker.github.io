<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>116 jmlr-2012-Transfer in Reinforcement Learning via Shared Features</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-116" href="#">jmlr2012-116</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>116 jmlr-2012-Transfer in Reinforcement Learning via Shared Features</h1>
<br/><p>Source: <a title="jmlr-2012-116-pdf" href="http://jmlr.org/papers/volume13/konidaris12a/konidaris12a.pdf">pdf</a></p><p>Author: George Konidaris, Ilya Scheidwasser, Andrew Barto</p><p>Abstract: We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to signiﬁcantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that signiﬁcantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-speciﬁc skills. Keywords: reinforcement learning, transfer, shaping, skills</p><p>Reference: <a title="jmlr-2012-116-reference" href="../jmlr2012_reference/jmlr-2012-Transfer_in_Reinforcement_Learning_via_Shared_Features_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to signiﬁcantly improve performance in a later related task, even given a very brief training period. [sent-11, score-1.398]
</p><p>2 We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that signiﬁcantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-speciﬁc skills. [sent-12, score-1.19]
</p><p>3 Although reinforcement learning researchers study algorithms for improving task performance with experience, we do not yet understand how to effectively transfer learned skills and knowledge from one problem setting to another. [sent-16, score-0.629]
</p><p>4 In this paper we present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features and that transfer can take place through functions deﬁned only over those shared features. [sent-20, score-0.92]
</p><p>5 1 Reinforcement Learning Reinforcement learning (Sutton and Barto, 1998) is a machine learning paradigm where an agent attempts to learn how to maximize a numerical reward signal over time in a given environment. [sent-30, score-0.643]
</p><p>6 As a reinforcement learning agent interacts with its environment, it receives a reward (or sometimes incurs a cost) for each action taken. [sent-31, score-0.805]
</p><p>7 If the reward received by the agent at time k is denoted rk , we denote this cumulative reward (termed return) from time t as Rt = ∑∞ γi rt+i+1 , where 0 < γ ≤ 1 is a discount factor that expresses the extent to which the agent i=0 prefers immediate reward over delayed reward. [sent-35, score-1.38]
</p><p>8 Given a policy π mapping states to actions, a reinforcement learning agent may learn a value function, V , mapping states to expected return. [sent-36, score-0.741]
</p><p>9 The options framework provides methods for learning and planning using options as temporally extended actions in the standard reinforcement learning framework (Sutton and Barto, 1998). [sent-60, score-0.826]
</p><p>10 Policy learning is usually performed by an off-policy reinforcement learning algorithm so that the agent can update many options simultaneously after taking an action (Sutton et al. [sent-62, score-0.913]
</p><p>11 Since transfer hinges on the tasks being related, the nature of that relationship will deﬁne how transfer can take place. [sent-82, score-0.7]
</p><p>12 Related Tasks Share Common Features Successful transfer requires an agent that must solve a sequence of tasks that are related but distinct— different, but not so different that experience in one is irrelevant to experience in another. [sent-88, score-0.998]
</p><p>13 A sequence of tasks must be (at least approximately) reward-linked if we aim to transfer information about the optimal value function: if the reward functions in two tasks use different sensors then there is no reason to hope that their value functions contain useful information about each other. [sent-130, score-0.804]
</p><p>14 If learning to solve each individual task is possible and feasible in agent-space, then transfer is trivial: the tasks are effectively a single task and we can learn a single policy in agent-space for all tasks. [sent-137, score-0.663]
</p><p>15 Section 4 shows that an agent can transfer value-functions learned in agent-space to signiﬁcantly decrease the time taken to ﬁnd an initial solution to a task, given experience in a sequence of related and reward-linked tasks. [sent-140, score-0.826]
</p><p>16 In Section 5 we show that an agent can learn portable high-level skills directly in agent-space which can dramatically improve task performance, given experience in a sequence of related tasks. [sent-141, score-0.787]
</p><p>17 We empirically demonstrate the effects of knowledge transfer using a relatively simple demonstration domain (a rod positioning task with an artiﬁcial agent space) and a more challenging domain (Keepaway). [sent-145, score-0.828]
</p><p>18 1985), so that the agent can safely learn easier versions of the same task and use the resulting policy to speed learning as the task becomes more complex. [sent-152, score-0.629]
</p><p>19 2 Unfortunately, this type of shaping does not generally transfer between tasks— it can only be used to gently introduce an agent to a single task, and is therefore not suited to a sequence of distinct tasks. [sent-153, score-0.95]
</p><p>20 Alternatively, the agent’s reward function could be augmented through the use of intermediate shaping rewards or “progress indicators” (Matari´ , 1997) that provide an augmented (and hopefully c more informative) reinforcement signal to the agent. [sent-154, score-0.703]
</p><p>21 This has the effect of shortening the reward horizon of the problem—the number of correct actions the agent must execute before receiving a useful reward signal (Laud and DeJong, 2003). [sent-155, score-0.845]
</p><p>22 (1999) proved that an arbitrary externally speciﬁed reward function could be included as a potential-based shaping function in a reinforcement learning system without modifying its optimal policy. [sent-157, score-0.658]
</p><p>23 In the following sections we show that an agent can learn its own shaping function from experience across several related, reward-linked tasks without having it speciﬁed in advance. [sent-165, score-0.971]
</p><p>24 Alternatively, once an agent has completed some task S j and has learned a good j j approximation of the value of each state using V j , it can use its (di , vi ) pairs as training examples for a supervised learning algorithm to learn L. [sent-181, score-0.639]
</p><p>25 Thus, when facing a new task Mk , the agent can use L to provide a good initial estimate for Vk that can be reﬁned using a standard reinforcement learning algorithm. [sent-184, score-0.654]
</p><p>26 3 A Rod Positioning Experiment In this section we empirically evaluate the potential beneﬁts of a learned shaping function in a rod positioning task (Moore and Atkeson, 1993), where we add a simple artiﬁcial agent space that can be easily manipulated for experimental purposes. [sent-187, score-0.885]
</p><p>27 The agent receives a reward of −1 for each action, and a reward of 1000 when reaching the goal (whereupon the current episode ends). [sent-195, score-0.886]
</p><p>28 Since these beacons are present in every task, the sensor readings are an agent-space and we include an element in the agent that learns L and uses it to predict reward for each adjacent state given the ﬁve signal levels present there. [sent-198, score-0.709]
</p><p>29 The usefulness of L as a reward predictor will depend on the relationship between beacon placement and reward across a sequence of individual rod positioning tasks. [sent-199, score-0.628]
</p><p>30 01) in problem-space and used training tasks that were either 10x10 (where it was given 100 episodes to converge in each training task), or 15x15 (when it was given 150 episodes to converge), and tested in a 40x40 task. [sent-212, score-0.718]
</p><p>31 Figure 3(a) shows the number of steps (averaged over 50 runs) required to ﬁrst reach the goal in the test task, against the number of training tasks completed by the agent for the four types of learned shaping elements (linear and quadratic L, and either 10x10 or 15x15 training tasks). [sent-223, score-1.022]
</p><p>32 This illustrates the difference in overall learning performance on a single new task between agents that have had many training experiences and agents that have 3. [sent-233, score-0.906]
</p><p>33 It also shows that the number of episodes required for convergence is roughly the same as that of an agent using a uniformly optimistic value table initialization of 500, and slightly longer than that of an agent using a uniformly pessimistic value table initialization of 0. [sent-242, score-1.032]
</p><p>34 4  11  x 10  Uni0 Uni500 10Lin 10Quad 15Lin 15Quad  10  9  8  Steps to Goal  7  6  5  4  3  2  1  0  0  20  40  60  80  100  120  140  160  180  Episodes in the Test Task  (b) Steps to reward against episodes in the homing test task for agents that have completed 20 training tasks. [sent-255, score-0.946]
</p><p>35 This suggests that extra information in the agent-space more than makes up for a shaping function being difﬁcult to accurately represent—in all cases the performance of agents learning using the triangle beacon arrangement is better than that of those learning using the homing beacon arrangement. [sent-260, score-0.847]
</p><p>36 4  11  x 10  Uni0 Uni500 10Lin 10Quad 15Lin 15Quad  10  9  8  Steps to Goal  7  6  5  4  3  2  1  0  0  20  40  60  80  100  120  140  160  Episodes in the Test Task  (b) Steps to reward against episodes in the triangle test task for agents that have completed 20 training tasks. [sent-276, score-0.919]
</p><p>37 5  0  0  20  40  60  80  100  120  140  160  Episodes in the Test Task  (b) Steps to reward against episodes in the test task for agents that have completed 20 training task episodes using a noisy signal. [sent-297, score-1.242]
</p><p>38 We see that, although these agents often do worse than learning from scratch in the ﬁrst episode, they subsequently do better when η < 1, and again converge at roughly the same rate as agents that use an optimistic initial value function. [sent-304, score-0.726]
</p><p>39 However, the possible performance penalty for high η is more severe—an agent using a learned shaping function that rewards it for following a beacon signal may take nearly four times as long to ﬁrst solve the test problem when that feature becomes random (at η = 1). [sent-311, score-0.864]
</p><p>40 Again, however, when η < 1 the agents recover after their ﬁrst episode to outperform agents that learn from scratch within the ﬁrst few episodes. [sent-312, score-0.816]
</p><p>41 5 S UMMARY The ﬁrst two experiments above show that an agent able to learn its own shaping rewards through training can use even a few training experiences to signiﬁcantly improve its initial policy in a novel task. [sent-315, score-0.996]
</p><p>42 They also show that such training results in agents with convergence characteristics similar to that of agents using uniformly optimistic initial value functions. [sent-316, score-0.74]
</p><p>43 Thus, an agent that learns its own shaping rewards can improve its initial speed at solving a task when compared to an agent that cannot, but it will not converge to an approximately optimal policy in less time (as measured in episodes). [sent-317, score-1.243]
</p><p>44 Beyond that, however, agents may experience negative transfer where either noisy features or an imperfect or approximate set of agent-space features result in poor learned shaping functions. [sent-321, score-1.083]
</p><p>45 5  0  0  20  40  60  80  100  120  140  160  Episodes in the Test Task  (b) Steps to reward against episodes in the test task for agents that have completed 20 training task episodes using features with imperfectly preserved semantics. [sent-344, score-1.261]
</p><p>46 We use Keepaway to illustrate the use of learned shaping rewards on a standard but challenging benchmark that has been used in other transfer studies (Taylor et al. [sent-350, score-0.635]
</p><p>47 Figure 9 shows sample learning curves for agents learning from scratch and agents using transferred knowledge from 5000 episodes of 3v2 Keepaway, demonstrating that agents that transfer knowledge start with better policies and learn faster. [sent-439, score-1.715]
</p><p>48 5 Discussion The results presented above suggest that agents that employ reinforcement learning methods can be augmented to use their experience to learn their own shaping rewards. [sent-447, score-0.928]
</p><p>49 It also creates the possibility of training such agents on easy tasks as a way of equipping them with knowledge that will make harder tasks tractable, and is thus an instance of an autonomous developmental learning system (Weng et al. [sent-449, score-0.702]
</p><p>50 In some situations, the learning algorithm chosen to learn the shaping function, or the sensory patterns given to it, might result in an agent that is completely unable to learn anything useful. [sent-451, score-0.742]
</p><p>51 We do not expect such an agent to do much worse than one without any shaping rewards at all. [sent-452, score-0.7]
</p><p>52 Given that an agent facing a sequence of tasks receives many example transitions between pairs of agentspace descriptors, it may prove efﬁcient to instead learn an approximate transition model in agentspace and then use that model to obtain a shaping function via planning. [sent-458, score-0.91]
</p><p>53 In reinforcement learning the state space is searched by the agent itself, but its initial value function (either directly or via a shaping function) acts to order the selection of unvisited nodes by the agent. [sent-461, score-0.91]
</p><p>54 Therefore, we argue that reinforcement learning agents using non-uniform initial value functions are using something very similar to a heuristic, and those that are able to learn their own portable shaping functions are in effect able to learn their own heuristics. [sent-462, score-1.03]
</p><p>55 Although this can lead to faster learning on later tasks in the same state space, learned options would be more useful if they could be reused in later tasks that are related but have distinct state spaces. [sent-550, score-0.757]
</p><p>56 In this section we demonstrate empirically that an agent that learns portable options directly in agent-space can reuse those options in future related tasks to signiﬁcantly improve performance. [sent-551, score-1.295]
</p><p>57 The agent is also either given, or learns, a set of higher-level options to reduce the time required j to solve the task. [sent-563, score-0.698]
</p><p>58 Although the agent will be learning task and option policies in different spaces, both types of policies can be updated simultaneously as the agent receives both agent-space and problem-space descriptors at each state. [sent-567, score-1.036]
</p><p>59 To support learning a portable shaping function, an agent space should contain some features that are correlated to return across tasks. [sent-568, score-0.857]
</p><p>60 Five pieces of data form a problem-space descriptor for any lightworld instance: the current room number, the x and y coordinates of the agent in that room, whether or not the agent has the key, and whether or not the door is open. [sent-593, score-1.012]
</p><p>61 1 T YPES OF AGENT We used ﬁve types of reinforcement learning agents: agents without options, agents with problemspace options, agents with perfect problem-space options, agents with agent-space options, and agents with both option types. [sent-598, score-1.996]
</p><p>62 The agents without options used Sarsa(λ) with ε-greedy action selection (α = 0. [sent-599, score-0.693]
</p><p>63 Agents with perfect problem-space options were given options with pre-learned policies for each salient event, though they still performed option updates and were otherwise identical to the standard agent with options. [sent-612, score-1.154]
</p><p>64 Finally, agents with both types of options were included to represent agents that learn both general portable and speciﬁc non-portable skills simultaneously. [sent-624, score-1.23]
</p><p>65 To evaluate the performance of agent-space options as the agents gained more experience, we similarly obtained 1000 lightworld samples and test tasks, but for each test task we ran the agents once without training and then with between 1 and 10 training experiences. [sent-632, score-1.235]
</p><p>66 Each training experience for a test lightworld task consisted of 100 episodes in a training lightworld randomly selected from the remaining 99. [sent-633, score-0.685]
</p><p>67 Although the agents updated their options during evaluation in the test lightworld, these updates were discarded before the next training experience so the agent-space options never received prior training in the test lightworld. [sent-634, score-1.118]
</p><p>68 3 R ESULTS Figure 11(a) shows average learning curves for agents employing problem-space options, and Figure 11(b) shows the same for agents employing agent-space options. [sent-637, score-0.71]
</p><p>69 The ﬁrst time an agent-space option agent encounters a lightworld, it performs similarly to an agent without options (as evidenced by the two topmost learning curves in each ﬁgure), but its performance rapidly improves with experience in other lightworlds. [sent-638, score-1.272]
</p><p>70 (PO), agent-space options with 0-10 training experiences (dark bars), and both option types with 0-10 training experiences (light bars). [sent-644, score-0.672]
</p><p>71 4 The ﬁrst time such agents encounter a lightworld, they perform as well as agents using problem-space 4. [sent-647, score-0.688]
</p><p>72 This explains why agents using only agent-space options and no training experiences perform more like agents without options than like agents with problem-space options. [sent-652, score-1.795]
</p><p>73 Second, options learned in our problem-space can represent exact solutions to speciﬁc subgoals, whereas options learned in our agent-space are general and must be approximated, and are therefore likely to be slightly less efﬁcient for any speciﬁc subgoal. [sent-653, score-0.714]
</p><p>74 This explains why agents using both types of options perform better in the long run than agents using only agent-space options. [sent-654, score-0.996]
</p><p>75 Figure 11(d) shows the mean total number of steps required over 70 episodes for agents using no options, problem-space options, perfect options, agent-space options, and both option types. [sent-655, score-0.72]
</p><p>76 It also clearly shows that agents using both types of options do consistently better than those using agent-space options alone. [sent-657, score-0.96]
</p><p>77 3 The Conveyor Belt Domain In the previous section we showed that an agent can use experience in related tasks to learn portable options, and that those options can improve performance in later tasks, when the agent has a highdimensional agent-space. [sent-663, score-1.503]
</p><p>78 , 2000), it did not occur during the same number of samples obtained for agents with agent-space options only. [sent-674, score-0.652]
</p><p>79 We ran experiments where the agents learned three options: one to move the current object to the bin at the end of the belt it is currently on, one for moving it to the belt above it, and one for moving it to the belt below it. [sent-683, score-0.739]
</p><p>80 We used the same agent types and experimental structure as before, except that the agent-space options did not use function approximation. [sent-684, score-0.698]
</p><p>81 1 R ESULTS Figures 13(a), 13(b) and 13(c) show learning curves for agents employing no options, problemspace options and perfect options; agents employing agent-space options; and agents employing both types of options, respectively. [sent-687, score-1.394]
</p><p>82 Figure 13(b) shows that the agents with agent-space options and no prior experience initially improve quickly but eventually obtain lower quality solutions than agents with problem-space options (Figure 13(a)). [sent-688, score-1.396]
</p><p>83 One or two training experiences result in roughly the same curve as agents using problem-space options, but by 5 training experiences the agent-space options are a signiﬁcant improvement (although due to their limited range they are never as good as perfect options). [sent-689, score-0.978]
</p><p>84 This initial dip relatively to agents with no prior experience is probably due to the limited range of the agent-space options (due to the limited range of the camera) and the fact that they are only locally Markov, even for their own subgoals. [sent-690, score-0.763]
</p><p>85 tions (NO), learned problem-space options (LO), perfect options (PO), agent-space options with 0-10 training experiences (dark bars), and both option types with 0-10 training experiences (light bars). [sent-694, score-1.369]
</p><p>86 Figure 13(c) shows that agents with both option types do not experience this initial dip relative to agents with no prior experience and outperform problem-space options immediately, most likely because the agent-space options are able to generalise across belts. [sent-696, score-1.6]
</p><p>87 4 Summary Our results show that options learned in agent-space can be successfully transferred between related tasks, and that this signiﬁcantly improves performance in sequences of tasks where the agent space cannot be used for learning directly. [sent-702, score-0.929]
</p><p>88 Our results suggest that when the agent space is large but can support global policies, experience in related tasks can eventually result in options that perform as well as perfect problem-speciﬁc options. [sent-703, score-0.97]
</p><p>89 When the agent space is only locally Markov, learned portable options will improve performance but are unlikely to reach the performance of perfect problem-speciﬁc options due to their limited range. [sent-704, score-1.251]
</p><p>90 In such situations, learning both problem-speciﬁc and agent space options simultaneously will likely obtain better performance than either individually. [sent-706, score-0.698]
</p><p>91 Konidaris and Hayes (2004) describe a similar method to ours that uses training tasks to learn associations between reward and strong signals at reward states, resulting in a signiﬁcant improvement in the total reward obtained by a simulated robot learning to ﬁnd a puck in a novel maze. [sent-745, score-0.859]
</p><p>92 2 learn options in the same state space in which the agent is performing reinforcement learning, and thus the options can only be reused for the same problem or for a new problem in the same space. [sent-759, score-1.275]
</p><p>93 Discussion The work in the preceding sections has shown that both knowledge and skill transfer can be effected across a sequence of tasks through the use of features common to all tasks in the sequence. [sent-778, score-0.662]
</p><p>94 When learning portable shaping functions, if the action space differs across tasks then we can simply learn shaping functions deﬁned over states only (as potential-based shaping functions were originally deﬁned by Ng et al. [sent-791, score-1.259]
</p><p>95 Although we expect that learning using portable state-only shaping functions will not perform as well as learning using portable state-action shaping functions, we nevertheless expect that they will result in substantial performance gains for reward-linked tasks. [sent-793, score-0.85]
</p><p>96 More broadly, given a set of previously learned related tasks that are not reward-linked, which one should we use as the source of a portable shaping function for a new related task? [sent-823, score-0.65]
</p><p>97 If the method used to compare the two reward functions returns a distance metric, then the agent could use it to cluster portable shaping functions and build libraries of them, drawing on an appropriate one for each new task it encounters. [sent-827, score-1.086]
</p><p>98 However, we do not believe the complete absence of prior information about a task is representative of applied reinforcement learning settings where the agent must solve multiple tasks sequentially. [sent-830, score-0.783]
</p><p>99 Summary and Conclusions We have presented a framework for transfer in reinforcement learning based on the idea that related tasks share common features and that transfer can take place through functions deﬁned over those related features. [sent-835, score-0.893]
</p><p>100 Learning relational options for inductive transfer in relational reinforcement learning. [sent-880, score-0.758]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('agent', 0.39), ('agents', 0.344), ('options', 0.308), ('shaping', 0.284), ('transfer', 0.276), ('episodes', 0.252), ('reward', 0.2), ('reinforcement', 0.174), ('keepaway', 0.166), ('tasks', 0.148), ('portable', 0.141), ('barto', 0.137), ('konidaris', 0.134), ('experiences', 0.114), ('cheidwasser', 0.102), ('einforcement', 0.102), ('hared', 0.102), ('lightworld', 0.102), ('ransfer', 0.102), ('beacon', 0.096), ('belt', 0.096), ('experience', 0.092), ('eatures', 0.087), ('episode', 0.075), ('keepers', 0.075), ('task', 0.071), ('option', 0.07), ('keeper', 0.07), ('rod', 0.064), ('policy', 0.063), ('descriptor', 0.06), ('conveyor', 0.059), ('skills', 0.059), ('learned', 0.049), ('possession', 0.048), ('skill', 0.048), ('policies', 0.046), ('robot', 0.044), ('sutton', 0.044), ('state', 0.043), ('takers', 0.043), ('action', 0.041), ('actions', 0.036), ('door', 0.035), ('room', 0.035), ('transferred', 0.034), ('learn', 0.034), ('earning', 0.033), ('training', 0.033), ('perfect', 0.032), ('sensors', 0.032), ('precup', 0.032), ('semantics', 0.032), ('environment', 0.03), ('sensor', 0.03), ('autonomous', 0.029), ('stone', 0.028), ('source', 0.028), ('positioning', 0.027), ('shared', 0.027), ('agentspace', 0.027), ('beacons', 0.027), ('belts', 0.027), ('homing', 0.027), ('simsek', 0.027), ('usefully', 0.027), ('rewards', 0.026), ('across', 0.023), ('reach', 0.023), ('descriptors', 0.023), ('ferguson', 0.023), ('ball', 0.023), ('bin', 0.022), ('curves', 0.022), ('steps', 0.022), ('jonsson', 0.021), ('ravindran', 0.021), ('wilson', 0.021), ('goal', 0.021), ('lock', 0.021), ('moore', 0.021), ('taylor', 0.021), ('mappings', 0.021), ('states', 0.02), ('mapping', 0.02), ('features', 0.019), ('completed', 0.019), ('initial', 0.019), ('scratch', 0.019), ('termination', 0.019), ('mdps', 0.019), ('light', 0.019), ('signal', 0.019), ('noise', 0.019), ('ave', 0.018), ('reused', 0.018), ('placement', 0.018), ('lo', 0.018), ('po', 0.018), ('moving', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="116-tfidf-1" href="./jmlr-2012-Transfer_in_Reinforcement_Learning_via_Shared_Features.html">116 jmlr-2012-Transfer in Reinforcement Learning via Shared Features</a></p>
<p>Author: George Konidaris, Ilya Scheidwasser, Andrew Barto</p><p>Abstract: We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to signiﬁcantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that signiﬁcantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-speciﬁc skills. Keywords: reinforcement learning, transfer, shaping, skills</p><p>2 0.13834541 <a title="116-tfidf-2" href="./jmlr-2012-Exploration_in_Relational_Domains_for_Model-based_Reinforcement_Learning.html">41 jmlr-2012-Exploration in Relational Domains for Model-based Reinforcement Learning</a></p>
<p>Author: Tobias Lang, Marc Toussaint, Kristian Kersting</p><p>Abstract: A fundamental problem in reinforcement learning is balancing exploration and exploitation. We address this problem in the context of model-based reinforcement learning in large stochastic relational domains by developing relational extensions of the concepts of the E 3 and R- MAX algorithms. Efﬁcient exploration in exponentially large state spaces needs to exploit the generalization of the learned model: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be a well-known context in which exploitation is promising. To address this we introduce relational count functions which generalize the classical notion of state and action visitation counts. We provide guarantees on the exploration efﬁciency of our framework using count functions under the assumption that we had a relational KWIK learner and a near-optimal planner. We propose a concrete exploration algorithm which integrates a practically efﬁcient probabilistic rule learner and a relational planner (for which there are no guarantees, however) and employs the contexts of learned relational rules as features to model the novelty of states and actions. Our results in noisy 3D simulated robot manipulation problems and in domains of the international planning competition demonstrate that our approach is more effective than existing propositional and factored exploration techniques. Keywords: reinforcement learning, statistical relational learning, exploration, relational transition models, robotics</p><p>3 0.095492341 <a title="116-tfidf-3" href="./jmlr-2012-Integrating_a_Partial_Model_into_Model_Free_Reinforcement_Learning.html">51 jmlr-2012-Integrating a Partial Model into Model Free Reinforcement Learning</a></p>
<p>Author: Aviv Tamar, Dotan Di Castro, Ron Meir</p><p>Abstract: In reinforcement learning an agent uses online feedback from the environment in order to adaptively select an effective policy. Model free approaches address this task by directly mapping environmental states to actions, while model based methods attempt to construct a model of the environment, followed by a selection of optimal actions based on that model. Given the complementary advantages of both approaches, we suggest a novel procedure which augments a model free algorithm with a partial model. The resulting hybrid algorithm switches between a model based and a model free mode, depending on the current state and the agent’s knowledge. Our method relies on a novel deﬁnition for a partially known model, and an estimator that incorporates such knowledge in order to reduce uncertainty in stochastic approximation iterations. We prove that such an approach leads to improved policy evaluation whenever environmental knowledge is available, without compromising performance when such knowledge is absent. Numerical simulations demonstrate the effectiveness of the approach on policy gradient and Q-learning algorithms, and its usefulness in solving a call admission control problem. Keywords: reinforcement learning, temporal difference, stochastic approximation, markov decision processes, hybrid model based model free algorithms</p><p>4 0.091963008 <a title="116-tfidf-4" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>Author: Grigorios Skolidis, Guido Sanguinetti</p><p>Abstract: We propose a novel model for meta-generalisation, that is, performing prediction on novel tasks based on information from multiple different but related tasks. The model is based on two coupled Gaussian processes with structured covariance function; one model performs predictions by learning a constrained covariance function encapsulating the relations between the various training tasks, while the second model determines the similarity of new tasks to previously seen tasks. We demonstrate empirically on several real and synthetic data sets both the strengths of the approach and its limitations due to the distributional assumptions underpinning it. Keywords: transfer learning, meta-generalising, multi-task learning, Gaussian processes, mixture of experts</p><p>5 0.090976998 <a title="116-tfidf-5" href="./jmlr-2012-Linear_Fitted-Q_Iteration_with_Multiple_Reward_Functions.html">58 jmlr-2012-Linear Fitted-Q Iteration with Multiple Reward Functions</a></p>
<p>Author: Daniel J. Lizotte, Michael Bowling, Susan A. Murphy</p><p>Abstract: We present a general and detailed development of an algorithm for ﬁnite-horizon ﬁtted-Q iteration with an arbitrary number of reward signals and linear value function approximation using an arbitrary number of state features. This includes a detailed treatment of the 3-reward function case using triangulation primitives from computational geometry and a method for identifying globally dominated actions. We also present an example of how our methods can be used to construct a realworld decision aid by considering symptom reduction, weight gain, and quality of life in sequential treatments for schizophrenia. Finally, we discuss future directions in which to take this work that will further enable our methods to make a positive impact on the ﬁeld of evidence-based clinical decision support. Keywords: reinforcement learning, dynamic programming, decision making, linear regression, preference elicitation</p><p>6 0.078056417 <a title="116-tfidf-6" href="./jmlr-2012-Dynamic_Policy_Programming.html">34 jmlr-2012-Dynamic Policy Programming</a></p>
<p>7 0.071786501 <a title="116-tfidf-7" href="./jmlr-2012-Optimistic_Bayesian_Sampling_in_Contextual-Bandit_Problems.html">86 jmlr-2012-Optimistic Bayesian Sampling in Contextual-Bandit Problems</a></p>
<p>8 0.038318288 <a title="116-tfidf-8" href="./jmlr-2012-Finite-Sample_Analysis_of_Least-Squares_Policy_Iteration.html">46 jmlr-2012-Finite-Sample Analysis of Least-Squares Policy Iteration</a></p>
<p>9 0.027683331 <a title="116-tfidf-9" href="./jmlr-2012-Human_Gesture_Recognition_on_Product_Manifolds.html">50 jmlr-2012-Human Gesture Recognition on Product Manifolds</a></p>
<p>10 0.026843963 <a title="116-tfidf-10" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>11 0.024695925 <a title="116-tfidf-11" href="./jmlr-2012-Discriminative_Hierarchical_Part-based_Models_for_Human_Parsing_and_Action_Recognition.html">32 jmlr-2012-Discriminative Hierarchical Part-based Models for Human Parsing and Action Recognition</a></p>
<p>12 0.02466001 <a title="116-tfidf-12" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>13 0.024538824 <a title="116-tfidf-13" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>14 0.021811498 <a title="116-tfidf-14" href="./jmlr-2012-Static_Prediction_Games_for_Adversarial_Learning_Problems.html">110 jmlr-2012-Static Prediction Games for Adversarial Learning Problems</a></p>
<p>15 0.021781418 <a title="116-tfidf-15" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>16 0.021490218 <a title="116-tfidf-16" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>17 0.020394634 <a title="116-tfidf-17" href="./jmlr-2012-Towards_Integrative_Causal_Analysis_of_Heterogeneous_Data_Sets_and_Studies.html">114 jmlr-2012-Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies</a></p>
<p>18 0.020182407 <a title="116-tfidf-18" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>19 0.019659201 <a title="116-tfidf-19" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>20 0.019258881 <a title="116-tfidf-20" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.116), (1, -0.036), (2, 0.133), (3, -0.171), (4, 0.038), (5, -0.189), (6, -0.147), (7, -0.117), (8, 0.001), (9, 0.206), (10, -0.155), (11, 0.078), (12, 0.055), (13, -0.127), (14, -0.038), (15, -0.055), (16, -0.006), (17, 0.097), (18, 0.004), (19, 0.004), (20, 0.08), (21, 0.11), (22, 0.002), (23, -0.05), (24, 0.101), (25, 0.034), (26, -0.054), (27, 0.221), (28, -0.124), (29, 0.176), (30, -0.097), (31, 0.004), (32, -0.019), (33, 0.029), (34, 0.038), (35, 0.059), (36, 0.025), (37, -0.169), (38, 0.057), (39, 0.034), (40, -0.093), (41, -0.002), (42, -0.059), (43, 0.068), (44, -0.011), (45, -0.03), (46, -0.111), (47, -0.085), (48, -0.01), (49, 0.175)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96835262 <a title="116-lsi-1" href="./jmlr-2012-Transfer_in_Reinforcement_Learning_via_Shared_Features.html">116 jmlr-2012-Transfer in Reinforcement Learning via Shared Features</a></p>
<p>Author: George Konidaris, Ilya Scheidwasser, Andrew Barto</p><p>Abstract: We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to signiﬁcantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that signiﬁcantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-speciﬁc skills. Keywords: reinforcement learning, transfer, shaping, skills</p><p>2 0.77161998 <a title="116-lsi-2" href="./jmlr-2012-Exploration_in_Relational_Domains_for_Model-based_Reinforcement_Learning.html">41 jmlr-2012-Exploration in Relational Domains for Model-based Reinforcement Learning</a></p>
<p>Author: Tobias Lang, Marc Toussaint, Kristian Kersting</p><p>Abstract: A fundamental problem in reinforcement learning is balancing exploration and exploitation. We address this problem in the context of model-based reinforcement learning in large stochastic relational domains by developing relational extensions of the concepts of the E 3 and R- MAX algorithms. Efﬁcient exploration in exponentially large state spaces needs to exploit the generalization of the learned model: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be a well-known context in which exploitation is promising. To address this we introduce relational count functions which generalize the classical notion of state and action visitation counts. We provide guarantees on the exploration efﬁciency of our framework using count functions under the assumption that we had a relational KWIK learner and a near-optimal planner. We propose a concrete exploration algorithm which integrates a practically efﬁcient probabilistic rule learner and a relational planner (for which there are no guarantees, however) and employs the contexts of learned relational rules as features to model the novelty of states and actions. Our results in noisy 3D simulated robot manipulation problems and in domains of the international planning competition demonstrate that our approach is more effective than existing propositional and factored exploration techniques. Keywords: reinforcement learning, statistical relational learning, exploration, relational transition models, robotics</p><p>3 0.55031997 <a title="116-lsi-3" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>Author: Grigorios Skolidis, Guido Sanguinetti</p><p>Abstract: We propose a novel model for meta-generalisation, that is, performing prediction on novel tasks based on information from multiple different but related tasks. The model is based on two coupled Gaussian processes with structured covariance function; one model performs predictions by learning a constrained covariance function encapsulating the relations between the various training tasks, while the second model determines the similarity of new tasks to previously seen tasks. We demonstrate empirically on several real and synthetic data sets both the strengths of the approach and its limitations due to the distributional assumptions underpinning it. Keywords: transfer learning, meta-generalising, multi-task learning, Gaussian processes, mixture of experts</p><p>4 0.34242746 <a title="116-lsi-4" href="./jmlr-2012-Integrating_a_Partial_Model_into_Model_Free_Reinforcement_Learning.html">51 jmlr-2012-Integrating a Partial Model into Model Free Reinforcement Learning</a></p>
<p>Author: Aviv Tamar, Dotan Di Castro, Ron Meir</p><p>Abstract: In reinforcement learning an agent uses online feedback from the environment in order to adaptively select an effective policy. Model free approaches address this task by directly mapping environmental states to actions, while model based methods attempt to construct a model of the environment, followed by a selection of optimal actions based on that model. Given the complementary advantages of both approaches, we suggest a novel procedure which augments a model free algorithm with a partial model. The resulting hybrid algorithm switches between a model based and a model free mode, depending on the current state and the agent’s knowledge. Our method relies on a novel deﬁnition for a partially known model, and an estimator that incorporates such knowledge in order to reduce uncertainty in stochastic approximation iterations. We prove that such an approach leads to improved policy evaluation whenever environmental knowledge is available, without compromising performance when such knowledge is absent. Numerical simulations demonstrate the effectiveness of the approach on policy gradient and Q-learning algorithms, and its usefulness in solving a call admission control problem. Keywords: reinforcement learning, temporal difference, stochastic approximation, markov decision processes, hybrid model based model free algorithms</p><p>5 0.33690834 <a title="116-lsi-5" href="./jmlr-2012-Linear_Fitted-Q_Iteration_with_Multiple_Reward_Functions.html">58 jmlr-2012-Linear Fitted-Q Iteration with Multiple Reward Functions</a></p>
<p>Author: Daniel J. Lizotte, Michael Bowling, Susan A. Murphy</p><p>Abstract: We present a general and detailed development of an algorithm for ﬁnite-horizon ﬁtted-Q iteration with an arbitrary number of reward signals and linear value function approximation using an arbitrary number of state features. This includes a detailed treatment of the 3-reward function case using triangulation primitives from computational geometry and a method for identifying globally dominated actions. We also present an example of how our methods can be used to construct a realworld decision aid by considering symptom reduction, weight gain, and quality of life in sequential treatments for schizophrenia. Finally, we discuss future directions in which to take this work that will further enable our methods to make a positive impact on the ﬁeld of evidence-based clinical decision support. Keywords: reinforcement learning, dynamic programming, decision making, linear regression, preference elicitation</p><p>6 0.30858469 <a title="116-lsi-6" href="./jmlr-2012-Optimistic_Bayesian_Sampling_in_Contextual-Bandit_Problems.html">86 jmlr-2012-Optimistic Bayesian Sampling in Contextual-Bandit Problems</a></p>
<p>7 0.28637663 <a title="116-lsi-7" href="./jmlr-2012-Dynamic_Policy_Programming.html">34 jmlr-2012-Dynamic Policy Programming</a></p>
<p>8 0.18542191 <a title="116-lsi-8" href="./jmlr-2012-Towards_Integrative_Causal_Analysis_of_Heterogeneous_Data_Sets_and_Studies.html">114 jmlr-2012-Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies</a></p>
<p>9 0.18485555 <a title="116-lsi-9" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>10 0.18138191 <a title="116-lsi-10" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>11 0.15474105 <a title="116-lsi-11" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>12 0.15338269 <a title="116-lsi-12" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>13 0.15053411 <a title="116-lsi-13" href="./jmlr-2012-A_Geometric_Approach_to_Sample_Compression.html">3 jmlr-2012-A Geometric Approach to Sample Compression</a></p>
<p>14 0.14293846 <a title="116-lsi-14" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>15 0.13660881 <a title="116-lsi-15" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>16 0.13638383 <a title="116-lsi-16" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>17 0.13498865 <a title="116-lsi-17" href="./jmlr-2012-Human_Gesture_Recognition_on_Product_Manifolds.html">50 jmlr-2012-Human Gesture Recognition on Product Manifolds</a></p>
<p>18 0.120718 <a title="116-lsi-18" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>19 0.1184897 <a title="116-lsi-19" href="./jmlr-2012-Nonparametric_Guidance_of_Autoencoder_Representations_using_Label_Information.html">78 jmlr-2012-Nonparametric Guidance of Autoencoder Representations using Label Information</a></p>
<p>20 0.11571088 <a title="116-lsi-20" href="./jmlr-2012-Discriminative_Hierarchical_Part-based_Models_for_Human_Parsing_and_Action_Recognition.html">32 jmlr-2012-Discriminative Hierarchical Part-based Models for Human Parsing and Action Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.015), (21, 0.029), (26, 0.037), (27, 0.015), (29, 0.039), (35, 0.021), (48, 0.402), (56, 0.02), (57, 0.056), (64, 0.013), (69, 0.02), (75, 0.048), (77, 0.015), (79, 0.014), (81, 0.01), (92, 0.081), (96, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72607708 <a title="116-lda-1" href="./jmlr-2012-Transfer_in_Reinforcement_Learning_via_Shared_Features.html">116 jmlr-2012-Transfer in Reinforcement Learning via Shared Features</a></p>
<p>Author: George Konidaris, Ilya Scheidwasser, Andrew Barto</p><p>Abstract: We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to signiﬁcantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that signiﬁcantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-speciﬁc skills. Keywords: reinforcement learning, transfer, shaping, skills</p><p>2 0.34758458 <a title="116-lda-2" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<p>Author: Mario Frank, Andreas P. Streich, David Basin, Joachim M. Buhmann</p><p>Abstract: We propose a probabilistic model for clustering Boolean data where an object can be simultaneously assigned to multiple clusters. By explicitly modeling the underlying generative process that combines the individual source emissions, highly structured data are expressed with substantially fewer clusters compared to single-assignment clustering. As a consequence, such a model provides robust parameter estimators even when the number of samples is low. We extend the model with different noise processes and demonstrate that maximum-likelihood estimation with multiple assignments consistently infers source parameters more accurately than single-assignment clustering. Our model is primarily motivated by the task of role mining for role-based access control, where users of a system are assigned one or more roles. In experiments with real-world access-control data, our model exhibits better generalization performance than state-of-the-art approaches. Keywords: clustering, multi-assignments, overlapping clusters, Boolean data, role mining, latent feature models</p><p>3 0.33243111 <a title="116-lda-3" href="./jmlr-2012-Exploration_in_Relational_Domains_for_Model-based_Reinforcement_Learning.html">41 jmlr-2012-Exploration in Relational Domains for Model-based Reinforcement Learning</a></p>
<p>Author: Tobias Lang, Marc Toussaint, Kristian Kersting</p><p>Abstract: A fundamental problem in reinforcement learning is balancing exploration and exploitation. We address this problem in the context of model-based reinforcement learning in large stochastic relational domains by developing relational extensions of the concepts of the E 3 and R- MAX algorithms. Efﬁcient exploration in exponentially large state spaces needs to exploit the generalization of the learned model: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be a well-known context in which exploitation is promising. To address this we introduce relational count functions which generalize the classical notion of state and action visitation counts. We provide guarantees on the exploration efﬁciency of our framework using count functions under the assumption that we had a relational KWIK learner and a near-optimal planner. We propose a concrete exploration algorithm which integrates a practically efﬁcient probabilistic rule learner and a relational planner (for which there are no guarantees, however) and employs the contexts of learned relational rules as features to model the novelty of states and actions. Our results in noisy 3D simulated robot manipulation problems and in domains of the international planning competition demonstrate that our approach is more effective than existing propositional and factored exploration techniques. Keywords: reinforcement learning, statistical relational learning, exploration, relational transition models, robotics</p><p>4 0.32892817 <a title="116-lda-4" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>Author: Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, Lin Xiao</p><p>Abstract: Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment. We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem. Keywords: distributed computing, online learning, stochastic optimization, regret bounds, convex optimization</p><p>5 0.32321921 <a title="116-lda-5" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>Author: Sangkyun Lee, Stephen J. Wright</p><p>Abstract: Iterative methods that calculate their steps from approximate subgradient directions have proved to be useful for stochastic learning problems over large and streaming data sets. When the objective consists of a loss function plus a nonsmooth regularization term, the solution often lies on a lowdimensional manifold of parameter space along which the regularizer is smooth. (When an ℓ1 regularizer is used to induce sparsity in the solution, for example, this manifold is deﬁned by the set of nonzero components of the parameter vector.) This paper shows that a regularized dual averaging algorithm can identify this manifold, with high probability, before reaching the solution. This observation motivates an algorithmic strategy in which, once an iterate is suspected of lying on an optimal or near-optimal manifold, we switch to a “local phase” that searches in this manifold, thus converging rapidly to a near-optimal point. Computational results are presented to verify the identiﬁcation property and to illustrate the effectiveness of this approach. Keywords: regularization, dual averaging, partly smooth manifold, manifold identiﬁcation</p><p>6 0.32195252 <a title="116-lda-6" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>7 0.32172105 <a title="116-lda-7" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>8 0.31741518 <a title="116-lda-8" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>9 0.31688502 <a title="116-lda-9" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>10 0.31555048 <a title="116-lda-10" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>11 0.31504321 <a title="116-lda-11" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>12 0.31437135 <a title="116-lda-12" href="./jmlr-2012-Optimistic_Bayesian_Sampling_in_Contextual-Bandit_Problems.html">86 jmlr-2012-Optimistic Bayesian Sampling in Contextual-Bandit Problems</a></p>
<p>13 0.31390238 <a title="116-lda-13" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>14 0.31319869 <a title="116-lda-14" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>15 0.31229499 <a title="116-lda-15" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>16 0.31209287 <a title="116-lda-16" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>17 0.31203696 <a title="116-lda-17" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>18 0.3114458 <a title="116-lda-18" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>19 0.31070036 <a title="116-lda-19" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>20 0.30999899 <a title="116-lda-20" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
