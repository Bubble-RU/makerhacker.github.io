<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-112" href="#">jmlr2012-112</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</h1>
<br/><p>Source: <a title="jmlr-2012-112-pdf" href="http://jmlr.org/papers/volume13/qin12a/qin12a.pdf">pdf</a></p><p>Author: Zhiwei Qin, Donald Goldfarb</p><p>Abstract: We consider a class of sparse learning problems in high dimensional feature space regularized by a structured sparsity-inducing norm that incorporates prior knowledge of the group structure of the features. Such problems often pose a considerable challenge to optimization algorithms due to the non-smoothness and non-separability of the regularization term. In this paper, we focus on two commonly adopted sparsity-inducing regularization terms, the overlapping Group Lasso penalty l1 /l2 -norm and the l1 /l∞ -norm. We propose a uniﬁed framework based on the augmented Lagrangian method, under which problems with both types of regularization and their variants can be efﬁciently solved. As one of the core building-blocks of this framework, we develop new algorithms using a partial-linearization/splitting technique and prove that the accelerated versions 1 of these algorithms require O( √ε ) iterations to obtain an ε-optimal solution. We compare the performance of these algorithms against that of the alternating direction augmented Lagrangian and FISTA methods on a collection of data sets and apply them to two real-world problems to compare the relative merits of the two norms. Keywords: structured sparsity, overlapping Group Lasso, alternating direction methods, variable splitting, augmented Lagrangian</p><p>Reference: <a title="jmlr-2012-112-reference" href="../jmlr2012_reference/jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We propose a uniﬁed framework based on the augmented Lagrangian method, under which problems with both types of regularization and their variants can be efﬁciently solved. [sent-6, score-0.109]
</p><p>2 We compare the performance of these algorithms against that of the alternating direction augmented Lagrangian and FISTA methods on a collection of data sets and apply them to two real-world problems to compare the relative merits of the two norms. [sent-8, score-0.132]
</p><p>3 Keywords: structured sparsity, overlapping Group Lasso, alternating direction methods, variable splitting, augmented Lagrangian  1. [sent-9, score-0.202]
</p><p>4 In particular, (1)-(2) cannot be cast into a nonoverlapping group lasso problem as done by Jacob et al. [sent-33, score-0.108]
</p><p>5 Liu and Ye (2010) also apply FISTA to solve (3), but in each iteration, they transform the computation of the proximal operator associated with the combined penalty term into an equivalent constrained smooth problem and solve it by Nesterov’s accelerated gradient descent method (Nesterov, 2005). [sent-40, score-0.109]
</p><p>6 (2010) apply the accelerated proximal gradient method to (1) with l1 /l∞ penalty and propose a network ﬂow algorithm to solve the proximal problem associated with Ωl1 /l∞ (x). [sent-42, score-0.15]
</p><p>7 , 2011) and FISTA for solving an equivalent constrained version of problem (1) (to be introduced in Section 2) in an augmented Lagrangian method framework. [sent-49, score-0.089]
</p><p>8 Speciﬁcally, we make the following contributions in this paper: • We build a general framework based on the augmented Lagrangian method, under which learning problems with both l1 /l2 and l1 /l∞ regularizations (and their variants) can be solved. [sent-50, score-0.089]
</p><p>9 A Variable-Splitting Augmented Lagrangian Framework In this section, we present a uniﬁed framework, based on variable splitting and the augmented Lagrangian method for solving (1) with both l1 /l2 - and l1 /l∞ -regularizations. [sent-62, score-0.089]
</p><p>10 To solve (4), we apply the augmented Lagrangian method (Hestenes, 1969; Powell, 1972; Nocedal and Wright, 1999; Bertsekas, 1999) to it. [sent-77, score-0.089]
</p><p>11 This method, Algorithm 1, minimizes the augmented Lagrangian 1 1 ˜ L (x, y, v) = Ax − b 2 − vT (Cx − y) + Cx − y 2 + Ω(y) (5) 2 2µ 1437  Q IN AND G OLDFARB  exactly for a given Lagrange multiplier v in every iteration followed by an update to v. [sent-78, score-0.157]
</p><p>12 2: for l = 0, 1, · · · do 3: (xl+1 , yl+1 ) ← arg minx,y L (x, y, vl ) 1 4: vl+1 ← vl − µ (Cxl+1 − yl+1 ) 5: Update µ according to the chosen updating scheme. [sent-82, score-0.302]
</p><p>13 We instead seek an approximate minimizer of the augmented Lagrangian via the abstract subroutine ApproxAugLagMin(x, y, v). [sent-85, score-0.116]
</p><p>14 Theorem 1 Let αl := L (xl , yl , vl ) − infx∈Rm ,y∈RM L (x, y, vl ) and F ∗ be the optimal value of problem (4). [sent-87, score-0.383]
</p><p>15 (6)  l=1  Then, the sequence {vl } converges to v∗ , which satisﬁes inf  x∈Rm ,y∈RM  Fob j (x, y) − (v∗ )T (Cx − y) = F ∗ ,  while the sequence {xl , yl } satisﬁes liml→∞ Cxl − yl = 0 and liml→∞ Fob j (xl , yl ) = F ∗ . [sent-89, score-0.441]
</p><p>16 The condition (6) requires the augmented Lagrangian subproblem be solved with increasing accuracy. [sent-90, score-0.113]
</p><p>17 2: for l = 0, 1, · · · do 3: (xl+1 , yl+1 ) ← ApproxAugLagMin(xl , yl , vl ), to compute an approximate minimizer of L (x, y, vl ) 4: vl+1 ← vl − 1 (Cxl+1 − yl+1 ) µ 5: Update µ according to the chosen updating scheme. [sent-93, score-0.524]
</p><p>18 Methods for Approximately Minimizing the Augmented Lagrangian In this section, we use the overlapping Group Lasso penalty Ω(x) = λ ∑s∈S ws xs to illustrate the optimization algorithms under discussion. [sent-98, score-0.096]
</p><p>19 , 2010)2 approximately minimizes the augmented Lagrangian by minimizing (5) with respect to x and y alternatingly and then updates the Lagrange multiplier v on each iteration (e. [sent-103, score-0.157]
</p><p>20 2: xl+1 ← arg minx L (x, yl , vl ) 3: yl+1 ← arg miny L (xl+1 , y, vl ) 4: return xl+1 , yl+1 . [sent-109, score-0.53]
</p><p>21 Note that the problem solved in Line 3 of Algorithm 3, yl+1 = arg min L (xl+1 , y, vl ) ≡ arg min y  y  1 l d −y 2µ  2  ˜ + Ω(y) ,  (7)  where d l = Cxl+1 − µvl , is group-separable and hence can be solved in parallel. [sent-124, score-0.204]
</p><p>22 1 PARTIAL L INEARIZATION  AND  C ONVERGENCE R ATE A NALYSIS  Let us deﬁne f (x, y) + g(y) = L (x, y; v),  F(x, y) :=  Lρ (x, y, y, γ) := f (x, y) + g(y) + γT (y − y) + ¯ ¯ ¯  1 y − y 2, ¯ 2ρ  (14)  where γ is the Lagrange multiplier in the augmented Lagrangian (14) corresponding to problem (13). [sent-155, score-0.124]
</p><p>23 We now present our partial-split alternating linearization algorithm to implement ApproxAugLagMin(x, y, v) in Algorithm 2. [sent-156, score-0.093]
</p><p>24 ¯ ¯ 2: for k = 0, 1, · · · until stopping criterion is satisﬁed do 3: (xk+1 , yk+1 ) ← arg minx,y Lρ (x, y, yk , γk ). [sent-160, score-0.371]
</p><p>25 When this condition is violated, a skipping step occurs in which the value of y is set to the value of y in the previous iteration (Line 5) and Lρ ¯ re-minimized with respect to x (Line 6) to ensure convergence. [sent-169, score-0.106]
</p><p>26 Let us deﬁne a regular iteration of Algorithm 4 to be an iteration where no skipping step occurs, that is, Lines 5 and 6 are not executed. [sent-170, score-0.139]
</p><p>27 Likewise, we deﬁne a skipping iteration to be an iteration where a skipping step occurs. [sent-171, score-0.212]
</p><p>28 For ρ ≤ Ly1 f ) , the iterates ( (xk , yk ) in Algorithm 4 satisfy ¯  F(xk , yk ) − F(x∗ , y∗ ) ≤ ¯  y0 − y∗ 2 ¯ , 2ρ(k + kn )  ∀k,  (20)  where (x∗ , y∗ ) is an optimal solution to (10), and kn is the number of regular iterations among the ﬁrst k iterations. [sent-174, score-0.667]
</p><p>29 3, we will discuss the case where the iterations entirely consist of skipping steps. [sent-180, score-0.096]
</p><p>30 First, observe that since ρ = µ, ¯ arg min Lρ (x, y, y, ∇y f (x, y)) ≡ arg min ∇y f (x, y)T y + ¯ y ¯  y ¯  ≡ arg min y ¯  1 d −y ¯ 2µ  2  1 y−y ¯ 2µ  + λ ∑ ys ¯  2  + g(y) ¯  ,  s  where d = Cx − µv. [sent-186, score-0.176]
</p><p>31 4, we present an algorithm FISTA-p, which is a special version of FAPLM-S in which every iteration is a skipping iteration and which has a much simpler form than FAPLM-S, while having essentially the same iteration complexity. [sent-198, score-0.172]
</p><p>32 It is also possible to apply ALM-S directly, which splits both x and y, to solve the augmented Lagrangian subproblem. [sent-199, score-0.089]
</p><p>33 Since  λmax (AT A) is usually not known, it is necessary to perform a backtracking line-search on ρ to ensure that F(xk+1 , yk+1 ) ≤ Lρ (xk+1 , yk+1 , xk , yk , γk ). [sent-209, score-0.403]
</p><p>34 3 ISTA: Partial Linearization (ISTA-p) We can also minimize the augmented Lagrangian (5), which we write as L (x, y, v) = f (x, y) + g(y) with f (x, y) and g(y) deﬁned as in (11) and (12), using a variant of ISTA that only linearizes f (x, y) with respect to the y variables. [sent-219, score-0.089]
</p><p>35 We compute y+ by y+ = arg min Lρ (x, y, y′ , ∇y f (x, y)) ′ y  = arg min ′ y  1 ( y′ − dy j 2µ ∑ j j  2  + λ y′j ) ,  (24)  where dy = Cx − µv. [sent-223, score-0.16]
</p><p>36 Now given y+ , we solve for x+ by x+ = arg min f (x′ , y+ ) ′ x  = arg min ′ x  1 Ax′ − b 2  2  − vT (Cx′ − y+ ) +  1 Cx′ − y+ 2µ  2  (25)  The algorithm that implements subroutine ApproxAugLagMin(x, y, v) in Algorithm 2 by ISTA with partial linearization is stated below as Algorithm 5. [sent-225, score-0.163]
</p><p>37 ¯ 2: for k = 0, 1, · · · until stopping criterion is satisﬁed do 3: xk+1 ← arg minx f (x; yk ) ¯ 4: yk+1 ← arg miny Lρ (xk+1 , yk , y, ∇y f (xk+1 , yk )) ¯ ¯ ¯ 5: end for 6: return (xK+1 , yK+1 ) ¯ As we remarked in Section 3. [sent-229, score-1.079]
</p><p>38 2, Algorithm 5 is equivalent to Algorithm 4 (APLM-S) where every iteration is a skipping iteration. [sent-230, score-0.106]
</p><p>39 For ρ ≤ the iterates (xk , yk ) in Algorithm 5 satisfy ¯  F(xk , yk ) − F(x∗ , y∗ ) ≤ ¯ where (x∗ , y∗ ) is an optimal solution to (10). [sent-233, score-0.604]
</p><p>40 Speciﬁcally, where d =  x+ = dx , dy j y+ = max(0, dy j − λρ), j dy j  (27) j = 1, . [sent-257, score-0.111]
</p><p>41 Using ISTA to solve the outer augmented Lagrangian (5) subproblem is equivalent to taking only skipping steps in ALM-S. [sent-261, score-0.269]
</p><p>42 zk dx x − ρ∇ f (zk , zk ) = 4: x y zk dy y 5: xk+1 ← dx ¯ dy 6: yk+1 ← dy j max(0, dy j − λρ), j = 1, . [sent-268, score-0.802]
</p><p>43 ¯j j √ 2 1+ 1+4tk 7: tk+1 ← 2 −1 ¯ ¯ ¯ 8: zk+1 ← xk + ttkk+1 (xk+1 − xk ) x −1 zk+1 ← yk + ttkk+1 (yk+1 − yk ) ¯ ¯ ¯ y 10: end for 11: return (xK+1 , yK+1 ) ¯ ¯  9:  FISTA (resp. [sent-272, score-0.806]
</p><p>44 ISTA-p), where we minimize with respect to x a linearized approximation 1 f˜(x, zk ) := f (xk , zk ) + ∇x f (xk , zk )(x − xk ) + x − xk 2ρ  2  of the quadratic objective function f (x, zk ) in (25). [sent-274, score-1.074]
</p><p>45 Overlapping Group l1 /l∞ -Regularization The subproblems with respect to y (or y) involved in all the algorithms presented in the previous ¯ sections take the following form 1 ˜ min c − y 2 + Ω(y), (28) y 2ρ ˜ where Ω(y) = λ ∑s∈S ws ys ∞ in the case of l1 /l∞ -regularization. [sent-281, score-0.098]
</p><p>46 For each ˜ , the subproblem in (28) is of the form s∈S min ys  1 cs − ys 2  2  + ρλ ys  ∞. [sent-286, score-0.165]
</p><p>47 dual residual sl+1 CT (yl+1 −yl ) CT yl CT (yK+1 −zK ) ¯ CT zK CT (yK+1 −yK+1 ) ¯ CT  yK+1   xK+1   zK  ¯ x  − zK yK+1 ¯  y  zK  x  zK y  Inner iteration Rel. [sent-309, score-0.266]
</p><p>48 We used stopping criteria based on the primal and dual residuals suggested by Boyd et al. [sent-317, score-0.095]
</p><p>49 The maximum number of outer iterations was set to 500, and the tolerance for the outer loop was set at εout = 10−4 . [sent-320, score-0.189]
</p><p>50 The number of inner-iterations was capped at 2000, and the tolerance at the l-th outer iteration for the inner loops was εl . [sent-321, score-0.147]
</p><p>51 Our termination criterion for the in outer iterations was max{rl , sl } ≤ εout , l  l  where rl = max{Cx −y, yl } is the outer relative primal residual and sl is the relative dual residual, Cxl which is given for each algorithm in Table 1. [sent-322, score-0.62]
</p><p>52 Recall that K + 1 is the index of the last inner iteration of the l-th outer iteration; for example, for APLM-S, (xl+1 , yl+1 ) takes the value of the last inner iterate (xK+1 , yK+1 ). [sent-323, score-0.178]
</p><p>53 We stopped the inner iterations when the maximum of the relative primal ¯ residual and the relative objective gradient for the inner problem was less than εl . [sent-324, score-0.211]
</p><p>54 ) We see there that sl+1 can be obtained directly from the relative gradient residual computed in the last inner iteration of the l-th outer iteration. [sent-326, score-0.23]
</p><p>55 However, it is not possible to evaluate the sub-optimality quantity αl in (6) exactly because the optimal value of the augmented Lagrangian L (x, y, vl ) is not known in advance. [sent-332, score-0.207]
</p><p>56 1448  S TRUCTURED S PARSITY VIA A LTERNATING D IRECTION M ETHODS  and dual residuals (max{rl , sl }) as a surrogate to αl for two reasons: First, it has been shown (Boyd et al. [sent-337, score-0.089]
</p><p>57 , 2010) that rl and sl are closely related to αl . [sent-338, score-0.092]
</p><p>58 Second, the quantities rl and sl are readily available as bi-products of the inner and outer iterations. [sent-339, score-0.206]
</p><p>59 However, since we terminate the outer iterations at εout > 0, it is in not necessary to solve the subproblems to an accuracy much higher than the one for the outer loop. [sent-343, score-0.21]
</p><p>60 On the other hand, it is also important for εl to decrease to below εout , since sl is closely related in to the quantities involved in the inner stopping criteria. [sent-344, score-0.12]
</p><p>61 In our context, this criterion essentially requires that the absolute dual residual is less than a fraction of the absolute primal residual. [sent-348, score-0.129]
</p><p>62 99, w0 is a ¯ ¯ x 1 constant, and wy is an auxiliary variable updated in each outer iteration by wl+1 = wl − µ2 CT (yK+1 − ¯ y y zK ). [sent-350, score-0.116]
</p><p>63 2 Strategies for Updating µ The penalty parameter µ in the outer augmented Lagrangian (5) not only controls the infeasibility in the constraint Cx = y, but also serves as the step-length in the y-subproblem (and the x-subproblem in the case of FISTA). [sent-353, score-0.197]
</p><p>64 The second strategy is a dynamic scheme based on the values rl and sl (Boyd et al. [sent-359, score-0.14]
</p><p>65 Hence, to keep rl and sl approximately balanced in each outer iteration, our scheme updated µ as follows:   max{βµl , µmin }, if rl > τsl l+1 min{µl /β, µmax }, if sl > τrl µ ←  l µ, otherwise, where we set µmax = 10, µmin = 10−6 , τ = 10 and β = 0. [sent-363, score-0.293]
</p><p>66 We instead adopted a simple scheme of decreasing µ by half every 10 outer iterations. [sent-466, score-0.109]
</p><p>67 It is, however, well suited for our partial linearization methods (APLM-S and FISTA-p), since there is no need for the augmented Lagrangian framework. [sent-518, score-0.139]
</p><p>68 The l1 model (31) was treated as a special case of the group regularization model (32), with each group containing only one component of the feature vector. [sent-525, score-0.118]
</p><p>69 6 For the Ridge/Elastic-Net penalty model, we applied FISTA-p directly without the outer augmented Lagrangian layer. [sent-526, score-0.197]
</p><p>70 The solutions for the l1 /l2 , l1 /l∞ , and Lasso models were not strictly sparse in the sense that those supposedly zero feature coefﬁcients had non-zero (albeit extremely small) magnitudes, since we enforced the linear constraints Cx = y through an augmented Lagrangian approach. [sent-527, score-0.089]
</p><p>71 Interestingly, the majority of the APLM-S inner iterations consisted of a skipping step for the tests on synthetic 1456  S TRUCTURED S PARSITY VIA A LTERNATING D IRECTION M ETHODS  Model l1 /l2 l1 /l∞ l1 ridge + elastic net  Accuracy (percent) 97. [sent-549, score-0.127]
</p><p>72 It may seem straight-forward to apply FISTA directly to the Lasso problem (31) without the augmented Lagrangian framework. [sent-572, score-0.089]
</p><p>73 In (19), by letting (x, y) = (x∗ , y∗ ), and y = yn , we get (p, q) = (xn+1 , yn+1 ), and ¯ ¯ 2ρ(F(x∗ , y∗ ) − F(xn+1 , yn+1 )) ≥ yn+1 − y∗  2  − yn − y∗ 2 . [sent-603, score-0.232]
</p><p>74 Adding (42) to (41), we get 2ρ(2F(x∗ , y∗ ) − F(xn+1 , yn+1 ) − F(xn+1 , yn+1 )) ≥ yn+1 − y∗ ¯ ¯  2  − yn − y∗ 2 . [sent-606, score-0.116]
</p><p>75 Since yn+1 = yn , it follows that ¯ 2ρ(F(x∗ , y∗ ) − F(xn+1 , yn+1 )) ≥ yn+1 − y∗ ¯ ¯  2  − yn − y∗ 2 . [sent-608, score-0.232]
</p><p>76 , k − 1 and observing that 2|I| + |Ic | = k + kn , we obtain k−1  ¯ 2ρ (k + kn )F(x∗ , y∗ ) − ∑ F(xn+1 , yn+1 ) − ∑ F(xn+1 , yn+1 ) n=1  k−1  ≥ =  ∑(  n=0 k  yn+1 − y∗ ¯  y − y∗ ¯  2  2  (45)  n∈I  − yn − y∗ 2 ) ¯  − y0 − y∗ ¯  2  ≥ − y0 − y∗ 2 . [sent-612, score-0.156]
</p><p>77 (46)  Q IN AND G OLDFARB  Similarly, for n ∈ I, if we let (x, y) = (xn , yn ) instead of (x∗ , y∗ ) in (41), we have ¯ 2ρ(F(xn , yn ) − F(xn+1 , yn+1 )) ≥ yn+1 − yn ¯ ¯  2  ≥ 0. [sent-614, score-0.348]
</p><p>78 (47)  For n ∈ Ic , yn+1 = yn ; from (15), since xn+1 = arg minx F(x, y) with y = yn = yn+1 , ¯ ¯ 2ρ(F(xn , yn ) − F(xn+1 , yn+1 )) ≥ 0. [sent-615, score-0.418]
</p><p>79 ¯  (48)  Hence, from (46) and (47) to (48), F(xn , yn ) ≥ F(xn , yn ) ≥ F(xn+1 , yn+1 ) ≥ F(xn+1 , yn+1 ). [sent-616, score-0.232]
</p><p>80 Then, we ¯ ¯ have k−1  ¯ ¯ ∑ F(xn+1 , yn+1 ) ≥ kF(xk , yk ), and ∑ F(xn+1 , yn+1 ) ≥ kn F(xk , yk ). [sent-617, score-0.624]
</p><p>81 (49)  n∈I  n=0  Combining (45) and (49) yields 2ρ(k + kn )(F(x∗ , y∗ ) − F(xk , yk )) ≥ − y0 − y∗ 2 . [sent-618, score-0.322]
</p><p>82 Derivation of the Stopping Criteria In this section, we show that the quantities that we use in our stopping criteria correspond to the primal and dual residuals (Boyd et al. [sent-620, score-0.095]
</p><p>83 , 2010) for the outer iterations and the gradient residuals for the inner iterations. [sent-621, score-0.16]
</p><p>84 ¯ ¯ ∗  ∗  ∗  (52)  Since yk+1 = zk , the primal residual is thus yk+1 − yk+1 = yk+1 − zk . [sent-624, score-0.539]
</p><p>85 It follows from the ¯ ¯ k+1 in Line 3 of Algorithm 6 that optimality of x 1 1 AT (Axk+1 − b) −CT vl + CT (Cxk+1 − yk+1 ) + CT (yk+1 − zk ) = 0 ¯ ¯ µ µ 1 ¯ ⇒ ∇x f (xk+1 , yk+1 ) = CT (zk − yk+1 ). [sent-625, score-0.336]
</p><p>86 ¯ µ Similarly, from the optimality of yk+1 in Line 4, we have that ¯ 1 0 ∈ ∂g(yk+1 ) + ∇y f (xk+1 , zk ) + (yk+1 − zk ) ¯ ¯ ρ 1 1 = ∂g(yk+1 ) + ∇y f (xk+1 , yk+1 ) − (yk+1 − zk ) + (yk+1 − zk ) ¯ ¯ ¯ ¯ µ ρ = ∂g(yk+1 ) + ∇y f (xk+1 , yk+1 ), ¯ ¯ 1 ¯ where the last step follows from µ = ρ. [sent-626, score-0.872]
</p><p>87 Hence, we see that µ CT (zk − yk+1 ) is the gradient residual corresponding to (51), while (52) is satisﬁed in every inner iteration. [sent-627, score-0.114]
</p><p>88 1460  S TRUCTURED S PARSITY VIA A LTERNATING D IRECTION M ETHODS  APLM-S The primal residual is yk+1 − yk+1 from (50). [sent-628, score-0.103]
</p><p>89 ¯ ¯ ¯ Clearly, the primal residual is (xk+1 − zk , yk+1 − zk ) since (xk+1 , yk+1 ) ≡ (zk , zk ). [sent-631, score-0.757]
</p><p>90 From the ¯ x y y x ¯ k+1 , yk+1 ), it follows that optimality of (x ¯ ¯ 1 0 = ∇x f (zk , zk ) + (xk+1 − zk ), ¯ x y x ρ 1 ¯ 0 ∈ ∂g(yk+1 ) + ∇y f (zk , zk ) + (yk+1 − zk ). [sent-632, score-0.872]
</p><p>91 ¯ y x y ρ 1 1 Here, we simply use ρ (xk+1 − zk ) and ρ (yk+1 − zk ) to approximate the gradient residuals. [sent-633, score-0.459]
</p><p>92 The dual residual is ¯ ∇L(xl+1 ) −CT (vl − 1 (Cxl+1 − yl+1 )) µ 1 , recalling that vl+1 = vl − µ (Cxl+1 − yl+1 ). [sent-637, score-0.204]
</p><p>93 The above ¯ ˜ l+1 ) + vl − 1 (Cxl+1 − yl+1 ) ∂Ω(y ¯ µ is simply the gradient of the augmented Lagrangian (5) evaluated at (xl , yl , vl ). [sent-638, score-0.495]
</p><p>94 Now, since the objective function of an inner iteration is the augmented Lagrangian with v = vl , the dual residual for an outer iteration is readily available from the gradient residual computed for the last inner iteration of the outer iteration. [sent-639, score-0.703]
</p><p>95 Consistency of the group lasso and multiple kernel learning. [sent-794, score-0.108]
</p><p>96 Fast alternating linearization methods for minimizing the sum of two convex functions. [sent-1047, score-0.093]
</p><p>97 Tree-guided group lasso for multi-task regression with structured sparsity. [sent-1186, score-0.137]
</p><p>98 The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices. [sent-1193, score-0.15]
</p><p>99 A primal-dual algorithm for group sparse regularization with overlapping groups. [sent-1230, score-0.11]
</p><p>100 Group lasso with overlaps: the latent group lasso approach. [sent-1248, score-0.167]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('adal', 0.538), ('fista', 0.495), ('yk', 0.302), ('zk', 0.218), ('yl', 0.147), ('vl', 0.118), ('oldfarb', 0.116), ('proxflow', 0.116), ('proxgrad', 0.116), ('yn', 0.116), ('irection', 0.109), ('lternating', 0.109), ('xk', 0.101), ('cx', 0.1), ('parsity', 0.093), ('tructured', 0.093), ('augmented', 0.089), ('outer', 0.083), ('lagrangian', 0.08), ('skipping', 0.073), ('mairal', 0.068), ('ct', 0.068), ('cpu', 0.067), ('ethods', 0.065), ('sl', 0.063), ('cxl', 0.061), ('dct', 0.061), ('residual', 0.06), ('lasso', 0.059), ('xl', 0.056), ('approxauglagmin', 0.055), ('ax', 0.054), ('ista', 0.052), ('iters', 0.052), ('linearization', 0.05), ('group', 0.049), ('algs', 0.048), ('ys', 0.047), ('primal', 0.043), ('breast', 0.043), ('arg', 0.043), ('alternating', 0.043), ('goldfarb', 0.042), ('overlapping', 0.041), ('combettes', 0.041), ('jenatton', 0.041), ('avg', 0.041), ('proximal', 0.041), ('ly', 0.037), ('dy', 0.037), ('xn', 0.035), ('multiplier', 0.035), ('pcg', 0.035), ('eckstein', 0.034), ('miny', 0.034), ('pesquet', 0.034), ('iteration', 0.033), ('inner', 0.031), ('ws', 0.03), ('obozinski', 0.029), ('chen', 0.029), ('structured', 0.029), ('rl', 0.029), ('groups', 0.027), ('aplm', 0.027), ('breastcancerdata', 0.027), ('dyn', 0.027), ('ogl', 0.027), ('peyre', 0.027), ('subroutine', 0.027), ('rx', 0.027), ('minx', 0.027), ('jacob', 0.027), ('cancer', 0.027), ('ry', 0.026), ('dual', 0.026), ('lagrange', 0.026), ('stopping', 0.026), ('scheme', 0.026), ('penalty', 0.025), ('bertsekas', 0.025), ('ic', 0.024), ('subproblem', 0.024), ('rmse', 0.024), ('qin', 0.023), ('updating', 0.023), ('iterations', 0.023), ('gradient', 0.023), ('dynamic', 0.022), ('scalability', 0.022), ('sec', 0.022), ('boyd', 0.021), ('subproblems', 0.021), ('lipschitz', 0.021), ('fob', 0.02), ('ttkk', 0.02), ('cholesky', 0.02), ('kn', 0.02), ('accelerated', 0.02), ('regularization', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="112-tfidf-1" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<p>Author: Zhiwei Qin, Donald Goldfarb</p><p>Abstract: We consider a class of sparse learning problems in high dimensional feature space regularized by a structured sparsity-inducing norm that incorporates prior knowledge of the group structure of the features. Such problems often pose a considerable challenge to optimization algorithms due to the non-smoothness and non-separability of the regularization term. In this paper, we focus on two commonly adopted sparsity-inducing regularization terms, the overlapping Group Lasso penalty l1 /l2 -norm and the l1 /l∞ -norm. We propose a uniﬁed framework based on the augmented Lagrangian method, under which problems with both types of regularization and their variants can be efﬁciently solved. As one of the core building-blocks of this framework, we develop new algorithms using a partial-linearization/splitting technique and prove that the accelerated versions 1 of these algorithms require O( √ε ) iterations to obtain an ε-optimal solution. We compare the performance of these algorithms against that of the alternating direction augmented Lagrangian and FISTA methods on a collection of data sets and apply them to two real-world problems to compare the relative merits of the two norms. Keywords: structured sparsity, overlapping Group Lasso, alternating direction methods, variable splitting, augmented Lagrangian</p><p>2 0.061065122 <a title="112-tfidf-2" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>Author: Andreas Maurer, Massimiliano Pontil</p><p>Abstract: We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an inﬁnite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels. Keywords: empirical processes, Rademacher average, sparse estimation.</p><p>3 0.048823543 <a title="112-tfidf-3" href="./jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</a></p>
<p>Author: Gavin Brown, Adam Pocock, Ming-Jie Zhao, Mikel Luján</p><p>Abstract: We present a unifying framework for information theoretic feature selection, bringing almost two decades of research on heuristic ﬁlter criteria under a single theoretical interpretation. This is in response to the question: “what are the implicit statistical assumptions of feature selection criteria based on mutual information?”. To answer this, we adopt a different strategy than is usual in the feature selection literature—instead of trying to deﬁne a criterion, we derive one, directly from a clearly speciﬁed objective function: the conditional likelihood of the training labels. While many hand-designed heuristic criteria try to optimize a deﬁnition of feature ‘relevancy’ and ‘redundancy’, our approach leads to a probabilistic framework which naturally incorporates these concepts. As a result we can unify the numerous criteria published over the last two decades, and show them to be low-order approximations to the exact (but intractable) optimisation problem. The primary contribution is to show that common heuristics for information based feature selection (including Markov Blanket algorithms as a special case) are approximate iterative maximisers of the conditional likelihood. A large empirical study provides strong evidence to favour certain classes of criteria, in particular those that balance the relative size of the relevancy/redundancy terms. Overall we conclude that the JMI criterion (Yang and Moody, 1999; Meyer et al., 2008) provides the best tradeoff in terms of accuracy, stability, and ﬂexibility with small data samples. Keywords: feature selection, mutual information, conditional likelihood</p><p>4 0.048413597 <a title="112-tfidf-4" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>Author: Jian Huang, Cun-Hui Zhang</p><p>Abstract: The ℓ1 -penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ1 -penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1 -penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results. Keywords: variable selection, penalized estimation, oracle inequality, generalized linear models, selection consistency, sparsity</p><p>5 0.036154177 <a title="112-tfidf-5" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>Author: Michael U. Gutmann, Aapo Hyvärinen</p><p>Abstract: We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a ﬁnite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only speciﬁed up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artiﬁcially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities. Keywords: statistics unnormalized models, partition function, computation, estimation, natural image</p><p>6 0.036054522 <a title="112-tfidf-6" href="./jmlr-2012-Dynamic_Policy_Programming.html">34 jmlr-2012-Dynamic Policy Programming</a></p>
<p>7 0.035404529 <a title="112-tfidf-7" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>8 0.034005586 <a title="112-tfidf-8" href="./jmlr-2012-Integrating_a_Partial_Model_into_Model_Free_Reinforcement_Learning.html">51 jmlr-2012-Integrating a Partial Model into Model Free Reinforcement Learning</a></p>
<p>9 0.03335809 <a title="112-tfidf-9" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>10 0.032844275 <a title="112-tfidf-10" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>11 0.031912763 <a title="112-tfidf-11" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>12 0.031341642 <a title="112-tfidf-12" href="./jmlr-2012-An_Active_Learning_Algorithm_for_Ranking_from_Pairwise_Preferences_with_an_Almost_Optimal_Query_Complexity.html">17 jmlr-2012-An Active Learning Algorithm for Ranking from Pairwise Preferences with an Almost Optimal Query Complexity</a></p>
<p>13 0.02846716 <a title="112-tfidf-13" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>14 0.028112976 <a title="112-tfidf-14" href="./jmlr-2012-PAC-Bayes_Bounds_with_Data_Dependent_Priors.html">87 jmlr-2012-PAC-Bayes Bounds with Data Dependent Priors</a></p>
<p>15 0.026249411 <a title="112-tfidf-15" href="./jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</a></p>
<p>16 0.02562706 <a title="112-tfidf-16" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>17 0.025085483 <a title="112-tfidf-17" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>18 0.024608731 <a title="112-tfidf-18" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>19 0.023703603 <a title="112-tfidf-19" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>20 0.02334537 <a title="112-tfidf-20" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.121), (1, 0.029), (2, -0.016), (3, -0.015), (4, 0.013), (5, 0.046), (6, -0.002), (7, -0.086), (8, -0.003), (9, 0.007), (10, -0.003), (11, 0.066), (12, 0.044), (13, -0.011), (14, 0.051), (15, -0.006), (16, 0.017), (17, 0.027), (18, 0.041), (19, 0.082), (20, -0.039), (21, 0.002), (22, 0.147), (23, -0.049), (24, -0.033), (25, -0.074), (26, 0.124), (27, -0.077), (28, 0.15), (29, -0.05), (30, 0.002), (31, 0.231), (32, 0.278), (33, -0.036), (34, 0.374), (35, 0.142), (36, 0.124), (37, 0.026), (38, 0.048), (39, -0.178), (40, -0.06), (41, 0.018), (42, -0.081), (43, 0.031), (44, -0.054), (45, -0.017), (46, -0.019), (47, -0.105), (48, -0.004), (49, -0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95806849 <a title="112-lsi-1" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<p>Author: Zhiwei Qin, Donald Goldfarb</p><p>Abstract: We consider a class of sparse learning problems in high dimensional feature space regularized by a structured sparsity-inducing norm that incorporates prior knowledge of the group structure of the features. Such problems often pose a considerable challenge to optimization algorithms due to the non-smoothness and non-separability of the regularization term. In this paper, we focus on two commonly adopted sparsity-inducing regularization terms, the overlapping Group Lasso penalty l1 /l2 -norm and the l1 /l∞ -norm. We propose a uniﬁed framework based on the augmented Lagrangian method, under which problems with both types of regularization and their variants can be efﬁciently solved. As one of the core building-blocks of this framework, we develop new algorithms using a partial-linearization/splitting technique and prove that the accelerated versions 1 of these algorithms require O( √ε ) iterations to obtain an ε-optimal solution. We compare the performance of these algorithms against that of the alternating direction augmented Lagrangian and FISTA methods on a collection of data sets and apply them to two real-world problems to compare the relative merits of the two norms. Keywords: structured sparsity, overlapping Group Lasso, alternating direction methods, variable splitting, augmented Lagrangian</p><p>2 0.56733739 <a title="112-lsi-2" href="./jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</a></p>
<p>Author: Gavin Brown, Adam Pocock, Ming-Jie Zhao, Mikel Luján</p><p>Abstract: We present a unifying framework for information theoretic feature selection, bringing almost two decades of research on heuristic ﬁlter criteria under a single theoretical interpretation. This is in response to the question: “what are the implicit statistical assumptions of feature selection criteria based on mutual information?”. To answer this, we adopt a different strategy than is usual in the feature selection literature—instead of trying to deﬁne a criterion, we derive one, directly from a clearly speciﬁed objective function: the conditional likelihood of the training labels. While many hand-designed heuristic criteria try to optimize a deﬁnition of feature ‘relevancy’ and ‘redundancy’, our approach leads to a probabilistic framework which naturally incorporates these concepts. As a result we can unify the numerous criteria published over the last two decades, and show them to be low-order approximations to the exact (but intractable) optimisation problem. The primary contribution is to show that common heuristics for information based feature selection (including Markov Blanket algorithms as a special case) are approximate iterative maximisers of the conditional likelihood. A large empirical study provides strong evidence to favour certain classes of criteria, in particular those that balance the relative size of the relevancy/redundancy terms. Overall we conclude that the JMI criterion (Yang and Moody, 1999; Meyer et al., 2008) provides the best tradeoff in terms of accuracy, stability, and ﬂexibility with small data samples. Keywords: feature selection, mutual information, conditional likelihood</p><p>3 0.3079434 <a title="112-lsi-3" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>Author: Andreas Maurer, Massimiliano Pontil</p><p>Abstract: We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an inﬁnite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels. Keywords: empirical processes, Rademacher average, sparse estimation.</p><p>4 0.26467291 <a title="112-lsi-4" href="./jmlr-2012-Dynamic_Policy_Programming.html">34 jmlr-2012-Dynamic Policy Programming</a></p>
<p>Author: Mohammad Gheshlaghi Azar, Vicenç Gómez, Hilbert J. Kappen</p><p>Abstract: In this paper, we propose a novel policy iteration method, called dynamic policy programming (DPP), to estimate the optimal policy in the inﬁnite-horizon Markov decision processes. DPP is an incremental algorithm that forces a gradual change in policy update. This allows us to prove ﬁnite-iteration and asymptotic ℓ∞ -norm performance-loss bounds in the presence of approximation/estimation error which depend on the average accumulated error as opposed to the standard bounds which are expressed in terms of the supremum of the errors. The dependency on the average error is important in problems with limited number of samples per iteration, for which the average of the errors can be signiﬁcantly smaller in size than the supremum of the errors. Based on these theoretical results, we prove that a sampling-based variant of DPP (DPP-RL) asymptotically converges to the optimal policy. Finally, we illustrate numerically the applicability of these results on some benchmark problems and compare the performance of the approximate variants of DPP with some existing reinforcement learning (RL) methods. Keywords: approximate dynamic programming, reinforcement learning, Markov decision processes, Monte-Carlo methods, function approximation</p><p>5 0.25073323 <a title="112-lsi-5" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>Author: Jian Huang, Cun-Hui Zhang</p><p>Abstract: The ℓ1 -penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ1 -penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1 -penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results. Keywords: variable selection, penalized estimation, oracle inequality, generalized linear models, selection consistency, sparsity</p><p>6 0.24737088 <a title="112-lsi-6" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>7 0.22298963 <a title="112-lsi-7" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>8 0.20125431 <a title="112-lsi-8" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>9 0.19873114 <a title="112-lsi-9" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>10 0.19790855 <a title="112-lsi-10" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>11 0.19045655 <a title="112-lsi-11" href="./jmlr-2012-Active_Clustering_of_Biological_Sequences.html">12 jmlr-2012-Active Clustering of Biological Sequences</a></p>
<p>12 0.17439666 <a title="112-lsi-12" href="./jmlr-2012-An_Active_Learning_Algorithm_for_Ranking_from_Pairwise_Preferences_with_an_Almost_Optimal_Query_Complexity.html">17 jmlr-2012-An Active Learning Algorithm for Ranking from Pairwise Preferences with an Almost Optimal Query Complexity</a></p>
<p>13 0.16580917 <a title="112-lsi-13" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>14 0.16506726 <a title="112-lsi-14" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>15 0.15633829 <a title="112-lsi-15" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>16 0.15514162 <a title="112-lsi-16" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>17 0.15373315 <a title="112-lsi-17" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>18 0.15194167 <a title="112-lsi-18" href="./jmlr-2012-Iterative_Reweighted_Algorithms_for_Matrix_Rank_Minimization.html">52 jmlr-2012-Iterative Reweighted Algorithms for Matrix Rank Minimization</a></p>
<p>19 0.14647238 <a title="112-lsi-19" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>20 0.14645377 <a title="112-lsi-20" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.019), (21, 0.043), (26, 0.542), (29, 0.02), (35, 0.015), (49, 0.015), (56, 0.011), (64, 0.019), (69, 0.015), (75, 0.039), (77, 0.015), (92, 0.049), (96, 0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97745889 <a title="112-lda-1" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>Author: David Verstraeten, Benjamin Schrauwen, Sander Dieleman, Philemon Brakel, Pieter Buteneers, Dejan Pecevski</p><p>Abstract: Oger (OrGanic Environment for Reservoir computing) is a Python toolbox for building, training and evaluating modular learning architectures on large data sets. It builds on MDP for its modularity, and adds processing of sequential data sets, gradient descent training, several crossvalidation schemes and parallel parameter optimization methods. Additionally, several learning algorithms are implemented, such as different reservoir implementations (both sigmoid and spiking), ridge regression, conditional restricted Boltzmann machine (CRBM) and others, including GPU accelerated versions. Oger is released under the GNU LGPL, and is available from http: //organic.elis.ugent.be/oger. Keywords: Python, modular architectures, sequential processing</p><p>same-paper 2 0.88615113 <a title="112-lda-2" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<p>Author: Zhiwei Qin, Donald Goldfarb</p><p>Abstract: We consider a class of sparse learning problems in high dimensional feature space regularized by a structured sparsity-inducing norm that incorporates prior knowledge of the group structure of the features. Such problems often pose a considerable challenge to optimization algorithms due to the non-smoothness and non-separability of the regularization term. In this paper, we focus on two commonly adopted sparsity-inducing regularization terms, the overlapping Group Lasso penalty l1 /l2 -norm and the l1 /l∞ -norm. We propose a uniﬁed framework based on the augmented Lagrangian method, under which problems with both types of regularization and their variants can be efﬁciently solved. As one of the core building-blocks of this framework, we develop new algorithms using a partial-linearization/splitting technique and prove that the accelerated versions 1 of these algorithms require O( √ε ) iterations to obtain an ε-optimal solution. We compare the performance of these algorithms against that of the alternating direction augmented Lagrangian and FISTA methods on a collection of data sets and apply them to two real-world problems to compare the relative merits of the two norms. Keywords: structured sparsity, overlapping Group Lasso, alternating direction methods, variable splitting, augmented Lagrangian</p><p>3 0.87530291 <a title="112-lda-3" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>Author: Michael U. Gutmann, Aapo Hyvärinen</p><p>Abstract: We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a ﬁnite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only speciﬁed up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artiﬁcially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities. Keywords: statistics unnormalized models, partition function, computation, estimation, natural image</p><p>4 0.86528778 <a title="112-lda-4" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>Author: Prateek Jain, Brian Kulis, Jason V. Davis, Inderjit S. Dhillon</p><p>Abstract: Metric and kernel learning arise in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem. In particular, we propose a general optimization framework for learning metrics via linear transformations, and analyze in detail a special case of our framework—that of minimizing the LogDet divergence subject to linear constraints. We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework. Our theoretical connections between metric and kernel learning have two main consequences: 1) the learned kernel matrix parameterizes a linear transformation kernel function and can be applied inductively to new data points, 2) our result yields a constructive method for kernelizing most existing Mahalanobis metric learning formulations. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction. Keywords: divergence metric learning, kernel learning, linear transformation, matrix divergences, logdet</p><p>5 0.56211352 <a title="112-lda-5" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel</p><p>Abstract: The success of many machine learning and pattern recognition methods relies heavily upon the identiﬁcation of an appropriate distance metric on the input data. It is often beneﬁcial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semideﬁnite. Semideﬁnite programming is often used to enforce this constraint, but does not scale well and is not easy to implement. B OOST M ETRIC is instead based on the observation that any positive semideﬁnite matrix can be decomposed into a linear combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting methods are easy to implement, efﬁcient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semideﬁnite matrix with trace and rank being one rather than a classiﬁer or regressor. Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. Keywords: Mahalanobis distance, semideﬁnite programming, column generation, boosting, Lagrange duality, large margin nearest neighbor</p><p>6 0.53602451 <a title="112-lda-6" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>7 0.53391844 <a title="112-lda-7" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>8 0.52483135 <a title="112-lda-8" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>9 0.49972194 <a title="112-lda-9" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>10 0.49002314 <a title="112-lda-10" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>11 0.48739463 <a title="112-lda-11" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>12 0.47782198 <a title="112-lda-12" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>13 0.47731516 <a title="112-lda-13" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>14 0.47665408 <a title="112-lda-14" href="./jmlr-2012-Large-scale_Linear_Support_Vector_Regression.html">54 jmlr-2012-Large-scale Linear Support Vector Regression</a></p>
<p>15 0.47465461 <a title="112-lda-15" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>16 0.46966758 <a title="112-lda-16" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>17 0.46681309 <a title="112-lda-17" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>18 0.46159053 <a title="112-lda-18" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>19 0.44085309 <a title="112-lda-19" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>20 0.43901163 <a title="112-lda-20" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
