<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-47" href="#">jmlr2012-47</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</h1>
<br/><p>Source: <a title="jmlr-2012-47-pdf" href="http://jmlr.org/papers/volume13/park12a/park12a.pdf">pdf</a></p><p>Author: Chiwoo Park, Jianhua Z. Huang, Yu Ding</p><p>Abstract: This paper presents the Getting-started style documentation for the local and parallel computation toolbox for Gaussian process regression (GPLP), an open source software package written in Matlab (but also compatible with Octave). The working environment and the usage of the software package will be presented in this paper. Keywords: Gaussian process regression, domain decomposition method, partial independent conditional, bagging for Gaussian process, local probabilistic regression</p><p>Reference: <a title="jmlr-2012-47-reference" href="../jmlr2012_reference/jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The working environment and the usage of the software package will be presented in this paper. [sent-10, score-0.111]
</p><p>2 Keywords: Gaussian process regression, domain decomposition method, partial independent conditional, bagging for Gaussian process, local probabilistic regression  1. [sent-11, score-0.325]
</p><p>3 Introduction The Gaussian process regression (GP regression) has recently developed to be a useful tool in machine learning (Rasmussen and Williams, 2006). [sent-12, score-0.148]
</p><p>4 A GP regression provides the best unbiased linear estimator computable by a simple closed form expression and is a popular method for interpolation or extrapolation. [sent-13, score-0.086]
</p><p>5 A major limitation of GP regression is its computational complexity, scaled by O(N 3 ), where N is the number of training observations. [sent-14, score-0.086]
</p><p>6 Many fast computation methods have been introduced in the literature to relieve the computation burden: matrix approximation (Williams and Seeger, 2000; Smola and Bartlett, 2001), likelihood approximation (Seeger et al. [sent-15, score-0.031]
</p><p>7 , 2003; Snelson and Ghahramani, 2006, 2007) and localized regression (Tresp, 2000; Schwaighofer et al. [sent-16, score-0.431]
</p><p>8 In particular, most of the localized regression methods are not implemented in spite of their unique advantages such as adaptivity to non-stationary changes and easiness of being parallelized for faster computation. [sent-23, score-0.556]
</p><p>9 The GPLP is the Octave and Matlab implementation of several localized regression methods: the domain decomposition method (Park et al. [sent-24, score-0.544]
</p><p>10 , 2011, DDM), partial independent conditional (Snelson and Ghahramani, 2007, PIC), localized probabilistic regression (Urtasun and Darrell, 2008, LPR), and bagging for Gaussian process regression (Chen and Ren, 2009, BGP). [sent-25, score-0.651]
</p><p>11 Most of the localized regression methods can be applied for general machine learning problems although DDM is only applicable for spatial data sets. [sent-26, score-0.461]
</p><p>12 In addition, the GPLP provides two parallel computation versions of the domain decomposition method. [sent-27, score-0.219]
</p><p>13 The easiness of being parallelized is one of the advantages of the localized regression, and the two parallel implementations will provide a good guidance about how to materialize this advantage as software. [sent-28, score-0.566]
</p><p>14 This manual is written in Getting-started style; it introduces the working environment of GPLP (in Section 2) and illustrates the usage with an simple example (in Section 3). [sent-29, score-0.09]
</p><p>15 If you need more detailed documentation, please refer to User Manual at . [sent-30, score-0.057]
</p><p>16 Implementation The GPLP is implemented in Matlab code such that it is executable and has been tested in Matlab Version 7. [sent-33, score-0.062]
</p><p>17 It might be executable in any of Matlab Version 7. [sent-37, score-0.062]
</p><p>18 One exception is the implementation of LPR that only works in Matlab 7. [sent-41, score-0.038]
</p><p>19 0 or later versions with a compiler supporting mex-compile, or in Octave 3. [sent-45, score-0.048]
</p><p>20 For information on the list of compilers to support the mex-compile in Matlab, please refer to the technical support webpage at http://www. [sent-48, score-0.088]
</p><p>21 The GPLP also includes the parallel computation version of DDM, which requires the open source message passing interface, MatMPI Version 1. [sent-52, score-0.096]
</p><p>22 All of the Matlab, Octave and MatMPI are working in many versions of Windows and Unix, so GPLP is virtually OS-independent. [sent-54, score-0.048]
</p><p>23 The implementation consists of six different main modules for the six different methods implemented, but all of the main modules are structured in the common form having the similar input and output arguments. [sent-55, score-0.188]
</p><p>24 In addition, the implementation partially supports the separation of the main 1. [sent-56, score-0.038]
</p><p>25 With such separation, users can easily extend the function of GPLP by adding a new covariance function and adding a new mesh generation function without major modiﬁcation of the main logic. [sent-84, score-0.466]
</p><p>26 The code and documentation of GPLP are publicly available on the JMLR MOSS website at http://www. [sent-85, score-0.051]
</p><p>27 GPLP: A software Package for Localized and Parallel Computation of GP Regression The GPLP provides an individual function for calling each one of the six localized regression methods (including two parallel implementations. [sent-91, score-0.602]
</p><p>28 The GP regression predicts the realization of the random function at test locations xs, given a set of observations x from the realization. [sent-95, score-0.115]
</p><p>29 The localized GP regression partitions x into many smaller chunks, x_j’s, and it does localized predictions at xs with each one of x_j’s as the training data for every j. [sent-96, score-0.896]
</p><p>30 Finally, the localized GP regression combines the localized predictions to make a global prediction in many different ways. [sent-97, score-0.869]
</p><p>31 The key design parameters for the localized GP regression are (1) mean function and covariance function deﬁning the GP, and (2) mesh generation function for partitioning x into x_j’s. [sent-98, score-0.897]
</p><p>32 1 2 3 4 5 6  % define the structure of local param1. [sent-99, score-0.061]
</p><p>33 q = 3;  regions % mesh generation function % mesh generation function parameters % parameters defining the interaction % between local regions for improving % prediction accuracy  7 8 9 10 11 12 13 14 15 16 17  % set the prior GP by specifying a covariance function param2. [sent-103, score-0.9]
</p><p>34 logtheta0 = logtheta0; % initial value of log hyperparameters param2. [sent-106, score-0.073]
</p><p>35 5; % fraction of training data used for learning % hyperparameters param2. [sent-108, score-0.073]
</p><p>36 The mesh generation function decomposes X (domain of f ) into 14-by-21 rectangular meshes, {X j }, and it partitions x into x_j’s such that x_j belongs to X j . [sent-110, score-0.366]
</p><p>37 777  PARK , H UANG AND D ING  In line 4, there are two parameters that deﬁnes how many localized predictions are combined to produce a global prediction. [sent-111, score-0.4]
</p><p>38 In the domain decomposition method (DDM), a localized prediction is available for each mesh X j , which becomes the global prediction if the test input is in the interior of local domain X j . [sent-112, score-0.823]
</p><p>39 q is the the number of control points on the boundary where the DDM checks the consistency of boundary prediction, and the param1. [sent-115, score-0.172]
</p><p>40 p is the number of degrees of freedom to constrain the ﬂexibility of boundary prediction. [sent-116, score-0.116]
</p><p>41 In line 9 through 13, we specify the covSum composite covariance function. [sent-117, score-0.144]
</p><p>42 The composite covariance function generates the covariance by summing two base covariance functions: the anisotropic version of squared exponential covariance function (covSEard) and the noise covariance function (covNoise). [sent-118, score-0.544]
</p><p>43 The covSEard is parameterized by (D + 1) hyperparameters as follows: ′ 2 1 D xd − xd ′ 2 K(x, x ) = θD+1 exp − ∑ , 2 d=1 θd where D is the dimension of X . [sent-119, score-0.171]
</p><p>44 The covNoise is parameterized by noise variance parameter σ2 as K(x, x′ ) = σ2 δ(x, x′ ). [sent-120, score-0.03]
</p><p>45 In total, the composite covariance function is parameterized by (D+2) parameter values, so the initial guess of hyperparameter, logtheta0, should be (D + 2)-dimensional. [sent-121, score-0.209]
</p><p>46 In line 11 and 12, the ﬁrst (D + 1) elements of logtheta0 are initialized for the hyperparameter values of covSEard, and the last one element of logtheta0 is initialized for the value of σ2 . [sent-122, score-0.065]
</p><p>47 nIter are the process parameters used in maximizing the likelihood function with respect to the hyperparameters. [sent-125, score-0.062]
</p><p>48 The maximization is an iterative process that updates the log hyperparameter values, starting with the initial guess logtheta0. [sent-126, score-0.162]
</p><p>49 Last, in line 20, the function ddmGP trains the domain decomposition method for the localized GP regression with training data set x and the previously speciﬁed parameters, and ddmGP returns the trained model (model) and the elapsed time (elpasedTrain). [sent-134, score-0.506]
</p><p>50 For more details, please refer to User Manual at . [sent-136, score-0.057]
</p><p>51 In line 22, the function ddm_pred produces the mean prediction meanPred and the variance prediction varPred at test locations xs, and also reports the time used for prediction (elpasedPred). [sent-138, score-0.143]
</p><p>52 Bayesian treed Gaussian process models with an application to computer modeling. [sent-146, score-0.062]
</p><p>53 Domain decomposition approach for fast gaussian process regression of large spatial data sets. [sent-150, score-0.278]
</p><p>54 Transductive and inductive methods for approximate Gaussian process regression. [sent-164, score-0.062]
</p><p>55 Fast forward selection to speed up sparse Gaussian process regression. [sent-171, score-0.062]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gplp', 0.437), ('localized', 0.345), ('gp', 0.267), ('mesh', 0.259), ('ddm', 0.218), ('snelson', 0.168), ('octave', 0.168), ('park', 0.165), ('chiwoo', 0.146), ('jianhua', 0.146), ('tamu', 0.146), ('covseard', 0.109), ('ddmgp', 0.109), ('gramacy', 0.109), ('schwaighofer', 0.109), ('generation', 0.107), ('covariance', 0.1), ('parallel', 0.096), ('matlab', 0.092), ('ghahramani', 0.087), ('regression', 0.086), ('boundary', 0.086), ('urtasun', 0.084), ('seeger', 0.082), ('covnoise', 0.073), ('easiness', 0.073), ('elpasedtrain', 0.073), ('lpr', 0.073), ('matmpi', 0.073), ('meanpred', 0.073), ('varpred', 0.073), ('hyperparameters', 0.073), ('rasmussen', 0.073), ('bagging', 0.072), ('xs', 0.065), ('hyperparameter', 0.065), ('zoubin', 0.065), ('williams', 0.064), ('gaussian', 0.063), ('oolbox', 0.062), ('omputation', 0.062), ('executable', 0.062), ('process', 0.062), ('please', 0.057), ('predictions', 0.055), ('manual', 0.054), ('tresp', 0.052), ('volker', 0.052), ('ocal', 0.052), ('parallelized', 0.052), ('station', 0.052), ('christopher', 0.051), ('documentation', 0.051), ('uang', 0.048), ('darrell', 0.048), ('versions', 0.048), ('matthias', 0.046), ('edward', 0.046), ('texas', 0.046), ('composite', 0.044), ('huang', 0.043), ('college', 0.042), ('florida', 0.041), ('six', 0.039), ('package', 0.039), ('prediction', 0.038), ('implementation', 0.038), ('domain', 0.038), ('style', 0.038), ('decomposition', 0.037), ('usage', 0.036), ('carl', 0.036), ('modules', 0.036), ('chen', 0.036), ('software', 0.036), ('egression', 0.035), ('tx', 0.035), ('guess', 0.035), ('ren', 0.034), ('xd', 0.034), ('toolbox', 0.033), ('smola', 0.032), ('yu', 0.032), ('industrial', 0.032), ('relieve', 0.031), ('compilers', 0.031), ('fsu', 0.031), ('tallahassee', 0.031), ('anton', 0.031), ('define', 0.031), ('gpml', 0.031), ('moss', 0.031), ('raquel', 0.031), ('unix', 0.031), ('parameterized', 0.03), ('local', 0.03), ('spatial', 0.03), ('exibility', 0.03), ('degrees', 0.03), ('locations', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="47-tfidf-1" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<p>Author: Chiwoo Park, Jianhua Z. Huang, Yu Ding</p><p>Abstract: This paper presents the Getting-started style documentation for the local and parallel computation toolbox for Gaussian process regression (GPLP), an open source software package written in Matlab (but also compatible with Octave). The working environment and the usage of the software package will be presented in this paper. Keywords: Gaussian process regression, domain decomposition method, partial independent conditional, bagging for Gaussian process, local probabilistic regression</p><p>2 0.13395593 <a title="47-tfidf-2" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>Author: Odalric-Ambrym Maillard, Rémi Munos</p><p>Abstract: We investigate a method for regression that makes use of a randomly generated subspace GP ⊂ F (of ﬁnite dimension P) of a given large (possibly inﬁnite) dimensional function space F , for example, L2 ([0, 1]d ; R). GP is deﬁned as the span of P random features that are linear combinations of a basis functions of F weighted by random Gaussian i.i.d. coefﬁcients. We show practical motivation for the use of this approach, detail the link that this random projections method share with RKHS and Gaussian objects theory and prove, both in deterministic and random design, approximation error bounds when searching for the best regression function in GP rather than in F , and derive excess risk bounds for a speciﬁc regression algorithm (least squares regression in GP ). This paper stresses the motivation to study such methods, thus the analysis developed is kept simple for explanations purpose and leaves room for future developments. Keywords: regression, random matrices, dimension reduction</p><p>3 0.075541802 <a title="47-tfidf-3" href="./jmlr-2012-Nonparametric_Guidance_of_Autoencoder_Representations_using_Label_Information.html">78 jmlr-2012-Nonparametric Guidance of Autoencoder Representations using Label Information</a></p>
<p>Author: Jasper Snoek, Ryan P. Adams, Hugo Larochelle</p><p>Abstract: While unsupervised learning has long been useful for density modeling, exploratory data analysis and visualization, it has become increasingly important for discovering features that will later be used for discriminative tasks. Discriminative algorithms often work best with highly-informative features; remarkably, such features can often be learned without the labels. One particularly effective way to perform such unsupervised learning has been to use autoencoder neural networks, which ﬁnd latent representations that are constrained but nevertheless informative for reconstruction. However, pure unsupervised learning with autoencoders can ﬁnd representations that may or may not be useful for the ultimate discriminative task. It is a continuing challenge to guide the training of an autoencoder so that it ﬁnds features which will be useful for predicting labels. Similarly, we often have a priori information regarding what statistical variation will be irrelevant to the ultimate discriminative task, and we would like to be able to use this for guidance as well. Although a typical strategy would be to include a parametric discriminative model as part of the autoencoder training, here we propose a nonparametric approach that uses a Gaussian process to guide the representation. By using a nonparametric model, we can ensure that a useful discriminative function exists for a given set of features, without explicitly instantiating it. We demonstrate the superiority of this guidance mechanism on four data sets, including a real-world application to rehabilitation research. We also show how our proposed approach can learn to explicitly ignore statistically signiﬁcant covariate information that is label-irrelevant, by evaluating on the small NORB image recognition problem in which pose and lighting labels are available. Keywords: autoencoder, gaussian process, gaussian process latent variable model, representation learning, unsupervised learning</p><p>4 0.069431409 <a title="47-tfidf-4" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>Author: Hannes Nickisch</p><p>Abstract: The glm-ie toolbox contains functionality for estimation and inference in generalised linear models over continuous-valued variables. Besides a variety of penalised least squares solvers for estimation, it offers inference based on (convex) variational bounds, on expectation propagation and on factorial mean ﬁeld. Scalable and efﬁcient inference in fully-connected undirected graphical models or Markov random ﬁelds with Gaussian and non-Gaussian potentials is achieved by casting all the computations as matrix vector multiplications. We provide a wide choice of penalty functions for estimation, potential functions for inference and matrix classes with lazy evaluation for convenient modelling. We designed the glm-ie package to be simple, generic and easily expansible. Most of the code is written in Matlab including some MEX ﬁles to be fully compatible to both Matlab 7.x and GNU Octave 3.3.x. Large scale probabilistic classiﬁcation as well as sparse linear modelling can be performed in a common algorithmical framework by the glm-ie toolkit. Keywords: sparse linear models, generalised linear models, Bayesian inference, approximate inference, probabilistic regression and classiﬁcation, penalised least squares estimation, lazy evaluation matrix class</p><p>5 0.063145719 <a title="47-tfidf-5" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>Author: Philipp Hennig, Christian J. Schuler</p><p>Abstract: Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly addresses the decision problem of maximizing information gain from each evaluation. Keywords: optimization, probability, information, Gaussian processes, expectation propagation</p><p>6 0.05473939 <a title="47-tfidf-6" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>7 0.036638953 <a title="47-tfidf-7" href="./jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</a></p>
<p>8 0.036092956 <a title="47-tfidf-8" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>9 0.034641974 <a title="47-tfidf-9" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>10 0.033548936 <a title="47-tfidf-10" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>11 0.031656031 <a title="47-tfidf-11" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>12 0.031310562 <a title="47-tfidf-12" href="./jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering.html">101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</a></p>
<p>13 0.02914192 <a title="47-tfidf-13" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>14 0.026200728 <a title="47-tfidf-14" href="./jmlr-2012-Random_Search_for_Hyper-Parameter_Optimization.html">95 jmlr-2012-Random Search for Hyper-Parameter Optimization</a></p>
<p>15 0.026112646 <a title="47-tfidf-15" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>16 0.025452057 <a title="47-tfidf-16" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>17 0.025253458 <a title="47-tfidf-17" href="./jmlr-2012-The_huge_Package_for_High-dimensional_Undirected_Graph_Estimation_in_R.html">113 jmlr-2012-The huge Package for High-dimensional Undirected Graph Estimation in R</a></p>
<p>18 0.023712739 <a title="47-tfidf-18" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>19 0.023408469 <a title="47-tfidf-19" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>20 0.022181965 <a title="47-tfidf-20" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.118), (1, 0.068), (2, 0.13), (3, -0.067), (4, 0.064), (5, 0.051), (6, 0.015), (7, 0.009), (8, -0.239), (9, -0.05), (10, -0.051), (11, 0.008), (12, -0.111), (13, 0.121), (14, -0.155), (15, 0.008), (16, -0.118), (17, 0.088), (18, -0.153), (19, -0.004), (20, 0.09), (21, -0.084), (22, -0.011), (23, -0.125), (24, 0.083), (25, 0.116), (26, 0.156), (27, -0.026), (28, 0.018), (29, -0.05), (30, 0.113), (31, 0.031), (32, 0.108), (33, 0.204), (34, -0.002), (35, 0.016), (36, -0.031), (37, 0.101), (38, 0.109), (39, 0.003), (40, 0.165), (41, -0.028), (42, -0.059), (43, 0.022), (44, -0.091), (45, 0.068), (46, -0.052), (47, 0.163), (48, -0.086), (49, 0.126)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96678162 <a title="47-lsi-1" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<p>Author: Chiwoo Park, Jianhua Z. Huang, Yu Ding</p><p>Abstract: This paper presents the Getting-started style documentation for the local and parallel computation toolbox for Gaussian process regression (GPLP), an open source software package written in Matlab (but also compatible with Octave). The working environment and the usage of the software package will be presented in this paper. Keywords: Gaussian process regression, domain decomposition method, partial independent conditional, bagging for Gaussian process, local probabilistic regression</p><p>2 0.53244704 <a title="47-lsi-2" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>Author: Odalric-Ambrym Maillard, Rémi Munos</p><p>Abstract: We investigate a method for regression that makes use of a randomly generated subspace GP ⊂ F (of ﬁnite dimension P) of a given large (possibly inﬁnite) dimensional function space F , for example, L2 ([0, 1]d ; R). GP is deﬁned as the span of P random features that are linear combinations of a basis functions of F weighted by random Gaussian i.i.d. coefﬁcients. We show practical motivation for the use of this approach, detail the link that this random projections method share with RKHS and Gaussian objects theory and prove, both in deterministic and random design, approximation error bounds when searching for the best regression function in GP rather than in F , and derive excess risk bounds for a speciﬁc regression algorithm (least squares regression in GP ). This paper stresses the motivation to study such methods, thus the analysis developed is kept simple for explanations purpose and leaves room for future developments. Keywords: regression, random matrices, dimension reduction</p><p>3 0.50591755 <a title="47-lsi-3" href="./jmlr-2012-Nonparametric_Guidance_of_Autoencoder_Representations_using_Label_Information.html">78 jmlr-2012-Nonparametric Guidance of Autoencoder Representations using Label Information</a></p>
<p>Author: Jasper Snoek, Ryan P. Adams, Hugo Larochelle</p><p>Abstract: While unsupervised learning has long been useful for density modeling, exploratory data analysis and visualization, it has become increasingly important for discovering features that will later be used for discriminative tasks. Discriminative algorithms often work best with highly-informative features; remarkably, such features can often be learned without the labels. One particularly effective way to perform such unsupervised learning has been to use autoencoder neural networks, which ﬁnd latent representations that are constrained but nevertheless informative for reconstruction. However, pure unsupervised learning with autoencoders can ﬁnd representations that may or may not be useful for the ultimate discriminative task. It is a continuing challenge to guide the training of an autoencoder so that it ﬁnds features which will be useful for predicting labels. Similarly, we often have a priori information regarding what statistical variation will be irrelevant to the ultimate discriminative task, and we would like to be able to use this for guidance as well. Although a typical strategy would be to include a parametric discriminative model as part of the autoencoder training, here we propose a nonparametric approach that uses a Gaussian process to guide the representation. By using a nonparametric model, we can ensure that a useful discriminative function exists for a given set of features, without explicitly instantiating it. We demonstrate the superiority of this guidance mechanism on four data sets, including a real-world application to rehabilitation research. We also show how our proposed approach can learn to explicitly ignore statistically signiﬁcant covariate information that is label-irrelevant, by evaluating on the small NORB image recognition problem in which pose and lighting labels are available. Keywords: autoencoder, gaussian process, gaussian process latent variable model, representation learning, unsupervised learning</p><p>4 0.46330124 <a title="47-lsi-4" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>Author: Philipp Hennig, Christian J. Schuler</p><p>Abstract: Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly addresses the decision problem of maximizing information gain from each evaluation. Keywords: optimization, probability, information, Gaussian processes, expectation propagation</p><p>5 0.33501706 <a title="47-lsi-5" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>Author: Hannes Nickisch</p><p>Abstract: The glm-ie toolbox contains functionality for estimation and inference in generalised linear models over continuous-valued variables. Besides a variety of penalised least squares solvers for estimation, it offers inference based on (convex) variational bounds, on expectation propagation and on factorial mean ﬁeld. Scalable and efﬁcient inference in fully-connected undirected graphical models or Markov random ﬁelds with Gaussian and non-Gaussian potentials is achieved by casting all the computations as matrix vector multiplications. We provide a wide choice of penalty functions for estimation, potential functions for inference and matrix classes with lazy evaluation for convenient modelling. We designed the glm-ie package to be simple, generic and easily expansible. Most of the code is written in Matlab including some MEX ﬁles to be fully compatible to both Matlab 7.x and GNU Octave 3.3.x. Large scale probabilistic classiﬁcation as well as sparse linear modelling can be performed in a common algorithmical framework by the glm-ie toolkit. Keywords: sparse linear models, generalised linear models, Bayesian inference, approximate inference, probabilistic regression and classiﬁcation, penalised least squares estimation, lazy evaluation matrix class</p><p>6 0.27414885 <a title="47-lsi-6" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>7 0.20011972 <a title="47-lsi-7" href="./jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering.html">101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</a></p>
<p>8 0.18857446 <a title="47-lsi-8" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>9 0.18108332 <a title="47-lsi-9" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>10 0.17565562 <a title="47-lsi-10" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>11 0.16069449 <a title="47-lsi-11" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>12 0.15887089 <a title="47-lsi-12" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>13 0.15818621 <a title="47-lsi-13" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>14 0.15503758 <a title="47-lsi-14" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>15 0.15361102 <a title="47-lsi-15" href="./jmlr-2012-An_Introduction_to_Artificial_Prediction_Markets_for_Classification.html">19 jmlr-2012-An Introduction to Artificial Prediction Markets for Classification</a></p>
<p>16 0.15188338 <a title="47-lsi-16" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>17 0.15088183 <a title="47-lsi-17" href="./jmlr-2012-Random_Search_for_Hyper-Parameter_Optimization.html">95 jmlr-2012-Random Search for Hyper-Parameter Optimization</a></p>
<p>18 0.14906903 <a title="47-lsi-18" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>19 0.14739737 <a title="47-lsi-19" href="./jmlr-2012-The_huge_Package_for_High-dimensional_Undirected_Graph_Estimation_in_R.html">113 jmlr-2012-The huge Package for High-dimensional Undirected Graph Estimation in R</a></p>
<p>20 0.14508449 <a title="47-lsi-20" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.013), (21, 0.03), (26, 0.039), (27, 0.017), (29, 0.028), (35, 0.01), (49, 0.607), (56, 0.023), (64, 0.011), (75, 0.02), (79, 0.018), (92, 0.033), (96, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90634555 <a title="47-lda-1" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<p>Author: Chiwoo Park, Jianhua Z. Huang, Yu Ding</p><p>Abstract: This paper presents the Getting-started style documentation for the local and parallel computation toolbox for Gaussian process regression (GPLP), an open source software package written in Matlab (but also compatible with Octave). The working environment and the usage of the software package will be presented in this paper. Keywords: Gaussian process regression, domain decomposition method, partial independent conditional, bagging for Gaussian process, local probabilistic regression</p><p>2 0.73343837 <a title="47-lda-2" href="./jmlr-2012-EP-GIG_Priors_and_Applications_in_Bayesian_Sparse_Learning.html">35 jmlr-2012-EP-GIG Priors and Applications in Bayesian Sparse Learning</a></p>
<p>Author: Zhihua Zhang, Shusen Wang, Dehua Liu, Michael I. Jordan</p><p>Abstract: In this paper we propose a novel framework for the construction of sparsity-inducing priors. In particular, we deﬁne such priors as a mixture of exponential power distributions with a generalized inverse Gaussian density (EP-GIG). EP-GIG is a variant of generalized hyperbolic distributions, and the special cases include Gaussian scale mixtures and Laplace scale mixtures. Furthermore, Laplace scale mixtures can subserve a Bayesian framework for sparse learning with nonconvex penalization. The densities of EP-GIG can be explicitly expressed. Moreover, the corresponding posterior distribution also follows a generalized inverse Gaussian distribution. We exploit these properties to develop EM algorithms for sparse empirical Bayesian learning. We also show that these algorithms bear an interesting resemblance to iteratively reweighted ℓ2 or ℓ1 methods. Finally, we present two extensions for grouped variable selection and logistic regression. Keywords: sparsity priors, scale mixtures of exponential power distributions, generalized inverse Gaussian distributions, expectation-maximization algorithms, iteratively reweighted minimization methods</p><p>3 0.6186949 <a title="47-lda-3" href="./jmlr-2012-On_the_Convergence_Rate_oflp-Norm_Multiple_Kernel_Learning.html">81 jmlr-2012-On the Convergence Rate oflp-Norm Multiple Kernel Learning</a></p>
<p>Author: Marius Kloft, Gilles Blanchard</p><p>Abstract: We derive an upper bound on the local Rademacher complexity of ℓ p -norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p = 1 only while our analysis covers all cases 1 ≤ p ≤ ∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding excess loss, namely α fast convergence rates of the order O(n− 1+α ), where α is the minimum eigenvalue decay rate of the individual kernels. Keywords: multiple kernel learning, learning kernels, generalization bounds, local Rademacher complexity</p><p>4 0.27668792 <a title="47-lda-4" href="./jmlr-2012-PAC-Bayes_Bounds_with_Data_Dependent_Priors.html">87 jmlr-2012-PAC-Bayes Bounds with Data Dependent Priors</a></p>
<p>Author: Emilio Parrado-Hernández, Amiran Ambroladze, John Shawe-Taylor, Shiliang Sun</p><p>Abstract: This paper presents the prior PAC-Bayes bound and explores its capabilities as a tool to provide tight predictions of SVMs’ generalization. The computation of the bound involves estimating a prior of the distribution of classiﬁers from the available data, and then manipulating this prior in the usual PAC-Bayes generalization bound. We explore two alternatives: to learn the prior from a separate data set, or to consider an expectation prior that does not need this separate data set. The prior PAC-Bayes bound motivates two SVM-like classiﬁcation algorithms, prior SVM and ηprior SVM, whose regularization term pushes towards the minimization of the prior PAC-Bayes bound. The experimental work illustrates that the new bounds can be signiﬁcantly tighter than the original PAC-Bayes bound when applied to SVMs, and among them the combination of the prior PAC-Bayes bound and the prior SVM algorithm gives the tightest bound. Keywords: PAC-Bayes bound, support vector machine, generalization capability prediction, classiﬁcation</p><p>5 0.27126285 <a title="47-lda-5" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>Author: Hannes Nickisch</p><p>Abstract: The glm-ie toolbox contains functionality for estimation and inference in generalised linear models over continuous-valued variables. Besides a variety of penalised least squares solvers for estimation, it offers inference based on (convex) variational bounds, on expectation propagation and on factorial mean ﬁeld. Scalable and efﬁcient inference in fully-connected undirected graphical models or Markov random ﬁelds with Gaussian and non-Gaussian potentials is achieved by casting all the computations as matrix vector multiplications. We provide a wide choice of penalty functions for estimation, potential functions for inference and matrix classes with lazy evaluation for convenient modelling. We designed the glm-ie package to be simple, generic and easily expansible. Most of the code is written in Matlab including some MEX ﬁles to be fully compatible to both Matlab 7.x and GNU Octave 3.3.x. Large scale probabilistic classiﬁcation as well as sparse linear modelling can be performed in a common algorithmical framework by the glm-ie toolkit. Keywords: sparse linear models, generalised linear models, Bayesian inference, approximate inference, probabilistic regression and classiﬁcation, penalised least squares estimation, lazy evaluation matrix class</p><p>6 0.2635214 <a title="47-lda-6" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>7 0.25668576 <a title="47-lda-7" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>8 0.24334122 <a title="47-lda-8" href="./jmlr-2012-Jstacs%3A_A_Java_Framework_for_Statistical_Analysis_and_Classification_of_Biological_Sequences.html">53 jmlr-2012-Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences</a></p>
<p>9 0.23565505 <a title="47-lda-9" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>10 0.23298541 <a title="47-lda-10" href="./jmlr-2012-Sally%3A_A_Tool_for_Embedding_Strings_in_Vector_Spaces.html">102 jmlr-2012-Sally: A Tool for Embedding Strings in Vector Spaces</a></p>
<p>11 0.23196329 <a title="47-lda-11" href="./jmlr-2012-Finding_Recurrent_Patterns_from_Continuous_Sign_Language_Sentences_for_Automated_Extraction_of_Signs.html">45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</a></p>
<p>12 0.23062237 <a title="47-lda-12" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>13 0.22915038 <a title="47-lda-13" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>14 0.2290844 <a title="47-lda-14" href="./jmlr-2012-Eliminating_Spammers_and_Ranking_Annotators_for_Crowdsourced_Labeling_Tasks.html">37 jmlr-2012-Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks</a></p>
<p>15 0.22377369 <a title="47-lda-15" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>16 0.2237155 <a title="47-lda-16" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>17 0.22327481 <a title="47-lda-17" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>18 0.21957408 <a title="47-lda-18" href="./jmlr-2012-An_Introduction_to_Artificial_Prediction_Markets_for_Classification.html">19 jmlr-2012-An Introduction to Artificial Prediction Markets for Classification</a></p>
<p>19 0.21902588 <a title="47-lda-19" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>20 0.2184871 <a title="47-lda-20" href="./jmlr-2012-Towards_Integrative_Causal_Analysis_of_Heterogeneous_Data_Sets_and_Studies.html">114 jmlr-2012-Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
