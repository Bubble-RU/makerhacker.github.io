<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>111 jmlr-2012-Structured Sparsity and Generalization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-111" href="#">jmlr2012-111</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>111 jmlr-2012-Structured Sparsity and Generalization</h1>
<br/><p>Source: <a title="jmlr-2012-111-pdf" href="http://jmlr.org/papers/volume13/maurer12a/maurer12a.pdf">pdf</a></p><p>Author: Andreas Maurer, Massimiliano Pontil</p><p>Abstract: We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an inﬁnite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels. Keywords: empirical processes, Rademacher average, sparse estimation.</p><p>Reference: <a title="jmlr-2012-111-reference" href="../jmlr2012_reference/jmlr-2012-Structured_Sparsity_and_Generalization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 London, UK  Editor: Gabor Lugosi  Abstract We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. [sent-8, score-0.131]
</p><p>2 The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. [sent-9, score-0.214]
</p><p>3 A novel feature of our bound is that it can be applied in an inﬁnite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels. [sent-11, score-0.205]
</p><p>4 The regularizer is expressed as an inﬁmum convolution which involves a set M of linear transformations (see Equation (1) below). [sent-15, score-0.146]
</p><p>5 As we shall see, this regularizer generalizes, depending on the choice of the set M , the regularizers used by several learning algorithms, such as ridge regression, the Lasso, the group Lasso (Yuan and Lin, 2006), multiple kernel learning (Lanckriet et al. [sent-16, score-0.25]
</p><p>6 , 2004), the group Lasso with overlap (Obozinski et al. [sent-18, score-0.088]
</p><p>7 We give a bound on the Rademacher average of the linear function class associated with this regularizer. [sent-21, score-0.068]
</p><p>8 In particular, the bound applies to the Lasso in a separable Hilbert space or to multiple kernel learning with a countable number of kernels, under certain ﬁnite second-moment conditions. [sent-23, score-0.205]
</p><p>9 Let M be an at most countable set of symmetric bounded linear operators on H such that for every x ∈ H, x = 0, there is some linear operator M ∈ M with Mx = 0 and that supM∈M |||M||| < ∞, where ||| · ||| is the operator norm. [sent-26, score-0.129]
</p><p>10 2 that the chosen notation is justiﬁed, because · M is indeed a norm on the subspace of H where it is ﬁnite, and the dual norm is, for every z ∈ H, given by z M ∗ = sup Mz . [sent-30, score-0.264]
</p><p>11 1 Given a bound on RM (x) we obtain uniform bounds on the estimation error, for example using the following standard result (adapted from Bartlett and Mendelson 2002), where the Lipschitz function φ is to be interpreted as a loss function. [sent-43, score-0.093]
</p><p>12 , Xn ) be a vector of iid random variables with values in H, let X be iid to X1 , let φ : R → [0, 1] have Lipschitz constant L and δ ∈ (0, 1). [sent-47, score-0.126]
</p><p>13 Then with probability at least 1 − δ in the draw of X it holds, for every β ∈ Rd with β M ≤ 1, that Eφ ( β, X ) ≤  1 n ∑ φ ( β, Xi ) + L RM (X) + n i=1  9 ln 2/δ . [sent-48, score-0.124]
</p><p>14 2n  A similar (slightly better) bound is obtained if RM (X) is replaced by its expectation RM = ERM (X) (see Bartlett and Mendelson 2002). [sent-49, score-0.068]
</p><p>15 The following is the main result of this paper and leads to consistency proofs and ﬁnite sample generalization guarantees for all algorithms which use a regularizer of the form (1). [sent-50, score-0.082]
</p><p>16 Then    RM (x) ≤  ≤  23/2 n  23/2 n  n  sup  ∑  M∈M i=1 n  ∑  i=1  Mxi  2   2 +  xi 2 ∗ 2 + M  ln M   ln   ∑  M∈M  ∑i Mxi  2  sup ∑ j Nx j  N∈M     2  . [sent-57, score-0.594]
</p><p>17 In this case we can draw the following conclusion: If we have an a priori bound on X M ∗ for some data distribution, say X M ∗ ≤ C, and X = (X1 , . [sent-63, score-0.068]
</p><p>18 , Xn ), with Xi iid to X, then 23/2C RM (X) ≤ √ 2 + ln M , n thus passing from a data-dependent to a distribution dependent bound. [sent-66, score-0.187]
</p><p>19 2 But the ﬁrst bound in Theorem 2 can be considerably smaller than the second and may be ﬁnite even if M is inﬁnite. [sent-71, score-0.068]
</p><p>20 Corollary 3 Under the conditions of Theorem 2 we have  RM (x) ≤  23/2 n  sup  ∑  Mxi  2  2+  ln  M∈M i  1 ∑ Mxi n ∑ M∈M i  2  2 +√ . [sent-73, score-0.284]
</p><p>21 To obtain a distribution dependent bound we retain the condition X M ∗ ≤ C and replace ﬁniteness of M by the condition that R2 := E  ∑  MX  2  < ∞. [sent-76, score-0.068]
</p><p>22 (3)  M∈M  Taking the expectation in Corollary 3 and using Jensen’s inequality then gives a bound on the expected Rademacher complexity √ 23/2C 2 2 + ln R2 + √ . [sent-77, score-0.256]
</p><p>23 We note that the numerical implementation and practical application of speciﬁc cases of the regularizer described here have been addressed in detail in a number of papers. [sent-85, score-0.082]
</p><p>24 Examples Before giving the examples we mention a great simpliﬁcation in the deﬁnition of the norm · M which occurs when the members of M have mutually orthogonal ranges. [sent-94, score-0.177]
</p><p>25 If, in addition, every member of M is an orthogonal projection P, the norm further simpliﬁes to β M = ∑ Pβ , P∈M  and the quantity R2 occurring in the second moment condition (3) simpliﬁes to R2 = E  ∑  PX  2  =E X  2  . [sent-96, score-0.14]
</p><p>26 , Xn ) will be a generic iid random vector of data points, Xi ∈ H, and X will be a generic data variable, iid to Xi . [sent-100, score-0.126]
</p><p>27 Then β M = β , z M ∗ = z , and the bound on the empirical Rademacher complexity becomes  RM (x) ≤  25/2 n  ∑  xi 2 ,  i  worse by a constant factor of 23/2 than the corresponding result in Bartlett and Mendelson (2002), a tribute paid to the generality of our result. [sent-104, score-0.094]
</p><p>28 The bound on RM (x) now reads √ 23/2 RM (x) ≤ ∑ xi 2 2 + ln d . [sent-111, score-0.24]
</p><p>29 ∞ n i If X  ∞  ≤ 1 almost surely we obtain √ 23/2 2 + ln d , n  RM (X) ≤ √  which agrees with the bound in Kakade et al. [sent-112, score-0.218]
</p><p>30 674  S TRUCTURED S PARSITY AND G ENERALIZATION  Our last bound is useless if d ≥ en or if d is inﬁnite. [sent-114, score-0.068]
</p><p>31 But whenever the norm of the data has ﬁnite second moments we can use Corollary 3 and inequality (4) to obtain 23/2 n  RM (X) ≤ √  2+  ln E X  2 2  2 +√ . [sent-115, score-0.24]
</p><p>32 Then β M = ∑ α−1 |βk | k k  and z M ∗ = sup αk |zk | . [sent-130, score-0.16]
</p><p>33 n n  RM (X) ≤ √  So in this case the second moment bound is enforced by the weighting sequence. [sent-135, score-0.1]
</p><p>34 The ranges of the PJℓ then provide an orthogonal decomposition of Rd and the above mentioned simpliﬁcations also apply. [sent-148, score-0.09]
</p><p>35 ℓ=1 The algorithm which uses β M as a regularizer is called the group Lasso (see, for example, Yuan and Lin 2006). [sent-150, score-0.136]
</p><p>36 , r} then we get √ 23/2 2 + ln r , n  RM (X) ≤ √  (5)  in complete symmetry with the Lasso and essentially the same as given in Kakade et al. [sent-155, score-0.124]
</p><p>37 5 Overlapping Groups In the previous examples the members of M always had mutually orthogonal ranges, which gave a simple appearance to the norm β M . [sent-160, score-0.177]
</p><p>38 If the ranges are not mutually orthogonal, the norm has a more complicated form. [sent-161, score-0.123]
</p><p>39 For example, in the group Lasso setting, if the groups Jℓ cover {1, . [sent-162, score-0.078]
</p><p>40 , d}, but are not disjoint, we obtain the regularizer of Obozinski et al. [sent-165, score-0.082]
</p><p>41 , r} then the Rademacher complexity of the set of linear functionals with Ωoverlap (β) ≤ 1 is bounded as in (5), in complete equivalence to the bound for the group Lasso. [sent-170, score-0.145]
</p><p>42 The same bound also holds for the class satisfying Ωgroup (β) ≤ 1, where the function Ωgroup is deﬁned, for every β ∈ Rd , as Ωgroup (β) =  r  ∑  PJℓ β  ℓ=1  which has been proposed by Jenatton et al. [sent-171, score-0.068]
</p><p>43 The bound obtained from ℓ=1 this simple comparison may however be quite loose. [sent-175, score-0.068]
</p><p>44 6 Regularizers Generated from Cones Our next example considers structured sparsity regularizers as in Micchelli et al. [sent-177, score-0.139]
</p><p>45 (2011) that ΩΛ is a norm and that the dual norm is given by   1/2  d  z Λ∗ = sup µ j z2 : µ j = λ/ λ 1 with λ ∈ Λ . [sent-181, score-0.264]
</p><p>46 If E (Λ) is ﬁnite and x is a sample then the Rademacher complexity of the class with ΩΛ (β) ≤ 1 is bounded by 23/2 n  n  ∑  xi  i=1  2 Λ∗  2+  ln |E (Λ)| . [sent-186, score-0.15]
</p><p>47 7 Kernel Learning This is the most general case to which the simpliﬁcation applies: Suppose that H is the direct sum H = ⊕ j∈J H j of an at most countable number of Hilbert spaces H j . [sent-188, score-0.099]
</p><p>48 Then  ∑  β M =  Pj β  j∈J  and z M ∗ = sup Pj z . [sent-190, score-0.16]
</p><p>49 Let φ j : X → H j be the feature map representation associated with kernel K j , so that, for every x,t ∈ X K j (x,t) = φ j (x), φ j (t) (for background on kernel methods see, for example, Shawe-Taylor and Cristianini 2004). [sent-195, score-0.076]
</p><p>50 Deﬁne the kernel matrix K j = (K j (xi , xk ))n . [sent-200, score-0.101]
</p><p>51 i,k=1 Using this notation the bound in Theorem 2 reads  R ((φ(x1 ), . [sent-201, score-0.09]
</p><p>52 , φ(xn ))) ≤  23/2 n  sup trK j 2 + j∈J  ln  ∑ j∈J trK j sup j∈J trK j  . [sent-204, score-0.444]
</p><p>53 In particular, if J is ﬁnite and K j (x, x) ≤ 1 for every x ∈ X and j ∈ J , then the the bound reduces to 23/2 √ 2+ n  ln |J | ,  essentially in agreement with Cortes et al. [sent-205, score-0.192]
</p><p>54 j∈J  677  M AURER AND P ONTIL  We conclude this section by noting that, for every set M we may choose a set of kernels such that empirical risk minimization with the norm · M is equivalent to multiple kernel learning with kernels KM (x,t) = Mx, Mt , M ∈ M . [sent-211, score-0.15]
</p><p>55 The following concentration inequality, known as the bounded difference inequality (see McDiarmid 1998), goes back to the work of Hoeffding (1963). [sent-238, score-0.088]
</p><p>56 Theorem 4 Let F : X n → R and write B2 =  n  sup ∑ y ,y ∈X , x∈X  k=1  1  2  n  (F (xk←y1 ) − F (xk←y2 ))2 . [sent-240, score-0.16]
</p><p>57 Then ∞ δ  exp  −t 2 2a2  dt ≤ 678  a2 −δ2 exp δ 2a2  . [sent-247, score-0.171]
</p><p>58 Thus ∞ δ  exp  −t 2 2a2  ∞  dt = a  δ/a  e−t  2 /2  dt ≤  a2 δ  ∞ δ/a  te−t  2 /2  dt =  −δ2 a2 exp δ 2a2  . [sent-249, score-0.373]
</p><p>59 2 Properties of the Regularizer In this section, we show that the regularizer in Equation (1) is indeed a norm and we derive the associated dual norm. [sent-251, score-0.134]
</p><p>60 Condition 6 M is an at most countable set of symmetric bounded linear operators on a real separable Hilbert space H such that (a) For every x ∈ H with x = 0, there exists M ∈ M such that Mx = 0 (b) supM∈M |||M||| < ∞ if q = 1 and ∑M∈M |||M||| p < ∞ if q > 1. [sent-257, score-0.129]
</p><p>61 For z ∈ H the norm of the linear functional β ∈ ℓq (M ) → β, z is   sup Mz , if q = 1,  M∈M 1/p z Mq∗ =   ∑ Mz p , if q > 1. [sent-263, score-0.212]
</p><p>62 If w = (wM )M∈M is an H-valued sequence indexed by M , then the linear functional v ∈ Vq (M ) → has norm  ∑  vM , wM  M∈M    sup MwM ,  M∈M  w Vq (M )∗ =     ∑  vM  if q = 1, 1/p  p  ,  if q > 1. [sent-266, score-0.212]
</p><p>63 By Condition 6(b) and H¨ lder’s inequality A is a bounded linear transformation whose kernel K o is therefore closed, making the quotient space Vq (M ) /K into a Banach space with quotient norm w+K  Q  = inf  v Vq (M ) : w − v ∈ K . [sent-270, score-0.234]
</p><p>64 ˆ ˆ The range of A is ℓq (M ) and becomes a Banach space with the norm A−1 (β) Q  = inf  . [sent-272, score-0.086]
</p><p>65 Then z Mq∗  = sup  z, β : β Mq ≤ 1  = sup  z, Av : v Vq (M ) ≤ 1  = sup  A∗ z, v : v Vq (M ) ≤ 1  =  A∗ z Vq (M )∗  1/p  =  sup Mz if q = 1 or M∈M  ∑  M∈M  680  Mz  p  if q > 1. [sent-282, score-0.64]
</p><p>66 S TRUCTURED S PARSITY AND G ENERALIZATION  Proposition 8 If the ranges of the members of M are mutually orthogonal then for β ∈ ℓ1 (M )  ∑  β M =  M+β ,  M∈M  where M + is the pseudoinverse of M. [sent-283, score-0.159]
</p><p>67 Proof The ranges of the members of M provide an orthogonal decomposition of H, so  ∑  β=  M M+β ,  M∈M  where we used the fact that MM + is the orthogonal projection onto the range of M. [sent-284, score-0.178]
</p><p>68 3 Bounds for the ℓ1 (M )-Norm Regularizer We use the bounded difference inequality to derive a concentration inequality for linearly transformed random vectors. [sent-288, score-0.152]
</p><p>69 By the triangle inequality n  ∑  sup  ∑  sup  n  k=1 y1 ,y2 ∈[−1,1], x∈[−1,1] n  ≤ =  (F (xk←y1 ) − F (xk←y2 ))2  n  k=1 y1 ,y2 ∈[−1,1], x∈[−1,1] n  ∑ y ,y sup (y1 − y2 )2 ∈[−1,1]  k=1  1  ≤ 4 M  2  M (xk←y1 − xk←y2 ) Mek  2  2  2 HS . [sent-298, score-0.544]
</p><p>70 (ii) If ε is orthonormal then it follows from Jensen’s inequality that   E Mε ≤ E  2  n  ∑ εi Mei  1/2   i=1  1/2  n  =  ∑  Mei  2  = M  HS . [sent-300, score-0.104]
</p><p>71 We now use integration by parts, a union bound and the above concentration inequality to derive a bound on the expectation of the supremum of the norms Mε . [sent-305, score-0.272]
</p><p>72 682  S TRUCTURED S PARSITY AND G ENERALIZATION  Lemma 10 Let M be an at most countable set of linear transformations M : Rn → H and ε = (ε1 , . [sent-309, score-0.163]
</p><p>73 Then √ M 2 ∑ HS E sup Mε ≤ 2 sup M HS 2 + ln M∈M . [sent-313, score-0.444]
</p><p>74 We now use integration by  HS  ∞  E sup Mε  =  sup Mε > t dt  Pr 0  M∈M  M∈M ∞  ≤ M∞ + δ +  M∞ +δ  ≤ M∞ + δ +  ∑  sup Mε > t dt  Pr  M∈M ∞  M∈M  M∞ +δ  Pr { Mε > t} dt,  where we have introduced a parameter δ ≥ 0. [sent-315, score-0.708]
</p><p>75 Substitution in the previous chain of inequalities and using Hoelder’s inequality (in the ℓ1 /ℓ∞ -version) give E sup Mε ≤ M∞ + δ + M∈M  1 δ  ∑  M  2 HS  exp  M∈M  −δ2 2 2M∞  . [sent-318, score-0.259]
</p><p>76 (8)  We now set δ = M∞  2 ln e  ∑M∈M M 2 M∞  2 HS  . [sent-319, score-0.124]
</p><p>77 The substitution makes the last term in (8) smaller than M∞ / e 2 , and √ √ since 1 + 1/ e 2 < 2, we obtain  √ E sup Mε ≤ 2M∞ 1 + M∈M  683  ln  e ∑M∈M M 2 M∞  2 HS    . [sent-321, score-0.319]
</p><p>78 M AURER AND P ONTIL  Finally we use  √  √ ln es ≤ 1 + ln s for s ≥ 1. [sent-322, score-0.248]
</p><p>79 We have i=1 2 n  n  β, ∑ εi xi  RM (x) = E sup  β: β M ≤1  i=1  n  2 ≤ E n  ∑ εi xi  i=1  M∗  2 = E sup Mxε . [sent-328, score-0.372]
</p><p>80 n M∈M  Applying Lemma 10 to the set of transformations M x = Mx : M ∈ M  RM (x) ≤ Substitution of Mx  2 HS  23/2 supM∈M Mx n 2  = ∑n Mxi i=1  M∈M  2+  ln  2 ∑M∈M Mx HS supM∈M Mx 2 HS  . [sent-329, score-0.188]
</p><p>81 gives the ﬁrst inequality of Theorem 2 and  2 HS  sup Mx  HS  gives  n  ≤ ∑ sup Mxi i=1 M∈M  2  n  = ∑ xi 2 ∗ M i=1  gives the second inequality. [sent-330, score-0.41]
</p><p>82 For A, B > 0 and n ∈ N this implies that B = n [(A/n) ln (B/n) − (A/n) ln (A/n)] ≤ A ln (B/n) + n/e. [sent-332, score-0.372]
</p><p>83 A Now multiply out the ﬁrst inequality of Theorem 2 and use (9) with A ln  n  A = sup  ∑  Mxi  2  n  and B =  M∈M i=1  Finally use  √  a+b ≤  ∑ ∑  Mxi  2  (9)  . [sent-333, score-0.348]
</p><p>84 The second result is not dimension free, but it approaches the bound in Theorem 2 for arbitrarily large dimensions. [sent-342, score-0.068]
</p><p>85 M  S TRUCTURED S PARSITY AND G ENERALIZATION  The proof is based on the following Lemma 12 Let M be an at most countable set of linear transformations M : Rn → H and ε = (ε1 , . [sent-346, score-0.189]
</p><p>86 (10)  M∈M  We rewrite the expectation appearing in the right hand side using integration by parts and a change of variable as ∞  E [ Mε p ] = 0  Pr { Mε  p  ∞  > t} dt = A p + p 0  p  Pr { Mε  > s p + A p } s p−1 ds  (11)  where A ≥ 0. [sent-352, score-0.127]
</p><p>87 (1 − λ)1/p  This allows us to bound Pr { Mε  p  > s p + A p } ≤ Pr = Pr  Mε  p  Mε > λ  p−1 p  Combining Equations (11) and (12), choosing A = (1 − λ) variable t = λ  p−1 p  ∞  s + (1 − λ)  s + (1 − λ)  1−p p  M  HS  p−1 p  p−1 p  p  A  A . [sent-354, score-0.068]
</p><p>88 One can verify that the leading constant in our bound is smaller than the one in Cortes et al. [sent-366, score-0.068]
</p><p>89 Theorem 13 Under the conditions of Theorem 11  RMq (x) ≤  4 M n  1/p  sup  ∑  Mxi  2  M∈M i  2+  ln ∑ M  ∑i Mxi  2  supN∈M ∑i Nxi  2  . [sent-371, score-0.284]
</p><p>90 The key step in the proof of Theorem 13 is the following 686  S TRUCTURED S PARSITY AND G ENERALIZATION  Lemma 14 Let M be a ﬁnite set of linear transformations M : Rn → H and ε = (ε1 , . [sent-373, score-0.09]
</p><p>91 Then   1/p  ∑  E  Mε  p    ≤2 M  M  p  Proof If t ≥ 0 and ∑M Mε  ∑  Mε  p  sup M  HS  M∈M  2+  ln  2 ∑M M HS supN∈M N 2 HS  > t p , then there must exist some M ∈ M such that Mε  which in turn implies that Mε > t/ M Pr  1/p  > tp  ≤ ∑ Pr  1/p  p  > t p/ M ,  . [sent-377, score-0.284]
</p><p>92 It then follows from a union bound that 1/p  Mε > t/ M  ≤ exp  M  M  . [sent-378, score-0.103]
</p><p>93 4 M  −t 2  2/p  M  2 HS  ,  where we used the subgaussian concentration inequality Lemma 9-(ii) with r = 2. [sent-379, score-0.088]
</p><p>94 We now o substitute e ∑M M 2 1/p HS δ=2 M sup M HS ln supN∈M N 2 M∈M HS and use 1 + 1/e ≤ 2 to arrive at the conclusion. [sent-381, score-0.284]
</p><p>95 This gives   E  1/p  ∑ M  Mxε  p    ≤2 M  1/p  sup Mx M∈M  HS  2+  We now proceed as in the proof of Theorem 11 to obtain the result. [sent-383, score-0.186]
</p><p>96 Conclusion and Future Work We have presented a bound on the Rademacher average for linear function classes described by inﬁmum convolution norms which are associated with a class of bounded linear operators on a Hilbert space. [sent-386, score-0.098]
</p><p>97 When the bound is applied to speciﬁc cases (ℓ2 , ℓ1 , mixed ℓ1 /ℓ2 norms) it recovers existing bounds (up to small changes in the constants). [sent-388, score-0.093]
</p><p>98 Speciﬁcally, we have shown that the bound can be applied in inﬁnite dimensional settings, provided that the moment condition (3) is satisﬁed. [sent-390, score-0.1]
</p><p>99 We have also applied the bound to multiple kernel learning. [sent-391, score-0.106]
</p><p>100 While in the standard case the bound is only slightly worse in the constants, the bound is potentially smaller and applies to the more general case in which there is a countable set of kernels, provided the expectation of the sum of the kernels is bounded. [sent-392, score-0.265]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hs', 0.579), ('vq', 0.265), ('mx', 0.258), ('mxi', 0.211), ('aurer', 0.177), ('ontil', 0.177), ('vm', 0.164), ('pj', 0.164), ('sup', 0.16), ('parsity', 0.136), ('tructured', 0.136), ('lasso', 0.128), ('ln', 0.124), ('mq', 0.11), ('mz', 0.106), ('supm', 0.106), ('eneralization', 0.105), ('pr', 0.104), ('dt', 0.101), ('countable', 0.099), ('rademacher', 0.088), ('regularizer', 0.082), ('regularizers', 0.076), ('mvm', 0.076), ('rm', 0.073), ('rmq', 0.071), ('supn', 0.071), ('bound', 0.068), ('cortes', 0.065), ('inequality', 0.064), ('banach', 0.064), ('transformations', 0.064), ('iid', 0.063), ('xk', 0.063), ('mendelson', 0.06), ('orthogonal', 0.056), ('kakade', 0.055), ('group', 0.054), ('mei', 0.053), ('trk', 0.053), ('norm', 0.052), ('micchelli', 0.051), ('obozinski', 0.05), ('hilbert', 0.046), ('morales', 0.045), ('maurer', 0.045), ('jenatton', 0.045), ('meir', 0.041), ('structured', 0.04), ('orthonormal', 0.04), ('bartlett', 0.039), ('andreas', 0.038), ('kernel', 0.038), ('mutually', 0.037), ('av', 0.037), ('baldassarre', 0.035), ('mol', 0.035), ('shimamura', 0.035), ('exp', 0.035), ('substitution', 0.035), ('theorem', 0.035), ('inf', 0.034), ('overlap', 0.034), ('ranges', 0.034), ('ying', 0.033), ('xn', 0.033), ('lanckriet', 0.033), ('moment', 0.032), ('members', 0.032), ('kloft', 0.031), ('kernels', 0.03), ('baraniuk', 0.03), ('panchenko', 0.03), ('rd', 0.03), ('operators', 0.03), ('assertion', 0.027), ('nx', 0.027), ('meier', 0.027), ('xi', 0.026), ('proof', 0.026), ('integration', 0.026), ('surely', 0.026), ('bounds', 0.025), ('ledoux', 0.025), ('lighten', 0.025), ('lemma', 0.025), ('pontil', 0.025), ('groups', 0.024), ('concentration', 0.024), ('functionals', 0.023), ('quotient', 0.023), ('massimiliano', 0.023), ('sparsity', 0.023), ('supremum', 0.022), ('wm', 0.022), ('lounici', 0.022), ('pd', 0.022), ('campbell', 0.022), ('reads', 0.022), ('geer', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="111-tfidf-1" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>Author: Andreas Maurer, Massimiliano Pontil</p><p>Abstract: We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an inﬁnite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels. Keywords: empirical processes, Rademacher average, sparse estimation.</p><p>2 0.1314522 <a title="111-tfidf-2" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>Author: Jian Huang, Cun-Hui Zhang</p><p>Abstract: The ℓ1 -penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ1 -penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1 -penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results. Keywords: variable selection, penalized estimation, oracle inequality, generalized linear models, selection consistency, sparsity</p><p>3 0.120288 <a title="111-tfidf-3" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>Author: Sham M. Kakade, Shai Shalev-Shwartz, Ambuj Tewari</p><p>Abstract: There is growing body of learning problems for which it is natural to organize the parameters into a matrix. As a result, it becomes easy to impose sophisticated prior knowledge by appropriately regularizing the parameters under some matrix norm. This work describes and analyzes a systematic method for constructing such matrix-based regularization techniques. In particular, we focus on how the underlying statistical properties of a given problem can help us decide which regularization function is appropriate. Our methodology is based on a known duality phenomenon: a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. We demonstrate the potential of this framework by deriving novel generalization and regret bounds for multi-task learning, multi-class learning, and multiple kernel learning. Keywords: regularization, strong convexity, regret bounds, generalization bounds, multi-task learning, multi-class learning, multiple kernel learning</p><p>4 0.10603953 <a title="111-tfidf-4" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>Author: Wojciech Rejchel</p><p>Abstract: The problem of ranking is to predict or to guess the ordering between objects on the basis of their observed features. In this paper we consider ranking estimators that minimize the empirical convex risk. We prove generalization bounds for the excess risk of such estimators with rates that are 1 faster than √n . We apply our results to commonly used ranking algorithms, for instance boosting or support vector machines. Moreover, we study the performance of considered estimators on real data sets. Keywords: convex risk minimization, excess risk, support vector machine, empirical process, U-process</p><p>5 0.10405479 <a title="111-tfidf-5" href="./jmlr-2012-On_the_Convergence_Rate_oflp-Norm_Multiple_Kernel_Learning.html">81 jmlr-2012-On the Convergence Rate oflp-Norm Multiple Kernel Learning</a></p>
<p>Author: Marius Kloft, Gilles Blanchard</p><p>Abstract: We derive an upper bound on the local Rademacher complexity of ℓ p -norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p = 1 only while our analysis covers all cases 1 ≤ p ≤ ∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding excess loss, namely α fast convergence rates of the order O(n− 1+α ), where α is the minimum eigenvalue decay rate of the individual kernels. Keywords: multiple kernel learning, learning kernels, generalization bounds, local Rademacher complexity</p><p>6 0.091413528 <a title="111-tfidf-6" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>7 0.089220352 <a title="111-tfidf-7" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>8 0.083892792 <a title="111-tfidf-8" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>9 0.082575671 <a title="111-tfidf-9" href="./jmlr-2012-Algorithms_for_Learning_Kernels_Based_on_Centered_Alignment.html">16 jmlr-2012-Algorithms for Learning Kernels Based on Centered Alignment</a></p>
<p>10 0.07959675 <a title="111-tfidf-10" href="./jmlr-2012-Activized_Learning%3A_Transforming_Passive_to_Active_with_Improved_Label_Complexity.html">14 jmlr-2012-Activized Learning: Transforming Passive to Active with Improved Label Complexity</a></p>
<p>11 0.076310806 <a title="111-tfidf-11" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>12 0.061691433 <a title="111-tfidf-12" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>13 0.061633173 <a title="111-tfidf-13" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>14 0.061065122 <a title="111-tfidf-14" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<p>15 0.056280449 <a title="111-tfidf-15" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<p>16 0.054273341 <a title="111-tfidf-16" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>17 0.050649036 <a title="111-tfidf-17" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>18 0.050553657 <a title="111-tfidf-18" href="./jmlr-2012-PAC-Bayes_Bounds_with_Data_Dependent_Priors.html">87 jmlr-2012-PAC-Bayes Bounds with Data Dependent Priors</a></p>
<p>19 0.050481822 <a title="111-tfidf-19" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>20 0.049939625 <a title="111-tfidf-20" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.236), (1, 0.102), (2, -0.222), (3, 0.113), (4, -0.002), (5, 0.003), (6, -0.021), (7, -0.06), (8, -0.005), (9, 0.001), (10, 0.006), (11, -0.094), (12, 0.07), (13, -0.018), (14, 0.007), (15, -0.244), (16, 0.098), (17, 0.03), (18, -0.009), (19, -0.003), (20, 0.084), (21, 0.023), (22, 0.182), (23, -0.158), (24, 0.048), (25, -0.096), (26, 0.19), (27, 0.015), (28, -0.007), (29, -0.046), (30, -0.02), (31, 0.056), (32, 0.197), (33, -0.009), (34, 0.039), (35, 0.057), (36, 0.096), (37, 0.029), (38, -0.038), (39, 0.087), (40, 0.026), (41, -0.023), (42, 0.052), (43, 0.055), (44, 0.098), (45, -0.032), (46, 0.07), (47, -0.105), (48, -0.021), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93201268 <a title="111-lsi-1" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>Author: Andreas Maurer, Massimiliano Pontil</p><p>Abstract: We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an inﬁnite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels. Keywords: empirical processes, Rademacher average, sparse estimation.</p><p>2 0.60626227 <a title="111-lsi-2" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>Author: Jian Huang, Cun-Hui Zhang</p><p>Abstract: The ℓ1 -penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ1 -penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1 -penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results. Keywords: variable selection, penalized estimation, oracle inequality, generalized linear models, selection consistency, sparsity</p><p>3 0.54718739 <a title="111-lsi-3" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>Author: Sham M. Kakade, Shai Shalev-Shwartz, Ambuj Tewari</p><p>Abstract: There is growing body of learning problems for which it is natural to organize the parameters into a matrix. As a result, it becomes easy to impose sophisticated prior knowledge by appropriately regularizing the parameters under some matrix norm. This work describes and analyzes a systematic method for constructing such matrix-based regularization techniques. In particular, we focus on how the underlying statistical properties of a given problem can help us decide which regularization function is appropriate. Our methodology is based on a known duality phenomenon: a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. We demonstrate the potential of this framework by deriving novel generalization and regret bounds for multi-task learning, multi-class learning, and multiple kernel learning. Keywords: regularization, strong convexity, regret bounds, generalization bounds, multi-task learning, multi-class learning, multiple kernel learning</p><p>4 0.49969047 <a title="111-lsi-4" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>Author: Matus Telgarsky</p><p>Abstract: Boosting combines weak learners into a predictor with low empirical risk. Its dual constructs a high entropy distribution upon which weak learners and training labels are uncorrelated. This manuscript studies this primal-dual relationship under a broad family of losses, including the exponential loss of AdaBoost and the logistic loss, revealing: • Weak learnability aids the whole loss family: for any ε > 0, O (ln(1/ε)) iterations sufﬁce to produce a predictor with empirical risk ε-close to the inﬁmum; • The circumstances granting the existence of an empirical risk minimizer may be characterized in terms of the primal and dual problems, yielding a new proof of the known rate O (ln(1/ε)); • Arbitrary instances may be decomposed into the above two, granting rate O (1/ε), with a matching lower bound provided for the logistic loss. Keywords: boosting, convex analysis, weak learnability, coordinate descent, maximum entropy</p><p>5 0.48074332 <a title="111-lsi-5" href="./jmlr-2012-On_the_Convergence_Rate_oflp-Norm_Multiple_Kernel_Learning.html">81 jmlr-2012-On the Convergence Rate oflp-Norm Multiple Kernel Learning</a></p>
<p>Author: Marius Kloft, Gilles Blanchard</p><p>Abstract: We derive an upper bound on the local Rademacher complexity of ℓ p -norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p = 1 only while our analysis covers all cases 1 ≤ p ≤ ∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding excess loss, namely α fast convergence rates of the order O(n− 1+α ), where α is the minimum eigenvalue decay rate of the individual kernels. Keywords: multiple kernel learning, learning kernels, generalization bounds, local Rademacher complexity</p><p>6 0.46307305 <a title="111-lsi-6" href="./jmlr-2012-Algorithms_for_Learning_Kernels_Based_on_Centered_Alignment.html">16 jmlr-2012-Algorithms for Learning Kernels Based on Centered Alignment</a></p>
<p>7 0.45710599 <a title="111-lsi-7" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>8 0.45417419 <a title="111-lsi-8" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>9 0.44338927 <a title="111-lsi-9" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>10 0.44274664 <a title="111-lsi-10" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<p>11 0.44112077 <a title="111-lsi-11" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>12 0.36833474 <a title="111-lsi-12" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>13 0.3310504 <a title="111-lsi-13" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>14 0.32727116 <a title="111-lsi-14" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<p>15 0.31171957 <a title="111-lsi-15" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>16 0.27707103 <a title="111-lsi-16" href="./jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</a></p>
<p>17 0.27268058 <a title="111-lsi-17" href="./jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</a></p>
<p>18 0.27120739 <a title="111-lsi-18" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>19 0.26652858 <a title="111-lsi-19" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>20 0.25396574 <a title="111-lsi-20" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.03), (21, 0.069), (26, 0.054), (29, 0.03), (49, 0.043), (64, 0.012), (69, 0.013), (75, 0.083), (77, 0.01), (79, 0.022), (80, 0.32), (92, 0.131), (96, 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73426151 <a title="111-lda-1" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>Author: Andreas Maurer, Massimiliano Pontil</p><p>Abstract: We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an inﬁnite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels. Keywords: empirical processes, Rademacher average, sparse estimation.</p><p>2 0.50876719 <a title="111-lda-2" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>Author: Matus Telgarsky</p><p>Abstract: Boosting combines weak learners into a predictor with low empirical risk. Its dual constructs a high entropy distribution upon which weak learners and training labels are uncorrelated. This manuscript studies this primal-dual relationship under a broad family of losses, including the exponential loss of AdaBoost and the logistic loss, revealing: • Weak learnability aids the whole loss family: for any ε > 0, O (ln(1/ε)) iterations sufﬁce to produce a predictor with empirical risk ε-close to the inﬁmum; • The circumstances granting the existence of an empirical risk minimizer may be characterized in terms of the primal and dual problems, yielding a new proof of the known rate O (ln(1/ε)); • Arbitrary instances may be decomposed into the above two, granting rate O (1/ε), with a matching lower bound provided for the logistic loss. Keywords: boosting, convex analysis, weak learnability, coordinate descent, maximum entropy</p><p>3 0.50153655 <a title="111-lda-3" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>Author: Sangkyun Lee, Stephen J. Wright</p><p>Abstract: Iterative methods that calculate their steps from approximate subgradient directions have proved to be useful for stochastic learning problems over large and streaming data sets. When the objective consists of a loss function plus a nonsmooth regularization term, the solution often lies on a lowdimensional manifold of parameter space along which the regularizer is smooth. (When an ℓ1 regularizer is used to induce sparsity in the solution, for example, this manifold is deﬁned by the set of nonzero components of the parameter vector.) This paper shows that a regularized dual averaging algorithm can identify this manifold, with high probability, before reaching the solution. This observation motivates an algorithmic strategy in which, once an iterate is suspected of lying on an optimal or near-optimal manifold, we switch to a “local phase” that searches in this manifold, thus converging rapidly to a near-optimal point. Computational results are presented to verify the identiﬁcation property and to illustrate the effectiveness of this approach. Keywords: regularization, dual averaging, partly smooth manifold, manifold identiﬁcation</p><p>4 0.49950999 <a title="111-lda-4" href="./jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</a></p>
<p>Author: Ji Liu, Peter Wonka, Jieping Ye</p><p>Abstract: We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ Rn×m (m ≫ n) and a noisy observation vector y ∈ Rn satisfying y = Xβ∗ + ε where ε is the noise vector following a Gaussian distribution N(0, σ2 I), how to recover the signal (or parameter vector) β∗ when the signal is sparse? The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively reﬁnes the target signal β∗ . We show that if X obeys a certain condition, then with a large probability ˆ the difference between the solution β estimated by the proposed method and the true solution β∗ measured in terms of the ℓ p norm (p ≥ 1) is bounded as ˆ β − β∗ p ≤ C(s − N)1/p log m + ∆ σ, where C is a constant, s is the number of nonzero entries in β∗ , the risk of the oracle estimator ∆ is independent of m and is much smaller than the ﬁrst term, and N is the number of entries of β∗ larger √ than a certain value in the order of O (σ log m). The proposed √ method improves the estimation √ bound of the standard Dantzig selector approximately from Cs1/p log mσ to C(s − N)1/p log mσ where the value N depends on the number of large entries in β∗ . When N = s, the proposed algorithm achieves the oracle solution with a high probability, where the oracle solution is the projection of the observation vector y onto true features. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector. Finally, we extend this multi-stage procedure to the LASSO case. Keywords: multi-stage, Dantzig selector, LASSO, sparse signal recovery</p><p>5 0.4987736 <a title="111-lda-5" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>Author: Lan Xue, Annie Qu</p><p>Abstract: The varying-coefﬁcient model is ﬂexible and powerful for modeling the dynamic changes of regression coefﬁcients. It is important to identify signiﬁcant covariates associated with response variables, especially for high-dimensional settings where the number of covariates can be larger than the sample size. We consider model selection in the high-dimensional setting and adopt difference convex programming to approximate the L0 penalty, and we investigate the global optimality properties of the varying-coefﬁcient estimator. The challenge of the variable selection problem here is that the dimension of the nonparametric form for the varying-coefﬁcient modeling could be inﬁnite, in addition to dealing with the high-dimensional linear covariates. We show that the proposed varying-coefﬁcient estimator is consistent, enjoys the oracle property and achieves an optimal convergence rate for the non-zero nonparametric components for high-dimensional data. Our simulations and numerical examples indicate that the difference convex algorithm is efﬁcient using the coordinate decent algorithm, and is able to select the true model at a higher frequency than the least absolute shrinkage and selection operator (LASSO), the adaptive LASSO and the smoothly clipped absolute deviation (SCAD) approaches. Keywords: coordinate decent algorithm, difference convex programming, L0 - regularization, large-p small-n, model selection, nonparametric function, oracle property, truncated L1 penalty</p><p>6 0.498191 <a title="111-lda-6" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>7 0.49778581 <a title="111-lda-7" href="./jmlr-2012-On_the_Convergence_Rate_oflp-Norm_Multiple_Kernel_Learning.html">81 jmlr-2012-On the Convergence Rate oflp-Norm Multiple Kernel Learning</a></p>
<p>8 0.49479714 <a title="111-lda-8" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>9 0.49348 <a title="111-lda-9" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>10 0.49245709 <a title="111-lda-10" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>11 0.49138123 <a title="111-lda-11" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>12 0.49036074 <a title="111-lda-12" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>13 0.49035889 <a title="111-lda-13" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>14 0.4879148 <a title="111-lda-14" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>15 0.48783171 <a title="111-lda-15" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>16 0.48702165 <a title="111-lda-16" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>17 0.48644546 <a title="111-lda-17" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>18 0.48597139 <a title="111-lda-18" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>19 0.48165411 <a title="111-lda-19" href="./jmlr-2012-Refinement_of_Operator-valued_Reproducing_Kernels.html">96 jmlr-2012-Refinement of Operator-valued Reproducing Kernels</a></p>
<p>20 0.48128149 <a title="111-lda-20" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
