<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-42" href="#">jmlr2012-42</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</h1>
<br/><p>Source: <a title="jmlr-2012-42-pdf" href="http://jmlr.org/papers/volume13/su12a/su12a.pdf">pdf</a></p><p>Author: Xiaogang Su, Joseph Kang, Juanjuan Fan, Richard A. Levine, Xin Yan</p><p>Abstract: Assessing treatment effects in observational studies is a multifaceted problem that not only involves heterogeneous mechanisms of how the treatment or cause is exposed to subjects, known as propensity, but also differential causal effects across sub-populations. We introduce a concept termed the facilitating score to account for both the confounding and interacting impacts of covariates on the treatment effect. Several approaches for estimating the facilitating score are discussed. In particular, we put forward a machine learning method, called causal inference tree (CIT), to provide a piecewise constant approximation of the facilitating score. With interpretable rules, CIT splits data in such a way that both the propensity and the treatment effect become more homogeneous within each resultant partition. Causal inference at different levels can be made on the basis of CIT. Together with an aggregated grouping procedure, CIT stratiﬁes data into strata where causal effects can be conveniently assessed within each. Besides, a feasible way of predicting individual causal effects (ICE) is made available by aggregating ensemble CIT models. Both the stratiﬁed results and the estimated ICE provide an assessment of heterogeneity of causal effects and can be integrated for estimating the average causal effect (ACE). Mean square consistency of CIT is also established. We evaluate the performance of proposed methods with simulations and illustrate their use with the NSW data in Dehejia and Wahba (1999) where the objective is to assess the impact of c 2012 Xiaogang Su, Joseph Kang, Juanjuan Fan, Richard A. Levine and Xin Yan. S U , K ANG , FAN , L EVINE AND YAN a labor training program, the National Supported Work (NSW) demonstration, on post-intervention earnings. Keywords: CART, causal inference, confounding, interaction, observational study, personalized medicine, recursive partitioning</p><p>Reference: <a title="jmlr-2012-42-reference" href="../jmlr2012_reference/jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We introduce a concept termed the facilitating score to account for both the confounding and interacting impacts of covariates on the treatment effect. [sent-12, score-0.744]
</p><p>2 In particular, we put forward a machine learning method, called causal inference tree (CIT), to provide a piecewise constant approximation of the facilitating score. [sent-14, score-0.846]
</p><p>3 With interpretable rules, CIT splits data in such a way that both the propensity and the treatment effect become more homogeneous within each resultant partition. [sent-15, score-0.736]
</p><p>4 Together with an aggregated grouping procedure, CIT stratiﬁes data into strata where causal effects can be conveniently assessed within each. [sent-17, score-0.634]
</p><p>5 Both the stratiﬁed results and the estimated ICE provide an assessment of heterogeneity of causal effects and can be integrated for estimating the average causal effect (ACE). [sent-19, score-0.943]
</p><p>6 Keywords: CART, causal inference, confounding, interaction, observational study, personalized medicine, recursive partitioning  1. [sent-24, score-0.546]
</p><p>7 A confounding variable or confounder is an extraneous covariate that relates to both the treatment and the response and hence inﬂuences the treatment effect estimation. [sent-28, score-0.689]
</p><p>8 The main obstacle is the nonrandom treatment assignment mechanism, in which the subjects select a treatment that they believe best serve their interests or are exposed to a treatment according to individual traits. [sent-33, score-0.807]
</p><p>9 Common approaches include analysis of covariance (ANCOVA), propensity score methods (Rosenbaum and Rubin, 1983), and directed acyclic graphs (DAGs; Pearl 2000 and Spirtes, Glymour, and Scheines 2001). [sent-36, score-0.565]
</p><p>10 An effect modiﬁer is a covariate that interacts with the treatment and changes the direction and/or degree of its causal effect on the outcome. [sent-50, score-0.746]
</p><p>11 Nevertheless, sorting out differential causal effects often entails large data that are collected at post-trial periods, for example, the Medicare or Medicaid databases. [sent-65, score-0.515]
</p><p>12 First of all, confounding emerges as one primary issue in the assessment of the main effect of treatment, also known as the average causal effect (ACE). [sent-67, score-0.553]
</p><p>13 Rubin’s causal model (Rubin, 1974, 1977, 1978, 2005) provides a general framework for making these assessments, within which the treatment effect is ﬁnely calibrated at three different hierarchical levels (i. [sent-77, score-0.671]
</p><p>14 In this article, causal inference is explicitly reformulated as a predictive modeling problem within the framework of Rubin’s causal model. [sent-80, score-0.851]
</p><p>15 To approach, we introduce a concept, termed facilitating score, to address both the confounding and interacting impact of extraneous variables on causal inference. [sent-81, score-0.748]
</p><p>16 Then we put forward a causal inference tree (CIT) procedure, to approximate the facilitating score with a piecewise constant function. [sent-83, score-0.953]
</p><p>17 CIT recursively splits data into disjoint groups in such a way that both treatment assignment mechanisms and the treatment effects become more homogeneous within each group. [sent-84, score-0.601]
</p><p>18 On the basis of CIT, a group of recursive partitioning methods are devised to make causal inference at different levels. [sent-85, score-0.579]
</p><p>19 In Section 2, following an outline of Rubin’s causal inference framework, the concept of facilitating score is introduced and methods for estimating the facilitating score are discussed. [sent-87, score-1.186]
</p><p>20 Facilitating Scores We ﬁrst review Rubin’s causal models, then we introduce the facilitating score concept and discuss methods for estimating the facilitating score. [sent-94, score-1.014]
</p><p>21 1 Causal Inference In Rubin’s causal model (Rubin, 1974, 1977, 1978, 2005), a ﬁne calibration of treatment effect is facilitated by a comparison between the observed outcome on an individual or unit and the potential outcome if the individual had been assigned to the counterfactual treatment group. [sent-96, score-0.975]
</p><p>22 Let T = T (ω) be a binary treatment assignment variable with value 1 if unit ω receives the putative treatment and 0 otherwise. [sent-98, score-0.518]
</p><p>23 The causal effect in a subpopulation {ω : X(ω) ∈ B} is E(Y1 |X ∈ B) − E(Y0 |X ∈ B) for some Borel set B in the predictor space X. [sent-118, score-0.485]
</p><p>24 These three levels form a hierarchy of causal inference in decreasing order of strength, in the sense that knowledge of upper-level causal inferences can be inferred from that of lowered-level causal inferences, but not vice versa. [sent-120, score-1.244]
</p><p>25 A preponderance of the literature in causal inference is centered on schemes for making the population-level inference or estimating ACE under various scenarios. [sent-121, score-0.523]
</p><p>26 Rosenbaum and Rubin (1983) introduced the concept of balancing score to tackle the confounding issue in causal inference. [sent-122, score-0.61]
</p><p>27 They showed that the propensity score e(x) = P(T = 1|X = x), which is deﬁned as the conditional probability of assignment to the treated group given the measured covariates X, is the coarsest balancing score. [sent-126, score-0.751]
</p><p>28 In propensity score analysis, the assumption of strong ignorability plays a pivotal role. [sent-129, score-0.625]
</p><p>29 (2) Equations (1) and (2) provide the basis for propensity score based methods. [sent-141, score-0.565]
</p><p>30 2 Facilitating Score Parallel to confounding, interaction is concerned with differential causal effects among units or subpopulations. [sent-143, score-0.587]
</p><p>31 Speciﬁcally, if substantial differences in causal effects are present at a lower level of inference, then transition to an upper-level inference may not be plausible 2959  S U , K ANG , FAN , L EVINE AND YAN  and conclusions based on upper-level causal effects can be misleading. [sent-146, score-1.017]
</p><p>32 It is therefore critical to take both heterogeneous treatment assignment mechanisms and differential treatment effects into consideration when assessing the treatment effects. [sent-157, score-0.874]
</p><p>33 We use the term ‘facilitating’ because conditioning on a(X) helps facilitate causal inference, in the sense that causal inference within the sub-population Ωa = {ω : a(X(ω)) = a} can be conveniently obtained via direct comparison of sample mean responses. [sent-173, score-0.851]
</p><p>34 This is because both propensity and the treatment effect δa become constant within Ωa . [sent-174, score-0.736]
</p><p>35 Since the propensity e(X) is the coarsest balancing score, it follows that e(X) = e in Ωa . [sent-175, score-0.496]
</p><p>36 ⊥ Assuming that treatment assignment is strongly ignorable, h(X) is a weak facilitating score when 0 < e(X) < 1. [sent-181, score-0.648]
</p><p>37 Theorem 3 basically states that both confounding and interacting effect of X on causal inference with the potential outcomes (Y1 ,Y0 ) can be handled by working with the observed data (Y, T, X). [sent-183, score-0.6]
</p><p>38 With a parametric approach, we assume a model for each of the terms in (4): propensity score model for f (T |X) and outcome regression models for f (Y |T = 0, X) and f (Y |T = 1, X). [sent-197, score-0.626]
</p><p>39 Note that (β1 , β2 ) are involved only in L1 for the outcome regression model while β3 is involved only in L2 for the propensity score model. [sent-209, score-0.6]
</p><p>40 This property not only simpliﬁes the likelihood optimization, but also allows for variable selection to be performed separately for the propensity model and outcome regression models. [sent-210, score-0.493]
</p><p>41 ˆt ˆt With an estimated h(x) = (β2 x, β3 x)t , data can be stratiﬁed via combined use of the medians or terciles of its components, similar to propensity score subclassiﬁcation. [sent-211, score-0.565]
</p><p>42 Both stratiﬁed and individualized causal effect estimates can help depict variations in propensity and treatment effects and make available a natural evaluation of the plausibility of treatment comparability and ACE assessment. [sent-247, score-1.446]
</p><p>43 When restricted to a node τ, the distribution of (Y, T ) no longer depends on X, implying a constant propensity and a constant treatment effect. [sent-256, score-0.763]
</p><p>44 3 Summarizing Strata and ACE Estimation To summarize the ﬁnal K strata obtained from either one single CIT or the aggregated grouping method, estimated propensity rate ek and the treatment effect ∆k can be obtained for each stratum. [sent-370, score-0.894]
</p><p>45 One may take a liberal approach when inspecting differential causal effects across K strata. [sent-373, score-0.515]
</p><p>46 Comparatively, CIT and aggregating grouping offer reﬁned stratiﬁcation so that the causal effect within each resultant stratum Ωa can be correctly captured, which consequently offers improved estimation of ACE. [sent-381, score-0.541]
</p><p>47 Alternatively, one may try to correct the problem with propensity score stratiﬁcation by applying additional ANCOVA-typed adjustment within each stratum. [sent-382, score-0.589]
</p><p>48 In the ﬁrst step, a number of strata are obtained by stratifying propensity scores. [sent-385, score-0.506]
</p><p>49 Strictly speaking, ICE makes conditional causal inference at the subpopulation level {ω : X(ω) = x}. [sent-401, score-0.506]
</p><p>50 (b) ˆ — Estimate causal effects ∆τ and propensity eτ for each τ ∈ T(v) based on L(v) . [sent-417, score-0.934]
</p><p>51 For each terminal node τ ∈ T(v) , estimates of the causal effect and propensity, ˆ ∆τ = yτ1 − yτ0 and eτ = nτ1 /nτ , ¯ ¯ ˆ (b) ˆ (b) are computed using data in L(v) . [sent-430, score-0.658]
</p><p>52 Then we apply T(v) to L(v) and predict the ICE ∆i and propensity (b)  score ei ˆ  for each individual i ∈ Lv . [sent-431, score-0.565]
</p><p>53 The same procedure is repeated for each fold to estimate ICE and propensity scores for all individuals in L . [sent-435, score-0.486]
</p><p>54 When used for ACE estimation, Schafer and Kang (2008) found that it is not among the top performers, but may be possibly improved by incorporating the propensity score into the model. [sent-452, score-0.565]
</p><p>55 To make it more robust to non-random treatment assignment mechanism, it might be possible to incorporate propensity score into the weights as well. [sent-473, score-0.849]
</p><p>56 The following theorem establishes the mean square risk consistency for (yτ1 − yτ0 ), the causal effect ¯ ¯ estimate based on direct comparison of sample means in the terminal node τ = τ(x). [sent-500, score-0.658]
</p><p>57 It is worth noting that the Horvitz-Thompson ( 1952) typed estimator via inverse probability weighting has fundamental use in both causal inference with observational data and in estimation the superpopulation mean with stratiﬁed survey data. [sent-509, score-0.5]
</p><p>58 An interaction tree explicitly models the treatment-by-covariates interactions for detecting differential treatment effects. [sent-521, score-0.504]
</p><p>59 As we shall demonstrate, failure or inadequacy to account for propensity information may lead to misleading interaction results, in that the superﬁcial difference in treatment effects might have been caused merely by heterogenous treatment selection mechanisms. [sent-523, score-1.081]
</p><p>60 In the above simulation strategy, covariate X1 is an exposure or treatment predictor involved in the propensity model only, X2 is a confounder that relates to both T and Y, X3 is a response predictor or prognostic factor, X4 is an effect-modiﬁer, and X5 is a totally irrelevant covariate. [sent-540, score-0.797]
</p><p>61 We can also investigate how these tree methods handle covariates that play different types of roles in the causal pathway between T and Y. [sent-546, score-0.572]
</p><p>62 0  Table 1: Simulation Results Based on the Test Sample Method: Relative frequencies (in percentages) of the ﬁnal tree sizes in 200 runs identiﬁed by the causal inference tree (CIT), interaction tree (IT), and propensity tree (PT). [sent-947, score-1.512]
</p><p>63 Model D is basically a propensity model, involving both the exposure predictor X1 and the confounder X2 only. [sent-952, score-0.532]
</p><p>64 The estimated treatment effect and propensity for each group were averaged over 100 runs. [sent-1010, score-0.786]
</p><p>65 It comes as no surprise that PT, concerning propensity only, gives a null tree for most of the time. [sent-1013, score-0.589]
</p><p>66 Based on true grouping, the causal effect and propensity for each group are computed and presented in Table 2. [sent-1044, score-0.945]
</p><p>67 Each ﬁnal CIT (based on BIC) is applied to the validation set to compute the individual causal effect ∆i and propensity ei for each observation in the validation set. [sent-1048, score-0.895]
</p><p>68 The grouped causal effect and propensity estimates are then averaged over 100 simulation runs. [sent-1050, score-0.895]
</p><p>69 Among others, Dehejia and Wahba (1999) obtained estimates of the treatment effect that are close to the experimental benchmark estimate or the ‘gold’ standard using propensity score matching and stratiﬁcation. [sent-1065, score-0.843]
</p><p>70 4565  Table 4: Summary statistics for the terminal nodes: (a) the ﬁnal propensity tree (PT); (b) the ﬁnal interaction tree (IT); and (c) the ﬁnal causal inference tree (CIT). [sent-1208, score-1.531]
</p><p>71 Underneath each terminal node is the number of exposed subjects versus the number of unexposed subjects within the terminal node. [sent-1217, score-0.524]
</p><p>72 PT also identiﬁes a group, terminal node II, with extremely low propensity (3. [sent-1221, score-0.679]
</p><p>73 Apparently remarkable differential treatment effects seem to exist across the three terminal nodes based on Table 4(b). [sent-1227, score-0.542]
</p><p>74 Since CIT accounts for both propensity and differential causal effects, it is valid to estimate the NSW effect via direct comparison of sample means within each terminal node. [sent-1253, score-1.084]
</p><p>75 8  propensity score  Figure 4: Plot of the Estimated Personal Causal Effects vs. [sent-1263, score-0.565]
</p><p>76 If it is agreed that terminal node II be excluded from consideration due to lack of comparison basis and the minor negative effect of NSW in terminal node III be ignored, then one may tentatively conclude the absence of qualitative interactions. [sent-1266, score-0.486]
</p><p>77 We would like to emphasize that the excluded Group II in CIT can be explained by the fact that people who were not black and had some income in 1974 seemed unlikely (with estimated propensity 3. [sent-1286, score-0.489]
</p><p>78 Finally, ensemble CITs were used to estimate the ICE and propensity score for each individual. [sent-1289, score-0.565]
</p><p>79 It can be seen that both propensity and individual causal effects are reasonably homogeneous within each stratum, even though the individuals were from different treatment groups. [sent-1298, score-1.196]
</p><p>80 Extension to Ordinal/Continuous Treatments The concept and properties of the facilitating score can be extended to scenarios where the treatment variable is nominal (Lechner, 1999) or ordinal (Imbens, 2000). [sent-1300, score-0.598]
</p><p>81 Let et (X) = Pr{T = t|X} be the generalized propensity score (GPS). [sent-1303, score-0.565]
</p><p>82 It shows that, if the joint distribution of (Y, T ) can be modeled through a vector-valued function h(X), then h(X) is a generalized weak facilitating score and direct estimates of causal effects can be obtained by conditioning on h(X) = h. [sent-1340, score-0.84]
</p><p>83 Concerning the causal effect in subpopulation Ωh = {ω : h(X(ω)) = h}, E(Yt −Yt ′ |h(X) = h) = E{Yt |T = t, h(X) = h} − E{Yt ′ |T = t ′ , h(X) = h} = E{Y |T = t, h(X) = h} − E{Y |T = t ′ , h(X) = h} is independent of X. [sent-1386, score-0.485]
</p><p>84 This is analogous to the assumption of uniquely parameterized propensity function in Imai and van Dyk (2004), where a parametric form is prescribed for et (X). [sent-1390, score-0.484]
</p><p>85 To estimate h(X), a multinomial or cumulative logit model can be used for propensity and the outcome can be modeled with 2985  S U , K ANG , FAN , L EVINE AND YAN  multiple linear regression. [sent-1391, score-0.493]
</p><p>86 Discussion Embedded in Rubin’s causal model, we have introduced a new concept, the facilitating score, to help tackle the heterogeneity in both propensity and causal effects. [sent-1394, score-1.531]
</p><p>87 The facilitating score is a ﬁner balancing score of Rosenbaum and Rubin (1983), plus additional conditions for dealing with differential causal effects. [sent-1395, score-0.941]
</p><p>88 Accordingly we have devised recursive partitioning methods to aid in causal inference at different levels. [sent-1397, score-0.529]
</p><p>89 As demonstrated in the NSW data example, causal inference in observational studies could be very complex, owing to the confounding and interacting effects complicated by covariates. [sent-1404, score-0.681]
</p><p>90 Insight into a greater degree of personalized treatment can be gained by studying the personal treatment effects with ensemble CITs. [sent-1408, score-0.591]
</p><p>91 Some steps are standard arguments in propensity score theories. [sent-1430, score-0.565]
</p><p>92 This justiﬁes the direct use of mean response comparison for causal inference in subpopulation Ωh . [sent-1448, score-0.506]
</p><p>93 Causal inference with general treatment regimes: generalizing the propensity score. [sent-1577, score-0.757]
</p><p>94 The role of the propensity score in estimating dose-response functions. [sent-1622, score-0.565]
</p><p>95 Identiﬁcation and estimation of causal effects o fmultiple treatments under the conditional independence assumption. [sent-1646, score-0.504]
</p><p>96 Stratiﬁcation and weighting via the propensity score in estimation of causal treatment effects: a comparative study. [sent-1676, score-1.192]
</p><p>97 Propensity score estimation with boosted regression for evaluating causal effects in observational studies. [sent-1684, score-0.625]
</p><p>98 The analysis of randomized and non-randomized AIDS treatment trials using a new approach to causal inference in longitudinal studies. [sent-1702, score-0.692]
</p><p>99 The central role of the propensity score in observational studies for causal effects. [sent-1741, score-1.0]
</p><p>100 Estimating causal effects of treatments in randomized and nonrandomized studies. [sent-1756, score-0.504]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('propensity', 0.458), ('causal', 0.393), ('cit', 0.34), ('facilitating', 0.257), ('treatment', 0.234), ('ice', 0.169), ('terminal', 0.15), ('strati', 0.144), ('nsw', 0.139), ('tree', 0.131), ('evine', 0.12), ('rees', 0.114), ('ace', 0.111), ('score', 0.107), ('aic', 0.107), ('bic', 0.088), ('ang', 0.086), ('effects', 0.083), ('nference', 0.081), ('ausal', 0.081), ('confounder', 0.074), ('rubin', 0.072), ('confounding', 0.072), ('interaction', 0.072), ('yan', 0.071), ('node', 0.071), ('grouping', 0.068), ('unexposed', 0.066), ('inference', 0.065), ('yt', 0.061), ('fan', 0.061), ('ignorability', 0.06), ('db', 0.053), ('kb', 0.051), ('group', 0.05), ('assignment', 0.05), ('strata', 0.048), ('subpopulation', 0.048), ('covariates', 0.048), ('pt', 0.047), ('effect', 0.044), ('statistic', 0.044), ('observational', 0.042), ('aggregated', 0.042), ('personalized', 0.04), ('differential', 0.039), ('balancing', 0.038), ('recursive', 0.037), ('ti', 0.036), ('earnings', 0.036), ('laan', 0.036), ('stratum', 0.036), ('collider', 0.036), ('bootstrap', 0.036), ('splitting', 0.036), ('nodes', 0.036), ('outcome', 0.035), ('partitioning', 0.034), ('lv', 0.034), ('subgroup', 0.034), ('confounders', 0.034), ('sd', 0.033), ('pruning', 0.033), ('subjects', 0.032), ('breiman', 0.031), ('income', 0.031), ('rosenbaum', 0.031), ('cutoff', 0.031), ('core', 0.031), ('covariate', 0.031), ('ancova', 0.03), ('dehejia', 0.03), ('sse', 0.03), ('heterogeneity', 0.03), ('medicine', 0.029), ('interactions', 0.028), ('treatments', 0.028), ('individuals', 0.028), ('robins', 0.028), ('subtree', 0.028), ('su', 0.026), ('ii', 0.026), ('imbens', 0.026), ('kang', 0.026), ('gordon', 0.026), ('interacting', 0.026), ('parametric', 0.026), ('misspeci', 0.025), ('ignorable', 0.024), ('imai', 0.024), ('lrt', 0.024), ('nonexperimental', 0.024), ('sliced', 0.024), ('subclassi', 0.024), ('adjustment', 0.024), ('iv', 0.023), ('epidemiology', 0.023), ('trees', 0.023), ('age', 0.023), ('exposed', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000023 <a title="42-tfidf-1" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>Author: Xiaogang Su, Joseph Kang, Juanjuan Fan, Richard A. Levine, Xin Yan</p><p>Abstract: Assessing treatment effects in observational studies is a multifaceted problem that not only involves heterogeneous mechanisms of how the treatment or cause is exposed to subjects, known as propensity, but also differential causal effects across sub-populations. We introduce a concept termed the facilitating score to account for both the confounding and interacting impacts of covariates on the treatment effect. Several approaches for estimating the facilitating score are discussed. In particular, we put forward a machine learning method, called causal inference tree (CIT), to provide a piecewise constant approximation of the facilitating score. With interpretable rules, CIT splits data in such a way that both the propensity and the treatment effect become more homogeneous within each resultant partition. Causal inference at different levels can be made on the basis of CIT. Together with an aggregated grouping procedure, CIT stratiﬁes data into strata where causal effects can be conveniently assessed within each. Besides, a feasible way of predicting individual causal effects (ICE) is made available by aggregating ensemble CIT models. Both the stratiﬁed results and the estimated ICE provide an assessment of heterogeneity of causal effects and can be integrated for estimating the average causal effect (ACE). Mean square consistency of CIT is also established. We evaluate the performance of proposed methods with simulations and illustrate their use with the NSW data in Dehejia and Wahba (1999) where the objective is to assess the impact of c 2012 Xiaogang Su, Joseph Kang, Juanjuan Fan, Richard A. Levine and Xin Yan. S U , K ANG , FAN , L EVINE AND YAN a labor training program, the National Supported Work (NSW) demonstration, on post-intervention earnings. Keywords: CART, causal inference, confounding, interaction, observational study, personalized medicine, recursive partitioning</p><p>2 0.2122111 <a title="42-tfidf-2" href="./jmlr-2012-Causal_Bounds_and_Observable_Constraints_for_Non-deterministic_Models.html">24 jmlr-2012-Causal Bounds and Observable Constraints for Non-deterministic Models</a></p>
<p>Author: Roland R. Ramsahai</p><p>Abstract: Conditional independence relations involving latent variables do not necessarily imply observable independences. They may imply inequality constraints on observable parameters and causal bounds, which can be used for falsiﬁcation and identiﬁcation. The literature on computing such constraints often involve a deterministic underlying data generating process in a counterfactual framework. If an analyst is ignorant of the nature of the underlying mechanisms then they may wish to use a model which allows the underlying mechanisms to be probabilistic. A method of computation for a weaker model without any determinism is given here and demonstrated for the instrumental variable model, though applicable to other models. The approach is based on the analysis of mappings with convex polytopes in a decision theoretic framework and can be implemented in readily available polyhedral computation software. Well known constraints and bounds are replicated in a probabilistic model and novel ones are computed for instrumental variable models without non-deterministic versions of the randomization, exclusion restriction and monotonicity assumptions respectively. Keywords: instrumental variables, instrumental inequality, causal bounds, convex polytope, latent variables, directed acyclic graph</p><p>3 0.13941629 <a title="42-tfidf-3" href="./jmlr-2012-Towards_Integrative_Causal_Analysis_of_Heterogeneous_Data_Sets_and_Studies.html">114 jmlr-2012-Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies</a></p>
<p>Author: Ioannis Tsamardinos, Sofia Triantafillou, Vincenzo Lagani</p><p>Abstract: We present methods able to predict the presence and strength of conditional and unconditional dependencies (correlations) between two variables Y and Z never jointly measured on the same samples, based on multiple data sets measuring a set of common variables. The algorithms are specializations of prior work on learning causal structures from overlapping variable sets. This problem has also been addressed in the ﬁeld of statistical matching. The proposed methods are applied to a wide range of domains and are shown to accurately predict the presence of thousands of dependencies. Compared against prototypical statistical matching algorithms and within the scope of our experiments, the proposed algorithms make predictions that are better correlated with the sample estimates of the unknown parameters on test data ; this is particularly the case when the number of commonly measured variables is low. The enabling idea behind the methods is to induce one or all causal models that are simultaneously consistent with (ﬁt) all available data sets and prior knowledge and reason with them. This allows constraints stemming from causal assumptions (e.g., Causal Markov Condition, Faithfulness) to propagate. Several methods have been developed based on this idea, for which we propose the unifying name Integrative Causal Analysis (INCA). A contrived example is presented demonstrating the theoretical potential to develop more general methods for co-analyzing heterogeneous data sets. The computational experiments with the novel methods provide evidence that causallyinspired assumptions such as Faithfulness often hold to a good degree of approximation in many real systems and could be exploited for statistical inference. Code, scripts, and data are available at www.mensxmachina.org. Keywords: integrative causal analysis, causal discovery, Bayesian networks, maximal ancestral graphs, structural equation models, causality, statistical matching, data fusion</p><p>4 0.092164718 <a title="42-tfidf-4" href="./jmlr-2012-Learning_Linear_Cyclic_Causal_Models_with_Latent_Variables.html">56 jmlr-2012-Learning Linear Cyclic Causal Models with Latent Variables</a></p>
<p>Author: Antti Hyttinen, Frederick Eberhardt, Patrik O. Hoyer</p><p>Abstract: Identifying cause-effect relationships between variables of interest is a central problem in science. Given a set of experiments we describe a procedure that identiﬁes linear models that may contain cycles and latent variables. We provide a detailed description of the model family, full proofs of the necessary and sufﬁcient conditions for identiﬁability, a search algorithm that is complete, and a discussion of what can be done when the identiﬁability conditions are not satisﬁed. The algorithm is comprehensively tested in simulations, comparing it to competing algorithms in the literature. Furthermore, we adapt the procedure to the problem of cellular network inference, applying it to the biologically realistic data of the DREAM challenges. The paper provides a full theoretical foundation for the causal discovery procedure ﬁrst presented by Eberhardt et al. (2010) and Hyttinen et al. (2010). Keywords: causality, graphical models, randomized experiments, structural equation models, latent variables, latent confounders, cycles</p><p>5 0.091178887 <a title="42-tfidf-5" href="./jmlr-2012-Characterization_and_Greedy_Learning_of_Interventional_Markov_Equivalence_Classes_of_Directed_Acyclic_Graphs.html">25 jmlr-2012-Characterization and Greedy Learning of Interventional Markov Equivalence Classes of Directed Acyclic Graphs</a></p>
<p>Author: Alain Hauser, Peter Bühlmann</p><p>Abstract: The investigation of directed acyclic graphs (DAGs) encoding the same Markov property, that is the same conditional independence relations of multivariate observational distributions, has a long tradition; many algorithms exist for model selection and structure learning in Markov equivalence classes. In this paper, we extend the notion of Markov equivalence of DAGs to the case of interventional distributions arising from multiple intervention experiments. We show that under reasonable assumptions on the intervention experiments, interventional Markov equivalence deﬁnes a ﬁner partitioning of DAGs than observational Markov equivalence and hence improves the identiﬁability of causal models. We give a graph theoretic criterion for two DAGs being Markov equivalent under interventions and show that each interventional Markov equivalence class can, analogously to the observational case, be uniquely represented by a chain graph called interventional essential graph (also known as CPDAG in the observational case). These are key insights for deriving a generalization of the Greedy Equivalence Search algorithm aimed at structure learning from interventional data. This new algorithm is evaluated in a simulation study. Keywords: causal inference, interventions, graphical model, Markov equivalence, greedy equivalence search</p><p>6 0.08903899 <a title="42-tfidf-6" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>7 0.064161249 <a title="42-tfidf-7" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>8 0.040734604 <a title="42-tfidf-8" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>9 0.035143014 <a title="42-tfidf-9" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>10 0.03513477 <a title="42-tfidf-10" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>11 0.034009531 <a title="42-tfidf-11" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>12 0.0336859 <a title="42-tfidf-12" href="./jmlr-2012-Linear_Fitted-Q_Iteration_with_Multiple_Reward_Functions.html">58 jmlr-2012-Linear Fitted-Q Iteration with Multiple Reward Functions</a></p>
<p>13 0.033263691 <a title="42-tfidf-13" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>14 0.033097763 <a title="42-tfidf-14" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>15 0.029894331 <a title="42-tfidf-15" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<p>16 0.029680476 <a title="42-tfidf-16" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>17 0.029647313 <a title="42-tfidf-17" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>18 0.025853995 <a title="42-tfidf-18" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>19 0.025542101 <a title="42-tfidf-19" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<p>20 0.025405115 <a title="42-tfidf-20" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.152), (1, 0.096), (2, 0.099), (3, -0.164), (4, 0.116), (5, 0.146), (6, -0.098), (7, 0.336), (8, 0.335), (9, 0.064), (10, -0.006), (11, 0.008), (12, 0.116), (13, 0.199), (14, 0.078), (15, -0.012), (16, -0.014), (17, 0.058), (18, -0.07), (19, -0.083), (20, -0.069), (21, -0.006), (22, 0.002), (23, -0.078), (24, -0.036), (25, 0.004), (26, -0.018), (27, 0.094), (28, -0.019), (29, 0.029), (30, 0.032), (31, 0.094), (32, 0.0), (33, 0.037), (34, -0.033), (35, -0.019), (36, 0.024), (37, 0.04), (38, 0.008), (39, -0.031), (40, -0.012), (41, -0.058), (42, 0.036), (43, -0.042), (44, 0.002), (45, -0.005), (46, 0.054), (47, -0.007), (48, -0.001), (49, -0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96294516 <a title="42-lsi-1" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>Author: Xiaogang Su, Joseph Kang, Juanjuan Fan, Richard A. Levine, Xin Yan</p><p>Abstract: Assessing treatment effects in observational studies is a multifaceted problem that not only involves heterogeneous mechanisms of how the treatment or cause is exposed to subjects, known as propensity, but also differential causal effects across sub-populations. We introduce a concept termed the facilitating score to account for both the confounding and interacting impacts of covariates on the treatment effect. Several approaches for estimating the facilitating score are discussed. In particular, we put forward a machine learning method, called causal inference tree (CIT), to provide a piecewise constant approximation of the facilitating score. With interpretable rules, CIT splits data in such a way that both the propensity and the treatment effect become more homogeneous within each resultant partition. Causal inference at different levels can be made on the basis of CIT. Together with an aggregated grouping procedure, CIT stratiﬁes data into strata where causal effects can be conveniently assessed within each. Besides, a feasible way of predicting individual causal effects (ICE) is made available by aggregating ensemble CIT models. Both the stratiﬁed results and the estimated ICE provide an assessment of heterogeneity of causal effects and can be integrated for estimating the average causal effect (ACE). Mean square consistency of CIT is also established. We evaluate the performance of proposed methods with simulations and illustrate their use with the NSW data in Dehejia and Wahba (1999) where the objective is to assess the impact of c 2012 Xiaogang Su, Joseph Kang, Juanjuan Fan, Richard A. Levine and Xin Yan. S U , K ANG , FAN , L EVINE AND YAN a labor training program, the National Supported Work (NSW) demonstration, on post-intervention earnings. Keywords: CART, causal inference, confounding, interaction, observational study, personalized medicine, recursive partitioning</p><p>2 0.88036358 <a title="42-lsi-2" href="./jmlr-2012-Causal_Bounds_and_Observable_Constraints_for_Non-deterministic_Models.html">24 jmlr-2012-Causal Bounds and Observable Constraints for Non-deterministic Models</a></p>
<p>Author: Roland R. Ramsahai</p><p>Abstract: Conditional independence relations involving latent variables do not necessarily imply observable independences. They may imply inequality constraints on observable parameters and causal bounds, which can be used for falsiﬁcation and identiﬁcation. The literature on computing such constraints often involve a deterministic underlying data generating process in a counterfactual framework. If an analyst is ignorant of the nature of the underlying mechanisms then they may wish to use a model which allows the underlying mechanisms to be probabilistic. A method of computation for a weaker model without any determinism is given here and demonstrated for the instrumental variable model, though applicable to other models. The approach is based on the analysis of mappings with convex polytopes in a decision theoretic framework and can be implemented in readily available polyhedral computation software. Well known constraints and bounds are replicated in a probabilistic model and novel ones are computed for instrumental variable models without non-deterministic versions of the randomization, exclusion restriction and monotonicity assumptions respectively. Keywords: instrumental variables, instrumental inequality, causal bounds, convex polytope, latent variables, directed acyclic graph</p><p>3 0.78574079 <a title="42-lsi-3" href="./jmlr-2012-Towards_Integrative_Causal_Analysis_of_Heterogeneous_Data_Sets_and_Studies.html">114 jmlr-2012-Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies</a></p>
<p>Author: Ioannis Tsamardinos, Sofia Triantafillou, Vincenzo Lagani</p><p>Abstract: We present methods able to predict the presence and strength of conditional and unconditional dependencies (correlations) between two variables Y and Z never jointly measured on the same samples, based on multiple data sets measuring a set of common variables. The algorithms are specializations of prior work on learning causal structures from overlapping variable sets. This problem has also been addressed in the ﬁeld of statistical matching. The proposed methods are applied to a wide range of domains and are shown to accurately predict the presence of thousands of dependencies. Compared against prototypical statistical matching algorithms and within the scope of our experiments, the proposed algorithms make predictions that are better correlated with the sample estimates of the unknown parameters on test data ; this is particularly the case when the number of commonly measured variables is low. The enabling idea behind the methods is to induce one or all causal models that are simultaneously consistent with (ﬁt) all available data sets and prior knowledge and reason with them. This allows constraints stemming from causal assumptions (e.g., Causal Markov Condition, Faithfulness) to propagate. Several methods have been developed based on this idea, for which we propose the unifying name Integrative Causal Analysis (INCA). A contrived example is presented demonstrating the theoretical potential to develop more general methods for co-analyzing heterogeneous data sets. The computational experiments with the novel methods provide evidence that causallyinspired assumptions such as Faithfulness often hold to a good degree of approximation in many real systems and could be exploited for statistical inference. Code, scripts, and data are available at www.mensxmachina.org. Keywords: integrative causal analysis, causal discovery, Bayesian networks, maximal ancestral graphs, structural equation models, causality, statistical matching, data fusion</p><p>4 0.52001858 <a title="42-lsi-4" href="./jmlr-2012-Characterization_and_Greedy_Learning_of_Interventional_Markov_Equivalence_Classes_of_Directed_Acyclic_Graphs.html">25 jmlr-2012-Characterization and Greedy Learning of Interventional Markov Equivalence Classes of Directed Acyclic Graphs</a></p>
<p>Author: Alain Hauser, Peter Bühlmann</p><p>Abstract: The investigation of directed acyclic graphs (DAGs) encoding the same Markov property, that is the same conditional independence relations of multivariate observational distributions, has a long tradition; many algorithms exist for model selection and structure learning in Markov equivalence classes. In this paper, we extend the notion of Markov equivalence of DAGs to the case of interventional distributions arising from multiple intervention experiments. We show that under reasonable assumptions on the intervention experiments, interventional Markov equivalence deﬁnes a ﬁner partitioning of DAGs than observational Markov equivalence and hence improves the identiﬁability of causal models. We give a graph theoretic criterion for two DAGs being Markov equivalent under interventions and show that each interventional Markov equivalence class can, analogously to the observational case, be uniquely represented by a chain graph called interventional essential graph (also known as CPDAG in the observational case). These are key insights for deriving a generalization of the Greedy Equivalence Search algorithm aimed at structure learning from interventional data. This new algorithm is evaluated in a simulation study. Keywords: causal inference, interventions, graphical model, Markov equivalence, greedy equivalence search</p><p>5 0.4428387 <a title="42-lsi-5" href="./jmlr-2012-Learning_Linear_Cyclic_Causal_Models_with_Latent_Variables.html">56 jmlr-2012-Learning Linear Cyclic Causal Models with Latent Variables</a></p>
<p>Author: Antti Hyttinen, Frederick Eberhardt, Patrik O. Hoyer</p><p>Abstract: Identifying cause-effect relationships between variables of interest is a central problem in science. Given a set of experiments we describe a procedure that identiﬁes linear models that may contain cycles and latent variables. We provide a detailed description of the model family, full proofs of the necessary and sufﬁcient conditions for identiﬁability, a search algorithm that is complete, and a discussion of what can be done when the identiﬁability conditions are not satisﬁed. The algorithm is comprehensively tested in simulations, comparing it to competing algorithms in the literature. Furthermore, we adapt the procedure to the problem of cellular network inference, applying it to the biologically realistic data of the DREAM challenges. The paper provides a full theoretical foundation for the causal discovery procedure ﬁrst presented by Eberhardt et al. (2010) and Hyttinen et al. (2010). Keywords: causality, graphical models, randomized experiments, structural equation models, latent variables, latent confounders, cycles</p><p>6 0.36942214 <a title="42-lsi-6" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>7 0.28781465 <a title="42-lsi-7" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>8 0.28436053 <a title="42-lsi-8" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>9 0.19477727 <a title="42-lsi-9" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>10 0.18152936 <a title="42-lsi-10" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>11 0.17975186 <a title="42-lsi-11" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<p>12 0.16533358 <a title="42-lsi-12" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>13 0.16366649 <a title="42-lsi-13" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>14 0.157464 <a title="42-lsi-14" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>15 0.1572293 <a title="42-lsi-15" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>16 0.14538978 <a title="42-lsi-16" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>17 0.14184602 <a title="42-lsi-17" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>18 0.14158633 <a title="42-lsi-18" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>19 0.14079951 <a title="42-lsi-19" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>20 0.14013456 <a title="42-lsi-20" href="./jmlr-2012-Stability_of_Density-Based_Clustering.html">109 jmlr-2012-Stability of Density-Based Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.017), (21, 0.076), (24, 0.377), (26, 0.028), (27, 0.011), (29, 0.056), (35, 0.014), (49, 0.016), (56, 0.02), (57, 0.025), (69, 0.029), (75, 0.045), (77, 0.025), (79, 0.02), (81, 0.011), (91, 0.012), (92, 0.077), (96, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73293728 <a title="42-lda-1" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>Author: Xiaogang Su, Joseph Kang, Juanjuan Fan, Richard A. Levine, Xin Yan</p><p>Abstract: Assessing treatment effects in observational studies is a multifaceted problem that not only involves heterogeneous mechanisms of how the treatment or cause is exposed to subjects, known as propensity, but also differential causal effects across sub-populations. We introduce a concept termed the facilitating score to account for both the confounding and interacting impacts of covariates on the treatment effect. Several approaches for estimating the facilitating score are discussed. In particular, we put forward a machine learning method, called causal inference tree (CIT), to provide a piecewise constant approximation of the facilitating score. With interpretable rules, CIT splits data in such a way that both the propensity and the treatment effect become more homogeneous within each resultant partition. Causal inference at different levels can be made on the basis of CIT. Together with an aggregated grouping procedure, CIT stratiﬁes data into strata where causal effects can be conveniently assessed within each. Besides, a feasible way of predicting individual causal effects (ICE) is made available by aggregating ensemble CIT models. Both the stratiﬁed results and the estimated ICE provide an assessment of heterogeneity of causal effects and can be integrated for estimating the average causal effect (ACE). Mean square consistency of CIT is also established. We evaluate the performance of proposed methods with simulations and illustrate their use with the NSW data in Dehejia and Wahba (1999) where the objective is to assess the impact of c 2012 Xiaogang Su, Joseph Kang, Juanjuan Fan, Richard A. Levine and Xin Yan. S U , K ANG , FAN , L EVINE AND YAN a labor training program, the National Supported Work (NSW) demonstration, on post-intervention earnings. Keywords: CART, causal inference, confounding, interaction, observational study, personalized medicine, recursive partitioning</p><p>2 0.34826222 <a title="42-lda-2" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>Author: Gérard Biau</p><p>Abstract: Random forests are a scheme proposed by Leo Breiman in the 2000’s for building a predictor ensemble with a set of decision trees that grow in randomly selected subspaces of data. Despite growing interest and practical use, there has been little exploration of the statistical properties of random forests, and little is known about the mathematical forces driving the algorithm. In this paper, we offer an in-depth analysis of a random forests model suggested by Breiman (2004), which is very close to the original algorithm. We show in particular that the procedure is consistent and adapts to sparsity, in the sense that its rate of convergence depends only on the number of strong features and not on how many noise variables are present. Keywords: random forests, randomization, sparsity, dimension reduction, consistency, rate of convergence</p><p>3 0.34035844 <a title="42-lda-3" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>Author: Lan Xue, Annie Qu</p><p>Abstract: The varying-coefﬁcient model is ﬂexible and powerful for modeling the dynamic changes of regression coefﬁcients. It is important to identify signiﬁcant covariates associated with response variables, especially for high-dimensional settings where the number of covariates can be larger than the sample size. We consider model selection in the high-dimensional setting and adopt difference convex programming to approximate the L0 penalty, and we investigate the global optimality properties of the varying-coefﬁcient estimator. The challenge of the variable selection problem here is that the dimension of the nonparametric form for the varying-coefﬁcient modeling could be inﬁnite, in addition to dealing with the high-dimensional linear covariates. We show that the proposed varying-coefﬁcient estimator is consistent, enjoys the oracle property and achieves an optimal convergence rate for the non-zero nonparametric components for high-dimensional data. Our simulations and numerical examples indicate that the difference convex algorithm is efﬁcient using the coordinate decent algorithm, and is able to select the true model at a higher frequency than the least absolute shrinkage and selection operator (LASSO), the adaptive LASSO and the smoothly clipped absolute deviation (SCAD) approaches. Keywords: coordinate decent algorithm, difference convex programming, L0 - regularization, large-p small-n, model selection, nonparametric function, oracle property, truncated L1 penalty</p><p>4 0.33752775 <a title="42-lda-4" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>Author: Matus Telgarsky</p><p>Abstract: Boosting combines weak learners into a predictor with low empirical risk. Its dual constructs a high entropy distribution upon which weak learners and training labels are uncorrelated. This manuscript studies this primal-dual relationship under a broad family of losses, including the exponential loss of AdaBoost and the logistic loss, revealing: • Weak learnability aids the whole loss family: for any ε > 0, O (ln(1/ε)) iterations sufﬁce to produce a predictor with empirical risk ε-close to the inﬁmum; • The circumstances granting the existence of an empirical risk minimizer may be characterized in terms of the primal and dual problems, yielding a new proof of the known rate O (ln(1/ε)); • Arbitrary instances may be decomposed into the above two, granting rate O (1/ε), with a matching lower bound provided for the logistic loss. Keywords: boosting, convex analysis, weak learnability, coordinate descent, maximum entropy</p><p>5 0.33255872 <a title="42-lda-5" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>Author: Jian Huang, Cun-Hui Zhang</p><p>Abstract: The ℓ1 -penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ1 -penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ1 -penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results. Keywords: variable selection, penalized estimation, oracle inequality, generalized linear models, selection consistency, sparsity</p><p>6 0.33152726 <a title="42-lda-6" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>7 0.33112895 <a title="42-lda-7" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>8 0.32996914 <a title="42-lda-8" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>9 0.32836175 <a title="42-lda-9" href="./jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</a></p>
<p>10 0.32324269 <a title="42-lda-10" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>11 0.32317591 <a title="42-lda-11" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>12 0.3229191 <a title="42-lda-12" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>13 0.32278332 <a title="42-lda-13" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>14 0.32212549 <a title="42-lda-14" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>15 0.32212108 <a title="42-lda-15" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>16 0.32068521 <a title="42-lda-16" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>17 0.320501 <a title="42-lda-17" href="./jmlr-2012-Refinement_of_Operator-valued_Reproducing_Kernels.html">96 jmlr-2012-Refinement of Operator-valued Reproducing Kernels</a></p>
<p>18 0.32044998 <a title="42-lda-18" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>19 0.31969553 <a title="42-lda-19" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>20 0.31956458 <a title="42-lda-20" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
