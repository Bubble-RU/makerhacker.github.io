<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-22" href="#">jmlr2012-22</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</h1>
<br/><p>Source: <a title="jmlr-2012-22-pdf" href="http://jmlr.org/papers/volume13/huang12a/huang12a.pdf">pdf</a></p><p>Author: Gary B. Huang, Andrew Kae, Carl Doersch, Erik Learned-Miller</p><p>Abstract: We consider a model for which it is important, early in processing, to estimate some variables with high precision, but perhaps at relatively low recall. If some variables can be identiﬁed with near certainty, they can be conditioned upon, allowing further inference to be done efﬁciently. Speciﬁcally, we consider optical character recognition (OCR) systems that can be bootstrapped by identifying a subset of correctly translated document words with very high precision. This “clean set” is subsequently used as document-speciﬁc training data. While OCR systems produce conﬁdence measures for the identity of each letter or word, thresholding these values still produces a signiﬁcant number of errors. We introduce a novel technique for identifying a set of correct words with very high precision. Rather than estimating posterior probabilities, we bound the probability that any given word is incorrect using an approximate worst case analysis. We give empirical results on a data set of difﬁcult historical newspaper scans, demonstrating that our method for identifying correct words makes only two errors in 56 documents. Using document-speciﬁc character models generated from this data, we are able to reduce the error over properly segmented characters by 34.1% from an initial OCR system’s translation.1 Keywords: optical character recognition, probability bounding, document-speciﬁc modeling, computer vision 1. This work is an expanded and revised version of Kae et al. (2010). Supported by NSF Grant IIS-0916555. c 2012 Gary B. Huang, Andrew Kae, Carl Doersch and Erik Learned-Miller. H UANG , K AE , D OERSCH AND L EARNED -M ILLER</p><p>Reference: <a title="jmlr-2012-22-reference" href="../jmlr2012_reference/jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Speciﬁcally, we consider optical character recognition (OCR) systems that can be bootstrapped by identifying a subset of correctly translated document words with very high precision. [sent-12, score-0.64]
</p><p>2 Using document-speciﬁc character models generated from this data, we are able to reduce the error over properly segmented characters by 34. [sent-18, score-0.578]
</p><p>3 2 Document-Speciﬁc OCR In this paper, we focus on the problem of improving optical character recognition (OCR) performance on difﬁcult test cases. [sent-63, score-0.461]
</p><p>4 If one had some method for ﬁnding a sample of words in a document that were known to be correct with high conﬁdence, one could effectively use the characters in such words as training data with which to build document-speciﬁc models of the fonts in a document. [sent-108, score-0.524]
</p><p>5 As we mention later, we only make 2 errors in 4465 words in our clean word lists, even when they come from quite challenging documents. [sent-122, score-0.671]
</p><p>6 Once again, our goal is to produce large lists of clean words from OCR output and demonstrate how they can be used for document-speciﬁc modeling. [sent-124, score-0.507]
</p><p>7 After presenting our method for producing clean word lists, we provide a formal analysis of the bounds on the probability of incorrectly including a word in our clean word list, under certain assumptions. [sent-125, score-1.301]
</p><p>8 Contextdependent word correction methods include using statistical language models such as word n-gram probabilities to correct errors using neighboring words. [sent-141, score-0.511]
</p><p>9 They assume a generative process that produces words, characters, and word boundaries, in order to model segmentation and character recognition errors of an OCR system. [sent-143, score-0.696]
</p><p>10 Speciﬁcally, they cluster whole word images and use majority voting of the associated OCR labels to decide on the correct output and create character image prototypes. [sent-147, score-0.679]
</p><p>11 , a word is a preﬁx of another word) and decompositions of unknown words into known word patterns using the document images. [sent-150, score-0.578]
</p><p>12 Our work is also related to a variety of approaches that leverage inter-character similarity in documents in order to reduce the dependence upon a priori character models. [sent-152, score-0.443]
</p><p>13 This particular problem can be overcome by solving the decoding problem iteratively, using word and character statistics to ﬁrst decode least ambiguous characters, then to iteratively decode progressively more difﬁcult characters (Kae and Learned-Miller, 2009). [sent-155, score-0.726]
</p><p>14 An alternative approach to obtaining document-speciﬁc character models is presented by Edwards and Forsyth (2005), using an iterative algorithm to extract character templates from high conﬁdence regions. [sent-156, score-0.702]
</p><p>15 Hobby and Ho (1997) perform clustering in order to replace individual, potentially degraded character images, with a smoothed image over the cluster. [sent-160, score-0.445]
</p><p>16 Method for Producing Clean Word Lists In this section, we present our method for examining a document bitmap and the output of an OCR system for that document to produce a so-called clean word list, that is, a list of words which we believe to be correct, with high conﬁdence. [sent-203, score-0.947]
</p><p>17 Our success will be measured by the number of words that can be produced, and whether we achieve a very low error rate in the clean list. [sent-204, score-0.432]
</p><p>18 Ideally, we must produce a clean word list which is large enough to provide sufﬁcient training data for documentspeciﬁc modeling. [sent-205, score-0.641]
</p><p>19 • In addition to a segmentation of words and letters, the system should produce a best guess for every character it has segmented, and hence, by extension, of every word (or string) it has 370  B OUNDING THE P ROBABILITY OF E RROR FOR H IGH P RECISION OCR  segmented. [sent-211, score-0.714]
</p><p>20 • Using the segmentations provided by the OCR system, we assume we can extract the grayvalued bitmaps representing each guessed character from the original document image. [sent-213, score-0.484]
</p><p>21 The Hamming distance between two strings of the same number of characters is the number of character substitutions necessary to convert one string to the other. [sent-217, score-0.576]
</p><p>22 Later, after deﬁning certain equivalence relationships among highly confusable characters such as ’o’ and ’c’, we deﬁne a pseudo-Hamming distance which is equivalent to the Hamming distance except that it ignores substitutions among characters in the same equivalence class. [sent-219, score-0.457]
</p><p>23 Our method for identifying words in the clean list has three basic steps. [sent-221, score-0.49]
</p><p>24 That is, we do not put it on the clean word list. [sent-225, score-0.539]
</p><p>25 Given that T is a lexicon word, we evaluate whether H1 (T ) is non-empty, that is, whether there are any lexicon words for which a single change of a letter can produce T . [sent-227, score-0.514]
</p><p>26 Assuming we have passed the ﬁrst two tests, we now perform a consistency check (described below) of each character in the word. [sent-230, score-0.444]
</p><p>27 If the consistency check is passed, we declare the word to be correctly recognized and include it in the clean list. [sent-231, score-0.632]
</p><p>28 However, our method will not fail in this way because if the true string were in fact “Rumpledpigskin”, the character consistency check would never pass. [sent-237, score-0.444]
</p><p>29 It is for this reason that our method is highly non-trivial, and represents a signiﬁcant advance in the creation of highly accurate clean word lists. [sent-238, score-0.539]
</p><p>30 For such results, it is much more likely that a word labeled as a lexicon word with an empty Hamming ball of some radius is, in fact, correctly labeled. [sent-240, score-0.592]
</p><p>31 In this work, simply forcing all predicted words to be lexicon words led to a 3 percentage point increase in word accuracy, and incorporating factors with lexicon information into the probability model led to an additional 5 percentage point increase in word accuracy. [sent-245, score-0.95]
</p><p>32 Let W j be the true character class of the jth glyph of a word W , and let T j be the initial OCR system’s interpretation of the same glyph. [sent-249, score-0.67]
</p><p>33 Imagine that a document contains a stray mark that does not look like any character at all, but was interpreted by the initial OCR system as a character. [sent-253, score-0.582]
</p><p>34 If the OCR system thought that the stray mark was a character, it would have to assign it to a character class like ’t’. [sent-254, score-0.436]
</p><p>35 Our scheme for doing this is to ﬁnd other characters that are similar to this glyph, and to check the identity assigned to those characters by the initial OCR system. [sent-256, score-0.436]
</p><p>36 If a large majority of those characters are given the same interpretation by the OCR system, then we consider the original character to be reliable. [sent-257, score-0.527]
</p><p>37 Since it is unlikely that the characters closest to the stray mark are clustered tightly around the true character ’t’, we hope to detect that the stray mark is atypical, and hence unreliable. [sent-258, score-0.619]
</p><p>38 If a fraction θ of the M glyphs most similar to g have the character label c, then we say that the glyph g is θ-dominated by c. [sent-260, score-0.508]
</p><p>39 Furthermore, a word is included in the clean list only if all of the characters in the word are reliable. [sent-271, score-0.97]
</p><p>40 Theoretical Bound For a word in a document, let W be the ground truth label of the word and T be the initial OCR system’s labeling of the word. [sent-278, score-0.486]
</p><p>41 Consider the problem of trying to estimate the probability that the labeling was correct, P(W = wt |T = wt ). [sent-279, score-0.499]
</p><p>42 We want to upper bound the probability Pr(W = wt |T = wt ,C = 1) when wt is a lexicon word and has an empty Hamming ball of size 1. [sent-288, score-1.123]
</p><p>43 The ﬁrst term considers all words in the lexicon with the same length as wt , and accounts for the most likely type of error. [sent-291, score-0.489]
</p><p>44 We will need an upper bound on the probability of the consistency check passing for a speciﬁc character when the label is incorrect (2ε), and a lower bound on the probability of the consistency check 373  H UANG , K AE , D OERSCH AND L EARNED -M ILLER  passing when the label is correct (δ). [sent-298, score-0.73]
</p><p>45 Next, we will need to relate these bounds on the character consistency check, Pr(C = 1|T = wt ,W ), to the terms in the geometric series, Pr(W = w|T = wt ,C = 1), which we do in Section 4. [sent-300, score-0.846]
</p><p>46 1 Bounding the Character Consistency Check We will rewrite the Pr(W = w|T = wt ,C = 1) terms as bounds involving Pr(C = 1|T = wt ,W = w) using Bayes’ rule. [sent-303, score-0.45]
</p><p>47 We will make the assumption that the individual character consistency checks are independent, although this is not exactly true, since there may be local noise that degrades characters in a word in the same way. [sent-304, score-0.771]
</p><p>48 Assume that each character is formed on the page by taking a single true, latent appearance based on the font and the particular character class and adding some amount of noise. [sent-305, score-0.771]
</p><p>49 More formally, letting pc (a) be the probability of a character appearance a for a given class c under the noise model, ε satisﬁes, for all character classes c1 , c2 , c1 = c2 , ε>  a|pc1 (a) < 1,  or, equivalently, Pr(W = w|T = wt ) Pr(W = wt |T = wt )  < 1. [sent-307, score-1.431]
</p><p>50 ) Thus, we are assuming that a test document does not differ from the training documents used to train the initial OCR system so much as to change the most probable word conditioned on the feature T = wt , as suggested by Equation 4. [sent-311, score-0.749]
</p><p>51 4 Note that wt has an empty Hamming ball of size 1, so w differs from wt by at least two letters. [sent-312, score-0.45]
</p><p>52 As we discuss later, the ﬁrst case is also problematic for the character consistency check as well, and so falls outside the scope of documents for which our method will be applicable. [sent-314, score-0.536]
</p><p>53 Applying this to Equation 3, we get Pr(W = w|T = wt ,C = 1) ≤  Pr(C = 1|T = wt ,W = w) . [sent-316, score-0.45]
</p><p>54 1 B OUNDING  THE  P ROBABILITY OF L EXICON H AMMING W ORDS  Consider a lexicon word w that is a pseudo-Hamming distance i from wt . [sent-321, score-0.644]
</p><p>55 We can then simplify Equation 5 to (2ε)i δi  Pr(W = w|T = wt ,C = 1) ≤  by making use of the assumption that the character consistency checks are independent, and that w and wt only differ in i characters. [sent-322, score-0.846]
</p><p>56 Now let Di be the number of lexicon words of pseudo-Hamming distance i away from wt . [sent-324, score-0.515]
</p><p>57 ) To get the total contribution to the error from all lexicon Hamming words, we sum over Di for all i > 1,  ∑  Pr(W = w|T = wt ,C = 1) ≤  ∑ Di  i=2  w=wt ,w∈Lex,|w|=|wt |  = D2  (2ε)i δi  (2ε)2 ε (2ε)2 + D2 2 ∑ (2rD )i 2 δ δ i=1 δ  ≤ 8D2  ε2 . [sent-329, score-0.441]
</p><p>58 Recall that δ, as deﬁned earlier, is a lower bound on the probability that a character consistency check will succeed if the OCR system’s label of the character is correct. [sent-342, score-0.877]
</p><p>59 376  B OUNDING THE P ROBABILITY OF E RROR FOR H IGH P RECISION OCR  Consider a lexicon word w with pseudo-edit distance i from wt , and involving at least one insertion or deletion (so |w| = |wt |). [sent-343, score-0.67]
</p><p>60 Similar to the lexicon Hamming words, we can simplify Equation 5 for w as (2ε)i+1 , δi  Pr(W = w|T = wt ,C = 1) ≤ since each substitution contributes a (2ε)2  2ε δ  and each insertion or deletion, of which there is at least one,  contributes a δ . [sent-344, score-0.47]
</p><p>61 Let Ei be the number of lexicon words w with a pseudo-edit distance i away from wt and |w| = |wt |. [sent-345, score-0.515]
</p><p>62 Summing the total 2 δ contribution to the error from lexicon edit words,  ∑  Pr(W = w|T = wt ,C = 1) ≤  ∑ Ei  i=1  w=wt ,w∈Lex,|w|=|wt |  = E1  (2ε)i+1 δi  (2ε)2 ε (2ε)2 + E1 ∑ (2rE δ )i δ δ i=1  ε2 δ ε2 ≤ 8E1 2 . [sent-347, score-0.49]
</p><p>63 5 Final Bound Combining each of the individual bounds derived above, we have Pr(W = wt |T = wt ,C = 1) ≤  (8D2 + 8E1 )ε2 + 4p1 ε . [sent-353, score-0.45]
</p><p>64 For ε < 10−3 , 8D2 + 8E1 < 102 , 4p1 < 10−1 , δ2 > 10−1 , Pr(W = wt |T = wt ,C = 1) ≤ 2 · 10−3 . [sent-355, score-0.45]
</p><p>65 In a sense, these inequalities deﬁne the set of documents for which our algorithm is expected to work, and for heavily degraded documents that fall outside this set, the character consistency checks may no longer be robust enough to guarantee a very low probability of error. [sent-358, score-0.636]
</p><p>66 The bound on p1 , the non-lexicon word probabilities, is not a tight upper bound, as non-lexicon words mislabeled as lexicon words are rare. [sent-365, score-0.564]
</p><p>67 (The character consistency check requires a character to match to at least a certain number of other similarly labeled characters, so, for example, if that number isn’t present in the document to begin with, then the check will fail with certainty. [sent-374, score-0.974]
</p><p>68 In the future, we believe these clean word lists can be incorporated into more sophisticated document-speciﬁc OCR models to obtain further improvements in recognition accuracy, as we discuss in future work. [sent-381, score-0.665]
</p><p>69 Compute the SIFT descriptor for each character image in the clean list, at the center of the image. [sent-389, score-0.792]
</p><p>70 Compute the component-wise arithmetic mean of all SIFT descriptors for each character class in the clean list. [sent-391, score-0.712]
</p><p>71 For each character image in the clean list, compute a SIFT descriptor for each point in a window in the center of the image (we use a 5x5 window) and select the descriptor with smallest L2 distance to the mean SIFT descriptor for this character class. [sent-394, score-1.309]
</p><p>72 We start by collecting all character images that are not in the clean list (since we do not want to test on images we trained on). [sent-398, score-0.848]
</p><p>73 If Tesseract gives a label to an image that is not a label for any of the clean set characters, then we do not include this character in our test set. [sent-400, score-0.832]
</p><p>74 Since our method will only assign to a character a label that appears in the clean set, then if the character was originally correct, we will deﬁnitely introduce an error by attempting to correct it. [sent-402, score-1.12]
</p><p>75 Furthermore, we have no direct information about the appearance of characters whose labels are not in the clean set, so it is relatively difﬁcult to assess if the original label is unreasonable. [sent-403, score-0.573]
</p><p>76 For these reasons, we only attempt to correct characters whose Tesseract label appears as one of the clean set labels. [sent-404, score-0.572]
</p><p>77 This makes sense since we generally do not have many instances of each character class in the clean list, and so we want a minimum of slack, which a high C value enforces. [sent-413, score-0.691]
</p><p>78 In particular, we show that our clean sets have far fewer errors and result in document-speciﬁc models that can correct a much larger number of errors in the original OCR output. [sent-429, score-0.493]
</p><p>79 10 This initial set of documents was used to evaluate our clean list generation algorithm and develop our algorithm for producing character models from the clean lists (Kae et al. [sent-433, score-1.289]
</p><p>80 In this work, our clean lists selected an average of 6% of the words from each document. [sent-435, score-0.484]
</p><p>81 These clean lists did not contain a single error, that is, the precision of our clean lists was 100%. [sent-436, score-0.828]
</p><p>82 380  B OUNDING THE P ROBABILITY OF E RROR FOR H IGH P RECISION OCR  Figure 2: Character error reduction rates for SIFT Align using the clean list (SIFT Align Clean) and Tesseract’s conﬁdent word list (SIFT Align Tess) on the test sets of 56 documents. [sent-446, score-0.698]
</p><p>83 The clean list contains 2 errors out of a total of 4465 words, within the theoretical bound of 0. [sent-454, score-0.489]
</p><p>84 In an effort to increase the size of the clean lists beyond 6% per document, we experimented with relaxing some of the criteria used to select the clean lists. [sent-456, score-0.754]
</p><p>85 By making this small change, we were able to increase the size of the clean lists to an average of 18% per document while introducing at most one error per document. [sent-458, score-0.546]
</p><p>86 We refer to the original clean lists as conservative clean lists and to the modiﬁed, larger, 11. [sent-459, score-0.828]
</p><p>87 )  and slightly less accurate clean lists as aggressive clean lists. [sent-468, score-0.776]
</p><p>88 We decided to use the aggressive clean lists for our experiments because they contain few errors and there are more character instances. [sent-469, score-0.849]
</p><p>89 Next, we used Mechanical Turk14 to label all character bounding boxes to produce a ground truth labeling. [sent-472, score-0.465]
</p><p>90 We instructed annotators to only label bounding boxes for which a single character is clearly visible. [sent-473, score-0.442]
</p><p>91 After the initial OCR system was used to make an initial pass at each document, the clean list for that document was extracted. [sent-475, score-0.642]
</p><p>92 This leaves room for some experimentation to ﬁnd the optimal balance between probability of error and clean set size, while still maintaining an empirical error close to the predicted bound of 0. [sent-484, score-0.439]
</p><p>93 3 Comparison to Another Conﬁdence Measure In order to judge the effectiveness of using our clean list, we also generated another conﬁdent word list using Tesseract’s own measure of conﬁdence. [sent-495, score-0.595]
</p><p>94 15 To generate the conﬁdent word list, we sort Tesseract’s recognized words by their measure of conﬁdence and take the top n words that result in the same number of characters as our clean list. [sent-496, score-0.855]
</p><p>95 In Figure 1, we show a portion of a document and the corresponding subset of clean list words (generated by our process) and highly conﬁdent Tesseract words. [sent-497, score-0.576]
</p><p>96 All of the words in our clean list were, in fact, correctly labeled by the initial Tesseract pass. [sent-498, score-0.502]
</p><p>97 SIFT Align Clean also reduces the character error in more documents than does Sift Align Tess. [sent-515, score-0.465]
</p><p>98 By using this framework for speech recognition, we can potentially obtain the equivalent of clean lists: portions of speech for which we are very conﬁdent the initial labeling was correct. [sent-533, score-0.497]
</p><p>99 By combining this document-speciﬁc training data with simple appearance-based character recognition techniques, we are able to achieve signiﬁcant reductions in average character error. [sent-572, score-0.777]
</p><p>100 In addition, while our work has taken the character segmentations produced by the initial OCR system as ﬁxed, we believe that the clean lists can also be used to re-segment and ﬁx the large percentage of initial errors that result from incorrect character segmentation. [sent-575, score-1.312]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ocr', 0.519), ('character', 0.351), ('clean', 0.34), ('wt', 0.225), ('word', 0.199), ('lexicon', 0.194), ('tesseract', 0.19), ('sift', 0.176), ('characters', 0.176), ('align', 0.168), ('document', 0.11), ('documents', 0.092), ('iller', 0.091), ('oersch', 0.091), ('pr', 0.09), ('glyph', 0.084), ('recision', 0.084), ('robability', 0.078), ('earned', 0.078), ('lists', 0.074), ('rror', 0.072), ('ae', 0.07), ('ounding', 0.07), ('words', 0.07), ('hamming', 0.065), ('errors', 0.062), ('image', 0.062), ('kae', 0.061), ('uang', 0.061), ('query', 0.059), ('lex', 0.059), ('list', 0.056), ('igh', 0.056), ('recognition', 0.052), ('edit', 0.049), ('speech', 0.048), ('check', 0.048), ('dent', 0.046), ('fonts', 0.046), ('glyphs', 0.046), ('stray', 0.046), ('consistency', 0.045), ('system', 0.039), ('font', 0.039), ('descriptor', 0.039), ('captioning', 0.038), ('images', 0.038), ('bounding', 0.038), ('dence', 0.037), ('initial', 0.036), ('letter', 0.033), ('optical', 0.033), ('degraded', 0.032), ('segmentation', 0.032), ('bound', 0.031), ('ri', 0.031), ('chum', 0.03), ('confusable', 0.03), ('appearance', 0.03), ('hong', 0.03), ('segmented', 0.029), ('correct', 0.029), ('con', 0.029), ('label', 0.027), ('insertion', 0.026), ('distance', 0.026), ('ni', 0.026), ('boxes', 0.026), ('substitution', 0.025), ('amherst', 0.025), ('test', 0.025), ('pass', 0.025), ('spatial', 0.025), ('labeling', 0.025), ('identifying', 0.024), ('probability', 0.024), ('substitutions', 0.023), ('training', 0.023), ('acoustic', 0.023), ('captions', 0.023), ('documentspeci', 0.023), ('doersch', 0.023), ('joins', 0.023), ('nagy', 0.023), ('recognizer', 0.023), ('rumpledpigskin', 0.023), ('segmentations', 0.023), ('tess', 0.023), ('produce', 0.023), ('reject', 0.023), ('veri', 0.022), ('error', 0.022), ('labelings', 0.022), ('probabilities', 0.022), ('aggressive', 0.022), ('match', 0.021), ('text', 0.021), ('ho', 0.021), ('descriptors', 0.021), ('exponents', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="22-tfidf-1" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>Author: Gary B. Huang, Andrew Kae, Carl Doersch, Erik Learned-Miller</p><p>Abstract: We consider a model for which it is important, early in processing, to estimate some variables with high precision, but perhaps at relatively low recall. If some variables can be identiﬁed with near certainty, they can be conditioned upon, allowing further inference to be done efﬁciently. Speciﬁcally, we consider optical character recognition (OCR) systems that can be bootstrapped by identifying a subset of correctly translated document words with very high precision. This “clean set” is subsequently used as document-speciﬁc training data. While OCR systems produce conﬁdence measures for the identity of each letter or word, thresholding these values still produces a signiﬁcant number of errors. We introduce a novel technique for identifying a set of correct words with very high precision. Rather than estimating posterior probabilities, we bound the probability that any given word is incorrect using an approximate worst case analysis. We give empirical results on a data set of difﬁcult historical newspaper scans, demonstrating that our method for identifying correct words makes only two errors in 56 documents. Using document-speciﬁc character models generated from this data, we are able to reduce the error over properly segmented characters by 34.1% from an initial OCR system’s translation.1 Keywords: optical character recognition, probability bounding, document-speciﬁc modeling, computer vision 1. This work is an expanded and revised version of Kae et al. (2010). Supported by NSF Grant IIS-0916555. c 2012 Gary B. Huang, Andrew Kae, Carl Doersch and Erik Learned-Miller. H UANG , K AE , D OERSCH AND L EARNED -M ILLER</p><p>2 0.1254724 <a title="22-tfidf-2" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>Author: Trinh Minh Tri Do, Thierry Artières</p><p>Abstract: Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efﬁcient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efﬁcient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random ﬁelds, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy ε with a rate O(1/λε) where λ is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difﬁcult and requires a stronger and more disputable assumption. Yet we provide experimental results on artiﬁcial test problems, and on ﬁve standard and difﬁcult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms. Keywords: optimization, non-convex, non-smooth, cutting plane, bundle method, regularized risk</p><p>3 0.065564588 <a title="22-tfidf-3" href="./jmlr-2012-Breaking_the_Curse_of_Kernelization%3A_Budgeted_Stochastic_Gradient_Descent_for_Large-Scale_SVM_Training.html">23 jmlr-2012-Breaking the Curse of Kernelization: Budgeted Stochastic Gradient Descent for Large-Scale SVM Training</a></p>
<p>Author: Zhuang Wang, Koby Crammer, Slobodan Vucetic</p><p>Abstract: Online algorithms that process one example at a time are advantageous when dealing with very large data or with data streams. Stochastic Gradient Descent (SGD) is such an algorithm and it is an attractive choice for online Support Vector Machine (SVM) training due to its simplicity and effectiveness. When equipped with kernel functions, similarly to other SVM learning algorithms, SGD is susceptible to the curse of kernelization that causes unbounded linear growth in model size and update time with data size. This may render SGD inapplicable to large data sets. We address this issue by presenting a class of Budgeted SGD (BSGD) algorithms for large-scale kernel SVM training which have constant space and constant time complexity per update. Speciﬁcally, BSGD keeps the number of support vectors bounded during training through several budget maintenance strategies. We treat the budget maintenance as a source of the gradient error, and show that the gap between the BSGD and the optimal SVM solutions depends on the model degradation due to budget maintenance. To minimize the gap, we study greedy budget maintenance methods based on removal, projection, and merging of support vectors. We propose budgeted versions of several popular online SVM algorithms that belong to the SGD family. We further derive BSGD algorithms for multi-class SVM training. Comprehensive empirical results show that BSGD achieves higher accuracy than the state-of-the-art budgeted online algorithms and comparable to non-budget algorithms, while achieving impressive computational efﬁciency both in time and space during training and prediction. Keywords: SVM, large-scale learning, online learning, stochastic gradient descent, kernel methods</p><p>4 0.064188249 <a title="22-tfidf-4" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>Author: Ofer Dekel, Claudio Gentile, Karthik Sridharan</p><p>Abstract: We present a new online learning algorithm in the selective sampling framework, where labels must be actively queried before they are revealed. We prove bounds on the regret of our algorithm and on the number of labels it queries when faced with an adaptive adversarial strategy of generating the instances. Our bounds both generalize and strictly improve over previous bounds in similar settings. Additionally, our selective sampling algorithm can be converted into an efﬁcient statistical active learning algorithm. We extend our algorithm and analysis to the multiple-teacher setting, where the algorithm can choose which subset of teachers to query for each label. Finally, we demonstrate the effectiveness of our techniques on a real-world Internet search problem. Keywords: online learning, regret, label-efﬁcient, crowdsourcing</p><p>5 0.063874379 <a title="22-tfidf-5" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>Author: Francesco Orabona, Luo Jie, Barbara Caputo</p><p>Abstract: In recent years there has been a lot of interest in designing principled classiﬁcation algorithms over multiple cues, based on the intuitive notion that using more features should lead to better performance. In the domain of kernel methods, a principled way to use multiple features is the Multi Kernel Learning (MKL) approach. Here we present a MKL optimization algorithm based on stochastic gradient descent that has a guaranteed convergence rate. We directly solve the MKL problem in the primal formulation. By having a p-norm formulation of MKL, we introduce a parameter that controls the level of sparsity of the solution, while leading to an easier optimization problem. We prove theoretically and experimentally that 1) our algorithm has a faster convergence rate as the number of kernels grows; 2) the training complexity is linear in the number of training examples; 3) very few iterations are sufﬁcient to reach good solutions. Experiments on standard benchmark databases support our claims. Keywords: multiple kernel learning, learning kernels, online optimization, stochastic subgradient descent, convergence bounds, large scale</p><p>6 0.060919233 <a title="22-tfidf-6" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>7 0.049346864 <a title="22-tfidf-7" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>8 0.045538414 <a title="22-tfidf-8" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>9 0.041225135 <a title="22-tfidf-9" href="./jmlr-2012-Algorithms_for_Learning_Kernels_Based_on_Centered_Alignment.html">16 jmlr-2012-Algorithms for Learning Kernels Based on Centered Alignment</a></p>
<p>10 0.040931638 <a title="22-tfidf-10" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>11 0.039430793 <a title="22-tfidf-11" href="./jmlr-2012-Discriminative_Hierarchical_Part-based_Models_for_Human_Parsing_and_Action_Recognition.html">32 jmlr-2012-Discriminative Hierarchical Part-based Models for Human Parsing and Action Recognition</a></p>
<p>12 0.038901817 <a title="22-tfidf-12" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>13 0.038738079 <a title="22-tfidf-13" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>14 0.037628923 <a title="22-tfidf-14" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>15 0.037396919 <a title="22-tfidf-15" href="./jmlr-2012-A_Topic_Modeling_Toolbox_Using_Belief_Propagation.html">9 jmlr-2012-A Topic Modeling Toolbox Using Belief Propagation</a></p>
<p>16 0.036475338 <a title="22-tfidf-16" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>17 0.036247373 <a title="22-tfidf-17" href="./jmlr-2012-Sally%3A_A_Tool_for_Embedding_Strings_in_Vector_Spaces.html">102 jmlr-2012-Sally: A Tool for Embedding Strings in Vector Spaces</a></p>
<p>18 0.033864267 <a title="22-tfidf-18" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>19 0.032794196 <a title="22-tfidf-19" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>20 0.031777818 <a title="22-tfidf-20" href="./jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems.html">89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.156), (1, -0.073), (2, 0.096), (3, 0.053), (4, 0.047), (5, 0.013), (6, 0.215), (7, -0.029), (8, 0.077), (9, 0.024), (10, 0.038), (11, 0.224), (12, 0.011), (13, -0.087), (14, 0.027), (15, 0.02), (16, -0.015), (17, 0.03), (18, 0.034), (19, 0.032), (20, 0.004), (21, -0.032), (22, -0.071), (23, -0.28), (24, -0.123), (25, -0.038), (26, 0.033), (27, -0.03), (28, 0.021), (29, 0.063), (30, -0.011), (31, -0.109), (32, 0.167), (33, 0.098), (34, -0.225), (35, -0.018), (36, 0.09), (37, 0.047), (38, 0.016), (39, 0.101), (40, -0.052), (41, 0.029), (42, 0.067), (43, 0.093), (44, -0.017), (45, -0.032), (46, 0.137), (47, 0.052), (48, 0.035), (49, -0.131)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.959674 <a title="22-lsi-1" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>Author: Gary B. Huang, Andrew Kae, Carl Doersch, Erik Learned-Miller</p><p>Abstract: We consider a model for which it is important, early in processing, to estimate some variables with high precision, but perhaps at relatively low recall. If some variables can be identiﬁed with near certainty, they can be conditioned upon, allowing further inference to be done efﬁciently. Speciﬁcally, we consider optical character recognition (OCR) systems that can be bootstrapped by identifying a subset of correctly translated document words with very high precision. This “clean set” is subsequently used as document-speciﬁc training data. While OCR systems produce conﬁdence measures for the identity of each letter or word, thresholding these values still produces a signiﬁcant number of errors. We introduce a novel technique for identifying a set of correct words with very high precision. Rather than estimating posterior probabilities, we bound the probability that any given word is incorrect using an approximate worst case analysis. We give empirical results on a data set of difﬁcult historical newspaper scans, demonstrating that our method for identifying correct words makes only two errors in 56 documents. Using document-speciﬁc character models generated from this data, we are able to reduce the error over properly segmented characters by 34.1% from an initial OCR system’s translation.1 Keywords: optical character recognition, probability bounding, document-speciﬁc modeling, computer vision 1. This work is an expanded and revised version of Kae et al. (2010). Supported by NSF Grant IIS-0916555. c 2012 Gary B. Huang, Andrew Kae, Carl Doersch and Erik Learned-Miller. H UANG , K AE , D OERSCH AND L EARNED -M ILLER</p><p>2 0.66580534 <a title="22-lsi-2" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>Author: Trinh Minh Tri Do, Thierry Artières</p><p>Abstract: Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efﬁcient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efﬁcient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random ﬁelds, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy ε with a rate O(1/λε) where λ is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difﬁcult and requires a stronger and more disputable assumption. Yet we provide experimental results on artiﬁcial test problems, and on ﬁve standard and difﬁcult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms. Keywords: optimization, non-convex, non-smooth, cutting plane, bundle method, regularized risk</p><p>3 0.43591824 <a title="22-lsi-3" href="./jmlr-2012-Breaking_the_Curse_of_Kernelization%3A_Budgeted_Stochastic_Gradient_Descent_for_Large-Scale_SVM_Training.html">23 jmlr-2012-Breaking the Curse of Kernelization: Budgeted Stochastic Gradient Descent for Large-Scale SVM Training</a></p>
<p>Author: Zhuang Wang, Koby Crammer, Slobodan Vucetic</p><p>Abstract: Online algorithms that process one example at a time are advantageous when dealing with very large data or with data streams. Stochastic Gradient Descent (SGD) is such an algorithm and it is an attractive choice for online Support Vector Machine (SVM) training due to its simplicity and effectiveness. When equipped with kernel functions, similarly to other SVM learning algorithms, SGD is susceptible to the curse of kernelization that causes unbounded linear growth in model size and update time with data size. This may render SGD inapplicable to large data sets. We address this issue by presenting a class of Budgeted SGD (BSGD) algorithms for large-scale kernel SVM training which have constant space and constant time complexity per update. Speciﬁcally, BSGD keeps the number of support vectors bounded during training through several budget maintenance strategies. We treat the budget maintenance as a source of the gradient error, and show that the gap between the BSGD and the optimal SVM solutions depends on the model degradation due to budget maintenance. To minimize the gap, we study greedy budget maintenance methods based on removal, projection, and merging of support vectors. We propose budgeted versions of several popular online SVM algorithms that belong to the SGD family. We further derive BSGD algorithms for multi-class SVM training. Comprehensive empirical results show that BSGD achieves higher accuracy than the state-of-the-art budgeted online algorithms and comparable to non-budget algorithms, while achieving impressive computational efﬁciency both in time and space during training and prediction. Keywords: SVM, large-scale learning, online learning, stochastic gradient descent, kernel methods</p><p>4 0.4009434 <a title="22-lsi-4" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>Author: Ofer Dekel, Claudio Gentile, Karthik Sridharan</p><p>Abstract: We present a new online learning algorithm in the selective sampling framework, where labels must be actively queried before they are revealed. We prove bounds on the regret of our algorithm and on the number of labels it queries when faced with an adaptive adversarial strategy of generating the instances. Our bounds both generalize and strictly improve over previous bounds in similar settings. Additionally, our selective sampling algorithm can be converted into an efﬁcient statistical active learning algorithm. We extend our algorithm and analysis to the multiple-teacher setting, where the algorithm can choose which subset of teachers to query for each label. Finally, we demonstrate the effectiveness of our techniques on a real-world Internet search problem. Keywords: online learning, regret, label-efﬁcient, crowdsourcing</p><p>5 0.30772135 <a title="22-lsi-5" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>Author: Tom De Smedt, Walter Daelemans</p><p>Abstract: Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classiﬁers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern. Keywords: Python, data mining, natural language processing, machine learning, graph networks</p><p>6 0.30763102 <a title="22-lsi-6" href="./jmlr-2012-Sally%3A_A_Tool_for_Embedding_Strings_in_Vector_Spaces.html">102 jmlr-2012-Sally: A Tool for Embedding Strings in Vector Spaces</a></p>
<p>7 0.26844367 <a title="22-lsi-7" href="./jmlr-2012-Eliminating_Spammers_and_Ranking_Annotators_for_Crowdsourced_Labeling_Tasks.html">37 jmlr-2012-Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks</a></p>
<p>8 0.25136897 <a title="22-lsi-8" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>9 0.25072709 <a title="22-lsi-9" href="./jmlr-2012-Discriminative_Hierarchical_Part-based_Models_for_Human_Parsing_and_Action_Recognition.html">32 jmlr-2012-Discriminative Hierarchical Part-based Models for Human Parsing and Action Recognition</a></p>
<p>10 0.23727539 <a title="22-lsi-10" href="./jmlr-2012-Algorithms_for_Learning_Kernels_Based_on_Centered_Alignment.html">16 jmlr-2012-Algorithms for Learning Kernels Based on Centered Alignment</a></p>
<p>11 0.20851144 <a title="22-lsi-11" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>12 0.20507093 <a title="22-lsi-12" href="./jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems.html">89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</a></p>
<p>13 0.20173994 <a title="22-lsi-13" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>14 0.20101203 <a title="22-lsi-14" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<p>15 0.19519269 <a title="22-lsi-15" href="./jmlr-2012-Mal-ID%3A_Automatic_Malware_Detection_Using_Common_Segment_Analysis_and_Meta-Features.html">63 jmlr-2012-Mal-ID: Automatic Malware Detection Using Common Segment Analysis and Meta-Features</a></p>
<p>16 0.183452 <a title="22-lsi-16" href="./jmlr-2012-A_Topic_Modeling_Toolbox_Using_Belief_Propagation.html">9 jmlr-2012-A Topic Modeling Toolbox Using Belief Propagation</a></p>
<p>17 0.18017113 <a title="22-lsi-17" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>18 0.17386353 <a title="22-lsi-18" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>19 0.1680308 <a title="22-lsi-19" href="./jmlr-2012-Finding_Recurrent_Patterns_from_Continuous_Sign_Language_Sentences_for_Automated_Extraction_of_Signs.html">45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</a></p>
<p>20 0.16580337 <a title="22-lsi-20" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(21, 0.03), (26, 0.024), (29, 0.032), (35, 0.023), (49, 0.01), (56, 0.023), (64, 0.015), (69, 0.024), (75, 0.058), (77, 0.463), (79, 0.03), (81, 0.013), (92, 0.056), (96, 0.098)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84438485 <a title="22-lda-1" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>Author: Gary B. Huang, Andrew Kae, Carl Doersch, Erik Learned-Miller</p><p>Abstract: We consider a model for which it is important, early in processing, to estimate some variables with high precision, but perhaps at relatively low recall. If some variables can be identiﬁed with near certainty, they can be conditioned upon, allowing further inference to be done efﬁciently. Speciﬁcally, we consider optical character recognition (OCR) systems that can be bootstrapped by identifying a subset of correctly translated document words with very high precision. This “clean set” is subsequently used as document-speciﬁc training data. While OCR systems produce conﬁdence measures for the identity of each letter or word, thresholding these values still produces a signiﬁcant number of errors. We introduce a novel technique for identifying a set of correct words with very high precision. Rather than estimating posterior probabilities, we bound the probability that any given word is incorrect using an approximate worst case analysis. We give empirical results on a data set of difﬁcult historical newspaper scans, demonstrating that our method for identifying correct words makes only two errors in 56 documents. Using document-speciﬁc character models generated from this data, we are able to reduce the error over properly segmented characters by 34.1% from an initial OCR system’s translation.1 Keywords: optical character recognition, probability bounding, document-speciﬁc modeling, computer vision 1. This work is an expanded and revised version of Kae et al. (2010). Supported by NSF Grant IIS-0916555. c 2012 Gary B. Huang, Andrew Kae, Carl Doersch and Erik Learned-Miller. H UANG , K AE , D OERSCH AND L EARNED -M ILLER</p><p>2 0.7389406 <a title="22-lda-2" href="./jmlr-2012-Large-scale_Linear_Support_Vector_Regression.html">54 jmlr-2012-Large-scale Linear Support Vector Regression</a></p>
<p>Author: Chia-Hua Ho, Chih-Jen Lin</p><p>Abstract: Support vector regression (SVR) and support vector classiﬁcation (SVC) are popular learning techniques, but their use with kernels is often time consuming. Recently, linear SVC without kernels has been shown to give competitive accuracy for some applications, but enjoys much faster training/testing. However, few studies have focused on linear SVR. In this paper, we extend state-of-theart training methods for linear SVC to linear SVR. We show that the extension is straightforward for some methods, but is not trivial for some others. Our experiments demonstrate that for some problems, the proposed linear-SVR training methods can very efﬁciently produce models that are as good as kernel SVR. Keywords: support vector regression, Newton methods, coordinate descent methods</p><p>3 0.68223828 <a title="22-lda-3" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>Author: Christopher R. Genovese, Jiashun Jin, Larry Wasserman, Zhigang Yao</p><p>Abstract: The lasso is an important method for sparse, high-dimensional regression problems, with efﬁcient algorithms available, a long history of practical success, and a large body of theoretical results supporting and explaining its performance. But even with the best available algorithms, ﬁnding the lasso solutions remains a computationally challenging task in cases where the number of covariates vastly exceeds the number of data points. Marginal regression, where each dependent variable is regressed separately on each covariate, offers a promising alternative in this case because the estimates can be computed roughly two orders faster than the lasso solutions. The question that remains is how the statistical performance of the method compares to that of the lasso in these cases. In this paper, we study the relative statistical performance of the lasso and marginal regression for sparse, high-dimensional regression problems. We consider the problem of learning which coefﬁcients are non-zero. Our main results are as follows: (i) we compare the conditions under which the lasso and marginal regression guarantee exact recovery in the ﬁxed design, noise free case; (ii) we establish conditions under which marginal regression provides exact recovery with high probability in the ﬁxed design, noise free, random coefﬁcients case; and (iii) we derive rates of convergence for both procedures, where performance is measured by the number of coefﬁcients with incorrect sign, and characterize the regions in the parameter space recovery is and is not possible under this metric. In light of the computational advantages of marginal regression in very high dimensional problems, our theoretical and simulations results suggest that the procedure merits further study. Keywords: high-dimensional regression, lasso, phase diagram, regularization</p><p>4 0.42787856 <a title="22-lda-4" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>Author: Guo-Xun Yuan, Chia-Hua Ho, Chih-Jen Lin</p><p>Abstract: Recently, Yuan et al. (2010) conducted a comprehensive comparison on software for L1-regularized classiﬁcation. They concluded that a carefully designed coordinate descent implementation CDN is the fastest among state-of-the-art solvers. In this paper, we point out that CDN is less competitive on loss functions that are expensive to compute. In particular, CDN for logistic regression is much slower than CDN for SVM because the logistic loss involves expensive exp/log operations. In optimization, Newton methods are known to have fewer iterations although each iteration costs more. Because solving the Newton sub-problem is independent of the loss calculation, this type of methods may surpass CDN under some circumstances. In L1-regularized classiﬁcation, GLMNET by Friedman et al. is already a Newton-type method, but experiments in Yuan et al. (2010) indicated that the existing GLMNET implementation may face difﬁculties for some largescale problems. In this paper, we propose an improved GLMNET to address some theoretical and implementation issues. In particular, as a Newton-type method, GLMNET achieves fast local convergence, but may fail to quickly obtain a useful solution. By a careful design to adjust the effort for each iteration, our method is efﬁcient for both loosely or strictly solving the optimization problem. Experiments demonstrate that our improved GLMNET is more efﬁcient than CDN for L1-regularized logistic regression. Keywords: L1 regularization, linear classiﬁcation, optimization methods, logistic regression, support vector machines</p><p>5 0.35118437 <a title="22-lda-5" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>Author: Jun Zhu, Amr Ahmed, Eric P. Xing</p><p>Abstract: A supervised topic model can use side information such as ratings or labels associated with documents or images to discover more predictive low dimensional topical representations of the data. However, existing supervised topic models predominantly employ likelihood-driven objective functions for learning and inference, leaving the popular and potentially powerful max-margin principle unexploited for seeking predictive representations of data and more discriminative topic bases for the corpus. In this paper, we propose the maximum entropy discrimination latent Dirichlet allocation (MedLDA) model, which integrates the mechanism behind the max-margin prediction models (e.g., SVMs) with the mechanism behind the hierarchical Bayesian topic models (e.g., LDA) under a uniﬁed constrained optimization framework, and yields latent topical representations that are more discriminative and more suitable for prediction tasks such as document classiﬁcation or regression. The principle underlying the MedLDA formalism is quite general and can be applied for jointly max-margin and maximum likelihood learning of directed or undirected topic models when supervising side information is available. Efﬁcient variational methods for posterior inference and parameter estimation are derived and extensive empirical studies on several real data sets are also provided. Our experimental results demonstrate qualitatively and quantitatively that MedLDA could: 1) discover sparse and highly discriminative topical representations; 2) achieve state of the art prediction performance; and 3) be more efﬁcient than existing supervised topic models, especially for classiﬁcation. Keywords: supervised topic models, max-margin learning, maximum entropy discrimination, latent Dirichlet allocation, support vector machines</p><p>6 0.32971469 <a title="22-lda-6" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>7 0.32684156 <a title="22-lda-7" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>8 0.3200393 <a title="22-lda-8" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>9 0.31924987 <a title="22-lda-9" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>10 0.31749085 <a title="22-lda-10" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>11 0.31745851 <a title="22-lda-11" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>12 0.31695861 <a title="22-lda-12" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>13 0.31614828 <a title="22-lda-13" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>14 0.31540924 <a title="22-lda-14" href="./jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</a></p>
<p>15 0.31328291 <a title="22-lda-15" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>16 0.31268561 <a title="22-lda-16" href="./jmlr-2012-Security_Analysis_of_Online_Centroid_Anomaly_Detection.html">104 jmlr-2012-Security Analysis of Online Centroid Anomaly Detection</a></p>
<p>17 0.31206545 <a title="22-lda-17" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>18 0.31167555 <a title="22-lda-18" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>19 0.31017178 <a title="22-lda-19" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>20 0.30967808 <a title="22-lda-20" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
