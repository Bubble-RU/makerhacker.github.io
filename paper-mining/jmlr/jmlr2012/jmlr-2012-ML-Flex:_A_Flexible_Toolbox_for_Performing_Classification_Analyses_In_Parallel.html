<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-61" href="#">jmlr2012-61</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</h1>
<br/><p>Source: <a title="jmlr-2012-61-pdf" href="http://jmlr.org/papers/volume13/piccolo12a/piccolo12a.pdf">pdf</a></p><p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>Reference: <a title="jmlr-2012-61-reference" href="../jmlr2012_reference/jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. [sent-10, score-0.156]
</p><p>2 It can handle multiple input-data formats and supports a variety of customizations. [sent-11, score-0.22]
</p><p>3 MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. [sent-12, score-0.314]
</p><p>4 Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. [sent-13, score-0.205]
</p><p>5 This open-source software package is freely available from http://mlflex. [sent-14, score-0.089]
</p><p>6 Introduction The machine-learning community has developed a wide array of classiﬁcation algorithms, but they are implemented in diverse programming languages, have heterogeneous interfaces, and require disparate ﬁle formats. [sent-18, score-0.081]
</p><p>7 Also, because input data come in assorted formats, custom transformations often must precede classiﬁcation. [sent-19, score-0.169]
</p><p>8 To address these challenges, we developed ML-Flex, a generalpurpose toolbox for performing two-class and multi-class classiﬁcation analyses. [sent-20, score-0.078]
</p><p>9 Via command-line interfaces, ML-Flex can invoke algorithms implemented in any programming language. [sent-21, score-0.087]
</p><p>10 In many cases, new packages can be integrated with ML-Flex through only minor modiﬁcations to conﬁguration ﬁles. [sent-26, score-0.081]
</p><p>11 However, via a simple extension mechanism, ML-Flex also supports a great amount of ﬂexibility for custom integrations. [sent-27, score-0.199]
</p><p>12 ML-Flex can parse input data in delimited and ARFF formats, and it can easily be extended to parse data from custom sources. [sent-28, score-0.287]
</p><p>13 ML-Flex can perform systematic evaluations across multiple algorithms and data sets. [sent-29, score-0.084]
</p><p>14 Furthermore, it can aggregate evidence across algorithms and data sets via ensemble learning. [sent-30, score-0.13]
</p><p>15 (When ensemble learners are applied, predictions from individual classiﬁers are reused from prior execution steps, thus decreasing computational overhead. [sent-35, score-0.228]
</p><p>16 ) ML-Flex provides implementations of various validation strategies, including simple train-test validation, K-fold cross validation, repeated random sampling validation, and leave-one-out cross validation. [sent-36, score-0.078]
</p><p>17 For each validation strategy, ML-Flex can also apply feature selection/ranking algorithms and perform nested cross-validation within respective training sets. [sent-37, score-0.078]
</p><p>18 To enable shorter execution times for computationally intensive validation strategies, ML-Flex can be executed in parallel across multiple computing cores/processors and multiple nodes on a network. [sent-38, score-0.575]
</p><p>19 Individual computing nodes may have heterogeneous hardware conﬁgurations so long as each node can access a shared ﬁle system. [sent-39, score-0.375]
</p><p>20 In a recent analysis of a large biomedical data set, ML-Flex was executed simultaneously across hundreds of cluster-computing cores (Piccolo, 2011). [sent-40, score-0.237]
</p><p>21 Upon completing classiﬁcation tasks, ML-Flex produces parsable text ﬁles that report performance metrics, confusion matrices, outputs from individual algorithms, and a record of all conﬁguration settings used. [sent-41, score-0.049]
</p><p>22 These features enable reproducibility and transparency about how a given set of results was obtained. [sent-43, score-0.122]
</p><p>23 ” For a given experiment, the user speciﬁes one or more sets of independent (predictor) variables and a dependent variable (class) as well as any algorithm(s) that should be applied to the data. [sent-51, score-0.042]
</p><p>24 To conﬁgure an experiment, users can specify three types of settings: 1) learner templates, 2) algorithm parameters, and 3) experiment-speciﬁc preferences. [sent-53, score-0.124]
</p><p>25 For example, if a user wanted to apply the Weka implementation of the One Rule classiﬁcation algorithm, with a minimum bucket size of 6, to the classic Iris data set, she would ﬁrst create a learner template such as the following (simpliﬁed for brevity): wekac;mlflex. [sent-54, score-0.186]
</p><p>26 jar {ALGORITHM} -t {INPUT_TRAINING_FILE} -T {INPUT_TEST_FILE} -p 0 -distribution The above template contains three items, separated by semicolons: 1) a unique key, 2) the name of a Java class that supports interfacing with Weka, and 3) a templated shell command for invoking Weka on that system. [sent-57, score-0.337]
</p><p>27 (When ML-Flex executes a command, placeholders—for example, “{ALGORITHM}”—are replaced with relevant values. [sent-58, score-0.062]
</p><p>28 ) Having speciﬁed this template, the user would specify the following in an algorithm-parameters ﬁle: one_r;wekac;weka. [sent-59, score-0.042]
</p><p>29 OneR -B 6 This entry indicates 1) a unique key, 2) a reference to the learner template, and 3) the parameters that should be passed to Weka. [sent-62, score-0.045]
</p><p>30 Finally, the user would include the following in an experiment ﬁle: 556  ML-F LEX  CLASSIFICATION_ALGORITHMS=one_r DATA_PROCESSORS=mlflex. [sent-63, score-0.082]
</p><p>31 arff") The ﬁrst line references the algorithm-parameters entry, and the second line indicates the name of a Java class that can parse the input data. [sent-66, score-0.063]
</p><p>32 ) At each stage of an experiment, ML-Flex can execute in parallel, using a simple, coarse-grained architecture. [sent-68, score-0.042]
</p><p>33 Independent computing tasks—for example, parsing a given input ﬁle, classifying a given combination of data set and algorithm and cross-validation fold, or outputting results— are packaged by each computing node into uniquely deﬁned Java objects. [sent-69, score-0.172]
</p><p>34 Then thread(s) on each computing node compete to execute each task via a locking mechanism. [sent-70, score-0.163]
</p><p>35 Initially, each thread checks for a status ﬁle that would indicate whether a given task has been executed and the corresponding result has been stored. [sent-71, score-0.316]
</p><p>36 If the task has not yet been processed, the thread checks the ﬁle system for a correspondingly named “lock ﬁle” that indicates whether the task is currently being processed by another thread. [sent-72, score-0.311]
</p><p>37 If no lock ﬁle exists, the thread attempts to create the ﬁle atomically. [sent-73, score-0.356]
</p><p>38 Having successfully created the lock ﬁle, the thread executes the task, stores the result on the ﬁle system, and deletes the lock ﬁle. [sent-74, score-0.624]
</p><p>39 If a system error or outage occurs, the lock ﬁle will persist for a userconﬁgurable period of time, after which it will be deleted and the task reattempted. [sent-75, score-0.169]
</p><p>40 Because this parallel-processing approach requires many input/output operations and because individual computing nodes do not communicate with each other directly, minor inefﬁciencies may arise. [sent-76, score-0.211]
</p><p>41 The latter three features are particularly desirable in cluster-computing environments where server reliability may be less than optimal and where computing nodes may become available incrementally. [sent-78, score-0.162]
</p><p>42 Related Work Machine-learning toolboxes like caret, Weka, Orange, and KNIME implement a broad range of classiﬁcation algorithms, but many useful algorithms are not included in these packages, perhaps due to licensing restrictions, resource constraints, or programming-language preferences. [sent-80, score-0.101]
</p><p>43 , 2010), ML-Flex provides a harness that allows developers to implement algorithms natively in the language of their choice. [sent-82, score-0.089]
</p><p>44 Because no language-speciﬁc interfaces are necessary in ML-Flex, integration can often occur with no change to the ML-Flex source code. [sent-83, score-0.137]
</p><p>45 This approach also empowers algorithm developers to take the lead in integrating with ML-Flex and thus beneﬁt from its other features, including model evaluation and parallelization. [sent-84, score-0.052]
</p><p>46 In an alternative approach, ML-Flex provides an extension mechanism, which allows users to preprocess data using custom Java code. [sent-87, score-0.203]
</p><p>47 ) This approach may be especially useful in research settings where unusual data formats are prevalent, advanced transformations are desired, or data must be accessed remotely (for example, via Web services, including those that require authentication). [sent-89, score-0.227]
</p><p>48 557  P ICCOLO AND F REY  Other toolboxes support the ability to distribute workloads across multiple computers. [sent-90, score-0.263]
</p><p>49 For example, a client machine executing the Weka Experimenter module can distribute its workload via Java Remote Method Invocation. [sent-91, score-0.258]
</p><p>50 The caret R package (Kuhn, 2008) uses the NetWorkSpacesTM technology to distribute workloads. [sent-92, score-0.337]
</p><p>51 , 2007) can distribute its workload to cluster servers running Oracle R Grid Engine. [sent-94, score-0.183]
</p><p>52 And Apache MahoutTM (Ingersol, 2009) uses the map/reduce paradigm to enable execution on cluster-computing environments. [sent-95, score-0.15]
</p><p>53 ML-Flex differentiates itself from these tools by 1) supporting heterogeneous conﬁgurations among computing nodes, 2) allowing recovery from system outages due to no single point of failure (assuming redundant disk storage), and 3) supporting restartability and incremental node allocations. [sent-96, score-0.241]
</p><p>54 Many toolboxes—including Weka, Orange, KNIME, caret, SHOGUN, and Wafﬂes (Gashler, 2011)—support experimental reproducibility via application programming interfaces (API), command-line interfaces (CLI), and/or visual workﬂow pipelines. [sent-97, score-0.385]
</p><p>55 Users can write client tools that invoke APIs and that can later be re-executed. [sent-98, score-0.201]
</p><p>56 Scripts that invoke CLIs can be repeated; and visual pipelines typically encapsulate execution logic. [sent-99, score-0.226]
</p><p>57 In ML-Flex, users encode all conﬁguration settings in text ﬁles. [sent-100, score-0.079]
</p><p>58 With this approach, users are not required to write code nor extensive scripts. [sent-101, score-0.079]
</p><p>59 However, with a modest scripting effort, it is possible to generate conﬁguration ﬁles dynamically, an approach that may not be feasible with visual workﬂows. [sent-102, score-0.044]
</p><p>60 Building predictive models in R using the caret package. [sent-156, score-0.175]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('weka', 0.28), ('utah', 0.263), ('knime', 0.219), ('java', 0.191), ('thread', 0.187), ('le', 0.181), ('caret', 0.175), ('piccolo', 0.175), ('lock', 0.169), ('formats', 0.145), ('interfaces', 0.137), ('salt', 0.131), ('custom', 0.124), ('distribute', 0.116), ('nodes', 0.111), ('electronically', 0.101), ('lake', 0.101), ('toolboxes', 0.101), ('template', 0.099), ('orange', 0.099), ('guration', 0.095), ('execution', 0.095), ('command', 0.088), ('demsar', 0.088), ('iccolo', 0.088), ('mierswa', 0.088), ('rulequest', 0.088), ('wekac', 0.088), ('executed', 0.087), ('invoke', 0.087), ('shogun', 0.087), ('ensemble', 0.084), ('heterogeneous', 0.081), ('packages', 0.081), ('users', 0.079), ('toolbox', 0.078), ('validation', 0.078), ('supports', 0.075), ('waf', 0.075), ('hsc', 0.075), ('rey', 0.075), ('apache', 0.075), ('berthold', 0.075), ('client', 0.075), ('interfacing', 0.075), ('les', 0.073), ('node', 0.07), ('holmes', 0.067), ('reproducibility', 0.067), ('lex', 0.067), ('workload', 0.067), ('parse', 0.063), ('gurations', 0.063), ('lewis', 0.063), ('city', 0.063), ('frey', 0.062), ('executes', 0.062), ('hardware', 0.062), ('stacked', 0.062), ('enable', 0.055), ('cores', 0.055), ('parallel', 0.052), ('informatics', 0.052), ('developers', 0.052), ('computing', 0.051), ('stephen', 0.05), ('biomedical', 0.049), ('individual', 0.049), ('majority', 0.047), ('package', 0.046), ('across', 0.046), ('littlestone', 0.045), ('transformations', 0.045), ('vote', 0.045), ('learner', 0.045), ('visual', 0.044), ('software', 0.043), ('checks', 0.042), ('execute', 0.042), ('user', 0.042), ('currently', 0.041), ('processed', 0.041), ('experiment', 0.04), ('tools', 0.039), ('systematic', 0.038), ('rule', 0.038), ('con', 0.038), ('begun', 0.037), ('accessed', 0.037), ('flexible', 0.037), ('fulton', 0.037), ('gashler', 0.037), ('prototyping', 0.037), ('deletes', 0.037), ('delimited', 0.037), ('explorations', 0.037), ('gabriel', 0.037), ('geoff', 0.037), ('harness', 0.037), ('leban', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="61-tfidf-1" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>2 0.059492171 <a title="61-tfidf-2" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>Author: Félix-Antoine Fortin, François-Michel De Rainville, Marc-André Gardner, Marc Parizeau, Christian Gagné</p><p>Abstract: DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black-box frameworks. Freely available with extensive documentation at http://deap.gel.ulaval.ca, DEAP is an open source project under an LGPL license. Keywords: distributed evolutionary algorithms, software tools</p><p>3 0.057620771 <a title="61-tfidf-3" href="./jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering.html">101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</a></p>
<p>Author: Tianqi Chen, Weinan Zhang, Qiuxia Lu, Kailong Chen, Zhao Zheng, Yong Yu</p><p>Abstract: In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative ﬁltering. SVDFeature is designed to efﬁciently solve the feature-based matrix factorization. The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information. The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efﬁcient training on large-scale data set. Using this toolkit, we built solutions to win KDD Cup for two consecutive years. Keywords: large-scale collaborative ﬁltering, context-aware recommendation, ranking</p><p>4 0.056199707 <a title="61-tfidf-4" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>Author: Tom De Smedt, Walter Daelemans</p><p>Abstract: Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classiﬁers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern. Keywords: Python, data mining, natural language processing, machine learning, graph networks</p><p>5 0.054698061 <a title="61-tfidf-5" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>Author: Joonseok Lee, Mingxuan Sun, Guy Lebanon</p><p>Abstract: Recommendation systems are important business applications with signiﬁcant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms. Keywords: recommender systems, collaborative ﬁltering, evaluation metrics</p><p>6 0.052555718 <a title="61-tfidf-6" href="./jmlr-2012-Sally%3A_A_Tool_for_Embedding_Strings_in_Vector_Spaces.html">102 jmlr-2012-Sally: A Tool for Embedding Strings in Vector Spaces</a></p>
<p>7 0.051603727 <a title="61-tfidf-7" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>8 0.051045612 <a title="61-tfidf-8" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>9 0.048461426 <a title="61-tfidf-9" href="./jmlr-2012-Mal-ID%3A_Automatic_Malware_Detection_Using_Common_Segment_Analysis_and_Meta-Features.html">63 jmlr-2012-Mal-ID: Automatic Malware Detection Using Common Segment Analysis and Meta-Features</a></p>
<p>10 0.047251321 <a title="61-tfidf-10" href="./jmlr-2012-Jstacs%3A_A_Java_Framework_for_Statistical_Analysis_and_Classification_of_Biological_Sequences.html">53 jmlr-2012-Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences</a></p>
<p>11 0.038982838 <a title="61-tfidf-11" href="./jmlr-2012-MULTIBOOST%3A_A_Multi-purpose_Boosting_Package.html">62 jmlr-2012-MULTIBOOST: A Multi-purpose Boosting Package</a></p>
<p>12 0.0359395 <a title="61-tfidf-12" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>13 0.035277005 <a title="61-tfidf-13" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>14 0.033697061 <a title="61-tfidf-14" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>15 0.033690508 <a title="61-tfidf-15" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>16 0.031887501 <a title="61-tfidf-16" href="./jmlr-2012-Random_Search_for_Hyper-Parameter_Optimization.html">95 jmlr-2012-Random Search for Hyper-Parameter Optimization</a></p>
<p>17 0.028541867 <a title="61-tfidf-17" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>18 0.028481973 <a title="61-tfidf-18" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>19 0.027264062 <a title="61-tfidf-19" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>20 0.027130324 <a title="61-tfidf-20" href="./jmlr-2012-The_huge_Package_for_High-dimensional_Undirected_Graph_Estimation_in_R.html">113 jmlr-2012-The huge Package for High-dimensional Undirected Graph Estimation in R</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.11), (1, 0.029), (2, 0.181), (3, -0.037), (4, 0.011), (5, 0.087), (6, 0.086), (7, -0.002), (8, -0.036), (9, -0.031), (10, -0.083), (11, -0.056), (12, 0.22), (13, -0.13), (14, -0.139), (15, 0.061), (16, 0.072), (17, -0.093), (18, -0.171), (19, -0.26), (20, 0.028), (21, 0.028), (22, -0.024), (23, 0.022), (24, 0.033), (25, -0.057), (26, 0.104), (27, -0.059), (28, 0.009), (29, -0.011), (30, 0.057), (31, 0.014), (32, 0.021), (33, 0.056), (34, -0.007), (35, -0.018), (36, -0.009), (37, -0.108), (38, -0.072), (39, 0.044), (40, -0.07), (41, -0.051), (42, -0.04), (43, -0.073), (44, 0.032), (45, -0.05), (46, -0.082), (47, -0.016), (48, -0.0), (49, -0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9646998 <a title="61-lsi-1" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>2 0.53824443 <a title="61-lsi-2" href="./jmlr-2012-Sally%3A_A_Tool_for_Embedding_Strings_in_Vector_Spaces.html">102 jmlr-2012-Sally: A Tool for Embedding Strings in Vector Spaces</a></p>
<p>Author: Konrad Rieck, Christian Wressnegger, Alexander Bikadorov</p><p>Abstract: Strings and sequences are ubiquitous in many areas of data analysis. However, only few learning methods can be directly applied to this form of data. We present Sally, a tool for embedding strings in vector spaces that allows for applying a wide range of learning methods to string data. Sally implements a generalized form of the bag-of-words model, where strings are mapped to a vector space that is spanned by a set of string features, such as words or n-grams of words. The implementation of Sally builds on efﬁcient string algorithms and enables processing millions of strings and features. The tool supports several data formats and is capable of interfacing with common learning environments, such as Weka, Shogun, Matlab, or Pylab. Sally has been successfully applied for learning with natural language text, DNA sequences and monitored program behavior. Keywords: string embedding, bag-of-words models, learning with sequential data</p><p>3 0.51026255 <a title="61-lsi-3" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>Author: David Verstraeten, Benjamin Schrauwen, Sander Dieleman, Philemon Brakel, Pieter Buteneers, Dejan Pecevski</p><p>Abstract: Oger (OrGanic Environment for Reservoir computing) is a Python toolbox for building, training and evaluating modular learning architectures on large data sets. It builds on MDP for its modularity, and adds processing of sequential data sets, gradient descent training, several crossvalidation schemes and parallel parameter optimization methods. Additionally, several learning algorithms are implemented, such as different reservoir implementations (both sigmoid and spiking), ridge regression, conditional restricted Boltzmann machine (CRBM) and others, including GPU accelerated versions. Oger is released under the GNU LGPL, and is available from http: //organic.elis.ugent.be/oger. Keywords: Python, modular architectures, sequential processing</p><p>4 0.4909274 <a title="61-lsi-4" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>Author: Stephen Gould</p><p>Abstract: We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data ﬂows. Keywords: machine learning, graphical models, computer vision, open-source software</p><p>5 0.47389987 <a title="61-lsi-5" href="./jmlr-2012-Mal-ID%3A_Automatic_Malware_Detection_Using_Common_Segment_Analysis_and_Meta-Features.html">63 jmlr-2012-Mal-ID: Automatic Malware Detection Using Common Segment Analysis and Meta-Features</a></p>
<p>Author: Gil Tahan, Lior Rokach, Yuval Shahar</p><p>Abstract: This paper proposes several novel methods, based on machine learning, to detect malware in executable ﬁles without any need for preprocessing, such as unpacking or disassembling. The basic method (Mal-ID) is a new static (form-based) analysis methodology that uses common segment analysis in order to detect malware ﬁles. By using common segment analysis, Mal-ID is able to discard malware parts that originate from benign code. In addition, Mal-ID uses a new kind of feature, termed meta-feature, to better capture the properties of the analyzed segments. Rather than using the entire ﬁle, as is usually the case with machine learning based techniques, the new approach detects malware on the segment level. This study also introduces two Mal-ID extensions that improve the Mal-ID basic method in various aspects. We rigorously evaluated Mal-ID and its two extensions with more than ten performance measures, and compared them to the highly rated boosted decision tree method under identical settings. The evaluation demonstrated that Mal-ID and the two Mal-ID extensions outperformed the boosted decision tree method in almost all respects. In addition, the results indicated that by extracting meaningful features, it is sufﬁcient to employ one simple detection rule for classifying executable ﬁles. Keywords: computer security, malware detection, common segment analysis, supervised learning</p><p>6 0.46754515 <a title="61-lsi-6" href="./jmlr-2012-Jstacs%3A_A_Java_Framework_for_Statistical_Analysis_and_Classification_of_Biological_Sequences.html">53 jmlr-2012-Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences</a></p>
<p>7 0.4464846 <a title="61-lsi-7" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>8 0.44581434 <a title="61-lsi-8" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>9 0.4191727 <a title="61-lsi-9" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>10 0.37480053 <a title="61-lsi-10" href="./jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering.html">101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</a></p>
<p>11 0.26976103 <a title="61-lsi-11" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>12 0.23406996 <a title="61-lsi-12" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>13 0.22870567 <a title="61-lsi-13" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>14 0.22395042 <a title="61-lsi-14" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>15 0.21012372 <a title="61-lsi-15" href="./jmlr-2012-MULTIBOOST%3A_A_Multi-purpose_Boosting_Package.html">62 jmlr-2012-MULTIBOOST: A Multi-purpose Boosting Package</a></p>
<p>16 0.19940139 <a title="61-lsi-16" href="./jmlr-2012-A_Local_Spectral_Method_for_Graphs%3A_With_Applications_to_Improving_Graph_Partitions_and_Exploring_Data_Graphs_Locally.html">5 jmlr-2012-A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally</a></p>
<p>17 0.19610444 <a title="61-lsi-17" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>18 0.1939279 <a title="61-lsi-18" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>19 0.18138698 <a title="61-lsi-19" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<p>20 0.17222209 <a title="61-lsi-20" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.019), (21, 0.039), (26, 0.017), (27, 0.015), (29, 0.017), (49, 0.011), (56, 0.697), (57, 0.012), (69, 0.013), (75, 0.023), (92, 0.022), (96, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93954504 <a title="61-lda-1" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>2 0.64567196 <a title="61-lda-2" href="./jmlr-2012-Fast_Approximation_of_Matrix_Coherence_and_Statistical_Leverage.html">43 jmlr-2012-Fast Approximation of Matrix Coherence and Statistical Leverage</a></p>
<p>Author: Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, David P. Woodruff</p><p>Abstract: The statistical leverage scores of a matrix A are the squared row-norms of the matrix containing its (top) left singular vectors and the coherence is the largest leverage score. These quantities are of interest in recently-popular problems such as matrix completion and Nystr¨ m-based low-rank o matrix approximation as well as in large-scale statistical data analysis applications more generally; moreover, they are of interest since they deﬁne the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms. Our main result is a randomized algorithm that takes as input an arbitrary n × d matrix A, with n ≫ d, and that returns as output relative-error approximations to all n of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of n and d) in O(nd log n) time, as opposed to the O(nd 2 ) time required by the na¨ve algorithm that involves computing an orthogonal basis for the ı range of A. Our analysis may be viewed in terms of computing a relative-error approximation to an underconstrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with n ≈ d, and the extension to streaming environments. Keywords: matrix coherence, statistical leverage, randomized algorithm</p><p>3 0.24659455 <a title="61-lda-3" href="./jmlr-2012-Jstacs%3A_A_Java_Framework_for_Statistical_Analysis_and_Classification_of_Biological_Sequences.html">53 jmlr-2012-Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences</a></p>
<p>Author: Jan Grau, Jens Keilwagen, André Gohr, Berit Haldemann, Stefan Posch, Ivo Grosse</p><p>Abstract: Jstacs is an object-oriented Java library for analysing and classifying sequence data, which emerged from the need for a standardized implementation of statistical models, learning principles, classiﬁers, and performance measures. In Jstacs, these components can be used, combined, and extended easily, which allows for a direct comparison of different approaches and fosters the development of new components. Jstacs is especially tailored to biological sequence data, but is also applicable to general discrete and continuous data. Jstacs is freely available at http://www.jstacs.de under the GNU GPL license including an API documentation, a cookbook, and code examples. Keywords: machine learning, statistical models, Java, bioinformatics, classiﬁcation</p><p>4 0.24105005 <a title="61-lda-4" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>Author: Marinka Žitnik, Blaž Zupan</p><p>Abstract: NIMFA is an open-source Python library that provides a uniﬁed interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA’s component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks. Keywords: nonnegative matrix factorization, initialization methods, quality measures, scripting, Python</p><p>5 0.23625045 <a title="61-lda-5" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>Author: Sanjiv Kumar, Mehryar Mohri, Ameet Talwalkar</p><p>Abstract: The Nystr¨ m method is an efﬁcient technique to generate low-rank matrix approximations and is o used in several large-scale learning applications. A key aspect of this method is the procedure according to which columns are sampled from the original matrix. In this work, we explore the efﬁcacy of a variety of ﬁxed and adaptive sampling schemes. We also propose a family of ensemble-based sampling algorithms for the Nystr¨ m method. We report results of extensive experiments o that provide a detailed comparison of various ﬁxed and adaptive sampling techniques, and demonstrate the performance improvement associated with the ensemble Nystr¨ m method when used in o conjunction with either ﬁxed or adaptive sampling schemes. Corroborating these empirical ﬁndings, we present a theoretical analysis of the Nystr¨ m method, providing novel error bounds guaro anteeing a better convergence rate of the ensemble Nystr¨ m method in comparison to the standard o Nystr¨ m method. o Keywords: low-rank approximation, nystr¨ m method, ensemble methods, large-scale learning o</p><p>6 0.22509816 <a title="61-lda-6" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>7 0.20731404 <a title="61-lda-7" href="./jmlr-2012-Sally%3A_A_Tool_for_Embedding_Strings_in_Vector_Spaces.html">102 jmlr-2012-Sally: A Tool for Embedding Strings in Vector Spaces</a></p>
<p>8 0.20628376 <a title="61-lda-8" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>9 0.20569818 <a title="61-lda-9" href="./jmlr-2012-A_Local_Spectral_Method_for_Graphs%3A_With_Applications_to_Improving_Graph_Partitions_and_Exploring_Data_Graphs_Locally.html">5 jmlr-2012-A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally</a></p>
<p>10 0.20484042 <a title="61-lda-10" href="./jmlr-2012-MULTIBOOST%3A_A_Multi-purpose_Boosting_Package.html">62 jmlr-2012-MULTIBOOST: A Multi-purpose Boosting Package</a></p>
<p>11 0.18775225 <a title="61-lda-11" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>12 0.17522837 <a title="61-lda-12" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>13 0.1737166 <a title="61-lda-13" href="./jmlr-2012-Mal-ID%3A_Automatic_Malware_Detection_Using_Common_Segment_Analysis_and_Meta-Features.html">63 jmlr-2012-Mal-ID: Automatic Malware Detection Using Common Segment Analysis and Meta-Features</a></p>
<p>14 0.17093982 <a title="61-lda-14" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>15 0.16780451 <a title="61-lda-15" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>16 0.16428001 <a title="61-lda-16" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>17 0.16426705 <a title="61-lda-17" href="./jmlr-2012-Towards_Integrative_Causal_Analysis_of_Heterogeneous_Data_Sets_and_Studies.html">114 jmlr-2012-Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies</a></p>
<p>18 0.16383603 <a title="61-lda-18" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>19 0.16378862 <a title="61-lda-19" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<p>20 0.16219801 <a title="61-lda-20" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
