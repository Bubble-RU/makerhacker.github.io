<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 jmlr-2012-A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-5" href="#">jmlr2012-5</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 jmlr-2012-A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally</h1>
<br/><p>Source: <a title="jmlr-2012-5-pdf" href="http://jmlr.org/papers/volume13/mahoney12a/mahoney12a.pdf">pdf</a></p><p>Author: Michael W. Mahoney, Lorenzo Orecchia, Nisheeth K. Vishnoi</p><p>Abstract: The second eigenvalue of the Laplacian matrix and its associated eigenvector are fundamental features of an undirected graph, and as such they have found widespread use in scientiﬁc computing, machine learning, and data analysis. In many applications, however, graphs that arise have several local regions of interest, and the second eigenvector will typically fail to provide information ﬁne-tuned to each local region. In this paper, we introduce a locally-biased analogue of the second eigenvector, and we demonstrate its usefulness at highlighting local properties of data graphs in a semi-supervised manner. To do so, we ﬁrst view the second eigenvector as the solution to a constrained optimization problem, and we incorporate the local information as an additional constraint; we then characterize the optimal solution to this new problem and show that it can be interpreted as a generalization of a Personalized PageRank vector; and ﬁnally, as a consequence, we show that the solution can be computed in nearly-linear time. In addition, we show that this locally-biased vector can be used to compute an approximation to the best partition near an input seed set in a manner analogous to the way in which the second eigenvector of the Laplacian can be used to obtain an approximation to the best partition in the entire input graph. Such a primitive is useful for identifying and reﬁning clusters locally, as it allows us to focus on a local region of interest in a semi-supervised manner. Finally, we provide a detailed empirical evaluation of our method by showing how it can applied to ﬁnding locally-biased sparse cuts around an input vertex seed set in social and information networks. Keywords: spectral graph partitioning, local spectral algorithms, Laplacian matrix, semi-supervised learning, personalized pagerank c 2012 Michael W. Mahoney and Lorenzo Orecchia and Nisheeth K. Vishnoi. M AHONEY, O RECCHIA , AND V ISHNOI</p><p>Reference: <a title="jmlr-2012-5-reference" href="../jmlr2012_reference/jmlr-2012-A_Local_Spectral_Method_for_Graphs%3A_With_Applications_to_Improving_Graph_Partitions_and_Exploring_Data_Graphs_Locally_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Finally, we provide a detailed empirical evaluation of our method by showing how it can applied to ﬁnding locally-biased sparse cuts around an input vertex seed set in social and information networks. [sent-15, score-0.642]
</p><p>2 Keywords: spectral graph partitioning, local spectral algorithms, Laplacian matrix, semi-supervised learning, personalized pagerank c 2012 Michael W. [sent-16, score-0.496]
</p><p>3 For instance, a sparse cut in a graph may be poorly correlated with the second eigenvector (and even with all the eigenvectors of the Laplacian) and thus invisible to a method based only on eigenvector analysis. [sent-26, score-0.48]
</p><p>4 Finally, the optimal solution to LocalSpectral can be used to derive bounds on the mixing time of random walks that start near the local target region as well as on the existence of sparse cuts near the locally-biased target region. [sent-39, score-0.342]
</p><p>5 In particular, it lower bounds the conductance of cuts as a function of how well-correlated they are with the seed vector. [sent-40, score-0.775]
</p><p>6 This will allow us to exploit the analogy between global eigenvectors 2340  A L OCAL S PECTRAL M ETHOD FOR G RAPHS  and our localized analogue to design an algorithm for discovering sparse cuts near an input seed set of vertices. [sent-41, score-0.589]
</p><p>7 Then, we will describe in detail how our algorithm for discovering sparse cuts near an input seed set of vertices may be applied to the problem of exploring data graphs locally and to identifying locally-biased clusters and communities in a more difﬁcult-tovisualize social network application. [sent-45, score-0.712]
</p><p>8 Recent theoretical work has focused on using spectral ideas to ﬁnd good clusters nearby an input seed set of nodes (Spielman and Teng, 2004; Andersen, Chung, and Lang, 2006; Chung, 2007). [sent-48, score-0.463]
</p><p>9 def  ¯  |E(S,S)| ¯ In this paper, the conductance φ(S) of a cut (S, S) is φ(S) = vol(G) · vol(S)·vol(S) . [sent-76, score-0.515]
</p><p>10 Note that the conductance of a set S, or equivalently a cut ¯ ¯ ¯ (S, S), is often deﬁned as φ′ (S) = |E(S, S)|/ min{vol(S), vol(S)}. [sent-79, score-0.469]
</p><p>11 This notion is equivalent to that φ(S), in that the value φ(G) thereby obtained for the conductance of the graph G differs by no more than a factor of 2 times the constant vol(G), depending on which notion we use for the conductance of a set. [sent-80, score-0.551]
</p><p>12 Theorem 1 (Solution Characterization) Let s ∈ RV be a seed vector such that sT DG 1 = 0, sT DG s = 1, and sT DG v2 = 0, where v2 is the second generalized eigenvector of LG with respect to DG . [sent-121, score-0.331]
</p><p>13 Application to Partitioning Graphs Locally In this section, we describe the application of LocalSpectral to ﬁnding locally-biased partitions in a graph, that is, to ﬁnding sparse cuts around an input seed vertex set in the graph. [sent-215, score-0.598]
</p><p>14 To understand this optimization problem, recall that xT LG x counts the number of edges crossing the cut and that xT DG x = 1 encodes a variance constraint; thus, the goal of Spectral(G) is to minimize the number of edges crossing the cut subject to a given variance. [sent-221, score-0.468]
</p><p>15 Moreover, this program is a good relaxation in that a good cut can be recovered by considering a truncation, that is, a sweep cut, of the vector v2 that is the optimal solution to Spectral(G). [sent-226, score-0.408]
</p><p>16 (That is, for example, consider each of the n cuts deﬁned by the vector v2 , and return the cut with minimum conductance value. [sent-227, score-0.75]
</p><p>17 That is, the problem is to ﬁnd the best conductance set of nodes of volume no greater than k that contains the input node v. [sent-238, score-0.332]
</p><p>18 As a ﬁrst step, we show that we can choose the seed set and correlation parameters s and κ such that LocalSpectral(G, s, κ) is a relaxation for this locally-biased graph partitioning problem. [sent-239, score-0.433]
</p><p>19 Lemma 5 For u ∈ V , LocalSpectral(G, v{u} , 1/k) is a relaxation of the problem of ﬁnding a minimum conductance cut T in G which contains the vertex u and is of volume at most k. [sent-240, score-0.57]
</p><p>20 du (2m−vol(T )) vol(T )(2m−du )  ≥ 1/k, which establishes the  Next, we can apply Theorem 4 to the optimal solution for LocalSpectral(G, v{u} , 1/k) and obtain a cut T whose conductance is quadratically close to the optimal value λ(G, v{u} , 1/k). [sent-244, score-0.49]
</p><p>21 Theorem 6 (Finding a Cut) Given an unweighted graph G = (V, E), a vertex u ∈ V and a positive integer k, we can ﬁnd a cut in G of conductance at most O( φ(u, k)) by computing a sweep cut of the optimal vector for LocalSpectral(G, v{u} , 1/k). [sent-247, score-0.937]
</p><p>22 Our ﬁnal theorem shows that the optimal value of LocalSpectral also provides a lower bound on the conductance of other cuts, as a function of how well-correlated they are with the input seed vector. [sent-250, score-0.494]
</p><p>23 In particular, when the seed vector corresponds to a cut U, this result allows us to lower bound the conductance of an arbitrary cut T , in terms of the correlation between U and T . [sent-251, score-0.991]
</p><p>24 Although one can imagine many applications of this primitive, the main application that motivated this work was to explore clusters nearby or around a given seed set of nodes in data graphs. [sent-264, score-0.361]
</p><p>25 Given a cut (T, T ) in a graph G = (V, E), a natural vector in RV to associate ¯ with it is its characteristic vector, in which case the correlation between a cut (T, T ) and another ¯ can be captured by the inner product of the characteristic vectors of the two cuts. [sent-268, score-0.578]
</p><p>26 A cut (U, U) somewhat more reﬁned vector to associate with a cut is the vector obtained after removing from the characteristic vector its projection along the all-ones vector. [sent-269, score-0.468]
</p><p>27 More precisely, given a ¯ set of nodes T ⊆ V , or equivalently a cut (T, T ), one can deﬁne the unit vector sT as sT (i) = def  −  ¯ vol(T )vol(T )/2m · 1/vol(T )  ¯ vol(T )vol(T )/2m · 1/vol(T ) ¯  ¯  if i ∈ T ¯ if i ∈ T . [sent-271, score-0.328]
</p><p>28 1 Second, one can introduce the following measure of correlation between two ¯ ¯ sets of nodes, or equivalently between two cuts, say a cut (T, T ) and a cut (U, U): def  K(T,U) = (sT DG sU )2 . [sent-278, score-0.543]
</p><p>29 Empirical Evaluation In this section, we provide an empirical evaluation of LocalSpectral by illustrating its use at ﬁnding and evaluating locally-biased low-conductance cuts, that is, sparse cuts or good clusters, around an input seed set of nodes in a data graph. [sent-285, score-0.588]
</p><p>30 Note that no sweep cut of these eigenvectors reveals the leopard. [sent-304, score-0.35]
</p><p>31 A standard approach to ﬁnding clusters and communities in many network analysis applications is to formalize the idea of a good community with an “edge counting” metric such as conductance or modularity and then to use a spectral relaxation to optimize it approximately (Newman, 2006b,a). [sent-310, score-0.457]
</p><p>32 0029; and a sweep cut of the eigenvector corresponding to this second eigenvalue yields the globally-optimal spectral cut separating the graph into two wellbalanced partitions, corresponding to the left half and the right half of the network, as shown in Figure 5. [sent-321, score-0.818]
</p><p>33 We do this by detecting a wider range of low-conductance cuts at a given volume and by interpolating smoothly between very locally-biased solutions to LocalSpectral and the global solution provided by the Spectral program. [sent-325, score-0.348]
</p><p>34 • Third, we demonstrate how our method can ﬁnd low-conductance cuts that are well-correlated to more general input seed vectors by demonstrating an application to the detection of sparse peripheral regions, for example, regions of the network that are well-correlated with lowdegree nodes. [sent-326, score-0.578]
</p><p>35 1 A LGORITHM D ESCRIPTION AND I MPLEMENTATION We refer to our cut-ﬁnding algorithm, which will be used to guide our empirical study of ﬁnding and evaluating cuts around an input seed set of nodes and which is a straightforward extension of the algorithm referred to in Theorem 6, as LocalCut. [sent-333, score-0.588]
</p><p>36 • First, compute the vector x⋆ of Equation (1) with seed s and teleportation γ. [sent-338, score-0.382]
</p><p>37 In addition, when the seed vector corresponds to a single vertex v, it follows from Lemma 5 that x⋆ yields a lower bound to the conductance of cuts that contain v and have less than a certain volume kγ . [sent-343, score-0.851]
</p><p>38 Thus, in our empirical evaluation, we also consider volume-constrained sweep cuts (which departs slightly from the theory but can be useful in practice). [sent-345, score-0.376]
</p><p>39 That is, we also introduce a new input parameter, a size factor c > 0, that regulates the maximum volume of the sweep cuts considered when s represents a single vertex. [sent-346, score-0.394]
</p><p>40 In this case, LocalCut does not consider 2354  A L OCAL S PECTRAL M ETHOD FOR G RAPHS  all n cuts deﬁned by the vector x⋆ , but instead it considers only sweep cuts of volume at most c · kγ that contain the vertex v. [sent-347, score-0.733]
</p><p>41 (Note that it is a simple consequence of our optimization characterization that the optimal vector has sweep cuts of volume at most kγ containing v. [sent-348, score-0.394]
</p><p>42 ) This new input parameter turns out to be extremely useful in exploring cuts at different sizes, as it neglects sweep cuts of low conductance at large volume and allows us to pick out more local cuts around the seed vertex. [sent-349, score-1.469]
</p><p>43 3, we used singlevertex seed vectors, and we analyzed the effects of varying the parameters γ and c, as a function of the location of the seed vertex in the input graph. [sent-354, score-0.598]
</p><p>44 4, we considered more general seed vectors, including both seed vectors that correspond to multiple nodes, that is, to cuts or partitions in the graph, as well as seed vectors that do not have an obvious interpretation in terms of input cuts. [sent-357, score-1.058]
</p><p>45 Figure 6 displays, for each of these three seeds, a plot of the conductance as a function of volume of the cuts found by each run of LocalCut. [sent-369, score-0.534]
</p><p>46 We refer to this type of plot as a local proﬁle plot since it is a specialization of the network community proﬁle plot (Leskovec, Lang, Dasgupta, and Mahoney, 2008, 2009; Leskovec, Lang, and Mahoney, 2010) to cuts around the speciﬁed seed vertex. [sent-370, score-0.597]
</p><p>47 5  M AHONEY, O RECCHIA , AND V ISHNOI  T h eoreti cal Low er B oun d A lg ori th m O utput ( Cut Sh ow n ) A lg ori th m O utput ( Cut N ot Sh ow n ) Sh ortes t P ath A lg ori th m O utput  0 . [sent-377, score-0.949]
</p><p>48 0 0 1 3 q q qqq qq q  qqq q  qqqq qq q q q q q q q q q q q q  q  q  q  q  q  q  q  q  q  q  q  q  q  q  q  q  q  q  q  q q q  q  q  q  q  q  4 − 2 e− 0 4 . [sent-386, score-1.014]
</p><p>49 0 0 2 8 − 0 q q q  q  q  q  q  q  q  q  q  q  q  q  q qq q  q  P aj ek  0  2 0 0  4 0 0  6 0 0  8 0 0  V olum e 0 . [sent-387, score-0.467]
</p><p>50 0 8 5 2  T h eoreti cal Low er B oun d A lg ori th m O utput ( Cut Sh ow n ) A lg ori th m O utput ( Cut N ot Sh ow n ) Sh ortes t P ath A lg ori th m O utput  q  0 . [sent-390, score-0.949]
</p><p>51 0 5  Con d uctan ce  q q  3  5 6  q q q q q  q q  qq qq q  0 . [sent-395, score-0.863]
</p><p>52 0 0  Con d uctan ce  1  T h eoreti cal Low er B oun d A lg ori th m O utput ( Cut Sh ow n ) A lg ori th m O utput ( Cut N ot Sh ow n ) Sh ortes t P ath A lg ori th m O utput  7 6  0 . [sent-403, score-0.97]
</p><p>53 0 0 2 4  qq  q q q  q  q  q  q  q  q  q  q  q  − 0 . [sent-405, score-0.421]
</p><p>54 0 q 2 8 0 q  q  P aj ek  0  2 0 0  4 0 0  6 0 0  8 0 0  V olum e  (c) Selected cuts and proﬁle plot for the periphery-like node. [sent-409, score-0.327]
</p><p>55 The cuts on the left are displayed by assigning to each vertex a color corresponding to the smallest selected cut in which the vertex was included. [sent-411, score-0.654]
</p><p>56 Smaller cuts are darker, larger cuts are lighter; and the seed vertex is shown slightly larger. [sent-412, score-0.879]
</p><p>57 0207, the output cuts are forced to be small and hence display high conductance, as the region around the node is somewhat expander-like. [sent-419, score-0.333]
</p><p>58 By decreasing the teleportation, the conductance progressively decreases, as the rounding starts to hit nodes in peripheral regions, whose inclusion only improves conductance (since it increases the cut volume without adding many additional cut edges). [sent-420, score-1.04]
</p><p>59 0013, when a cut of conductance value close to that of the global optimum is found. [sent-422, score-0.497]
</p><p>60 (After that, larger and slightly better conductance cuts can still be found, but, as discussed below, they require γ > 0. [sent-423, score-0.516]
</p><p>61 Here, however, the global component of the network containing the seed has smaller volume, around 300, and a very low conductance (again, requiring γ > 0). [sent-425, score-0.542]
</p><p>62 Thus, the proﬁle plot jumps from this cut to the much larger eigenvector sweep cut, as will be discussed below. [sent-426, score-0.401]
</p><p>63 The only cuts of lower conductance in the network are those separating the global components, which can only be accessed when γ > 0. [sent-430, score-0.564]
</p><p>64 Hence, the teleportation must be greatly decreased before the algorithm starts outputting cuts at larger volumes. [sent-431, score-0.404]
</p><p>65 • First, LocalCut found low-conductance cuts of different volumes around each seed vertex, outperforming the shortest-path algorithm (as it should) by a factor of roughly 4 in most cases. [sent-434, score-0.561]
</p><p>66 0028 ≈ λ2 (G), LocalCut outputs the same cut as the eigenvector sweep cut for all three seeds. [sent-444, score-0.635]
</p><p>67 • Third, recall that, given a teleportation parameter γ, the rounding step selects the cut of smallest conductance along the sweep cut of the solution vector. [sent-445, score-0.96]
</p><p>68 (Alternatively, if volume2357  M AHONEY, O RECCHIA , AND V ISHNOI  constrained sweeps are considered, then it selects the cut of smallest conductance among sweep cuts of volume at most c · kγ , where kγ is the lower bound obtained from the optimization program. [sent-446, score-0.863]
</p><p>69 In those cases, LocalCut may output the same small sweep cut for a range of teleportation parameters until a much larger, much lower-conductance cut is then found. [sent-449, score-0.686]
</p><p>70 ) We have observed that varying c, like varying γ, tends to have the effect of producing low-conductance cuts of different volumes around the seed vertex. [sent-457, score-0.605]
</p><p>71 0207); but the yellow/lighter crosses in Figure 7 illustrate that by allowing larger values of c, better conductance cuts of larger volume can be obtained. [sent-462, score-0.534]
</p><p>72 0 2  q  q  q  q  q  q  q  q  q  P aj ek  0  2 0 0  4 0 0  6 0 0  8 0 0  V olum e  Figure 7: Selected cuts and local proﬁle plots for varying c with the core-like node as the seed. [sent-470, score-0.399]
</p><p>73 The cuts are displayed by assigning to each vertex a color corresponding to the smallest selected cut in which the vertex was included. [sent-471, score-0.654]
</p><p>74 ) To address (and rule out) this possibility, we selected three choices of the teleportation parameter for each of the three seed nodes, and then we let c vary. [sent-479, score-0.382]
</p><p>75 The resulting output cuts for the core-like node as the seed are plotted in Figure 7. [sent-480, score-0.571]
</p><p>76 ) Clearly, no single teleportation setting dominates the others: in particular, at volume 200 the lowestconductance cut was produced with −γ = 0. [sent-482, score-0.375]
</p><p>77 The highest choice of γ = 0 performed marginally better overall, recording lowest conductance cuts at both small and large volumes. [sent-485, score-0.516]
</p><p>78 We consider two examples— for the ﬁrst example, there is an interpretation as a cut or partition consisting of multiple nodes; while the second example does not have any immediate interpretation in terms of cuts or partitions. [sent-492, score-0.531]
</p><p>79 8(a) shows selected cuts for varying γ with the seed vector corresponding to a subset of 4 vertices lying in the periphery-like region of the network. [sent-496, score-0.612]
</p><p>80 8(b) shows selected cuts for varying γ with the seed vertex equal to a normalized version of the degree vector. [sent-497, score-0.62]
</p><p>81 In both cases, the cuts are displayed by assigning to each vertex a color corresponding to the smallest selected cut in which the vertex was included. [sent-498, score-0.654]
</p><p>82 In our ﬁrst example, we consider a seed vector representing a subset of four nodes, located in different peripheral branches of the left half of the global partition of the the network: see the four slightly larger (and darker) vertices in Figure 8(a). [sent-500, score-0.35]
</p><p>83 The smaller cuts tend to contain the branches in which each seed node is found, while larger cuts start to incorporate nearby branches. [sent-504, score-0.871]
</p><p>84 A selection of the cuts found on this seed vector when varying the teleportation with c = 2 is displayed in Figure 8(b). [sent-510, score-0.708]
</p><p>85 These cuts partition the network naturally into three well-separated regions: a sparser periphery-like region in darker colors, a lighter-colored intermediate region, and a white dense core-like region, where higher-degree vertices tend to lie. [sent-511, score-0.388]
</p><p>86 1 Relationship to Local Graph Partitioning Recent theoretical work has focused on using spectral ideas to ﬁnd good clusters nearby an input seed set of nodes (Spielman and Teng, 2004; Andersen, Chung, and Lang, 2006; Chung, 2007). [sent-516, score-0.463]
</p><p>87 In particular, local graph partitioning—roughly, the problem of ﬁnding a low-conductance cut in a graph in time depending only on the volume of the output cut—was introduced by Spielman and Teng (2004). [sent-517, score-0.433]
</p><p>88 In our language, a local graph partitioning algorithm would start a random walk at a seed node, truncating the walk after a suitably chosen number of steps, and outputting the nodes visited by the walk. [sent-519, score-0.486]
</p><p>89 These and subsequent papers building on them were motivated by local graph partitioning (Chung, 2007), but they do not address the problem of discovering cuts near general seed vectors, as do we, or of generalizing the second eigenvector of the Laplacian. [sent-521, score-0.751]
</p><p>90 In particular, Andersen and Lang used local spectral methods to identify communities in certain informatics graphs using an input set of nodes as a seed set (Andersen and Lang, 2006). [sent-526, score-0.472]
</p><p>91 Our optimization program and empirical results suggest that this line of work can be extended to ask in a theoretically principled manner much more reﬁned questions about graph structure near prespeciﬁed seed vectors. [sent-528, score-0.373]
</p><p>92 This primitive is referred to as a cutlower-conductance cut that is well correlated with (T, T improvement algorithm (Lang and Rao, 2004; Andersen and Lang, 2008), as its original purpose was limited to post-processing cuts output by other algorithms. [sent-531, score-0.533]
</p><p>93 Recently, cut-improvement algorithms have also been used to ﬁnd low conductance cuts in speciﬁc regions of large graphs (Leskovec, Lang, and Mahoney, 2010). [sent-532, score-0.538]
</p><p>94 Finally, Andersen and Lang (2008) give a more general algorithm that uses a small number of single-commodity maximum-ﬂows to ﬁnd low-conductance cuts not ¯ only inside the input subset T , but among all cuts which are well-correlated with (T, T ). [sent-537, score-0.562]
</p><p>95 Viewed from this perspective, our work may be seen as a spectral analogue of these ﬂow-based techniques, since Theorem 7 provides lower bounds on the conductance of other cuts as a function of how well-correlated they are with the seed vector. [sent-538, score-0.877]
</p><p>96 Recall that LocalSpectral may be interpreted as augmenting the standard spectral optimization program with a constraint that the output cut be well-correlated with the input seed set. [sent-541, score-0.628]
</p><p>97 ¯ Depending on the value of β, the latter terms clearly discourage cuts that substantially cut into T or ¯ ¯ T , thus encouraging partitions that are well-correlated with the input cut (T, T ). [sent-556, score-0.749]
</p><p>98 5 Bounding the Size of the Output Cut Readers familiar with the spectral method may recall that given a graph with a small balanced cut, it is not possible, in general, to guarantee that the sweep cut procedure of Theorem 4 applied to the optimal of Spectral outputs a balanced cut. [sent-558, score-0.512]
</p><p>99 Our setting, building up on the spectral method, also suffers from this; we cannot hope, in general, to bound the size of the output cut (which is a sweep cut) in terms of the correlation parameter κ. [sent-560, score-0.46]
</p><p>100 This was the reason for considering volume-constrained sweep cuts in our empirical evaluation. [sent-561, score-0.376]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('qq', 0.421), ('localspectral', 0.321), ('dg', 0.314), ('cuts', 0.281), ('seed', 0.259), ('conductance', 0.235), ('cut', 0.234), ('lg', 0.202), ('lang', 0.194), ('leskovec', 0.166), ('lkn', 0.139), ('pagerank', 0.128), ('mahoney', 0.125), ('teleportation', 0.123), ('vol', 0.11), ('localcut', 0.107), ('spectral', 0.102), ('st', 0.095), ('sweep', 0.095), ('qqq', 0.086), ('rv', 0.082), ('graph', 0.081), ('andersen', 0.073), ('eigenvector', 0.072), ('ishnoi', 0.07), ('recchia', 0.07), ('sdp', 0.07), ('personalized', 0.064), ('ahoney', 0.059), ('vertex', 0.058), ('pectral', 0.054), ('raphs', 0.054), ('ocal', 0.049), ('nodes', 0.048), ('utput', 0.048), ('def', 0.046), ('ethod', 0.046), ('social', 0.044), ('laplacian', 0.041), ('ori', 0.041), ('slackness', 0.041), ('partitioning', 0.039), ('chung', 0.039), ('dasgupta', 0.038), ('cheeger', 0.037), ('lkt', 0.037), ('vishnoi', 0.037), ('clusters', 0.035), ('program', 0.033), ('orecchia', 0.032), ('sdpd', 0.032), ('spielman', 0.032), ('xt', 0.032), ('node', 0.031), ('sh', 0.03), ('correlation', 0.029), ('vertices', 0.029), ('global', 0.028), ('maji', 0.027), ('pro', 0.026), ('relaxation', 0.025), ('aj', 0.025), ('vt', 0.024), ('displayed', 0.023), ('teng', 0.023), ('malik', 0.023), ('communities', 0.022), ('duality', 0.022), ('varying', 0.022), ('graphs', 0.022), ('coauthorship', 0.021), ('eoreti', 0.021), ('olum', 0.021), ('oun', 0.021), ('uctan', 0.021), ('eigenvectors', 0.021), ('volumes', 0.021), ('solution', 0.021), ('darker', 0.021), ('region', 0.021), ('le', 0.021), ('network', 0.02), ('complementary', 0.02), ('walk', 0.02), ('local', 0.019), ('feasibility', 0.019), ('nearby', 0.019), ('web', 0.019), ('peripheral', 0.018), ('sst', 0.018), ('community', 0.018), ('segments', 0.018), ('primitive', 0.018), ('rounding', 0.018), ('volume', 0.018), ('segmentation', 0.017), ('ow', 0.017), ('prespeci', 0.016), ('www', 0.016), ('partition', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="5-tfidf-1" href="./jmlr-2012-A_Local_Spectral_Method_for_Graphs%3A_With_Applications_to_Improving_Graph_Partitions_and_Exploring_Data_Graphs_Locally.html">5 jmlr-2012-A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally</a></p>
<p>Author: Michael W. Mahoney, Lorenzo Orecchia, Nisheeth K. Vishnoi</p><p>Abstract: The second eigenvalue of the Laplacian matrix and its associated eigenvector are fundamental features of an undirected graph, and as such they have found widespread use in scientiﬁc computing, machine learning, and data analysis. In many applications, however, graphs that arise have several local regions of interest, and the second eigenvector will typically fail to provide information ﬁne-tuned to each local region. In this paper, we introduce a locally-biased analogue of the second eigenvector, and we demonstrate its usefulness at highlighting local properties of data graphs in a semi-supervised manner. To do so, we ﬁrst view the second eigenvector as the solution to a constrained optimization problem, and we incorporate the local information as an additional constraint; we then characterize the optimal solution to this new problem and show that it can be interpreted as a generalization of a Personalized PageRank vector; and ﬁnally, as a consequence, we show that the solution can be computed in nearly-linear time. In addition, we show that this locally-biased vector can be used to compute an approximation to the best partition near an input seed set in a manner analogous to the way in which the second eigenvector of the Laplacian can be used to obtain an approximation to the best partition in the entire input graph. Such a primitive is useful for identifying and reﬁning clusters locally, as it allows us to focus on a local region of interest in a semi-supervised manner. Finally, we provide a detailed empirical evaluation of our method by showing how it can applied to ﬁnding locally-biased sparse cuts around an input vertex seed set in social and information networks. Keywords: spectral graph partitioning, local spectral algorithms, Laplacian matrix, semi-supervised learning, personalized pagerank c 2012 Michael W. Mahoney and Lorenzo Orecchia and Nisheeth K. Vishnoi. M AHONEY, O RECCHIA , AND V ISHNOI</p><p>2 0.064557225 <a title="5-tfidf-2" href="./jmlr-2012-Linear_Fitted-Q_Iteration_with_Multiple_Reward_Functions.html">58 jmlr-2012-Linear Fitted-Q Iteration with Multiple Reward Functions</a></p>
<p>Author: Daniel J. Lizotte, Michael Bowling, Susan A. Murphy</p><p>Abstract: We present a general and detailed development of an algorithm for ﬁnite-horizon ﬁtted-Q iteration with an arbitrary number of reward signals and linear value function approximation using an arbitrary number of state features. This includes a detailed treatment of the 3-reward function case using triangulation primitives from computational geometry and a method for identifying globally dominated actions. We also present an example of how our methods can be used to construct a realworld decision aid by considering symptom reduction, weight gain, and quality of life in sequential treatments for schizophrenia. Finally, we discuss future directions in which to take this work that will further enable our methods to make a positive impact on the ﬁeld of evidence-based clinical decision support. Keywords: reinforcement learning, dynamic programming, decision making, linear regression, preference elicitation</p><p>3 0.052621707 <a title="5-tfidf-3" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>Author: Gérard Biau</p><p>Abstract: Random forests are a scheme proposed by Leo Breiman in the 2000’s for building a predictor ensemble with a set of decision trees that grow in randomly selected subspaces of data. Despite growing interest and practical use, there has been little exploration of the statistical properties of random forests, and little is known about the mathematical forces driving the algorithm. In this paper, we offer an in-depth analysis of a random forests model suggested by Breiman (2004), which is very close to the original algorithm. We show in particular that the procedure is consistent and adapts to sparsity, in the sense that its rate of convergence depends only on the number of strong features and not on how many noise variables are present. Keywords: random forests, randomization, sparsity, dimension reduction, consistency, rate of convergence</p><p>4 0.050029039 <a title="5-tfidf-4" href="./jmlr-2012-Online_Submodular_Minimization.html">84 jmlr-2012-Online Submodular Minimization</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We consider an online decision problem over a discrete space in which the loss function is submodular. We give algorithms which are computationally efﬁcient and are Hannan-consistent in both the full information and partial feedback settings. Keywords: submodular optimization, online learning, regret minimization</p><p>5 0.045349535 <a title="5-tfidf-5" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>Author: Mehrdad Mahdavi, Rong Jin, Tianbao Yang</p><p>Abstract: In this paper we propose efﬁcient algorithms for solving constrained online convex optimization problems. Our motivation stems from the observation that most algorithms proposed for online convex optimization require a projection onto the convex set K from which the decisions are made. While the projection is straightforward for simple shapes (e.g., Euclidean ball), for arbitrary complex sets it is the main computational challenge and may be inefﬁcient in practice. In this paper, we consider an alternative online convex optimization problem. Instead of requiring that decisions belong to K for all rounds, we only require that the constraints, which deﬁne the set K , be satisﬁed in the long run. By turning the problem into an online convex-concave optimization problem, √ we propose an efﬁcient algorithm which achieves O( T ) regret bound and O(T 3/4 ) bound on the violation of constraints. Then, we modify the algorithm in order to guarantee that the constraints are satisﬁed in the long run. This gain is achieved at the price of getting O(T 3/4 ) regret bound. Our second algorithm is based on the mirror prox method (Nemirovski, 2005) to solve variational inequalities which achieves O(T 2/3 ) bound for both regret and the violation of constraints when the domain K can be described by a ﬁnite number of linear constraints. Finally, we extend the results to the setting where we only have partial access to the convex set K and propose a multipoint bandit feedback algorithm with the same bounds in expectation as our ﬁrst algorithm. Keywords: online convex optimization, convex-concave optimization, bandit feedback, variational inequality</p><p>6 0.045106031 <a title="5-tfidf-6" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>7 0.044950988 <a title="5-tfidf-7" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>8 0.0447709 <a title="5-tfidf-8" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>9 0.043820262 <a title="5-tfidf-9" href="./jmlr-2012-Fast_Approximation_of_Matrix_Coherence_and_Statistical_Leverage.html">43 jmlr-2012-Fast Approximation of Matrix Coherence and Statistical Leverage</a></p>
<p>10 0.04257137 <a title="5-tfidf-10" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>11 0.03740279 <a title="5-tfidf-11" href="./jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</a></p>
<p>12 0.030192802 <a title="5-tfidf-12" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>13 0.027735306 <a title="5-tfidf-13" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>14 0.026539559 <a title="5-tfidf-14" href="./jmlr-2012-Characterization_and_Greedy_Learning_of_Interventional_Markov_Equivalence_Classes_of_Directed_Acyclic_Graphs.html">25 jmlr-2012-Characterization and Greedy Learning of Interventional Markov Equivalence Classes of Directed Acyclic Graphs</a></p>
<p>15 0.02624711 <a title="5-tfidf-15" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>16 0.024278048 <a title="5-tfidf-16" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>17 0.023664428 <a title="5-tfidf-17" href="./jmlr-2012-Feature_Selection_via_Dependence_Maximization.html">44 jmlr-2012-Feature Selection via Dependence Maximization</a></p>
<p>18 0.023487175 <a title="5-tfidf-18" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>19 0.023141315 <a title="5-tfidf-19" href="./jmlr-2012-Minimax_Manifold_Estimation.html">68 jmlr-2012-Minimax Manifold Estimation</a></p>
<p>20 0.02202075 <a title="5-tfidf-20" href="./jmlr-2012-A_Geometric_Approach_to_Sample_Compression.html">3 jmlr-2012-A Geometric Approach to Sample Compression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.112), (1, -0.042), (2, 0.047), (3, -0.076), (4, -0.032), (5, 0.052), (6, -0.091), (7, 0.026), (8, 0.005), (9, -0.071), (10, -0.005), (11, -0.038), (12, -0.041), (13, -0.025), (14, -0.087), (15, 0.022), (16, 0.001), (17, -0.034), (18, 0.112), (19, -0.067), (20, 0.064), (21, -0.124), (22, -0.329), (23, -0.022), (24, 0.065), (25, 0.068), (26, 0.044), (27, -0.084), (28, -0.023), (29, -0.18), (30, 0.025), (31, 0.058), (32, 0.021), (33, -0.122), (34, -0.006), (35, -0.126), (36, 0.027), (37, -0.027), (38, -0.037), (39, -0.072), (40, -0.078), (41, -0.202), (42, -0.029), (43, 0.159), (44, -0.163), (45, -0.032), (46, 0.011), (47, -0.294), (48, -0.164), (49, -0.228)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96375096 <a title="5-lsi-1" href="./jmlr-2012-A_Local_Spectral_Method_for_Graphs%3A_With_Applications_to_Improving_Graph_Partitions_and_Exploring_Data_Graphs_Locally.html">5 jmlr-2012-A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally</a></p>
<p>Author: Michael W. Mahoney, Lorenzo Orecchia, Nisheeth K. Vishnoi</p><p>Abstract: The second eigenvalue of the Laplacian matrix and its associated eigenvector are fundamental features of an undirected graph, and as such they have found widespread use in scientiﬁc computing, machine learning, and data analysis. In many applications, however, graphs that arise have several local regions of interest, and the second eigenvector will typically fail to provide information ﬁne-tuned to each local region. In this paper, we introduce a locally-biased analogue of the second eigenvector, and we demonstrate its usefulness at highlighting local properties of data graphs in a semi-supervised manner. To do so, we ﬁrst view the second eigenvector as the solution to a constrained optimization problem, and we incorporate the local information as an additional constraint; we then characterize the optimal solution to this new problem and show that it can be interpreted as a generalization of a Personalized PageRank vector; and ﬁnally, as a consequence, we show that the solution can be computed in nearly-linear time. In addition, we show that this locally-biased vector can be used to compute an approximation to the best partition near an input seed set in a manner analogous to the way in which the second eigenvector of the Laplacian can be used to obtain an approximation to the best partition in the entire input graph. Such a primitive is useful for identifying and reﬁning clusters locally, as it allows us to focus on a local region of interest in a semi-supervised manner. Finally, we provide a detailed empirical evaluation of our method by showing how it can applied to ﬁnding locally-biased sparse cuts around an input vertex seed set in social and information networks. Keywords: spectral graph partitioning, local spectral algorithms, Laplacian matrix, semi-supervised learning, personalized pagerank c 2012 Michael W. Mahoney and Lorenzo Orecchia and Nisheeth K. Vishnoi. M AHONEY, O RECCHIA , AND V ISHNOI</p><p>2 0.3862417 <a title="5-lsi-2" href="./jmlr-2012-Linear_Fitted-Q_Iteration_with_Multiple_Reward_Functions.html">58 jmlr-2012-Linear Fitted-Q Iteration with Multiple Reward Functions</a></p>
<p>Author: Daniel J. Lizotte, Michael Bowling, Susan A. Murphy</p><p>Abstract: We present a general and detailed development of an algorithm for ﬁnite-horizon ﬁtted-Q iteration with an arbitrary number of reward signals and linear value function approximation using an arbitrary number of state features. This includes a detailed treatment of the 3-reward function case using triangulation primitives from computational geometry and a method for identifying globally dominated actions. We also present an example of how our methods can be used to construct a realworld decision aid by considering symptom reduction, weight gain, and quality of life in sequential treatments for schizophrenia. Finally, we discuss future directions in which to take this work that will further enable our methods to make a positive impact on the ﬁeld of evidence-based clinical decision support. Keywords: reinforcement learning, dynamic programming, decision making, linear regression, preference elicitation</p><p>3 0.36262593 <a title="5-lsi-3" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random ﬁelds (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter ﬁtting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random ﬁeld via the graphical lasso.</p><p>4 0.3121081 <a title="5-lsi-4" href="./jmlr-2012-Mixability_is_Bayes_Risk_Curvature_Relative_to_Log_Loss.html">69 jmlr-2012-Mixability is Bayes Risk Curvature Relative to Log Loss</a></p>
<p>Author: Tim van Erven, Mark D. Reid, Robert C. Williamson</p><p>Abstract: Mixability of a loss characterizes fast rates in the online learning setting of prediction with expert advice. The determination of the mixability constant for binary losses is straightforward but opaque. In the binary case we make this transparent and simpler by characterising mixability in terms of the second derivative of the Bayes risk of proper losses. We then extend this result to multiclass proper losses where there are few existing results. We show that mixability is governed by the maximum eigenvalue of the Hessian of the Bayes risk, relative to the Hessian of the Bayes risk for log loss. We conclude by comparing our result to other work that bounds prediction performance in terms of the geometry of the Bayes risk. Although all calculations are for proper losses, we also show how to carry the results across to improper losses. Keywords: mixability, multiclass, prediction with expert advice, proper loss, learning rates</p><p>5 0.27956513 <a title="5-lsi-5" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>Author: Gérard Biau</p><p>Abstract: Random forests are a scheme proposed by Leo Breiman in the 2000’s for building a predictor ensemble with a set of decision trees that grow in randomly selected subspaces of data. Despite growing interest and practical use, there has been little exploration of the statistical properties of random forests, and little is known about the mathematical forces driving the algorithm. In this paper, we offer an in-depth analysis of a random forests model suggested by Breiman (2004), which is very close to the original algorithm. We show in particular that the procedure is consistent and adapts to sparsity, in the sense that its rate of convergence depends only on the number of strong features and not on how many noise variables are present. Keywords: random forests, randomization, sparsity, dimension reduction, consistency, rate of convergence</p><p>6 0.23387872 <a title="5-lsi-6" href="./jmlr-2012-Fast_Approximation_of_Matrix_Coherence_and_Statistical_Leverage.html">43 jmlr-2012-Fast Approximation of Matrix Coherence and Statistical Leverage</a></p>
<p>7 0.21639492 <a title="5-lsi-7" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>8 0.21603489 <a title="5-lsi-8" href="./jmlr-2012-A_Geometric_Approach_to_Sample_Compression.html">3 jmlr-2012-A Geometric Approach to Sample Compression</a></p>
<p>9 0.21014932 <a title="5-lsi-9" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>10 0.20767181 <a title="5-lsi-10" href="./jmlr-2012-Online_Submodular_Minimization.html">84 jmlr-2012-Online Submodular Minimization</a></p>
<p>11 0.17442873 <a title="5-lsi-11" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>12 0.17026992 <a title="5-lsi-12" href="./jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</a></p>
<p>13 0.16934186 <a title="5-lsi-13" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>14 0.1631642 <a title="5-lsi-14" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>15 0.16117001 <a title="5-lsi-15" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>16 0.1341607 <a title="5-lsi-16" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>17 0.13339515 <a title="5-lsi-17" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>18 0.13073789 <a title="5-lsi-18" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>19 0.13035762 <a title="5-lsi-19" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>20 0.12593369 <a title="5-lsi-20" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.013), (15, 0.439), (21, 0.022), (26, 0.042), (27, 0.013), (29, 0.024), (35, 0.02), (49, 0.019), (56, 0.04), (64, 0.028), (69, 0.023), (75, 0.038), (79, 0.032), (92, 0.06), (96, 0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73830116 <a title="5-lda-1" href="./jmlr-2012-A_Local_Spectral_Method_for_Graphs%3A_With_Applications_to_Improving_Graph_Partitions_and_Exploring_Data_Graphs_Locally.html">5 jmlr-2012-A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally</a></p>
<p>Author: Michael W. Mahoney, Lorenzo Orecchia, Nisheeth K. Vishnoi</p><p>Abstract: The second eigenvalue of the Laplacian matrix and its associated eigenvector are fundamental features of an undirected graph, and as such they have found widespread use in scientiﬁc computing, machine learning, and data analysis. In many applications, however, graphs that arise have several local regions of interest, and the second eigenvector will typically fail to provide information ﬁne-tuned to each local region. In this paper, we introduce a locally-biased analogue of the second eigenvector, and we demonstrate its usefulness at highlighting local properties of data graphs in a semi-supervised manner. To do so, we ﬁrst view the second eigenvector as the solution to a constrained optimization problem, and we incorporate the local information as an additional constraint; we then characterize the optimal solution to this new problem and show that it can be interpreted as a generalization of a Personalized PageRank vector; and ﬁnally, as a consequence, we show that the solution can be computed in nearly-linear time. In addition, we show that this locally-biased vector can be used to compute an approximation to the best partition near an input seed set in a manner analogous to the way in which the second eigenvector of the Laplacian can be used to obtain an approximation to the best partition in the entire input graph. Such a primitive is useful for identifying and reﬁning clusters locally, as it allows us to focus on a local region of interest in a semi-supervised manner. Finally, we provide a detailed empirical evaluation of our method by showing how it can applied to ﬁnding locally-biased sparse cuts around an input vertex seed set in social and information networks. Keywords: spectral graph partitioning, local spectral algorithms, Laplacian matrix, semi-supervised learning, personalized pagerank c 2012 Michael W. Mahoney and Lorenzo Orecchia and Nisheeth K. Vishnoi. M AHONEY, O RECCHIA , AND V ISHNOI</p><p>2 0.26353121 <a title="5-lda-2" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>Author: Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, Lin Xiao</p><p>Abstract: Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment. We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem. Keywords: distributed computing, online learning, stochastic optimization, regret bounds, convex optimization</p><p>3 0.26184976 <a title="5-lda-3" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel</p><p>Abstract: The success of many machine learning and pattern recognition methods relies heavily upon the identiﬁcation of an appropriate distance metric on the input data. It is often beneﬁcial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semideﬁnite. Semideﬁnite programming is often used to enforce this constraint, but does not scale well and is not easy to implement. B OOST M ETRIC is instead based on the observation that any positive semideﬁnite matrix can be decomposed into a linear combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting methods are easy to implement, efﬁcient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semideﬁnite matrix with trace and rank being one rather than a classiﬁer or regressor. Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. Keywords: Mahalanobis distance, semideﬁnite programming, column generation, boosting, Lagrange duality, large margin nearest neighbor</p><p>4 0.26096207 <a title="5-lda-4" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>Author: Sanjiv Kumar, Mehryar Mohri, Ameet Talwalkar</p><p>Abstract: The Nystr¨ m method is an efﬁcient technique to generate low-rank matrix approximations and is o used in several large-scale learning applications. A key aspect of this method is the procedure according to which columns are sampled from the original matrix. In this work, we explore the efﬁcacy of a variety of ﬁxed and adaptive sampling schemes. We also propose a family of ensemble-based sampling algorithms for the Nystr¨ m method. We report results of extensive experiments o that provide a detailed comparison of various ﬁxed and adaptive sampling techniques, and demonstrate the performance improvement associated with the ensemble Nystr¨ m method when used in o conjunction with either ﬁxed or adaptive sampling schemes. Corroborating these empirical ﬁndings, we present a theoretical analysis of the Nystr¨ m method, providing novel error bounds guaro anteeing a better convergence rate of the ensemble Nystr¨ m method in comparison to the standard o Nystr¨ m method. o Keywords: low-rank approximation, nystr¨ m method, ensemble methods, large-scale learning o</p><p>5 0.25989339 <a title="5-lda-5" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random ﬁelds (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter ﬁtting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random ﬁeld via the graphical lasso.</p><p>6 0.25923306 <a title="5-lda-6" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>7 0.25765604 <a title="5-lda-7" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>8 0.25750893 <a title="5-lda-8" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>9 0.25724506 <a title="5-lda-9" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>10 0.25541037 <a title="5-lda-10" href="./jmlr-2012-Fast_Approximation_of_Matrix_Coherence_and_Statistical_Leverage.html">43 jmlr-2012-Fast Approximation of Matrix Coherence and Statistical Leverage</a></p>
<p>11 0.2551443 <a title="5-lda-11" href="./jmlr-2012-Query_Strategies_for_Evading_Convex-Inducing_Classifiers.html">94 jmlr-2012-Query Strategies for Evading Convex-Inducing Classifiers</a></p>
<p>12 0.25165507 <a title="5-lda-12" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>13 0.25136971 <a title="5-lda-13" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>14 0.25134563 <a title="5-lda-14" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>15 0.25057521 <a title="5-lda-15" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>16 0.25042152 <a title="5-lda-16" href="./jmlr-2012-An_Active_Learning_Algorithm_for_Ranking_from_Pairwise_Preferences_with_an_Almost_Optimal_Query_Complexity.html">17 jmlr-2012-An Active Learning Algorithm for Ranking from Pairwise Preferences with an Almost Optimal Query Complexity</a></p>
<p>17 0.25024804 <a title="5-lda-17" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>18 0.25005794 <a title="5-lda-18" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>19 0.25003806 <a title="5-lda-19" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>20 0.24915779 <a title="5-lda-20" href="./jmlr-2012-Linear_Fitted-Q_Iteration_with_Multiple_Reward_Functions.html">58 jmlr-2012-Linear Fitted-Q Iteration with Multiple Reward Functions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
