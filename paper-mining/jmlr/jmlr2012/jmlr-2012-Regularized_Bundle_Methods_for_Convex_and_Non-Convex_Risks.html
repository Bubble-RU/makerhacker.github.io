<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-98" href="#">jmlr2012-98</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</h1>
<br/><p>Source: <a title="jmlr-2012-98-pdf" href="http://jmlr.org/papers/volume13/do12a/do12a.pdf">pdf</a></p><p>Author: Trinh Minh Tri Do, Thierry Artières</p><p>Abstract: Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efﬁcient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efﬁcient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random ﬁelds, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy ε with a rate O(1/λε) where λ is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difﬁcult and requires a stronger and more disputable assumption. Yet we provide experimental results on artiﬁcial test problems, and on ﬁve standard and difﬁcult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms. Keywords: optimization, non-convex, non-smooth, cutting plane, bundle method, regularized risk</p><p>Reference: <a title="jmlr-2012-98-reference" href="../jmlr2012_reference/jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. [sent-12, score-0.69]
</p><p>2 First, in Section 2 we provide background on the cutting plane technique and on bundle methods, and we describe two main existing extensions, the convex regularized bundle method (CRBM) and the non-convex bundle method (NBM). [sent-100, score-1.347]
</p><p>3 Background on Cutting Plane and Bundle Methods We provide now some background on the cutting plane principle and on optimization methods that have been built on this idea for convex and non-convex objective functions. [sent-108, score-0.772]
</p><p>4 For a given function f (w), a cutting plane cw′ (w) is a ﬁrst-order Taylor approximation computed at a particular point w′ : f (w) ≈ cw′ (w) = f (w′ ) + aw′ , w − w′ where aw′ ∈ ∂ f (w′ ) is a subgradient of f at w′ . [sent-111, score-0.68]
</p><p>5 Importantly, a cutting plane of a convex function f is an underestimator of f . [sent-120, score-0.832]
</p><p>6 3542  R EGULARIZED B UNDLE M ETHODS FOR C ONVEX AND N ON -C ONVEX R ISKS  Figure 1: Basic approximation of a function f by a (underestimator) cutting plane at a point w′ (left), and a more accurate approximation by taking the maximum over many cutting planes of f (right). [sent-121, score-1.172]
</p><p>7 In the case of a convex objective, any cutting plane of the objective f is an underestimator of f . [sent-124, score-0.896]
</p><p>8 The idea of the cutting plane method is that one can build an accurate approximation function (named gt hereafter) of f , which is also an underestimator of f , as the maximum over many cutting plane approximation built at different points {w1 , . [sent-125, score-1.663]
</p><p>9 , wt } as follows: f (w) ≈ gt (w) = max aw j , w + bw j . [sent-128, score-0.685]
</p><p>10 The cutting plane method aims at iteratively building an increasingly accurate piecewise linear underestimator of the objective function by successively adding new cutting planes to the approximation g of f . [sent-133, score-1.37]
</p><p>11 Every iteration, one adds a new cutting plane underestimator built at current solution, yielding a new piece-wise linear underestimator of f as in Equation 3. [sent-135, score-0.996]
</p><p>12 t f (w j )] − gt (wt+1 ) 7: if gap < ε then return wt 8: end for Algorithm 2 Convex Regularized Bundle Method (CRBM) 1: Input: w1 , R, ε 2: Output: w∗ 3: for t = 1 to ∞ do 4: Compute awt and bwt of R at wt 5: wt∗ = argminw∈{w1 ,. [sent-146, score-1.095]
</p><p>13 wt } f (w) ˜ 6: wt ← argminw gt (w) where gt (w) is deﬁned as in Equation 6 ˜ 7: gapt = f (wt∗ ) − gt (wt ) ˜ 8: wt+1 = wt 9: if gapt < ε then return wt∗ 10: end for term. [sent-149, score-2.076]
</p><p>14 The approximation function becomes: f (w) ≈ gt (w) = (w − wt )⊤ Ht (w − wt ) + max aw j , w + bw j j=1. [sent-150, score-1.164]
</p><p>15 Note that this quadratic approximation of f (w) is more accurate than a cutting plane approximation on f (w). [sent-162, score-0.716]
</p><p>16 Equation 5)  CRBM is very similar to the cutting plane technique described before, where every iteration a new cutting plane approximation is built (at the current solution) and added to the current approximation function. [sent-165, score-1.384]
</p><p>17 t  (6)  and the approximation problem is ˜ wt = argmin gt (w) = argmin w  w  λ w 2  2  + max aw j , w + bw j j=1. [sent-168, score-0.73]
</p><p>18 t  (7)  where aw j , w + bw j is the approximation cutting plane of R built at w j , the solution at iteration j. [sent-170, score-0.868]
</p><p>19 Importantly, if R(w) is convex then any cutting plane aw j , w + bw j is an underestimator of R(w), and its maximum, max j=1. [sent-171, score-0.935]
</p><p>20 Let αt be the solution of the above dual problem at iteration t, the solution of the primal problem is given by: t ˜ wt = − αλAt , 1 ˜ gt (wt ) = − 2λ αt At  2 +α B . [sent-209, score-0.738]
</p><p>21 Basically NBM works similarly as standard bundle methods by building iteratively an approximation function via the cutting plane technique. [sent-222, score-0.875]
</p><p>22 The approximation problem is an instance of quadratic programming similar to the one in Equation 4, except that the raw cutting planes are adjusted to make sure that the approximation is a local underestimator of the objective function. [sent-228, score-0.844]
</p><p>23 The set of cutting planes is expanded with the new cutting plane built at wt+1 . [sent-250, score-1.112]
</p><p>24 Importantly, note that one gets more cutting planes in the bundle as the algorithm iterates, and such a ever increasing number of cutting planes may represent a potential problem wrt. [sent-252, score-1.17]
</p><p>25 Usually to overcome such a problem, one uses an aggregated cutting plane in order to accumulate information of all cutting planes in previous iterations (Kiwiel, 1985). [sent-254, score-1.17]
</p><p>26 For instance, one may keep a ﬁxed number of cutting planes in the bundle Bt by removing the oldest cutting plane. [sent-256, score-1.073]
</p><p>27 Then, the aggregated cutting plane allows preserving part of the information brought by removed cutting planes. [sent-257, score-1.057]
</p><p>28 Then, the standard approximation function, which is deﬁned as the maximum over a set of cutting plane approximations, is not an underestimator of the non-convex objective function anymore. [sent-261, score-0.896]
</p><p>29 In addition although one may reasonably assume that a cutting plane built at a point w′ is an accurate approximation of f in a small region around w′ , such an approximation may become very poor for w far from w′ . [sent-262, score-0.728]
</p><p>30 3547  ` D O AND A RTI E RES  ✁   ✁   ✁  ✞✝✆☎✄✂ ✁   (a) Conﬂict  (b) Bad approximation  (c) Adjusting cutting plane  Figure 3: Cutting planes and linearization errors. [sent-272, score-0.786]
</p><p>31 The condition (9) ensures that if the linearization error, f (w′′ ) − cw′ (w′′ ), is negative then the cutting plane has to be lowered at least twice the amount that is required to have linearization error zero. [sent-274, score-0.696]
</p><p>32 In other words, in the case of negative linearization error at w′′ , the cutting plane is adjusted so that the new linearization error is positive, with at least the same magnitude as the “old” negative linearization error. [sent-275, score-0.735]
</p><p>33 Every iteration a new cutting plane is added to the bundle so that the size of the bundle at iteration t is t. [sent-283, score-1.148]
</p><p>34 The extension of CRBM for non-convex function is not straightforward since, as we already observed when presenting NBM, the cutting plane approximation does not yield an underestimator of the objective function. [sent-293, score-0.896]
</p><p>35 On one hand, we use standard techniques such as the introduction of locality measure and the adjustment of cutting planes in order to build local underestimator of the function at a given point. [sent-295, score-0.744]
</p><p>36 Actually, the dual program of the approximation problem minimization in Equation 8 has a memory cost of O(tD + t 2 ) for storing all the cutting planes and the dot product matrix between cutting planes’ normal vectors (i. [sent-302, score-0.948]
</p><p>37 It is based on the use of a cutting plane aggregation method which allows drastically limiting the number of CPs in the working set at the price of a less accurate underestimator approximation. [sent-312, score-0.821]
</p><p>38 wt } f (w) 8: Jt ← UpdateWorkingSet(Jt−1 ,t, M) ˜ ˜ 9: [wt , ct ] ← Minimize gt (w) in Equation 11 ˜ 10: gapt = f (wt∗ ) − gt (wt ) 11: if gapt < ε then return wt∗ 12: end for 3. [sent-325, score-1.133]
</p><p>39 1 Limited Memory for Convex Case Our goal here is to limit the number of cutting planes used in the approximation function, which can be done by removing some of the previous cutting planes if the number of cutting planes reaches a given limit. [sent-326, score-1.49]
</p><p>40 Our proposal is to apply a similar technique to the set of cutting planes approximation of the risk function R, yielding an aggregated cutting plane. [sent-329, score-0.968]
</p><p>41 4 Interestingly, we can show that if such an aggregated cutting plane is included in the approximation function, then one can remove any (or even all) previous cutting plane(s) while preserving the theoretical convergence rate O(1/λε) iterations of CRBM. [sent-330, score-1.142]
</p><p>42 ,t} stands for a working set of active cutting plane indexes that we keep at iteration t ˜ ˜ and ct−1 (w) = at−1 , w + bt−1 is the aggregated cutting plane which accumulates information from ˜ previous cutting planes, c1 , . [sent-333, score-1.713]
</p><p>43 At iteration t, a new cutting plane is added to the current set of cutting planes Jt−1 , but if Jt−1 is full (i. [sent-342, score-1.13]
</p><p>44 3550  R EGULARIZED B UNDLE M ETHODS FOR C ONVEX AND N ON -C ONVEX R ISKS  Figure 4: Quadratic underestimator of gt (w) (solid line) and corresponding aggregated cutting plane ct (w) (dash line)). [sent-348, score-1.1]
</p><p>45 The use of an aggregated cutting plane is a key issue to limit storage requirements and computational effort per iteration. [sent-353, score-0.68]
</p><p>46 At iteration t of Algorithm 3, the cutting plane aggregation ct (w) is derived from the mini˜ mization of gt (w). [sent-364, score-0.931]
</p><p>47 We use the cutting plane technique to build an underestimator of gt (w) at its ˜ minimum wt = argminw gt (w). [sent-365, score-1.56]
</p><p>48 3551  ` D O AND A RTI E RES  Figure 4 illustrates the quadratic function (in red dash line) derived from the aggregated cutting plane at iteration t = 2. [sent-368, score-0.746]
</p><p>49 The cutting plane ct (w) can be deﬁned based on the dual solution of the ˜ approximation problem which may be characterized in primal and dual forms as follows:  minw s. [sent-369, score-0.838]
</p><p>50 ˜ ˜ Proof First, by construction we have wt = − at which implies that the derivative of λ ˜ ˜ ˜ is null at wt . [sent-388, score-0.89]
</p><p>51 Actually: ˜ ˜ 2 ˜ 1 ˜ ˜ = − λ at 2 + bt gt (wt ) = − 2λ αt At 2 + αt Bt 2 λ ˜ ˜ at 2 λ λ at 2 2 − a , at + b ˜t ˜t ˜t ˜ ˜ = 2 wt = 2 λ −λ λ +b λ λ ˜ ˜ ˜ ˜ = 2 wt 2 + at , wt + bt . [sent-390, score-1.742]
</p><p>52 λ 2  w  2 + c (w) ˜t  (12)  ˜ In other words, the quadratic function λ w 2 + ct (w) and the approximation function gt (w) reach 2 ˜ ˜ the same minimum value gt (w) at the same point wt . [sent-391, score-0.929]
</p><p>53 Let ˜ 2 ˜ ˜ ht (w) = max max a j , w + b j , at−1 , w + bt−1 j=∈Jt  be the piecewise linear approximation of R(w) at iteration t, we have: ˜ ˜ ˜ 0 ∈ ∂gt (wt ) ≡ λwt + ∂ht (wt ) ˜ ˜ ˜ since wt is the optimum solution of minimizing gt (w). [sent-393, score-0.729]
</p><p>54 Furthermore, since ˜ ˜ gt (wt ) = λ wt 2 + ht (wt ), Equation 12 gives: 2 ˜ ˜ ˜ ˜ ˜ at , wt + bt = ht (wt ). [sent-396, score-1.202]
</p><p>55 3552  R EGULARIZED B UNDLE M ETHODS FOR C ONVEX AND N ON -C ONVEX R ISKS  ˜ The cutting plane ct (w) is then an underestimator of ht (w) built at wt (recall that ht (w) is convex), ˜ and thus λ w 2 + ct (w) is a quadratic underestimator of gt (w) = λ w 2 + ht (w). [sent-397, score-1.842]
</p><p>56 Note that since ˜ 2 2 λ w 2 + ct (w) is an underestimator of gt (w) and gt (w) is an underestimator of f (w) at wt∗ , the ˜ 2 ˜ quadratic function λ w 2 + ct (w) is also an underestimator of f (w) at wt∗ . [sent-398, score-1.037]
</p><p>57 We have to distinguish between a raw linear cutting plane of the risk cw j (with cw j (w) = aw j , w + bw j ) that is built at a particular iteration j of the algorithm and the eventually modiﬁed versions of this cutting plane that might be used in posterior iterations. [sent-407, score-1.581]
</p><p>58 At iteration t we note ctj (with ctj (w) = a j , w + btj ) the cutting plane which is derived from cw j , the raw CP originally built at iteration j. [sent-409, score-0.938]
</p><p>59 Similarly to non-convex bundle methods, we deﬁne a locality measure which is associated to any active cutting plane. [sent-416, score-0.69]
</p><p>60 It is related to the locality measure between the cutting plane (actually the point where the cutting plane was built) and the best current observed solution. [sent-417, score-1.307]
</p><p>61 We note stj the locality measure between cutting plane ctj and the best observed solution up to iteration t, wt∗ . [sent-418, score-0.873]
</p><p>62 The full bundle information is: ˜t−1 ˜t−1 Bt = {ctj , stj } j∈Jt ∪ {ct , st } ˜t−1 where ct is an aggregated cutting plane and st is its locality measure to the best observed ˜t−1 ∗ . [sent-419, score-1.23]
</p><p>63 1, the aggregated CP ct ˜t−1 solution wt can be viewed as a convex combination of CPs in previous iterations. [sent-421, score-0.678]
</p><p>64 Note that at iteration t = 1, there is only one 1 cutting plane c1 and the aggregated cutting plane is also c1 : [c1 , s1 ] = [c1 , s1 ]. [sent-437, score-1.336]
</p><p>65 coincide) with their corresponding locality mesures to the best solution w1 (s1 ˜ 1 Iteration t Every iteration the algorithm determine a new bundle Bt , the best observed solution up to iteration t, wt∗ , and the new current (and temporary) solution wt . [sent-440, score-0.945]
</p><p>66 At iteration t > 1, few steps are successively performed: ˜ • Build a new cutting plane at wt−1 the minimizer of approximation function in previous iteration (gt−1 (w)). [sent-441, score-0.749]
</p><p>67 , wt−1 ), we use a 3554  R EGULARIZED B UNDLE M ETHODS FOR C ONVEX AND N ON -C ONVEX R ISKS  special aggregated cutting plane, ct for gathering information of previous cutting planes up ˜t−1 to iteration t − 1. [sent-451, score-1.064]
</p><p>68 Note that a side effect of this minimization is the deﬁnition of a new aggregated cutting plane and its locality measure to the best observed solutions. [sent-456, score-0.771]
</p><p>69 2 L OCALITY M EASURE AND C ONDITIONS  ON  CP S  Given a set of cutting plane approximation of R, one could build a local underestimator of f in the vicinity of w by descending CPs that yields non positive linearization error of f at w. [sent-465, score-0.868]
</p><p>70 We propose to deﬁne the locality measure between a cutting plane previously built at iteration j and the current best solution wt∗ based on the trajectory from w j to wt∗ . [sent-469, score-0.811]
</p><p>71 (15)  Unfortunately if a cutting plane is lowered too much, the minimum of the approximation function is not guaranteed to improve every iteration anymore. [sent-489, score-0.74]
</p><p>72 It concerns the new added cutting plane only and writes: λ wt 2 + at , wt + bt ≥ f (wt∗ ). [sent-493, score-1.622]
</p><p>73 In other words, we need t 2 to ensure that the approximation at wt using the new added cutting plane is greater or equal to the best observed function value. [sent-494, score-1.087]
</p><p>74 The condition can be seen as a lower bound on the modiﬁed offset: λ bt ≥ f (wt∗ ) − wt 2 − at , wt . [sent-496, score-1.014]
</p><p>75 It takes as input: • The bundle at previous iteration ∗ • The best observed solutions at previous iteration wt−1 ∗ • The best observed solutions at current iteration wt • The current solution wt and its corresponding raw cutting plane, cwt . [sent-500, score-1.701]
</p><p>76 The algorithm is designed so that at the end of iteration t, all (|Jt | + 1) cutting planes in the bundle (i. [sent-501, score-0.744]
</p><p>77 the |Jt | “normal” cutting planes and the aggregated cutting plane) satisfy condition in Equation 15 while the new added cutting plane ct also satisﬁes condition in Equation 16. [sent-503, score-1.624]
</p><p>78 In the case of a descent step, condition (16) is trivially satisﬁed for the new added cutting plane since ct ≡ cwt . [sent-512, score-0.757]
</p><p>79 A similar modiﬁcation may be applied to the aggregated cutting plane: ˜ t−1 ˜ ˜ bt = min[bt−1 , R(wt∗ ) − at−1 , wt∗ − st ] ˜t−1 t−1 ∗ where st = st−1 + λ wt∗ − wt−1 2 . [sent-515, score-0.691]
</p><p>80 the best solution at previous iteration, a conﬂict (if any) may only arise between the new cutting plane cwt and the best observed solution wt∗ . [sent-523, score-0.732]
</p><p>81 Algorithm 6 modiﬁes ct in such a way that it guarantees that the new cutting plane ct with t t parameters at and bt satisﬁes conditions (15) and (16). [sent-526, score-0.94]
</p><p>82 Indeed conditions (15) and (16) may be rewritten as: t bt t bt t  ≤ R(wt∗ ) − awt , wt∗ − st = U, t ≥ f (wt∗ ) − λ wt 2 − awt , wt = L 2  (17)  which deﬁne an upper bound U and a lower bound L for bt . [sent-528, score-1.418]
</p><p>83 The quadratic approximation corresponding to cutting plane cwt is plotted in orange, which is not a local underestimator of f (w) at wt∗ . [sent-533, score-0.906]
</p><p>84 This t 2 quadratic function is deﬁned so that it reaches its minimum at wt∗ and the linearization error of the cutting plane at , w + bt at wt∗ is λ wt − wt∗ 2 (see the orange quadratic curve in Figure 6 (bottomt 2 left)). [sent-542, score-1.306]
</p><p>85 The new cutting plane is deﬁned as: ct (w) t at bt t st t  = at , w + bt , t = −λwt∗ , = f (wt∗ ) − λ wt 2 = λ wt − wt∗ 2 . [sent-543, score-1.909]
</p><p>86 It also satisﬁes condition (15) as we show now: at , wt∗ + bt t  = at , wt∗ + f (wt∗ ) − λ wt 2 − at , wt 2 = R(wt∗ ) + at , wt∗ − wt + λ ( wt∗ 2 − wt 2 = R(wt∗ ) + at + λ (wt∗ + wt ), wt∗ − wt 2  where we used the deﬁnition of the objective function f (wt∗ ) = −λwt∗ for at (Cf. [sent-545, score-2.814]
</p><p>87 t  = R(wt∗ ) − λ wt∗ − wt 2 2 = R(wt∗ ) − st t = R(wt∗ ) − at , wt∗ − λ wt∗ − wt 2  and condition in Equation 15 is satisﬁed. [sent-547, score-0.916]
</p><p>88 3559  2)  2  Then, substituting  ` D O AND A RTI E RES  Figure 7: Quadratic underestimator of gt (w) derived from the aggregated cutting plane ct (w). [sent-548, score-1.1]
</p><p>89 Figure 7 illustrates the quadratic function (in orange) derived from the aggregated cutting plane at iteration t = 2. [sent-552, score-0.746]
</p><p>90 λ Hence the deﬁnition of the aggregated cutting plane follows: ˜ wt = −  ˜ at ˜ bt  = αt At , = αt Bt . [sent-571, score-1.26]
</p><p>91 The aggregated CP ct accumulates in˜t formation from many cutting planes built at different points so that one cannot immediately deﬁne a ˜t ˜t locality measure st between ct and the current best observed solution wt∗ . [sent-573, score-0.935]
</p><p>92 However, ct being a con˜t t as the corresponding convex combination vex combination of cutting planes, we chose to deﬁne st ˜ of locality measures associated to cutting planes: st = ˜t  ˜ ∑ α j stj + αs˜tt−1 . [sent-574, score-1.127]
</p><p>93 If the search direction is not a descent direction then the line search returns the best solution along the search direction (should be close to the current solution), which will be used to build a new cutting plane in the next iteration. [sent-597, score-0.696]
</p><p>94 Proof We focus on deriving an underestimator on the minimum value of gt (w) based solely on this aggregated cutting plane and on the new added cutting plane at iteration t. [sent-640, score-1.686]
</p><p>95 Note that this is possible since the aggregated cutting plane accumulates information about the approximation problem at previous iterations. [sent-642, score-0.725]
</p><p>96 Hence the linear factor may be rewritten as: 2  ˜ a ˜ l = at−1 − at ,˜ t−1 + bt − bt−1 λ λ ˜ ˜ = at , wt + bt − at−1 , wt − bt−1 λ 2 + a ,w +b − λ w 2 − a ˜ ˜ t−1 , wt − bt−1 = 2 wt t t t t 2 = f (wt ) − gt−1 (wt ). [sent-649, score-2.028]
</p><p>97 Proof We have gt (wt∗ ) = f (wt∗ ) since the approximation errors are zero at points where cutting plane ˜ were built. [sent-671, score-0.801]
</p><p>98 Recall that there is a NullStep2 at iteration t if and only if the raw cutting plane built at current solution wt is not compatible with the best observed solution wt∗ . [sent-700, score-1.188]
</p><p>99 ˜ Theorem 4 If gapt = 0 at iteration t of Algorithm 4, then wt∗ = wt and wt∗ is a stationary point of objective function f , i. [sent-714, score-0.726]
</p><p>100 We built on ideas from Convex Regularized Bundle Methods and on ideas from Non-Convex Bundle Methods, exploiting the regularization term of the objective and using a particular design of the aggregated cutting plane to build limited memory variants. [sent-1710, score-0.846]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wt', 0.434), ('cutting', 0.377), ('nbm', 0.307), ('nrbmls', 0.291), ('plane', 0.231), ('bundle', 0.222), ('nrbm', 0.219), ('underestimator', 0.179), ('gapt', 0.155), ('gt', 0.148), ('bt', 0.146), ('eval', 0.116), ('lbfgs', 0.116), ('onvex', 0.112), ('crbm', 0.109), ('planes', 0.097), ('ct', 0.093), ('cw', 0.092), ('undle', 0.092), ('locality', 0.091), ('isks', 0.088), ('rti', 0.088), ('jt', 0.085), ('sgd', 0.082), ('res', 0.08), ('egularized', 0.075), ('aggregated', 0.072), ('cp', 0.07), ('objective', 0.064), ('cwt', 0.056), ('bw', 0.055), ('ethods', 0.052), ('cccp', 0.051), ('st', 0.048), ('iteration', 0.048), ('stj', 0.048), ('aw', 0.048), ('gap', 0.047), ('convex', 0.045), ('approximation', 0.045), ('ctj', 0.044), ('ict', 0.044), ('cps', 0.037), ('crf', 0.037), ('linearization', 0.036), ('svmstruct', 0.034), ('solution', 0.034), ('ocr', 0.034), ('memory', 0.034), ('aggregation', 0.034), ('awt', 0.032), ('kiwiel', 0.032), ('risks', 0.032), ('icts', 0.031), ('built', 0.03), ('epochs', 0.028), ('neurocrf', 0.028), ('universvm', 0.028), ('regularized', 0.028), ('chained', 0.027), ('tsvm', 0.027), ('subgradient', 0.027), ('sg', 0.025), ('stationary', 0.025), ('transductive', 0.025), ('optimization', 0.025), ('deep', 0.025), ('yes', 0.025), ('convergence', 0.024), ('btj', 0.024), ('offset', 0.024), ('reaches', 0.023), ('minimum', 0.023), ('primal', 0.022), ('null', 0.022), ('ht', 0.02), ('cputime', 0.02), ('makela', 0.02), ('stoping', 0.02), ('argminw', 0.02), ('limited', 0.02), ('reach', 0.02), ('margin', 0.019), ('ad', 0.019), ('lagrange', 0.019), ('hidden', 0.019), ('optimizers', 0.019), ('adjusted', 0.019), ('linesearch', 0.018), ('dual', 0.018), ('quadratic', 0.018), ('search', 0.018), ('regularization', 0.018), ('optimizer', 0.017), ('variant', 0.017), ('dedicated', 0.017), ('iterations', 0.016), ('cdhmm', 0.016), ('cessent', 0.016), ('lowered', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="98-tfidf-1" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>Author: Trinh Minh Tri Do, Thierry Artières</p><p>Abstract: Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efﬁcient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efﬁcient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random ﬁelds, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy ε with a rate O(1/λε) where λ is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difﬁcult and requires a stronger and more disputable assumption. Yet we provide experimental results on artiﬁcial test problems, and on ﬁve standard and difﬁcult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms. Keywords: optimization, non-convex, non-smooth, cutting plane, bundle method, regularized risk</p><p>2 0.14359429 <a title="98-tfidf-2" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>Author: Ofer Dekel, Claudio Gentile, Karthik Sridharan</p><p>Abstract: We present a new online learning algorithm in the selective sampling framework, where labels must be actively queried before they are revealed. We prove bounds on the regret of our algorithm and on the number of labels it queries when faced with an adaptive adversarial strategy of generating the instances. Our bounds both generalize and strictly improve over previous bounds in similar settings. Additionally, our selective sampling algorithm can be converted into an efﬁcient statistical active learning algorithm. We extend our algorithm and analysis to the multiple-teacher setting, where the algorithm can choose which subset of teachers to query for each label. Finally, we demonstrate the effectiveness of our techniques on a real-world Internet search problem. Keywords: online learning, regret, label-efﬁcient, crowdsourcing</p><p>3 0.13732418 <a title="98-tfidf-3" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>Author: Sangkyun Lee, Stephen J. Wright</p><p>Abstract: Iterative methods that calculate their steps from approximate subgradient directions have proved to be useful for stochastic learning problems over large and streaming data sets. When the objective consists of a loss function plus a nonsmooth regularization term, the solution often lies on a lowdimensional manifold of parameter space along which the regularizer is smooth. (When an ℓ1 regularizer is used to induce sparsity in the solution, for example, this manifold is deﬁned by the set of nonzero components of the parameter vector.) This paper shows that a regularized dual averaging algorithm can identify this manifold, with high probability, before reaching the solution. This observation motivates an algorithmic strategy in which, once an iterate is suspected of lying on an optimal or near-optimal manifold, we switch to a “local phase” that searches in this manifold, thus converging rapidly to a near-optimal point. Computational results are presented to verify the identiﬁcation property and to illustrate the effectiveness of this approach. Keywords: regularization, dual averaging, partly smooth manifold, manifold identiﬁcation</p><p>4 0.13244295 <a title="98-tfidf-4" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>Author: Francesco Orabona, Luo Jie, Barbara Caputo</p><p>Abstract: In recent years there has been a lot of interest in designing principled classiﬁcation algorithms over multiple cues, based on the intuitive notion that using more features should lead to better performance. In the domain of kernel methods, a principled way to use multiple features is the Multi Kernel Learning (MKL) approach. Here we present a MKL optimization algorithm based on stochastic gradient descent that has a guaranteed convergence rate. We directly solve the MKL problem in the primal formulation. By having a p-norm formulation of MKL, we introduce a parameter that controls the level of sparsity of the solution, while leading to an easier optimization problem. We prove theoretically and experimentally that 1) our algorithm has a faster convergence rate as the number of kernels grows; 2) the training complexity is linear in the number of training examples; 3) very few iterations are sufﬁcient to reach good solutions. Experiments on standard benchmark databases support our claims. Keywords: multiple kernel learning, learning kernels, online optimization, stochastic subgradient descent, convergence bounds, large scale</p><p>5 0.12879138 <a title="98-tfidf-5" href="./jmlr-2012-Breaking_the_Curse_of_Kernelization%3A_Budgeted_Stochastic_Gradient_Descent_for_Large-Scale_SVM_Training.html">23 jmlr-2012-Breaking the Curse of Kernelization: Budgeted Stochastic Gradient Descent for Large-Scale SVM Training</a></p>
<p>Author: Zhuang Wang, Koby Crammer, Slobodan Vucetic</p><p>Abstract: Online algorithms that process one example at a time are advantageous when dealing with very large data or with data streams. Stochastic Gradient Descent (SGD) is such an algorithm and it is an attractive choice for online Support Vector Machine (SVM) training due to its simplicity and effectiveness. When equipped with kernel functions, similarly to other SVM learning algorithms, SGD is susceptible to the curse of kernelization that causes unbounded linear growth in model size and update time with data size. This may render SGD inapplicable to large data sets. We address this issue by presenting a class of Budgeted SGD (BSGD) algorithms for large-scale kernel SVM training which have constant space and constant time complexity per update. Speciﬁcally, BSGD keeps the number of support vectors bounded during training through several budget maintenance strategies. We treat the budget maintenance as a source of the gradient error, and show that the gap between the BSGD and the optimal SVM solutions depends on the model degradation due to budget maintenance. To minimize the gap, we study greedy budget maintenance methods based on removal, projection, and merging of support vectors. We propose budgeted versions of several popular online SVM algorithms that belong to the SGD family. We further derive BSGD algorithms for multi-class SVM training. Comprehensive empirical results show that BSGD achieves higher accuracy than the state-of-the-art budgeted online algorithms and comparable to non-budget algorithms, while achieving impressive computational efﬁciency both in time and space during training and prediction. Keywords: SVM, large-scale learning, online learning, stochastic gradient descent, kernel methods</p><p>6 0.1254724 <a title="98-tfidf-6" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>7 0.11688011 <a title="98-tfidf-7" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>8 0.070209369 <a title="98-tfidf-8" href="./jmlr-2012-Online_Submodular_Minimization.html">84 jmlr-2012-Online Submodular Minimization</a></p>
<p>9 0.05916959 <a title="98-tfidf-9" href="./jmlr-2012-Query_Strategies_for_Evading_Convex-Inducing_Classifiers.html">94 jmlr-2012-Query Strategies for Evading Convex-Inducing Classifiers</a></p>
<p>10 0.057498354 <a title="98-tfidf-10" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>11 0.057021528 <a title="98-tfidf-11" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>12 0.03800549 <a title="98-tfidf-12" href="./jmlr-2012-Linear_Fitted-Q_Iteration_with_Multiple_Reward_Functions.html">58 jmlr-2012-Linear Fitted-Q Iteration with Multiple Reward Functions</a></p>
<p>13 0.033803597 <a title="98-tfidf-13" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>14 0.032828815 <a title="98-tfidf-14" href="./jmlr-2012-Large-scale_Linear_Support_Vector_Regression.html">54 jmlr-2012-Large-scale Linear Support Vector Regression</a></p>
<p>15 0.032578494 <a title="98-tfidf-15" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>16 0.027629625 <a title="98-tfidf-16" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>17 0.027528008 <a title="98-tfidf-17" href="./jmlr-2012-A_Geometric_Approach_to_Sample_Compression.html">3 jmlr-2012-A Geometric Approach to Sample Compression</a></p>
<p>18 0.026518432 <a title="98-tfidf-18" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>19 0.025700379 <a title="98-tfidf-19" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>20 0.025085483 <a title="98-tfidf-20" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.172), (1, -0.245), (2, -0.0), (3, 0.085), (4, 0.074), (5, 0.088), (6, 0.217), (7, -0.083), (8, 0.13), (9, -0.03), (10, -0.027), (11, 0.377), (12, -0.026), (13, -0.016), (14, 0.033), (15, 0.098), (16, -0.015), (17, 0.13), (18, -0.026), (19, 0.079), (20, 0.039), (21, 0.036), (22, -0.131), (23, -0.168), (24, -0.205), (25, 0.039), (26, 0.093), (27, 0.006), (28, -0.07), (29, -0.009), (30, -0.034), (31, -0.049), (32, 0.077), (33, 0.04), (34, -0.069), (35, 0.037), (36, -0.003), (37, 0.027), (38, 0.053), (39, -0.032), (40, -0.0), (41, 0.027), (42, -0.014), (43, -0.038), (44, 0.029), (45, 0.079), (46, 0.017), (47, -0.032), (48, 0.033), (49, -0.094)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96909231 <a title="98-lsi-1" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>Author: Trinh Minh Tri Do, Thierry Artières</p><p>Abstract: Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efﬁcient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efﬁcient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random ﬁelds, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy ε with a rate O(1/λε) where λ is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difﬁcult and requires a stronger and more disputable assumption. Yet we provide experimental results on artiﬁcial test problems, and on ﬁve standard and difﬁcult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms. Keywords: optimization, non-convex, non-smooth, cutting plane, bundle method, regularized risk</p><p>2 0.67764056 <a title="98-lsi-2" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>Author: Gary B. Huang, Andrew Kae, Carl Doersch, Erik Learned-Miller</p><p>Abstract: We consider a model for which it is important, early in processing, to estimate some variables with high precision, but perhaps at relatively low recall. If some variables can be identiﬁed with near certainty, they can be conditioned upon, allowing further inference to be done efﬁciently. Speciﬁcally, we consider optical character recognition (OCR) systems that can be bootstrapped by identifying a subset of correctly translated document words with very high precision. This “clean set” is subsequently used as document-speciﬁc training data. While OCR systems produce conﬁdence measures for the identity of each letter or word, thresholding these values still produces a signiﬁcant number of errors. We introduce a novel technique for identifying a set of correct words with very high precision. Rather than estimating posterior probabilities, we bound the probability that any given word is incorrect using an approximate worst case analysis. We give empirical results on a data set of difﬁcult historical newspaper scans, demonstrating that our method for identifying correct words makes only two errors in 56 documents. Using document-speciﬁc character models generated from this data, we are able to reduce the error over properly segmented characters by 34.1% from an initial OCR system’s translation.1 Keywords: optical character recognition, probability bounding, document-speciﬁc modeling, computer vision 1. This work is an expanded and revised version of Kae et al. (2010). Supported by NSF Grant IIS-0916555. c 2012 Gary B. Huang, Andrew Kae, Carl Doersch and Erik Learned-Miller. H UANG , K AE , D OERSCH AND L EARNED -M ILLER</p><p>3 0.67426962 <a title="98-lsi-3" href="./jmlr-2012-Breaking_the_Curse_of_Kernelization%3A_Budgeted_Stochastic_Gradient_Descent_for_Large-Scale_SVM_Training.html">23 jmlr-2012-Breaking the Curse of Kernelization: Budgeted Stochastic Gradient Descent for Large-Scale SVM Training</a></p>
<p>Author: Zhuang Wang, Koby Crammer, Slobodan Vucetic</p><p>Abstract: Online algorithms that process one example at a time are advantageous when dealing with very large data or with data streams. Stochastic Gradient Descent (SGD) is such an algorithm and it is an attractive choice for online Support Vector Machine (SVM) training due to its simplicity and effectiveness. When equipped with kernel functions, similarly to other SVM learning algorithms, SGD is susceptible to the curse of kernelization that causes unbounded linear growth in model size and update time with data size. This may render SGD inapplicable to large data sets. We address this issue by presenting a class of Budgeted SGD (BSGD) algorithms for large-scale kernel SVM training which have constant space and constant time complexity per update. Speciﬁcally, BSGD keeps the number of support vectors bounded during training through several budget maintenance strategies. We treat the budget maintenance as a source of the gradient error, and show that the gap between the BSGD and the optimal SVM solutions depends on the model degradation due to budget maintenance. To minimize the gap, we study greedy budget maintenance methods based on removal, projection, and merging of support vectors. We propose budgeted versions of several popular online SVM algorithms that belong to the SGD family. We further derive BSGD algorithms for multi-class SVM training. Comprehensive empirical results show that BSGD achieves higher accuracy than the state-of-the-art budgeted online algorithms and comparable to non-budget algorithms, while achieving impressive computational efﬁciency both in time and space during training and prediction. Keywords: SVM, large-scale learning, online learning, stochastic gradient descent, kernel methods</p><p>4 0.59640968 <a title="98-lsi-4" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>Author: Sangkyun Lee, Stephen J. Wright</p><p>Abstract: Iterative methods that calculate their steps from approximate subgradient directions have proved to be useful for stochastic learning problems over large and streaming data sets. When the objective consists of a loss function plus a nonsmooth regularization term, the solution often lies on a lowdimensional manifold of parameter space along which the regularizer is smooth. (When an ℓ1 regularizer is used to induce sparsity in the solution, for example, this manifold is deﬁned by the set of nonzero components of the parameter vector.) This paper shows that a regularized dual averaging algorithm can identify this manifold, with high probability, before reaching the solution. This observation motivates an algorithmic strategy in which, once an iterate is suspected of lying on an optimal or near-optimal manifold, we switch to a “local phase” that searches in this manifold, thus converging rapidly to a near-optimal point. Computational results are presented to verify the identiﬁcation property and to illustrate the effectiveness of this approach. Keywords: regularization, dual averaging, partly smooth manifold, manifold identiﬁcation</p><p>5 0.49895665 <a title="98-lsi-5" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<p>Author: Ofer Dekel, Claudio Gentile, Karthik Sridharan</p><p>Abstract: We present a new online learning algorithm in the selective sampling framework, where labels must be actively queried before they are revealed. We prove bounds on the regret of our algorithm and on the number of labels it queries when faced with an adaptive adversarial strategy of generating the instances. Our bounds both generalize and strictly improve over previous bounds in similar settings. Additionally, our selective sampling algorithm can be converted into an efﬁcient statistical active learning algorithm. We extend our algorithm and analysis to the multiple-teacher setting, where the algorithm can choose which subset of teachers to query for each label. Finally, we demonstrate the effectiveness of our techniques on a real-world Internet search problem. Keywords: online learning, regret, label-efﬁcient, crowdsourcing</p><p>6 0.36718267 <a title="98-lsi-6" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>7 0.30181891 <a title="98-lsi-7" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>8 0.19224703 <a title="98-lsi-8" href="./jmlr-2012-Query_Strategies_for_Evading_Convex-Inducing_Classifiers.html">94 jmlr-2012-Query Strategies for Evading Convex-Inducing Classifiers</a></p>
<p>9 0.19038324 <a title="98-lsi-9" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>10 0.18422844 <a title="98-lsi-10" href="./jmlr-2012-Linear_Fitted-Q_Iteration_with_Multiple_Reward_Functions.html">58 jmlr-2012-Linear Fitted-Q Iteration with Multiple Reward Functions</a></p>
<p>11 0.16933091 <a title="98-lsi-11" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<p>12 0.14726162 <a title="98-lsi-12" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>13 0.14451405 <a title="98-lsi-13" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>14 0.14436096 <a title="98-lsi-14" href="./jmlr-2012-Large-scale_Linear_Support_Vector_Regression.html">54 jmlr-2012-Large-scale Linear Support Vector Regression</a></p>
<p>15 0.13495108 <a title="98-lsi-15" href="./jmlr-2012-Static_Prediction_Games_for_Adversarial_Learning_Problems.html">110 jmlr-2012-Static Prediction Games for Adversarial Learning Problems</a></p>
<p>16 0.13114572 <a title="98-lsi-16" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>17 0.12842265 <a title="98-lsi-17" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>18 0.12620135 <a title="98-lsi-18" href="./jmlr-2012-A_Geometric_Approach_to_Sample_Compression.html">3 jmlr-2012-A Geometric Approach to Sample Compression</a></p>
<p>19 0.12480195 <a title="98-lsi-19" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>20 0.12332091 <a title="98-lsi-20" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.018), (21, 0.038), (26, 0.057), (27, 0.429), (29, 0.031), (35, 0.023), (49, 0.011), (64, 0.018), (75, 0.093), (77, 0.017), (92, 0.056), (96, 0.086)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97834682 <a title="98-lda-1" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>Author: Tom De Smedt, Walter Daelemans</p><p>Abstract: Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classiﬁers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern. Keywords: Python, data mining, natural language processing, machine learning, graph networks</p><p>2 0.89861292 <a title="98-lda-2" href="./jmlr-2012-A_Geometric_Approach_to_Sample_Compression.html">3 jmlr-2012-A Geometric Approach to Sample Compression</a></p>
<p>Author: Benjamin I.P. Rubinstein, J. Hyam Rubinstein</p><p>Abstract: The Sample Compression Conjecture of Littlestone & Warmuth has remained unsolved for a quarter century. While maximum classes (concept classes meeting Sauer’s Lemma with equality) can be compressed, the compression of general concept classes reduces to compressing maximal classes (classes that cannot be expanded without increasing VC dimension). Two promising ways forward are: embedding maximal classes into maximum classes with at most a polynomial increase to VC dimension, and compression via operating on geometric representations. This paper presents positive results on the latter approach and a ﬁrst negative result on the former, through a systematic investigation of ﬁnite maximum classes. Simple arrangements of hyperplanes in hyperbolic space are shown to represent maximum classes, generalizing the corresponding Euclidean result. We show that sweeping a generic hyperplane across such arrangements forms an unlabeled compression scheme of size VC dimension and corresponds to a special case of peeling the one-inclusion graph, resolving a recent conjecture of Kuzmin & Warmuth. A bijection between ﬁnite maximum classes and certain arrangements of piecewise-linear (PL) hyperplanes in either a ball or Euclidean space is established. Finally we show that d-maximum classes corresponding to PL-hyperplane arrangements in Rd have cubical complexes homeomorphic to a d-ball, or equivalently complexes that are manifolds with boundary. A main result is that PL arrangements can be swept by a moving hyperplane to unlabeled d-compress any ﬁnite maximum class, forming a peeling scheme as conjectured by Kuzmin & Warmuth. A corollary is that some d-maximal classes cannot be embedded into any maximum class of VC-dimension d + k, for any constant k. The construction of the PL sweeping involves Pachner moves on the one-inclusion graph, corresponding to moves of a hyperplane across the intersection of d other hyperplanes. This extends the well known Pachner moves for triangulations to c</p><p>same-paper 3 0.74418426 <a title="98-lda-3" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>Author: Trinh Minh Tri Do, Thierry Artières</p><p>Abstract: Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efﬁcient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efﬁcient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random ﬁelds, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy ε with a rate O(1/λε) where λ is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difﬁcult and requires a stronger and more disputable assumption. Yet we provide experimental results on artiﬁcial test problems, and on ﬁve standard and difﬁcult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms. Keywords: optimization, non-convex, non-smooth, cutting plane, bundle method, regularized risk</p><p>4 0.38765106 <a title="98-lda-4" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>Author: Stephen Gould</p><p>Abstract: We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data ﬂows. Keywords: machine learning, graphical models, computer vision, open-source software</p><p>5 0.37219244 <a title="98-lda-5" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random ﬁelds (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter ﬁtting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random ﬁeld via the graphical lasso.</p><p>6 0.36611757 <a title="98-lda-6" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>7 0.36243606 <a title="98-lda-7" href="./jmlr-2012-Finding_Recurrent_Patterns_from_Continuous_Sign_Language_Sentences_for_Automated_Extraction_of_Signs.html">45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</a></p>
<p>8 0.35783345 <a title="98-lda-8" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>9 0.35537875 <a title="98-lda-9" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>10 0.35497147 <a title="98-lda-10" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>11 0.35173175 <a title="98-lda-11" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>12 0.35066327 <a title="98-lda-12" href="./jmlr-2012-Hope_and_Fear_for_Discriminative_Training_of_Statistical_Translation_Models.html">49 jmlr-2012-Hope and Fear for Discriminative Training of Statistical Translation Models</a></p>
<p>13 0.34838802 <a title="98-lda-13" href="./jmlr-2012-Activized_Learning%3A_Transforming_Passive_to_Active_with_Improved_Label_Complexity.html">14 jmlr-2012-Activized Learning: Transforming Passive to Active with Improved Label Complexity</a></p>
<p>14 0.34806684 <a title="98-lda-14" href="./jmlr-2012-Feature_Selection_via_Dependence_Maximization.html">44 jmlr-2012-Feature Selection via Dependence Maximization</a></p>
<p>15 0.34124285 <a title="98-lda-15" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>16 0.33965844 <a title="98-lda-16" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>17 0.33852965 <a title="98-lda-17" href="./jmlr-2012-Query_Strategies_for_Evading_Convex-Inducing_Classifiers.html">94 jmlr-2012-Query Strategies for Evading Convex-Inducing Classifiers</a></p>
<p>18 0.33570224 <a title="98-lda-18" href="./jmlr-2012-Security_Analysis_of_Online_Centroid_Anomaly_Detection.html">104 jmlr-2012-Security Analysis of Online Centroid Anomaly Detection</a></p>
<p>19 0.33554444 <a title="98-lda-19" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>20 0.33540469 <a title="98-lda-20" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
