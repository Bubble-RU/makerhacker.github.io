<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-101" href="#">jmlr2012-101</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</h1>
<br/><p>Source: <a title="jmlr-2012-101-pdf" href="http://jmlr.org/papers/volume13/chen12a/chen12a.pdf">pdf</a></p><p>Author: Tianqi Chen, Weinan Zhang, Qiuxia Lu, Kailong Chen, Zhao Zheng, Yong Yu</p><p>Abstract: In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative ﬁltering. SVDFeature is designed to efﬁciently solve the feature-based matrix factorization. The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information. The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efﬁcient training on large-scale data set. Using this toolkit, we built solutions to win KDD Cup for two consecutive years. Keywords: large-scale collaborative ﬁltering, context-aware recommendation, ranking</p><p>Reference: <a title="jmlr-2012-101-reference" href="../jmlr2012_reference/jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 CN  Apex Data and Knowledge Management Lab Shanghai Jiao Tong University 800 Dongchuan Road Shanghai 200240 China  Editor: Mikio Braun  Abstract In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative ﬁltering. [sent-19, score-0.323]
</p><p>2 The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information. [sent-21, score-0.138]
</p><p>3 The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efﬁcient training on large-scale data set. [sent-22, score-0.356]
</p><p>4 Using this toolkit, we built solutions to win KDD Cup for two consecutive years. [sent-23, score-0.042]
</p><p>5 Keywords: large-scale collaborative ﬁltering, context-aware recommendation, ranking  1. [sent-24, score-0.209]
</p><p>6 Introduction Recommender system, which recommends items based on users’ interests, has become more and more popular in many real-world situations. [sent-25, score-0.087]
</p><p>7 Collaborative ﬁltering (CF) techniques, as the main thrust behind recommender systems, have been developed for many years and keep to be a hot area in both academia and industry. [sent-26, score-0.116]
</p><p>8 In this paper, we focus on building collaborative ﬁltering based recommendation toolkit which can effectively leverage the rich information of data collected and naturally scale up to very large data set. [sent-27, score-0.392]
</p><p>9 Matrix factorization (MF) is one of the most popular CF methods, and variants of it have been proposed in speciﬁc settings. [sent-28, score-0.096]
</p><p>10 However, traditional approaches design speciﬁc models for each problem, demanding great efforts in engineering. [sent-29, score-0.044]
</p><p>11 Fortunately the majority of factorization models share many common patterns, which enables us to summarize them into a single model and implement a uniﬁed toolkit, called SVDFeature. [sent-30, score-0.126]
</p><p>12 This helps save the efforts of engineering for speciﬁc models and allows users to focus on model design. [sent-32, score-0.138]
</p><p>13 C HEN , Z HANG , L U , C HEN , Z HENG AND Y U  2GB memory to get an excellent performance of 22. [sent-40, score-0.047]
</p><p>14 Model of SVDFeature There are three important factors in most CF problems: users’ interests, items’ properties and other factors that directly affect users’ preferences over items. [sent-45, score-0.098]
</p><p>15 Various kinds of information can be used to model these factors. [sent-46, score-0.028]
</p><p>16 For example, users’ browsing history over movie reviews may correlate with users’ taste over movies; the information of the director and actors of a movie can be used to predict its properties; the rating history over similar movies directly affect whether a user will favor the current one. [sent-47, score-0.475]
</p><p>17 Our model summarizes the three factors as feature vectors (denoted by α ∈ Rm , β ∈ Rn , γ ∈ Rs ) and predicts the preference score y as ˆ s  y(α, β, γ) = ˆ  ∑  (g) γ jb j +  j=1  n  ∑  j=1  (u) α jb j +  m  ∑  (i) β jb j  T  n  +  j=1  ∑ α jp j  j=1  m  ∑ β jq j  . [sent-48, score-0.386]
</p><p>18 p j ∈ Rd and q j ∈ Rd are d (u) (i) (g) dimensional latent factors associated with each feature. [sent-50, score-0.079]
</p><p>19 We call α user feature, β item feature, and γ global feature. [sent-52, score-0.255]
</p><p>20 Intuitively, we use a linear model to construct user and item latent factors from features. [sent-56, score-0.334]
</p><p>21 Many state-of-the-art collaborative ﬁltering algorithms can be implemented using SVDFeature. [sent-59, score-0.168]
</p><p>22 Let us suppose we want to recommend music tracks for users using the rating history, and we have known the albums of tracks and the timestamps of the ratings as auxiliary information. [sent-60, score-0.413]
</p><p>23 As we know, the album information can be used to better represent the content of tracks, and the temporal information can be used to detect the changes of item popularity. [sent-61, score-0.417]
</p><p>24 All these auxiliary information can help improve the performance of a recommender system. [sent-62, score-0.123]
</p><p>25 Taking them into consideration, we can represent a user u’s preference towards track i at time t as follows y(u, i,t) = bi,bin(t) + bi + b p(i) + bu + pT (qi + qal(i) ) ˆ u where al(i) is the album of a track i. [sent-63, score-0.427]
</p><p>26 We segment time into consecutive bins and deﬁne bin(t) to map timestamp to corresponding bin index. [sent-64, score-0.173]
</p><p>27 To incorporate the taxonomical information, we introduce qal(i) to make the prediction towards track i depend on the latent factor of corresponding album. [sent-66, score-0.091]
</p><p>28 To implement this model using SVDFeature, we can simply deﬁne the features as follows αh =  1 h=u , βh = 0 h=u  1 h = i or h = al(i) , γh = 0 otherwise  1 h = #bins × i + bin(t) . [sent-67, score-0.03]
</p><p>29 0 otherwise  The item feature β is deﬁned to be indicator of whether the record is related to the track and its corresponding album. [sent-68, score-0.242]
</p><p>30 We also deﬁne a global feature γh to encode the temporally varying item bias bi,bin(t) . [sent-69, score-0.243]
</p><p>31 Using SVDFeature Figure 1 gives a usage example of SVDFeature to implement the model introduced in previous section. [sent-74, score-0.107]
</p><p>32 We encode the album information as item features and item day biases as global features. [sent-75, score-0.682]
</p><p>33 6 associated with album id is an empirical parameter chosen by the user to control the inﬂuence of album information in prediction. [sent-77, score-0.462]
</p><p>34 Assuming there are only two days’ records in the data, the global feature index is deﬁned as gid = 2 × iid + day,where the number of item day biases is twice the number of items. [sent-78, score-0.275]
</p><p>35 SVDFeature will learn a feature-based matrix factorization model with the given training data and make predictions on supplied test feature ﬁles. [sent-79, score-0.129]
</p><p>36 We provide a manual to give more details about the usage of SVDFeature. [sent-80, score-0.116]
</p><p>37 Handling Big Data Recommendation algorithms often have to deal with problems with large scale data set in real world, which has been taken into consideration in designing SVDFeature. [sent-82, score-0.029]
</p><p>38 In our approach, we store the data into a buffer ﬁle in hard disk. [sent-83, score-0.117]
</p><p>39 The data is shufﬂed before storing, and then the training program linearly iterates over the buffer and updates the model with each training sample. [sent-84, score-0.183]
</p><p>40 This approach allows us to do training as long as the model ﬁts into memory. [sent-85, score-0.033]
</p><p>41 An independent thread is created to fetch the data from hard disk into a memory queue. [sent-87, score-0.304]
</p><p>42 At the same time, the training thread reads the data from memory queue and updates the model. [sent-88, score-0.213]
</p><p>43 This pipeline style of execution releases the burden of I/O from training thread. [sent-89, score-0.143]
</p><p>44 As long as I/O speed is similar to (or faster than) training speed, the cost of I/O is negligible. [sent-90, score-0.033]
</p><p>45 3621  C HEN , Z HANG , L U , C HEN , Z HENG AND Y U  Data in Disk  FETCH Thread 1  Buffer in Memory  INPUT Thread 2  Matrix Factorization Stochastic Gradient Descend  Figure 2: Pipeline training  5. [sent-92, score-0.033]
</p><p>46 Extra Features SVDFeature also provides several other extra features for better modeling capability. [sent-93, score-0.028]
</p><p>47 We list notable  features here: (1) Efﬁcient speedup training for user feedback information; (2) Supporting collaborative ranking training; (3) Different kinds of regularization options; (4) Separated feature table extension for user and item features. [sent-94, score-0.599]
</p><p>48 The details are described in the project page and manual. [sent-95, score-0.034]
</p><p>49 The project is self-contained and only depends on standard libraries. [sent-98, score-0.034]
</p><p>50 We provide a technical document introducing the algorithm and a user manual describing the usage details of the toolkit. [sent-101, score-0.19]
</p><p>51 To help users get started, we also provide a demo folder with example shell scripts that show the procedures from feature generation to training and prediction. [sent-102, score-0.19]
</p><p>52 Acknowledgments Yong Yu is supported by grants from NSFC-RGC joint research project 60931160445. [sent-103, score-0.034]
</p><p>53 We greatly appreciate the discussions with Zhengdong Lu, Diyi Yang and Li’ang Yin. [sent-105, score-0.03]
</p><p>54 Combining factorization model and additive forest for collaborative followee recommendation. [sent-135, score-0.264]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('svdfeature', 0.622), ('apex', 0.272), ('sjtu', 0.233), ('album', 0.194), ('item', 0.181), ('collaborative', 0.168), ('toolkit', 0.155), ('thread', 0.133), ('chen', 0.127), ('buffer', 0.117), ('hen', 0.11), ('yong', 0.1), ('cn', 0.098), ('factorization', 0.096), ('users', 0.094), ('jb', 0.09), ('zheng', 0.083), ('recommender', 0.083), ('fetch', 0.078), ('heng', 0.078), ('kailong', 0.078), ('qal', 0.078), ('qiuxia', 0.078), ('tianqi', 0.078), ('weinan', 0.078), ('usage', 0.077), ('pipeline', 0.077), ('cf', 0.077), ('lu', 0.075), ('user', 0.074), ('tracks', 0.073), ('recommendation', 0.069), ('track', 0.061), ('shanghai', 0.06), ('cao', 0.06), ('history', 0.056), ('eature', 0.055), ('movies', 0.055), ('ltering', 0.055), ('bin', 0.054), ('items', 0.054), ('day', 0.052), ('kdd', 0.051), ('factors', 0.049), ('cup', 0.049), ('ratings', 0.049), ('hang', 0.049), ('movie', 0.049), ('memory', 0.047), ('disk', 0.046), ('interests', 0.046), ('jiang', 0.046), ('bins', 0.044), ('efforts', 0.044), ('music', 0.044), ('biases', 0.042), ('consecutive', 0.042), ('temporal', 0.042), ('ranking', 0.041), ('rating', 0.04), ('auxiliary', 0.04), ('manual', 0.039), ('liu', 0.038), ('preference', 0.037), ('yu', 0.034), ('project', 0.034), ('demo', 0.033), ('taste', 0.033), ('actors', 0.033), ('loading', 0.033), ('recommends', 0.033), ('releases', 0.033), ('apache', 0.033), ('hot', 0.033), ('dongchuan', 0.033), ('oolkit', 0.033), ('descend', 0.033), ('iltering', 0.033), ('timestamp', 0.033), ('training', 0.033), ('al', 0.033), ('encode', 0.032), ('zhao', 0.031), ('yao', 0.03), ('appreciate', 0.03), ('correlate', 0.03), ('kong', 0.03), ('xie', 0.03), ('jp', 0.03), ('shell', 0.03), ('temporally', 0.03), ('latent', 0.03), ('implement', 0.03), ('edu', 0.029), ('consideration', 0.029), ('extra', 0.028), ('kinds', 0.028), ('meanwhile', 0.028), ('tang', 0.028), ('rmse', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="101-tfidf-1" href="./jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering.html">101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</a></p>
<p>Author: Tianqi Chen, Weinan Zhang, Qiuxia Lu, Kailong Chen, Zhao Zheng, Yong Yu</p><p>Abstract: In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative ﬁltering. SVDFeature is designed to efﬁciently solve the feature-based matrix factorization. The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information. The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efﬁcient training on large-scale data set. Using this toolkit, we built solutions to win KDD Cup for two consecutive years. Keywords: large-scale collaborative ﬁltering, context-aware recommendation, ranking</p><p>2 0.14643867 <a title="101-tfidf-2" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>Author: Joonseok Lee, Mingxuan Sun, Guy Lebanon</p><p>Abstract: Recommendation systems are important business applications with signiﬁcant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms. Keywords: recommender systems, collaborative ﬁltering, evaluation metrics</p><p>3 0.057620771 <a title="101-tfidf-3" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>4 0.048862521 <a title="101-tfidf-4" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>Author: Marinka Žitnik, Blaž Zupan</p><p>Abstract: NIMFA is an open-source Python library that provides a uniﬁed interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA’s component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks. Keywords: nonnegative matrix factorization, initialization methods, quality measures, scripting, Python</p><p>5 0.031310562 <a title="101-tfidf-5" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<p>Author: Chiwoo Park, Jianhua Z. Huang, Yu Ding</p><p>Abstract: This paper presents the Getting-started style documentation for the local and parallel computation toolbox for Gaussian process regression (GPLP), an open source software package written in Matlab (but also compatible with Octave). The working environment and the usage of the software package will be presented in this paper. Keywords: Gaussian process regression, domain decomposition method, partial independent conditional, bagging for Gaussian process, local probabilistic regression</p><p>6 0.03040871 <a title="101-tfidf-6" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<p>7 0.027689489 <a title="101-tfidf-7" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<p>8 0.023412239 <a title="101-tfidf-8" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>9 0.020594448 <a title="101-tfidf-9" href="./jmlr-2012-An_Active_Learning_Algorithm_for_Ranking_from_Pairwise_Preferences_with_an_Almost_Optimal_Query_Complexity.html">17 jmlr-2012-An Active Learning Algorithm for Ranking from Pairwise Preferences with an Almost Optimal Query Complexity</a></p>
<p>10 0.020453781 <a title="101-tfidf-10" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>11 0.020145927 <a title="101-tfidf-11" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>12 0.019312415 <a title="101-tfidf-12" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>13 0.018722896 <a title="101-tfidf-13" href="./jmlr-2012-The_huge_Package_for_High-dimensional_Undirected_Graph_Estimation_in_R.html">113 jmlr-2012-The huge Package for High-dimensional Undirected Graph Estimation in R</a></p>
<p>14 0.018690363 <a title="101-tfidf-14" href="./jmlr-2012-Refinement_of_Operator-valued_Reproducing_Kernels.html">96 jmlr-2012-Refinement of Operator-valued Reproducing Kernels</a></p>
<p>15 0.018066341 <a title="101-tfidf-15" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>16 0.017960409 <a title="101-tfidf-16" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>17 0.017299328 <a title="101-tfidf-17" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>18 0.017072789 <a title="101-tfidf-18" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>19 0.016824983 <a title="101-tfidf-19" href="./jmlr-2012-Feature_Selection_via_Dependence_Maximization.html">44 jmlr-2012-Feature Selection via Dependence Maximization</a></p>
<p>20 0.016794313 <a title="101-tfidf-20" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.082), (1, 0.034), (2, 0.115), (3, -0.03), (4, -0.015), (5, 0.042), (6, 0.048), (7, -0.044), (8, -0.038), (9, -0.143), (10, -0.12), (11, -0.087), (12, 0.192), (13, -0.084), (14, -0.005), (15, 0.127), (16, -0.025), (17, -0.027), (18, -0.118), (19, -0.147), (20, 0.007), (21, 0.161), (22, 0.175), (23, -0.107), (24, 0.013), (25, 0.479), (26, 0.08), (27, -0.136), (28, -0.026), (29, -0.002), (30, -0.035), (31, 0.046), (32, -0.071), (33, -0.104), (34, 0.024), (35, 0.064), (36, 0.034), (37, 0.06), (38, -0.026), (39, -0.062), (40, -0.053), (41, 0.074), (42, 0.029), (43, -0.092), (44, -0.032), (45, 0.052), (46, 0.104), (47, 0.056), (48, 0.002), (49, 0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97402102 <a title="101-lsi-1" href="./jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering.html">101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</a></p>
<p>Author: Tianqi Chen, Weinan Zhang, Qiuxia Lu, Kailong Chen, Zhao Zheng, Yong Yu</p><p>Abstract: In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative ﬁltering. SVDFeature is designed to efﬁciently solve the feature-based matrix factorization. The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information. The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efﬁcient training on large-scale data set. Using this toolkit, we built solutions to win KDD Cup for two consecutive years. Keywords: large-scale collaborative ﬁltering, context-aware recommendation, ranking</p><p>2 0.92381668 <a title="101-lsi-2" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>Author: Joonseok Lee, Mingxuan Sun, Guy Lebanon</p><p>Abstract: Recommendation systems are important business applications with signiﬁcant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms. Keywords: recommender systems, collaborative ﬁltering, evaluation metrics</p><p>3 0.32049516 <a title="101-lsi-3" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>4 0.19020136 <a title="101-lsi-4" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<p>Author: Mario Frank, Andreas P. Streich, David Basin, Joachim M. Buhmann</p><p>Abstract: We propose a probabilistic model for clustering Boolean data where an object can be simultaneously assigned to multiple clusters. By explicitly modeling the underlying generative process that combines the individual source emissions, highly structured data are expressed with substantially fewer clusters compared to single-assignment clustering. As a consequence, such a model provides robust parameter estimators even when the number of samples is low. We extend the model with different noise processes and demonstrate that maximum-likelihood estimation with multiple assignments consistently infers source parameters more accurately than single-assignment clustering. Our model is primarily motivated by the task of role mining for role-based access control, where users of a system are assigned one or more roles. In experiments with real-world access-control data, our model exhibits better generalization performance than state-of-the-art approaches. Keywords: clustering, multi-assignments, overlapping clusters, Boolean data, role mining, latent feature models</p><p>5 0.18678445 <a title="101-lsi-5" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<p>Author: Chiwoo Park, Jianhua Z. Huang, Yu Ding</p><p>Abstract: This paper presents the Getting-started style documentation for the local and parallel computation toolbox for Gaussian process regression (GPLP), an open source software package written in Matlab (but also compatible with Octave). The working environment and the usage of the software package will be presented in this paper. Keywords: Gaussian process regression, domain decomposition method, partial independent conditional, bagging for Gaussian process, local probabilistic regression</p><p>6 0.18312003 <a title="101-lsi-6" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<p>7 0.15208785 <a title="101-lsi-7" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>8 0.13006018 <a title="101-lsi-8" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>9 0.12898941 <a title="101-lsi-9" href="./jmlr-2012-Nonparametric_Guidance_of_Autoencoder_Representations_using_Label_Information.html">78 jmlr-2012-Nonparametric Guidance of Autoencoder Representations using Label Information</a></p>
<p>10 0.11896859 <a title="101-lsi-10" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>11 0.10840919 <a title="101-lsi-11" href="./jmlr-2012-Iterative_Reweighted_Algorithms_for_Matrix_Rank_Minimization.html">52 jmlr-2012-Iterative Reweighted Algorithms for Matrix Rank Minimization</a></p>
<p>12 0.1050473 <a title="101-lsi-12" href="./jmlr-2012-Mal-ID%3A_Automatic_Malware_Detection_Using_Common_Segment_Analysis_and_Meta-Features.html">63 jmlr-2012-Mal-ID: Automatic Malware Detection Using Common Segment Analysis and Meta-Features</a></p>
<p>13 0.10403015 <a title="101-lsi-13" href="./jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</a></p>
<p>14 0.10349646 <a title="101-lsi-14" href="./jmlr-2012-An_Active_Learning_Algorithm_for_Ranking_from_Pairwise_Preferences_with_an_Almost_Optimal_Query_Complexity.html">17 jmlr-2012-An Active Learning Algorithm for Ranking from Pairwise Preferences with an Almost Optimal Query Complexity</a></p>
<p>15 0.093325995 <a title="101-lsi-15" href="./jmlr-2012-The_huge_Package_for_High-dimensional_Undirected_Graph_Estimation_in_R.html">113 jmlr-2012-The huge Package for High-dimensional Undirected Graph Estimation in R</a></p>
<p>16 0.092103288 <a title="101-lsi-16" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>17 0.091574244 <a title="101-lsi-17" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>18 0.091268964 <a title="101-lsi-18" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<p>19 0.08948645 <a title="101-lsi-19" href="./jmlr-2012-Towards_Integrative_Causal_Analysis_of_Heterogeneous_Data_Sets_and_Studies.html">114 jmlr-2012-Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies</a></p>
<p>20 0.088954389 <a title="101-lsi-20" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.61), (21, 0.022), (26, 0.022), (27, 0.011), (29, 0.013), (49, 0.023), (56, 0.037), (57, 0.013), (69, 0.053), (75, 0.03), (92, 0.019), (96, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91228986 <a title="101-lda-1" href="./jmlr-2012-SVDFeature%3A_A_Toolkit_for_Feature-based_Collaborative_Filtering.html">101 jmlr-2012-SVDFeature: A Toolkit for Feature-based Collaborative Filtering</a></p>
<p>Author: Tianqi Chen, Weinan Zhang, Qiuxia Lu, Kailong Chen, Zhao Zheng, Yong Yu</p><p>Abstract: In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative ﬁltering. SVDFeature is designed to efﬁciently solve the feature-based matrix factorization. The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information. The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efﬁcient training on large-scale data set. Using this toolkit, we built solutions to win KDD Cup for two consecutive years. Keywords: large-scale collaborative ﬁltering, context-aware recommendation, ranking</p><p>2 0.55997634 <a title="101-lda-2" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>Author: Sham M. Kakade, Shai Shalev-Shwartz, Ambuj Tewari</p><p>Abstract: There is growing body of learning problems for which it is natural to organize the parameters into a matrix. As a result, it becomes easy to impose sophisticated prior knowledge by appropriately regularizing the parameters under some matrix norm. This work describes and analyzes a systematic method for constructing such matrix-based regularization techniques. In particular, we focus on how the underlying statistical properties of a given problem can help us decide which regularization function is appropriate. Our methodology is based on a known duality phenomenon: a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. We demonstrate the potential of this framework by deriving novel generalization and regret bounds for multi-task learning, multi-class learning, and multiple kernel learning. Keywords: regularization, strong convexity, regret bounds, generalization bounds, multi-task learning, multi-class learning, multiple kernel learning</p><p>3 0.2680473 <a title="101-lda-3" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>Author: Matus Telgarsky</p><p>Abstract: Boosting combines weak learners into a predictor with low empirical risk. Its dual constructs a high entropy distribution upon which weak learners and training labels are uncorrelated. This manuscript studies this primal-dual relationship under a broad family of losses, including the exponential loss of AdaBoost and the logistic loss, revealing: • Weak learnability aids the whole loss family: for any ε > 0, O (ln(1/ε)) iterations sufﬁce to produce a predictor with empirical risk ε-close to the inﬁmum; • The circumstances granting the existence of an empirical risk minimizer may be characterized in terms of the primal and dual problems, yielding a new proof of the known rate O (ln(1/ε)); • Arbitrary instances may be decomposed into the above two, granting rate O (1/ε), with a matching lower bound provided for the logistic loss. Keywords: boosting, convex analysis, weak learnability, coordinate descent, maximum entropy</p><p>4 0.22356747 <a title="101-lda-4" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>Author: Joonseok Lee, Mingxuan Sun, Guy Lebanon</p><p>Abstract: Recommendation systems are important business applications with signiﬁcant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms. Keywords: recommender systems, collaborative ﬁltering, evaluation metrics</p><p>5 0.21685082 <a title="101-lda-5" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>Author: Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, Lin Xiao</p><p>Abstract: Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment. We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem. Keywords: distributed computing, online learning, stochastic optimization, regret bounds, convex optimization</p><p>6 0.21632892 <a title="101-lda-6" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>7 0.20884061 <a title="101-lda-7" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>8 0.17944276 <a title="101-lda-8" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>9 0.17531699 <a title="101-lda-9" href="./jmlr-2012-MULTIBOOST%3A_A_Multi-purpose_Boosting_Package.html">62 jmlr-2012-MULTIBOOST: A Multi-purpose Boosting Package</a></p>
<p>10 0.17347997 <a title="101-lda-10" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>11 0.17145237 <a title="101-lda-11" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>12 0.16787228 <a title="101-lda-12" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>13 0.16308874 <a title="101-lda-13" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>14 0.1629111 <a title="101-lda-14" href="./jmlr-2012-Online_Submodular_Minimization.html">84 jmlr-2012-Online Submodular Minimization</a></p>
<p>15 0.16280395 <a title="101-lda-15" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>16 0.16243777 <a title="101-lda-16" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>17 0.16114461 <a title="101-lda-17" href="./jmlr-2012-Sparse_and_Unique_Nonnegative_Matrix_Factorization_Through_Data_Preprocessing.html">108 jmlr-2012-Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing</a></p>
<p>18 0.160321 <a title="101-lda-18" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<p>19 0.15999483 <a title="101-lda-19" href="./jmlr-2012-A_Model_of_the_Perception_of_Facial_Expressions_of_Emotion_by_Humans%3A_Research_Overview_and_Perspectives.html">6 jmlr-2012-A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives</a></p>
<p>20 0.15979433 <a title="101-lda-20" href="./jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
