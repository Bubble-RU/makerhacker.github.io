<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-67" href="#">jmlr2012-67</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</h1>
<br/><p>Source: <a title="jmlr-2012-67-pdf" href="http://jmlr.org/papers/volume13/raskutti12a/raskutti12a.pdf">pdf</a></p><p>Author: Garvesh Raskutti, Martin J. Wainwright, Bin Yu</p><p>Abstract: Sparse additive models are families of d-variate functions with the additive decomposition f ∗ = ∑ j∈S f j∗ , where S is an unknown subset of cardinality s ≪ d. In this paper, we consider the case where each univariate component function f j∗ lies in a reproducing kernel Hilbert space (RKHS), and analyze a method for estimating the unknown function f ∗ based on kernels combined with ℓ1 -type convex regularization. Working within a high-dimensional framework that allows both the dimension d and sparsity s to increase with n, we derive convergence rates in the L2 (P) and L2 (Pn ) norms over the class Fd,s,H of sparse additive models with each univariate function f j∗ in the unit ball of a univariate RKHS with bounded kernel function. We complement our upper bounds by deriving minimax lower bounds on the L2 (P) error, thereby showing the optimality of our method. Thus, we obtain optimal minimax rates for many interesting classes of sparse additive models, including polynomials, splines, and Sobolev classes. We also show that if, in contrast to our univariate conditions, the d-variate function class is assumed to be globally bounded, then much √ faster estimation rates are possible for any sparsity s = Ω( n), showing that global boundedness is a signiﬁcant restriction in the high-dimensional setting. Keywords: sparsity, kernel, non-parametric, convex, minimax</p><p>Reference: <a title="jmlr-2012-67-reference" href="../jmlr2012_reference/jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we consider the case where each univariate component function f j∗ lies in a reproducing kernel Hilbert space (RKHS), and analyze a method for estimating the unknown function f ∗ based on kernels combined with ℓ1 -type convex regularization. [sent-9, score-0.413]
</p><p>2 We complement our upper bounds by deriving minimax lower bounds on the L2 (P) error, thereby showing the optimality of our method. [sent-11, score-0.365]
</p><p>3 Thus, we obtain optimal minimax rates for many interesting classes of sparse additive models, including polynomials, splines, and Sobolev classes. [sent-12, score-0.451]
</p><p>4 We also show that if, in contrast to our univariate conditions, the d-variate function class is assumed to be globally bounded, then much √ faster estimation rates are possible for any sparsity s = Ω( n), showing that global boundedness is a signiﬁcant restriction in the high-dimensional setting. [sent-13, score-0.649]
</p><p>5 Note that the sparse additive model (2) is a natural generalization of the sparse linear model, to which it reduces when each univariate function is constrained to be linear. [sent-49, score-0.455]
</p><p>6 Past work by Koltchinskii and Yuan (2010) establishes rates for sparse additive models with an additional global boundedness condition, but as will be discussed at more length in the sequel, these rates are not minimax optimal in general. [sent-60, score-0.727]
</p><p>7 These minimax lower bounds, stated in Theorem 2, are speciﬁed in terms of the metric entropy of the underlying univariate function classes. [sent-75, score-0.579]
</p><p>8 Our third contribution is to determine upper bounds on minimax L2 (P) and L2 (Pn ) error when we impose a global boundedness assumption on the class Fd,s,H . [sent-82, score-0.539]
</p><p>9 More precisely, a global boundedness condition means that the quantity B(Fd,s,H ) = sup f ∈Fd,s,H supx | ∑d f j (x j )| is assumed to be j=1 bounded independently of (s, d). [sent-83, score-0.417]
</p><p>10 As mentioned earlier, our upper bound in Theorem 1 does not impose a global boundedness condition, whereas in contrast, the analysis of Koltchinskii and Yuan (2010), or KY for short, does impose such a global boundedness condition. [sent-84, score-0.634]
</p><p>11 It is natural to wonder whether or not this difference is actually signiﬁcant— that is, do the minimax rates for the class of sparse additive models depend on whether or not global boundedness is imposed? [sent-86, score-0.647]
</p><p>12 In particular, Theorem 3 and Corollary 3 provide upper bounds on the minimax rates, as measured in either the L2 (P) and L2 (Pn )-norms, under a global boundedness condition. [sent-89, score-0.539]
</p><p>13 Section 3 is devoted to the statement of our main results and discussion of their consequences; it includes description of our method, the upper bounds on the convergence rate that it achieves, and a matching set of minimax lower bounds. [sent-94, score-0.309]
</p><p>14 5 investigates the restrictiveness of the global uniform boundedness assumption and in particular, Theorem 3 and Corollary 3 demonstrate that there are classes of globally bounded functions for which faster rates are possible. [sent-96, score-0.376]
</p><p>15 The decay rate of these eigenvalues will play a crucial role in our analysis, since they ultimately determine the rate νn for the univariate RKHS’s in our function classes. [sent-108, score-0.382]
</p><p>16 , d, let H j ⊂ L2 (Q) be a reproducing kernel Hilbert space of univariate functions on the domain X ⊂ R. [sent-122, score-0.413]
</p><p>17 Note that H (S) is also (a subset of) a reproducing kernel Hilbert space, in particular with the norm f 2 (S) = H  ∑  j∈S  fj 2 j, H  where · H j denotes the norm on the univariate Hilbert space H j . [sent-132, score-0.699]
</p><p>18 1 by describing a regularized M-estimator for sparse additive models, and we state our upper bounds for this estimator in Section 3. [sent-161, score-0.294]
</p><p>19 (2009) is based on leastsquares loss regularized with single sparsity constraint, and separate smoothness constraints for each univariate function. [sent-204, score-0.371]
</p><p>20 ≥ 0 denote the non-negative eigenvalues of the kernel operator deﬁning the univariate Hilbert space H , as deﬁned in Equation (3), and deﬁne the function 1 n  Qσ,n (t) : = √  ∞  ∑ min{t 2 , µℓ }  1/2  . [sent-228, score-0.396]
</p><p>21 We refer to νn as the critical univariate rate, as it is the minimax-optimal rate for L2 (P)-estimation of a single univariate function in the Hilbert space H (e. [sent-230, score-0.582]
</p><p>22 We assume that each function within the unit ball of the univariate Hilbert space is uniformly bounded by a constant multiple of its Hilbert norm—that is, for each j = 1, . [sent-235, score-0.304]
</p><p>23 , d and each f j ∈ H , fj  ∞  : = sup | f j (x j )| ≤ c f j H . [sent-238, score-0.389]
</p><p>24 (10)  xj  This condition is satisﬁed for many kernel classes including Sobolev spaces, and any univariate RKHS in which the kernel function2 bounded uniformly by c. [sent-239, score-0.478]
</p><p>25 Such a condition is routinely imposed for proving upper bounds on rates of convergence for non-parametric least squares in the univariate case d = 1 (see, e. [sent-240, score-0.47]
</p><p>26 Note that this univariate boundedness does not imply that the multivariate functions f = ∑ j∈S f j in F are uniformly bounded independently of √ (d, s); rather, since such functions are the sum of s terms, they can take on values of the order s. [sent-243, score-0.484]
</p><p>27 The following result applies to any class Fd,s,H of sparse additive models based on a univariate Hilbert space satisfying condition (10), and to the estimator (6) based on n i. [sent-244, score-0.448]
</p><p>28 Depending on the scaling of the triple (n, d, s) and the smoothness of the univariate RKHS H , either the subset selection term or function estimation term may dominate. [sent-258, score-0.336]
</p><p>29 M INIMAX -O PTIMAL R ATES FOR S PARSE A DDITIVE M ODELS  where νn, j is the critical univariate rate associated with the Hilbert space H j , and S is the subset on which f ∗ is supported. [sent-267, score-0.31]
</p><p>30 Corollary 1 Under the same conditions as Theorem 1, consider an univariate kernel with ﬁnite rank m. [sent-272, score-0.342]
</p><p>31 Proof : It sufﬁces to show that the critical univariate rate (8) satisﬁes the scaling ν2 = O (m/n). [sent-274, score-0.342]
</p><p>32 Corollary 2 Under the same conditions as Theorem 1, consider an univariate kernel with eigenvalue decay µk ≃ (1/k)2α for some α > 1/2. [sent-280, score-0.398]
</p><p>33 Proof : As in the previous corollary, we need to compute the critical univariate rate νn . [sent-282, score-0.31]
</p><p>34 1− 1 √ Consequently, the critical univariate rate (8) satisﬁes the scaling ν2 ≍ νn 2α / n, or equivalently, n 2α ν2 ≍ n− 2α+1 . [sent-284, score-0.342]
</p><p>35 2  Central to our proof of the lower bounds is the metric entropy structure of the univariate reproducing kernel Hilbert spaces. [sent-289, score-0.636]
</p><p>36 The packing entropy is the simply the logarithm of the packing number, namely the quantity log M(ε; G , ρ), to which we also refer as the metric entropy. [sent-297, score-0.377]
</p><p>37 In this paper, we derive explicit minimax lower bounds for two different scalings of the univariate metric entropy. [sent-298, score-0.603]
</p><p>38 (12)  Function classes with metric entropy of this type include linear functions (for which m = k), univariate polynomials of degree k (for which m = k + 1), and more generally, any function space with ﬁnite VC-dimension (van der Vaart and Wellner, 1996). [sent-302, score-0.422]
</p><p>39 In fact, any RKHS in which the kernel eigenvalues decay at a rate k−2α have a metric entropy with this scaling (Carl and Stephani, 1990; Carl and Triebel, 1980). [sent-312, score-0.328]
</p><p>40 samples from the sparse additive model (5) with sparsity s ≤ d/4, there is an universal constant C > 0 such that: (a) For a univariate class H with logarithmic metric entropy (12) indexed by parameter m, we have MP (Fd,s,H ) ≥ C 398  m s log(d/s) . [sent-316, score-0.666]
</p><p>41 +s n n  M INIMAX -O PTIMAL R ATES FOR S PARSE A DDITIVE M ODELS  (b) For a univariate class H with polynomial metric entropy (13) indexed by α, we have s log(d/s) 1 +s n n  MP (Fd,s,H ) ≥ C  2α 2α+1  . [sent-317, score-0.388]
</p><p>42 , for which log d = O (log(d/s))), the combination of Theorem 2 with these corollaries identiﬁes the minimax rates up to constant factors. [sent-322, score-0.34]
</p><p>43 In quantitative terms, however, their rates are 3 looser than those given here; in particular, their bound includes a term of the order s log d , which is n larger than the bound in Theorem 1. [sent-328, score-0.373]
</p><p>44 Note that relative to optimal rates given here in The2α orem 2(b), this scaling is sub-optimal: more precisely, we either have log d < ( log d ) 2α+1 , when the n n 2α  2α  1 subset selection term dominates, or ( n ) 2α+1 < ( log d ) 2α+1 , when the s-dimensional estimation term n dominates. [sent-335, score-0.319]
</p><p>45 Finally, Koltchinskii and Yuan (2010) analyze the same estimator as the M-estimator (6), and for the case of polynomial metric entropy, establish the same rates Theorem 1, albeit under a global boundedness condition. [sent-337, score-0.427]
</p><p>46 In terms of rates obtained, they establish a convergence rate based on two terms as in Theorem 1, but with a pre-factor that depends on the global quantity B = sup f ∈Fd,s,H  f  ∞  =  sup sup | f (x)|,  f ∈Fd,s,H  x  assumed to be bounded independently of dimension and sparsity. [sent-341, score-0.557]
</p><p>47 Such types of global boundedness conditions are fairly standard in classical non-parametric estimation, where they have no effect on minimax rates. [sent-342, score-0.421]
</p><p>48 In sharp contrast, the analysis of this section shows that for sparse additive models in √ the regime s = Ω( n), such global boundedness can substantially speed up minimax rates, showing that the rates proven in KY are not minimax optimal for these classes. [sent-343, score-0.838]
</p><p>49 The following theorem provides sharper rates for the Sobolev case, in which each univariate Hilbert space has eigenvalues decaying as µk ≃ k−2α for some smoothness parameter α > 1/2. [sent-350, score-0.493]
</p><p>50 Our probabilistic bounds involve the quantity 1  s log(d/s) s α log s 1/4 , B( ) , n n  δn : = max  (14)  and our rates are stated in terms of the function log s(s−1/2α n1/(4α+2) )2α−1 , √ where it should be noted that KB (s, n) → 0 if s = Ω( n). [sent-351, score-0.326]
</p><p>51 With this notation, we have the following upper bound on the minimax risk over the function ∗ class Fd,s,H (B). [sent-352, score-0.365]
</p><p>52 An immediate consequence of ∗ Theorem 3 is that the minimax rates over the function class Fd,s,H (B) can be strictly faster than minimax rates for the class Fd,s,H , which does not impose global boundedness. [sent-360, score-0.592]
</p><p>53 Recall that the minimax lower bound from Theorem 2 (b) is based on the quantity MP (Fd,s,H ) : = C1 s  1 n  2α 2α+1  +  s log(d/s) n  2α  = C1 sn− 2α+1 1 + n−1/(2α+1) log(d/s) ,  for a universal constant C1 . [sent-361, score-0.42]
</p><p>54 As an alternative view of the differences, it can be noted that there are scalings of (n, s, d) for which the minimax rate MP (Fd,s,H ) over Fd,s,H is constant—that is, does not vanish as n → +∞—while the minimax rate ∗ MP (Fd,s,H (B)) does vanish. [sent-369, score-0.413]
</p><p>55 In contrast, under a global boundedness condition, Theorem 3 shows that √ ∗ the minimax rate is upper bounded as MP (Fd,s,H (B)) = O n−1/5 log n , which tends to zero. [sent-372, score-0.584]
</p><p>56 Thus, global boundedness is a stringent condition in the high-dimensional setting; in particular, the rates given in Theorem 3 of Koltchinskii and Yuan √ (2010) are not minimax optimal when s = Ω( n). [sent-374, score-0.501]
</p><p>57 Under this conditioning, the bound (17) simpliﬁes to: 1 ∆ 2  2 n  ≤  √ 1 n ∑ wi ∆(xi ) + sγn ∆ n i=1  n + λn  ∆  where we have applied the Cauchy-Schwarz inequality to write 402  n,1 + ρn  1 n  ∆ H ,1 ,  ∑n ∆(xi ) ≤ ∆ n . [sent-419, score-0.354]
</p><p>58 2 C ONTROLLING  THE  G AUSSIAN C OMPLEXITY T ERM  The following lemma provides control the Gaussian complexity term on the right-hand side of inequality (17) by bounding the Gaussian complexity for the univariate functions ∆ j , j = 1, 2, . [sent-422, score-0.435]
</p><p>59 Consequently, n if we deﬁne the event A (λn ) = ∩ j∈S A j (λn ), then this tail bound together with the union bound implies that P[A c (λn )] ≤ s c1 exp(−c2 nλ2 ) ≤ c1 exp(−c′ nλ2 ), 2 n n where we have used the fact that λn = Ω( ∆S  n,1  =  ∑  j∈S  ∆j  n  log s n ). [sent-458, score-0.44]
</p><p>60 Now, conditioned on the event A (λn ), we have  ≤ 2 ∑ ∆j  2 + sλn  √ ≤ 2 s ∆S  2 + sλn  Substituting this upper bound (22) on ∆S √ ∆ 2 ≤ 2 sλn ∆ n  (21)  j∈S  (22) √ ≤2 s ∆  2 + sλn . [sent-459, score-0.297]
</p><p>61 If one were to assume global boundedness of the multivariate functions f and f ∗ , as done in past work of Koltchinskii and Yuan (2010), then an upper bound on ∆ 2 of this form would directly follow from known results (e. [sent-463, score-0.404]
</p><p>62 2 Proof of Theorem 2 We now turn to the proof of the minimax lower bounds stated in Theorem 2. [sent-500, score-0.298]
</p><p>63 , Has’minskii, 1978; Yang and Barron, 1999; Yu, 1996) so as to obtain a lower bound on the minimax error MP (Fd,s,H ) in terms of the probability of error in a multi-way hypothesis testing. [sent-503, score-0.303]
</p><p>64 (29) log M  406  M INIMAX -O PTIMAL R ATES FOR S PARSE A DDITIVE M ODELS  The remainder of the proof consists of constructing appropriate packing sets of F , and obtaining good upper bounds on the mutual information term in the lower bound (29). [sent-524, score-0.452]
</p><p>65 From Lemma 4(b), our packing set satisﬁes log M ≥ C sm + s log(d/s) , so that so that the choice δ2 = C′ sm + s log(d/s) , for a suitably small C′ > 0, can be used to guarantee the n n 3 error bound P[Θ = Θ] ≥ 4 . [sent-557, score-0.335]
</p><p>66 The polynomial scaling (13) of the metric entropy guarantees that their conditions are satisﬁed, and we conclude that the minimax error is lower bounded by any δn > 0 such that nδ2 ≥ C log N(δn ; F , · 2 ). [sent-584, score-0.44]
</p><p>67 In order to establish upper bounds on the minimax rate in L2 (P)-error over Fd,s,H (B), we analyze a least-squares estimator—albeit not the same as the original M-estimator (6)—constrained to ∗ Fd,s,H (B), namely n  f ∈ arg  min ∗  f ∈Fd,s,H  ¯ ∑ (yi − yn − f (xi ))2 . [sent-589, score-0.416]
</p><p>68 (B)  (30)  i=1  Since our goal is to upper bound the minimax rate in L2 (P) error, it is sufﬁcient to upper bound the L2 (P)-norm of f − f ∗ where f is any solution to (30). [sent-590, score-0.539]
</p><p>69 Since | f ∗ (xi )| is at most B and wi is standard ¯ n i=1 Gaussian and independent, the random variable yi − f = f ∗ (xi ) + wi is sub-Gaussian with parameter √ B2 + 1. [sent-595, score-0.33]
</p><p>70 Discussion In this paper, we have studied estimation in the class of sparse additive models in which each univariate function lies within a reproducing kernel Hilbert space. [sent-623, score-0.559]
</p><p>71 In conjunction, Theorems 1 and 2 410  M INIMAX -O PTIMAL R ATES FOR S PARSE A DDITIVE M ODELS  provide a precise characterization of the minimax-optimal rates for estimating f ∗ in the L2 (P)-norm for various kernel classes with bounded univariate functions. [sent-624, score-0.488]
</p><p>72 In order to establish achievable rates, we analyzed a simple M-estimator based on regularizing the least-squares loss with two kinds of ℓ1 -based norms, one deﬁned by the univariate Hilbert norm and the other by the univariate empirical norm. [sent-626, score-0.578]
</p><p>73 Theorem 3 in our paper shows that the rates obtained under global boundedness conditions are not minimax optimal for Sobolev spaces in the regime √ s = Ω( n). [sent-631, score-0.501]
</p><p>74 Proof of Lemma 1 The proof of this lemma is based on peeling and weighting techniques from empirical process theory (Alexander, 1987; van de Geer, 2000) combined with results on the local Rademacher and Gaussian complexities of kernel classes (Bartlett et al. [sent-673, score-0.323]
</p><p>75 For each univariate Hilbert space H j = H , let us introduce the random variables Zn (w,t; H ) : = sup g j H ≤1 g j n ≤t  1 n ∑ wi g j (xi j ) , n i=1  and  Zn (w,t; H ) : = Ex  sup g j H ≤1 g j 2 ≤t  1 n ∑ wi g j (xi j ) , n i=1  (37) where wi ∼ N(0, 1) are i. [sent-675, score-0.973]
</p><p>76 Recall that the critical univariate rate νn is deﬁned in terms of the population Gaussian complexity (see Equation (8)). [sent-685, score-0.346]
</p><p>77 Otherwise we renormalize f j by deﬁning g j : = f j / f j H , and we write 1 n 1 n ∑ wi f j (xi j ) = f j H n ∑ wi g j (xi j ) ≤ n i=1 i=1  f j H Zn w; g j n , H ,  where the ﬁnal inequality uses the deﬁnition (37), and the fact that g j H = 1. [sent-725, score-0.407]
</p><p>78 n  (43)  Applying the bound (40) with t = r j and δ = γn , we are guaranteed an upper bound of the form Zn (w; r j ; H ) ≤ Qw,n (r j , H ) + r j γn with probability at least 1 − c1 exp − c2 nγ2 ). [sent-735, score-0.342]
</p><p>79 Since r j > γn > νn, j , we have  Qw,n (r j , H ) =  rj Ew νn, j  sup n ≤νn, j νn, gj H ≤ r j j  gj  1 n ∑ wi g j (xi j ) n i=1  ≤  rj Qw,n (νn, j , H ) ≤ 4 r j νn, j , νn, j  where the ﬁnal inequality uses the fact that Qw,n (νn, j , H ) ≤ 4 ν2 j . [sent-737, score-0.345]
</p><p>80 Now n n i=1 j deﬁne the event  T j (γn ) : = ∃ f j ∈ BH (1) | and the sets Sm : = 2m−1 γn ≤  1 n ∑ wi f j (xi j ) > 8 f j H γn n i=1 fj n fj H  fj n , and fj H  fj n ∈ (γn , 1] . [sent-744, score-1.682]
</p><p>81 Consequently, by union bound and the tail bound (43), we have ≤ c1 exp − c′ nγ2 2 n  P[T j (γn )] ≤ M c1 exp − c2 nγ2 n  by the condition nγ2 = Ω(log(1/γn )), which completes the proof. [sent-750, score-0.396]
</p><p>82 From 1 the inequality L (∆) ≤ L (0), we obtain the upper bound 2 ∆ 2 ≤ T1 + T2 , where n T1 : =  1 n 1 n ¯ ∑ wi ∆(xi ) + |yn − f | n ∑ ∆(xi ) , n i=1 i=1 d  T2 : = λn ∑  f j∗  j=1  ∗ n − fj +∆j  and  d  + ρn ∑  n  f j∗ H − f j∗ + ∆ j H . [sent-753, score-0.702]
</p><p>83 j=1  1 n  Conditioned on the event C (γn ), we have the bound |yn − f | ¯ 1 2 ≤ T + 1 n w ∆(x ) + √sγ ∆ , or equivalently n n 2 i 2 ∆ n n ∑i=1 i 0 ≤  1 2  ∆  n−  √ sγn  2  ≤ T2 +  ∑n ∆(xi ) ≤ i=1  √  sγn ∆ n , and hence  1 n 1 ∑ wi ∆(xi ) + 2 sγ2 . [sent-754, score-0.364]
</p><p>84 2 n  Recalling our conditioning on the event T (γn ), by Lemma 1, we have the upper bound 1 n ∑ wi ∆(xi )| ≤ 8 γn ∆ n i=1 416  2 n,1 + γn  ∆ H ,1 . [sent-759, score-0.472]
</p><p>85 1 in Einmahl and ˜ Mason, 1996) with t = δ2 /4 to obtain the one-sided tail bound n ˜ P[δ2 − n  ˜ ˜ 1 n 2 δ2 nδ4 n , g (xi ) ≥ n ] ≤ exp − ∑ n i=1 4 32E[g4 (x)]  (46)  where we used the upper bound var(g2 (x)) ≤ E[g4 (x)]. [sent-790, score-0.402]
</p><p>86 Combining this upper bound on E[g4 (x)] with the earlier tail bound (46) and applying union bound yields P[ max  k=1,2,. [sent-792, score-0.458]
</p><p>87 320  (47)  ˜ It remains to bound the covering entropy log Npr (δn /8, G ′ , · n ). [sent-796, score-0.301]
</p><p>88 Since the proper covering en˜ ˜ tropy log Npr (δn /8, G ′ , · n ) is at most log N(δn /16, G ′ , · n ), it sufﬁces to upper bound the usual covering entropy. [sent-797, score-0.426]
</p><p>89 ˜ Setting ε = δn /16 and performing some algebra, we obtain the upper bound 1 √ n  ˜ log N(δn /16; G ′ , ·  n)  ≤  1 n 64 Eε [ sup ∑ εi g(xi )]. [sent-807, score-0.346]
</p><p>90 , d}, the univariate Gaussian complexity is upper bounded as E  sup g j n ≤r j g j H ≤R j  1 n ∑ εi g j (xi j ) ≤ C γn r j + γ2 R j . [sent-812, score-0.469]
</p><p>91 In particular, any u ∈ S uniquely deﬁnes a function gu = ∑d g j j ∈ F , with elements j=1 u  fj j 0  u  gjj =  if u j = 0 otherwise. [sent-837, score-0.391]
</p><p>92 2 Proof of Part (b) 1 In order to prove part (b), we instead let N = M( 2 ; BH (1), · Since log N = Ω(m), we have the modiﬁed lower bound  log N ∗ = Ω s log  2 ) − 1, and then follow the same steps. [sent-852, score-0.319]
</p><p>93 d + sm , s  Moreover, instead of the lower bound (51), we have gu − hv  2 2  d  =  ∑  u  v  fj j − fj j  j=1  2 2  1 4  ≥  d  ∑ I[u j = v j ]  j=1  ≥  s , 8  using our previous result on the Hamming separation. [sent-853, score-0.918]
</p><p>94 Furthermore, since f j univariate function, we have the upper bound gu − hv  2 2  d  =  ∑  j=1  u  v  2 2  fj j − fj j  d  ≤  By the deﬁnition (50) of S, at most 2s of the terms v u construction we have f j j − f j j H ≤ 2, and hence  ∑  j=1  u fj j  u  2  ≤ f j H for any  v  fj j − fj j 2 . [sent-854, score-2.068]
</p><p>95 2 √ √ Finally, by rescaling the functions by 8 δ/ s, we obtain a class of N ∗ rescaled functions {gu , u ∈ I } such that gu − hv 2 ≥ δ2 , and gu − hv 2 ≤ 64δ2 , 2 2 as claimed. [sent-857, score-0.384]
</p><p>96 Adding up the k=1 j,k k=1 j,k bounds over all co-ordinates, we obtain s  a  1  =  ∑  M  ∑ |a j,k | ≤  j=1 k=1  √ s M ∑ fj  ∞  =  √ M f  ∞  j=1  ≤  √  MB,  where the ﬁnal step uses the uniform boundedness condition. [sent-871, score-0.522]
</p><p>97 2  Next we prove an upper bound on the expectations  Qw,n (t; H (S, 2B)) : = Ew  1 n ∑ wi g(xi ) , g∈H (S,2B) n i=1 sup  and  g n ≤t  Qw,n (t; H (S, 2B)) : = Ex,w  1 n ∑ wi g(xi ) . [sent-879, score-0.607]
</p><p>98 For any function g ∈ H (S, 2B), triangle inequality yields the upper bound 1 n 1 n | ∑ wi Φ(xi ), a | ≤ sup | ∑ wi Φ·,1:M (xi ), a·,1:M +A2 g∈2H (S,2B) n i=1 g∈2H (S,2B) n i=1 sup  (52)  A1  where A2 : = supg∈2H (S,2B) 1 | ∑n wi Φ·,M+1:∞ (xi ), a·,M+1:∞ |. [sent-893, score-0.952]
</p><p>99 1 B OUNDING  THE  Q UANTITIES Ex,w [A1 ] AND Ew [A1 ]  By H¨ lder’s inequality and Lemma 10, we have o 1 A1 ≤ √ sup a·,1:M n g∈2H (S,2B)  √ n wi wi 2B M √ max | ∑ √ Φ j,k (xi )|. [sent-896, score-0.51]
</p><p>100 2 B OUNDING  THE  Q UANTITIES Ex,w [A2 ] AND Ew [A2 ]  In order to control this term, we simply recognize that it corresponds to the usual Gaussian complexity of the sum of 2s univariate Hilbert spaces, each of which is an RKHS truncated to the eigenfunctions {µk }k≥M+1 . [sent-902, score-0.303]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fj', 0.286), ('univariate', 0.272), ('askutti', 0.235), ('dditive', 0.235), ('inimax', 0.201), ('minimax', 0.191), ('zn', 0.187), ('ates', 0.181), ('boundedness', 0.18), ('ptimal', 0.167), ('wi', 0.165), ('wainwright', 0.154), ('hilbert', 0.154), ('bh', 0.131), ('odels', 0.117), ('parse', 0.113), ('bound', 0.112), ('additive', 0.109), ('gu', 0.105), ('sup', 0.103), ('xi', 0.102), ('ew', 0.101), ('rkhs', 0.097), ('kb', 0.095), ('mp', 0.09), ('event', 0.087), ('hv', 0.087), ('npr', 0.087), ('koltchinskii', 0.086), ('lemma', 0.086), ('sobolev', 0.086), ('geer', 0.085), ('rates', 0.08), ('inequality', 0.077), ('pn', 0.076), ('yn', 0.073), ('reproducing', 0.071), ('kernel', 0.07), ('packing', 0.07), ('log', 0.069), ('sparsity', 0.067), ('meier', 0.067), ('gs', 0.066), ('universal', 0.065), ('yuan', 0.065), ('entropy', 0.063), ('upper', 0.062), ('ky', 0.062), ('barron', 0.062), ('negahban', 0.062), ('tail', 0.06), ('covering', 0.057), ('exp', 0.056), ('bounds', 0.056), ('decay', 0.056), ('theorem', 0.055), ('sc', 0.054), ('eigenvalues', 0.054), ('metric', 0.053), ('quantity', 0.052), ('proof', 0.051), ('global', 0.05), ('wg', 0.05), ('qk', 0.047), ('fano', 0.046), ('conditioning', 0.046), ('peeling', 0.044), ('einmahl', 0.042), ('ravikumar', 0.042), ('sm', 0.042), ('consequently', 0.04), ('critical', 0.038), ('rademacher', 0.038), ('van', 0.038), ('minskii', 0.037), ('ontrolling', 0.037), ('sparse', 0.037), ('ex', 0.036), ('conditioned', 0.036), ('nt', 0.036), ('population', 0.036), ('norms', 0.035), ('ledoux', 0.035), ('raskutti', 0.035), ('pisier', 0.035), ('mendelson', 0.035), ('classes', 0.034), ('establish', 0.034), ('ak', 0.033), ('chung', 0.033), ('scaling', 0.032), ('smoothness', 0.032), ('mutual', 0.032), ('minoration', 0.032), ('bounded', 0.032), ('eigenfunctions', 0.031), ('scalings', 0.031), ('carl', 0.031), ('yang', 0.031), ('estimator', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="67-tfidf-1" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>Author: Garvesh Raskutti, Martin J. Wainwright, Bin Yu</p><p>Abstract: Sparse additive models are families of d-variate functions with the additive decomposition f ∗ = ∑ j∈S f j∗ , where S is an unknown subset of cardinality s ≪ d. In this paper, we consider the case where each univariate component function f j∗ lies in a reproducing kernel Hilbert space (RKHS), and analyze a method for estimating the unknown function f ∗ based on kernels combined with ℓ1 -type convex regularization. Working within a high-dimensional framework that allows both the dimension d and sparsity s to increase with n, we derive convergence rates in the L2 (P) and L2 (Pn ) norms over the class Fd,s,H of sparse additive models with each univariate function f j∗ in the unit ball of a univariate RKHS with bounded kernel function. We complement our upper bounds by deriving minimax lower bounds on the L2 (P) error, thereby showing the optimality of our method. Thus, we obtain optimal minimax rates for many interesting classes of sparse additive models, including polynomials, splines, and Sobolev classes. We also show that if, in contrast to our univariate conditions, the d-variate function class is assumed to be globally bounded, then much √ faster estimation rates are possible for any sparsity s = Ω( n), showing that global boundedness is a signiﬁcant restriction in the high-dimensional setting. Keywords: sparsity, kernel, non-parametric, convex, minimax</p><p>2 0.14539506 <a title="67-tfidf-2" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>Author: Sahand Negahban, Martin J. Wainwright</p><p>Abstract: We consider the matrix completion problem under a form of row/column weighted entrywise sampling, including the case of uniform entrywise sampling as a special case. We analyze the associated random observation operator, and prove that with high probability, it satisﬁes a form of restricted strong convexity with respect to weighted Frobenius norm. Using this property, we obtain as corollaries a number of error bounds on matrix completion in the weighted Frobenius norm under noisy sampling and for both exact and near low-rank matrices. Our results are based on measures of the “spikiness” and “low-rankness” of matrices that are less restrictive than the incoherence conditions imposed in previous work. Our technique involves an M-estimator that includes controls on both the rank and spikiness of the solution, and we establish non-asymptotic error bounds in weighted Frobenius norm for recovering matrices lying with ℓq -“balls” of bounded spikiness. Using information-theoretic methods, we show that no algorithm can achieve better estimates (up to a logarithmic factor) over these same sets, showing that our conditions on matrices and associated rates are essentially optimal. Keywords: matrix completion, collaborative ﬁltering, convex optimization</p><p>3 0.11189838 <a title="67-tfidf-3" href="./jmlr-2012-Minimax_Manifold_Estimation.html">68 jmlr-2012-Minimax Manifold Estimation</a></p>
<p>Author: Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli, Larry Wasserman</p><p>Abstract: We ﬁnd the minimax rate of convergence in Hausdorff distance for estimating a manifold M of dimension d embedded in RD given a noisy sample from the manifold. Under certain conditions, we show that the optimal rate of convergence is n−2/(2+d) . Thus, the minimax rate depends only on the dimension of the manifold, not on the dimension of the space in which M is embedded. Keywords: manifold learning, minimax estimation</p><p>4 0.1045379 <a title="67-tfidf-4" href="./jmlr-2012-On_the_Convergence_Rate_oflp-Norm_Multiple_Kernel_Learning.html">81 jmlr-2012-On the Convergence Rate oflp-Norm Multiple Kernel Learning</a></p>
<p>Author: Marius Kloft, Gilles Blanchard</p><p>Abstract: We derive an upper bound on the local Rademacher complexity of ℓ p -norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p = 1 only while our analysis covers all cases 1 ≤ p ≤ ∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding excess loss, namely α fast convergence rates of the order O(n− 1+α ), where α is the minimum eigenvalue decay rate of the individual kernels. Keywords: multiple kernel learning, learning kernels, generalization bounds, local Rademacher complexity</p><p>5 0.10380727 <a title="67-tfidf-5" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>Author: Wojciech Rejchel</p><p>Abstract: The problem of ranking is to predict or to guess the ordering between objects on the basis of their observed features. In this paper we consider ranking estimators that minimize the empirical convex risk. We prove generalization bounds for the excess risk of such estimators with rates that are 1 faster than √n . We apply our results to commonly used ranking algorithms, for instance boosting or support vector machines. Moreover, we study the performance of considered estimators on real data sets. Keywords: convex risk minimization, excess risk, support vector machine, empirical process, U-process</p><p>6 0.09916544 <a title="67-tfidf-6" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>7 0.097818851 <a title="67-tfidf-7" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>8 0.095088296 <a title="67-tfidf-8" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>9 0.091413528 <a title="67-tfidf-9" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>10 0.08950384 <a title="67-tfidf-10" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<p>11 0.086650781 <a title="67-tfidf-11" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>12 0.08437907 <a title="67-tfidf-12" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>13 0.081382789 <a title="67-tfidf-13" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>14 0.081303738 <a title="67-tfidf-14" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>15 0.077304088 <a title="67-tfidf-15" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>16 0.075149268 <a title="67-tfidf-16" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>17 0.0748519 <a title="67-tfidf-17" href="./jmlr-2012-Refinement_of_Operator-valued_Reproducing_Kernels.html">96 jmlr-2012-Refinement of Operator-valued Reproducing Kernels</a></p>
<p>18 0.066526726 <a title="67-tfidf-18" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>19 0.064299665 <a title="67-tfidf-19" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>20 0.062878735 <a title="67-tfidf-20" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.308), (1, 0.129), (2, -0.171), (3, 0.064), (4, 0.018), (5, -0.063), (6, -0.086), (7, -0.04), (8, -0.019), (9, -0.071), (10, 0.021), (11, -0.04), (12, -0.016), (13, 0.068), (14, 0.025), (15, -0.02), (16, 0.005), (17, -0.251), (18, -0.105), (19, 0.018), (20, 0.115), (21, -0.061), (22, 0.126), (23, -0.019), (24, -0.095), (25, -0.034), (26, -0.143), (27, -0.04), (28, -0.21), (29, -0.142), (30, -0.002), (31, 0.056), (32, 0.105), (33, 0.066), (34, -0.155), (35, -0.064), (36, 0.048), (37, -0.034), (38, -0.022), (39, 0.073), (40, -0.113), (41, 0.081), (42, -0.048), (43, 0.122), (44, -0.027), (45, 0.036), (46, 0.009), (47, 0.033), (48, 0.128), (49, -0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93457794 <a title="67-lsi-1" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>Author: Garvesh Raskutti, Martin J. Wainwright, Bin Yu</p><p>Abstract: Sparse additive models are families of d-variate functions with the additive decomposition f ∗ = ∑ j∈S f j∗ , where S is an unknown subset of cardinality s ≪ d. In this paper, we consider the case where each univariate component function f j∗ lies in a reproducing kernel Hilbert space (RKHS), and analyze a method for estimating the unknown function f ∗ based on kernels combined with ℓ1 -type convex regularization. Working within a high-dimensional framework that allows both the dimension d and sparsity s to increase with n, we derive convergence rates in the L2 (P) and L2 (Pn ) norms over the class Fd,s,H of sparse additive models with each univariate function f j∗ in the unit ball of a univariate RKHS with bounded kernel function. We complement our upper bounds by deriving minimax lower bounds on the L2 (P) error, thereby showing the optimality of our method. Thus, we obtain optimal minimax rates for many interesting classes of sparse additive models, including polynomials, splines, and Sobolev classes. We also show that if, in contrast to our univariate conditions, the d-variate function class is assumed to be globally bounded, then much √ faster estimation rates are possible for any sparsity s = Ω( n), showing that global boundedness is a signiﬁcant restriction in the high-dimensional setting. Keywords: sparsity, kernel, non-parametric, convex, minimax</p><p>2 0.61039042 <a title="67-lsi-2" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>Author: Sahand Negahban, Martin J. Wainwright</p><p>Abstract: We consider the matrix completion problem under a form of row/column weighted entrywise sampling, including the case of uniform entrywise sampling as a special case. We analyze the associated random observation operator, and prove that with high probability, it satisﬁes a form of restricted strong convexity with respect to weighted Frobenius norm. Using this property, we obtain as corollaries a number of error bounds on matrix completion in the weighted Frobenius norm under noisy sampling and for both exact and near low-rank matrices. Our results are based on measures of the “spikiness” and “low-rankness” of matrices that are less restrictive than the incoherence conditions imposed in previous work. Our technique involves an M-estimator that includes controls on both the rank and spikiness of the solution, and we establish non-asymptotic error bounds in weighted Frobenius norm for recovering matrices lying with ℓq -“balls” of bounded spikiness. Using information-theoretic methods, we show that no algorithm can achieve better estimates (up to a logarithmic factor) over these same sets, showing that our conditions on matrices and associated rates are essentially optimal. Keywords: matrix completion, collaborative ﬁltering, convex optimization</p><p>3 0.55594391 <a title="67-lsi-3" href="./jmlr-2012-On_the_Convergence_Rate_oflp-Norm_Multiple_Kernel_Learning.html">81 jmlr-2012-On the Convergence Rate oflp-Norm Multiple Kernel Learning</a></p>
<p>Author: Marius Kloft, Gilles Blanchard</p><p>Abstract: We derive an upper bound on the local Rademacher complexity of ℓ p -norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p = 1 only while our analysis covers all cases 1 ≤ p ≤ ∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding excess loss, namely α fast convergence rates of the order O(n− 1+α ), where α is the minimum eigenvalue decay rate of the individual kernels. Keywords: multiple kernel learning, learning kernels, generalization bounds, local Rademacher complexity</p><p>4 0.5309878 <a title="67-lsi-4" href="./jmlr-2012-Minimax_Manifold_Estimation.html">68 jmlr-2012-Minimax Manifold Estimation</a></p>
<p>Author: Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli, Larry Wasserman</p><p>Abstract: We ﬁnd the minimax rate of convergence in Hausdorff distance for estimating a manifold M of dimension d embedded in RD given a noisy sample from the manifold. Under certain conditions, we show that the optimal rate of convergence is n−2/(2+d) . Thus, the minimax rate depends only on the dimension of the manifold, not on the dimension of the space in which M is embedded. Keywords: manifold learning, minimax estimation</p><p>5 0.52927727 <a title="67-lsi-5" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>Author: Sivan Sabato, Naftali Tishby</p><p>Abstract: In the supervised learning setting termed Multiple-Instance Learning (MIL), the examples are bags of instances, and the bag label is a function of the labels of its instances. Typically, this function is the Boolean OR. The learner observes a sample of bags and the bag labels, but not the instance labels that determine the bag labels. The learner is then required to emit a classiﬁcation rule for bags based on the sample. MIL has numerous applications, and many heuristic algorithms have been used successfully on this problem, each adapted to speciﬁc settings or applications. In this work we provide a uniﬁed theoretical analysis for MIL, which holds for any underlying hypothesis class, regardless of a speciﬁc application or problem domain. We show that the sample complexity of MIL is only poly-logarithmically dependent on the size of the bag, for any underlying hypothesis class. In addition, we introduce a new PAC-learning algorithm for MIL, which uses a regular supervised learning algorithm as an oracle. We prove that efﬁcient PAC-learning for MIL can be generated from any efﬁcient non-MIL supervised learning algorithm that handles one-sided error. The computational complexity of the resulting algorithm is only polynomially dependent on the bag size. Keywords: multiple-instance learning, learning theory, sample complexity, PAC learning, supervised classiﬁcation</p><p>6 0.52313578 <a title="67-lsi-6" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>7 0.50254947 <a title="67-lsi-7" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>8 0.49394903 <a title="67-lsi-8" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>9 0.45069873 <a title="67-lsi-9" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>10 0.43864322 <a title="67-lsi-10" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>11 0.43708688 <a title="67-lsi-11" href="./jmlr-2012-Estimation_and_Selection_via_Absolute_Penalized_Convex_Minimization_And_Its_Multistage_Adaptive_Applications.html">39 jmlr-2012-Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications</a></p>
<p>12 0.42869782 <a title="67-lsi-12" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>13 0.42386913 <a title="67-lsi-13" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<p>14 0.41229111 <a title="67-lsi-14" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>15 0.41077983 <a title="67-lsi-15" href="./jmlr-2012-Entropy_Search_for_Information-Efficient_Global_Optimization.html">38 jmlr-2012-Entropy Search for Information-Efficient Global Optimization</a></p>
<p>16 0.3992607 <a title="67-lsi-16" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>17 0.38012263 <a title="67-lsi-17" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>18 0.37777364 <a title="67-lsi-18" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>19 0.37408355 <a title="67-lsi-19" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>20 0.356141 <a title="67-lsi-20" href="./jmlr-2012-Robust_Kernel_Density_Estimation.html">100 jmlr-2012-Robust Kernel Density Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.016), (21, 0.048), (26, 0.169), (29, 0.042), (49, 0.028), (75, 0.046), (77, 0.019), (79, 0.033), (92, 0.218), (96, 0.084), (99, 0.173)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89297235 <a title="67-lda-1" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>Author: Garvesh Raskutti, Martin J. Wainwright, Bin Yu</p><p>Abstract: Sparse additive models are families of d-variate functions with the additive decomposition f ∗ = ∑ j∈S f j∗ , where S is an unknown subset of cardinality s ≪ d. In this paper, we consider the case where each univariate component function f j∗ lies in a reproducing kernel Hilbert space (RKHS), and analyze a method for estimating the unknown function f ∗ based on kernels combined with ℓ1 -type convex regularization. Working within a high-dimensional framework that allows both the dimension d and sparsity s to increase with n, we derive convergence rates in the L2 (P) and L2 (Pn ) norms over the class Fd,s,H of sparse additive models with each univariate function f j∗ in the unit ball of a univariate RKHS with bounded kernel function. We complement our upper bounds by deriving minimax lower bounds on the L2 (P) error, thereby showing the optimality of our method. Thus, we obtain optimal minimax rates for many interesting classes of sparse additive models, including polynomials, splines, and Sobolev classes. We also show that if, in contrast to our univariate conditions, the d-variate function class is assumed to be globally bounded, then much √ faster estimation rates are possible for any sparsity s = Ω( n), showing that global boundedness is a signiﬁcant restriction in the high-dimensional setting. Keywords: sparsity, kernel, non-parametric, convex, minimax</p><p>2 0.84899282 <a title="67-lda-2" href="./jmlr-2012-Sparse_and_Unique_Nonnegative_Matrix_Factorization_Through_Data_Preprocessing.html">108 jmlr-2012-Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing</a></p>
<p>Author: Nicolas Gillis</p><p>Abstract: Nonnegative matrix factorization (NMF) has become a very popular technique in machine learning because it automatically extracts meaningful features through a sparse and part-based representation. However, NMF has the drawback of being highly ill-posed, that is, there typically exist many different but equivalent factorizations. In this paper, we introduce a completely new way to obtaining more well-posed NMF problems whose solutions are sparser. Our technique is based on the preprocessing of the nonnegative input data matrix, and relies on the theory of M-matrices and the geometric interpretation of NMF. This approach provably leads to optimal and sparse solutions under the separability assumption of Donoho and Stodden (2003), and, for rank-three matrices, makes the number of exact factorizations ﬁnite. We illustrate the effectiveness of our technique on several image data sets. Keywords: nonnegative matrix factorization, data preprocessing, uniqueness, sparsity, inversepositive matrices</p><p>3 0.78769612 <a title="67-lda-3" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>Author: Michael U. Gutmann, Aapo Hyvärinen</p><p>Abstract: We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a ﬁnite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only speciﬁed up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artiﬁcially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities. Keywords: statistics unnormalized models, partition function, computation, estimation, natural image</p><p>4 0.77506936 <a title="67-lda-4" href="./jmlr-2012-Dynamic_Policy_Programming.html">34 jmlr-2012-Dynamic Policy Programming</a></p>
<p>Author: Mohammad Gheshlaghi Azar, Vicenç Gómez, Hilbert J. Kappen</p><p>Abstract: In this paper, we propose a novel policy iteration method, called dynamic policy programming (DPP), to estimate the optimal policy in the inﬁnite-horizon Markov decision processes. DPP is an incremental algorithm that forces a gradual change in policy update. This allows us to prove ﬁnite-iteration and asymptotic ℓ∞ -norm performance-loss bounds in the presence of approximation/estimation error which depend on the average accumulated error as opposed to the standard bounds which are expressed in terms of the supremum of the errors. The dependency on the average error is important in problems with limited number of samples per iteration, for which the average of the errors can be signiﬁcantly smaller in size than the supremum of the errors. Based on these theoretical results, we prove that a sampling-based variant of DPP (DPP-RL) asymptotically converges to the optimal policy. Finally, we illustrate numerically the applicability of these results on some benchmark problems and compare the performance of the approximate variants of DPP with some existing reinforcement learning (RL) methods. Keywords: approximate dynamic programming, reinforcement learning, Markov decision processes, Monte-Carlo methods, function approximation</p><p>5 0.76337165 <a title="67-lda-5" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>Author: Andreas Maurer, Massimiliano Pontil</p><p>Abstract: We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an inﬁnite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels. Keywords: empirical processes, Rademacher average, sparse estimation.</p><p>6 0.75368357 <a title="67-lda-6" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>7 0.74872714 <a title="67-lda-7" href="./jmlr-2012-Finite-Sample_Analysis_of_Least-Squares_Policy_Iteration.html">46 jmlr-2012-Finite-Sample Analysis of Least-Squares Policy Iteration</a></p>
<p>8 0.74109405 <a title="67-lda-8" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>9 0.73961675 <a title="67-lda-9" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>10 0.73845029 <a title="67-lda-10" href="./jmlr-2012-Integrating_a_Partial_Model_into_Model_Free_Reinforcement_Learning.html">51 jmlr-2012-Integrating a Partial Model into Model Free Reinforcement Learning</a></p>
<p>11 0.73742729 <a title="67-lda-11" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>12 0.73688567 <a title="67-lda-12" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>13 0.73643976 <a title="67-lda-13" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>14 0.73579311 <a title="67-lda-14" href="./jmlr-2012-Characterization_and_Greedy_Learning_of_Interventional_Markov_Equivalence_Classes_of_Directed_Acyclic_Graphs.html">25 jmlr-2012-Characterization and Greedy Learning of Interventional Markov Equivalence Classes of Directed Acyclic Graphs</a></p>
<p>15 0.73309284 <a title="67-lda-15" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>16 0.7329461 <a title="67-lda-16" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>17 0.73061287 <a title="67-lda-17" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<p>18 0.72946453 <a title="67-lda-18" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>19 0.72886592 <a title="67-lda-19" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>20 0.72880906 <a title="67-lda-20" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
