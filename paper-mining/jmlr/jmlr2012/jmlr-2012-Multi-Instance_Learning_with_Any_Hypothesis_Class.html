<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-71" href="#">jmlr2012-71</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</h1>
<br/><p>Source: <a title="jmlr-2012-71-pdf" href="http://jmlr.org/papers/volume13/sabato12a/sabato12a.pdf">pdf</a></p><p>Author: Sivan Sabato, Naftali Tishby</p><p>Abstract: In the supervised learning setting termed Multiple-Instance Learning (MIL), the examples are bags of instances, and the bag label is a function of the labels of its instances. Typically, this function is the Boolean OR. The learner observes a sample of bags and the bag labels, but not the instance labels that determine the bag labels. The learner is then required to emit a classiﬁcation rule for bags based on the sample. MIL has numerous applications, and many heuristic algorithms have been used successfully on this problem, each adapted to speciﬁc settings or applications. In this work we provide a uniﬁed theoretical analysis for MIL, which holds for any underlying hypothesis class, regardless of a speciﬁc application or problem domain. We show that the sample complexity of MIL is only poly-logarithmically dependent on the size of the bag, for any underlying hypothesis class. In addition, we introduce a new PAC-learning algorithm for MIL, which uses a regular supervised learning algorithm as an oracle. We prove that efﬁcient PAC-learning for MIL can be generated from any efﬁcient non-MIL supervised learning algorithm that handles one-sided error. The computational complexity of the resulting algorithm is only polynomially dependent on the bag size. Keywords: multiple-instance learning, learning theory, sample complexity, PAC learning, supervised classiﬁcation</p><p>Reference: <a title="jmlr-2012-71-reference" href="../jmlr2012_reference/jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bag', 0.664), ('mil', 0.313), ('milearn', 0.227), ('hypothes', 0.203), ('ln', 0.176), ('abato', 0.151), ('isbhy', 0.151), ('nstance', 0.151), ('ypothes', 0.151), ('zb', 0.142), ('bool', 0.139), ('dr', 0.139), ('fat', 0.137), ('wi', 0.105), ('sx', 0.087), ('rc', 0.079), ('zn', 0.076), ('hpo', 0.076), ('ult', 0.075), ('nm', 0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="71-tfidf-1" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>2 0.15457833 <a title="71-tfidf-2" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>3 0.114476 <a title="71-tfidf-3" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>4 0.10152158 <a title="71-tfidf-4" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>5 0.09816318 <a title="71-tfidf-5" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>6 0.093858324 <a title="71-tfidf-6" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>7 0.078603841 <a title="71-tfidf-7" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>8 0.071356587 <a title="71-tfidf-8" href="./jmlr-2012-Activized_Learning%3A_Transforming_Passive_to_Active_with_Improved_Label_Complexity.html">14 jmlr-2012-Activized Learning: Transforming Passive to Active with Improved Label Complexity</a></p>
<p>9 0.063502178 <a title="71-tfidf-9" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>10 0.061300471 <a title="71-tfidf-10" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>11 0.058038455 <a title="71-tfidf-11" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>12 0.053572979 <a title="71-tfidf-12" href="./jmlr-2012-On_the_Convergence_Rate_oflp-Norm_Multiple_Kernel_Learning.html">81 jmlr-2012-On the Convergence Rate oflp-Norm Multiple Kernel Learning</a></p>
<p>13 0.053284708 <a title="71-tfidf-13" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>14 0.052925829 <a title="71-tfidf-14" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>15 0.045965269 <a title="71-tfidf-15" href="./jmlr-2012-Fast_Approximation_of_Matrix_Coherence_and_Statistical_Leverage.html">43 jmlr-2012-Fast Approximation of Matrix Coherence and Statistical Leverage</a></p>
<p>16 0.045475859 <a title="71-tfidf-16" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>17 0.041613262 <a title="71-tfidf-17" href="./jmlr-2012-GPLP%3A_A_Local_and_Parallel_Computation_Toolbox_for_Gaussian_Process_Regression.html">47 jmlr-2012-GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression</a></p>
<p>18 0.041549422 <a title="71-tfidf-18" href="./jmlr-2012-Variable_Selection_in_High-dimensional_Varying-coefficient_Models_with_Global_Optimality.html">117 jmlr-2012-Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality</a></p>
<p>19 0.041520834 <a title="71-tfidf-19" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<p>20 0.037858956 <a title="71-tfidf-20" href="./jmlr-2012-PAC-Bayes_Bounds_with_Data_Dependent_Priors.html">87 jmlr-2012-PAC-Bayes Bounds with Data Dependent Priors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.215), (1, -0.062), (2, 0.082), (3, -0.104), (4, 0.119), (5, 0.156), (6, 0.084), (7, 0.144), (8, 0.022), (9, 0.031), (10, -0.03), (11, 0.068), (12, 0.052), (13, -0.112), (14, -0.185), (15, 0.062), (16, -0.119), (17, 0.08), (18, 0.129), (19, -0.005), (20, 0.048), (21, 0.017), (22, -0.018), (23, -0.096), (24, 0.013), (25, -0.096), (26, 0.058), (27, 0.072), (28, 0.056), (29, -0.136), (30, -0.101), (31, 0.005), (32, 0.049), (33, 0.054), (34, 0.089), (35, -0.103), (36, -0.032), (37, -0.121), (38, 0.072), (39, -0.223), (40, -0.119), (41, 0.131), (42, 0.154), (43, -0.019), (44, 0.103), (45, 0.057), (46, -0.082), (47, -0.047), (48, -0.105), (49, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93628478 <a title="71-lsi-1" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>2 0.69106549 <a title="71-lsi-2" href="./jmlr-2012-On_the_Necessity_of_Irrelevant_Variables.html">82 jmlr-2012-On the Necessity of Irrelevant Variables</a></p>
<p>3 0.54405534 <a title="71-lsi-3" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>4 0.43190742 <a title="71-lsi-4" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>5 0.4236753 <a title="71-lsi-5" href="./jmlr-2012-On_Ranking_and_Generalization_Bounds.html">80 jmlr-2012-On Ranking and Generalization Bounds</a></p>
<p>6 0.41871679 <a title="71-lsi-6" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>7 0.40403756 <a title="71-lsi-7" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>8 0.39328325 <a title="71-lsi-8" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>9 0.37436622 <a title="71-lsi-9" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>10 0.36479703 <a title="71-lsi-10" href="./jmlr-2012-Activized_Learning%3A_Transforming_Passive_to_Active_with_Improved_Label_Complexity.html">14 jmlr-2012-Activized Learning: Transforming Passive to Active with Improved Label Complexity</a></p>
<p>11 0.3158145 <a title="71-lsi-11" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<p>12 0.31339532 <a title="71-lsi-12" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>13 0.30421668 <a title="71-lsi-13" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>14 0.30361751 <a title="71-lsi-14" href="./jmlr-2012-Variational_Multinomial_Logit_Gaussian_Process.html">118 jmlr-2012-Variational Multinomial Logit Gaussian Process</a></p>
<p>15 0.2978619 <a title="71-lsi-15" href="./jmlr-2012-On_the_Convergence_Rate_oflp-Norm_Multiple_Kernel_Learning.html">81 jmlr-2012-On the Convergence Rate oflp-Norm Multiple Kernel Learning</a></p>
<p>16 0.29534256 <a title="71-lsi-16" href="./jmlr-2012-PAC-Bayes_Bounds_with_Data_Dependent_Priors.html">87 jmlr-2012-PAC-Bayes Bounds with Data Dependent Priors</a></p>
<p>17 0.2622095 <a title="71-lsi-17" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>18 0.24464718 <a title="71-lsi-18" href="./jmlr-2012-Linear_Regression_With_Random_Projections.html">59 jmlr-2012-Linear Regression With Random Projections</a></p>
<p>19 0.23031151 <a title="71-lsi-19" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>20 0.22713344 <a title="71-lsi-20" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(20, 0.016), (28, 0.058), (38, 0.022), (48, 0.075), (50, 0.028), (67, 0.102), (69, 0.032), (71, 0.389), (81, 0.049), (82, 0.031), (90, 0.039), (94, 0.011), (95, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.61043394 <a title="71-lda-1" href="./jmlr-2012-Multi-Instance_Learning_with_Any_Hypothesis_Class.html">71 jmlr-2012-Multi-Instance Learning with Any Hypothesis Class</a></p>
<p>2 0.36365211 <a title="71-lda-2" href="./jmlr-2012-Active_Learning_via_Perfect_Selective_Classification.html">13 jmlr-2012-Active Learning via Perfect Selective Classification</a></p>
<p>3 0.35780019 <a title="71-lda-3" href="./jmlr-2012-Activized_Learning%3A_Transforming_Passive_to_Active_with_Improved_Label_Complexity.html">14 jmlr-2012-Activized Learning: Transforming Passive to Active with Improved Label Complexity</a></p>
<p>4 0.35548213 <a title="71-lda-4" href="./jmlr-2012-Analysis_of_a_Random_Forests_Model.html">20 jmlr-2012-Analysis of a Random Forests Model</a></p>
<p>5 0.35326141 <a title="71-lda-5" href="./jmlr-2012-Plug-in_Approach_to_Active_Learning.html">91 jmlr-2012-Plug-in Approach to Active Learning</a></p>
<p>6 0.35154238 <a title="71-lda-6" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>7 0.34647375 <a title="71-lda-7" href="./jmlr-2012-On_the_Convergence_Rate_oflp-Norm_Multiple_Kernel_Learning.html">81 jmlr-2012-On the Convergence Rate oflp-Norm Multiple Kernel Learning</a></p>
<p>8 0.34582254 <a title="71-lda-8" href="./jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</a></p>
<p>9 0.34356934 <a title="71-lda-9" href="./jmlr-2012-Finite-Sample_Analysis_of_Least-Squares_Policy_Iteration.html">46 jmlr-2012-Finite-Sample Analysis of Least-Squares Policy Iteration</a></p>
<p>10 0.34328693 <a title="71-lda-10" href="./jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</a></p>
<p>11 0.34262916 <a title="71-lda-11" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>12 0.33963421 <a title="71-lda-12" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>13 0.33810708 <a title="71-lda-13" href="./jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</a></p>
<p>14 0.33750749 <a title="71-lda-14" href="./jmlr-2012-Structured_Sparsity_and_Generalization.html">111 jmlr-2012-Structured Sparsity and Generalization</a></p>
<p>15 0.33735588 <a title="71-lda-15" href="./jmlr-2012-Online_Submodular_Minimization.html">84 jmlr-2012-Online Submodular Minimization</a></p>
<p>16 0.33601362 <a title="71-lda-16" href="./jmlr-2012-Consistent_Model_Selection_Criteria_on_High_Dimensions.html">29 jmlr-2012-Consistent Model Selection Criteria on High Dimensions</a></p>
<p>17 0.33598918 <a title="71-lda-17" href="./jmlr-2012-A_Comparison_of_the_Lasso_and__Marginal_Regression.html">2 jmlr-2012-A Comparison of the Lasso and  Marginal Regression</a></p>
<p>18 0.33487448 <a title="71-lda-18" href="./jmlr-2012-A_Primal-Dual_Convergence_Analysis_of_Boosting.html">8 jmlr-2012-A Primal-Dual Convergence Analysis of Boosting</a></p>
<p>19 0.33473608 <a title="71-lda-19" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>20 0.3347244 <a title="71-lda-20" href="./jmlr-2012-Stability_of_Density-Based_Clustering.html">109 jmlr-2012-Stability of Density-Based Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
