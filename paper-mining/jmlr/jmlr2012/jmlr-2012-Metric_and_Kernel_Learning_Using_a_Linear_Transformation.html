<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-66" href="#">jmlr2012-66</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</h1>
<br/><p>Source: <a title="jmlr-2012-66-pdf" href="http://jmlr.org/papers/volume13/jain12a/jain12a.pdf">pdf</a></p><p>Author: Prateek Jain, Brian Kulis, Jason V. Davis, Inderjit S. Dhillon</p><p>Abstract: Metric and kernel learning arise in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem. In particular, we propose a general optimization framework for learning metrics via linear transformations, and analyze in detail a special case of our framework—that of minimizing the LogDet divergence subject to linear constraints. We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework. Our theoretical connections between metric and kernel learning have two main consequences: 1) the learned kernel matrix parameterizes a linear transformation kernel function and can be applied inductively to new data points, 2) our result yields a constructive method for kernelizing most existing Mahalanobis metric learning formulations. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction. Keywords: divergence metric learning, kernel learning, linear transformation, matrix divergences, logdet</p><p>Reference: <a title="jmlr-2012-66-reference" href="../jmlr2012_reference/jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. [sent-12, score-0.573]
</p><p>2 In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem. [sent-13, score-0.603]
</p><p>3 We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework. [sent-15, score-0.367]
</p><p>4 We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction. [sent-17, score-0.287]
</p><p>5 Keywords: divergence  metric learning, kernel learning, linear transformation, matrix divergences, logdet  1. [sent-18, score-1.156]
</p><p>6 One prominent approach has been to learn a distance metric between objects given additional side information such as pairwise similarity and dissimilarity constraints over the data. [sent-28, score-0.404]
</p><p>7 A class of distance metrics that has shown excellent generalization properties is the learned Mahalanobis distance function (Davis et al. [sent-29, score-0.284]
</p><p>8 The Mahalanobis distance can be viewed as a method in which data is subject to a linear transformation, and the goal of such metric learning methods is to learn the linear transformation for a given task. [sent-35, score-0.34]
</p><p>9 To address the latter shortcoming, kernel learning algorithms typically attempt to learn a kernel matrix over the data. [sent-37, score-0.496]
</p><p>10 However, many existing kernel learning methods are still limited in that the learned kernels do not generalize to new points (Kwok and Tsang, 2003; Kulis et al. [sent-39, score-0.422]
</p><p>11 In this paper, we explore metric learning with linear transformations over arbitrarily highdimensional spaces; as we will see, this is equivalent to learning a linear transformation kernel function φ(x)T W φ(y) given an input kernel function φ(x)T φ(y). [sent-47, score-0.662]
</p><p>12 But perhaps most importantly, the loss function permits efﬁcient kernelization, allowing efﬁcient learning of a linear transformation in kernel space. [sent-52, score-0.319]
</p><p>13 As a result, unlike transductive kernel learning methods, our method easily handles out-of-sample extensions, that is, it can be applied to unseen data. [sent-53, score-0.269]
</p><p>14 We build upon our results of kernelization for the LogDet formulation to develop a general framework for learning linear transformation kernel functions and show that such kernels can be efﬁciently learned over a wide class of convex constraints and loss functions. [sent-54, score-0.61]
</p><p>15 In our case, even though the matrix W may be inﬁnite-dimensional, it can be 520  M ETRIC AND K ERNEL L EARNING U SING A L INEAR T RANSFORMATION  fully represented in terms of the constrained data points, making it possible to compute the learned kernel function over arbitrary points. [sent-56, score-0.344]
</p><p>16 We demonstrate the beneﬁts of a generalized framework for inductive kernel learning by applying our techniques to the problem of inductive kernelized semi-supervised dimensionality reduction. [sent-57, score-0.369]
</p><p>17 Finally, we apply our metric and kernel learning algorithms to a number of challenging learning problems, including ones from the domains of computer vision and text mining. [sent-59, score-0.397]
</p><p>18 Unlike existing techniques, we can learn linear transformation-based distance or kernel functions over these domains, and we show that the resulting functions lead to improvements over state-of-the-art techniques for a variety of problems. [sent-60, score-0.346]
</p><p>19 Related Work Most of the existing work in metric learning has been done in the Mahalanobis distance (or metric) learning paradigm, which has been found to be a sufﬁciently powerful class of metrics for a variety of data. [sent-62, score-0.302]
</p><p>20 (2004) provided the ﬁrst demonstration of Mahalanobis distance learning in kernel space. [sent-77, score-0.281]
</p><p>21 The ﬁrst category includes parametric approaches, where the learned kernel function is restricted to be of a 521  JAIN , K ULIS , DAVIS AND D HILLON  speciﬁc form and then the relevant parameters are learned according to the provided data. [sent-84, score-0.407]
</p><p>22 However, based on our choice of regularization and constraints, we show that our learned kernel matrix corresponds to a linear transformation kernel function parameterized by a PSD matrix. [sent-97, score-0.639]
</p><p>23 In addition, our kernel learning method naturally provides kernelization for many existing metric learning methods. [sent-99, score-0.466]
</p><p>24 (2010) showed kernelization for a class of metric learning algorithms including LMNN and NCA; as we will see, our result is more general and we can prove kernelization over a larger class of problems and can also reduce the number of parameters to be learned. [sent-101, score-0.294]
</p><p>25 (2010), we showed the equivalence between a general class of kernel learning problems and metric learning problems. [sent-114, score-0.367]
</p><p>26 Finally, we address limitations of the method when the amount of training data is large, and propose a modiﬁed algorithm to efﬁciently learn a kernel under such circumstances. [sent-120, score-0.249]
</p><p>27 1 Mahalanobis Distances and Parameterized Kernels First we introduce the framework for metric and kernel learning that is employed in this paper. [sent-123, score-0.367]
</p><p>28 , xn ], xi ∈ Rd0 (when working in kernel space, the data 522  M ETRIC AND K ERNEL L EARNING U SING A L INEAR T RANSFORMATION  matrix will be represented as Φ = [φ(x1 ), . [sent-127, score-0.278]
</p><p>29 As a result, we are interested in working in kernel space; that is, we will express the Mahalanobis distance in kernel space using an appropriate mapping φ from input to feature space: dW (φ(xi ), φ(x j )) = (φ(xi ) − φ(x j ))T W (φ(xi ) − φ(x j )). [sent-135, score-0.494]
</p><p>30 As is standard with kernel-based algorithms, we require that this distance be computable given the ability to compute the kernel function κ0 (x, y) = φ(x)T φ(y). [sent-137, score-0.281]
</p><p>31 We can therefore equivalently pose the problem as learning a parameterized kernel function κ(x, y) = φ(x)T W φ(y) given some input kernel function κ0 (x, y) = φ(x)T φ(y). [sent-138, score-0.426]
</p><p>32 In this paper, we assume that pairwise similarity and dissimilarity constraints are given over the data—that is, pairs of points that should be similar under the learned metric/kernel, and pairs of points that should be dissimilar under the learned metric/kernel. [sent-140, score-0.364]
</p><p>33 Further, (2) considers simple similarity and dissimilarity constraints over the learned Mahalanobis distance, but other linear constraints are possible. [sent-166, score-0.3]
</p><p>34 Note that the squared Euclidean distance in kernel space may be written as K(i, i) + K( j, j) − 2K(i, j), where K is the learned kernel matrix; equivalently, we may write the distance as tr(K(ei − e j )(ei − e j )T ), where ei is the i-th canonical basis vector. [sent-173, score-0.756]
</p><p>35 (3)  This kernel learning problem was ﬁrst proposed in the transductive setting in Kulis et al. [sent-177, score-0.269]
</p><p>36 Note that problem (2) optimizes over a d × d matrix W , while the kernel learning problem (3) optimizes over an n × n matrix K. [sent-179, score-0.281]
</p><p>37 The above theorem shows that the LogDet metric learning problem (2) can be solved implicitly by solving an equivalent kernel learning problem (3). [sent-187, score-0.367]
</p><p>38 In fact, in Section 4 we show an equivalence between metric and kernel learning for a general class of regularization functions. [sent-188, score-0.367]
</p><p>39 This equivalence implies that we can implicitly solve the metric learning problem by instead solving for the optimal kernel matrix K ∗ . [sent-210, score-0.401]
</p><p>40 4 Generalizing to New Points In this section, we see how to generalize to new points using the learned kernel matrix K ∗ . [sent-215, score-0.385]
</p><p>41 The distance between two points φ(xi ) and φ(x j ) that are in the training set can be computed directly from the learned kernel matrix as K(i, i) + K( j, j) − 2K(i, j). [sent-217, score-0.412]
</p><p>42 (6)  Thus, the expression above can be used to evaluate kernelized distances with respect to the learned kernel function between arbitrary data objects. [sent-224, score-0.375]
</p><p>43 In summary, the connection between kernel learning and metric learning allows us to generalize our metrics to new points in kernel space. [sent-225, score-0.672]
</p><p>44 This is performed by ﬁrst solving the kernel learning problem for K, then using the learned kernel matrix and the input kernel function to compute learned distances using (6). [sent-226, score-0.896]
</p><p>45 5 Kernel Learning Algorithm Given the connection between the Mahalanobis metric learning problem for the d × d matrix W and the kernel learning problem for the n × n kernel matrix K, we develop an algorithm for efﬁciently performing metric learning in kernel space. [sent-228, score-1.015]
</p><p>46 3, we proposed a LogDet divergence-based Mahalanobis metric learning problem (2) and an equivalent kernel learning problem (3). [sent-272, score-0.367]
</p><p>47 In particular, we propose a method to efﬁciently learn an identity plus low-rank Mahalanobis distance matrix and its equivalent kernel function. [sent-280, score-0.351]
</p><p>48 For learning the kernel function, the basis R = ΦJ can be selected by: 1) using a randomly sampled coefﬁcient matrix J, 2) clustering Φ using kernel k-means or a spectral clustering method, 3) choosing a random subset of Φ, that is, the columns of J are random indicator vectors. [sent-310, score-0.615]
</p><p>49 A natural question is whether one can learn similar kernel functions with other loss functions, such as those considered previously in the literature for Mahalanobis metric learning. [sent-314, score-0.427]
</p><p>50 In this section, we propose and analyze a general kernel matrix learning problem similar to (3) but using a more general class of loss functions. [sent-315, score-0.271]
</p><p>51 As in the LogDet loss function case, we show that our kernel matrix learning problem is equivalent to learning a linear transformation (LT) kernel function with a speciﬁc loss function. [sent-316, score-0.59]
</p><p>52 This implies that the learned LT kernel function can be naturally applied to new data. [sent-317, score-0.31]
</p><p>53 Additionally, since a large class of metric learning methods can be seen as learning a LT kernel function, our result provides a constructive method for kernelizing these methods. [sent-318, score-0.405]
</p><p>54 As in the LogDet formulation, we will ﬁrst consider a transductive scenario, where we learn a kernel matrix K that is regularized against K0 while satisfying the available side-information. [sent-328, score-0.339]
</p><p>55 529  JAIN , K ULIS , DAVIS AND D HILLON  Recall that the LogDet divergence based loss function in the kernel matrix learning problem (3) is given by: −1 −1 Dℓd (K, K0 ) = tr(KK0 ) − log det(KK0 ) − n, −1/2  = tr(K0  −1/2  KK0  −1/2  ) − log det(K0  −1/2  KK0  ) − n. [sent-329, score-0.334]
</p><p>56 We also generalize our constraints to include arbitrary constraints over the kernel matrix K rather than just the pairwise distance constraints in the above problem. [sent-334, score-0.527]
</p><p>57 In general, such formulations are limited in that the learned kernel cannot readily be applied to new data points. [sent-342, score-0.31]
</p><p>58 However, we will show that the above proposed problem is equivalent to learning linear transformation (LT) kernel functions. [sent-343, score-0.295]
</p><p>59 Formally, an LT kernel function κW is a kernel function of the form κ(x, y) = φ(x)T W φ(y), where W is a positive semi-deﬁnite (PSD) matrix. [sent-344, score-0.426]
</p><p>60 A natural way to learn an LT kernel function would be to learn the parameterization matrix W using the provided side-information. [sent-345, score-0.319]
</p><p>61 gi (ΦT W Φ) ≤ bi , 1 ≤ i ≤ m,  (13)  where, as before, the function f is the loss function and the functions gi are the constraints that encode the side information. [sent-348, score-0.309]
</p><p>62 The constraints gi are assumed to be a function of the matrix ΦT W Φ of learned kernel values over the training data. [sent-349, score-0.482]
</p><p>63 Further, this result will yield insight into the type of kernel that is learned by the kernel learning problem. [sent-355, score-0.523]
</p><p>64 Similarly, most of the existing metric learning formulations have a spectral function as their objective function. [sent-363, score-0.258]
</p><p>65 Also, note that the constraints in (15) can be simpliﬁed to: 1/2 1/2 gi (αΦT Φ + ΦT ULU T Φ) ≤ bi ≡ gi (αK0 + K0 LK0 ) ≤ bi . [sent-422, score-0.351]
</p><p>66 Given that K = ΦT W Φ, we can see that the learned kernel function is a linear transformation kernel; that is, κ(xi , x j ) = φ(xi )T W φ(x j ). [sent-426, score-0.392]
</p><p>67 Given a pair of new data points z1 and z2 , we use the fact that the learned kernel is a linear transformation kernel, along with the ﬁrst result of the theorem (W = αI d + ΦSΦT ) to compute the learned kernel as: T φ(z1 )T W φ(z2 ) = α · κ0 (z1 , z2 ) + k1 Sk2 , where ki = [κ0 (zi , x1 ), . [sent-427, score-0.702]
</p><p>68 (18)  Since LogDet divergence is also a spectral function, Theorem 4 is a generalization of Theorem 1 and implies kernelization for our metric learning formulation (2). [sent-431, score-0.387]
</p><p>69 Then, we will explicitly enforce that the learned kernel is of this form. [sent-443, score-0.31]
</p><p>70 Below, 533  JAIN , K ULIS , DAVIS AND D HILLON  we show that for any spectral function f and linear constraints gi (K) = tr(Ci K), (19) reduces to a problem that applies f and gi ’s to k × k matrices only, which provides signiﬁcant scalability. [sent-450, score-0.294]
</p><p>71 Note that (20) is over k × k matrices (after initial pre-processing) and is in fact similar to the kernel learning problem (12), but with a kernel K J of smaller size k × k, k ≪ n. [sent-461, score-0.426]
</p><p>72 This enables us to naturally apply the above kernel learning problem in the inductive setting. [sent-463, score-0.251]
</p><p>73 Then, (19) and (20) with gi (K) = tr(Ci K) are equivalent to the following linear transformation kernel learning problem (analogous to the connection between (12) and (13)): min f (W ),  W 0,L  s. [sent-466, score-0.399]
</p><p>74 Note that, in contrast to (13), where the last constraint over W is achieved automatically, (21) requires this constraint on W to be satisﬁed during the optimization process, which leads to a reduced number of parameters for our kernel learning problem. [sent-474, score-0.293]
</p><p>75 The above theorem shows that our reducedparameter kernel learning method (19) also implicitly learns a linear transformation kernel function, hence we can generalize the learned kernel to unseen data points using an expression similar to (18). [sent-475, score-0.859]
</p><p>76 Special Cases In the previous section, we proved a general result showing the connections between metric and kernel learning using a wide class of loss functions and constraints. [sent-477, score-0.391]
</p><p>77 Now, the kernel learning problem (12) with loss function fvN and linear constraints is: −1/2  min fvN (K0 K 0  −1/2  KK0  ),  s. [sent-488, score-0.317]
</p><p>78 (22)  As fvN is an spectral function, using Theorem 4, the above kernel learning problem is equivalent to the following metric learning problem: min DvN (W, I),  W 0  s. [sent-491, score-0.465]
</p><p>79 In this case, f is once again a strictly convex spectral function, and its global minimum is α = 0, so we can use (12) to solve for the learned kernel K as −1 min KK0 K 0  2 F,  s. [sent-509, score-0.408]
</p><p>80 gi (K) ≤ bi , 1 ≤ i ≤ m,  The constraints gi for this problem can be easily constructed by re-writing each of POLA’s constraints as a function of ΦT W Φ. [sent-511, score-0.342]
</p><p>81 (23)  Here we show that this problem can be efﬁciently solved for high dimensional data in its kernel space, hence kernelizing the metric learning methods introduced by Weinberger et al. [sent-520, score-0.405]
</p><p>82 4 Trace-norm Based Inductive Semi-supervised Kernel Dimensionality Reduction (Trace-SSIKDR) Finally, we apply our framework to semi-supervised kernel dimensionality reduction, which provides a novel and practical application of our framework. [sent-538, score-0.257]
</p><p>83 While there exists a variety of methods for kernel dimensionality reduction, most of these methods are unsupervised (e. [sent-539, score-0.257]
</p><p>84 In contrast, we can use our kernel learning framework to learn a low-rank transformation of the feature vectors implicitly that in turn provides a low-dimensional embedding of the data set. [sent-542, score-0.37]
</p><p>85 However, using elementary linear algebra we can show that K and 1/2 the learned kernel function can be computed efﬁciently without computing K0 by maintaining −1/2 ˜ −1/2 S = K0 KK0 from step to step. [sent-564, score-0.31]
</p><p>86 Note also that the learned embedding xi → K 1/2 K0 ki , where ki is a vector of input kernel function values between xi and the training data, can be computed efﬁciently 1/2 1/2 as xi → Σk DkVk ki , which does not require K0 explicitly. [sent-583, score-0.442]
</p><p>87 6, we showed that the kernelized problem can be learned with respect to a reduced basis of size k, admitting a learned kernel parameterized by O(k2 ) values. [sent-589, score-0.471]
</p><p>88 A special case of our approach is the trace-norm based kernel function learning problem, which can be applied to the task of semi-supervised inductive kernel dimensionality reduction. [sent-592, score-0.508]
</p><p>89 We evaluate performance of our learned distance metrics or kernel functions in the context of a) classiﬁcation accuracy for the k-nearest neighbor algorithm, b) kernel dimensionality reduction. [sent-594, score-0.723]
</p><p>90 The upper and lower bounds for the similarity and dissimilarity constraints are determined empirically as the 1st and 99th percentiles of the distribution of distances computed using a baseline Mahalanobis distance parameterized by W0 . [sent-603, score-0.28]
</p><p>91 1 Low-Dimensional Data Sets First we evaluate our LogDet divergence based metric learning method (see Algorithm 1) on the standard UCI data sets in the low-dimensional (non-kernelized) setting, to directly compare with several existing metric learning methods. [sent-615, score-0.4]
</p><p>92 In Figure 1 (a), we compare LogDet Linear (K0 equals the linear kernel) and the LogDet Gaussian (K0 equals Gaussian kernel in kernel space) algorithms 539  JAIN , K ULIS , DAVIS AND D HILLON  0. [sent-616, score-0.426]
</p><p>93 LogDet metric learning was run with in input space (LogDet Linear) as well as in kernel space with a Gaussian kernel (LogDet Gaussian). [sent-636, score-0.58]
</p><p>94 In general, for the Mpg321, Foxpro, and Iptables data sets, learned metrics yield only marginal gains over the baseline Euclidean distance measure. [sent-679, score-0.253]
</p><p>95 We compute distances between images using learning kernels with three different base image kernels: 1) PMK: Grauman and Darrell’s pyramid match kernel (Grauman and Darrell, 2007) applied to SIFT features, 2) CORR: the kernel designed by Zhang et al. [sent-768, score-0.525]
</p><p>96 For the baseline kernel, we use the data-dependent kernel function proposed by Sindhwani et al. [sent-808, score-0.25]
</p><p>97 Conclusions In this paper, we considered the general problem of learning a linear transformation of the input data and applied it to the problems of metric and kernel learning, with a focus on establishing connections between the two problems. [sent-815, score-0.449]
</p><p>98 We also showed that our learned metric can be restricted to a small dimensional basis efﬁciently, thereby permitting scalability of our method to large data sets with high-dimensional feature spaces. [sent-817, score-0.279]
</p><p>99 A key consequence of our analysis is that a number of existing approaches for Mahalanobis metric learning may be applied in kernel space using our kernel learning formulation. [sent-820, score-0.609]
</p><p>100 Finally, we presented several experiments on benchmark data, high-dimensional vision, and text classiﬁcation problems as well as a semi-supervised kernel dimensionality reduction problem, demonstrating our method compared to several existing state-of-the-art techniques. [sent-821, score-0.316]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('logdet', 0.692), ('kernel', 0.213), ('mahalanobis', 0.209), ('metric', 0.154), ('proccedings', 0.146), ('davis', 0.144), ('ulu', 0.144), ('tr', 0.132), ('jain', 0.127), ('hillon', 0.126), ('ransformation', 0.126), ('ulis', 0.126), ('kulis', 0.125), ('pmk', 0.117), ('sing', 0.097), ('learned', 0.097), ('etric', 0.089), ('transformation', 0.082), ('jlj', 0.081), ('gi', 0.081), ('dw', 0.078), ('spectral', 0.075), ('ernel', 0.075), ('kernelization', 0.07), ('ei', 0.069), ('distance', 0.068), ('bi', 0.066), ('divergence', 0.063), ('inear', 0.063), ('lmnn', 0.062), ('fs', 0.057), ('constraints', 0.057), ('transductive', 0.056), ('fvn', 0.054), ('latex', 0.054), ('dissimilarity', 0.054), ('metrics', 0.051), ('ci', 0.047), ('mcml', 0.046), ('grauman', 0.046), ('dk', 0.045), ('dimensionality', 0.044), ('weinberger', 0.044), ('lt', 0.043), ('kernels', 0.042), ('earning', 0.041), ('generalize', 0.041), ('embedding', 0.039), ('digits', 0.039), ('kernelizing', 0.038), ('corr', 0.038), ('inductive', 0.038), ('baseline', 0.037), ('neighbor', 0.037), ('learn', 0.036), ('vi', 0.036), ('pola', 0.036), ('zti', 0.036), ('usps', 0.036), ('kernelized', 0.036), ('similarity', 0.035), ('matrix', 0.034), ('neumann', 0.034), ('vk', 0.034), ('bregman', 0.031), ('xi', 0.031), ('von', 0.03), ('darrell', 0.03), ('dhillon', 0.03), ('text', 0.03), ('distances', 0.029), ('nn', 0.029), ('existing', 0.029), ('constraint', 0.029), ('basis', 0.028), ('slack', 0.028), ('pyramid', 0.028), ('euclidean', 0.028), ('dkvk', 0.027), ('foxpro', 0.027), ('iptables', 0.027), ('globerson', 0.027), ('clustering', 0.026), ('formulation', 0.025), ('semide', 0.025), ('berg', 0.024), ('nips', 0.024), ('loss', 0.024), ('dissimilar', 0.024), ('blur', 0.023), ('pami', 0.023), ('lsa', 0.023), ('spmk', 0.023), ('min', 0.023), ('uci', 0.023), ('nearest', 0.023), ('det', 0.023), ('goldberger', 0.022), ('optimization', 0.022), ('eigenvectors', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="66-tfidf-1" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>Author: Prateek Jain, Brian Kulis, Jason V. Davis, Inderjit S. Dhillon</p><p>Abstract: Metric and kernel learning arise in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem. In particular, we propose a general optimization framework for learning metrics via linear transformations, and analyze in detail a special case of our framework—that of minimizing the LogDet divergence subject to linear constraints. We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework. Our theoretical connections between metric and kernel learning have two main consequences: 1) the learned kernel matrix parameterizes a linear transformation kernel function and can be applied inductively to new data points, 2) our result yields a constructive method for kernelizing most existing Mahalanobis metric learning formulations. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction. Keywords: divergence metric learning, kernel learning, linear transformation, matrix divergences, logdet</p><p>2 0.1751707 <a title="66-tfidf-2" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel</p><p>Abstract: The success of many machine learning and pattern recognition methods relies heavily upon the identiﬁcation of an appropriate distance metric on the input data. It is often beneﬁcial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semideﬁnite. Semideﬁnite programming is often used to enforce this constraint, but does not scale well and is not easy to implement. B OOST M ETRIC is instead based on the observation that any positive semideﬁnite matrix can be decomposed into a linear combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting methods are easy to implement, efﬁcient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semideﬁnite matrix with trace and rank being one rather than a classiﬁer or regressor. Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. Keywords: Mahalanobis distance, semideﬁnite programming, column generation, boosting, Lagrange duality, large margin nearest neighbor</p><p>3 0.17423247 <a title="66-tfidf-3" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>Author: Yiming Ying, Peng Li</p><p>Abstract: The main theme of this paper is to develop a novel eigenvalue optimization framework for learning a Mahalanobis metric. Within this context, we introduce a novel metric learning approach called DML-eig which is shown to be equivalent to a well-known eigenvalue optimization problem called minimizing the maximal eigenvalue of a symmetric matrix (Overton, 1988; Lewis and Overton, 1996). Moreover, we formulate LMNN (Weinberger et al., 2005), one of the state-of-the-art metric learning methods, as a similar eigenvalue optimization problem. This novel framework not only provides new insights into metric learning but also opens new avenues to the design of efﬁcient metric learning algorithms. Indeed, ﬁrst-order algorithms are developed for DML-eig and LMNN which only need the computation of the largest eigenvector of a matrix per iteration. Their convergence characteristics are rigorously established. Various experiments on benchmark data sets show the competitive performance of our new approaches. In addition, we report an encouraging result on a difﬁcult and challenging face veriﬁcation data set called Labeled Faces in the Wild (LFW). Keywords: metric learning, convex optimization, semi-deﬁnite programming, ﬁrst-order methods, eigenvalue optimization, matrix factorization, face veriﬁcation</p><p>4 0.077980705 <a title="66-tfidf-4" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>Author: Fei Yan, Josef Kittler, Krystian Mikolajczyk, Atif Tahir</p><p>Abstract: Sparsity-inducing multiple kernel Fisher discriminant analysis (MK-FDA) has been studied in the literature. Building on recent advances in non-sparse multiple kernel learning (MKL), we propose a non-sparse version of MK-FDA, which imposes a general ℓ p norm regularisation on the kernel weights. We formulate the associated optimisation problem as a semi-inﬁnite program (SIP), and adapt an iterative wrapper algorithm to solve it. We then discuss, in light of latest advances in MKL optimisation techniques, several reformulations and optimisation strategies that can potentially lead to signiﬁcant improvements in the efﬁciency and scalability of MK-FDA. We carry out extensive experiments on six datasets from various application areas, and compare closely the performance of ℓ p MK-FDA, ﬁxed norm MK-FDA, and several variants of SVM-based MKL (MK-SVM). Our results demonstrate that ℓ p MK-FDA improves upon sparse MK-FDA in many practical situations. The results also show that on image categorisation problems, ℓ p MK-FDA tends to outperform its SVM counterpart. Finally, we also discuss the connection between (MK-)FDA and (MK-)SVM, under the uniﬁed framework of regularised kernel machines. Keywords: multiple kernel learning, kernel ﬁsher discriminant analysis, regularised least squares, support vector machines</p><p>5 0.070504621 <a title="66-tfidf-5" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>Author: Francesco Orabona, Luo Jie, Barbara Caputo</p><p>Abstract: In recent years there has been a lot of interest in designing principled classiﬁcation algorithms over multiple cues, based on the intuitive notion that using more features should lead to better performance. In the domain of kernel methods, a principled way to use multiple features is the Multi Kernel Learning (MKL) approach. Here we present a MKL optimization algorithm based on stochastic gradient descent that has a guaranteed convergence rate. We directly solve the MKL problem in the primal formulation. By having a p-norm formulation of MKL, we introduce a parameter that controls the level of sparsity of the solution, while leading to an easier optimization problem. We prove theoretically and experimentally that 1) our algorithm has a faster convergence rate as the number of kernels grows; 2) the training complexity is linear in the number of training examples; 3) very few iterations are sufﬁcient to reach good solutions. Experiments on standard benchmark databases support our claims. Keywords: multiple kernel learning, learning kernels, online optimization, stochastic subgradient descent, convergence bounds, large scale</p><p>6 0.069025837 <a title="66-tfidf-6" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>7 0.063681222 <a title="66-tfidf-7" href="./jmlr-2012-Regularization_Techniques_for_Learning_with_Matrices.html">97 jmlr-2012-Regularization Techniques for Learning with Matrices</a></p>
<p>8 0.063365318 <a title="66-tfidf-8" href="./jmlr-2012-Feature_Selection_via_Dependence_Maximization.html">44 jmlr-2012-Feature Selection via Dependence Maximization</a></p>
<p>9 0.060943432 <a title="66-tfidf-9" href="./jmlr-2012-Algorithms_for_Learning_Kernels_Based_on_Centered_Alignment.html">16 jmlr-2012-Algorithms for Learning Kernels Based on Centered Alignment</a></p>
<p>10 0.059388459 <a title="66-tfidf-10" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>11 0.058173608 <a title="66-tfidf-11" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>12 0.056989126 <a title="66-tfidf-12" href="./jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems.html">89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</a></p>
<p>13 0.055491131 <a title="66-tfidf-13" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>14 0.055180389 <a title="66-tfidf-14" href="./jmlr-2012-Robust_Kernel_Density_Estimation.html">100 jmlr-2012-Robust Kernel Density Estimation</a></p>
<p>15 0.054996308 <a title="66-tfidf-15" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>16 0.053449422 <a title="66-tfidf-16" href="./jmlr-2012-Refinement_of_Operator-valued_Reproducing_Kernels.html">96 jmlr-2012-Refinement of Operator-valued Reproducing Kernels</a></p>
<p>17 0.051769372 <a title="66-tfidf-17" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>18 0.047250833 <a title="66-tfidf-18" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>19 0.046982363 <a title="66-tfidf-19" href="./jmlr-2012-Active_Clustering_of_Biological_Sequences.html">12 jmlr-2012-Active Clustering of Biological Sequences</a></p>
<p>20 0.044638887 <a title="66-tfidf-20" href="./jmlr-2012-On_the_Convergence_Rate_oflp-Norm_Multiple_Kernel_Learning.html">81 jmlr-2012-On the Convergence Rate oflp-Norm Multiple Kernel Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.222), (1, -0.029), (2, 0.175), (3, 0.211), (4, -0.179), (5, 0.009), (6, -0.208), (7, 0.082), (8, -0.04), (9, -0.088), (10, 0.032), (11, 0.015), (12, -0.171), (13, -0.028), (14, 0.073), (15, 0.046), (16, -0.01), (17, -0.035), (18, 0.083), (19, -0.007), (20, -0.055), (21, 0.129), (22, 0.052), (23, -0.147), (24, 0.066), (25, -0.124), (26, -0.03), (27, -0.084), (28, -0.087), (29, 0.029), (30, 0.017), (31, 0.025), (32, -0.084), (33, -0.021), (34, -0.006), (35, 0.026), (36, -0.054), (37, -0.057), (38, 0.021), (39, 0.05), (40, 0.061), (41, -0.098), (42, -0.083), (43, -0.178), (44, 0.041), (45, -0.029), (46, 0.031), (47, 0.055), (48, 0.068), (49, 0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91768533 <a title="66-lsi-1" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>Author: Prateek Jain, Brian Kulis, Jason V. Davis, Inderjit S. Dhillon</p><p>Abstract: Metric and kernel learning arise in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem. In particular, we propose a general optimization framework for learning metrics via linear transformations, and analyze in detail a special case of our framework—that of minimizing the LogDet divergence subject to linear constraints. We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework. Our theoretical connections between metric and kernel learning have two main consequences: 1) the learned kernel matrix parameterizes a linear transformation kernel function and can be applied inductively to new data points, 2) our result yields a constructive method for kernelizing most existing Mahalanobis metric learning formulations. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction. Keywords: divergence metric learning, kernel learning, linear transformation, matrix divergences, logdet</p><p>2 0.77593625 <a title="66-lsi-2" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>Author: Yiming Ying, Peng Li</p><p>Abstract: The main theme of this paper is to develop a novel eigenvalue optimization framework for learning a Mahalanobis metric. Within this context, we introduce a novel metric learning approach called DML-eig which is shown to be equivalent to a well-known eigenvalue optimization problem called minimizing the maximal eigenvalue of a symmetric matrix (Overton, 1988; Lewis and Overton, 1996). Moreover, we formulate LMNN (Weinberger et al., 2005), one of the state-of-the-art metric learning methods, as a similar eigenvalue optimization problem. This novel framework not only provides new insights into metric learning but also opens new avenues to the design of efﬁcient metric learning algorithms. Indeed, ﬁrst-order algorithms are developed for DML-eig and LMNN which only need the computation of the largest eigenvector of a matrix per iteration. Their convergence characteristics are rigorously established. Various experiments on benchmark data sets show the competitive performance of our new approaches. In addition, we report an encouraging result on a difﬁcult and challenging face veriﬁcation data set called Labeled Faces in the Wild (LFW). Keywords: metric learning, convex optimization, semi-deﬁnite programming, ﬁrst-order methods, eigenvalue optimization, matrix factorization, face veriﬁcation</p><p>3 0.59381407 <a title="66-lsi-3" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel</p><p>Abstract: The success of many machine learning and pattern recognition methods relies heavily upon the identiﬁcation of an appropriate distance metric on the input data. It is often beneﬁcial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semideﬁnite. Semideﬁnite programming is often used to enforce this constraint, but does not scale well and is not easy to implement. B OOST M ETRIC is instead based on the observation that any positive semideﬁnite matrix can be decomposed into a linear combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting methods are easy to implement, efﬁcient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semideﬁnite matrix with trace and rank being one rather than a classiﬁer or regressor. Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. Keywords: Mahalanobis distance, semideﬁnite programming, column generation, boosting, Lagrange duality, large margin nearest neighbor</p><p>4 0.50585616 <a title="66-lsi-4" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random ﬁelds (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter ﬁtting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random ﬁeld via the graphical lasso.</p><p>5 0.41899267 <a title="66-lsi-5" href="./jmlr-2012-Algorithms_for_Learning_Kernels_Based_on_Centered_Alignment.html">16 jmlr-2012-Algorithms for Learning Kernels Based on Centered Alignment</a></p>
<p>Author: Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difﬁcult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classiﬁcation and regression. Our algorithms are based on the notion of centered alignment which is used as a similarity measure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efﬁcient algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization. Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classiﬁcation and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classiﬁcation and regression. Keywords: kernel methods, learning kernels, feature selection</p><p>6 0.37987331 <a title="66-lsi-6" href="./jmlr-2012-Iterative_Reweighted_Algorithms_for_Matrix_Rank_Minimization.html">52 jmlr-2012-Iterative Reweighted Algorithms for Matrix Rank Minimization</a></p>
<p>7 0.37308168 <a title="66-lsi-7" href="./jmlr-2012-Feature_Selection_via_Dependence_Maximization.html">44 jmlr-2012-Feature Selection via Dependence Maximization</a></p>
<p>8 0.37065369 <a title="66-lsi-8" href="./jmlr-2012-Active_Clustering_of_Biological_Sequences.html">12 jmlr-2012-Active Clustering of Biological Sequences</a></p>
<p>9 0.36156955 <a title="66-lsi-9" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>10 0.35405466 <a title="66-lsi-10" href="./jmlr-2012-Multi-task_Regression_using_Minimal_Penalties.html">73 jmlr-2012-Multi-task Regression using Minimal Penalties</a></p>
<p>11 0.34376153 <a title="66-lsi-11" href="./jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems.html">89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</a></p>
<p>12 0.33702129 <a title="66-lsi-12" href="./jmlr-2012-Robust_Kernel_Density_Estimation.html">100 jmlr-2012-Robust Kernel Density Estimation</a></p>
<p>13 0.33603588 <a title="66-lsi-13" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>14 0.33575469 <a title="66-lsi-14" href="./jmlr-2012-A_Kernel_Two-Sample_Test.html">4 jmlr-2012-A Kernel Two-Sample Test</a></p>
<p>15 0.32198748 <a title="66-lsi-15" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>16 0.32038286 <a title="66-lsi-16" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>17 0.31688839 <a title="66-lsi-17" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>18 0.31272414 <a title="66-lsi-18" href="./jmlr-2012-Security_Analysis_of_Online_Centroid_Anomaly_Detection.html">104 jmlr-2012-Security Analysis of Online Centroid Anomaly Detection</a></p>
<p>19 0.30466974 <a title="66-lsi-19" href="./jmlr-2012-Refinement_of_Operator-valued_Reproducing_Kernels.html">96 jmlr-2012-Refinement of Operator-valued Reproducing Kernels</a></p>
<p>20 0.28421217 <a title="66-lsi-20" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.02), (21, 0.026), (26, 0.466), (27, 0.011), (29, 0.02), (35, 0.024), (49, 0.014), (56, 0.017), (64, 0.032), (69, 0.02), (75, 0.076), (79, 0.011), (81, 0.014), (92, 0.061), (96, 0.099)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98331469 <a title="66-lda-1" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>Author: David Verstraeten, Benjamin Schrauwen, Sander Dieleman, Philemon Brakel, Pieter Buteneers, Dejan Pecevski</p><p>Abstract: Oger (OrGanic Environment for Reservoir computing) is a Python toolbox for building, training and evaluating modular learning architectures on large data sets. It builds on MDP for its modularity, and adds processing of sequential data sets, gradient descent training, several crossvalidation schemes and parallel parameter optimization methods. Additionally, several learning algorithms are implemented, such as different reservoir implementations (both sigmoid and spiking), ridge regression, conditional restricted Boltzmann machine (CRBM) and others, including GPU accelerated versions. Oger is released under the GNU LGPL, and is available from http: //organic.elis.ugent.be/oger. Keywords: Python, modular architectures, sequential processing</p><p>2 0.91275889 <a title="66-lda-2" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<p>Author: Zhiwei Qin, Donald Goldfarb</p><p>Abstract: We consider a class of sparse learning problems in high dimensional feature space regularized by a structured sparsity-inducing norm that incorporates prior knowledge of the group structure of the features. Such problems often pose a considerable challenge to optimization algorithms due to the non-smoothness and non-separability of the regularization term. In this paper, we focus on two commonly adopted sparsity-inducing regularization terms, the overlapping Group Lasso penalty l1 /l2 -norm and the l1 /l∞ -norm. We propose a uniﬁed framework based on the augmented Lagrangian method, under which problems with both types of regularization and their variants can be efﬁciently solved. As one of the core building-blocks of this framework, we develop new algorithms using a partial-linearization/splitting technique and prove that the accelerated versions 1 of these algorithms require O( √ε ) iterations to obtain an ε-optimal solution. We compare the performance of these algorithms against that of the alternating direction augmented Lagrangian and FISTA methods on a collection of data sets and apply them to two real-world problems to compare the relative merits of the two norms. Keywords: structured sparsity, overlapping Group Lasso, alternating direction methods, variable splitting, augmented Lagrangian</p><p>same-paper 3 0.90462536 <a title="66-lda-3" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>Author: Prateek Jain, Brian Kulis, Jason V. Davis, Inderjit S. Dhillon</p><p>Abstract: Metric and kernel learning arise in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem. In particular, we propose a general optimization framework for learning metrics via linear transformations, and analyze in detail a special case of our framework—that of minimizing the LogDet divergence subject to linear constraints. We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework. Our theoretical connections between metric and kernel learning have two main consequences: 1) the learned kernel matrix parameterizes a linear transformation kernel function and can be applied inductively to new data points, 2) our result yields a constructive method for kernelizing most existing Mahalanobis metric learning formulations. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction. Keywords: divergence metric learning, kernel learning, linear transformation, matrix divergences, logdet</p><p>4 0.90331769 <a title="66-lda-4" href="./jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>
<p>Author: Michael U. Gutmann, Aapo Hyvärinen</p><p>Abstract: We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a ﬁnite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only speciﬁed up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artiﬁcially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities. Keywords: statistics unnormalized models, partition function, computation, estimation, natural image</p><p>5 0.62742531 <a title="66-lda-5" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>Author: Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel</p><p>Abstract: The success of many machine learning and pattern recognition methods relies heavily upon the identiﬁcation of an appropriate distance metric on the input data. It is often beneﬁcial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed B OOST M ETRIC, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semideﬁnite. Semideﬁnite programming is often used to enforce this constraint, but does not scale well and is not easy to implement. B OOST M ETRIC is instead based on the observation that any positive semideﬁnite matrix can be decomposed into a linear combination of trace-one rank-one matrices. B OOST M ETRIC thus uses rank-one positive semideﬁnite matrices as weak learners within an efﬁcient and scalable boosting-based learning process. The resulting methods are easy to implement, efﬁcient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semideﬁnite matrix with trace and rank being one rather than a classiﬁer or regressor. Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classiﬁcation accuracy and running time. Keywords: Mahalanobis distance, semideﬁnite programming, column generation, boosting, Lagrange duality, large margin nearest neighbor</p><p>6 0.60103047 <a title="66-lda-6" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>7 0.5920915 <a title="66-lda-7" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>8 0.58247751 <a title="66-lda-8" href="./jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</a></p>
<p>9 0.56245267 <a title="66-lda-9" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>10 0.55513704 <a title="66-lda-10" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>11 0.55290556 <a title="66-lda-11" href="./jmlr-2012-Optimal_Distributed_Online_Prediction_Using_Mini-Batches.html">85 jmlr-2012-Optimal Distributed Online Prediction Using Mini-Batches</a></p>
<p>12 0.54404986 <a title="66-lda-12" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>13 0.53780353 <a title="66-lda-13" href="./jmlr-2012-Coherence_Functions_with_Applications_in_Large-Margin_Classification_Methods.html">26 jmlr-2012-Coherence Functions with Applications in Large-Margin Classification Methods</a></p>
<p>14 0.53354985 <a title="66-lda-14" href="./jmlr-2012-Trading_Regret_for_Efficiency%3A_Online_Convex_Optimization_with_Long_Term_Constraints.html">115 jmlr-2012-Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints</a></p>
<p>15 0.53354394 <a title="66-lda-15" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>16 0.52954805 <a title="66-lda-16" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>17 0.52562106 <a title="66-lda-17" href="./jmlr-2012-Sampling_Methods_for_the_Nystr%C3%B6m_Method.html">103 jmlr-2012-Sampling Methods for the Nyström Method</a></p>
<p>18 0.51772463 <a title="66-lda-18" href="./jmlr-2012-Large-scale_Linear_Support_Vector_Regression.html">54 jmlr-2012-Large-scale Linear Support Vector Regression</a></p>
<p>19 0.4966301 <a title="66-lda-19" href="./jmlr-2012-Security_Analysis_of_Online_Centroid_Anomaly_Detection.html">104 jmlr-2012-Security Analysis of Online Centroid Anomaly Detection</a></p>
<p>20 0.49500695 <a title="66-lda-20" href="./jmlr-2012-Robust_Kernel_Density_Estimation.html">100 jmlr-2012-Robust Kernel Density Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
