<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 jmlr-2012-A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-6" href="#">jmlr2012-6</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6 jmlr-2012-A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives</h1>
<br/><p>Source: <a title="jmlr-2012-6-pdf" href="http://jmlr.org/papers/volume13/martinez12a/martinez12a.pdf">pdf</a></p><p>Author: Aleix Martinez, Shichuan Du</p><p>Abstract: In cognitive science and neuroscience, there have been two leading models describing how humans perceive and classify facial expressions of emotion—the continuous and the categorical model. The continuous model deﬁnes each facial expression of emotion as a feature vector in a face space. This model explains, for example, how expressions of emotion can be seen at different intensities. In contrast, the categorical model consists of C classiﬁers, each tuned to a speciﬁc emotion category. This model explains, among other ﬁndings, why the images in a morphing sequence between a happy and a surprise face are perceived as either happy or surprise but not something in between. While the continuous model has a more difﬁcult time justifying this latter ﬁnding, the categorical model is not as good when it comes to explaining how expressions are recognized at different intensities or modes. Most importantly, both models have problems explaining how one can recognize combinations of emotion categories such as happily surprised versus angrily surprised versus surprise. To resolve these issues, in the past several years, we have worked on a revised model that justiﬁes the results reported in the cognitive science and neuroscience literature. This model consists of C distinct continuous spaces. Multiple (compound) emotion categories can be recognized by linearly combining these C face spaces. The dimensions of these spaces are shown to be mostly conﬁgural. According to this model, the major task for the classiﬁcation of facial expressions of emotion is precise, detailed detection of facial landmarks rather than recognition. We provide an overview of the literature justifying the model, show how the resulting model can be employed to build algorithms for the recognition of facial expression of emotion, and propose research directions in machine learning and computer vision researchers to keep pushing the state of the art in these areas. We also discuss how the model can aid in stu</p><p>Reference: <a title="jmlr-2012-6-reference" href="../jmlr2012_reference/jmlr-2012-A_Model_of_the_Perception_of_Facial_Expressions_of_Emotion_by_Humans%3A_Research_Overview_and_Perspectives_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The continuous model deﬁnes each facial expression of emotion as a feature vector in a face space. [sent-6, score-1.222]
</p><p>2 This model explains, for example, how expressions of emotion can be seen at different intensities. [sent-7, score-0.763]
</p><p>3 In contrast, the categorical model consists of C classiﬁers, each tuned to a speciﬁc emotion category. [sent-8, score-0.639]
</p><p>4 This model explains, among other ﬁndings, why the images in a morphing sequence between a happy and a surprise face are perceived as either happy or surprise but not something in between. [sent-9, score-0.514]
</p><p>5 While the continuous model has a more difﬁcult time justifying this latter ﬁnding, the categorical model is not as good when it comes to explaining how expressions are recognized at different intensities or modes. [sent-10, score-0.372]
</p><p>6 Most importantly, both models have problems explaining how one can recognize combinations of emotion categories such as happily surprised versus angrily surprised versus surprise. [sent-11, score-0.904]
</p><p>7 Multiple (compound) emotion categories can be recognized by linearly combining these C face spaces. [sent-14, score-0.815]
</p><p>8 According to this model, the major task for the classiﬁcation of facial expressions of emotion is precise, detailed detection of facial landmarks rather than recognition. [sent-16, score-1.751]
</p><p>9 Keywords: vision, face perception, emotions, computational modeling, categorical perception, face detection  1. [sent-19, score-0.474]
</p><p>10 Of primary interest is the production and recognition of facial expressions of emotion. [sent-22, score-0.728]
</p><p>11 Facial expressions of emotion could also c 2012 Aleix Martinez and Shichuan Du. [sent-24, score-0.763]
</p><p>12 And, sign languages use facial expressions to encode part of the grammar (Wilbur, 2011). [sent-26, score-0.677]
</p><p>13 It has also been speculated that expressions of emotion were relevant in human evolution (Darwin, 1872). [sent-27, score-0.845]
</p><p>14 Models of the perception of facial expressions of emotion are thus important for the advance of many scientiﬁc disciplines. [sent-28, score-1.408]
</p><p>15 A ﬁrst reason machine learning and computer vision researchers are interested in creating computational models of the perception of facial expressions of emotion is to aid studies in the above sciences (Martinez, 2003). [sent-29, score-1.507]
</p><p>16 Furthermore, computational models of facial expressions of emotion are important for the development of artiﬁcial intelligence (Minsky, 1988) and are essential in humancomputer interaction (HCI) systems (Pentland, 2000). [sent-30, score-1.223]
</p><p>17 Yet, as much as we understand how facial expressions of emotion are produced, very little is known on how they are interpreted by the human visual system. [sent-31, score-1.333]
</p><p>18 A HCI system that can easily recognize expressions of no interest to the human user is of limited interest. [sent-33, score-0.338]
</p><p>19 • A model of human perception of facial expressions of emotion: We provide an overview of the cognitive science literature and deﬁne a computational model consistent with it. [sent-39, score-1.012]
</p><p>20 • Dimensions of the computational space: Recent research has shown that human used mostly shape for the perception and recognition of facial expressions of emotion. [sent-40, score-1.103]
</p><p>21 A conﬁgural feature is deﬁned as a non-rotation invariant modeling of the distance between facial components; for example, the vertical distance between eyebrows and mouth. [sent-42, score-0.46]
</p><p>22 Section 2 reviews relevant research on the perception of facial expressions of emotion by humans. [sent-47, score-1.408]
</p><p>23 Section 4 illustrates the importance of conﬁgural and shape features for the recognition of emotions in face images. [sent-49, score-0.512]
</p><p>24 The face muscles can be summarized as Action Unit (AU) (Ekman and Friesen, 1976) deﬁning positions characteristic of facial expressions of emotion. [sent-56, score-0.89]
</p><p>25 nurture) and whether the expressions of some emotions is universal (Izard, 2009). [sent-62, score-0.383]
</p><p>26 Facial expressions typically classiﬁed as universal are joy, surprise, anger, sadness, disgust and fear (Darwin, 1872; Ekman and Friesen, 1976). [sent-64, score-0.393]
</p><p>27 Universality of emotions is controversial, since it assumes facial expressions of emotion are innate (rather than culturally bound). [sent-65, score-1.389]
</p><p>28 It also favors a categorical perception of facial expressions of emotion. [sent-66, score-0.955]
</p><p>29 Each classiﬁer is speciﬁcally designed to recognize a single emotion label, such as surprise. [sent-70, score-0.585]
</p><p>30 Several psychophysical experiments suggest the perception of emotions by humans is categorical (Ekman and Rosenberg, 2005). [sent-71, score-0.515]
</p><p>31 Studies in neuroscience further suggest that distinct regions (or pathways) in the brain are used to recognize different expressions of emotion (Calder et al. [sent-72, score-0.85]
</p><p>32 Here, each emotion is represented as a feature vector in a multidimensional space given by some characteristics common to all emotions. [sent-75, score-0.546]
</p><p>33 It also allows for intensity in the perception of the emotion label. [sent-80, score-0.731]
</p><p>34 Yet, morphs between expressions of emotions are generally classiﬁed to the closest class rather than to an intermediate category (Beale and Keil, 1995). [sent-82, score-0.415]
</p><p>35 Under this model, there should be a neural mechanism responsible for the recognition of all facial expressions of emotion, which was assumed to take place in the limbic system. [sent-91, score-0.728]
</p><p>36 Furthermore, humans are only very good at recognizing a number of facial expressions of emotion. [sent-94, score-0.78]
</p><p>37 The reader should not have any problem recognizing the emotion in display even at the lowest of resolutions. [sent-101, score-0.578]
</p><p>38 However, humans are not as good at recognizing anger and sadness and are even worse at fear and disgust. [sent-102, score-0.463]
</p><p>39 Why are some facial conﬁgurations more easily recognizable than others? [sent-104, score-0.46]
</p><p>40 One possibility is that expressions such as joy and surprise involve larger face transformations than the others. [sent-105, score-0.508]
</p><p>41 Learning why some expressions are so readily classiﬁed by our visual system should facilitate the deﬁnition of the form and dimensions of the computational model of facial expressions of emotion. [sent-108, score-0.946]
</p><p>42 In particular, we provide a perspective on how machine learning and computer vision researcher should move forward if they are to deﬁne models based on the perception of facial expressions of emotion by humans. [sent-118, score-1.487]
</p><p>43 A Model of the Perception of Facial Expressions of Emotion In cognitive science and neuroscience researchers have been mostly concerned with models of the perception and classiﬁcation of the six facial expressions of emotion listed above. [sent-120, score-1.6]
</p><p>44 Sample feature vectors or regions of this feature space are used to represent each of these six emotion labels. [sent-122, score-0.582]
</p><p>45 This approach has a major drawback—it can only detect one emotion from a single image. [sent-123, score-0.575]
</p><p>46 Figure 2 shows images of faces expressing different surprises—happily surprised, angrily surprised, fearfully surprised, disgustedly surprised and the typically studied surprise. [sent-129, score-0.323]
</p><p>47 If we deﬁne an independent computational (face) space for a small number of emotion labels, we will only need sample faces of those few facial expressions of emotion. [sent-136, score-1.335]
</p><p>48 A large number of such expressions exist that are a combination of the six emotion categories listed above and, hence, the above list of six categories is a potential set of basic emotion classes. [sent-145, score-1.53]
</p><p>49 This allows for the perception of each emotion at different intensities, for example, less happy to exhilarant (Neth and Martinez, 2010). [sent-152, score-0.805]
</p><p>50 It also allows for the representation and recognition of a very large number of emotion categories without the need to have a categorical space for each or having to use many samples of each expression as in the continuous model. [sent-161, score-0.803]
</p><p>51 Inspired by this success, many algorithms developed in computer vision for the recognition of expressions of emotion have also used the appearance-based model (Torre and Cohn, 2011). [sent-175, score-0.87]
</p><p>52 This is also the case in the recognition of facial expressions of emotion (Neth and Martinez, 2010). [sent-196, score-1.274]
</p><p>53 These images all bear a neutral expression, that is, an expression associated to no emotion category. [sent-198, score-0.632]
</p><p>54 , 2010), since human perception is generalizing the learned features deﬁning these face spaces over to images with a different label. [sent-202, score-0.557]
</p><p>55 While the surprise-looking face has a large distance between eyes and 1595  M ARTINEZ AND D U  Figure 4: The four face images and schematics shown above all correspond to neutral expressions (i. [sent-211, score-0.658]
</p><p>56 , the sender does not intend to convey any emotion to the receiver). [sent-213, score-0.546]
</p><p>57 Expressions such as fear and disgust seem to be mostly (if not solely) based on shape features, making recognition less accurate and more susceptible to image manipulation. [sent-227, score-0.366]
</p><p>58 Thus, each of the six categories of emotion (happy, sad, surprise, angry, fear and disgust) is represented in a shape space given by classical statistical shape analysis. [sent-229, score-0.973]
</p><p>59 First the face and the shape of the major facial components are automatically detected. [sent-230, score-0.735]
</p><p>60 The dimensions of each emotion category can now be obtained with the use of an appropriate discriminant analysis method. [sent-237, score-0.621]
</p><p>61 Note that, in the proposed computational model, the face space deﬁning sadness corresponds to the rightbottom quadrant, while that of anger is given by the left-top quadrant. [sent-244, score-0.416]
</p><p>62 The dashed arrows in the ﬁgure reﬂect the fact that as we move away from the “mean” (or norm) face, recognition of that emotion become easier. [sent-245, score-0.62]
</p><p>63 1597  M ARTINEZ AND D U  As an example, the approach detailed in this section identiﬁes the distance between the brows and mouth and the width of the face as the two most important shape features of anger and sadness. [sent-246, score-0.548]
</p><p>64 The face space of anger and sadness is illustrated in Figure 5, where we have also plotted the feature vectors of the face set of Ekman and Friesen (1976). [sent-250, score-0.583]
</p><p>65 In each space, a simple linear classiﬁer in these spaces can successfully classify each emotion very accurately. [sent-253, score-0.566]
</p><p>66 Precise Detection of Faces and Facial Features As seen thus far, human perception is extremely tuned to small conﬁgural and shape changes. [sent-267, score-0.375]
</p><p>67 If we are to develop computer vision and machine learning systems that can emulate this capacity, the real problem to be addressed by the community is that of precise detection of faces and facial features (Ding and Martinez, 2010). [sent-268, score-0.714]
</p><p>68 The major drawback of this approach is that it yields imprecise detections of the learned object, because a window of an non-centered face is more similar to the learned template than a window with background (say, a tree). [sent-280, score-0.372]
</p><p>69 This approach can be used to precisely detect faces, eyes, mouth, or any 1598  A M ODEL OF THE P ERCEPTION OF FACIAL E XPRESSIONS OF E MOTION  Figure 6: Shown in the above are the six feature spaces deﬁning each of the six basic emotion categories. [sent-286, score-0.667]
</p><p>70 Figure 9: Precise detections of faces and facial features using the algorithm of (Ding and Martinez, 2010). [sent-297, score-0.657]
</p><p>71 1600  A M ODEL OF THE P ERCEPTION OF FACIAL E XPRESSIONS OF E MOTION  Figure 10: Manifold learning is ideal for learning mappings between face (object) images and their shape description vectors. [sent-298, score-0.336]
</p><p>72 other facial feature where there is a textural discrimination between it and its surroundings. [sent-299, score-0.46]
</p><p>73 Figure 9 shows some sample results of accurate detection of faces and facial features with this approach. [sent-300, score-0.639]
</p><p>74 Of course, it is impossible to be pixel accurate when marking the boundaries of each facial feature, because edges blur over several pixels. [sent-320, score-0.46]
</p><p>75 First, we designed a system that allows users to zoom in at any speciﬁed location to facilitate delineation of each of the facial features manually. [sent-323, score-0.48]
</p><p>76 Second, we asked three people (herein referred to as judges) to manually delineate each of the facial components of close to 4, 000 images of faces. [sent-324, score-0.552]
</p><p>77 But more targeted approaches are needed to deﬁne truly successful computational models of the perception of facial expressions of emotion. [sent-356, score-0.862]
</p><p>78 Most importantly though seems to be the fact that people are not very good at recognizing facial expressions of emotion even under favorable condition (Du and Martinez, 2011). [sent-360, score-1.286]
</p><p>79 However, we are not as good at recognizing anger and sadness and are worst at fear and disgust. [sent-362, score-0.392]
</p><p>80 , facial muscle positions) that is distinctive and readily detected by an observer at short or large distances. [sent-367, score-0.46]
</p><p>81 A computer vision system—especially a HCI—should make sure these expressions are accurately and robustly recognized across image degradation. [sent-369, score-0.342]
</p><p>82 A computer vision system should recognize these expressions in good quality images, but can be expected to fail as the image degrades due to resolution or other image manipulations. [sent-374, score-0.405]
</p><p>83 Under this model, the receiver has learned to identify those face conﬁgurations to some extent, but without the involvement of the sender—modifying the expression to maximize transmission of information through a noisy environment—the recognition of these emotions has remained poor. [sent-388, score-0.431]
</p><p>84 Another area that will require additional research is to exploit other types of facial expressions. [sent-390, score-0.46]
</p><p>85 However, without empirical proof for the need 1603  M ARTINEZ AND D U  of something more complex than linear combinations of basic emotion categories, such extensions are unlikely. [sent-396, score-0.546]
</p><p>86 Improvements may also be possible if it were to better understand how facial expressions of emotion affect these people. [sent-404, score-1.223]
</p><p>87 We know that autistic children do not perceive facial expressions of emotion as others do (Jemel et al. [sent-407, score-1.255]
</p><p>88 A modiﬁed computational model of the perception of facial expressions of emotion in autism could help design better teaching tools for this group and may bring us closer to understanding the syndrome. [sent-409, score-1.491]
</p><p>89 Conclusions In the present work we have summarized the development of a model of the perception of facial expressions of emotion by humans. [sent-415, score-1.408]
</p><p>90 A key idea in this model is to linearly combine a set of face spaces deﬁning some basic emotion categories. [sent-416, score-0.733]
</p><p>91 The model is consistent with our current understanding of human perception and can be successfully exploited to achieve great recognition results for computer vision and HCI applications. [sent-417, score-0.374]
</p><p>92 We conclude that to move the state of the art forward, face recognition research has to focus on a topic that has received little attention in recent years—precise, detailed detection of faces and facial features. [sent-419, score-0.86]
</p><p>93 Although we have focused our study on the recognition of facial expressions of emotion, we believe that the results apply to most face recognition tasks. [sent-420, score-0.946]
</p><p>94 Understanding emotions from standardized facial expressions in autism and normal development. [sent-462, score-0.926]
</p><p>95 Features versus context: An approach for precise and detailed detection and delineation of faces and facial features. [sent-495, score-0.638]
</p><p>96 Emotion perception in emotionless face images suggests a norm-based representation. [sent-625, score-0.413]
</p><p>97 Human facial expressions as adaptations: Evolutionary questions in facial expression. [sent-676, score-1.137]
</p><p>98 Smile through your fear and sadness: Transmitting and identifying facial expression signals over a range of viewing distances. [sent-691, score-0.596]
</p><p>99 Human and computer recognition of facial expressions of emotion. [sent-712, score-0.728]
</p><p>100 Can low level image differences account for the ability of human observers to discriminate facial identity? [sent-749, score-0.596]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('emotion', 0.546), ('facial', 0.46), ('expressions', 0.217), ('martinez', 0.204), ('perception', 0.185), ('face', 0.167), ('emotions', 0.166), ('anger', 0.129), ('gural', 0.12), ('sadness', 0.12), ('faces', 0.112), ('fear', 0.111), ('shape', 0.108), ('surprised', 0.095), ('categorical', 0.093), ('artinez', 0.092), ('autism', 0.083), ('ekman', 0.083), ('erception', 0.083), ('human', 0.082), ('hamsici', 0.074), ('happy', 0.074), ('neth', 0.074), ('humans', 0.071), ('xpressions', 0.071), ('surprise', 0.069), ('cognitive', 0.068), ('brows', 0.065), ('detections', 0.065), ('disgust', 0.065), ('categories', 0.064), ('motion', 0.063), ('images', 0.061), ('mouth', 0.059), ('vision', 0.056), ('joy', 0.055), ('recognition', 0.051), ('psychology', 0.049), ('neuroscience', 0.048), ('detection', 0.047), ('calder', 0.046), ('friesen', 0.046), ('muscles', 0.046), ('eyes', 0.046), ('odel', 0.045), ('recognize', 0.039), ('recognized', 0.038), ('aam', 0.037), ('gotardo', 0.037), ('happily', 0.037), ('occlusions', 0.037), ('susskind', 0.037), ('torre', 0.037), ('six', 0.036), ('hci', 0.036), ('category', 0.032), ('recognizing', 0.032), ('cognition', 0.032), ('perceive', 0.032), ('template', 0.031), ('image', 0.031), ('resolution', 0.031), ('people', 0.031), ('detect', 0.029), ('du', 0.029), ('ding', 0.029), ('disorders', 0.028), ('visual', 0.028), ('aleix', 0.028), ('angrily', 0.028), ('expressing', 0.027), ('nose', 0.026), ('expression', 0.025), ('dimensions', 0.024), ('continuous', 0.024), ('evolved', 0.024), ('happiness', 0.024), ('studies', 0.024), ('move', 0.023), ('discriminate', 0.023), ('windows', 0.023), ('appearance', 0.023), ('window', 0.022), ('learned', 0.022), ('subjects', 0.022), ('object', 0.022), ('eye', 0.021), ('imprecise', 0.021), ('landmarks', 0.021), ('listed', 0.021), ('rotation', 0.021), ('spaces', 0.02), ('features', 0.02), ('young', 0.02), ('discriminant', 0.019), ('researchers', 0.019), ('precise', 0.019), ('social', 0.019), ('cohn', 0.019), ('angry', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="6-tfidf-1" href="./jmlr-2012-A_Model_of_the_Perception_of_Facial_Expressions_of_Emotion_by_Humans%3A_Research_Overview_and_Perspectives.html">6 jmlr-2012-A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives</a></p>
<p>Author: Aleix Martinez, Shichuan Du</p><p>Abstract: In cognitive science and neuroscience, there have been two leading models describing how humans perceive and classify facial expressions of emotion—the continuous and the categorical model. The continuous model deﬁnes each facial expression of emotion as a feature vector in a face space. This model explains, for example, how expressions of emotion can be seen at different intensities. In contrast, the categorical model consists of C classiﬁers, each tuned to a speciﬁc emotion category. This model explains, among other ﬁndings, why the images in a morphing sequence between a happy and a surprise face are perceived as either happy or surprise but not something in between. While the continuous model has a more difﬁcult time justifying this latter ﬁnding, the categorical model is not as good when it comes to explaining how expressions are recognized at different intensities or modes. Most importantly, both models have problems explaining how one can recognize combinations of emotion categories such as happily surprised versus angrily surprised versus surprise. To resolve these issues, in the past several years, we have worked on a revised model that justiﬁes the results reported in the cognitive science and neuroscience literature. This model consists of C distinct continuous spaces. Multiple (compound) emotion categories can be recognized by linearly combining these C face spaces. The dimensions of these spaces are shown to be mostly conﬁgural. According to this model, the major task for the classiﬁcation of facial expressions of emotion is precise, detailed detection of facial landmarks rather than recognition. We provide an overview of the literature justifying the model, show how the resulting model can be employed to build algorithms for the recognition of facial expression of emotion, and propose research directions in machine learning and computer vision researchers to keep pushing the state of the art in these areas. We also discuss how the model can aid in stu</p><p>2 0.065593995 <a title="6-tfidf-2" href="./jmlr-2012-Discriminative_Hierarchical_Part-based_Models_for_Human_Parsing_and_Action_Recognition.html">32 jmlr-2012-Discriminative Hierarchical Part-based Models for Human Parsing and Action Recognition</a></p>
<p>Author: Yang Wang, Duan Tran, Zicheng Liao, David Forsyth</p><p>Abstract: We consider the problem of parsing human poses and recognizing their actions in static images with part-based models. Most previous work in part-based models only considers rigid parts (e.g., torso, head, half limbs) guided by human anatomy. We argue that this representation of parts is not necessarily appropriate. In this paper, we introduce hierarchical poselets—a new representation for modeling the pose conﬁguration of human bodies. Hierarchical poselets can be rigid parts, but they can also be parts that cover large portions of human bodies (e.g., torso + left arm). In the extreme case, they can be the whole bodies. The hierarchical poselets are organized in a hierarchical way via a structured model. Human parsing can be achieved by inferring the optimal labeling of this hierarchical model. The pose information captured by this hierarchical model can also be used as a intermediate representation for other high-level tasks. We demonstrate it in action recognition from static images. Keywords: human parsing, action recognition, part-based models, hierarchical poselets, maxmargin structured learning</p><p>3 0.059277497 <a title="6-tfidf-3" href="./jmlr-2012-Human_Gesture_Recognition_on_Product_Manifolds.html">50 jmlr-2012-Human Gesture Recognition on Product Manifolds</a></p>
<p>Author: Yui Man Lui</p><p>Abstract: Action videos are multidimensional data and can be naturally represented as data tensors. While tensor computing is widely used in computer vision, the geometry of tensor space is often ignored. The aim of this paper is to demonstrate the importance of the intrinsic geometry of tensor space which yields a very discriminating structure for action recognition. We characterize data tensors as points on a product manifold and model it statistically using least squares regression. To this aim, we factorize a data tensor relating to each order of the tensor using Higher Order Singular Value Decomposition (HOSVD) and then impose each factorized element on a Grassmann manifold. Furthermore, we account for underlying geometry on manifolds and formulate least squares regression as a composite function. This gives a natural extension from Euclidean space to manifolds. Consequently, classiﬁcation is performed using geodesic distance on a product manifold where each factor manifold is Grassmannian. Our method exploits appearance and motion without explicitly modeling the shapes and dynamics. We assess the proposed method using three gesture databases, namely the Cambridge hand-gesture, the UMD Keck body-gesture, and the CHALEARN gesture challenge data sets. Experimental results reveal that not only does the proposed method perform well on the standard benchmark data sets, but also it generalizes well on the one-shot-learning gesture challenge. Furthermore, it is based on a simple statistical model and the intrinsic geometry of tensor space. Keywords: gesture recognition, action recognition, Grassmann manifolds, product manifolds, one-shot-learning, kinect data</p><p>4 0.058605623 <a title="6-tfidf-4" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>Author: Helen Cooper, Eng-Jon Ong, Nicolas Pugeault, Richard Bowden</p><p>Abstract: This paper discusses sign language recognition using linguistic sub-units. It presents three types of sub-units for consideration; those learnt from appearance data as well as those inferred from both 2D or 3D tracking data. These sub-units are then combined using a sign level classiﬁer; here, two options are presented. The ﬁrst uses Markov Models to encode the temporal changes between sub-units. The second makes use of Sequential Pattern Boosting to apply discriminative feature selection at the same time as encoding temporal information. This approach is more robust to noise and performs well in signer independent tests, improving results from the 54% achieved by the Markov Chains to 76%. Keywords: sign language recognition, sequential pattern boosting, depth cameras, sub-units, signer independence, data set</p><p>5 0.048824459 <a title="6-tfidf-5" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>Author: Daniel L. Ly, Hod Lipson</p><p>Abstract: A hybrid dynamical system is a mathematical model suitable for describing an extensive spectrum of multi-modal, time-series behaviors, ranging from bouncing balls to air trafﬁc controllers. This paper describes multi-modal symbolic regression (MMSR): a learning algorithm to construct non-linear symbolic representations of discrete dynamical systems with continuous mappings from unlabeled, time-series data. MMSR consists of two subalgorithms—clustered symbolic regression, a method to simultaneously identify distinct behaviors while formulating their mathematical expressions, and transition modeling, an algorithm to infer symbolic inequalities that describe binary classiﬁcation boundaries. These subalgorithms are combined to infer hybrid dynamical systems as a collection of apt, mathematical expressions. MMSR is evaluated on a collection of four synthetic data sets and outperforms other multi-modal machine learning approaches in both accuracy and interpretability, even in the presence of noise. Furthermore, the versatility of MMSR is demonstrated by identifying and inferring classical expressions of transistor modes from recorded measurements. Keywords: hybrid dynamical systems, evolutionary computation, symbolic piecewise functions, symbolic binary classiﬁcation</p><p>6 0.045717701 <a title="6-tfidf-6" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>7 0.032560669 <a title="6-tfidf-7" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>8 0.029010544 <a title="6-tfidf-8" href="./jmlr-2012-Hope_and_Fear_for_Discriminative_Training_of_Statistical_Translation_Models.html">49 jmlr-2012-Hope and Fear for Discriminative Training of Statistical Translation Models</a></p>
<p>9 0.025278369 <a title="6-tfidf-9" href="./jmlr-2012-Finding_Recurrent_Patterns_from_Continuous_Sign_Language_Sentences_for_Automated_Extraction_of_Signs.html">45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</a></p>
<p>10 0.025234047 <a title="6-tfidf-10" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>11 0.024376679 <a title="6-tfidf-11" href="./jmlr-2012-Random_Search_for_Hyper-Parameter_Optimization.html">95 jmlr-2012-Random Search for Hyper-Parameter Optimization</a></p>
<p>12 0.024170198 <a title="6-tfidf-12" href="./jmlr-2012-Bayesian_Mixed-Effects_Inference_on_Classification_Performance_in_Hierarchical_Data_Sets.html">21 jmlr-2012-Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets</a></p>
<p>13 0.020467933 <a title="6-tfidf-13" href="./jmlr-2012-Active_Clustering_of_Biological_Sequences.html">12 jmlr-2012-Active Clustering of Biological Sequences</a></p>
<p>14 0.02026476 <a title="6-tfidf-14" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>15 0.019967964 <a title="6-tfidf-15" href="./jmlr-2012-Pairwise_Support_Vector_Machines_and_their_Application_to_Large_Scale_Problems.html">89 jmlr-2012-Pairwise Support Vector Machines and their Application to Large Scale Problems</a></p>
<p>16 0.019676195 <a title="6-tfidf-16" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>17 0.019518735 <a title="6-tfidf-17" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>18 0.019359337 <a title="6-tfidf-18" href="./jmlr-2012-Sparse_and_Unique_Nonnegative_Matrix_Factorization_Through_Data_Preprocessing.html">108 jmlr-2012-Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing</a></p>
<p>19 0.018654702 <a title="6-tfidf-19" href="./jmlr-2012-Multi_Kernel_Learning_with_Online-Batch_Optimization.html">74 jmlr-2012-Multi Kernel Learning with Online-Batch Optimization</a></p>
<p>20 0.018432321 <a title="6-tfidf-20" href="./jmlr-2012-A_Geometric_Approach_to_Sample_Compression.html">3 jmlr-2012-A Geometric Approach to Sample Compression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.087), (1, 0.014), (2, 0.153), (3, -0.008), (4, -0.03), (5, -0.073), (6, 0.046), (7, -0.051), (8, 0.061), (9, 0.018), (10, 0.122), (11, -0.059), (12, -0.014), (13, 0.016), (14, -0.037), (15, 0.02), (16, 0.011), (17, 0.076), (18, 0.048), (19, -0.001), (20, -0.009), (21, -0.016), (22, 0.037), (23, -0.031), (24, -0.0), (25, -0.109), (26, -0.103), (27, -0.106), (28, 0.021), (29, -0.091), (30, 0.083), (31, 0.058), (32, -0.057), (33, 0.044), (34, -0.101), (35, 0.368), (36, 0.046), (37, 0.352), (38, -0.165), (39, 0.069), (40, -0.059), (41, -0.07), (42, 0.031), (43, 0.033), (44, -0.024), (45, -0.076), (46, -0.219), (47, -0.047), (48, -0.073), (49, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9683013 <a title="6-lsi-1" href="./jmlr-2012-A_Model_of_the_Perception_of_Facial_Expressions_of_Emotion_by_Humans%3A_Research_Overview_and_Perspectives.html">6 jmlr-2012-A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives</a></p>
<p>Author: Aleix Martinez, Shichuan Du</p><p>Abstract: In cognitive science and neuroscience, there have been two leading models describing how humans perceive and classify facial expressions of emotion—the continuous and the categorical model. The continuous model deﬁnes each facial expression of emotion as a feature vector in a face space. This model explains, for example, how expressions of emotion can be seen at different intensities. In contrast, the categorical model consists of C classiﬁers, each tuned to a speciﬁc emotion category. This model explains, among other ﬁndings, why the images in a morphing sequence between a happy and a surprise face are perceived as either happy or surprise but not something in between. While the continuous model has a more difﬁcult time justifying this latter ﬁnding, the categorical model is not as good when it comes to explaining how expressions are recognized at different intensities or modes. Most importantly, both models have problems explaining how one can recognize combinations of emotion categories such as happily surprised versus angrily surprised versus surprise. To resolve these issues, in the past several years, we have worked on a revised model that justiﬁes the results reported in the cognitive science and neuroscience literature. This model consists of C distinct continuous spaces. Multiple (compound) emotion categories can be recognized by linearly combining these C face spaces. The dimensions of these spaces are shown to be mostly conﬁgural. According to this model, the major task for the classiﬁcation of facial expressions of emotion is precise, detailed detection of facial landmarks rather than recognition. We provide an overview of the literature justifying the model, show how the resulting model can be employed to build algorithms for the recognition of facial expression of emotion, and propose research directions in machine learning and computer vision researchers to keep pushing the state of the art in these areas. We also discuss how the model can aid in stu</p><p>2 0.66158426 <a title="6-lsi-2" href="./jmlr-2012-Discriminative_Hierarchical_Part-based_Models_for_Human_Parsing_and_Action_Recognition.html">32 jmlr-2012-Discriminative Hierarchical Part-based Models for Human Parsing and Action Recognition</a></p>
<p>Author: Yang Wang, Duan Tran, Zicheng Liao, David Forsyth</p><p>Abstract: We consider the problem of parsing human poses and recognizing their actions in static images with part-based models. Most previous work in part-based models only considers rigid parts (e.g., torso, head, half limbs) guided by human anatomy. We argue that this representation of parts is not necessarily appropriate. In this paper, we introduce hierarchical poselets—a new representation for modeling the pose conﬁguration of human bodies. Hierarchical poselets can be rigid parts, but they can also be parts that cover large portions of human bodies (e.g., torso + left arm). In the extreme case, they can be the whole bodies. The hierarchical poselets are organized in a hierarchical way via a structured model. Human parsing can be achieved by inferring the optimal labeling of this hierarchical model. The pose information captured by this hierarchical model can also be used as a intermediate representation for other high-level tasks. We demonstrate it in action recognition from static images. Keywords: human parsing, action recognition, part-based models, hierarchical poselets, maxmargin structured learning</p><p>3 0.45990738 <a title="6-lsi-3" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>Author: Daniel L. Ly, Hod Lipson</p><p>Abstract: A hybrid dynamical system is a mathematical model suitable for describing an extensive spectrum of multi-modal, time-series behaviors, ranging from bouncing balls to air trafﬁc controllers. This paper describes multi-modal symbolic regression (MMSR): a learning algorithm to construct non-linear symbolic representations of discrete dynamical systems with continuous mappings from unlabeled, time-series data. MMSR consists of two subalgorithms—clustered symbolic regression, a method to simultaneously identify distinct behaviors while formulating their mathematical expressions, and transition modeling, an algorithm to infer symbolic inequalities that describe binary classiﬁcation boundaries. These subalgorithms are combined to infer hybrid dynamical systems as a collection of apt, mathematical expressions. MMSR is evaluated on a collection of four synthetic data sets and outperforms other multi-modal machine learning approaches in both accuracy and interpretability, even in the presence of noise. Furthermore, the versatility of MMSR is demonstrated by identifying and inferring classical expressions of transistor modes from recorded measurements. Keywords: hybrid dynamical systems, evolutionary computation, symbolic piecewise functions, symbolic binary classiﬁcation</p><p>4 0.30943376 <a title="6-lsi-4" href="./jmlr-2012-Human_Gesture_Recognition_on_Product_Manifolds.html">50 jmlr-2012-Human Gesture Recognition on Product Manifolds</a></p>
<p>Author: Yui Man Lui</p><p>Abstract: Action videos are multidimensional data and can be naturally represented as data tensors. While tensor computing is widely used in computer vision, the geometry of tensor space is often ignored. The aim of this paper is to demonstrate the importance of the intrinsic geometry of tensor space which yields a very discriminating structure for action recognition. We characterize data tensors as points on a product manifold and model it statistically using least squares regression. To this aim, we factorize a data tensor relating to each order of the tensor using Higher Order Singular Value Decomposition (HOSVD) and then impose each factorized element on a Grassmann manifold. Furthermore, we account for underlying geometry on manifolds and formulate least squares regression as a composite function. This gives a natural extension from Euclidean space to manifolds. Consequently, classiﬁcation is performed using geodesic distance on a product manifold where each factor manifold is Grassmannian. Our method exploits appearance and motion without explicitly modeling the shapes and dynamics. We assess the proposed method using three gesture databases, namely the Cambridge hand-gesture, the UMD Keck body-gesture, and the CHALEARN gesture challenge data sets. Experimental results reveal that not only does the proposed method perform well on the standard benchmark data sets, but also it generalizes well on the one-shot-learning gesture challenge. Furthermore, it is based on a simple statistical model and the intrinsic geometry of tensor space. Keywords: gesture recognition, action recognition, Grassmann manifolds, product manifolds, one-shot-learning, kinect data</p><p>5 0.26525205 <a title="6-lsi-5" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>Author: Stephen Gould</p><p>Abstract: We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data ﬂows. Keywords: machine learning, graphical models, computer vision, open-source software</p><p>6 0.22822176 <a title="6-lsi-6" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>7 0.175952 <a title="6-lsi-7" href="./jmlr-2012-A_Geometric_Approach_to_Sample_Compression.html">3 jmlr-2012-A Geometric Approach to Sample Compression</a></p>
<p>8 0.16532601 <a title="6-lsi-8" href="./jmlr-2012-Hope_and_Fear_for_Discriminative_Training_of_Statistical_Translation_Models.html">49 jmlr-2012-Hope and Fear for Discriminative Training of Statistical Translation Models</a></p>
<p>9 0.16396102 <a title="6-lsi-9" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>10 0.16145515 <a title="6-lsi-10" href="./jmlr-2012-Mal-ID%3A_Automatic_Malware_Detection_Using_Common_Segment_Analysis_and_Meta-Features.html">63 jmlr-2012-Mal-ID: Automatic Malware Detection Using Common Segment Analysis and Meta-Features</a></p>
<p>11 0.15953165 <a title="6-lsi-11" href="./jmlr-2012-Algebraic_Geometric_Comparison_of_Probability_Distributions.html">15 jmlr-2012-Algebraic Geometric Comparison of Probability Distributions</a></p>
<p>12 0.14419059 <a title="6-lsi-12" href="./jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</a></p>
<p>13 0.14313881 <a title="6-lsi-13" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>14 0.13957182 <a title="6-lsi-14" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>15 0.12765226 <a title="6-lsi-15" href="./jmlr-2012-Integrating_a_Partial_Model_into_Model_Free_Reinforcement_Learning.html">51 jmlr-2012-Integrating a Partial Model into Model Free Reinforcement Learning</a></p>
<p>16 0.12567191 <a title="6-lsi-16" href="./jmlr-2012-Nonparametric_Guidance_of_Autoencoder_Representations_using_Label_Information.html">78 jmlr-2012-Nonparametric Guidance of Autoencoder Representations using Label Information</a></p>
<p>17 0.12000523 <a title="6-lsi-17" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>18 0.11404362 <a title="6-lsi-18" href="./jmlr-2012-Metric_and_Kernel_Learning_Using_a_Linear_Transformation.html">66 jmlr-2012-Metric and Kernel Learning Using a Linear Transformation</a></p>
<p>19 0.11400861 <a title="6-lsi-19" href="./jmlr-2012-Structured_Sparsity_via_Alternating_Direction_Methods.html">112 jmlr-2012-Structured Sparsity via Alternating Direction Methods</a></p>
<p>20 0.11186787 <a title="6-lsi-20" href="./jmlr-2012-Active_Clustering_of_Biological_Sequences.html">12 jmlr-2012-Active Clustering of Biological Sequences</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(21, 0.025), (26, 0.025), (27, 0.016), (29, 0.025), (35, 0.021), (49, 0.013), (56, 0.01), (69, 0.598), (75, 0.033), (92, 0.046), (96, 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89150286 <a title="6-lda-1" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>Author: Joonseok Lee, Mingxuan Sun, Guy Lebanon</p><p>Abstract: Recommendation systems are important business applications with signiﬁcant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms. Keywords: recommender systems, collaborative ﬁltering, evaluation metrics</p><p>same-paper 2 0.87598252 <a title="6-lda-2" href="./jmlr-2012-A_Model_of_the_Perception_of_Facial_Expressions_of_Emotion_by_Humans%3A_Research_Overview_and_Perspectives.html">6 jmlr-2012-A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives</a></p>
<p>Author: Aleix Martinez, Shichuan Du</p><p>Abstract: In cognitive science and neuroscience, there have been two leading models describing how humans perceive and classify facial expressions of emotion—the continuous and the categorical model. The continuous model deﬁnes each facial expression of emotion as a feature vector in a face space. This model explains, for example, how expressions of emotion can be seen at different intensities. In contrast, the categorical model consists of C classiﬁers, each tuned to a speciﬁc emotion category. This model explains, among other ﬁndings, why the images in a morphing sequence between a happy and a surprise face are perceived as either happy or surprise but not something in between. While the continuous model has a more difﬁcult time justifying this latter ﬁnding, the categorical model is not as good when it comes to explaining how expressions are recognized at different intensities or modes. Most importantly, both models have problems explaining how one can recognize combinations of emotion categories such as happily surprised versus angrily surprised versus surprise. To resolve these issues, in the past several years, we have worked on a revised model that justiﬁes the results reported in the cognitive science and neuroscience literature. This model consists of C distinct continuous spaces. Multiple (compound) emotion categories can be recognized by linearly combining these C face spaces. The dimensions of these spaces are shown to be mostly conﬁgural. According to this model, the major task for the classiﬁcation of facial expressions of emotion is precise, detailed detection of facial landmarks rather than recognition. We provide an overview of the literature justifying the model, show how the resulting model can be employed to build algorithms for the recognition of facial expression of emotion, and propose research directions in machine learning and computer vision researchers to keep pushing the state of the art in these areas. We also discuss how the model can aid in stu</p><p>3 0.86349368 <a title="6-lda-3" href="./jmlr-2012-Human_Gesture_Recognition_on_Product_Manifolds.html">50 jmlr-2012-Human Gesture Recognition on Product Manifolds</a></p>
<p>Author: Yui Man Lui</p><p>Abstract: Action videos are multidimensional data and can be naturally represented as data tensors. While tensor computing is widely used in computer vision, the geometry of tensor space is often ignored. The aim of this paper is to demonstrate the importance of the intrinsic geometry of tensor space which yields a very discriminating structure for action recognition. We characterize data tensors as points on a product manifold and model it statistically using least squares regression. To this aim, we factorize a data tensor relating to each order of the tensor using Higher Order Singular Value Decomposition (HOSVD) and then impose each factorized element on a Grassmann manifold. Furthermore, we account for underlying geometry on manifolds and formulate least squares regression as a composite function. This gives a natural extension from Euclidean space to manifolds. Consequently, classiﬁcation is performed using geodesic distance on a product manifold where each factor manifold is Grassmannian. Our method exploits appearance and motion without explicitly modeling the shapes and dynamics. We assess the proposed method using three gesture databases, namely the Cambridge hand-gesture, the UMD Keck body-gesture, and the CHALEARN gesture challenge data sets. Experimental results reveal that not only does the proposed method perform well on the standard benchmark data sets, but also it generalizes well on the one-shot-learning gesture challenge. Furthermore, it is based on a simple statistical model and the intrinsic geometry of tensor space. Keywords: gesture recognition, action recognition, Grassmann manifolds, product manifolds, one-shot-learning, kinect data</p><p>4 0.31210479 <a title="6-lda-4" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>Author: Helen Cooper, Eng-Jon Ong, Nicolas Pugeault, Richard Bowden</p><p>Abstract: This paper discusses sign language recognition using linguistic sub-units. It presents three types of sub-units for consideration; those learnt from appearance data as well as those inferred from both 2D or 3D tracking data. These sub-units are then combined using a sign level classiﬁer; here, two options are presented. The ﬁrst uses Markov Models to encode the temporal changes between sub-units. The second makes use of Sequential Pattern Boosting to apply discriminative feature selection at the same time as encoding temporal information. This approach is more robust to noise and performs well in signer independent tests, improving results from the 54% achieved by the Markov Chains to 76%. Keywords: sign language recognition, sequential pattern boosting, depth cameras, sub-units, signer independence, data set</p><p>5 0.29959178 <a title="6-lda-5" href="./jmlr-2012-Finding_Recurrent_Patterns_from_Continuous_Sign_Language_Sentences_for_Automated_Extraction_of_Signs.html">45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</a></p>
<p>Author: Sunita Nayak, Kester Duncan, Sudeep Sarkar, Barbara Loeding</p><p>Abstract: We present a probabilistic framework to automatically learn models of recurring signs from multiple sign language video sequences containing the vocabulary of interest. We extract the parts of the signs that are present in most occurrences of the sign in context and are robust to the variations produced by adjacent signs. Each sentence video is ﬁrst transformed into a multidimensional time series representation, capturing the motion and shape aspects of the sign. Skin color blobs are extracted from frames of color video sequences, and a probabilistic relational distribution is formed for each frame using the contour and edge pixels from the skin blobs. Each sentence is represented as a trajectory in a low dimensional space called the space of relational distributions. Given these time series trajectories, we extract signemes from multiple sentences concurrently using iterated conditional modes (ICM). We show results by learning single signs from a collection of sentences with one common pervading sign, multiple signs from a collection of sentences with more than one common sign, and single signs from a mixed collection of sentences. The extracted signemes demonstrate that our approach is robust to some extent to the variations produced within a sign due to different contexts. We also show results whereby these learned sign models are used for spotting signs in test sequences. Keywords: pattern extraction, sign language recognition, signeme extraction, sign modeling, iterated conditional modes</p><p>6 0.28841546 <a title="6-lda-6" href="./jmlr-2012-Learning_Symbolic_Representations_of_Hybrid_Dynamical_Systems.html">57 jmlr-2012-Learning Symbolic Representations of Hybrid Dynamical Systems</a></p>
<p>7 0.28252193 <a title="6-lda-7" href="./jmlr-2012-Robust_Kernel_Density_Estimation.html">100 jmlr-2012-Robust Kernel Density Estimation</a></p>
<p>8 0.26886564 <a title="6-lda-8" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>9 0.2372639 <a title="6-lda-9" href="./jmlr-2012-Online_Learning_in_the_Embedded_Manifold_of_Low-rank_Matrices.html">83 jmlr-2012-Online Learning in the Embedded Manifold of Low-rank Matrices</a></p>
<p>10 0.22959945 <a title="6-lda-10" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>11 0.22348116 <a title="6-lda-11" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>12 0.22199929 <a title="6-lda-12" href="./jmlr-2012-Positive_Semidefinite_Metric_Learning_Using_Boosting-like_Algorithms.html">92 jmlr-2012-Positive Semidefinite Metric Learning Using Boosting-like Algorithms</a></p>
<p>13 0.21960646 <a title="6-lda-13" href="./jmlr-2012-Efficient_Methods_for_Robust_Classification_Under_Uncertainty_in_Kernel_Matrices.html">36 jmlr-2012-Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices</a></p>
<p>14 0.21534917 <a title="6-lda-14" href="./jmlr-2012-Non-Sparse_Multiple_Kernel_Fisher_Discriminant_Analysis.html">77 jmlr-2012-Non-Sparse Multiple Kernel Fisher Discriminant Analysis</a></p>
<p>15 0.21451654 <a title="6-lda-15" href="./jmlr-2012-Facilitating_Score_and_Causal_Inference_Trees_for_Large_Observational_Studies.html">42 jmlr-2012-Facilitating Score and Causal Inference Trees for Large Observational Studies</a></p>
<p>16 0.21361713 <a title="6-lda-16" href="./jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</a></p>
<p>17 0.21103483 <a title="6-lda-17" href="./jmlr-2012-An_Improved_GLMNET_for_L1-regularized_Logistic_Regression.html">18 jmlr-2012-An Improved GLMNET for L1-regularized Logistic Regression</a></p>
<p>18 0.21017805 <a title="6-lda-18" href="./jmlr-2012-A_Local_Spectral_Method_for_Graphs%3A_With_Applications_to_Improving_Graph_Partitions_and_Exploring_Data_Graphs_Locally.html">5 jmlr-2012-A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally</a></p>
<p>19 0.20931053 <a title="6-lda-19" href="./jmlr-2012-Distance_Metric_Learning_with_Eigenvalue_Optimization.html">33 jmlr-2012-Distance Metric Learning with Eigenvalue Optimization</a></p>
<p>20 0.20923294 <a title="6-lda-20" href="./jmlr-2012-Sparse_and_Unique_Nonnegative_Matrix_Factorization_Through_Data_Preprocessing.html">108 jmlr-2012-Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
