<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>90 jmlr-2012-Pattern for Python</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-90" href="#">jmlr2012-90</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>90 jmlr-2012-Pattern for Python</h1>
<br/><p>Source: <a title="jmlr-2012-90-pdf" href="http://jmlr.org/papers/volume13/desmedt12a/desmedt12a.pdf">pdf</a></p><p>Author: Tom De Smedt, Walter Daelemans</p><p>Abstract: Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classiﬁers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern. Keywords: Python, data mining, natural language processing, machine learning, graph networks</p><p>Reference: <a title="jmlr-2012-90-reference" href="../jmlr2012_reference/jmlr-2012-Pattern_for_Python_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 BE  CLiPS Computational Linguistics Group University of Antwerp 2000 Antwerp, Belgium  Editor: Cheng Soon Ong  Abstract Pattern is a package for Python 2. [sent-7, score-0.066]
</p><p>2 It is well documented and bundled with 30+ examples and 350+ unit tests. [sent-9, score-0.115]
</p><p>3 The source code is licensed under BSD and available from http://www. [sent-10, score-0.142]
</p><p>4 Keywords: Python, data mining, natural language processing, machine learning, graph networks  1. [sent-15, score-0.117]
</p><p>5 Introduction The World Wide Web is an immense collection of linguistic information that has in the last decade gathered attention as a valuable resource for tasks such as machine translation, opinion mining and trend detection, that is, “Web as Corpus” (Kilgarriff and Grefenstette, 2003). [sent-16, score-0.048]
</p><p>6 This use of the WWW poses a challenge since the Web is interspersed with code (HTML markup) and lacks metadata (language identiﬁcation, part-of-speech tags, semantic labels). [sent-17, score-0.121]
</p><p>7 “Pattern” (BSD license) is a Python package for web mining, natural language processing, machine learning and network analysis, with a focus on ease-of-use. [sent-18, score-0.312]
</p><p>8 It offers a mash-up of tools often used when harnessing the Web as a corpus, which usually requires several independent toolkits chained together in a practical application. [sent-19, score-0.183]
</p><p>9 Several such toolkits with a user interface exist in the scientiﬁc community, for example ORANGE (Demˇar et al. [sent-20, score-0.086]
</p><p>10 By contrast, PATTERN is more related to toolkits such as NLTK (Bird et al. [sent-23, score-0.086]
</p><p>11 The package aims to be useful to both a scientiﬁc and a non-scientiﬁc audience. [sent-28, score-0.066]
</p><p>12 We believe that PATTERN is valuable as a learning environment for students, as a rapid development framework for web developers, and in research projects with a short development cycle. [sent-32, score-0.24]
</p><p>13 Text is mined from the web and searched by syntax and semantics. [sent-35, score-0.374]
</p><p>14 Package Overview PATTERN is organized in separate modules that can be chained together, as shown in Figure 1. [sent-38, score-0.101]
</p><p>15 web Tools for web data mining, using a download mechanism that supports caching, proxies, asynchronous requests and redirection. [sent-45, score-0.332]
</p><p>16 A SearchEngine class provides a uniform API to multiple web services: Google, Bing, Yahoo! [sent-46, score-0.166]
</p><p>17 , Twitter, Wikipedia, Flickr and news feeds using FEED PARSER (packages. [sent-47, score-0.04]
</p><p>18 The module includes an HTML parser based on BEAUTIFUL SOUP (crummy. [sent-50, score-0.407]
</p><p>19 com/software/beautifulsoup), a PDF parser based on PDF M INER (unixuser. [sent-51, score-0.259]
</p><p>20 en Fast, regular expressions-based shallow parser for English (identiﬁes sentence constituents, e. [sent-54, score-0.472]
</p><p>21 , nouns, verbs), using a ﬁnite state part-of-speech tagger (Brill, 1992) extended with a tokenizer, lemmatizer and chunker. [sent-56, score-0.075]
</p><p>22 A parser with higher accuracy (MBSP) can be plugged in. [sent-58, score-0.259]
</p><p>23 The module has a Sentence class for parse tree traversal, functions for singularization/pluralization (Conway, 1998), conjugation, modality and sentiment analysis. [sent-59, score-0.344]
</p><p>24 It comes bundled with WORDNET 3 (Fellbaum, 1998) and PYWORDNET. [sent-60, score-0.115]
</p><p>25 en for Dutch, using the BRILL - NL language model (Geertzen, 2010). [sent-63, score-0.08]
</p><p>26 Contributors are encouraged to read the developer documentation on how to add support for other languages. [sent-64, score-0.143]
</p><p>27 Documents are lemmatized bag-of-words that can be grouped in a sparse corpus to compute TF-IDF, distance metrics (cosine, Euclidean, Manhattan, Hamming) and dimension reduction (Latent Semantic Analysis). [sent-75, score-0.186]
</p><p>28 The module includes a hierarchical and a k-means clustering algorithm, optimized with the kmeans++ initialization algorithm (Arthur and Vassilvitskii, 2007) and triangle inequality (Elkan, 2003). [sent-76, score-0.148]
</p><p>29 A Naive Bayes, a k-NN, and a SVM classiﬁer using LIBSVM (Chang and Li, 2011) are included, with tools for feature selection (information gain) and K-fold cross validation. [sent-77, score-0.033]
</p><p>30 graph Graph data structure using Node, Edge and Graph classes, useful (for example) for modeling semantic networks. [sent-79, score-0.091]
</p><p>31 The module has algorithms for shortest path ﬁnding, subgraph partitioning, eigenvector centrality and betweenness centrality (Brandes, 2001). [sent-80, score-0.468]
</p><p>32 The module has a force-based layout algorithm that positions nodes in 2D space. [sent-82, score-0.148]
</p><p>33 Evaluation metrics including a code proﬁler, functions for accuracy, precision and recall, confusion matrix, inter-rater agreement (Fleiss’ kappa), string similarity (Levenshtein, Dice) and readability (Flesch). [sent-87, score-0.114]
</p><p>34 Example Script As an example, we chain together four PATTERN modules to train a k-NN classiﬁer on adjectives mined from Twitter. [sent-91, score-0.224]
</p><p>35 First, we mine 1,500 tweets with the hashtag #win or #fail (our classes), for example: “$20 tip off a sweet little old lady today #win”. [sent-92, score-0.075]
</p><p>36 We parse the part-of-speech tags for each tweet, keeping adjectives. [sent-93, score-0.158]
</p><p>37 We group the adjective vectors in a corpus and use it to train the classiﬁer. [sent-94, score-0.335]
</p><p>38 vector  import import import import  Twitter Sentence, parse search Document, Corpus, KNN  corpus = Corpus() for i in range(1,15): for tweet in Twitter(). [sent-103, score-0.729]
</p><p>39 lower() s = Sentence(parse(s)) s = search('JJ', s) # JJ = adjective s = [match[0]. [sent-108, score-0.149]
</p><p>40 append(Document(s, type=p)) classifier = KNN() for document in corpus: classifier. [sent-111, score-0.095]
</p><p>41 classify('stupid') # yields 'FAIL'd  Figure 2: Example source code for a k-NN classiﬁer trained on Twitter messages. [sent-114, score-0.142]
</p><p>42 Case Study As a case study, we used PATTERN to create a Dutch sentiment lexicon (De Smedt and Daelemans, 2012). [sent-116, score-0.239]
</p><p>43 We mined online Dutch book reviews and extracted the 1,000 most frequent adjectives. [sent-117, score-0.147]
</p><p>44 These were manually annotated with positivity, negativity, and subjectivity scores. [sent-118, score-0.178]
</p><p>45 , 2007) we extracted the most frequent nouns and the adjectives preceding those nouns. [sent-121, score-0.222]
</p><p>46 This results in a vector space with approximately 5,750 adjective vectors with nouns as features. [sent-122, score-0.261]
</p><p>47 For each annotated adjective we then computed k-NN and inherited its scores to neighbor adjectives. [sent-123, score-0.215]
</p><p>48 Documentation PATTERN comes bundled with examples and unit tests. [sent-127, score-0.115]
</p><p>49 The documentation contains a quick overview, installation instructions, and for each module a detailed page with the API reference, examples of use and a discussion of the scientiﬁc principles. [sent-128, score-0.234]
</p><p>50 The documentation assumes no prior knowledge, except for a background in Python programming. [sent-129, score-0.086]
</p><p>51 Source Code is written in pure Python, meaning that we sacriﬁce performance for development speed and readability (i. [sent-133, score-0.084]
</p><p>52 The package runs on all platforms and has no dependencies, with the exception of NumPy when LSA is used. [sent-136, score-0.066]
</p><p>53 The source code is released under a BSD license, so it can be incorporated into proprietary products or used in combination with other open source packages such as SCRAPY (web mining), NLTK (natural language processing), PYBRAIN and PYML (machine learning) and NETWORKX (network analysis). [sent-141, score-0.297]
</p><p>54 , 2010), a robust, memory-based shallow parser built on the TIMBL machine learning software. [sent-143, score-0.355]
</p><p>55 The API’s for the PATTERN parser and MBSP are identical. [sent-144, score-0.259]
</p><p>56 Gephi: An open source software for exploring and manipulating networks. [sent-150, score-0.075]
</p><p>57 Jeroen geertzen :: software & demos : Brill-nl, June 2010. [sent-184, score-0.075]
</p><p>58 Introduction to the special issue on the web as corpus. [sent-191, score-0.166]
</p><p>59 A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. [sent-197, score-0.112]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('parser', 0.259), ('smedt', 0.224), ('python', 0.187), ('daelemans', 0.187), ('dutch', 0.187), ('twitter', 0.187), ('corpus', 0.186), ('web', 0.166), ('walter', 0.16), ('adjective', 0.149), ('module', 0.148), ('tom', 0.133), ('centrality', 0.128), ('sentiment', 0.124), ('win', 0.124), ('sentence', 0.117), ('lexicon', 0.115), ('bundled', 0.115), ('brill', 0.112), ('mbsp', 0.112), ('mined', 0.112), ('nouns', 0.112), ('subjectivity', 0.112), ('wikipedia', 0.112), ('html', 0.106), ('import', 0.099), ('bsd', 0.096), ('syntax', 0.096), ('shallow', 0.096), ('wordnet', 0.096), ('documentation', 0.086), ('tags', 0.086), ('toolkits', 0.086), ('language', 0.08), ('source', 0.075), ('adjectives', 0.075), ('antwerp', 0.075), ('bastian', 0.075), ('geertzen', 0.075), ('gephi', 0.075), ('hagberg', 0.075), ('jeroen', 0.075), ('kilgarriff', 0.075), ('mathieu', 0.075), ('medt', 0.075), ('networkx', 0.075), ('nltk', 0.075), ('ordelman', 0.075), ('sweet', 0.075), ('tagger', 0.075), ('tweet', 0.075), ('twnc', 0.075), ('pattern', 0.073), ('parse', 0.072), ('code', 0.067), ('annotated', 0.066), ('api', 0.066), ('package', 0.066), ('beautiful', 0.064), ('betweenness', 0.064), ('ua', 0.064), ('pybrain', 0.064), ('schaul', 0.064), ('print', 0.064), ('ython', 0.064), ('chained', 0.064), ('document', 0.063), ('bird', 0.057), ('english', 0.057), ('developer', 0.057), ('clips', 0.057), ('pang', 0.057), ('semantic', 0.054), ('dem', 0.05), ('pdf', 0.05), ('mining', 0.048), ('readability', 0.047), ('knn', 0.047), ('orange', 0.042), ('news', 0.04), ('license', 0.04), ('development', 0.037), ('modules', 0.037), ('graph', 0.037), ('arthur', 0.036), ('naive', 0.036), ('google', 0.036), ('linguistics', 0.035), ('frequent', 0.035), ('chang', 0.033), ('libsvm', 0.033), ('tools', 0.033), ('ar', 0.032), ('asch', 0.032), ('parsed', 0.032), ('terribly', 0.032), ('vassilvitskii', 0.032), ('classifier', 0.032), ('contributors', 0.032), ('reilly', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="90-tfidf-1" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>Author: Tom De Smedt, Walter Daelemans</p><p>Abstract: Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classiﬁers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern. Keywords: Python, data mining, natural language processing, machine learning, graph networks</p><p>2 0.065119788 <a title="90-tfidf-2" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>Author: Félix-Antoine Fortin, François-Michel De Rainville, Marc-André Gardner, Marc Parizeau, Christian Gagné</p><p>Abstract: DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black-box frameworks. Freely available with extensive documentation at http://deap.gel.ulaval.ca, DEAP is an open source project under an LGPL license. Keywords: distributed evolutionary algorithms, software tools</p><p>3 0.061070994 <a title="90-tfidf-3" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>Author: Stephen Gould</p><p>Abstract: We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data ﬂows. Keywords: machine learning, graphical models, computer vision, open-source software</p><p>4 0.057863012 <a title="90-tfidf-4" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>Author: Marinka Žitnik, Blaž Zupan</p><p>Abstract: NIMFA is an open-source Python library that provides a uniﬁed interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA’s component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks. Keywords: nonnegative matrix factorization, initialization methods, quality measures, scripting, Python</p><p>5 0.056199707 <a title="90-tfidf-5" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>6 0.045538414 <a title="90-tfidf-6" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>7 0.04547051 <a title="90-tfidf-7" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>8 0.043376967 <a title="90-tfidf-8" href="./jmlr-2012-Finding_Recurrent_Patterns_from_Continuous_Sign_Language_Sentences_for_Automated_Extraction_of_Signs.html">45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</a></p>
<p>9 0.043310747 <a title="90-tfidf-9" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>10 0.038679823 <a title="90-tfidf-10" href="./jmlr-2012-Hope_and_Fear_for_Discriminative_Training_of_Statistical_Translation_Models.html">49 jmlr-2012-Hope and Fear for Discriminative Training of Statistical Translation Models</a></p>
<p>11 0.036372881 <a title="90-tfidf-11" href="./jmlr-2012-PREA%3A_Personalized_Recommendation_Algorithms_Toolkit.html">88 jmlr-2012-PREA: Personalized Recommendation Algorithms Toolkit</a></p>
<p>12 0.03484026 <a title="90-tfidf-12" href="./jmlr-2012-MULTIBOOST%3A_A_Multi-purpose_Boosting_Package.html">62 jmlr-2012-MULTIBOOST: A Multi-purpose Boosting Package</a></p>
<p>13 0.032323606 <a title="90-tfidf-13" href="./jmlr-2012-The_huge_Package_for_High-dimensional_Undirected_Graph_Estimation_in_R.html">113 jmlr-2012-The huge Package for High-dimensional Undirected Graph Estimation in R</a></p>
<p>14 0.02988068 <a title="90-tfidf-14" href="./jmlr-2012-Sally%3A_A_Tool_for_Embedding_Strings_in_Vector_Spaces.html">102 jmlr-2012-Sally: A Tool for Embedding Strings in Vector Spaces</a></p>
<p>15 0.027084816 <a title="90-tfidf-15" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>16 0.026193706 <a title="90-tfidf-16" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>17 0.023456959 <a title="90-tfidf-17" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>18 0.022402655 <a title="90-tfidf-18" href="./jmlr-2012-Mal-ID%3A_Automatic_Malware_Detection_Using_Common_Segment_Analysis_and_Meta-Features.html">63 jmlr-2012-Mal-ID: Automatic Malware Detection Using Common Segment Analysis and Meta-Features</a></p>
<p>19 0.021315718 <a title="90-tfidf-19" href="./jmlr-2012-Random_Search_for_Hyper-Parameter_Optimization.html">95 jmlr-2012-Random Search for Hyper-Parameter Optimization</a></p>
<p>20 0.019793747 <a title="90-tfidf-20" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.088), (1, 0.024), (2, 0.176), (3, -0.033), (4, 0.01), (5, 0.059), (6, 0.145), (7, -0.018), (8, -0.026), (9, -0.052), (10, -0.028), (11, -0.048), (12, 0.155), (13, -0.145), (14, -0.086), (15, 0.017), (16, 0.026), (17, -0.104), (18, -0.149), (19, -0.034), (20, -0.036), (21, 0.042), (22, -0.085), (23, -0.017), (24, -0.02), (25, -0.222), (26, 0.067), (27, -0.042), (28, -0.018), (29, 0.053), (30, -0.085), (31, 0.011), (32, -0.026), (33, 0.019), (34, -0.029), (35, -0.186), (36, -0.009), (37, -0.027), (38, 0.134), (39, 0.17), (40, 0.096), (41, -0.064), (42, 0.124), (43, 0.28), (44, -0.03), (45, -0.069), (46, 0.216), (47, -0.061), (48, -0.048), (49, 0.159)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97624326 <a title="90-lsi-1" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>Author: Tom De Smedt, Walter Daelemans</p><p>Abstract: Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classiﬁers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern. Keywords: Python, data mining, natural language processing, machine learning, graph networks</p><p>2 0.50487399 <a title="90-lsi-2" href="./jmlr-2012-DEAP%3A_Evolutionary_Algorithms_Made_Easy.html">31 jmlr-2012-DEAP: Evolutionary Algorithms Made Easy</a></p>
<p>Author: Félix-Antoine Fortin, François-Michel De Rainville, Marc-André Gardner, Marc Parizeau, Christian Gagné</p><p>Abstract: DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black-box frameworks. Freely available with extensive documentation at http://deap.gel.ulaval.ca, DEAP is an open source project under an LGPL license. Keywords: distributed evolutionary algorithms, software tools</p><p>3 0.43649286 <a title="90-lsi-3" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>Author: Stephen Gould</p><p>Abstract: We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data ﬂows. Keywords: machine learning, graphical models, computer vision, open-source software</p><p>4 0.42915043 <a title="90-lsi-4" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>Author: David Verstraeten, Benjamin Schrauwen, Sander Dieleman, Philemon Brakel, Pieter Buteneers, Dejan Pecevski</p><p>Abstract: Oger (OrGanic Environment for Reservoir computing) is a Python toolbox for building, training and evaluating modular learning architectures on large data sets. It builds on MDP for its modularity, and adds processing of sequential data sets, gradient descent training, several crossvalidation schemes and parallel parameter optimization methods. Additionally, several learning algorithms are implemented, such as different reservoir implementations (both sigmoid and spiking), ridge regression, conditional restricted Boltzmann machine (CRBM) and others, including GPU accelerated versions. Oger is released under the GNU LGPL, and is available from http: //organic.elis.ugent.be/oger. Keywords: Python, modular architectures, sequential processing</p><p>5 0.38299778 <a title="90-lsi-5" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>Author: Stephen R. Piccolo, Lewis J. Frey</p><p>Abstract: Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classiﬁcation analyses in a systematic yet ﬂexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net. Keywords: toolbox, classiﬁcation, parallel, ensemble, reproducible research</p><p>6 0.28488445 <a title="90-lsi-6" href="./jmlr-2012-Hope_and_Fear_for_Discriminative_Training_of_Statistical_Translation_Models.html">49 jmlr-2012-Hope and Fear for Discriminative Training of Statistical Translation Models</a></p>
<p>7 0.27874371 <a title="90-lsi-7" href="./jmlr-2012-Bounding_the_Probability_of_Error_for_High_Precision_Optical_Character_Recognition.html">22 jmlr-2012-Bounding the Probability of Error for High Precision Optical Character Recognition</a></p>
<p>8 0.27789208 <a title="90-lsi-8" href="./jmlr-2012-Confidence-Weighted_Linear_Classification_for_Text_Categorization.html">28 jmlr-2012-Confidence-Weighted Linear Classification for Text Categorization</a></p>
<p>9 0.2370902 <a title="90-lsi-9" href="./jmlr-2012-Eliminating_Spammers_and_Ranking_Annotators_for_Crowdsourced_Labeling_Tasks.html">37 jmlr-2012-Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks</a></p>
<p>10 0.22679386 <a title="90-lsi-10" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>11 0.21017885 <a title="90-lsi-11" href="./jmlr-2012-Sally%3A_A_Tool_for_Embedding_Strings_in_Vector_Spaces.html">102 jmlr-2012-Sally: A Tool for Embedding Strings in Vector Spaces</a></p>
<p>12 0.1786876 <a title="90-lsi-12" href="./jmlr-2012-Finding_Recurrent_Patterns_from_Continuous_Sign_Language_Sentences_for_Automated_Extraction_of_Signs.html">45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</a></p>
<p>13 0.17619944 <a title="90-lsi-13" href="./jmlr-2012-Multi-Assignment_Clustering_for_Boolean_Data.html">70 jmlr-2012-Multi-Assignment Clustering for Boolean Data</a></p>
<p>14 0.16634397 <a title="90-lsi-14" href="./jmlr-2012-MULTIBOOST%3A_A_Multi-purpose_Boosting_Package.html">62 jmlr-2012-MULTIBOOST: A Multi-purpose Boosting Package</a></p>
<p>15 0.1458372 <a title="90-lsi-15" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>16 0.13946249 <a title="90-lsi-16" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>17 0.13747938 <a title="90-lsi-17" href="./jmlr-2012-A_Local_Spectral_Method_for_Graphs%3A_With_Applications_to_Improving_Graph_Partitions_and_Exploring_Data_Graphs_Locally.html">5 jmlr-2012-A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally</a></p>
<p>18 0.12929888 <a title="90-lsi-18" href="./jmlr-2012-A_Case_Study_on_Meta-Generalising%3A_A_Gaussian_Processes_Approach.html">1 jmlr-2012-A Case Study on Meta-Generalising: A Gaussian Processes Approach</a></p>
<p>19 0.1236542 <a title="90-lsi-19" href="./jmlr-2012-The_huge_Package_for_High-dimensional_Undirected_Graph_Estimation_in_R.html">113 jmlr-2012-The huge Package for High-dimensional Undirected Graph Estimation in R</a></p>
<p>20 0.11957286 <a title="90-lsi-20" href="./jmlr-2012-Quantum_Set_Intersection_and_its_Application_to_Associative_Memory.html">93 jmlr-2012-Quantum Set Intersection and its Application to Associative Memory</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(21, 0.012), (26, 0.02), (27, 0.684), (29, 0.019), (35, 0.011), (49, 0.015), (56, 0.041), (57, 0.011), (69, 0.015), (75, 0.012), (77, 0.018), (92, 0.019), (96, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95144826 <a title="90-lda-1" href="./jmlr-2012-Pattern_for_Python.html">90 jmlr-2012-Pattern for Python</a></p>
<p>Author: Tom De Smedt, Walter Daelemans</p><p>Abstract: Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classiﬁers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern. Keywords: Python, data mining, natural language processing, machine learning, graph networks</p><p>2 0.78899145 <a title="90-lda-2" href="./jmlr-2012-A_Geometric_Approach_to_Sample_Compression.html">3 jmlr-2012-A Geometric Approach to Sample Compression</a></p>
<p>Author: Benjamin I.P. Rubinstein, J. Hyam Rubinstein</p><p>Abstract: The Sample Compression Conjecture of Littlestone & Warmuth has remained unsolved for a quarter century. While maximum classes (concept classes meeting Sauer’s Lemma with equality) can be compressed, the compression of general concept classes reduces to compressing maximal classes (classes that cannot be expanded without increasing VC dimension). Two promising ways forward are: embedding maximal classes into maximum classes with at most a polynomial increase to VC dimension, and compression via operating on geometric representations. This paper presents positive results on the latter approach and a ﬁrst negative result on the former, through a systematic investigation of ﬁnite maximum classes. Simple arrangements of hyperplanes in hyperbolic space are shown to represent maximum classes, generalizing the corresponding Euclidean result. We show that sweeping a generic hyperplane across such arrangements forms an unlabeled compression scheme of size VC dimension and corresponds to a special case of peeling the one-inclusion graph, resolving a recent conjecture of Kuzmin & Warmuth. A bijection between ﬁnite maximum classes and certain arrangements of piecewise-linear (PL) hyperplanes in either a ball or Euclidean space is established. Finally we show that d-maximum classes corresponding to PL-hyperplane arrangements in Rd have cubical complexes homeomorphic to a d-ball, or equivalently complexes that are manifolds with boundary. A main result is that PL arrangements can be swept by a moving hyperplane to unlabeled d-compress any ﬁnite maximum class, forming a peeling scheme as conjectured by Kuzmin & Warmuth. A corollary is that some d-maximal classes cannot be embedded into any maximum class of VC-dimension d + k, for any constant k. The construction of the PL sweeping involves Pachner moves on the one-inclusion graph, corresponding to moves of a hyperplane across the intersection of d other hyperplanes. This extends the well known Pachner moves for triangulations to c</p><p>3 0.54833895 <a title="90-lda-3" href="./jmlr-2012-Regularized_Bundle_Methods_for_Convex_and_Non-Convex_Risks.html">98 jmlr-2012-Regularized Bundle Methods for Convex and Non-Convex Risks</a></p>
<p>Author: Trinh Minh Tri Do, Thierry Artières</p><p>Abstract: Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efﬁcient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efﬁcient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random ﬁelds, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy ε with a rate O(1/λε) where λ is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difﬁcult and requires a stronger and more disputable assumption. Yet we provide experimental results on artiﬁcial test problems, and on ﬁve standard and difﬁcult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms. Keywords: optimization, non-convex, non-smooth, cutting plane, bundle method, regularized risk</p><p>4 0.20652016 <a title="90-lda-4" href="./jmlr-2012-DARWIN%3A_A_Framework_for_Machine_Learning_and_Computer_Vision_Research_and_Development.html">30 jmlr-2012-DARWIN: A Framework for Machine Learning and Computer Vision Research and Development</a></p>
<p>Author: Stephen Gould</p><p>Abstract: We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data ﬂows. Keywords: machine learning, graphical models, computer vision, open-source software</p><p>5 0.18807934 <a title="90-lda-5" href="./jmlr-2012-Finding_Recurrent_Patterns_from_Continuous_Sign_Language_Sentences_for_Automated_Extraction_of_Signs.html">45 jmlr-2012-Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs</a></p>
<p>Author: Sunita Nayak, Kester Duncan, Sudeep Sarkar, Barbara Loeding</p><p>Abstract: We present a probabilistic framework to automatically learn models of recurring signs from multiple sign language video sequences containing the vocabulary of interest. We extract the parts of the signs that are present in most occurrences of the sign in context and are robust to the variations produced by adjacent signs. Each sentence video is ﬁrst transformed into a multidimensional time series representation, capturing the motion and shape aspects of the sign. Skin color blobs are extracted from frames of color video sequences, and a probabilistic relational distribution is formed for each frame using the contour and edge pixels from the skin blobs. Each sentence is represented as a trajectory in a low dimensional space called the space of relational distributions. Given these time series trajectories, we extract signemes from multiple sentences concurrently using iterated conditional modes (ICM). We show results by learning single signs from a collection of sentences with one common pervading sign, multiple signs from a collection of sentences with more than one common sign, and single signs from a mixed collection of sentences. The extracted signemes demonstrate that our approach is robust to some extent to the variations produced within a sign due to different contexts. We also show results whereby these learned sign models are used for spotting signs in test sequences. Keywords: pattern extraction, sign language recognition, signeme extraction, sign modeling, iterated conditional modes</p><p>6 0.17933583 <a title="90-lda-6" href="./jmlr-2012-NIMFA_%3A_A_Python_Library_for_Nonnegative_Matrix_Factorization.html">75 jmlr-2012-NIMFA : A Python Library for Nonnegative Matrix Factorization</a></p>
<p>7 0.16337094 <a title="90-lda-7" href="./jmlr-2012-Jstacs%3A_A_Java_Framework_for_Statistical_Analysis_and_Classification_of_Biological_Sequences.html">53 jmlr-2012-Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences</a></p>
<p>8 0.16323012 <a title="90-lda-8" href="./jmlr-2012-Oger%3A_Modular_Learning_Architectures_For_Large-Scale_Sequential_Processing.html">79 jmlr-2012-Oger: Modular Learning Architectures For Large-Scale Sequential Processing</a></p>
<p>9 0.16305985 <a title="90-lda-9" href="./jmlr-2012-Sign_Language_Recognition_using_Sub-Units.html">106 jmlr-2012-Sign Language Recognition using Sub-Units</a></p>
<p>10 0.15412073 <a title="90-lda-10" href="./jmlr-2012-Hope_and_Fear_for_Discriminative_Training_of_Statistical_Translation_Models.html">49 jmlr-2012-Hope and Fear for Discriminative Training of Statistical Translation Models</a></p>
<p>11 0.14425299 <a title="90-lda-11" href="./jmlr-2012-glm-ie%3A_Generalised_Linear_Models_Inference_%26_Estimation_Toolbox.html">119 jmlr-2012-glm-ie: Generalised Linear Models Inference & Estimation Toolbox</a></p>
<p>12 0.14405674 <a title="90-lda-12" href="./jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</a></p>
<p>13 0.14204963 <a title="90-lda-13" href="./jmlr-2012-Mal-ID%3A_Automatic_Malware_Detection_Using_Common_Segment_Analysis_and_Meta-Features.html">63 jmlr-2012-Mal-ID: Automatic Malware Detection Using Common Segment Analysis and Meta-Features</a></p>
<p>14 0.13526312 <a title="90-lda-14" href="./jmlr-2012-Learning_Algorithms_for_the_Classification_Restricted_Boltzmann_Machine.html">55 jmlr-2012-Learning Algorithms for the Classification Restricted Boltzmann Machine</a></p>
<p>15 0.13362026 <a title="90-lda-15" href="./jmlr-2012-MedLDA%3A_Maximum_Margin_Supervised_Topic_Models.html">65 jmlr-2012-MedLDA: Maximum Margin Supervised Topic Models</a></p>
<p>16 0.13356625 <a title="90-lda-16" href="./jmlr-2012-MULTIBOOST%3A_A_Multi-purpose_Boosting_Package.html">62 jmlr-2012-MULTIBOOST: A Multi-purpose Boosting Package</a></p>
<p>17 0.12661016 <a title="90-lda-17" href="./jmlr-2012-ML-Flex%3A_A_Flexible_Toolbox_for_Performing_Classification_Analyses_In_Parallel.html">61 jmlr-2012-ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel</a></p>
<p>18 0.12627102 <a title="90-lda-18" href="./jmlr-2012-Local_and_Global_Scaling_Reduce_Hubs_in_Space.html">60 jmlr-2012-Local and Global Scaling Reduce Hubs in Space</a></p>
<p>19 0.12624663 <a title="90-lda-19" href="./jmlr-2012-Multi-Target_Regression_with_Rule_Ensembles.html">72 jmlr-2012-Multi-Target Regression with Rule Ensembles</a></p>
<p>20 0.12574027 <a title="90-lda-20" href="./jmlr-2012-Query_Strategies_for_Evading_Convex-Inducing_Classifiers.html">94 jmlr-2012-Query Strategies for Evading Convex-Inducing Classifiers</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
