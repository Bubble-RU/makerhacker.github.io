<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>96 jmlr-2006-Worst-Case Analysis of Selective Sampling for Linear Classification</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-96" href="../jmlr2006/jmlr-2006-Worst-Case_Analysis_of_Selective_Sampling_for_Linear_Classification.html">jmlr2006-96</a> <a title="jmlr-2006-96-reference" href="#">jmlr2006-96-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>96 jmlr-2006-Worst-Case Analysis of Selective Sampling for Linear Classification</h1>
<br/><p>Source: <a title="jmlr-2006-96-pdf" href="http://jmlr.org/papers/volume7/cesa-bianchi06b/cesa-bianchi06b.pdf">pdf</a></p><p>Author: Nicolò Cesa-Bianchi, Claudio Gentile, Luca Zaniboni</p><p>Abstract: A selective sampling algorithm is a learning algorithm for classiﬁcation that, based on the past observed data, decides whether to ask the label of each new instance to be classiﬁed. In this paper, we introduce a general technique for turning linear-threshold classiﬁcation algorithms from the general additive family into randomized selective sampling algorithms. For the most popular algorithms in this family we derive mistake bounds that hold for individual sequences of examples. These bounds show that our semi-supervised algorithms can achieve, on average, the same accuracy as that of their fully supervised counterparts, but using fewer labels. Our theoretical results are corroborated by a number of experiments on real-world textual data. The outcome of these experiments is essentially predicted by our theoretical results: Our selective sampling algorithms tend to perform as well as the algorithms receiving the true label after each classiﬁcation, while observing in practice substantially fewer labels. Keywords: selective sampling, semi-supervised learning, on-line learning, kernel algorithms, linear-threshold classiﬁers</p><br/>
<h2>reference text</h2><p>D. Angluin. Queries and concept learning. Machine Learning, 2(4):319–342, 1988. P. Auer, N. Cesa-Bianchi, and C. Gentile. Adaptive and self-conﬁdent on-line learning algorithms. Journal of Computer and System Sciences, 64:48–75, 2002. K. S. Azoury and M. K. Warmuth. Relative loss bounds for on-line density estimation with the exponential familiy of distributions. Machine Learning, 43(3):211–246, 2001. H. D. Block. The Perceptron: A model for brain functioning. Reviews of Modern Physics, 34: 123–135, 1962. A. Bordes, S. Ertekin, J. Weston, and L. Bottou. Fast kernel classiﬁers with online and active learning. Journal of Machine Learning reserarch, 6:1579–1619, 2005. C. Campbell, N. Cristianini, and A. Smola. Query learning with large margin classiﬁers. In Proceedings of the 17th International Conference on Machine Learning, pages 111–11. Morgan Kaufman, 2000. N. Cesa-Bianchi, A. Conconi, and C. Gentile. Learning probabilistic linear-threshold classiﬁers via selective sampling. In Proceedings of the 16th Annual Conference on Learning Theory, LNAI 2777, pages 373–386. Springer, 2003. N. Cesa-Bianchi, A. Conconi, and C. Gentile. A second-order Perceptron algorithm. SIAM Journal on Computing, 43(3):640–668, 2005. N. Cesa-Bianchi and G. Lugosi. Potential-based algorithms in on-line prediction and game theory. Machine Learning, 51(3):239–261, 2003. N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006. R. Cohn, L. Atlas, and R. Ladner. Training connectionist networks with queries and selective sampling. In Advances in Neural Information Processing Systems 2. MIT Press, 1990. N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge University Press, 2001. S. Dasgupta, A. T. Kalai, and C. Monteleoni. Analysis of Perceptron-based active learning. In Proceedings of the 18th Annual Conference on Learning Theory, LNAI 2777, pages 249–263. Springer, 2005. O. Dekel, S. Shalev-Shwartz, and Y. Singer. The Forgetron: a kernel-based Perceptron on a ﬁxed budget. In Advances in Neural Information Processing Systems 18, pages 259–266. MIT Press, 2006. 1228  W ORST-C ASE S ELECTIVE S AMPLING  R. Duda and P. Hart, and D. Stork. Pattern classiﬁcation, second edition. Wiley Interscience, 2000. J. Forster. On relative loss bounds in generalized linear regression. In Proceedings of the 12th International Symposium on Fundamentals of Computation Theory, LNCS 1684, pages 269–280. Springer, 1999. Y. Freund and R. Schapire. Large margin classiﬁcation using the Perceptron algorithm. Machine Learning, 37(3):277–296, 1999. Y. Freund, S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by committee algorithm. Machine Learning, 28(2/3):133–168, 1997. C. Gentile. A new approximate maximal margin classiﬁcation algorithm. Journal of Machine Learning Research, 2:213–242, 2001. C. Gentile. The robustness of the p-norm algorithms. Machine Learning, 53(3):265–299, 2003. C. Gentile and M. Warmuth. Linear hinge loss and average margin. In Advances in Neural Information Processing Systems 10, pages 225–231. MIT Press, 1999. A. J. Grove, N. Littlestone, and D. Schuurmans. General convergence results for linear discriminant updates. Machine Learning, 43(3):173–210, 2001. D. P. Helmbold, N. Littlestone, and P. M. Long. Apple tasting. Information and Computation, 161 (2):85–139, 2000. D. P. Helmbold and S. Panizza. Some label efﬁcient learning results. In Proceedings of the 10th Annual Conference on Computational Learning Theory, pages 218–230. ACM Press, 1997. M. Herbster and M. K. Warmuth. Tracking the Best Linear Predictor. Journal of Machine Learning Research, 1: 281–309, 2001. W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:13–30, 1963. J. Kivinen and M. K. Warmuth. Relative loss bounds for multidimensional regression problems. Machine Learning, 45(3):301–329, 2001. T. L. Lai and C. Z. Wei. Least squares estimates in stochastic regression models with applications to identiﬁcation and control of dynamic systems. The Annals of Statistics, 10(1):154–166, 1982. Y. Li and P. Long. The relaxed online maximum margin algorithm. Machine Learning, 46:361–387, 2002. N. Littlestone. Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm. Machine Learning, 2(4):285–318, 1988. N. Littlestone. Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. PhD thesis, University of California Santa Cruz, 1989. A. B. J. Novikov. On convergence proofs on Perceptrons. In Proceedings of the Symposium on the Mathematical Theory of Automata, vol. XII, pages 615–622, 1962. 1229  C ESA -B IANCHI , G ENTILE AND Z ANIBONI  Reuters. Reuters corpus vol. 1, 2000. URL about.reuters.com/researchandstandards/corpus/. F. Rosenblatt. The Perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65:386–408, 1958. B. Sch¨ lkopf and A. Smola. Learning with Kernels. MIT Press, 2002. o S. Tong and D. Koller. Support vector machine active learning with applications to text classiﬁcation. In Proceedings of the 17th International Conference on Machine Learning, pages 999–1006. Morgan Kaufmann, 2000. V. N. Vapnik. Statistical Learning Theory. Wiley, 1998. M. K. Warmuth and A. K. Jagota. Continuous and discrete-time nonlinear gradient descent: Relative loss bounds and convergence. In Electronic proceedings of the 5th International Symposium on Artiﬁcial Intelligence and Mathematics, 1997.  1230</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
