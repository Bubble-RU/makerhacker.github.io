<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-17" href="../jmlr2006/jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">jmlr2006-17</a> <a title="jmlr-2006-17-reference" href="#">jmlr2006-17-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</h1>
<br/><p>Source: <a title="jmlr-2006-17-pdf" href="http://jmlr.org/papers/volume7/ekdahl06a/ekdahl06a.pdf">pdf</a></p><p>Author: Magnus Ekdahl, Timo Koski</p><p>Abstract: In many pattern recognition/classiﬁcation problem the true class conditional model and class probabilities are approximated for reasons of reducing complexity and/or of statistical estimation. The approximated classiﬁer is expected to have worse performance, here measured by the probability of correct classiﬁcation. We present an analysis valid in general, and easily computable formulas for estimating the degradation in probability of correct classiﬁcation when compared to the optimal classiﬁer. An example of an approximation is the Na¨ve Bayes classiﬁer. We show that the perforı mance of the Na¨ve Bayes depends on the degree of functional dependence between the features ı and labels. We provide a sufﬁcient condition for zero loss of performance, too. Keywords: Bayesian networks, na¨ve Bayes, plug-in classiﬁer, Kolmogorov distance of variation, ı variational learning</p><br/>
<h2>reference text</h2><p>S.M. Ali and S.D. Silvey. A general class of coefﬁcients of divergence of one distribution from another. Journal of the Royal Statistical Society. Series B, 28(1):131–142, 1966. S. Anoulova, P. Fischer, S. Polt, and H.U. Simon. Probably almost Bayes decisions. Information and Computation, 129(1):63–71, 1996. 2476  B OUNDS IN M ODEL BASED C LASSIFICATION  S. Arimoto. Information-theoretical considerations on estimation problems. Information and Control, 19:181–194, 1971. R.R. Bahadur. On classiﬁcation based on responses to n dichotomous items. In H. Solomon, editor, Studies in Item Analysis and Prediction, pages 169–177, Stanford, California, 1961a. Stanford University Press. R.R. Bahadur. A representation of the joint distribution of responses to n dichotomous items. In H. Solomon, editor, Studies in Item Analysis and Prediction, pages 158–168, Stanford, California, 1961b. Stanford University Press. L.C. Barbosa. Maximum likelihood sequence estimators: A geometric view. IEEE Transactions on Information Theory, 35(2), 1989. B.K. Bhattacharyya and G.T. Toussaint. An upper bound on the probability of misclassiﬁcation in terms of Matusita’s measure of afﬁnity. Annals of the Institute of Statistical Mathematics, 34: 161–165, 1982. C. Boutilier, N. Friedman, M. Goldszmidt, and D. Koller. Context-speciﬁc independence in Bayesian networks. In Uncertainty in Artiﬁcial Intelligence (UAI-96), pages 115–123, 1996. D.T. Brown. A note on approximations to discrete probability distributions. Information and Control, 2:386–392, 1959. H.D. Brunk and D.A. Pierce. Estimation of discrete multivariate densities for computer-aided differential diagnosis. Biometrika, 61(3):493–499, 1974. D.M. Chickering. Optimal structure identiﬁcation with greedy search. Journal of Machine Learning Research, 3(Nov):507–554, 2002. C.K. Chow and C.N. Liu. Approximating discrete probability distributions with dependency trees. IEEE Transactions on Information Theory, 14(3):462–467, 1968. G. Cooper. The computational complexity of probabilistic inference using bayesian Belief networks. Artiﬁcial Intelligence, 42(2-3):393–405, 1990. T.M. Cover and J.A Thomas. Elements of Information Theory, chapter 2.2, page 169. Wiley, 1991. R.G. Cowell, A.P. Dawid, S.L. Lauritzen, and D.J. Spiegelhalter. Probabilistic Networks and Expert Systems. Springer, 1999. H. Cram´ r. Mathematical Methods of Statistics, 11 th printing. Princeton University Press, 1966. e L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, o 1996. P. Domingos and M. Pazzani. On the optimality of the simple bayesian classiﬁer under zero-one loss. Machine Learning, 29(2):103–130, 1997. M. Ekdahl. Approximations of Bayes Classiﬁers for Statistical Learning of Clusters. Licentiate thesis, Link¨ pings Universitet, 2006. o 2477  E KDAHL AND KOSKI  M. Ekdahl and T. Koski. On the performance of model based approximations in classiﬁcation. In Proceedings of The 23rd Annual Workshop of the Swedish Artiﬁcial Intelligence Society, pages 73–82. http://sais2006.cs.umu.se/, 2006. J.H. Friedman. On bias, variance, 0/1-loss, and the curse-of-dimensionality. Data Mining and Knowledge Discovery, 1:55–77, 1997. N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian network classiﬁers. Machine Learning, 29 (2):1–36, 1997. N. Glick. Sample-based classiﬁcation procedures derived from density estimators. Journal of the American Statistical Association, 67(337):116–122, 1972. N. Glick. Sample based multinomial classiﬁcation. Biometrics, 29(2):241–256, 1973. L.A. Goodman and W.H. Kruskal. Measures of association for cross classiﬁcations. Journal of the American Statistical Association, 49(268):732–763, 1954. M. Gyllenberg and T. Koski. Probabilistic models for bacterial taxonomy. International Statistical Review, 69:249–276, 2001. D. Hand, H. Mannila, and P. Smyth. Principles of Data Mining. The MIT Press, 2001. D.J. Hand and K. Yu. Idiot’s Bayes–not so stupid after all? International Statistical Review, 69(3): 385–398, 2001. W. Hoeffding. Stochastische unabh¨ ngigkeit und funktionaler zusammenhang. Skandinavisk Aktua arietidskrift, 25:200–227, 1942. W. Hoeffding and J. Wolfowitz. Distinguishability of sets of distributions. The Annals of Mathematical Statistics, 29(3):700–718, 1958. K. Huang, I. King, and M.R. Lyu. Finite mixture model of bounded semi-naive Bayesian network classiﬁer. In Proceedings of the International Conference on Artiﬁcial Neural Networks (ICANN2003), volume 2714 of Lecture Notes in Computer Science. Springer, 2003. M.I. Jordan, Z. Ghahramani, T. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. Machine Learning, 37(2):183–233, 1999. T. Kailath. The divergence and Bhattacharyya distance measures in signal selection. IEEE Transactions on Communications Technology, 15(1):52–60, 1967. G.K. Kaleh. Channel equalization for block transmission systems. IEEE Journal on Selected Areas in Communications, 13(1):150–121, 1995. K.B. Korb and A.E. Nicholson. Bayesian Artiﬁcial Intelligence. Chapman and Hall, 2004. H.H. Ku and S. Kullback. Approximating discrete probability distributions. IEEE Transactions on Information Theory, 15:444–447, 1969. S. Kullback. Information Theory and Statistics, chapter 9, page 190. Dover Publications Inc., 1997. 2478  B OUNDS IN M ODEL BASED C LASSIFICATION  S.L. Lauritzen. Propagation of probabilities, means, and variances in mixed graphical association models. Journal of the American Statistical Association, 87:1098–1108, 1990. P.M. Lewis. Approximating probability distributions to reduce storage requirements. Information and Control, 2:214–225, 1959. D.K. McGraw and J.F. Wagner. Elliptically symmetric distributions. IEEE Transactions on Information Theory, 14(1):110 – 120, 1968. D.H.II. Moore. Evaluation of ﬁve discrimination procedures for binary variables. Journal of the American Statistical Association, 68(342):399–404, 1973. X. Nguyen, M.J. Wainwright, and M.I. Jordan. On divergences, surrogate loss functions, and decentralized detection. Technical Report 695, University of California, Berkeley, 2005. J. Ott and R.A. Kronmal. Some classiﬁcation procedures for multivariate binary data using orthogonal functions. Journal of the American Statistical Association, 71(354):391–399, 1976. ¨ G.A. Pistone, E.A. Riccomagno, and H.P.A. Wynn. Gr obner bases and factorisation in discrete probability and Bayes. Statistics and Computing, 11(1):37–46, 2001. E.J.G. Pitman. Some Basic Theory for Statistical Inference, chapter 2. Chapman and Hall, 1979. B.D. Ripley. Pattern Recognition and Neural Networks. Cambridge University Press, 1996. I. Rish, J. Hellerstein, and J. Thathachar. An analysis of data characteristics that affect Naive Bayes performance. Technical Report RC21993, IBM, 2001. J.Van Ryzin. Bayes risk consistency of classiﬁcation procedures using density estimation. Sankhya Series A, 28:261–270, 1966. P. Scheinck. Symptom diagnosis Bayes’s and Bahadur’s distribution. International Journal of Biomedical Computing, 3:17–28, 1972. M.J. Schervish. Theory of Statistics. Springer, second edition, 1995. D. Slepian. On the symmetrized Kronecker power of a matrix and extensions of Mehler’s formula for Hermite polynomials. SIAM Journal on Mathematical Analysis, 3:606–616, 1972. H. Strasser. Mathematical Theory of Statistics, chapter 2.2. Walter de Gruyter, 1985. J.L. Teugels. Some representations of the multivariate Bernoulli and binomial distributions. Journal of Multivariate Analysis, 33(1):256–268, 1990. D.M. Titterington, G.D. Murray, L.S. Murray, D.J. Spiegelhalter, A.M. Skene, J.D.F. Habbema, and G.J. Gelpke. Comparison of discrimination techniques applied to a complex data set of head injured patients. Journal of the Royal Statistical Society., 144(2):145–175, 1981. F. Topsoe. Some inequalities for information divergence and related measures of discrimination. IEEE Transactions on Information Theory, 46(4):1602–1609, 2000. 2479  E KDAHL AND KOSKI  G.T. Toussaint. Polynomial representation of classiﬁers with independent discrete-valued features. IEEE Transactions on Computers, 21:205–208, 1972. R. van Engelen. Approximating bayesian Belief networks by arc removal. IEEE Transactions on Pattern Analysis and Machine Intellignce, 19(8):916–920, 1997. R. Vilata and I. Rish. A decomposition of classes via clustering to explain and improve Naive Bayes. In Machine Learning: ECML 2003: 14th European Conference on Machine Learning, pages 444 – 455, 2003. T.R. Vilmansen. Feature evaluation with measures of probabilistic dependence. IEEE Transactions on Computers, 22(4):381–387, 1973. T.R. Vilmansen. On dependence and discrimination in pattern recognition. IEEE Transactions on Computers, 21:1029–1031, 1971. M.J Wainwright and M.I. Jordan. Graphical models, exponential families, and variational inference. Technical Report 649, UC Berkeley, Dept. of Statistics, 2003. J. Williamson. Bayesian Nets and Causality: Philosophical and Computational Foundations. Oxford University Press, 2005.  2480</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
