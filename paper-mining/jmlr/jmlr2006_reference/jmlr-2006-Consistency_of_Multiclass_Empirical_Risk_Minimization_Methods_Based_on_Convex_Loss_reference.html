<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>24 jmlr-2006-Consistency of Multiclass Empirical Risk Minimization Methods Based on Convex Loss</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-24" href="../jmlr2006/jmlr-2006-Consistency_of_Multiclass_Empirical_Risk_Minimization_Methods_Based_on_Convex_Loss.html">jmlr2006-24</a> <a title="jmlr-2006-24-reference" href="#">jmlr2006-24-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>24 jmlr-2006-Consistency of Multiclass Empirical Risk Minimization Methods Based on Convex Loss</h1>
<br/><p>Source: <a title="jmlr-2006-24-pdf" href="http://jmlr.org/papers/volume7/chen06a/chen06a.pdf">pdf</a></p><p>Author: Di-Rong Chen, Tao Sun</p><p>Abstract: The consistency of classiﬁcation algorithm plays a central role in statistical learning theory. A consistent algorithm guarantees us that taking more samples essentially sufﬁces to roughly reconstruct the unknown distribution. We consider the consistency of ERM scheme over classes of combinations of very simple rules (base classiﬁers) in multiclass classiﬁcation. Our approach is, under some mild conditions, to establish a quantitative relationship between classiﬁcation errors and convex risks. In comparison with the related previous work, the feature of our result is that the conditions are mainly expressed in terms of the differences between some values of the convex function. Keywords: multiclass classiﬁcation, classiﬁer, consistency, empirical risk minimization, constrained comparison method, Tsybakov noise condition</p><br/>
<h2>reference text</h2><p>P. L. Bartlett, M. I. Jordan and J. D. Mcauliffe. Convexity, classiﬁcation, and risk bounds. Journal of the American Statistical Association, 101(473):138–156, 2006. P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: risk bounds and structural results. Journal of Machine Learning Research, 3:463–482, 2002. O. Bousquet, S. Boucheron and G. Lugosi. Theory of classiﬁcation: a survey of recent advances. ESAIM: Probability and Statistics, 9:323–375. 2005. D. R. Chen, Q. Wu, Y. Ying and D. X. Zhou. Support vector machine soft margin classiﬁer: error analysis. J. Machine Learning Reseach, 5: 1143–1175, 2004. D. R. Chen and D. H. Xiang. The consistency of multicategory support vector machines. Adv in Comput. Math., 24: 155–169, 2006. L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer– o Verlag, New York, 1996. G. Lugosi. Pattern classiﬁcation and learning theory, Principles of Nonparametric Learning. Springer, Wien, New York, pp 1–56, 2002. G. Lugosi and N. Vayatis. On the Bayes-risk consistency of regularized boosting metheds. Ann. Statist.,32: 30–55, 2004. C. McDiarmid. On the method of bounded differences. In Surveys on Combinatorics, 148–188, Combridge University Press, 1989. A. Tewari and P. L. Bartlett: On the consistency of multiclass classiﬁcation methods, In Proc. 18th International Conference on Computational Learning Theory, pages 143–157, 2005. V. N. Vapnik. Statistical Learning Theory. John Wiley and sons, New York, 1998. 2446  M ULTICLASS E MPIRICAL R ISK M INIMIZATION M ETHODS  T. Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. Annals of Statistics, 32: 56–134, 2004a. T. Zhang. Statistical analysis of some multiclass large margin classiﬁcation method. Journal of Machine Learning Research, 5: 1225–1251, 2004b.  2447</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
