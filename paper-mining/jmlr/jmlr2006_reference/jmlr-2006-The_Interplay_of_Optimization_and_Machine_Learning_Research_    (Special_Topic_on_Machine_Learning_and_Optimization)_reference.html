<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-91" href="../jmlr2006/jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">jmlr2006-91</a> <a title="jmlr-2006-91-reference" href="#">jmlr2006-91-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-91-pdf" href="http://jmlr.org/papers/volume7/MLOPT-intro06a/MLOPT-intro06a.pdf">pdf</a></p><p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><br/>
<h2>reference text</h2><p>F. R. Bach, G. R. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In Proceedings of the Twenty-ﬁrst International Conference on Machine Learning, 2004. M. Bazaraa, H. Sherali, and C. Shetty. Nonlinear Programming Theory and Algorithms. Wiley, 2006. K. P. Bennett and O. L. Mangasarian. Bilinear separation of two sets in n-space. Computational Optimization & Applications, 2:207–227, 1993. A. Bergkvist, P. Damaschke, and M. L¨ thi. Linear programs for hypotheses selection in probabilistic u inference models. Journal of Machine Learning Research, 7:1339–1355, 2006. D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, Cambridge, 2004. C. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, Oxford, 1996. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, 2004. P. S. Bradley and O. L. Mangasarian. Feature selection via concave minimization and support vector machines. In J. Shavlik, editor, Machine Learning Proceedings of the Fifteenth International Conference(ICML ’98), pages 82–90, San Francisco, California, 1998. Morgan Kaufmann. 1278  C ONSTRAINED O PTIMIZATION IN M ACHINE L EARNING  P. S. Bradley, O. L. Mangasarian, and W. N. Street. Clustering via concave minimization. In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems -9-, pages 368–374, Cambridge, MA, 1997. MIT Press. C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995. T. De Bie and N. Cristianini. Fast SDP relaxations approaches of graph cut clustering, transduction and other combinatorial problems. Journal of Machine Learning Research, 7:1409–1436, 2006. A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38, 1977. E. Dolan and J. Mor´ . Benchmarking optimization software with performance proﬁles. Mathemate ical Programming, 91(2):201–213, 2002. R.-E. Fan, P.-H. Chen, and C.-J. Lin. Working set selection using second order information for training support vector machines. Journal of Machine Learning Research, 5(Dec):1889–1918, 2005. G. Fung and O. L. Mangasarian. A feature selection newton method for support vector machine classiﬁcation. Computational Optimization and Applications, 28(2):185–202, 2004. T. Glasmachers and C. Igel. Maximum-gain working set selection for SVMs. Journal of Machine Learning Research, 7:1437–1466, 2006. M. A. Goberna and M. A. L´ pez. Linear Semi-Inﬁnite Optimization. John Wiley, New York, 1998. o G. H. Golub and U. von Matt. Generalized cross-validation for large scale problems. Journal of Computational and Graphical Statistics, 6(1):1–34, 1997. M. Heiler and C. Schn¨ rr. Learning sparse representations by non-negative matrix factorization and o sequential cone programming. Journal of Machine Learning Research, 7:1385–1407, 2006. R. Hettich and K. O. Kortanek. Semi-inﬁnite programming: theory, methods and application. SIAM Review, 3:380–429, 1993. T. Joachims. Making large-scale SVM learning practical. In B. Sch¨ lkopf, C. Burges, and A. Smola, o editors, Advances in Kernel Methods – Support Vector Learning, pages 169 –184. MIT Press, Cambridge, MA, 1999. S. S. Keerthi, O. Chapelle, and D. DeCoste. Building support vector machines with reduced classiﬁer complexity. Journal of Machine Learning Research, 7:1493–1515, 2006. P. Laskov, C. Gehl, S. Kr¨ ger, and K.-R. M¨ ller. Incremental support vector learning: Analysis, u u implementation and applications. Journal of Machine Learning Research, 7:1909–1936, 2006. O. L. Mangasarian. Exact 1-norm support vector machines via unconstrained convex differentiable minimization. Journal of Machine Learning Research, 7:1517–1530, 2006. O. L. Mangasarian and D. Musicant. Success overrelaxation for support vector machines. IEEE Transaction on Neural Networks, 10(5):1032–1037, 1999. 1279  ´ B ENNETT AND PARRADO -H ERN ANDEZ  O. L. Mangasarian and M. V. Solodov. Serial and parallel backpropagation convergence via nonmonotone perturbed minimization. Optimization Methods and Software, 4(2):103–116, 1994. H.D. Mittelmann. An independent benchmarking of SDP and SOCP solvers. Mathematical Programming, 95(2):407–430, 2003. G. Nemhauser and L. Wolsey. Integer and Combinatorial Optimization. Wiley, 1999. Y. Nesterov. Dual extrapolation and its application to solving variational inequalities and related problems. Core, Catholic University of Louven, 2003. R. S. Niculescu, T. M. Mitchell, and R. B. Rao. Bayesian network learning with parameter constraints. Journal of Machine Learning Research, 7:1357–1383, 2006. J. Nocedal and S. J. Wright. Numerical Optimization. Springer, New York, 1999. J. C. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Schlkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 185–208. MIT Press, Cambridge, MA, USA, 1999. R. L. Radin. Optimization in Operations Research. Prentice-Hall, New Jersey, 1998. R. Reemtsen and J. J. Ruckmann. Semi-inﬁnite programming. Kluwer Academic, 1998. R. Rifkin and A. Klautau. In defense of one-vs-all classiﬁcation. Journal of Machine Learning Research, 5(Jan):101–141, 2004. J. Rousu, C. Saunders, S. Szedmak, and J. Shawe-Taylor. Kernel-based learning of hierarchical multilabel classiﬁcation models. Journal of Machine Learning Research, 7:1601–1626, 2006. D. Rummelhart, G. Hinton, and R. Williams. Learning internal representations by error propagation. In D. Rummelhart and J. McClelland, editors, Parallel Distributed Processing, pages 318–362, Cambridge, 1986. MIT Press. K. Scheinberg. An efﬁcient implementation of an active set method for SVMs. Journal of Machine Learning Research, 7:2237–2257, 2006. S. Shalev-Shwartz and Y. Singer. Efﬁcient learning of label ranking by soft projections onto polyhedra. Journal of Machine Learning Research, 7:1567–1599, 2006. P. K. Shivaswamy, C. Bhattacharyya, and A. J. Smola. Second order cone programming approaches for handling missing and uncertain data. Journal of Machine Learning Research, 7:1283–1314, 2006. S. Sonnenburg, G. R¨ tsch, C. Sch¨ fer, and B. Sch¨ lkopf. Large scale multiple kernel learning. a a o Journal of Machine Learning Research, 7:1531–1565, 2006. B. Taskar, V. Chatalbashev, D. Koller, and C. Guestrin. Learning structured prediction models: a large margin approach. In International Conference on Machine Learning, 2005. B. Taskar, C. Guestrin, V. Chatalbashev, and D. Koller. Max-margin markov networks. Journal of Machine Learning Research, pages 1627–1653, 2006a. 1280  C ONSTRAINED O PTIMIZATION IN M ACHINE L EARNING  B. Taskar, S. Lacoste-Julien, and M. Jordan. Structured prediction, dual extragradient and Bregman projections. Journal of Machine Learning Research, 7:1627–1653, 2006b. C. Yanover, T. Meltzer, and Y. Weiss. Linear programming relaxations and belief propagation- an empirical study. Journal of Machine Learning Research, 7:1887–1907, 2006. L. Zanni, T. Seraﬁni, and G. Zanghirati. Parallel software for training large scale support vector machines on multiprocessor systems. Journal of Machine Learning Research, 7:1467–1492, 2006. Y. Zhang, S. Burer, and W. N. Street. Ensemble pruning via semi-deﬁnite programming. Journal of Machine Learning Research, 7:1315–1338, 2006.  1281</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
