<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-9" href="../jmlr2006/jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">jmlr2006-9</a> <a title="jmlr-2006-9-reference" href="#">jmlr2006-9-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</h1>
<br/><p>Source: <a title="jmlr-2006-9-pdf" href="http://jmlr.org/papers/volume7/braun06a/braun06a.pdf">pdf</a></p><p>Author: Mikio L. Braun</p><p>Abstract: The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to inﬁnity. We derive probabilistic ﬁnite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reﬂecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a signiﬁcant improvement over existing non-scaling bounds. Keywords: kernel matrix, eigenvalues, relative perturbation bounds</p><br/>
<h2>reference text</h2><p>Milton Abramowitz and Irene A. Stegun, editors. Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing, chapter 22, ”Legendre Functions”, and chapter 8, ”Orthogonal Polynomials”, pages 331–339, 771–802. Dover, New York, 1972. Gilles Blanchard, Olivier Bousquet, and Laurent Zwald. Statistical properties of kernel principal component analysis. Machine Learning, 2006. (to appear, published online March 30, 2006). Mikio L. Braun. Spectral Properties of the Kernel Matrix and their Relation to Kernel Methods in Machine Learning. PhD thesis, University of Bonn, Germany, 2005. Available electronically at http://hss.ulb.uni-bonn.de/diss online/math nat fak/2005/braun mikio. J. Dauxois, A. Pousse, and Y. Romain. Asymptotic theory for the principal component analysis of a vector random function: Some applications to statistical inference. Journal of Multivariate Analysis, 12:136–154, 1982. Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 1985. Vladimir Koltchinskii and Evarist Gin´ . Random matrix approximation of spectra of integral opere ators. Bernoulli, 6(1):113–167, 2000. Sebastian Mika. Kernel Fisher Discriminants. PhD thesis, Technische Universit¨ t Berlin, December a 2002. ¨ Bernhard Sch¨ lkopf, Alexander Smola, and Klaus-Robert Muller. Nonlinear compoment analysis o as a kernel eigenvalue problem. Neural Computation, 10:1299–1319, 1998. John Shawe-Taylor, Christopher K. I. Williams, Nello Christianini, and Jaz Kandola. On the eigenspectrum of the gram matrix and the generalization error of kernel-PCA. IEEE Transactions on Information Theory, 51(7):2510–2522, July 2005. Aad van der Vaart and Jon A. Wellner. Weak Convergence and Empirical Processes. SpringerVerlag, 1996. Ulrike von Luxburg. Statistical Learning with Similarity and Dissimilarity Functions. PhD thesis, Technische Universit¨ t Berlin, November 2004. a  2328</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
