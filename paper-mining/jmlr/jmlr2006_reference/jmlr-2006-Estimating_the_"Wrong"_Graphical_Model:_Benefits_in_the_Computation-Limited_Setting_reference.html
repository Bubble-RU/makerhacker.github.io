<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-28" href="../jmlr2006/jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">jmlr2006-28</a> <a title="jmlr-2006-28-reference" href="#">jmlr2006-28-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</h1>
<br/><p>Source: <a title="jmlr-2006-28-pdf" href="http://jmlr.org/papers/volume7/wainwright06a/wainwright06a.pdf">pdf</a></p><p>Author: Martin J. Wainwright</p><p>Abstract: Consider the problem of joint parameter estimation and prediction in a Markov random ﬁeld: that is, the model parameters are estimated on the basis of an initial set of data, and then the ﬁtted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working under the restriction of limited computation, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for ﬁtting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the “wrong” model even in the inﬁnite data limit) is provably beneﬁcial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of convex variational methods. This stability result provides additional incentive, apart from the obvious beneﬁt of unique global optima, for using message-passing methods based on convex variational relaxations. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. Keywords: graphical model, Markov random ﬁeld, belief propagation, variational method, parameter estimation, prediction error, algorithmic stability</p><br/>
<h2>reference text</h2><p>A. Benveniste, M. Metivier, and P. Priouret. Adaptive Algorithms and Stochastic Approximations. Springer-Verlag, New York, NY, 1990. D.P. Bertsekas. Nonlinear programming. Athena Scientiﬁc, Belmont, MA, 1995. J. Besag. Statistical analysis of non-lattice data. The Statistician, 24(3):179–195, 1975. 1856  E STIMATING THE “W RONG ” G RAPHICAL M ODEL  J. Besag. Efﬁciency of pseudolikelihood estimation for simple Gaussian ﬁelds. Biometrika, 64(3): 616–618, 1977. L.D. Brown. Fundamentals of statistical exponential families. Institute of Mathematical Statistics, Hayward, CA, 1986. M.S. Crouse, R.D. Nowak, and R.G. Baraniuk. Wavelet-based statistical signal processing using hidden Markov models. IEEE Trans. Signal Processing, 46:886–902, April 1998. M. Deza and M. Laurent. Geometry of Cuts and Metric Embeddings. Springer-Verlag, New York, 1997. W. T. Freeman, E. C. Pasztor, and O. T. Carmichael. Learning low-level vision. Intl. J. Computer Vision, 40(1):25–47, 2000. W. T. Freeman and Y. Weiss. On the optimality of solutions of the max-product belief propagation algorithm in arbitrary graphs. IEEE Trans. Info. Theory, 47:736–744, 2001. T. Heskes, K. Albers, and B. Kappen. Approximate inference and constrained optimization. In Uncertainty in Artiﬁcial Intelligence, volume 13, pages 313–320, July 2003. A. Ihler, J. Fisher, and A. S. Willsky. Loopy belief propagation: Convergence and effects of message errors. Journal of Machine Learning Research, 6:905–936, May 2005. S. L. Lauritzen. Graphical Models. Oxford University Press, Oxford, 1996. M. A. R. Leisink and H. J. Kappen. Learning in higher order Boltzmann machines using linear response. Neural Networks, 13:329–335, 2000. J. S. Liu. Monte Carlo strategies in Scientiﬁc Computing. Springer-Verlag, New York, NY, 2001. T. P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, MIT, January 2001. J. M. Mooij and H. J. Kappen. On the properties of the Bethe approximation and loopy belief propagation on binary networks. Journal of Statistical Mechanics: Theory and Experiment, P11012: 407–432, 2005a. J. M. Mooij and H. J. Kappen. Sufﬁcient conditions for convergence of loopy belief propagation. Technical Report arxiv:cs.IT:0504030, University of Nijmegen, April 2005b. Submitted to IEEE Trans. Info. Theory. S. Della Pietra, V. Della Pietra, and J. Lafferty. Inducing features of random ﬁelds. IEEE Trans. on Pattern Analysis and Machine Intelligence, 19(4):380–393, 1997. T. Richardson and R. Urbanke. The capacity of low-density parity check codes under messagepassing decoding. IEEE Trans. Info. Theory, 47:599–618, February 2001. B. D. Ripley. Spatial statistics. Wiley, New York, 1981. C. P. Robert and G. Casella. Monte Carlo statistical methods. Springer-Verlag, New York, NY, 1999. 1857  WAINWRIGHT  G. Rockafellar. Convex Analysis. Princeton University Press, Princeton, 1970. M. Ross and L. P. Kaebling. Learning static object segmentation from motion segmentation. In 20th National Conference on Artiﬁcial Intelligence, 2005. P. Rusmevichientong and B. Van Roy. An analysis of turbo decoding with Gaussian densities. In NIPS 12, pages 575–581. MIT Press, 2000. R. J. Serﬂing. Approximation Theorems of Mathematical Statistics. Wiley Series in Probability and Statistics. Wiley, 1980. C. Sutton and A. McCallum. Piecewise training of undirected models. In Uncertainty in Artiﬁcial Intelligence, July 2005. S. Tatikonda. Convergence of the sum-product algorithm. In Information Theory Workshop, April 2003. S. Tatikonda and M. I. Jordan. Loopy belief propagation and Gibbs measures. In Proc. Uncertainty in Artiﬁcial Intelligence, volume 18, pages 493–500, August 2002. Y. W. Teh and M. Welling. On improving the efﬁciency of the iterative proportional ﬁtting procedure. In Workshop on Artiﬁcial Intelligence and Statistics, 2003. D. M. Titterington, A.F.M. Smith, and U.E. Makov. Statistical analysis of ﬁnite mixture distributions. Wiley, New York, 1986. M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. Tree-based reparameterization framework for analysis of sum-product and related algorithms. IEEE Trans. Info. Theory, 49(5):1120–1146, May 2003a. M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. Tree-reweighted belief propagation algorithms and approximate ML estimation by pseudomoment matching. In Workshop on Artiﬁcial Intelligence and Statistics, January 2003b. M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. A new class of upper bounds on the log partition function. IEEE Trans. Info. Theory, 51(7):2313–2335, July 2005. M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Technical report, UC Berkeley, Department of Statistics, No. 649, September 2003. M. J. Wainwright and M. I. Jordan. A variational principle for graphical models. In New Directions in Statistical Signal Processing. MIT Press, Cambridge, MA, 2005. M. J. Wainwright and M. I. Jordan. Log-determinant relaxation for approximate inference in discrete Markov random ﬁelds. IEEE Trans. Signal Processing, 54(6):2099–2109, June 2006. Y. Weiss. Correctness of local probability propagation in graphical models with loops. Neural Computation, 12:1–41, 2000. W. Wiegerinck. Approximations with reweighted generalized belief propagation. In Workshop on Artiﬁcial Intelligence and Statistics, January 2005. 1858  E STIMATING THE “W RONG ” G RAPHICAL M ODEL  J. Yedidia. An idiosyncratic journey beyond mean ﬁeld theory. In M. Opper and D. Saad, editors, Advanced mean ﬁeld methods: Theory and practice, pages 21–36. MIT Press, 2001. J.S. Yedidia, W. T. Freeman, and Y. Weiss. Constructing free energy approximations and generalized belief propagation algorithms. IEEE Trans. Info. Theory, 51(7):2282–2312, July 2005. L. Younes. Estimation and annealing for Gibbsian ﬁelds. Ann. Inst. Henri Poincare, 24(2):269–294, 1988.  1859</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
