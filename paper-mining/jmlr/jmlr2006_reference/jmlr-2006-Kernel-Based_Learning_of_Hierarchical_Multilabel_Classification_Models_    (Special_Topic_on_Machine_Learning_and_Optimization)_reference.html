<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-41" href="../jmlr2006/jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">jmlr2006-41</a> <a title="jmlr-2006-41-reference" href="#">jmlr2006-41-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-41-pdf" href="http://jmlr.org/papers/volume7/rousu06a/rousu06a.pdf">pdf</a></p><p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><br/>
<h2>reference text</h2><p>S. M. Aji and R. McEliece. The generalized distributive law. IEEE Transactions on Information Theory, 46(2):325–343, 2000. Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden markov support vector machines. In International Conference of Machine Learning, 2003. D. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 1999. L. Cai and T. Hofmann. Hierarchical document categorization with support vector machines. In 13 ACM CIKM, 2004. N. Cancedda, E. Gaussier, C. Goutte, and J.-M. Renders. Word-sequence kernels. Journal of Machine Learning Research, 3:1059–1082, 2003. N. Cesa-Bianchi, C. Gentile, A. Tironi, and L. Zaniboni. Incremental algorithms for hierarchical classiﬁcation. In Neural Information Processing Systems, 2004. N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge University Press, 2000. O. Dekel, J. Keshet, and Y. Singer. Large margin hierarchical classiﬁcation. In ICML’04, pages 209–216, 2004. S. T. Dumais and H. Chen. Hierarchical classiﬁcation of web content. In SIGIR’00, pages 256–263, 2000. T. Gartner. A survey of kernels for structured data. ACM SIGKDD Explorations, pages 49–58, 2003. T. Hofmann, L. Cai., and M. Ciaramita. Learning with taxonomies: Classifying documents and words. In NIPS Workshop on Syntax, Semantics, and Statistics, 2003. T. Joachims. Text categorization with support vector machines: Learning with many relevant features. In Proceedings of the European Conference on Machine Learning, pages 137 – 142, Berlin, 1998. Springer. D. Koller and M. Sahami. Hierarchically classifying documents using very few words. In ICML’97, pages 170–178, 1997. F. Kschischang, B. Frey, and H.-A. Loeliger. Factor graphs and the sum-product algorithm. IEEE Transactions on Information Theory, 47:498–519, 2001. J. Lafferty, X. Zhu, and Y. Liu. Kernel conditional random ﬁelds: representation and clique selection. In Proc. 21th International Conference on Machine Learning, pages 504–511, 2004. C. Leslie, E. Eskin, and W. S. Noble. The spectrum kernel: A string kernel for SVM protein classiﬁcation. In Proceedings of the Paciﬁc Symposium on Biocomputing, pages 564 – 575, 2002. D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A new benchmark collection for text categorization research. JMLR, 5:361–397, Apr 2004. 1625  ROUSU , S AUNDERS , S ZEDMAK AND S HAWE -TAYLOR  H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. Text classiﬁcation using string kernels. Journal of Machine Learning Research, 2:419–444, February 2002. A. McCallum, R. Rosenfeld, T. Mitchell, and A. Y. Ng. Improving text classiﬁcation by shrinkage in a hierarchy of classes. In ICML’98, pages 359–367, 1998. J. Rousu and J. Shawe-Taylor. Efﬁcient computation of gapped substring kernels on large alphabets. JMLR, 6:1323–1344, 2005. G. Salton. Automatic Text Processing. Addison-Wesley, Massachusetts, 1989. C. Saunders, H. Tschach, and J. Shawe-Taylor. Syllables and other string kernel extensions. In ICML’02, pages 530–537, 2002. J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004. B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In Neural Information Processing Systems 2003, 2003. B. Taskar, V. Chatalbashev, and D. Koller. Learning associative markov networks. In Proc. 21th International Conference on Machine Learning, pages 807–814, 2004. I. Tsochantaridis, T. Hofmann, T. Joachims, and Y.n Altun. Support vector machine learning for interdependent and structured output spaces. In Proc. 21th International Conference on Machine Learning, pages 823–830, 2004. S. V. N. Vishwanathan and A. J. Smola. Fast kernels on strings and trees. In Proceedings of Neural Information Processing Systems 2002, 2002. URL http://users.rsise.anu.edu.au/ vishy/papers/VisSmo02.pdf. M. Wainwright and M. Jordan. Graphical models, exponential families, and variational inference. Technical Report 649, Department of Statistics, University of California, Berkeley, 2003. M. Wainwright, T. Jaakkola, and A. Willsky. Tree-based reparameterization framework for analysis of sum-product and related algorithms. IEEE Transactions on information theory, 49:1120–1146, May 2003. WIPO. World Intellectual Property Organization. http://www.wipo.int/classiﬁcations/en. 2001.  1626</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
