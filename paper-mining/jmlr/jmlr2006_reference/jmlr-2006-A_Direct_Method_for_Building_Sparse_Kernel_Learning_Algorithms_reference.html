<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-1" href="../jmlr2006/jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">jmlr2006-1</a> <a title="jmlr-2006-1-reference" href="#">jmlr2006-1-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</h1>
<br/><p>Source: <a title="jmlr-2006-1-pdf" href="http://jmlr.org/papers/volume7/wu06a/wu06a.pdf">pdf</a></p><p>Author: Mingrui Wu, Bernhard Schölkopf, Gökhan Bakır</p><p>Abstract: Many kernel learning algorithms, including support vector machines, result in a kernel machine, such as a kernel classiﬁer, whose key component is a weight vector in a feature space implicitly introduced by a positive deﬁnite kernel function. This weight vector is usually obtained by solving a convex optimization problem. Based on this fact we present a direct method to build sparse kernel learning algorithms by adding one more constraint to the original convex optimization problem, such that the sparseness of the resulting kernel machine is explicitly controlled while at the same time performance is kept as high as possible. A gradient based approach is provided to solve this modiﬁed optimization problem. Applying this method to the support vectom machine results in a concrete algorithm for building sparse large margin classiﬁers. These classiﬁers essentially ﬁnd a discriminating subspace that can be spanned by a small number of vectors, and in this subspace, the diﬀerent classes of data are linearly well separated. Experimental results over several classiﬁcation benchmarks demonstrate the eﬀectiveness of our approach. Keywords: sparse learning, sparse large margin classiﬁers, kernel learning algorithms, support vector machine, kernel Fisher discriminant</p><br/>
<h2>reference text</h2><p>K. P. Bennett. Combining support vector and mathematical programming methods for classiﬁcation. In B. Sch¨lkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in o Kernel Methods, pages 307–326. The MIT Press, Cambridge MA, 1999. C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, Oxford, UK, 1995. C. J. C. Burges. Simpliﬁed support vector decision rules. In L. Saitta, editor, Proc. 13th International Conference on Machine Learning, pages 71–77. Morgan Kaufmann, 1996. O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector machines. Machine Learning, 46(1-3):131–159, 2002. J. Gauvin and F. Dubeau. Diﬀerential properties of the marginal function in mathematical programming. Mathematical Programming Study, 19:101–119, 1982. 623  ¨ Wu, Scholkopf and Bakır  G. R. G. Lanckriet, L. E. Ghaoui, C. Bhattacharyya, and M. I. Jordan. A robust minimax approach to classiﬁcation. Journal of Machine Learning Research, 3:555–582, 2002. Y. Lee and O. L. Mangasarian. RSVM: reduced support vector machines. In CD Proceedings of the First SIAM International Conference on Data Mining, Chicago, 2001. K. Lin and C. Lin. A study on reduced support vector machines. IEEE Transactions on Neural Networks, 14:1449–1459, 2003. D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization. Math. Programming, 45(3, (Ser. B)):503–528, 1989. O. L. Mangasarian. Nonlinear Programming. McGraw-Hill, New York, 1969. S. Mika, G. R¨tsch, J. Weston, B. Sch¨lkopf, A. J. Smola, and K.-R. Mueller. Constructing a o descriptive and discriminative non-linear features: Rayleigh coeﬃcients in kernel feature spaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(5):623–628, 2003. P. B. Nair, A. Choudhury, and A. J. Keane. Some greedy learning algorithms for sparse regression and classiﬁcation with Mercer kernels. Journal of Machine Learning Research, 3:781–801, 2002. B. Sch¨lkopf and A. J. Smola. Learning with Kernels. The MIT Press, Cambridge, MA, o 2002. J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, Cambridge, UK, 2004. Edward Snelson and Zoubin Ghahramani. Sparse gaussian processes using pseudo-inputs. In Y. Weiss, B. Sch¨lkopf, and J. Platt, editors, Advances in Neural Information Processing o Systems 18, pages 1259–1266. MIT Press, Cambridge, MA, 2006. I. Steinwart. Sparseness of support vector machine. Journal of Machine Learning Research, 4:1071–1105, 2003. J. A. K. Suykens, T. V. Gestel, J. D. Brabanter, B. D. Moor, and J. Vandewalle. Least Squares Support Vector Machines. World Scientiﬁc, Singapore, 2002. M. E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1:211–244, 2001. V. Vapnik. The Nature of Statistical Learning Theory. Springer Verlag, New York, 1995. M. Wu, B. Sch¨lkopf, and G. Bakir. Building sparse large margin classiﬁers. In L. D. Raedt o and S. Wrobel, editors, Proc. 22th International Conference on Machine Learning, pages 1001–1008. ACM, 2005.  624</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
