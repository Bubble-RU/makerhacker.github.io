<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-26" href="../jmlr2006/jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">jmlr2006-26</a> <a title="jmlr-2006-26-reference" href="#">jmlr2006-26-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-26-pdf" href="http://jmlr.org/papers/volume7/shalev-shwartz06a/shalev-shwartz06a.pdf">pdf</a></p><p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><br/>
<h2>reference text</h2><p>S. Agarwal and P. Niyogi. Stability and generalization of bipartite ranking algorithms. In Proceedings of the Eighteenth Annual Conference on Computational Learning Theory, pages 32–47, 2005. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004. Y. Censor and S.A. Zenios. Parallel Optimization: Theory, Algorithms, and Applications. Oxford University Press, New York, NY, USA, 1997. S. Clemenon, G. Lugosi, and N. Vayatis. Ranking and scoring using empirical risk minimization. In Proceedings of the Eighteenth Annual Conference on Computational Learning Theory, 2005. W. W. Cohen, R. E. Schapire, and Y. Singer. Learning to order things. Journal of Artiﬁcial Intelligence Research, 10:243–270, 1999. T. H. Cormen, C. E. Leiserson, and R. L. Rivest. Introduction to Algorithms. MIT Press, 1990. C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, September 1995. K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. Jornal of Machine Learning Research, 2:265–292, 2001. K. Crammer and Y. Singer. A new family of online algorithms for category ranking. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002. K. Crammer and Y. Singer. Online ranking by projecting. Neural Computation, 17(1), 2005. O. Dekel, C. Manning, and Y. Singer. Log-linear models for label ranking. In Advances in Neural Information Processing Systems 16, 2003. A. Elisseeff and J. Weston. A kernel method for multi-labeled classiﬁcation. In Advances in Neural Information Processing Systems 14, 2001. G. Fung, R. Rosales, and B. Krishnapuram. Learning rankings via convex hull separation. In Advances in Neural Information Processing Systems 18, 2006. R. Herbrich, T. Graepel, and K. Obermayer. Large marging rank boundaries for ordinal regression. In A. Smola, B. Sch¨ lkopf, and D. Schuurmans, editors, Advances in Large Margin Classiﬁers. o MIT Press, 2000. C. Hildreth. A quadratic programming procedure. Naval Research Logistics Quarterly, 4:79–85, 1957. Erratum, ibidem, p.361. T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), 2002. C.-J. Lin. A formal analysis of stopping criteria of decomposition methods for support vector machines. IEEE Transactions on Neural Networks, 13(5):1045–1052, Sept. 2002. 1598  SOPOPO - S OFT P ROJECTIONS ONTO P OLYHEDRA  O. Mangasarian and D. Musicant. Successive overrelaxation for support vector machines. IEEE Transactions on Neural Networks, 10, 1999. J. C. Platt. Fast training of Support Vector Machines using sequential minimal optimization. In B. Sch¨ lkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector o Learning. MIT Press, 1998. C. Rudin, C. Cortes, M. Mohri, and R.E. Schapire. Margin-based ranking and boosting meet in the middle. In Proceedings of the Eighteenth Annual Conference on Computational Learning Theory, pages 63–78, 2005. R.J. Vanderbei. LOQO: An interior point code for quadratic programming. Optimization Mthods and Software, 12:451–484, 1999. V. N. Vapnik. Statistical Learning Theory. Wiley, 1998. J. Weston and C. Watkins. Support vector machines for multi-class pattern recognition. In Proceedings of the Seventh European Symposium on Artiﬁcial Neural Networks, April 1999.  1599</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
