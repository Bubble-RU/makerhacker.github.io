<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-11" href="../jmlr2006/jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">jmlr2006-11</a> <a title="jmlr-2006-11-reference" href="#">jmlr2006-11-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</h1>
<br/><p>Source: <a title="jmlr-2006-11-pdf" href="http://jmlr.org/papers/volume7/sugiyama06a/sugiyama06a.pdf">pdf</a></p><p>Author: Masashi Sugiyama</p><p>Abstract: The goal of active learning is to determine the locations of training input points so that the generalization error is minimized. We discuss the problem of active learning in linear regression scenarios. Traditional active learning methods using least-squares learning often assume that the model used for learning is correctly speciﬁed. In many practical situations, however, this assumption may not be fulﬁlled. Recently, active learning methods using “importance”-weighted least-squares learning have been proposed, which are shown to be robust against misspeciﬁcation of models. In this paper, we propose a new active learning method also using the weighted least-squares learning, which we call ALICE (Active Learning using the Importance-weighted least-squares learning based on Conditional Expectation of the generalization error). An important difference from existing methods is that we predict the conditional expectation of the generalization error given training input points, while existing methods predict the full expectation of the generalization error. Due to this difference, the training input design can be ﬁne-tuned depending on the realization of training input points. Theoretically, we prove that the proposed active learning criterion is a more accurate predictor of the single-trial generalization error than the existing criterion. Numerical studies with toy and benchmark data sets show that the proposed method compares favorably to existing methods. Keywords: Active Learning, Conditional Expectation of Generalization Error, Misspeciﬁcation of Models, Importance-Weighted Least-Squares Learning, Covariate Shift.</p><br/>
<h2>reference text</h2><p>H. Akaike. A new look at the statistical model identiﬁcation. IEEE Transactions on Automatic Control, AC-19(6):716–723, 1974. D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical models. Journal of Artiﬁcial Intelligence Research, 4:129–145, 1996. V. V. Fedorov. Theory of Optimal Experiments. Academic Press, New York, 1972. 165  S UGIYAMA  K. Fukumizu. Statistical active learning in multilayer perceptrons. IEEE Transactions on Neural Networks, 11(1):17–26, 2000. R. E. Henkel. Tests of Signiﬁcance. SAGE Publication, Beverly Hills, 1979. T. Kanamori and H. Shimodaira. Active learning algorithm using the maximum weighted loglikelihood estimator. Journal of Statistical Planning and Inference, 116(1):149–162, 2003. J. Kiefer. Optimum experimental designs. Journal of the Royal Statistical Society, Series B, 21: 272–304, 1959. D. E. Knuth. Seminumerical Algorithms, volume 2 of The Art of Computer Programming. AddisonWesley, Massachusetts, 1998. D. J. C. MacKay. Information-based objective functions for active data selection. Neural Computation, 4(4):590–604, 1992. F. Pukelsheim. Optimal Design of Experiments. John Wiley & Sons, 1993. C. R. Rao. Linear Statistical Inference and Its Applications. Wiley, New York, 1965. C. E. Rasmussen, R. M. Neal, G. E. Hinton, D. van Camp, M. Revow, Z. Ghahramani, R. Kustra, and R. Tibshirani. The DELVE manual, 1996. URL http://www.cs.toronto.edu/˜delve/. J. Rissanen. Modeling by shortest data description. Automatica, 14:465–471, 1978. G. Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6:461–464, 1978. H. Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of Statistical Planning and Inference, 90(2):227–244, 2000. M. Sugiyama and H. Ogawa. Incremental active learning for optimal generalization. Neural Computation, 12(12):2909–2940, 2000. M. Sugiyama and H. Ogawa. Active learning for optimal generalization in trigonometric polynomial models. IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences, E84-A(9):2319–2329, 2001. M. Sugiyama and H. Ogawa. Active learning with model selection — Simultaneous optimization of sample points and models for trigonometric polynomial models. IEICE Transactions on Information and Systems, E86-D(12):2753–2763, 2003. V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons, Inc., New York, 1998. D. P. Wiens. Robust weights and designs for biased regression models: Least squares and generalized M-estimation. Journal of Statistical Planning and Inference, 83(2):395–412, 2000.  166</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
