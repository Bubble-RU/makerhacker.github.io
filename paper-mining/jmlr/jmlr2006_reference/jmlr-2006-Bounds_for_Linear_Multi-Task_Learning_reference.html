<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>16 jmlr-2006-Bounds for Linear Multi-Task Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-16" href="../jmlr2006/jmlr-2006-Bounds_for_Linear_Multi-Task_Learning.html">jmlr2006-16</a> <a title="jmlr-2006-16-reference" href="#">jmlr2006-16-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>16 jmlr-2006-Bounds for Linear Multi-Task Learning</h1>
<br/><p>Source: <a title="jmlr-2006-16-pdf" href="http://jmlr.org/papers/volume7/maurer06a/maurer06a.pdf">pdf</a></p><p>Author: Andreas Maurer</p><p>Abstract: We give dimension-free and data-dependent bounds for linear multi-task learning where a common linear operator is chosen to preprocess data for a vector of task speciﬁc linear-thresholding classiﬁers. The complexity penalty of multi-task learning is bounded by a simple expression involving the margins of the task-speciﬁc classiﬁers, the Hilbert-Schmidt norm of the selected preprocessor and the Hilbert-Schmidt norm of the covariance operator for the total mixture of all task distributions, or, alternatively, the Frobenius norm of the total Gramian matrix for the data-dependent version. The results can be compared to state-of-the-art results on linear single-task learning. Keywords: learning to learn, transfer learning, multi-task learning</p><br/>
<h2>reference text</h2><p>[1] R. K. Ando, T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6: 1817-1853, 2005. 138  L INEAR M ULTI -TASK L EARNING</p>
<p>[2] M. Anthony and P. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, Cambridge, UK, 1999.</p>
<p>[3] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian Complexities: Risk Bounds and Structural Results. Journal of Machine Learning Research, 3: 463-482, 2002.</p>
<p>[4] P. Bartlett, O. Bousquet and S. Mendelson. Local Rademacher complexities. Available online: http://www.stat.berkeley.edu/˜bartlett/papers/bbm-lrc-02b.pdf.</p>
<p>[5] P. Baxendale. Gaussian measures on function spaces. Amer. J. Math., 98:891-952, 1976.</p>
<p>[6] J. Baxter. Theoretical Models of Learning to Learn, in Learning to Learn, S.Thrun, L.Pratt Eds. Springer 1998.</p>
<p>[7] J. Baxter. A Model of Inductive Bias Learning. Journal of Artiﬁcial Intelligence Research 12: 149-198, 2000.</p>
<p>[8] S. Ben-David and R. Schuller. Exploiting task relatedness for multiple task learning. In COLT 03, 2003.</p>
<p>[9] R. Caruana. Multitask Learning, in Learning to Learn, S.Thrun, L.Pratt Eds. Springer 1998.</p>
<p>[10] Nello Cristianini and John Shawe-Taylor. Support Vector Machines. Cambridge University Press, 2000.</p>
<p>[11] T. Evgeniou and M. Pontil. Regularized multi-task learning. Proc. Conference on Knowledge Discovery and Data Mining, 2004.</p>
<p>[12] V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error of combined classiﬁers. The Annals of Statistics, Vol. 30, No 1, 1-50.</p>
<p>[13] Colin McDiarmid. Concentration, in Probabilistic Methods of Algorithmic Discrete Mathematics, p. 195-248. Springer, Berlin, 1998.</p>
<p>[14] C. A. Miccheli and M. Pontil. Kernels for multi-task learning. Available online, 2005.</p>
<p>[15] S.Mika, B.Sch¨ lkopf, A.Smola, K.-R.M¨ ller, M.Scholz and G.R¨ tsch. Kernel PCA and Deo u a noising in Feature Spaces. Advances in Neural Information Processing Systems 11, 1998.</p>
<p>[16] J. Shawe-Taylor, N. Cristianini. Estimating the moments of a random vector. Proceedings of GRETSI 2003 Conference, I: 47–52, 2003.</p>
<p>[17] Michael Reed and Barry Simon. Functional Analysis, part I of Methods of Mathematical Physics, Academic Press, 1980.</p>
<p>[18] S. Thrun. Lifelong Learning Algorithms, in Learning to Learn, S.Thrun, L.Pratt Eds. Springer 1998  139</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
