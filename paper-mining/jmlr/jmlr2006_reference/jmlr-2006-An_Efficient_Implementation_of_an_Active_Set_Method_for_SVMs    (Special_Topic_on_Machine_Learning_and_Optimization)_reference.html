<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-14" href="../jmlr2006/jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">jmlr2006-14</a> <a title="jmlr-2006-14-reference" href="#">jmlr2006-14-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-14-pdf" href="http://jmlr.org/papers/volume7/scheinberg06a/scheinberg06a.pdf">pdf</a></p><p>Author: Katya Scheinberg</p><p>Abstract: We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efﬁcient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims’ SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difﬁcult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight , the generalization properties of these two approaches are identical and we do not discuss them in the paper. Keywords: active set methods, support vector machines, quadratic programming</p><br/>
<h2>reference text</h2><p>J. Balcazar, Y. Dai, and O Watanabe. Provably fast trainning algorithms for support vector machines. In Proc. 1st IEEE International Conference on Data Mining, pages 43–50. IEEE, 2001. R. E. Bixby, J. W. Gregory, I. J. Lustig, R. E. Marsten, and D. F. Shanno. Very large-scale linear programming: A case study in combining interior point and simplex methods. Operations Research, 40(5):885–897, 1992. C. L. Blake and C. J Merz. UCI repository of machine learning databases, 1998. http://www.ics.uci.edu/∼mlearn/MLRepository.html.  URL  B. Boser, I. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classiﬁers. In D. Haussler, editor, Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory, pages 144–152. ACM Press, 1992. G. Cauwenberghs and T. Poggio. Incremental and decremental support vector machine learning. In Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, Advances in Neural Information Processing Systems 13, pages 409–415. MIT Press, 2001. N. Cristianini and J. Shawe-Taylor. An Introductin to Support Vector Macines and Other KernelBased Learning Methods. Cambridge University Press, 2000. M. C. Ferris and T. S. Munson. Interior point methods for massive support vector machines. Technical Report 00-05, Computer Sciences Department, University of Wisconsin, Madison, WI, 2000. S. Fine and K. Scheinberg. Efﬁcient svm training using low-rank kernel representations. Journal of Machine Learning Research, 2:243–264, 2001. R. Fletcher. A general quadratic programming algorithm. Journal of the Institute of Mathematics and Its Applications, pages 76–91, 1971. J. J. Forrest. Mathematical programming with a library of optimization subroutines. Presented at the ORSA/TIMS Joint National Meeting, 1989. A. Frangioni. Solving semideﬁnite quadratic problems within nonsmooth optimization algorithms. Computers in Operations Research, 23(11):1099–1118, 1996. D. Goldfarb. Extension of newton’s method and simplex method for solving quadratic problems. In F. A. Lootsma, editor, Numerical Methods for Nonlinear Optimization, pages 239–254. Academic Press, London, 1972. D. Goldfarb and A. Idnani. A numerically stable dual method for solving strictly convex quadratic programms. Mathematical Programming, 27:1–33, 1983. 2256  ACTIVE S ET M ETHOD FOR SVM S  G. H. Golub and Ch. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore and London, 3rd edition, 1996. T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. The entire regularization path. Journal of Machine Learning Research, 5:1391–1415, 2004. ¨ T. Joachims. Making large-scale support vector machine nlearning practicle. In B. Sch olkopf, C. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods, chapter 12, pages 169–184. MIT Press, 1999. L. Kaufman. Solving the quadratic programming problem arising in support vector classiﬁcation. In B. Sch¨ lkopf, C. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods Support o Vector Learning, pages 147–168. MIT Press, 1998. K. C. Kiwiel. A dual method for certain positive semideﬁnite quadratic programming problems. SIAM Journal on Scientiﬁc and Statistical Computing, 10:175–186, 1989. J Nocedal and S. Wright. Numerical Optimization. Springer-Verlag, 1999. E. Osuna, R. Freund, and F. Girosi. An improved training algorithm for support vector machines. In Proceedings of the IEEE Neural Networks for Signal Processing VII Workshop, pages 276–285. IEEE, 1997. J. C. Platt. Fast trining support vector machines using sequential mininal optimization. In B. Sch¨ lkopf, C. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods, chapter 12, o pages 185–208. MIT Press, 1999. R. Vanderbei. Linear Programming: Foundations and Extensions. Sringer, New York, NY, 2nd edition, 2001.  2257</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
