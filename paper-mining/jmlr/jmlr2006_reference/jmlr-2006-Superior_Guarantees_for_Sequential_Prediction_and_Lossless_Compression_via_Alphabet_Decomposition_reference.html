<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>90 jmlr-2006-Superior Guarantees for Sequential Prediction and Lossless Compression via Alphabet Decomposition</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-90" href="../jmlr2006/jmlr-2006-Superior_Guarantees_for_Sequential_Prediction_and_Lossless_Compression_via_Alphabet_Decomposition.html">jmlr2006-90</a> <a title="jmlr-2006-90-reference" href="#">jmlr2006-90-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>90 jmlr-2006-Superior Guarantees for Sequential Prediction and Lossless Compression via Alphabet Decomposition</h1>
<br/><p>Source: <a title="jmlr-2006-90-pdf" href="http://jmlr.org/papers/volume7/begleiter06a/begleiter06a.pdf">pdf</a></p><p>Author: Ron Begleiter, Ran El-Yaniv</p><p>Abstract: We present worst case bounds for the learning rate of a known prediction method that is based on hierarchical applications of binary context tree weighting (CTW) predictors. A heuristic application of this approach that relies on Huffman’s alphabet decomposition is known to achieve state-ofthe-art performance in prediction and lossless compression benchmarks. We show that our new bound for this heuristic is tighter than the best known performance guarantees for prediction and lossless compression algorithms in various settings. This result substantiates the efﬁciency of this hierarchical method and provides a compelling explanation for its practical success. In addition, we present the results of a few experiments that examine other possibilities for improving the multialphabet prediction performance of CTW-based algorithms. Keywords: sequential prediction, the context tree weighting method, variable order Markov models, error bounds</p><br/>
<h2>reference text</h2><p>E. L. Allwein, R. E. Schapire, and Y. Singer. Reducing multiclass to binary: a unifying approach for margin classiﬁers. Journal of Machine Learning Research, 1:113–141, 2001. R. Begleiter, R. El-Yaniv, and G. Yona. On prediction using variable order Markov models. Journal of Artiﬁcial Intelligence Research, 22:385–421, 2004. ˆ 12. In Orlitsky et al. (2003) the z+1 estimator is denoted by q+1 and zGT * by q1/3 . ˆ 13. I. J. Good and A. M. Turing used this estimator to break the Enigma Cipher (Hodges, 2000) during World War II. 14. Orlitsky et al. mention that Turing had an intuitive motivation for this estimator. Unfortunately, this explanation was never published.  408  S UPERIOR G UARANTEES FOR S EQUENTIAL P REDICTION AND L OSSLESS C OMPRESSION  G. Bejerano and G. Yona. Variations on probabilistic sufﬁx trees: Statistical modeling and the prediction of protein families. Bioinformatics, 17(1):23–43, 2001. T. C. Bell, J. G. Cleary, and I. H. Witten. Text Compression. Prentice-Hall, Inc., 1990. M. Burrows and D. J. Wheeler. A block-sorting lossless data compression algorithm. Technical Report 124, Digital Equipement Corporation, 1994. O. Catoni. Statistical learning theory and stochastic optimization. Lecture Notes in Mathematics, 1851, 2004. S. F. Chen and J. Goodman. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 310–318, 1996. S. Cheong, S. H. Oh, and S. Lee. Support vector machines with binary tree architecture for multiclass classiﬁcation. Neural Information Processing - Letters and Reviews, 2(3):47–51, March 2004. J. G. Cleary and W. J. Teahan. Experiments on the zero frequency problem. In DCC ’95: Proceedings of the Conference on Data Compression, page 480, Washington, DC, USA, 1995. IEEE Computer Society. R. Courant and F. John. Introduction to Calculus and Analysis. Springer-Verlag, 1989. T. Cover and J. Thomas. Elements of Information Theory. John Wiley and Sons, Inc., 1991. M. Effros, K. Visweswariah, S. R. Kulkarni, and S. Verdu. Universal lossless source coding with the Burrows Wheeler transform. IEEE Transactions on Information Theory, 48(5):1061–1081, 2002. R. El-Yaniv and N. Etzion-Rosenberg. Hierarchical multiclass decompositions with application to authorship determination. Technical Report CS-2004-15, Technion - Israel Institute of Technology, March 2004. Y. Freund. Predicting a binary sequence almost as well as the optimal biased coin. Information and Computation, 182(2):73–94, 2003. C. R. Glassey and R. M. Karp. On the optimality of Huffman trees. SIAM Journal on Applied Mathematics, 31(2):368–378, September 1976. I. J. Good. The population frequencies of species and the estimation of population parameters. Biometrika, 1953. D. Haussler, J. Kivinen, and M. K. Warmuth. Sequential prediction of individual sequences under general loss functions. IEEE Transactions on Information Theory, 44(5):1906–1925, 1998. D. P. Helmbold and R. E. Schapire. Predicting nearly as well as the best pruning of a decision tree. Machine Learning, 27(1):51–68, 1997. A. Hodges. Alan Turing: the enigma. Walker and Co., 2000. 409  B EGLEITER AND E L -YANIV  X. Huo, J. Chen, S. Wang, and K. L. Tsui. Support vector trees: Simultaneously realizing the principles of maximal margin and maximal purity. Technical report, The Logistics Institute, Georgia Tech, and Asia Paciﬁc, National University of Singapore, 2002. P. Jacquet and W. Szpankowski. Markov types and minimax redundancy for Markov sources. IEEE Transactions on Information Theory, 50:1393–1402, 2004. J. C. Kieffer and E. H. Yang. A simple technique for bounding the pointwise redundancy of the 1978 Lempel-Ziv algorithm. In DCC ’99: Proceedings of the Conference on Data Compression, page 434. IEEE Computer Society, 1999. R. Krichevsky and V. Troﬁmov. The performance of universal encoding. IEEE Transactions on Information Theory, 27:199–207, 1981. P. Laplace. Philosophical essays on probabilities. Springer-Verlag, 1995. Translated by A. Dale from the 5th (1825) edition. D. McAllester and R. Schapire. On the convergence rate of Good-Turing estimators. In In Proceedings of the Thirteenth annual conference on computational learning theory, 2000. N. Merhav and M. Feder. Universal prediction. IEEE Transactions on Information Theory, 44(6): 2124–2147, 1998. A. Orlitsky, N. P. Santhanam, and J. Zhang. Always Good Turing: Asymptotically optimal probability estimation. Science, 302(5644):427–431, October 2003. C. H. Papadimitriou. Computational Complexity. Addison-Wesley, 1994. V. N. Potapov. Redundancy estimates for the Lempel-Ziv algorithm of data compression. Discrete Applied Mathematics, 135(1-3):245–254, 2004. ISSN 0166-218X. R. Rifkin and A. Klautau. In defense of one-vs-all classiﬁcation. Journal of Machine Learning Research, 5:101–141, 2004. D. Ron, Y. Singer, and N. Tishby. The power of amnesia: Learning probabilistic automata with variable memory length. Machine Learning, 25(2–3):117–149, 1996. K. Sadakane, T. Okazaki, and H. Imai. Implementing the context tree weighting method for text compression. In Data Compression Conference, pages 123–132, 2000. S. A. Savari. Redundancy of the Lempel-Ziv incremental parsing rule. IEEE Transactions on Information Theory, 43:9–21, 1997. D. Shkarin. PPM: One step to practicality. In Data Compression Conference, pages 202–212, 2002. Y. Shtarkov. Universal sequential coding of single messages. Problems in Information Transmission, 23:175–186, 1987. N. J. A. Sloane and S. Plouffe. The Encyclopedia of Integer Sequences. Academic Press, 1995. T. J. Tjalkens, Y. Shtarkov, and F. M. J. Willems. Context tree weighting: Multi-alphabet sources. In Proc. 14th Symp. on Info. Theory, Benelux, pages 128–135, 1993. 410  S UPERIOR G UARANTEES FOR S EQUENTIAL P REDICTION AND L OSSLESS C OMPRESSION  T. J. Tjalkens, P. A. Volf, and F. M. J. Willems. A context-tree weighting method for text generating sources. In Data Compression Conference, page 472, 1997. T. J. Tjalkens, F. M. J. Willems, and Y. Shtarkov. Multi-alphabet universal coding using a binary decomposition context tree weighting algorithm. In Proc. 15th Symp.on Info. Theory, Benelux, pages 259–265, 1994. P. A. Volf. Weighting Techniques in Data Compression Theory and Algorithms. PhD thesis, Technische Universiteit Eindhoven, 2002. V. Vovk. Aggregating strategies. In Proceedings of the 3rd Annual Workshop on Computational Learning Theory, pages 371–383, 1990. F. Wilcoxon. Individual comparisons by ranking methods. Biometrics, 1:80–83, 1945. F. M. J. Willems. The context-tree weighting method: Extensions. IEEE Transactions on Information Theory, 44(2):792–798, March 1998. F. M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens. The context-tree weighting method: Basic properties. IEEE Transactions on Information Theory, pages 653–664, 1995. I. H. Witten and T. C. Bell. The zero-frequency problem: estimating the probabilities of novelevents in adaptive text compression. IEEE Transactions on Information Theory, 37(4):1085–1094, 1991. Q. Xie and A. R. Barron. Asymptotic minimax regret for data compression, gambling, and prediction. IEEE Transactions on Information Theory, 46(2):431–445, 2000. E. H. Yang and J. C. Kieffer. Efﬁcient universal lossless data compression algorithms based on a greedy sequential grammar transform. part one: Without context models. IEEE Transactions on Information Theory, 46(3):755–777, 2000. J. Ziv and A. Lempel. A universal algorithm for sequential data compression. IEEE Transactions on Information Theory, 23:337–343, May 1977. J. Ziv and A. Lempel. Compression of individual sequences via variable-rate coding. IEEE Transactions on Information Theory, 24:530–536, 1978.  411</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
