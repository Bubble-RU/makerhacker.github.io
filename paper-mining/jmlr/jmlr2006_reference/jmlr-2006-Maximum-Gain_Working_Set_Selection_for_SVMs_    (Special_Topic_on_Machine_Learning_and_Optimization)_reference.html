<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-61" href="../jmlr2006/jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">jmlr2006-61</a> <a title="jmlr-2006-61-reference" href="#">jmlr2006-61-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-61-pdf" href="http://jmlr.org/papers/volume7/glasmachers06a/glasmachers06a.pdf">pdf</a></p><p>Author: Tobias Glasmachers, Christian Igel</p><p>Abstract: Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is signiﬁcantly faster. Keywords: working set selection, sequential minimal optimization, quadratic programming, support vector machines, large scale optimization</p><br/>
<h2>reference text</h2><p>C. L. Blake and C. J. Merz. UCI repository of machine learning databases, 1998. http://www. ics.uci.edu/∼mlearn/MLRepository.html. A. Bordes, S. Ertekin, J. Weston, and L´ on Bottou. Fast kernel classiﬁers with online and active e learning. Journal of Machine Learning Research, 5:1579–1619, 2005. C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines, 2001. http://www. csie.ntu.edu.tw/∼cjlin/libsvm. N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines and other kernelbased learning methods. Cambridge University Press, 2000. R.-E. Fan, P.-H. Chen, and C.-J. Lin. Working set selection using the second order information for training support vector machines. Journal of Machine Learning Research, 6:1889–1918, 2005. D. Hush and C. Scovel. Polynomial-time decomposition algorithms for support vector machines. Machine Learning, 51:51–71, 2003. T. Joachims. Making large-scale SVM learning practical. In B. Sch¨ lkopf, C. Burges, and A. Smola, o editors, Advances in Kernel Methods – Support Vector Learning, chapter 11, pages 169–184. MIT Press, 1999. S. S. Keerthi and E. G. Gilbert. Convergence of a generalized SMO algorithm for SVM classiﬁer design. Machine Learning, 46:351–360, 2002. S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and K. R. K. Murthy. A fast iterative nearest point algorithm for support vector machine classiﬁer design. IEEE Transactions on Neural Networks, 11(1):124–136, 2000. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. C.-J. Lin. On the convergence of the decomposition method for support vector machines. IEEE Transactions on Neural Networks, 12:1288–1298, 2001. N. List and H. U. Simon. A general convergence theorem for the decomposition method. In John Shawe-Taylor and Yoram Singer, editors, Proceedings of the 17th Annual Conference on Learning Theory, COLT 2004, volume 3120 of LNCS, pages 363–377. Springer-Verlag, 2004. N. List and H. U. Simon. General polynomial time decomposition algorithms. In Peter Auer and Ron Meir, editors, Proceedings of the 18th Annual Conference on Learning Theory, COLT 2005, volume 3559 of LNCS, pages 308–322. Springer-Verlag, 2005. E. Osuna, R. Freund, and F. Girosi. Improved training algorithm for support vector machines. In J. Principe, L. Giles, N. Morgan, and E. Wilson, editors, Neural Networks for Signal Processing VII, pages 276–285. IEEE Press, 1997. 1465  G LASMACHERS AND I GEL  J. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Sch¨ lkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support o Vector Learning, chapter 12, pages 185–208. MIT Press, 1999. B. Sch¨ lkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regularization, o Optimization, and Beyond. MIT Press, 2002. N. Takahashi and T. Nishi. Rigorous proof of termination of SMO algorithm for support vector machines. IEEE Transaction on Neural Networks, 16(3):774–776, 2005. I. W. Tsang, J. T. Kwok, and P.-M. Cheung. Core vector machines: Fast SVM training on very large data sets. Journal of Machine Learning Research, 6:363–392, 2005. V. N. Vapnik. Statistical Learning Theory. Wiley, New-York, 1998. S. V. N. Vishwanathan, A. J. Smola, and M. N. Murty. SimpleSVM. In T. Fawcett and N. Mishra, editors, Proceedings of the Twentieth International Conference on Machine Learning (ICML2003), pages 760–767. AAAI Press, 2003.  1466</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
