<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 jmlr-2006-Computational and Theoretical Analysis of  Null Space  and Orthogonal Linear Discriminant Analysis</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-21" href="../jmlr2006/jmlr-2006-Computational_and_Theoretical_Analysis_of__Null_Space__and_Orthogonal_Linear_Discriminant_Analysis.html">jmlr2006-21</a> <a title="jmlr-2006-21-reference" href="#">jmlr2006-21-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>21 jmlr-2006-Computational and Theoretical Analysis of  Null Space  and Orthogonal Linear Discriminant Analysis</h1>
<br/><p>Source: <a title="jmlr-2006-21-pdf" href="http://jmlr.org/papers/volume7/ye06a/ye06a.pdf">pdf</a></p><p>Author: Jieping Ye, Tao Xiong</p><p>Abstract: Dimensionality reduction is an important pre-processing step in many applications. Linear discriminant analysis (LDA) is a classical statistical approach for supervised dimensionality reduction. It aims to maximize the ratio of the between-class distance to the within-class distance, thus maximizing the class discrimination. It has been used widely in many applications. However, the classical LDA formulation requires the nonsingularity of the scatter matrices involved. For undersampled problems, where the data dimensionality is much larger than the sample size, all scatter matrices are singular and classical LDA fails. Many extensions, including null space LDA (NLDA) and orthogonal LDA (OLDA), have been proposed in the past to overcome this problem. NLDA aims to maximize the between-class distance in the null space of the within-class scatter matrix, while OLDA computes a set of orthogonal discriminant vectors via the simultaneous diagonalization of the scatter matrices. They have been applied successfully in various applications. In this paper, we present a computational and theoretical analysis of NLDA and OLDA. Our main result shows that under a mild condition which holds in many applications involving highdimensional data, NLDA is equivalent to OLDA. We have performed extensive experiments on various types of data and results are consistent with our theoretical analysis. We further apply the regularization to OLDA. The algorithm is called regularized OLDA (or ROLDA for short). An efﬁcient algorithm is presented to estimate the regularization value in ROLDA. A comparative study on classiﬁcation shows that ROLDA is very competitive with OLDA. This conﬁrms the effectiveness of the regularization in ROLDA. Keywords: linear discriminant analysis, dimensionality reduction, null space, orthogonal matrix, regularization</p><br/>
<h2>reference text</h2><p>P. N. Belhumeour, J. P. Hespanha, and D. J. Kriegman. Eigenfaces vs. Fisherfaces: Recognition using class speciﬁc linear projection. IEEE Trans Pattern Analysis and Machine Intelligence, 19 (7):711–720, 1997. 1202  A NALYSIS OF NULL SPACE AND ORTHOGONAL LINEAR DISCRIMINANT ANALYSIS  R. Bellmanna. Adaptive Control Processes: A Guided Tour. Princeton University Press, 1961. M. W. Berry, S. T. Dumais, and G. W. O’Brie. Using linear algebra for intelligent information retrieval. SIAM Review, 37:573–595, 1995. L. F. Chen, H. Y. M. Liao, M. T. Ko, J. C. Lin, and G. J. Yu. A new LDA-based face recognition system which can solve the small sample size problem. Pattern Recognition, 33:1713–1726, 2000. S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent semantic analysis. Journal of the Society for Information Scienc, 41:391–407, 1990. L. Duchene and S. Leclerq. An optimal transformation for discriminant and principal component analysis. IEEE Trans. Pattern Analysis and Machine Intelligence, 10(6):978–983, 1988. R. O. Duda, P. E. Hart, and D. Stork. Pattern Classiﬁcation. Wiley, 2000. S. Dudoit, J. Fridlyand, and T. P. Speed. Comparison of discrimination methods for the classiﬁcation of tumors using gene expression data. Journal of the American Statistical Association, 97(457): 77–87, 2002. D. H. Foley and J. W. Sammon. An optimal set of discriminant vectors. IEEE Trans Computers, 24 (3):281–289, 1975. J. H. Friedman. Regularized discriminant analysis. Journal of the American Statistical Association, 84(405):165–175, 1989. K. Fukunaga. Introduction to Statistical Pattern Classiﬁcation. Academic Press, San Diego, California, USA, 1990. G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore, MD, USA, third edition, 1996. P. Hall, J. S. Marron, and A. Neeman. Geometric representation of high dimension, low sample size data. Journal of the Royal Statistical Society series B, 67:427–444, 2005. T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of Statistical Learning : Data Mining, Inference, and Prediction. Springer, 2001. R. Huang, Q. Liu, H. Lu, and S. Ma. Solving the small sample size problem of LDA. In Proc. International Conference on Pattern Recognition, pages 29–32, 2002. Z. Jin, J. Y. Yang, Z. S. Hu, and Z. Lou. Face recognition based on the uncorrelated discriminant transformation. Pattern Recognition, 34:1405–1416, 2001. I. T. Jolliffe. Principal Component Analysis. Springer-Verlag, New York, 1986. W. J. Krzanowski, P. Jonathan, W.V McCarthy, and M. R. Thomas. Discriminant analysis with singular covariance matrices: methods and applications to spectroscopic data. Applied Statistics, 44:101–115, 1995. 1203  Y E AND X IONG  T. Li, C. Zhang, and M. Ogihara. A comparative study of feature selection and multiclass classiﬁcation methods for tissue classiﬁcation based on gene expression. Bioinformatics, 20(15): 2429–2437, 2004. W. Liu, Y. Wang, S. Z. Li, and T. Tan. Null space approach of Fisher discriminant analysis for face recognition. In Proc. European Conference on Computer Vision, Biometric Authentication Workshop, 2004. J. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos. Face recognition using kernel direct discriminant analysis algorithms. IEEE Trans. Neural Networks, 14(1):117–126, 2003. S. Raudys and R. P. W. Duin. On expected classiﬁcation error of the Fisher linear classiﬁer with pseudo-inverse covariance matrix. Pattern Recognition Letters, 19(5-6):385–392, 1998. B. Sch¨ kopf and A. Smola. Learning with Kernels: Support Vector Machines, Regularization, o Optimization and Beyond. MIT Press, 2002. M. Skurichina and R. P. W. Duin. Stabilizing classiﬁers for very small sample size. In Proc. International Conference on Pattern Recognition, pages 891–896, 1996. D. L. Swets and J. Y. Weng. Using discriminant eigenfeatures for image retrieval. IEEE Trans. Pattern Analysis and Machine Intelligence, 18(8):831–836, 1996. M. A. Turk and A. P. Pentland. Face recognition using Eigenfaces. In Proc. Computer Vision and Pattern Recognition Conference, pages 586–591, 1991. V. N. Vapnik. Statistical Learning Theory. Wiley, New York, 1998. J. Yang, A. F. Frangi, J. Y. Yang, D. Zhang, and Z. Jin. KPCA plus LDA: a complete kernel Fisher discriminant framework for feature extraction and recognition. IEEE Trans. Pattern Analysis and Machine Intelligence, 27(2):230– 244, 2005. J. Ye. Characterization of a family of algorithms for generalized discriminant analysis on undersampled problems. Journal of Machine Learning Research, 6:483–502, 2005. J. Ye, R. Janardan, Q. Li, and H. Park. Feature extraction via generalized uncorrelated linear discriminant analysis. In Proc. International Conference on Machine Learning, pages 895–902, 2004a. J. Ye, T. Li, T. Xiong, and R. Janardan. Using uncorrelated discriminant analysis for tissue classiﬁcation with gene expression data. IEEE/ACM Trans. Computational Biology and Bioinformatics, 1(4):181–190, 2004b. H. Yu and J. Yang. A direct LDA algorithm for high-dimensional data with applications to face recognition. Pattern Recognition, 34:2067–2070, 2001.  1204</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
