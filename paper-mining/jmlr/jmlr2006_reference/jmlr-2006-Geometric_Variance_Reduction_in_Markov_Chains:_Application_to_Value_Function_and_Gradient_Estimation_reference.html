<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>35 jmlr-2006-Geometric Variance Reduction in Markov Chains: Application to Value Function and Gradient Estimation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-35" href="../jmlr2006/jmlr-2006-Geometric_Variance_Reduction_in_Markov_Chains%3A_Application_to_Value_Function_and_Gradient_Estimation.html">jmlr2006-35</a> <a title="jmlr-2006-35-reference" href="#">jmlr2006-35-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>35 jmlr-2006-Geometric Variance Reduction in Markov Chains: Application to Value Function and Gradient Estimation</h1>
<br/><p>Source: <a title="jmlr-2006-35-pdf" href="http://jmlr.org/papers/volume7/munos06a/munos06a.pdf">pdf</a></p><p>Author: Rémi Munos</p><p>Abstract: We study a variance reduction technique for Monte Carlo estimation of functionals in Markov chains. The method is based on designing sequential control variates using successive approximations of the function of interest V . Regular Monte Carlo estimates have a variance of O(1/N), where N is the number of sample trajectories of the Markov chain. Here, we obtain a geometric variance reduction O(ρN ) (with ρ < 1) up to a threshold that depends on the approximation error V − A V , where A is an approximation operator linear in the values. Thus, if V belongs to the right approximation space (i.e. A V = V ), the variance decreases geometrically to zero. An immediate application is value function estimation in Markov chains, which may be used for policy evaluation in a policy iteration algorithm for solving Markov Decision Processes. Another important domain, for which variance reduction is highly needed, is gradient estimation, that is computing the sensitivity ∂αV of the performance measure V with respect to some parameter α of the transition probabilities. For example, in policy parametric optimization, computing an estimate of the policy gradient is required to perform a gradient optimization method. We show that, using two approximations for the value function and the gradient, a geometric variance reduction is also achieved, up to a threshold that depends on the approximation errors of both of those representations.</p><br/>
<h2>reference text</h2><p>C. G. Atkeson, S. A. Schaal, and Andrew W. Moore. Locally weighted learning. AI Review, 11, 1997. J. Baxter and P. L. Bartlett. Inﬁnite-horizon gradient-based policy search. Journal of Artiﬁcial Intelligence Research, 15:319–350, 2001. P. W. Glynn. Likelihood ratio gradient estimation: an overview. In A. Thesen, H. Grant, and W. D. Kelton, editors, Proceedings of the 1987 Winter Simulation Conference, pages 366–375, 1987. E. Gobet and S. Maire. Sequential control variates for functionals of Markov processes. SIAM Journal on Numerical Analysis, 43(3):1256–1275, 2005.  426  G EOMETRIC VARIANCE R EDUCTION IN M ARKOV C HAINS  E. Greensmith, P. L. Bartlett, and J. Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5:1471–1530, 2005. J. H. Halton. A retrospective and prospective survey of the Monte-Carlo method. SIAM Review, 12 (1):1–63, 1970. J. H. Halton. Sequential Monte-Carlo techniques for the solution of linear systems. Journal of Scientiﬁc Computing, 9:213–257, 1994. J. M. Hammersley and D. C. Handscomb. Monte-Carlo Methods. Chapman and Hall, 1964. T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer Series in Statistics, 2001. C. Kollman, K. Baggerly, D. Cox, and R. Picard. Adaptive importance sampling on discrete Markov chains. The Annals of Applied Probability, 9(2):391–412, 1999. V. R. Konda and V. S. Borkar. Actor-critic-type learning algorithms for Markov decision processes. SIAM Journal of Control and Optimization, 38:1:94–123, 1999. S. Maire. An iterative computation of approximations on Korobov-like spaces. J. Comput. Appl. Math., 54(6):261–281, 2003. P. Marbach and J. N. Tsitsiklis. Approximate gradient methods in policy-space optimization of Markov reward processes. Journal of Discrete Event Dynamical Systems, 13:111–148, 2003. M. I. Reiman and A. Weiss. Sensitivity analysis via likelihood ratios. In J. Wilson, J. Henriksen, and S. Roberts, editors, Proceedings of the 1986 Winter Simulation Conference, pages 285–289, 1986. R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. Neural Information Processing Systems. MIT Press, pages 1057–1063, 2000. V. Vapnik. Statistical Learning Theory. John Wiley & Sons, New York, 1998. V. Vapnik, S. E. Golowich, and A. Smola. Support vector method for function approximation, regression estimation and signal processing. In Advances in Neural Information Processing Systems, pages 281–287, 1997. R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229–256, 1992.  427</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
