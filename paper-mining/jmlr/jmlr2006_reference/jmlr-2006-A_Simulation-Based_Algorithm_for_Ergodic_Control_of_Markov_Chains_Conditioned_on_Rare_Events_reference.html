<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-7" href="../jmlr2006/jmlr-2006-A_Simulation-Based_Algorithm_for_Ergodic_Control_of_Markov_Chains_Conditioned_on_Rare_Events.html">jmlr2006-7</a> <a title="jmlr-2006-7-reference" href="#">jmlr2006-7-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</h1>
<br/><p>Source: <a title="jmlr-2006-7-pdf" href="http://jmlr.org/papers/volume7/bhatnagar06a/bhatnagar06a.pdf">pdf</a></p><p>Author: Shalabh Bhatnagar, Vivek S. Borkar, Madhukar Akarapu</p><p>Abstract: We study the problem of long-run average cost control of Markov chains conditioned on a rare event. In a related recent work, a simulation based algorithm for estimating performance measures associated with a Markov chain conditioned on a rare event has been developed. We extend ideas from this work and develop an adaptive algorithm for obtaining, online, optimal control policies conditioned on a rare event. Our algorithm uses three timescales or step-size schedules. On the slowest timescale, a gradient search algorithm for policy updates that is based on one-simulation simultaneous perturbation stochastic approximation (SPSA) type estimates is used. Deterministic perturbation sequences obtained from appropriate normalized Hadamard matrices are used here. The fast timescale recursions compute the conditional transition probabilities of an associated chain by obtaining solutions to the multiplicative Poisson equation (for a given policy estimate). Further, the risk parameter associated with the value function for a given policy estimate is updated on a timescale that lies in between the two scales above. We brieﬂy sketch the convergence analysis of our algorithm and present a numerical application in the setting of routing multiple ﬂows in communication networks. Keywords: Markov decision processes, optimal control conditioned on a rare event, simulation based algorithms, SPSA with deterministic perturbations, reinforcement learning</p><br/>
<h2>reference text</h2><p>T. P. I. Ahamed, V. S. Borkar, and S. Juneja. Adaptive importance sampling technique for markov chains using stochastic approximation. Operations Research, 54(3):489–504, 2006. S. Balaji and S. P. Meyn. Multiplicative ergodicity and large deviations for an irreducible markov chain. Stochastic Processes and their Appl., 90:123–144, 2000. J. Baxter and P. L. Bartlett. Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial Intelligence Research, 15:319–350, 2001. J. Baxter, P. L. Bartlett, and L. Weaver. Experiments with inﬁnite-horizon, policy-gradient estimation. Journal of Artiﬁcial Intelligence Research, 15:351–381, 2001. D. P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, Belmont, MA, USA, 2001. D. P. Bertsekas and R. Gallager. Data Networks. Prentice Hall, New Jersey, USA, 1991. D. P. Bertsekas and J. Tsitsiklis. Neuro-dynamic Programming. Athena Scientiﬁc, Boston, MA, USA, 1996. S. Bhatnagar. Adaptive multivariate three-timescale stochastic approximation algorithms for simulation based optimization. ACM Transactions on Modelling and Computer Simulation, 15(1): 74–107, 2005. S. Bhatnagar and V. S. Borkar. Multiscale stochastic approximation for parametric optimization of hidden Markov models. Prob. Engg. and Info. Sci., 11:509–522, 1997. S. Bhatnagar and V. S. Borkar. A two time scale stochastic approximation scheme for simulation based parametric optimization. Prob. Engg. and Info. Sci., 12:519–531, 1998. S. Bhatnagar, M. C. Fu, S. I. Marcus, and S. Bhatnagar. Two timescale algorithms for simulation optimization of hidden Markov models. IIE Transactions, 33(3):245–258, 2001. S. Bhatnagar, M. C. Fu, S. I. Marcus, and I-J. Wang. Two-timescale simultaneous perturbation stochastic approximation using deterministic perturbation sequences. ACM Transactions on Modelling and Computer Simulation, 13(2):180–209, 2003. S. Bhatnagar and S. Kumar. A simultaneous perturbation stochastic approximation based actorcritic algorithm for markov decision processes. IEEE Transactions on Automatic Control, 49(4): 592–598, 2004. 1960  E RGODIC C ONTROL OF M ARKOV C HAINS C ONDITIONED ON R ARE E VENTS  V. S. Borkar. Stochastic approximation with two timescales. System and Control Letters, 29:291– 294, 1997. V. S. Borkar. Asynchronous stochastic approximations. SIAM J. Control and Optimization, 36: 840–851, 1998. V. S. Borkar. A sensitivity formula for risk-sensitive cost and the actor-critic algorithm. System and Control Letters, 44:339–346, 2001. V. S. Borkar. Q-learning for risk-sensitive control. Mathematics of Operations Research, 27:294– 311, 2002. V. S. Borkar. Avoidance of traps in stochastic approximation. System and Control Letters, 50:1–9, 2003. V. S. Borkar, S. Juneja, and A. A. Kherani. Performance analysis conditioned on rare events: an adaptive simulation scheme. Communications in Information and Systems, 3:259–278, 2004. V. S. Borkar and S. P. Meyn. Risk-sensitive optimal control for markov decision processes with monotone cost. Mathematics of Operations Research, 27:192–209, 2002. O. Brandiere. Some pathological traps for stochastic approximation. SIAM J. Contr. and Optim., 36:1293–1314, 1998. J. Bucklew. Large Deviations Techniques in Decision, Simulation and Estimation. John Wiley, New York, 1990. X.-R. Cao. The relations among potentials, perturbation analysis, and markov decision processes. Discrete Event Dynamic Systems, 8:71–87, 1998. X.-R. Cao and X. Guo. A uniﬁed approach to markov decision problems and performance sensitivity analysis with discounted and average criteria: multichain cases. Automatica, 40:1749–1759, 2004. E. K. P. Chong and P. J. Ramadge. Stochastic optimization of regenerative systems using inﬁnitesimal perturbation analysis. IEEE Trans. Auto. Cont., 39(7):1400–1410, 1994. D. Hern´ ndez-Hern´ ndez and S. I. Marcus. Risk sensitive control of markov processes in countable a a state space. Systems and Control Letters, 29:147–155, 1996. Y. C. Ho and X. R. Cao. Perturbation Analysis of Discrete Event Dynamical Systems. Kluwer, Boston, 1991. V. R. Konda and V. S. Borkar. Actor-critic like learning algorithms for markov decision processes. SIAM Journal on Control and Optimization, 38(1):94–123, 1999. I. Kontoyiannis and S. P. Meyn. Spectral theory and limit theorems for geometrically ergodic markov processes. Annals of Applied Probability, 13:304–362, 2003. A. M. Law and W. D. Kelton. Simulation Modeling and Analysis. McGraw-Hill, New York, 2000. 1961  B HATNAGAR , B ORKAR AND M ADHUKAR  P. Marbach, O. Mihatsch, and J. N. Tsitsiklis. Call admission control and routing in integrated services networks using neuro-dynamic programming. IEEE J. Selected Areas in Communications, 18(2):197–208, 2000. P. Marbach and J. N. Tsitsiklis. Simulation-based optimization of markov reward processes. IEEE Transactions on Automatic Control, 46:191–209, 2001. A. Nowe, K. Steenhaut, M. Fakir, and K. Veerbeck. Q-learning for adaptive load based routing. In Proceedings of the IEEE International Conference on Systems, Man and Cybernetics, California, USA, 1998. San Diego. M. Puterman. Markov Decision Processes. Wiley Inter-science, 1994. R. Y. Rubinstein. Optimization of computer simulation models with rare events. European Journal of Operations Research, 19(1):89–112, 1997. J. C. Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. IEEE Transactions on Automatic Control, 37(3):332–341, 1992. J. C. Spall. A one-measurement form of simultaneous perturbation stochastic approximation. Automatica, 33:109–112, 1997. R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998. J. N. Tsitsiklis and D. P. Bertsekas. Distributed asynchronous optimal routing in data networks. IEEE Transactions on Automatic Control, 31:325–332, 1986. S. Varadarajan, N. Ramakrishnan, and M. Thirunavukkarasu. Reinforcing reachable routes. Computer Networks, 43(3):389–416, 2003. C. J. C. H. Watkins and P. Dayan. Q-learning. Machine Learning, 8:279–292, 1992.  1962</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
