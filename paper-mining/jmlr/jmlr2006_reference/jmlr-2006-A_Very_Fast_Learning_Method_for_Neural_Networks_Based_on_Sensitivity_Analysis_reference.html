<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 jmlr-2006-A Very Fast Learning Method for Neural Networks Based on Sensitivity Analysis</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-8" href="../jmlr2006/jmlr-2006-A_Very_Fast_Learning_Method_for_Neural_Networks_Based_on_Sensitivity_Analysis.html">jmlr2006-8</a> <a title="jmlr-2006-8-reference" href="#">jmlr2006-8-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>8 jmlr-2006-A Very Fast Learning Method for Neural Networks Based on Sensitivity Analysis</h1>
<br/><p>Source: <a title="jmlr-2006-8-pdf" href="http://jmlr.org/papers/volume7/castillo06a/castillo06a.pdf">pdf</a></p><p>Author: Enrique Castillo, Bertha Guijarro-Berdiñas, Oscar Fontenla-Romero, Amparo Alonso-Betanzos</p><p>Abstract: This paper introduces a learning method for two-layer feedforward neural networks based on sensitivity analysis, which uses a linear training algorithm for each of the two layers. First, random values are assigned to the outputs of the ﬁrst layer; later, these initial values are updated based on sensitivity formulas, which use the weights in each of the layers; the process is repeated until convergence. Since these weights are learnt solving a linear system of equations, there is an important saving in computational time. The method also gives the local sensitivities of the least square errors with respect to input and output data, with no extra computational cost, because the necessary information becomes available without extra calculations. This method, called the Sensitivity-Based Linear Learning Method, can also be used to provide an initial set of weights, which signiﬁcantly improves the behavior of other learning algorithms. The theoretical basis for the method is given and its performance is illustrated by its application to several examples in which it is compared with several learning algorithms and well known data sets. The results have shown a learning speed generally faster than other existing methods. In addition, it can be used as an initialization tool for other well known methods with signiﬁcant improvements. Keywords: supervised learning, neural networks, linear optimization, least-squares, initialization method, sensitivity analysis</p><br/>
<h2>reference text</h2><p>L. B. Almeida, T. Langlois, J. D. Amaral, and A. Plakhov. Parameter adaptation in stochastic optimization. In D. Saad, editor, On-line Learning in Neural Networks, chapter 6, pages 111– 134. Cambridge University Press, 1999. R. Battiti. First and second order methods for learning: Between steepest descent and Newton’s method. Neural Computation, 4(2):141–166, 1992. E. M. L. Beale. A derivation of conjugate gradients. In F. A. Lootsma, editor, Numerical methods for nonlinear optimization, pages 39–43. Academic Press, London, 1972. F. Biegler-K¨ nig and F. B¨ rmann. A learning algorithm for multilayered neural networks based on o a linear least-squares problems. Neural Networks, 6:127–131, 1993. W. L. Buntine and A. S. Weigend. Computing second derivatives in feed-forward networks: A review. IEEE Transactions on Neural Networks, 5(3):480–488, 1993. E. Castillo, J. M. Guti´ rrez, and A. Hadi. Sensitivity analysis in discrete bayesian networks. IEEE e Transactions on Systems, Man and Cybernetics, 26(7):412–423, 1997. E. Castillo, A. Cobo, J. M. Guti´ rrez, and R. E. Pruneda. Working with differential, functional and e difference equations using functional networks. Applied Mathematical Modelling, 23(2):89–107, 1999. E. Castillo, A. Cobo, J. M. Guti´ rrez, and R. E. Pruneda. Functional networks. a new neural network e based methodology. Computer-Aided Civil and Infrastructure Engineering, 15(2):90–106, 2000. E. Castillo, A. Conejo, P. Pedregal, R. Garc´a, and N. Alguacil. Building and Solving Mathematical ı Programming Models in Engineering and Science. John Wiley & Sons Inc., New York., 2001. E. Castillo, O. Fontenla-Romero, A. Alonso Betanzos, and B. Guijarro-Berdi˜ as. A global optimum n approach for one-layer neural networks. Neural Computation, 14(6):1429–1449, 2002. E. Castillo, A. S. Hadi, A. Conejo, and A. Fern´ ndez-Canteli. A general method for local sensitivity a analysis with application to regression models and other optimization problems. Technometrics, 46(4):430–445, 2004. E. Castillo, C. Castillo A. Conejo and, R. M´nguez, and D. Ortigosa. A perturbation approach to ı sensitivity analysis in nonlinear programming. Journal of Optimization Theory and Applications, 128(1):49–74, 2006. 1179  ˜ C ASTILLO , G UIJARRO -B ERDI NAS , F ONTENLA -ROMERO AND A LONSO -B ETANZOS  A. Chella, A. Gentile, F. Sorbello, and A. Tarantino. Supervised learning for feed-forward neural networks: a new minimax approach for fast convergence. Proceedings of the IEEE International Conference on Neural Networks, 1:605 – 609, 1993. V. Cherkassky and F. Mulier. Learning from Data: Concepts, Theory, and Methods. Wiley, New York, 1998. R. Collobert, Y. Bengio, and S. Bengio. Scaling large learning problems with hard parallel mixtures. International Journal of Pattern Recognition and Artiﬁcial Intelligence, 17(3):349–365, 2003. J. E. Dennis and R. B. Schnabel. Numerical Methods for Unconstrained Optimization and Nonlinear Equations. Prentice-Hall, Englewood Cliffs, NJ, 1983. G. P. Drago and S. Ridella. Statistically controlled activation weight initialization (SCAWI). IEEE Transactions on Neural Networks, 3:899–905, 1992. R. Fletcher and C. M. Reeves. Function minimization by conjugate gradients. Computer Journal, 7 (149–154), 1964. O. Fontenla-Romero, D. Erdogmus, J.C. Principe, A. Alonso-Betanzos, and E. Castillo. Linear least-squares based methods for neural networks learning. Lecture Notes in Computer Science, 2714(84–91), 2003. M. T. Hagan and M. Menhaj. Training feedforward networks with the marquardt algorithm. IEEE Transactions on Neural Networks, 5(6):989–993, 1994. M. T. Hagan, H. B. Demuth, and M. H. Beale. Neural Network Design. PWS Publishing, Boston, MA, 1996. M. Hollander and D. A. Wolfe. Nonparametric Statistical Methods. John Wiley & Sons, 1973. J. C. Hsu. Multiple Comparisons. Theory and Methods. Chapman&Hall;/CRC, Boca Raton, FL, 1996. D. R. Hush and J. M. Salas. Improving the learning rate of back-propagation with the gradient reuse algorithm. Proceedings of the IEEE Conference of Neural Networks, 1:441–447, 1988. B. C. Ihm and D. J. Park. Acceleration of learning speed in neural networks by reducing weight oscillations. Proceedings of the International Joint Conference on Neural Networks, 3:1729– 1732, 1999. R. A. Jacobs. Increased rates of convergence through learning rate adaptation. Neural Networks, 1 (4):295–308, 1988. Y. LeCun, I. Kanter, and S.A. Solla. Second order properties of error surfaces: Learning time and generalization. In R.P. Lippmann, J.E. Moody, and D.S. Touretzky, editors, Neural Information Processing Systems, volume 3, pages 918–924, San Mateo, CA, 1991. Morgan Kaufmann. Y. LeCun, L. Bottou, G.B. Orr, and K.-R. M¨ ller. Efﬁcient backprop. In G. B. Orr and K.-R. M¨ ller, u u editors, Neural Networks: Tricks of the trade, number 1524 in LNCS. Springer-Verlag, 1998. 1180  A V ERY FAST L EARNING M ETHOD FOR N EURAL N ETWORKS BASED ON S ENSITIVITY A NALYSIS  K. Levenberg. A method for the solution of certain non-linear problems in least squares. Quaterly Journal of Applied Mathematics, 2(2):164–168, 1944. E. Ley. On the peculiar distribution of the U.S. stock indeces’ ﬁrst digits. The American Statistician, 50(4):311–314, 1996. E. N. Lorenz. Deterministic nonperiodic ﬂow. Journal of the Atmospheric Sciences, 20:130–141, 1963. D. W. Marquardt. An algorithm for least-squares estimation of non-linear parameters. Journal of the Society of Industrial and Applied Mathematics, 11(2):431–441, 1963. M. F. Moller. A scaled conjugate gradient algorithm for fast supervised learning. Neural Networks, 6:525–533, 1993. D. Nguyen and B. Widrow. Improving the learning speed of 2-layer neural networks by choosing initial values of the adaptive weights. Proceedings of the International Joint Conference on Neural Networks, 3:21–26, 1990. G. B. Orr and T. K. Leen. Using curvature information for fast stochastic search. In M.I. Jordan, M.C. Mozer, and T. Petsche, editors, Neural Information Processing Systems, volume 9, pages 606–612, Cambridge, 1996. MIT Press. D. B. Parker. Optimal algorithms for adaptive networks: second order back propagation, second order direct propagation, and second order hebbian learning. Proceedings of the IEEE Conference on Neural Networks, 2:593–600, 1987. S. Pethel, C. Bowden, and M. Scalora. Characterization of optical instabilities and chaos using MLP training algorithms. SPIE Chaos Opt., 2039:129–140, 1993. M. J. D. Powell. Restart procedures for the conjugate gradient method. Mathematical Programming, 12:241–254, 1977. S. Ridella, S. Rovetta, and R. Zunino. Circular backpropagation networks for classiﬁcation. IEEE Transactions on Neural Networks, 8(1):84–97, January 1997. A. K. Rigler, J. M. Irvine, and T. P. Vogl. Rescaling of variables in back propagation learning. Neural Networks, 4:225–229, 1991. D. E. Rumelhart, G. E. Hinton, and R. J. Willian. Learning representations of back-propagation errors. Nature, 323:533–536, 1986. N. N. Schraudolph. Fast curvature matrix-vector products for second order gradient descent. Neural Computation, 14(7):1723–1738, 2002. A. Sperduti and S. Antonina. Speed up learning and network optimization with extended back propagation. Neural Networks, 6:365–383, 1993. J. A. K. Suykens and J. Vandewalle, editors. Nonlinear Modeling: advanced black-box techniques. Kluwer Academic Publishers Boston, 1998. 1181  ˜ C ASTILLO , G UIJARRO -B ERDI NAS , F ONTENLA -ROMERO AND A LONSO -B ETANZOS  T. Tollenaere. Supersab: Fast adaptive back propagation with good scaling properties. Neural Networks, 3(561–573), 1990. T. P. Vogl, J. K. Mangis, A. K. Rigler, W. T. Zink, and D. L. Alkon. Accelerating the convergence of back-propagation method. Biological Cybernetics, 59:257–263, 1988. M. K. Weir. A method for self-determination of adaptive learning rates in back propagation. Neural Networks, 4:371–379, 1991. B. M. Wilamowski, S. Iplikci, O. Kaynak, and M. O. Efe. An algorithm for fast convergence in training neural networks. Proceedings of the International Joint Conference on Neural Networks, 2:1778–1782, 2001. J. Y. F. Yam, T. W. S Chow, and C. T Leung. A new method in determining the initial weights of feedforward neural networks. Neurocomputing, 16(1):23–32, 1997.  1182</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
