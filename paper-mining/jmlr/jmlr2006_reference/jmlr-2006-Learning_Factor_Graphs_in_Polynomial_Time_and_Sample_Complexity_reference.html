<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-46" href="../jmlr2006/jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">jmlr2006-46</a> <a title="jmlr-2006-46-reference" href="#">jmlr2006-46-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</h1>
<br/><p>Source: <a title="jmlr-2006-46-pdf" href="http://jmlr.org/papers/volume7/abbeel06a/abbeel06a.pdf">pdf</a></p><p>Author: Pieter Abbeel, Daphne Koller, Andrew Y. Ng</p><p>Abstract: We study the computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Importantly, unlike standard maximum likelihood estimation algorithms, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. In addition to our main result, we show that the sample complexity of parameter learning in graphical models has an O(1) dependence on the number of variables in the model when using the KL-divergence normalized by the number of variables as the performance criterion.1 Keywords: probabilistic graphical models, parameter and structure learning, factor graphs, Markov networks, Bayesian networks</p><br/>
<h2>reference text</h2><p>P. Abbeel, D. Koller, and A. Y. Ng. Learning factor graphs in polynomial time & sample complexity. In Proc. UAI, 2005. N. Abe, J. Takeuchi, and M. Warmuth. Polynomial learnability of probabilistic concepts with respect to the Kullback-Leibler divergence. In Proc. COLT, 1991. N. Abe, J. Takeuchi, and M. Warmuth. On the computational complexity of approximating probability distributions by probabilistic automata. Machine Learning, 1992. F. Bach and M. Jordan. Thin junction trees. In NIPS 14, 2002. F. Barahona. On the computational complexity of Ising spin glass models. J. Phys. A, 1982. J. Besag. Efﬁciency of pseudo-likelihood estimation for simple Gaussian ﬁelds. Biometrika, 1974a. J. Besag. Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical Society, Series B, 1974b. J. Cheng, R. Greiner, J. Kelly, D. Bell, and W. Liu. Learning Bayesian networks from data: An information-theory based approach. Artiﬁcial Intelligence Journal, 2002. D. M. Chickering. Learning Bayesian networks is NP-Complete. In D. Fisher and H.J. Lenz, editors, Learning from Data: Artiﬁcial Intelligence and Statistics V, pages 121–130. SpringerVerlag, 1996. D. M. Chickering and C. Meek. Finding optimal Bayesian networks. In Proc. UAI, 2002. 1786  L EARNING FACTOR G RAPHS IN P OLYNOMIAL T IME AND S AMPLE C OMPLEXITY  D. M. Chickering, C. Meek, and D. Heckerman. Large-sample learning of Bayesian networks is hard. In Proc. UAI, 2003. C. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees. IEEE Transactions on Information Theory, 1968. F. Comets. On consistency of a class of estimators for exponential families of Markov random ﬁelds on the lattice. Annals of Statistics, 1992. T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley, 1991. R. G. Cowell, A. P. Dawid, S. L. Lauritzen, and D. J. Spiegelhalter. Probabilistic Networks and Expert Systems. Springer, 1999. S. Dasgupta. The sample complexity of learning ﬁxed structure Bayesian networks. Machine Learning, 1997. S. Dasgupta. Learning polytrees. In Proc. UAI, 1999. S. Della Pietra, V. J. Della Pietra, and J. D. Lafferty. Inducing features of random ﬁelds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393, 1997. A. Deshpande, M. N. Garofalakis, and M. I. Jordan. Efﬁcient stepwise selection in decomposable models. In Proc. UAI, pages 128–135. Morgan Kaufmann Publishers Inc., 2001. ISBN 1-55860800-1. N. Friedman and Z. Yakhini. On the sample complexity of learning Bayesian networks. In Proc. UAI, 1996. S. Geman and C. Grafﬁgne. Markov random ﬁeld image models and their applications to computer vision. In Proc. of the International Congress of Mathematicians, 1986. C. J. Geyer and E. A. Thompson. Constrained Monte Carlo maximum likelihood for dependent data. Journal of the Royal Statistical Society, Series B, 1992. B. Gidas. Consistency of maximum likelihood and pseudo-likelihood estimators for Gibbsian distributions. In W. Fleming and P.-L. Lions, editors, Stochastic differential systems, stochastic control theory and applications. Springer, New York, 1988. X. Guyon and H. R. K¨ nsch. Asymptotic comparison of estimators in the Ising model. In Stochasu tic Models, Statistical Methods, and Algorithms in Image Analysis, Lecture Notes in Statistics. Springer, Berlin, 1992. J. M. Hammersley and P. Clifford. Markov ﬁelds on ﬁnite graphs and lattices. Unpublished, 1971. K. L. H¨ ffgen. Learning and robust learning of product distributions. In Proc. COLT, 1993. o F. Huang and Y. Ogata. Generalized pseudo-likelihood estimates for Markov random ﬁelds on lattice. Annals of the Institute of Statistical Mathematics, 2002. M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for the Ising model. SIAM J. Comput., 1993. 1787  A BBEEL , KOLLER AND N G  C. Ji and L. Seymour. A consistent model selection procedure for Markov random ﬁelds based on penalized pseudolikelihood. Annals of Applied Probability, 1996. D. Karger and N. Srebro. Learning Markov networks: maximum bounded tree-width graphs. In Symposium on Discrete Algorithms, pages 392–401, 2001. F. R. Kschischang, B. J. Frey, and H. A. Loeliger. Factor graphs and the sum-product algorithm. IEEE Transactions on Information Theory, 2001. S. Kullback. Probability theory and statistics. Wiley, 1959. S. L. Lauritzen. Graphical Models. Oxford University Press, 1996. F. M. Malvestuto. Approximating discrete probability distributions with decomposable models. IEEE Transactions on Systems, Man and Cybernetics, 1991. A. McCallum. Efﬁciently inducing features of conditional random ﬁelds. In Proc. UAI, 2003. C. Meek. Finding a path is harder than ﬁnding a tree. Journal of Artiﬁcial Intelligence Research, 15:383–389, 2001. M. Narasimhan and J. Bilmes. PAC-learning bounded tree-width graphical models. In Proc. UAI, 2004. A. Y. Ng and M. I. Jordan. On discriminative vs. generative classiﬁers: A comparison of logistic regression and naive Bayes. In NIPS 14, 2002. P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search (second edition). MIT Press, 2000. N. Srebro. Maximum likelihood bounded tree-width Markov networks. In Proc. UAI, 2001. V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons, 1998. J. S. Yedidia, W. T. Freeman, and Y. Weiss. Understanding belief propagation and its generalizations. Technical report, Mitsubishi Electric Reseach Laboratories, 2001.  1788</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
