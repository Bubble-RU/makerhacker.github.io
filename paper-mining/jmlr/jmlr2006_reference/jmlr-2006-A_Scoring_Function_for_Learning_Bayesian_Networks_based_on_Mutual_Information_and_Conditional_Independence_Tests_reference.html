<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-6" href="../jmlr2006/jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">jmlr2006-6</a> <a title="jmlr-2006-6-reference" href="#">jmlr2006-6-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</h1>
<br/><p>Source: <a title="jmlr-2006-6-pdf" href="http://jmlr.org/papers/volume7/decampos06a/decampos06a.pdf">pdf</a></p><p>Author: Luis M. de Campos</p><p>Abstract: We propose a new scoring function for learning Bayesian networks from data using score+search algorithms. This is based on the concept of mutual information and exploits some well-known properties of this measure in a novel way. Essentially, a statistical independence test based on the chi-square distribution, associated with the mutual information measure, together with a property of additive decomposition of this measure, are combined in order to measure the degree of interaction between each variable and its parent variables in the network. The result is a non-Bayesian scoring function called MIT (mutual information tests) which belongs to the family of scores based on information theory. The MIT score also represents a penalization of the Kullback-Leibler divergence between the joint probability distributions associated with a candidate network and with the available data set. Detailed results of a complete experimental evaluation of the proposed scoring function and its comparison with the well-known K2, BDeu and BIC/MDL scores are also presented. Keywords: Bayesian networks, scoring functions, learning, mutual information, conditional independence tests</p><br/>
<h2>reference text</h2><p>B. Abramson, J. Brown, A. Murphy, and R. L. Winkler. Hailﬁnder: A Bayesian system for forecasting severe weather. International Journal of Forecasting, 12:57–71, 1996. S. Acid and L. M. de Campos. Learning right sized belief networks by means of a hybrid methodology. Lecture Notes in Artiﬁcial Intelligence, 1910:309–315, 2000. S. Acid and L. M. de Campos. A hybrid methodology for learning belief networks: Benedict. International Journal of Approximate Reasoning, 27:235–262, 2001. S. Acid and L. M. de Campos. Searching for Bayesian network structures in the space of restricted acyclic partially directed graphs. Journal of Artiﬁcial Intelligence Research, 18:445–490, 2003. S. Acid, L. M. de Campos, and J. G. Castellano. Learning Bayesian network classiﬁers: searching in a space of partially directed acyclic graphs. Machine Learning, 59:213–235, 2005. H. Akaike. A new look at the statistical model identiﬁcation. IEEE Transactions on Automatic Control, 19:716–723, 1974. S. Andersson, D. Madigan, and M. Perlman. A Characterization of Markov equivalence classes for acyclic digraphs. Annals of Statistics, 25:505–541, 1997. I. A. Beinlich, H. J. Suermondt, R. M. Chavez, and G. F. Cooper. The alarm monitoring system: A case study with two probabilistic inference techniques for belief networks. In Proceedings of the European Conference on Artiﬁcial Intelligence in Medicine, pages 247–256, 1989. J. Binder, D. Koller, S. Russell, and K. Kanazawa. Adaptive probabilistic networks with hidden variables. Machine Learning, 29:213–244, 1997. C. L. Blake and C. J. Merz. UCI Repository of machine learning databases. http://www.ics.uci.edu/∼mlearn/MLRepository.html, University of California, Irvine, Dept. of Information and Computer Sciences, 1998. 2182  S CORING BAYESIAN N ETWORKS USING M UTUAL I NFORMATION AND I NDEPENDENCE T ESTS  R. Blanco, I. Inza, and P. Larrañaga. Learning Bayesian networks in the space of structures by estimation of distribution algorithms. International Journal of Intelligent Systems, 18:205–220, 2003. R. R. Bouckaert. Belief networks construction using the minimum description length principle. Lecture Notes in Computer Science, 747:41–48, 1993. R. R. Bouckaert. Bayesian Belief Networks: from Construction to Inference. PhD thesis, University of Utrecht, 1995. W. Buntine. Theory reﬁnement of Bayesian networks. In Proceedings of the Seventh Conference on Uncertainty in Artiﬁcial Intelligence, pages 52–60, 1991. J. Cheng, R. Greiner, J. Kelly, D. A. Bell, and W. Liu. Learning Bayesian networks from data: an information-theory based approach. Artiﬁcial Intelligence, 137:43–90, 2002. J. Cheng and R. Greiner. Comparing Bayesian network classiﬁers. In Proceedings of the Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 101–108, 1999. D. M. Chickering. A transformational characterization of equivalent Bayesian network structures. In Proceedings of the Eleventh Conference on Uncertainty in Artiﬁcial Intelligence, pages 87–98, 1995. D. M. Chickering. Learning equivalence classes of Bayesian network structures. Journal of Machine Learning Research, 2:445–498, 2002. D. M. Chickering, D. Geiger, and D. Heckerman. Learning Bayesian networks: Search methods and experimental results. In Preliminary Papers of the Fifth International Workshop on Artiﬁcial Intelligence and Statistics, pages 112–128, 1995. C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. IEEE Transactions on Information Theory, 14:462–467, 1968. G. F. Cooper and E. Herskovits. A Bayesian method for the induction of probabilistic networks from data. Machine Learning, 9:309–348, 1992. D. Dash and M. Druzdzel. A hybrid anytime algorithm for the construction of causal models from sparse data. In Proceedings of the Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 142–149, 1999. L. M. de Campos. Independency relationships and learning algorithms for singly connected networks. Journal of Experimental and Theoretical Artiﬁcial Intelligence, 10:511–549, 1998. L. M. de Campos, J. M. Fernández-Luna, J. A. Gámez, and J. M. Puerta. Ant colony optimization for learning Bayesian networks. International Journal of Approximate Reasoning, 31:291–311, 2002. L. M. de Campos, J. M. Fernández-Luna, and J. M. Puerta. Local search methods for learning Bayesian networks using a modiﬁed neighborhood in the space of dags. Lecture Notes in Computer Science, 2527:182–192, 2002. 2183  DE  C AMPOS  L. M. de Campos, J. M. Fernández-Luna, and J. M. Puerta. An iterated local search algorithm for learning Bayesian networks with restarts based on conditional independence tests. International Journal of Intelligent Systems, 18:221–235, 2003. L. M. de Campos, J. A. Gámez, and J. M. Puerta. Learning Bayesian networks by ant colony optimization: Searching in two different spaces. Mathware and Soft Computing, IX:251–268, 2002. L. M. de Campos and J. F. Huete. A new approach for learning belief networks using independence criteria. International Journal of Approximate Reasoning, 24:11–37, 2000. L. M. de Campos and J. F. Huete. Stochastic algorithms for searching causal orderings in Bayesian networks. In Technologies for Constructing Intelligent Systems 2 - Tools, B. Bouchon-Menieur, J. Gutiérrez-Rios, L. Magdalena, R.R. Yager (Eds.), Physica-Verlag, pages 327–340, 2002. L. M. de Campos and J. M. Puerta. Stochastic local and distributed search algorithms for learning belief networks. In Proceedings of the III International Symposium on Adaptive Systems: Evolutionary Computation and Probabilistic Graphical Model, pages 109–115, 2001. L. M. de Campos and J. M. Puerta. Stochastic local search algorithms for learning belief networks: Searching in the space of orderings. Lecture Notes in Artiﬁcial Intelligence, 2143:228–239, 2001. Elvira Consortium. Elvira: An environment for probabilistic graphical models. In Proceedings of the First European Workshop on Probabilistic Graphical Models, pages 222–230, 2002. Available at http://www.leo.ugr.es/∼elvira. M. Evans, N. Hastings, and B. Peacock. Statistical Distributions, Second edition. Wiley, 1993. K. Ezawa, M. Singh, and S. Norton. Learning goal oriented Bayesian networks for telecommunications risk management. In Proceedings of the Thirteenth International Conference on Machine Learning, pages 139–147, 1996. U. M. Fayyad and K. B. Irani. Multi-valued interval discretization of continuous-valued attributes for classiﬁcation learning. In Proceedings of the 13th International Joint Conference on Artiﬁcial Intelligence, pages 1022–1027, 1993. N. Friedman and M. Goldszmidt. Learning Bayesian networks with local structure. In Proceedings of the Twelfth Conference on Uncertainty in Artiﬁcial Intelligence, pages 252–262, 1996. N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian network classiﬁers. Machine Learning, 29:131–163, 1997. N. Friedman and D. Koller. Being Bayesian about network structure: A Bayesian approach to structure discovery in Bayesian networks. Machine Learning, 50:95–126, 2003. I. J. Good. The Estimation of Probabilities. MIT Press, 1965. D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20:197–243, 1995. 2184  S CORING BAYESIAN N ETWORKS USING M UTUAL I NFORMATION AND I NDEPENDENCE T ESTS  E. Herskovits and G. F. Cooper. Kutató: An entropy-driven system for the construction of probabilistic expert systems from databases. In Proceedings of the Sixth Conference on Uncertainty in Artiﬁcial Intelligence, pages 54–62, 1990. I. D. Hill and M. C. Pike. Algorithm 299: Chi-squared integral. Communications of the ACM, 10:243–244, 1965. I. D. Hill and M. C. Pike. Remark on algorithm 299: Chi-squared integral. ACM Transactions on Mathematical Software, 11:185–185, 1985. R. V. Hogg and A. T. Craig. Introduction to Mathematical Statistics, 5th Edition. Prentice Hall, New York, 1994. F. V. Jensen. An Introduction to Bayesian Networks. UCL Press, 1996. M. Kayaalp and G. F. Cooper. A Bayesian network scoring metric that is based on globally uniform parameter priors. In Proceedings of the Eighteenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 251–258, 2002. T. Kocka and R. Castelo. Improved learning of Bayesian networks. In Proceedings of the Seventeenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 269–276, 2001. R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proceedings of the Fourteenth International Joint Conference on Artiﬁcial Intelligence, pages 1137–1143, 1995. R. Kohavi and G. H. John. Wrappers for feature subset selection. Artiﬁcial Intelligence, 97:273– 324, 1997. R. Kohavi, G. John, R. Long, D. Manley, and K. Pﬂeger. MLC++: A machine learning library in C++. In Proceedings of the Sixth International Conference on Tools with Artiﬁcial Intelligence, pages 740–743, 1994. S. Kullback. Information Theory and Statistics. Dover Publication, 1968. W. Lam and F. Bacchus. Learning Bayesian belief networks. An approach based on the MDL principle. Computational Intelligence, 10:269–293, 1994. P. Larrañaga, M. Poza, Y. Yurramendi, R. Murga, and C. Kuijpers. Structure learning of Bayesian networks by genetic algorithms: A performance analysis of control parameters. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18:912–926, 1996. P. Larrañaga, C. Kuijpers, and R. Murga. Learning Bayesian network structures by searching for the best ordering with genetic algorithms. IEEE Transactions on System, Man and Cybernetics, 26:487–493, 1996. D. Madigan, S. A. Andersson, M. D. Perlman, and C. T. Volinsky. Bayesian model averaging and model selection for Markov equivalence classes of acyclic digraphs. Communications in Statistics – Theory and Methods, 25:2493–2520, 1996. 2185  DE  C AMPOS  A. W. Marshall and I. Olkin. Inequalities: Theory of Majorization and Its Applications. Academic Press, New York, 1979. C. Meek. Causal inference and causal explanation with background knowledge. In Proceedings of the Eleventh Conference on Uncertainty in Artiﬁcial Intelligence, pages 403–410, 1995. J. W. Myers, K. B. Laskey, and T. Levitt. Learning Bayesian networks from incomplete data with stochastic search algorithms. In Proceedings of the Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 476–485, 1999. J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, 1988. J. Pearl and T. S. Verma. Equivalence and synthesis of causal models. In Proceedings of the Sixth Conference on Uncertainty in Artiﬁcial Intelligence, pages 220–227, 1990. J. Pearl and T. S. Verma. A theory of inferred causation. In Proceedings of the Second International Conference on Principles of Knowledge Representation and Reasoning, pages 441–452, 1991. L. K. Rasmussen. Bayesian network for blood typing and parentage veriﬁcation of cattle. PhD thesis, Research Centre Foulum, Denmark, 1995. J. Rissanen. Stochastic complexity and modeling. Annals of Statistics, 14:1080–1100, 1986. M. Sahami. Learning limited dependence Bayesian classiﬁers. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, pages 335–338, 1996. G. Schwarz. Estimating the dimension of a model. Annals of Statistics, 6:461–464, 1978. M. Singh and M. Valtorta. Construction of Bayesian network structures from data: A brief survey and an efﬁcient algorithm. International Journal of Approximate Reasoning, 12:111–131, 1995. P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction and Search. Lecture Notes in Statistics 81, Springer Verlag, New York, 1993. P. Spirtes and C. Meek. Learning Bayesian networks with discrete variables from data. In Proceedings of the First International Conference on Knowledge Discovery and Data Mining, pages 294–299, 1995. J. Suzuki. A construction of Bayesian networks from databases based on the MDL principle. In Proceedings of the Ninth Conference on Uncertainty in Artiﬁcial Intelligence, pages 266–273, 1993. J. Tian. A branch-and-bound algorithm for MDL learning Bayesian networks. In Proceedings of the Sixteenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 580–587, 2000. T. Verma and J. Pearl. Causal networks: Semantics and expressiveness. In Uncertainty in Artiﬁcial Intelligence, 4, R.D. Shachter, T.S. Lewitt, L.N. Kanal, J.F. Lemmer (Eds.), North-Holland, Amsterdam, pages 69–76, 1990. 2186  S CORING BAYESIAN N ETWORKS USING M UTUAL I NFORMATION AND I NDEPENDENCE T ESTS  N. Wermuth and S. Lauritzen. Graphical and recursive models for contingence tables. Biometrika, 72:537–552, 1983. M. L. Wong, W. Lam, and K. S. Leung. Using evolutionary computation and minimum description length principle for data mining of probabilistic knowledge. IEEE Transactions on Pattern Analysis and Machine Intelligence, 21:174–178, 1999. S. Yang and K. Chang. Comparison of score metrics for Bayesian network learning. IEEE Transactions on System, Man and Cybernetics–Part A: Systems and Humans, 32:419–428, 2002.  2187</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
