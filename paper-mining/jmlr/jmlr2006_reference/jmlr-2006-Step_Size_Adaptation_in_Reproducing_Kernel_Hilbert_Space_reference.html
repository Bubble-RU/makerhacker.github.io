<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-86" href="../jmlr2006/jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">jmlr2006-86</a> <a title="jmlr-2006-86-reference" href="#">jmlr2006-86-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</h1>
<br/><p>Source: <a title="jmlr-2006-86-pdf" href="http://jmlr.org/papers/volume7/schraudolph06a/schraudolph06a.pdf">pdf</a></p><p>Author: S. V. N. Vishwanathan, Nicol N. Schraudolph, Alex J. Smola</p><p>Abstract: This paper presents an online support vector machine (SVM) that uses the stochastic meta-descent (SMD) algorithm to adapt its step size automatically. We formulate the online learning problem as a stochastic gradient descent in reproducing kernel Hilbert space (RKHS) and translate SMD to the nonparametric setting, where its gradient trace parameter is no longer a coefﬁcient vector but an element of the RKHS. We derive efﬁcient updates that allow us to perform the step size adaptation in linear time. We apply the online SVM framework to a variety of loss functions, and in particular show how to handle structured output spaces and achieve efﬁcient online multiclass classiﬁcation. Experiments show that our algorithm outperforms more primitive methods for setting the gradient step size. Keywords: online SVM, stochastic meta-descent, structured output spaces</p><br/>
<h2>reference text</h2><p>L. B. Almeida, T. Langlois, J. D. Amaral, and A. Plakhov. Parameter adaptation in stochastic optimization. In David Saad, editor, On-Line Learning in Neural Networks, Publications of the Newton Institute, chapter 6, pages 111–134. Cambridge University Press, 1999. Y. Altun, A. J. Smola, and T. Hofmann. Exponential families for conditional random ﬁelds. In Uncertainty in Artiﬁcial Intelligence (UAI), pages 2–9, 2004. K. Azoury and M. K. Warmuth. Relative loss bounds for on-line density estimation with the exponential family of distributions. Machine Learning, 43(3):211–246, 2001. Special issue on Theoretical Advances in On-line Learning, Game Theory and Boosting. A. G. Barto and R. S. Sutton. Goal seeking components for adaptive intelligence: An initial assessment. Technical Report AFWAL-TR-81-1070, Air Force Wright Aeronautical Laboratories, Wright-Patterson AFB, Ohio 45433, USA, 1981. K. P. Bennett and O. L. Mangasarian. Robust linear programming discrimination of two linearly inseparable sets. Optimization Methods and Software, 1:23–34, 1992. 1129  V ISHWANATHAN , S CHRAUDOLPH AND S MOLA  H. D. Block. The perceptron: A model for brain functioning. Reviews of Modern Physics, 34: 123–135, 1962. Reprinted in Neurocomputing by Anderson and Rosenfeld. A. Bordes, S. Ertekin, J. Weston, and L. Bottou. Fast kernel classiﬁers with online and active learning. Journal of Machine Learning Research, 6:1579–1619, September 2005. M. Bray, E. Koller-Meier, P. M¨ ller, N. N. Schraudolph, and L. Van Gool. Stochastic optimization u for high-dimensional tracking in dense range maps. IEE Proceedings Vision, Image & Signal Processing, 152(4):501–512, 2005. M. Bray, E. Koller-Meier, N. N. Schraudolph, and L. Van Gool. Fast stochastic optimization for articulated structure tracking. Image and Vision Computing, 24, in press 2006. C. J. C. Burges and B. Sch¨ lkopf. Improving the accuracy and speed of support vector learning o machines. In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems 9, pages 375–381, Cambridge, MA, 1997. MIT Press. L. Cai and T. Hofmann. Hierarchical document categorization with support vector machines. In Proceedings of the Thirteenth ACM conference on Information and knowledge management, pages 78–87, New York, NY, USA, 2004. ACM Press. C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20(3):273–297, 1995. K. Crammer and Y. Singer. On the learnability and design of output codes for multiclass problems. In N. Cesa-Bianchi and S. Goldman, editors, Proc. Annual Conf. Computational Learning Theory, pages 35–46, San Francisco, CA, 2000. Morgan Kaufmann Publishers. K. Crammer and Y. Singer. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951–991, January 2003. K. Crammer, J. Kandola, and Y. Singer. Online classiﬁcation on a budget. In Sebastian Thrun, Lawrence Saul, and Bernhard Sch¨ lkopf, editors, Advances in Neural Information Processing o Systems 16, pages 225–232, Cambridge, MA, 2004. MIT Press. O. Dekel, S. Shalev-Shwartz, and Y. Singer. The Forgetron: A kernel-based perceptron on a ﬁxed budget. In Yair Weiss, Bernhard Sch¨ lkopf, and John Platt, editors, Advances in Neural Informao tion Processing Systems 18, Cambridge, MA, 2006. MIT Press. Y. Freund and R. E. Schapire. Large margin classiﬁcation using the perceptron algorithm. Machine Learning, 37(3):277–296, 1999. T.-T. Frieß, N. Cristianini, and C. Campbell. The kernel adatron algorithm: A fast and simple learning procedure for support vector machines. In J. Shavlik, editor, Proc. Intl. Conf. Machine Learning, pages 188–196. Morgan Kaufmann Publishers, 1998. C. Gentile. A new approximate maximal margin classiﬁcation algorithm. Journal of Machine Learning Research, 2:213–242, December 2001. C. Gentile and N. Littlestone. The robustness of the p-norm algorithms. In Proc. Annual Conf. Computational Learning Theory, pages 1–11, Santa Cruz, California, United States, 1999. ACM Press, New York, NY. 1130  S TEP S IZE A DAPTATION IN RKHS  A. Griewank. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. Frontiers in Applied Mathematics. SIAM, Philadelphia, 2000. M. E. Harmon and L. C. Baird, III. Multi-player residual advantage learning with general function approximation. Technical Report WL-TR-1065, Wright Laboratory, WL/AACF, WrightPatterson Air Force Base, OH 45433-7308, 1996. http://www.leemon.com/papers/sim tech/sim tech.pdf. D. Helmbold and M. K. Warmuth. On weak learning. Journal of Computer and System Sciences, 50(3):551–573, June 1995. M. Herbster. Learning additive models online with fast evaluating kernels. In D. P. Helmbold and R. C. Williamson, editors, Proc. Annual Conf. Computational Learning Theory, volume 2111 of Lecture Notes in Computer Science, pages 444–460. Springer, 2001. R. A. Jacobs. Increased rates of convergence through learning rate adaptation. Neural Networks, 1: 295–307, 1988. K. I. Kim, M. O. Franz, and B. Sch¨ lkopf. Iterative kernel principal component analysis for image o modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(9):1351–1366, 2005. J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1–64, January 1997. J. Kivinen, A. J. Smola, and R. C. Williamson. Online learning with kernels. IEEE Transactions on Signal Processing, 52(8), Aug 2004. Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. J. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1:541–551, 1989. Y. Li and P. M. Long. The relaxed online maximum margin algorithm. Machine Learning, 46(1–3): 361–387, 2002. D. J. C. MacKay. Introduction to Gaussian processes. In C. M. Bishop, editor, Neural Networks and Machine Learning, pages 133–165. Springer, Berlin, 1998. M. Milano. Machine Learning Techniques for Flow Modeling and Control. PhD thesis, Eidgen¨ so sische Technische Hochschule (ETH), Z¨ rich, Switzerland, 2002. u M. Minsky and S. Papert. Perceptrons: An Introduction To Computational Geometry. MIT Press, Cambridge, MA, 1969. A. B. J. Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on the Mathematical Theory of Automata, volume 12, pages 615–622. Polytechnic Institute of Brooklyn, 1962. B. A. Pearlmutter. Fast exact multiplication by the Hessian. Neural Computation, 6(1):147–160, 1994. 1131  V ISHWANATHAN , S CHRAUDOLPH AND S MOLA  M. Riedmiller and H. Braun. A direct adaptive method for faster backpropagation learning: The RPROP algorithm. In Proc. International Conference on Neural Networks, pages 586–591, San Francisco, CA, 1993. IEEE, New York. F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6):386–408, 1958. B. Sch¨ lkopf and A. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002. o B. Sch¨ lkopf, J. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Estimating the support o of a high-dimensional distribution. Neural Computation, 13(7):1443–1471, 2001. N. N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural Computation, 14(7):1723–1738, 2002. N. N. Schraudolph. Local gain adaptation in stochastic gradient descent. In Proc. Intl. Conf. Artiﬁcial Neural Networks, pages 569–574, Edinburgh, Scotland, 1999. IEE, London. N. N. Schraudolph and X. Giannakopoulos. Online independent component analysis with local learning rate adaptation. In S. A. Solla, T. K. Leen, and K.-R. M¨ ller, editors, Neural Information u Processing Systems, volume 12, pages 789–795, Vancouver, Canada, 2000. MIT Press. N. N. Schraudolph, J. Yu, and D. Aberdeen. Fast online policy gradient learning with SMD gain vector adaptation. In Yair Weiss, Bernhard Sch¨ lkopf, and John Platt, editors, Advances in Neural o Information Processing Systems 18, Cambridge, MA, 2006. MIT Press. S. Shalev-Shwartz and Y. Singer. A new perspective on an old perceptron algorithm. In P. Auer and R. Meir, editors, Proc. Annual Conf. Computational Learning Theory, number 3559 in Lecture Notes in Artiﬁcial Intelligence, pages 264 – 279, Bertinoro, Italy, June 2005. Springer-Verlag. F. M. Silva and L. B. Almeida. Acceleration techniques for the backpropagation algorithm. In Lu´s B. Almeida and C. J. Wellekens, editors, Neural Networks: Proc. EURASIP Workshop, ı volume 412 of Lecture Notes in Computer Science, pages 110–119. Springer Verlag, 1990. R. S. Sutton. Adaptation of learning rate parameters, 1981. URL http://www.cs.ualberta.ca/ ∼sutton/papers/sutton-81.pdf. Appendix C of (Barto and Sutton, 1981). R. S. Sutton. Gain adaptation beats least squares? In Proceedings of the 7th Yale Workshop on Adaptive and Learning Systems, pages 161–166, 1992. URL http://www.cs.ualberta.ca/ ∼sutton/papers/sutton-92b.pdf. T. Tollenaere. SuperSAB: Fast adaptive back propagation with good scaling properties. Neural Networks, 3:561–573, 1990. I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdependent and structured output spaces. In Proc. Intl. Conf. Machine Learning, New York, NY, USA, 2004. ACM Press. ISBN 1-58113-828-5. S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark Schmidt, and Kevin Murphy. Training conditional random ﬁelds with stochastic gradient methods. In Proc. Intl. Conf. Machine Learning, to appear 2006. 1132  S TEP S IZE A DAPTATION IN RKHS  J. Weston, A. Bordes, and L. Bottou. Online (and ofﬂine) on an even tighter budget. In Proceedings of International Workshop on Artiﬁcial Intelligence and Statistics, 2005.  1133</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
