<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-38" href="../jmlr2006/jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">jmlr2006-38</a> <a title="jmlr-2006-38-reference" href="#">jmlr2006-38-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-38-pdf" href="http://jmlr.org/papers/volume7/laskov06a/laskov06a.pdf">pdf</a></p><p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><br/>
<h2>reference text</h2><p>D. Angluin. Queries and concept learning. Machine Learning, 2:319–342, 1988. C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995. B.. Blankertz, G. Dornhege, C. Sch¨ fer, R. Krepki, J. Kohlmorgen, K.-R. M¨ ller, V. Kunzmann, a u F. Losch, and G. Curio. BCI bit rates and error detection for fast-pace motor commands based on single-trial EEG analysis. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 11:127–131, 2003. A. Bordes, S. Ertekin, J. Wesdon, and L. Bottou. Fast kernel classiﬁers for online and active learning. Journal of Machine Learning Research, 6:1579–1619, 2005. G. Cauwenberghs and T. Poggio. Incremental and decremental support vector machine learning. In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems, volume 13, pages 409–415. MIT Press, 2001. S. Chakrabarti. Mining the Web: Discovering Knowledge from Hypertext Data. Morgan-Kaufmann, 2002. ISBN 1-55860-754-4. C.-C. Chang and C.-J. Lin. Libsvm: Introduction and benchmarks. Technical report, Department of Computer Science and Information Engineering, National Taiwan University, Taipei, 2000. R. Collobert and S. Bengio. SVMTorch: Support vector machines for large-scale regression problems. Journal of Machine Learning Research, 1:143–160, 2001. C. Cortes and M. Mohri. AUC optimization vs. error rate minimization. In Proc. NIPS’2003, 2004. E. Eskin, A. Arnold, M. Prerau, L. Portnoy, and S. Stolfo. Applications of Data Mining in Computer Security, chapter A geometric framework for unsupervised anomaly detection: detecting intrusions in unlabeled data. Kluwer, 2002. G. H. Golub and C. F. van Loan. Matrix Computations. John Hopkins University Press, Baltimore, London, 3rd edition, 1996. T. Joachims. Making large–scale SVM learning practical. In B. Sch¨ lkopf, C. J. C. Burges, and o A. J. Smola, editors, Advances in Kernel Methods — Support Vector Learning, pages 169–184, Cambridge, MA, 1999. MIT Press. 1934  I NCREMENTAL S UPPORT V ECTOR L EARNING  J. Kivinen, A. J. Smola, and R. C. Williamson. Online learning with kernels. In T. G. Diettrich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Inf. Proc. Systems (NIPS 01), pages 785–792, 2001. F. Klee and G. J. Minty. How good is the simplex algorithm? In O. Sisha, editor, Inequalities III, pages 159–175. Academic Press, 1972. P. Laskov. Feasible direction decomposition algorithms for training support vector machines. Machine Learning, 46:315–349, 2002. P. Laskov, C. Sch¨ fer, and I. Kotenko. Intrusion detection in unlabeled data with quarter-sphere a support vector machines. In Proc. DIMVA, pages 71–82, 2004. Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨ ller. Efﬁcient backprop. In G. Orr and K.-R. M¨ ller, u u editors, Neural Networks: Tricks of the Trade, volume 1524, pages 9–53, Heidelberg, New York, 1998. Springer LNCS. N. Littlestone, P. M. Long, and M. K. Warmuth. On-line learning of linear functions. Technical Report CRL-91-29, University of California at Santa Cruz, October 1991. J. Ma and S. Perkins. Time-series novelty detection using one-class Support Vector Machines. In IJCNN, 2003. to appear. J. Ma, J. Theiler, and S. Perkins. Accurate online support vector regression. www.lanl.gov/˜jt/Papers/aosvr.pdf, 2003.  http://nis-  M. Martin. On-line Support Vector Machines for function approximation. Technical report, Universitat Polit` cnica de Catalunya, Departament de Llengatges i Sistemes Inform` tics, 2002. e a N. Murata. A statistical study on the asymptotic theory of learning. PhD thesis, University of Tokyo (In Japanese), 1992. N. Murata, M. Kawanabe, A. Ziehe, K.-R. M¨ ller, and S.-I. Amari. On-line learning in changing u environments with applications in supervised and unsupervised learning. Neural Networks, 15 (4-6):743–760, 2002. N. Murata, K.-R. M¨ ller, A. Ziehe, and S. i. Amari. Adaptive on-line learning in changing environu ments. In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems, volume 9, page 599. The MIT Press, 1997. G. Orr and K.-R. M¨ ller, editors. Neural Networks: Tricks of the Trade, volume 1524. Springer u LNCS, 1998. J. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Sch¨ lkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods — Supo port Vector Learning, pages 185–208, Cambridge, MA, 1999. MIT Press. L. Ralaivola and F. d’Alch´ Buc. Incremental support vector machine learning: A local approach. e Lecture Notes in Computer Science, 2130:322–329, 2001. 1935  ¨ ¨ L ASKOV, G EHL , K R UGER AND M ULLER  H. Robbins and S. Munro. A stochastic approximation method. Ann. Math. Stat., 22:400–407, 1951. S. R¨ ping. Incremental learning with support vector machines. Technical Report TR-18, Universit¨ t u a Dortmund, SFB475, 2002. D. Saad, editor. On-line learning in neural networks. Cambridge University Press, 1998. B. Sch¨ lkopf, S. Mika, C. J. C. Burges, P. Knirsch, K.-R. M¨ ller, G. R¨ tsch, and A. J. Smola. Input o u a space vs. feature space in kernel-based methods. IEEE Transactions on Neural Networks, 10(5): 1000–1017, September 1999. B. Sch¨ lkopf, J. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Estimating the support o of a high-dimensional distribution. Neural Computation, 13(7):1443–1471, 2001. N. A. Syed, H. Liu, and K. K. Sung. Incremental learning with support vector machines. In SVM workshop, IJCAI, 1999. D. Tax and R. Duin. Data domain description by support vectors. In M. Verleysen, editor, Proc. ESANN, pages 251–256, Brussels, 1999. D. Facto Press. D. M. J. Tax and P. Laskov. Online SVM learning: from classiﬁcation to data description and back. In C. et al. Molina, editor, Proc. NNSP, pages 499–508, 2003. I. Tsang, J. Kwok, and P.-M. Cheung. Core Vector Machines: fast SVM training on very large data sets. Journal of Machine Learning Research, 6:363–392, 2005. V. N. Vapnik. Statistical Learning Theory. Wiley, New York, 1998. M. K. Warmuth, J. Liao, G. R¨ tsch, M. Mathieson, S. Putta, and C. Lemmem. Support Vector a Machines for active learning in the drug discovery process. Journal of Chemical Information Sciences, 43(2):667–673, 2003.  1936</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
