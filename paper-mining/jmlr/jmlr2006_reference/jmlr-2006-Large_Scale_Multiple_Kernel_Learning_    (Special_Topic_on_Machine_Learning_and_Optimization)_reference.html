<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-43" href="../jmlr2006/jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">jmlr2006-43</a> <a title="jmlr-2006-43-reference" href="#">jmlr2006-43-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-43-pdf" href="http://jmlr.org/papers/volume7/sonnenburg06a/sonnenburg06a.pdf">pdf</a></p><p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer, Bernhard Schölkopf</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun. Keywords: multiple kernel learning, string kernels, large scale optimization, support vector machines, support vector regression, column generation, semi-inﬁnite linear programming</p><br/>
<h2>reference text</h2><p>F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In C. E. Brodley, editor, Twenty-ﬁrst international conference on Machine learning. ACM, 2004. 1563  S ONNENBURG , R ÄTSCH , S CHÄFER AND S CHÖLKOPF  K. P. Bennett, M. Momma, and M. J. Embrechts. MARK: a boosting algorithm for heterogeneous kernel models. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 24–31. ACM, 2002. J. Bi, T. Zhang, and K. P. Bennett. Column-generation boosting methods for mixture of kernels. In W. Kim, R. Kohavi, J. Gehrke, and W. DuMouchel, editors, Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 521–526. ACM, 2004. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, UK, 2004. O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector machines. Machine Learning, 46(1–3):131–159, 2002. C. Cortes and V.N. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995. J. Davis and M. Goadrich. The relationship between precision-recall and roc curves. Technical report #1551, University of Wisconsin Madison, January 2006. T. Fawcett. Roc graphs: Notes and practical considerations for data mining researchers. Technical report hpl-2003-4, HP Laboratories, Palo Alto, CA, USA, January 2003. E. Fredkin. Trie memory. Communications of the ACM, 3(9):490–499, 1960. Y. Grandvalet and S. Canu. Adaptive scaling for feature selection in SVMs. In S. Thrun S. Becker and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 553– 560, Cambridge, MA, 2003. MIT Press. R. Hettich and K. O. Kortanek. Semi-inﬁnite programming: Theory, methods and applications. SIAM Review, 3:380–429, 1993. T. Joachims. Text categorization with support vector machines: Learning with many relevant features. In C. Nédellec and C. Rouveirol, editors, ECML ’98: Proceedings of the 10th European Conference on Machine Learning, Lecture Notes in Computer Science, pages 137–142, Berlin / Heidelberg, 1998. Springer-Verlag. T. Joachims. Making large–scale SVM learning practical. In B. Schölkopf, C.J.C. Burges, and A.J. Smola, editors, Advances in Kernel Methods — Support Vector Learning, pages 169–184, Cambridge, MA, USA, 1999. MIT Press. G.R.G. Lanckriet, T. De Bie, N. Cristianini, M.I. Jordan, and W.S. Noble. A statistical framework for genomic data fusion. Bioinformatics, 20:2626–2635, 2004. C. Leslie, E. Eskin, and W.S. Noble. The spectrum kernel: A string kernel for SVM protein classiﬁcation. In R. B. Altman, A. K. Dunker, L. Hunter, K. Lauderdale, and T. E. Klein, editors, Proceedings of the Paciﬁc Symposium on Biocomputing, pages 564–575, Kaua’i, Hawaii, 2002. C. Leslie, R. Kuang, and E. Eskin. Inexact matching string kernels for protein classiﬁcation. In Kernel Methods in Computational Biology, MIT Press series on Computational Molecular Biology, pages 95–112. MIT Press, 2004. 1564  L ARGE S CALE MKL  R. Meir and G. Rätsch. An introduction to boosting and leveraging. In S. Mendelson and A. Smola, editors, Proc. of the ﬁrst Machine Learning Summer School in Canberra, LNCS, pages 119–184. Springer, 2003. C.E. Metz. Basic principles of ROC analysis. Seminars in Nuclear Medicine, VIII(4), October 1978. C. S. Ong, A. J. Smola, and R. C. Williamson. Hyperkernels. In S. Thrun S. Becker and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, volume 15, pages 478– 485, Cambridge, MA, 2003. MIT Press. J. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Schölkopf, C.J.C. Burges, and A.J. Smola, editors, Advances in Kernel Methods — Support Vector Learning, pages 185–208, Cambridge, MA, USA, 1999. MIT Press. G. Rätsch. Robust Boosting via Convex Optimization. PhD thesis, University of Potsdam, Potsdam, Germany, 2001. G. Rätsch and S. Sonnenburg. Accurate Splice Site Prediction for Caenorhabditis Elegans, pages 277–298. MIT Press series on Computational Molecular Biology. MIT Press, 2004. G. Rätsch and M.K. Warmuth. Efﬁcient margin maximization with boosting. Journal of Machine Learning Research, 6:2131–2152, 2005. G. Rätsch, A. Demiriz, and K. Bennett. Sparse regression ensembles in inﬁnite and ﬁnite hypothesis spaces. Machine Learning, 48(1–3):193–221, 2002. Special Issue on New Methods for Model Selection and Model Combination. Also NeuroCOLT2 Technical Report NC-TR-2000-085. G. Rätsch, S. Sonnenburg, and B. Schölkopf. RASE: Recognition of alternatively spliced exons in C. elegans. Bioinformatics, 21:i369–i377, 2005. B. Schölkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002. S. Sonnenburg, G. Rätsch, and C. Schäfer. Learning interpretable SVMs for biological sequence classiﬁcation. In S. Miyano, J. P. Mesirov, S. Kasif, S. Istrail, P. A. Pevzner, and M. Waterman, editors, Research in Computational Molecular Biology, 9th Annual International Conference, RECOMB 2005, volume 3500, pages 389–407. Springer-Verlag Berlin Heidelberg, 2005a. S. Sonnenburg, G. Rätsch, and B. Schölkopf. Large scale genomic sequence SVM classiﬁers. In L. D. Raedt and S. Wrobel, editors, ICML ’05: Proceedings of the 22nd international conference on Machine learning, pages 849–856, New York, NY, USA, 2005b. ACM Press. M.K. Warmuth, J. Liao, and G. Rätsch. Totally corrective boosting algorithms that maximize the margin. In ICML ’06: Proceedings of the 23nd international conference on Machine learning. ACM Press, 2006.  1565</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
