<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-76" href="../jmlr2006/jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">jmlr2006-76</a> <a title="jmlr-2006-76-reference" href="#">jmlr2006-76-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</h1>
<br/><p>Source: <a title="jmlr-2006-76-pdf" href="http://jmlr.org/papers/volume7/hush06a/hush06a.pdf">pdf</a></p><p>Author: Don Hush, Patrick Kelly, Clint Scovel, Ingo Steinwart</p><p>Abstract: We describe polynomial–time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classiﬁers. These algorithms employ a two–stage process where the ﬁrst stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an √ √ approximate dual solution with accuracy (2 2Kn + 8 λ)−2 λε2 to an approximate primal solution p with accuracy ε p where n is the number of data samples, Kn is the maximum kernel value over the data and λ > 0 is the SVM regularization parameter. For the ﬁrst stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for τ–rate certifying decomposition algorithms we establish the optimality of τ = 1/(n − 1). In addition we extend the recent τ = 1/(n − 1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the τ = 1/(n − 1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the τ–rate certifying property of these algorithms to produce new stopping rules that are computationally efﬁcient and that guarantee a speciﬁed accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classiﬁcation problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n2 ) bound on the overall run time for the ﬁrst stage. Combining the ﬁrst and second stages gives an overall run time of O(n2 (c</p><br/>
<h2>reference text</h2><p>Jose L. Balcazar, Yang Dai, and Osamu Watanabe. Provably fast training algorithms for support vector machines. In Proceedings of the 1st International Conference on Data Mining ICDM, pages 43–50, 2001. URL citeseer.ist.psu.edu/590348.html. C.L. Blake and C.J. Merz. UCI repository of machine learning databases, 1998. http://www.ics.uci.edu/∼mlearn/MLRepository.html.  URL  C.C. Chang, C.W. Hsu, and C.J. Lin. The analysis of decomposition methods for support vector machines. IEEE Transactions on Neural Networks, 11(4):1003–1008, 2000. Chih-Chung Chang and Chih-Jen Lin. LIBSVM : a library for support vector machines, 2001. 763  H USH , K ELLY, S COVEL AND S TEINWART  Procedure 3 The Composite–I Decomposition Algorithm. 1: Decomposition(Q, w, c, u, ε, α0 ) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16:  (g0 , M 0 , s0 ) ← Initialize (Q, w, u, α0 ) m←0 repeat m m (Wml p2 ,Wmv , σm ) ← Simon(gm , M m ) m = 0) then if (σ Return(αm , gm ) end if m m (αm+1 , δm ,W m ) ← CompositeUpdate(αm , gm , Q,Wml p2 ,Wmv ) R gm+1 ← gm − Q(αm+1 − αm ) M m+1 ← UpdateMlist(M m ,W m , αm , αm+1 ) sm+1 ← min ((n − 1)σm , sm ) − δm R m ← m+1 until sm ≤ ε Return(αm , gm )  Procedure 4 This routine uses Simon’s algorithm to compute a max–lp2 pair Wml p2 . It also computes and returns a max–violating pair Wmv and the value σ∗ = σ(α|Wml p2 ). It assumes that M is an sorted list arranged in nonincreasing order by the value of ﬁrst component. 1: Simon(g, M) { M = (µ, i, ς)1 , (µ, i, ς)2 , ..., (µ, i, ς)2n } 2:  3: imax ← 0, 4: k ← 1 5:  6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20:  / imin ← 0, gmax ← −∞, gmin ← ∞, σ∗ ← 0, Wml p2 ← 0  while (µk > 0) do if ((ςk = +1) and (gik > gmax )) then gmax ← gik , imax ← ik if (µk (gmax − gmin ) > σ∗ ) then Wml p2 ← {imax , imin }, σ∗ ← µk (gmax − gmin ) end if else if ((ςk = −1) and (gik < gmin )) then gmin ← gik , imin ← ik if (µk (gmax − gmin ) > σ∗ ) then Wml p2 ← {imax , imin }, σ∗ ← µk (gmax − gmin ) end if end if k ← k+1 end while Wmv ← {imax , imin } Return(Wml p2 , Wmv , σ∗ )  764  QP A LGORITHMS  Procedure 5 This routine accepts a feasible value α and computes the corresponding gradient g, a list M of 3–tuples (µ, i, ς) sorted by µ, and a trivial bound s = 1 on R∗ − R(α). 1: Initialize(Q, w, u, α) 2: 3: 4: 5: 6: 7: 8: 9: 10:  g ← −Qα + w / M←0 for (i = 1, ..., n) do M ← Insert(M, (αi , i, −)) M ← Insert(M, (ui − αi , i, +)) end for s←1 Return(g, M, s)  Procedure 6 This routine computes the stepwise improvements for a max–lp2 pair Wml p2 and a max–violating pair Wmv , and then updates α using the pair with the largest stepwise improvement. It returns the new value of α, and the corresponding stepwise improvement value and index pair. 1: CompositeUpdate(αold , g, Q, Wml p2 , Wmv ) 2:  {i1 , i2 } ← Wml p2 δg ← gi1 − gi2 , q ← Qi1 i1 + Qi2 i2 − 2Qi1 i2 , ∆ml p2 = min ui1 − αold , αold i2 i1 5: if (δg > q∆ml p2 ) then  3:  4:  q∆ml p2 2  δml p2 ← ∆ml p2 δg − 7: else δ2 g 8: δml p2 ← 2q , ∆ml p2 ← 9: end if 6:  δg q  10:  { j1 , j2 } ← Wmv δg ← g j1 − g j2 , q ← Q j1 j1 + Q j2 j2 − 2Q j1 j2 , ∆mv = min u j1 − αold , αold j1 j2 13: if (δg > q∆mv ) then  11:  12:  δmv ← ∆mv δg − q∆mv 2 15: else δ2 δ g 16: δmv ← 2q , ∆mv ← qg 17: end if 14:  18: 19: 20: 21: 22: 23: 24: 25:  if (δml p2 > δmv ) then αnew ← αold + ∆ml p2 , αnew ← αold − ∆ml p2 i2 i1 i2 i1 Return(αnew , δml p2 , Wml p2 ) else αnew ← αold + ∆mv , αnew ← αold − ∆mv j2 j1 j2 j1 Return(αnew , δmv , Wmv ) end if  765  H USH , K ELLY, S COVEL AND S TEINWART  Procedure 7 This routine updates the sorted list M. 1: UpdateMlist (M,W, αold , αnew ) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12:  {i1 , i2 } ← W M ← Delete M ← Delete M ← Delete M ← Delete M ← Insert M ← Insert M ← Insert M ← Insert Return(M)  M, (αold , i1 , −) i1 M, (ui1 − αold , i1 , +) i1 M, (αold , i2 , −) i2 M, (ui2 − αold , i2 , +) i2 M, (αnew , i1 , −) i1 M, (ui1 − αnew , i1 , +) i1 M, (αnew , i2 , −) i2 M, (ui2 − αnew , i2 , +) i2  Procedure 8 This routine determines the offset parameter according to Theorem 2. Note that the input g is the gradient vector from the canonical dual solution. 1: Offset(g, y, u) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  s+ ← ∑i:yi =1 ui , s− ← 0 (g1 , y1 , u1 ), ..., (gn , yn , un ) ← SortIncreasing (g1 , y1 , u1 ), ..., (gn , yn , un ) ¯ ¯ ¯ ¯ ¯ ¯ L ← ∑i:yi =1 ui (gi − g1 ) ¯ ¯ ¯ ¯ L∗ ← L, b ← g1 ¯ for (i = 1, ..., n − 1) do if (yi = 1) then ¯ s+ ← s+ − ui ¯ else s− ← s− + ui ¯ end if L ← L − (gi+1 − gi )(s+ − s− ) ¯ ¯ if (L < L∗ ) then L∗ ← L, b ← gi+1 ¯ end if end for Return(b)  766  QP A LGORITHMS  P.-H. Chen, R.-E. Fan, and C.-J. Lin. Training support vector machines via SMO–type decomposition methods. In Proceedings of the 16th International Conference on Algorithmic Learning Theory, pages 45–62, 2005. P.-H. Chen, R.-E. Fan, and C.-J. Lin. A study on SMO-type decomposition methods for support vector machines. Technical report, 2006. URL http://www.csie.ntu.edu.tw/∼cjlin/papers.html. to appear in IEEE Transactions on Neural Networks. N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines and Other Kernelbased Learning Methods. Cambridge University Press, Canbridge ; United Kingdom, 1st edition, 2000. R.-E. Fan, P.-H. Chen, and C.-J. Lin. Working set selection using the second order information for training support vector machines. Journal of Machine Learning Research, 6:1889–1918, 2005. C.-W. Hsu and C.-J. Lin. A simple decomposition algorithm for support vector machines. Machine Learning, 46:291–314, 2002. D. Hush and C. Scovel. Polynomial-time decomposition algorithms for support vector machines. Machine Learning, 51:51–71, 2003. D. Hush, C. Scovel, and I. Steinwart. Stability of unstable learning algorithms. Technical report, Los Alamos National Laboratory LA-UR-03-4845, 2003. URL http://wwwc3.lanl.gov/ml/pubs ml.shtml. submitted for publication. D. Hush, C. Scovel, and I. Steinwart. Polynomial time algorithms for computing approximate SVM solutions with guaranteed accuracy. Technical report, Los Alamos National Laboratory LA-UR 05-7738, 2005. URL http://wwwc3.lanl.gov/ml/pubs ml.shtml. T. Joachims. Making large-scale SVM learning practical. In B. Sch¨ lkopf, C.J.C. Burges, and A.J. o Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT Press, Cambridge, MA, 1998. S.S. Keerthi and E.G. Gilbert. Convergence of a generalized SMO algorithm for SVM classiﬁer design. Machine Learning, 46:351–360, 2002. S.S. Keerthi and C.J. Ong. On the role of the threshold parameter in SVM training algorithms. Control Division Technical Report CD-00-09, Dept. of Mechanical and Production Engineering, National University of Singapore, 2000. S.S. Keerthi, S.K. Shevade, C. Bhattacharyya, and K.R.K. Murthy. A fast iterative nearest point algorithm for support vector machine classiﬁer design. IEEE Transactions on Neural Networks, 11:637–649, 2000. S.S. Keerthi, S.K. Shevade, C. Bhattacharyya, and K.R.K. Murthy. Improvements to Platt’s SMO algorithm for SVM classiﬁer design. Neural Computation, 13:637–649, 2001. D. Lai, N. Mani, and M. Palaniswami. A new method to select working sets for faster training for support vector machines. Technical Report MESCE–30–2003, Dept. Electrical and Computer Systems Engineering, Monash University, Australia, 2003. 767  H USH , K ELLY, S COVEL AND S TEINWART  P. Laskov. Feasible direction decomposition algorithms for training support vector machines. Machine Learning, 46(1–3):315–349, 2002. S.-P. Liao, H.-T. Lin, and C.-J. Lin. A note on the decomposition methods for support vector regression. Neural Computation, 14:1267–1281, 2002. C.-J. Lin. Linear convergence of a decomposition method for support vector machines. Technical Report, 2001a. URL http://www.csie.ntu.edu.tw/∼cjlin/papers.html. C.-J. Lin. On the convergence of the decomposition method for support vector machines. IEEE Transactions on Neural Networks, 12:1288–1298, 2001b. C.-J. Lin. A formal analysis of stopping criteria of decomposition methods for support vector machines. IEEE Transactions on Neural Networks, 13:1045–1052, 2002a. C.-J. Lin. Asymptotic convergence of an SMO algorithm without any assumptions. IEEE Transactions on Neural Networks, 13:248–250, 2002b. N. List and H.U. Simon. A general convergence theorem for the desomposition method. In J. ShaweTaylor and Y. Singer, editors, 17th Annual Conference on Learning Theory, COLT 2004, volume 3120 of Lecture Notes in Computer Science, pages 363–377, 2004. N. List and H.U. Simon. General polynomial time decomposition algorithms. In P. Auer and R. Meir, editors, 18th Annual Conference on Learning Theory, COLT 2005, pages 308–322, 2005. O.L. Mangasarian and D.R. Musicant. Lagrangian support vector machines. Journal of Machine Learning Research, 1:161–177, 2001. O.L. Mangasarian and D.R. Musicant. Successive overrelaxation for support vector machines. IEEE Transactions on Neural Networks, 10:1032–1037, 1999. E.E. Osuna, R. Freund, and F. Girosi. Support vector machines: training and applications. Technical Report AIM-1602, MIT, 1997. J.C. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Sch¨ lkopf, C.J.C. Burges, and A.J. Smola, editors, Advances in Kernel Methods - Support o Vector Learning, pages 41–64. MIT Press, Cambridge, MA, 1998. B. Sch¨ lkopf, J.C. Platt, J. Shawe-Taylor, and A.J. Smola. Estimating the support of a higho dimensional distribution. Neural Computation, 13:1443–1471, 2001. C. Scovel, D. Hush, and I. Steinwart. Approximate duality. Technical report, Los Alamos National Laboratory LA-UR 05-6766, 2005a. URL http://wwwc3.lanl.gov/ml/pubs ml.shtml. to appear in Journal of Optimization Theory and Applications. C. Scovel, D. Hush, and I. Steinwart. Learning rates for density level detection. Analysis and Applications, 3(4):356–371, 2005b. 768  QP A LGORITHMS  H.U. Simon. On the complexity of working set selection. In Proceedings of the 15th International Conference on Algorithmic Learning Theory, 2004. URL http://eprints.pascal-network.org/archive/00000125/. I. Steinwart and C. Scovel. Fast rates for support vector machines. In P. Auer and R. Meir, editors, 18th Annual Conference on Learning Theory, COLT 2005, pages 279–294, 2005. I. Steinwart and C. Scovel. Fast rates for support vector machines using Gaussian kernels. Technical report, Los Alamos National Laboratory LA-UR 04-8796, 2004. URL http://www.c3.lanl.gov/∼ingo/publications/pubs.shtml. submitted to Annals of Statistics (2004). I. Steinwart, D. Hush, and C. Scovel. A classiﬁcation framework for anomaly detection. Journal of Machine Learning Research, 6:211–232, 2005. V. Vapnik. Statistical Learning Theory. John Wiley and Sons, Inc., New York, NY, 1998.  769</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
