<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-23" href="../jmlr2006/jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">jmlr2006-23</a> <a title="jmlr-2006-23-reference" href="#">jmlr2006-23-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</h1>
<br/><p>Source: <a title="jmlr-2006-23-pdf" href="http://jmlr.org/papers/volume7/vert06a/vert06a.pdf">pdf</a></p><p>Author: Régis Vert, Jean-Philippe Vert</p><p>Abstract: We determine the asymptotic behaviour of the function computed by support vector machines (SVM) and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel Hilbert space of the Gaussian RBF kernel, in the situation where the number of examples tends to inﬁnity, the bandwidth of the Gaussian kernel tends to 0, and the regularization parameter is held ﬁxed. Non-asymptotic convergence bounds to this limit in the L2 sense are provided, together with upper bounds on the classiﬁcation error that is shown to converge to the Bayes risk, therefore proving the Bayes-consistency of a variety of methods although the regularization term does not vanish. These results are particularly relevant to the one-class SVM, for which the regularization can not vanish by construction, and which is shown for the ﬁrst time to be a consistent density level set estimator. Keywords: regularization, Gaussian kernel RKHS, one-class SVM, convex loss functions, kernel density estimation</p><br/>
<h2>reference text</h2><p>N. Aronszajn. Theory of reproducing kernels. Trans. Am. Math. Soc., 68:337 – 404, 1950. P. I. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classiﬁcation and risk bounds. J. Amer. Statist. Assoc., 101(473):138–156, 2006. 852  C ONSISTENCY AND C ONVERGENCE R ATES OF O NE -C LASS SVM S AND R ELATED A LGORITHMS  P. L. Bartlett and A. Tewari. Sparseness vs estimating conditional probabilities: Some asymptotic results. In Lecture Notes in Computer Science, volume 3120, pages 564–578. Springer, 2004. P. L. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. Ann. Stat., 33(4): 1497–1537, 2005. B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classiﬁers. In Proceedings of the 5th annual ACM workshop on Computational Learning Theory, pages 144– 152. ACM Press, 1992. R. A. DeVore and G. G. Lorentz. Constructive Approximation. Springer Grundlehren der Mathematischen Wissenschaften. Springer Verlag, 1993. L. Devroye and G. Lugosi. Combinatorial Methods in Density Estimation. Springer Series in Statistics. Springer, 2000. L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition, volume 31 of o Applications of Mathematics. Springer, 1996. G. B. Folland. Fourier analysis and its applications. The Wadsworth & Brooks/Cole Mathematics Series. Wadsworth & Brooks/Cole Advanced Books & Software, Paciﬁc Grove, CA, 1992. J. A. Hartigan. Estimation of a convex density contour in two dimensions. J. Amer. Statist. Assoc., 82(397):267–270, 1987. V. Koltchinskii. Localized Rademacher complexities. Manuscript, September 2003. G. Lugosi and N. Vayatis. On the Bayes-risk consistency of regularized boosting methods. Ann. Stat., 32:30–55, 2004. E. Mammen and A. Tsybakov. Smooth discrimination analysis. Ann. Stat., 27(6):1808–1829, 1999. P. Massart. Some applications of concentration inequalities to statistics. Ann. Fac. Sc. Toulouse, IX (2):245–303, 2000. M. T. Matache and V. Matache. Hilbert spaces induced by Toeplitz covariance kernels. In Lecture Notes in Control and Information Sciences, volume 280, pages 319–334. Springer, Jan 2002. B. Sch¨ lkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Estimating the o support of a high-dimensional distribution. Neural Comput., 13:1443–1471, 2001. B. W. Silverman. On the estimation of a probability density function by the maximum penalized likelihood method. Ann. Stat., 10:795–810, 1982. I. Steinwart. Support vector machines are universally consistent. J. Complexity, 18:768–791, 2002. I. Steinwart. Sparseness of support vector machines. J. Mach. Learn. Res., 4:1071–1105, 2003. I. Steinwart. Consistency of support vector machines and other regularized kernel classiﬁers. IEEE Trans. Inform. Theory, 51(1):128–142, 2005a. 853  V ERT AND V ERT  I. Steinwart. How to compare loss functions and their risks. Technical report, Los Alamos National Laboratory, 2005b. I. Steinwart and C. Scovel. Fast rates for support vector machines using Gaussian kernels. Technical Report LA-UR 04-8796, Los Alamos National Laboratory, 2004. I. Steinwart, D. Hush, and C. Scovel. An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels. Technical Report LA-UR 04-8274, Los Alamos National Laboratory, 2004. I. Steinwart, D. Hush, and Scovel C. A classiﬁcation framework for anomaly detection. J. Mach. Learn. Res., 6:211–232, 2005. A.N. Tikhonov and V.Y. Arsenin. Solutions of ill-posed problems. W.H. Winston, 1977. A. B. Tsybakov. On nonparametric estimation of density level sets. Ann. Stat., 25:948–969, June 1997. T. Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. Ann. Stat., 32:56–134, 2004.  854</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
