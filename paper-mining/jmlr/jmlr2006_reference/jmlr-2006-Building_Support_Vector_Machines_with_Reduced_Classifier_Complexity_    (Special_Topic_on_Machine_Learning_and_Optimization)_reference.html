<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-18" href="../jmlr2006/jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">jmlr2006-18</a> <a title="jmlr-2006-18-reference" href="#">jmlr2006-18-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-18-pdf" href="http://jmlr.org/papers/volume7/keerthi06a/keerthi06a.pdf">pdf</a></p><p>Author: S. Sathiya Keerthi, Olivier Chapelle, Dennis DeCoste</p><p>Abstract: Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classiﬁcation speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily ﬁnds a set of kernel basis functions of a speciﬁed maximum size (dmax ) to approximate the SVM primal cost function well; (3) it is 2 efﬁcient and roughly scales as O(ndmax ) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors. Keywords: SVMs, classiﬁcation, sparse design</p><br/>
<h2>reference text</h2><p>J. Adler, B. D. Rao, and K. Kreutz-Delgado. Comparison of basis selection methods. In Proceedings of the 30th Asilomar conference on signals, systems and computers, pages 252–257, 1996. F. Bach and M. Jordan. Predictive low-rank decomposition for kernel methods. In Proceedings of the Twenty-second International Conference on Machine Learning (ICML), 2005. K. P. Bennett, M. Momma, and M. J. Embrechts. MARK: A boosting algorithm for heterogeneous kernel models. In Proceedings of SIGKDD’02, 2002. J. Bi, T. Zhang, and K. P. Bennet. Column generation boosting methods for mixture of kernels. In Proceedings of SIGKDD’04, 2004. C. J. C. Burges and B. Sch¨ lkopf. Improving the accuracy and speed of support vector learning o machines. In Proceedings of the 9th NIPS Conference, pages 375–381, 1997. O. Chapelle. Training a support vector machine in the primal. Journal of Machine Learning Research, 2005. submitted. D. DeCoste and B. Sch¨ lkopf. Training invariant support vector machines. Machine Learning, 46: o 161–190, 2002. T. Downs, K. E. Gates, and A. Masters. Exact simpliﬁcation of support vector solutions. Journal of Machine Learning Research, 2:293–297, 2001. J. H. Friedman. Greedy function approximation: a gradient boosting machine. Annals of Statistics, 29:1180, 2001. T. Joachims. Making large-scale SVM learning practical. In Advances in Kernel Methods - Support Vector Learning. MIT Press, Cambridge, Massachussetts, 1999. S. S. Keerthi and W. Chu. A matching pursuit approach to sparse Gaussian process regression. In Proceedings of the 18th NIPS Conference, 2006. S. S. Keerthi and D. DeCoste. A modiﬁed ﬁnite Newton method for fast solution of large scale linear svms. Journal of Machine Learning Research, 6:341–361, 2005. N. Lawrence, M. Seeger, and R. Herbrich. Fast sparse Gaussian process methods: The Informative Vector Machine. In Proceedings of the 15th NIPS Conference, pages 609–616, 2003. Y. J. Lee and O. L. Mangasarian. RSVM: Reduced support vector machines. In Proceedings of the SIAM International Conference on Data Mining. SIAM, Philadelphia, 2001. K. M. Lin and C. J. Lin. A study on reduced support vector machines. IEEE TNN, 14:1449–1459, 2003. S. G. Mallat and Z. Zhang. Matching pursuits with time-frequency dictionaries. IEEE Transactions on ASSP, 41:3397–3415, 1993. O. L. Mangasarian. A ﬁnite Newton method for classiﬁcation. Optimization Methods and Software, 17:913–929, 2002. 1514  B UILDING SVM S WITH R EDUCED C OMPLEXITY  J. A. Meijerink and H. A. van der Vorst. An iterative solution method for linear systems of which the coefﬁcient matrix is a symmetric M-matrix. Mathematics of Computation, 31:148–162, 1977. E. Osuna and F. Girosi. Reducing the run-time complexity of support vector machines. In Proceedings of the International Conference on Pattern Recognition, 1998. E. Parrado-Hern´ ndez, I. Mora-Jimen´ z, J. Arenas-Garc´a, A. R. Figueiras-Vidal, and A. Naviaa e ı V´ zquez. Growing support vector classiﬁers with controlled complexity. Pattern Recognition, a 36:1479–1488, 2003. J. Platt. Sequential minimal optimization: A fast algorithm for training support vector machines. Technical report, Microsoft Research, Redmond, 1998. G. R¨ tsch. Robust boosting via convex optimization. PhD thesis, University of Potsdam, Department a of Computer Science, Potsdam, Germany, 2001. G. R¨ tsch. Benchmark repository. http://ida.first.fraunhofer.de/∼raetsch/. a B. Sch¨ lkopf, S. Mika, C. J. C. Burges, P. Knirsch, K. R. Muller, G. Raetsch, and A. J. Smola. Input o space vs. feature space in kernel-based methods. IEEE TNN, 10:1000–1017, 1999. M. Seeger. Low rank updates for the Cholesky decomposition. Technical report, University of California, Berkeley, 2004. M. Seeger, C. Williams, and N. Lawrence. Fast forward selection to speed up sparse Gaussian process regression. In Proceedings of the Workshop on AI and Statistics, 2003. A. J. Smola and P. L. Bartlett. Sparse greedy Gaussian process regression. In Proceedings of the 13th NIPS Conference, pages 619–625, 2001. A. J. Smola and Sch¨ lkopf. Sparse greedy matrix approximation for machine learning. In Proceedo th International Conference on Machine Learning, pages 911–918, 2000. ings of the 17 I. Steinwart. Sparseness of support vector machines - some asymptotically sharp bounds. In Proceedings of the 16th NIPS Conference, pages 169–184, 2004. T. Thies and F. Weber. Optimal reduced-set vectors for support vector machines with a quadratic kernel. Neural Computation, 16:1769–1777, 2004. M. E. Tipping. Sparse Bayesian learning and the Relevance Vector Machine. Journal of Machine Learning Research, 1:211–244, 2001. P. Vincent and Y. Bengio. Kernel matching pursuit. Machine Learning, 48:165–187, 2002. M. Wu, B. Sch¨ lkopf, and G. Bakir. Building sparse large margin classiﬁers. In Proceedings of the o 22nd International Conference on Machine Learning, 2005.  1515</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
