<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-71" href="../jmlr2006/jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">jmlr2006-71</a> <a title="jmlr-2006-71-reference" href="#">jmlr2006-71-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</h1>
<br/><p>Source: <a title="jmlr-2006-71-pdf" href="http://jmlr.org/papers/volume7/centeno06a/centeno06a.pdf">pdf</a></p><p>Author: Tonatiuh Peña Centeno, Neil D. Lawrence</p><p>Abstract: In this paper we consider a novel Bayesian interpretation of Fisher’s discriminant analysis. We relate Rayleigh’s coefﬁcient to a noise model that minimises a cost based on the most probable class centres and that abandons the ‘regression to the labels’ assumption used by other algorithms. Optimisation of the noise model yields a direction of discrimination equivalent to Fisher’s discriminant, and with the incorporation of a prior we can apply Bayes’ rule to infer the posterior distribution of the direction of discrimination. Nonetheless, we argue that an additional constraining distribution has to be included if sensible results are to be obtained. Going further, with the use of a Gaussian process prior we show the equivalence of our model to a regularised kernel Fisher’s discriminant. A key advantage of our approach is the facility to determine kernel parameters and the regularisation coefﬁcient through the optimisation of the marginal log-likelihood of the data. An added bonus of the new formulation is that it enables us to link the regularisation coefﬁcient with the generalisation error.</p><br/>
<h2>reference text</h2><p>Deepak K. Agarwal. Shrinkage estimator generalizations of proximal support vector machines. In KDD ’02: Proceedings of the eighth ACM SIGKDD International conference on Knowledge Discovery and Data Mining, pages 173–182, New York, NY, USA, 2002. ACM Press. N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society, 68:337–404, May 1950. Gaston Baudat and Fatiha Anouar. Generalized discriminant analysis using a kernel approach. Neural Computation, 12(10):2385–2404, 2000. Christopher M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995. A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B (Methodological), 39:1–38, 1977. Richard O. Duda and Peter E. Hart. Pattern Recognition and Scene Analysis. John Wiley, 1973. Ronald A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7:179, 1936. Keinosuke Fukunaga. Introduction to Statistical Pattern Recognition. Academic Press Inc., Boston, Massachusetts, 2nd edition, 1990. Glenn Fung and Olvi L. Mangasarian. Proximal support vector machine classiﬁers. In KDD ’01: Proceedings of the seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 77–86, New York, NY, USA, 2001. ACM Press. Gene H. Golub and Charles F. Van Loan. Matrix computations. Johns Hopkins University Press, Baltimore, MD, USA, 3rd edition, 1996. Robert B. Gramacy and Herbert K. H. Lee. Gaussian processes and limiting linear models. Technical Report ams2005-01, Department of Applied Mathematics and Statistics, University of California, Santa Cruz., 2005. Ian T. Jolliffe. Principal Component Analysis. Springer-Verlag, New York, 1st edition, 1986. Kevin J. Lang and Michael J. Witbrock. Learning to tell two spirals apart. In David S. Touretzky, Geoff E. Hinton, and Terrence J. Sejnowski, editors, Proceedings of the 1988 Connectionist Models Summer School. Morgan Kauffman, 1988. Neil D. Lawrence and Bernhard Schölkopf. Estimating a kernel Fisher discriminant in the presence of label noise. In Carla E. Brodley and Andrea P. Danyluk, editors, Proceedings of the 18th International Conference on Machine Learning, Williamstown, MA, July 2001. Morgan Kauffman. Neil D. Lawrence, Matthias Seeger, and Ralf Herbrich. Fast sparse Gaussian process methods: the informative vector machine. In Suzanna Becker, Sebastian Thrun, and Klaus Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 609–616, Cambridge, MA, 2003. MIT Press. 489  P EÑA C ENTENO AND L AWRENCE  David J. C. Mackay. Probable networks and plausible predictions - a review of practical Bayesian methods for supervised neural networks. Network: Computation in Neural Systems, 6(3):469– 505, 1995. Donald Michie, David J. Spiegelhalter, and Charles C. Taylor, editors. Machine Learning, Neural and Statistical Classiﬁcation. Ellis Horwood, 1994. Sebastian Mika. A mathematical approach to kernel Fisher algorithm. In Todd K. Leen and Thomas G. Dietterich Völker Tresp, editors, Advances in Neural Information Processing Systems 13, pages 591–597, Cambridge, MA, 2001. MIT Press. Sebastian Mika. Kernel Fisher Discriminants. PhD thesis, Technischen Universität, Berlin, Germany, 2002. Sebastian Mika, Gunnar Rätsch, Jason Weston, Bernhard Schölkopf, and Klaus-Robert Müller. Fisher discriminant analysis with kernels. In Y.-H. Hu, E. Wilson J. Larsen, and S. Douglas, editors, Neural Networks for Signal Processing IX, pages 41–48. IEEE, 1999. Ian T. Nabney. Netlab: Algorithms for Pattern Recognition. Springer-Verlag, 2002. Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, 1996. Anthony O’Hagan. Curve ﬁtting and optimal design for prediction. Journal of the Royal Statistical Society, Series B (Methodological), 40(1):1–42, 1978. Kristiaan Pelckmans, Johan A. K. Suykens, Tony Van Gestel, Jos De Brabanter, Lukas Lukas, Bart Hamers, Bart De Moor, and Joos Vandewalle. LS-SVMlab Toolbox User’s Guide. Katholieke Universiteit, Leuven. ESAT-SCD-SISTA, 2003. Gunnar Rätsch, Takashi Onoda, and Klaus-Robert Müller. Soft margins for AdaBoost. Technical Report NC-TR-98-021, Royal Holloway College, University of London, U. K., 1998. Brian D. Ripley. Pattern Recognition and Neural Networks. Cambridge University Press, 1996. Volker Roth. Outlier detection with one-class kernel Fisher discriminants. In Lawrence K. Saul, Yair Weiss, and Léon Bottou, editors, Advances in Neural Information Processing Systems 17, pages 1169–1176, Cambridge, MA, 2005. MIT Press. David Ruppert, Matthew P. Wand, and Raymond J. Carroll. Semiparametric Regression. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, Cambridge, U. K., 2003. Bernhard Schölkopf and Alexander J. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimization and Beyond. MIT Press, 2002. Johan A. K. Suykens, Tony Van Gestel, Jos De Brabanter, Bart De Moor, and Joos Vandewalle. Least Squares Support Vector Machines. World Scientiﬁc, 2002. Johan A. K. Suykens and Joos Vandewalle. Least squares support vector machines. Neural Processing Letters, 9(3):293–300, 1999. 490  O PTIMIZING KERNEL PARAMETERS  Tony Van Gestel, Johan A. K. Suykens, Gert Lanckriet, Annemie Lambrechts, Bart de Moor, and Joos Vandewalle. Bayesian framework for least squares support vector machine classiﬁers, Gaussian processes and kernel discriminant analysis. Neural Computation, 14(5):1115–1147, 2002. Vladimir Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, 1995. Christopher K. I. Williams. Prediction with Gaussian processes: from linear regression to linear prediction and beyond. In Michael I. Jordan, editor, Learning in Graphical Models, D, Behavioural and social sciences 11. Kluwer, Dordrecht, The Netherlands, 1999.  491</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
