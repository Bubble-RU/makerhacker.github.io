<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>73 jmlr-2006-Pattern Recognition for  Conditionally Independent Data</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-73" href="../jmlr2006/jmlr-2006-Pattern_Recognition_for__Conditionally_Independent_Data.html">jmlr2006-73</a> <a title="jmlr-2006-73-reference" href="#">jmlr2006-73-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>73 jmlr-2006-Pattern Recognition for  Conditionally Independent Data</h1>
<br/><p>Source: <a title="jmlr-2006-73-pdf" href="http://jmlr.org/papers/volume7/ryabko06a/ryabko06a.pdf">pdf</a></p><p>Author: Daniil Ryabko</p><p>Abstract: In this work we consider the task of relaxing the i.i.d. assumption in pattern recognition (or classiﬁcation), aiming to make existing learning algorithms applicable to a wider range of tasks. Pattern recognition is guessing a discrete label of some object based on a set of given examples (pairs of objects and labels). We consider the case of deterministically deﬁned labels. Traditionally, this task is studied under the assumption that examples are independent and identically distributed. However, it turns out that many results of pattern recognition theory carry over a weaker assumption. Namely, under the assumption of conditional independence and identical distribution of objects, while the only assumption on the distribution of labels is that the rate of occurrence of each label should be above some positive threshold. We ﬁnd a broad class of learning algorithms for which estimations of the probability of the classiﬁcation error achieved under the classical i.i.d. assumption can be generalized to the similar estimates for case of conditionally i.i.d. examples.</p><br/>
<h2>reference text</h2><p>D. Aldous and U. Vazirani, A Markovian Extension of Valiant’s Learning Model. In Proceedings of the 31st Symposium on Foundations of Computer Science, pp. 392–396, 1990. P. Algoet, Universal Schemes for Learning the Best Nonlinear Predictor Given the Inﬁnite Past and Side Information. IEEE Transactions on Information Theory, Vol. 45, No. 4, 1999. 662  PATTERN R ECOGNITION FOR C ONDITIONALLY I NDEPENDENT DATA  P. Bartlett, S. Ben-David, S. Kulkarni, Learning Changing Concepts by Exploiting the Structure of Change. In Proceedings of the Workshop on Computational Learning Theory, pp. 131–139, Morgan Kaufmann Publishers, 1996. E. Baum and D. Haussler, What Size Net Gives Valid Generalisation? Neural Computation, 1:151160, 1989. A. Blumer, A. Ehrenfeucht, D. Haussler M and Warmuth Learnability and the Vapnik-Chervonenkis Dimension. Journal of the ACM, 36, pp. 929–965, 1989. O. Bousquet, A. Elisseeff. Stability and Generalization. Journal of Machine Learning Research, 2: 499-526, 2002. A.P. Dawid Conditional Independence in Statistical Theory. Journal of the Royal Statistical Society, Series B (Methodological), Vol. 41 No 1, pp. 1–31, 1979. L. Devroye, On Asymptotic Probability of Error in Nonparametric Discrimination. Annals of Statistics, Vol. 9, No. 6, pp. 1320–1327, 1981. L. Devroye, L. Gy¨ rﬁ, A. Krzyz˙ k, G. Lugosi, On the Strong Universal Consistency of Nearest o a Neighbor Regression Function Estimates. Annals of Statistics, Vol. 22, pp. 1371–1385, 1994. L. Devroye, L. Gy¨ rﬁ, G. Lugosi, A Probabilistic Theory of Pattern Recognition. New York: o Springer, 1996. R. Duda, P. Hart, D. Stork. Pattern Classiﬁcation, Second edition, Wiley-Interscience, 2001. L. Gy¨ rﬁ, G. Lugosi, G. Morvai, A Simple Randomized Algorithm for Sequential Prediction of o Ergodic Time Series. IEEE Transactions on Information Theory, Vol. 45, pp. 2642–2650, 1999. D. Helmbold and P. Long, Tracking Drifting Concepts by Minimizing Disagreements. Proceedings of the Fourth Annual Workshop on Computational Learning Theory, Santa Cruz, USA, pp. 13–23, 1991. D. Gamarnik, Extension of the PAC Framework to Finite and Countable Markov Chains. IEEE Transactions on Information Theory, 49(1):338-345, 2003. M. Kearns and D. Ron, Algorithmic Stability and Sanity-Check Bounds on Leave-One-Out CrossValidation. Neural Computation, Vol. 11, No. 6, pp. 1427–1453, 1999. M. Kearns M. and U. Vazirani, An Introduction to Computational Learning Theory. The MIT Press, Cambridge, Massachusetts, 1994. S. Kulkarni, S. Posner. Rates of Convergence of Nearest Neighbour Estimation Under Arbitrary Sampling. IEEE Transactions on Information Theory, Vol. 41, No. 10, pp. 1028–1039, 1995. S. Kulkarni, S. Posner, S. Sandilya. Data-Dependent kn -NN and Kernel Estimators Consistent for Arbitrary Processess. IEEE Transactions on Information Theory, Vol. 48, No. 10, pp. 2785–2788, 2002. 663  RYABKO  G. Lugosi, A. Nobel, Consistency of Data-Driven Histogram Methods for Density Estimation and Classiﬁcation. Annals of Statistics Vol. 24, No.2, pp. 687–706, 1996. G. Lugosi and K. Zeger, Nonparametric Estimation via Empirical Risk Minimization. IEEE Transactions on Information Theory, Vol. 41 No. 3 pp. 677–687, 1995. G. Morvai, S. Kulkarni, and A.B. Nobel, Regression Estimation from an Individual Stable Sequence. Statistics, vol. 33, pp.99–118, 1999. G. Morvai, S. Yakowitz, P. Algoet, Weakly Convergent Nonparametric Forecasting of Stationary Time Series. IEEE Transactions on Information Theory, Vol. 43, No. 2, 1997. A.B. Nobel, Limits to Classiﬁcation and Regression Estimation from Ergodic Process. Annals of Statistics, vol. 27, pp. 262–273, 1999. W. Rogers and T. Wagner. A ﬁnite Sample Distribution-Free Performance Bound for Local Discrimination Rules. Annals of Statistics, Vol. 6 No. 3 pp. 506–514, 1978. B. Ryabko, Prediction of random sequences and universal coding. Problems of Information Transmission, Vol. 24, pp. 87–96, 1988. D. Ryabko, Online Learning of Conditionally I.I.D. Data. In: C. E. Brodley (Ed.), Proceedings of the 21st International Conference on Machine Learning, Banff, Canada, pp. 727–734, 2004. D. Ryabko, Application of Classical Nonparametric Predictors to Learning Conditionally I.I.D. Data. In: S. Ben-David, J. Case, A. Maruoka (Eds.), Proceedings of 15th International Conference on Algorithmic Learning Theory, Padova, Italy, pp. 171–180, 2004. L. Valiant, A Theory of the Learnable. Communications of the ACM, 27, pp. 1134–1142. 1984. V. Vapnik, Statistical Learning Theory, New York etc.: John Wiley , Sons, Inc. 1998. V. Vapnik, and A. Chervonenkis. Theory of Pattern Recognition. Nauka, Moscow, 1974 (in Russian).  664</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
