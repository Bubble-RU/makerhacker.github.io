<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-15" href="../jmlr2006/jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">jmlr2006-15</a> <a title="jmlr-2006-15-reference" href="#">jmlr2006-15-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-15-pdf" href="http://jmlr.org/papers/volume7/niculescu06a/niculescu06a.pdf">pdf</a></p><p>Author: Radu Stefan Niculescu, Tom M. Mitchell, R. Bharat Rao</p><p>Abstract: The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. This paper considers a variety of types of domain knowledge for constraining parameter estimates when learning Bayesian networks. In particular, we consider domain knowledge that constrains the values or relationships among subsets of parameters in a Bayesian network with known structure. We incorporate a wide variety of parameter constraints into learning procedures for Bayesian networks, by formulating this task as a constrained optimization problem. The assumptions made in module networks, dynamic Bayes nets and context speciﬁc independence models can be viewed as particular cases of such parameter constraints. We present closed form solutions or fast iterative algorithms for estimating parameters subject to several speciﬁc classes of parameter constraints, including equalities and inequalities among parameters, constraints on individual parameters, and constraints on sums and ratios of parameters, for discrete and continuous variables. Our methods cover learning from both frequentist and Bayesian points of view, from both complete and incomplete data. We present formal guarantees for our estimators, as well as methods for automatically learning useful parameter constraints from data. To validate our approach, we apply it to the domain of fMRI brain image analysis. Here we demonstrate the ability of our system to ﬁrst learn useful relationships among parameters, and then to use them to constrain the training of the Bayesian network, resulting in improved cross-validated accuracy of the learned model. Experiments on synthetic data are also presented. Keywords: Bayesian networks, constrained optimization, domain knowledge c 2006 Radu Stefan Niculescu, Tom Mitchell and Bharat Rao. N ICULESCU , M ITCHELL AND R AO</p><br/>
<h2>reference text</h2><p>J. Bilmes. Dynamic bayesian multinets. In Proceedings of UAI, pages 38–45, 2000. C. Boutilier, N. Friedman, M. Goldszmidt, and D. Koller. Context-speciﬁc independence in bayesian networks. In Proceedings of 12th UAI, pages 115–123, 1996. P. A. Carpenter, M. A. Just, T. A. Keller, W. F. Eddy, and K. R. Thulborn. Time course of fMRIactivation in language and spatial networks during sentence comprehension. NeuroImage, 10: 216–224, 1999. A. M. Dale. Optimal experimental design for event-related fMRI. Human Brain Mapping, 8:109– 114, 1999. N. Friedman, L. Getoor, D. Koller, and A. Pfeffer. Learning probabilistic relational models. In Proceedings of 16th IJCAI, pages 1300–1307, 1999. D. Geiger and D. Heckerman. Knowledge representation and inference in similarity networks and bayesian multinets. Artiﬁcial Intelligence, 82:45–74, 1996. D. Geiger and D. Heckerman. A characterization of the dirichlet distribution through global and local parameter independence. The Annals of Statistics, 25:1344–1369, 1997. D. Heckerman. A tutorial on learning with bayesian networks. In M. Jordan, editor, Learning in Graphical Models. MIT Press, Cambridge, MA, 1999. P. Hooper. Dependent dirichlet priors and optimal linear estimators for belief net parameters. In AUAI Press, editor, Proceedings of the 20th Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-04), pages 251–259, 2004. R. Hutchinson, T.M. Mitchell, and I. Rustandi. Learning to identify overlapping and hidden cognitive processes from fMRI data. In 11th Conference on Human Brain Mapping, June 2005. R. Hutchinson, T.M. Mitchell, and I. Rustandi. Hidden process models. Technical Report CSCALD-05-116, Carnegie Mellon University, February 2006. D. Koller and A. Pfeffer. Object oriented bayesian networks. In Proceedings of 13th UAI, pages 302–313, 1997. H. W. Kuhn and A. W. Tucker. Nonlinear programming. In Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, pages 481–492. University of California Press, 1951. T. Minka. The dirichlet-tree distribution. This unpublished paper is available online at http://research.microsoft.com/∼minka/papers/dirichlet/minka-dirtree.pdf, 1999. T. Mitchell, R. Hutchinson, M. Just, S. Newman, R. S. Niculescu, F. Pereira, and X. Wang. Learning to decode cognitive states from brain images. Machine Learning, 57(1-2):145–175, 2004. K. P. Murphy. Dynamic Bayesian Networks: Representation, Inference and Learning. PhD thesis, UC Berkeley, 2002. 1382  BAYESIAN N ETWORK L EARNING WITH PARAMETER C ONSTRAINTS  R. S. Niculescu. Exploiting parameter domain knowledge for learning in bayesian networks. Technical Report CMU-TR-05-147, Carnegie Mellon University, 2005. R. S. Niculescu, T. Mitchell, and R. B. Rao. Parameter related domain knowledge for learning in graphical models. In Proceedings of SIAM Data Mining conference, 2005. J. M. Pena, J. A. Lozano, and P. Larranaga. Learning recursive bayesian multinets for data clustering by means of constructive induction. Machine Learning, 47(1):63–89, 2002. W. H. Press, S. A. Teukolsky, and W. T. Vetterling. Numerical Recipes in C: The Art of Scientiﬁc Computing. Cambridge University Press, 1993. R. L. Rabiner. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286, 1989. R. B. Rao, S. Sandilya, R. S. Niculescu, C. Germond, and H. Rao. Clinical and ﬁnancial outcomes analysis with existing hospital patient records. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 416–425, 2003. E. Segal, D. Pe’er, A. Regev, D. Koller, and N. Friedman. Learning module networks. In Proceedings of 19th UAI, pages 525–534, 2003. J. B. Tenenbaum and W. T. Freeman. Separating style and content with bilinear models. Neural Computation, 12(6):1247–1283, 2000. G. Welch and G. Bishop. An introduction to the kalman ﬁlter. Technical Report TR 95-041, University of North Carolina, 1995. C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of SIGIR, pages 334–342, 2001.  1383</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
