<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-56" href="../jmlr2006/jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">jmlr2006-56</a> <a title="jmlr-2006-56-reference" href="#">jmlr2006-56-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-56-pdf" href="http://jmlr.org/papers/volume7/bergkvist06a/bergkvist06a.pdf">pdf</a></p><p>Author: Anders Bergkvist, Peter Damaschke,  Marcel Lüthi</p><p>Abstract: We consider an optimization problem in probabilistic inference: Given n hypotheses H j , m possible observations Ok , their conditional probabilities pk j , and a particular Ok , select a possibly small subset of hypotheses excluding the true target only with some error probability ε. After specifying the optimization goal we show that this problem can be solved through a linear program in mn variables that indicate the probabilities to discard a hypothesis given an observation. Moreover, we can compute optimal strategies where only O(m + n) of these variables get fractional values. The manageable size of the linear programs and the mostly deterministic shape of optimal strategies makes the method practicable. We interpret the dual variables as worst-case distributions of hypotheses, and we point out some counterintuitive nonmonotonic behaviour of the variables as a function of the error bound ε. One of the open problems is the existence of a purely combinatorial algorithm that is faster than generic linear programming. Keywords: probabilistic inference, error probability, linear programming, cycle-free graphs, network ﬂows</p><br/>
<h2>reference text</h2><p>R. D. Beger and P. H. Bolton. Protein φ and ψ dihedral restraints determined from multidimensional hypersurface correlations of backbone chemical shifts and their use in the determination of protein tertiary structures. Journal of Biomolecular NMR, 10:129–142, 1997. K. P. Bennett. Decision tree construction via linear programming. In Proceedings of the 4th Midwest AI and Cognitive Science Sociecty Conference, pages 97–101, 1992. K. P. Bennett and O. L. Mangasarian. Neural network training via linear programming. In Advances in Optimization and Parallel Computing, pages 56–67. North-Holland, 1992a. K. P. Bennett and O. L. Mangasarian. Robust linear programming discrimination of two linearly inseparable sets. Optimization Methods and Software, 1:23–34, 1992b. K. P. Bennett and O. L. Mangasarian. Multicategory separation via linear programming. Optimization Methods and Software, 3:27–39, 1993. P. S. Bradley. Mathematical Programming Approaches to Machine Learning and Data Mining. PhD thesis, University of Wisconsin, 1998. C. Christin. Scatterplot partitioning algorithm for LETA-NMR. Master’s thesis, International Master’s Programme in Bioinformatics, Chalmers University, G¨ teborg (Sweden), 2006. o G. Cornilescu, F. Delaglio, and A. Bax. Protein backbone angle restraints from searching a database for chemical shift and sequence homology. Journal of Biomolecular NMR, 13:289–302, 1999. P. Damaschke. Scheduling search procedures. Journal of Scheduling, 7:349–364, 2004. F. Glover. Improved linear programming models for discriminant analysis. Decision Sciences, 21: 771–785, 1990. S. Martello and P. Toth. Knapsack Problems: Algorithms and Computer Implementations. Wiley, 1990. P. Szolovits, R. S. Patil, and W. B. Schwartz. Artiﬁcial intelligence in medical diagnosis. Annals of Internal Medicine, 108:80–87, 1988. 1354  L INEAR P ROGRAMS FOR H YPOTHESES S ELECTION  E. Tardos and K. D. Wayne. Simple generalized maximum ﬂow algorithms. In Proceedings of the 6th Integer Programming and Combinatorial Optimization Conference, Lecture Notes in Computer Science, volume 1412, pages 310–324, 1998. Y. Wang and O. Jardetzky. Probability-based protein secondary structure identiﬁcation using combined NMR chemical-shift data. Protein Science, 11:852–861, 2002. K. D. Wayne. A polynomial combinatorial algorithm for generalized minimum cost ﬂow. Mathematical Operations Research, 27:445–459, 2002. X. P. Xu and D. A. Case. Probing multiple effects on 15 N, 13 Cα , 13 Cβ and 13 C’ chemical shifts in peptides using density functional theory. Biopolymers, 65:408–423, 2002.  1355</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
