<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-72" href="../jmlr2006/jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">jmlr2006-72</a> <a title="jmlr-2006-72-reference" href="#">jmlr2006-72-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-72-pdf" href="http://jmlr.org/papers/volume7/zanni06a/zanni06a.pdf">pdf</a></p><p>Author: Luca Zanni, Thomas Serafini, Gaetano Zanghirati</p><p>Abstract: Parallel software for solving the quadratic program arising in training support vector machines for classiﬁcation problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-ofthe-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes. Keywords: support vector machines, large scale quadratic programs, decomposition techniques, gradient projection methods, parallel computation</p><br/>
<h2>reference text</h2><p>Jonathan Barzilai and Jonathan M. Borwein. Two-point step size gradient methods. IMA Journal of Numerical Analysis, 8(1):141–148, 1988. Dimitri P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, Belmont, MA, second edition, 1999. Ernesto G. Birgin, Jos´ Mario Mart´nez, and Marcos Raydan. Nonmonotone spectral projected e ı gradient methods on convex sets. SIAM Journal on Optimization, 10(4):1196–1211, 2000. Bernhard E. Boser, Isabelle Guyon, and Vladimir N. Vapnik. A training algorithm for optimal margin classiﬁers. In David Haussler, editor, Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory, pages 144–152. ACM Press, Pittsburgh, PA, 1992. Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines, 2001. URL http://www.csie.ntu.edu.tw/∼cjlin/libsvm. 1489  Z ANNI , S ERAFINI AND Z ANGHIRATI  Pai-Hsuen Chen, Rong-En Fan, and Chih-Jen Lin. A study on SMO-type decomposition methods for support vector machines. Technical report, Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan, 2005. To appear on IEEE Transaction on Neural Networks, 2006. Ronan Collobert and Samy Bengio. SVMTorch: Support vector machines for large-scale regression problems. Journal of Machine Learning Research, 1:143–160, 2001. Ronan Collobert, Samy Bengio, and Yoshua Bengio. A parallel mixture of SVMs for very large scale problems. Neural Computation, 14(5):1105–1114, 2002. Nello Cristianini and John Shawe-Taylor. An Introduction to Support Vector Machines and other Kernel-Based Learning Methods. Cambridge University Press, 2000. Yu-Hong Dai and Roger Fletcher. New algorithms for singly linearly constrained quadratic programs subject to lower and upper bounds. Mathematical Programming, 106(3):403–421, 2006. Yu-Hong Dai and Roger Fletcher. Projected Barzilai-Borwein methods for large-scale boxconstrained quadratic programming. Numerische Mathematik, 100(1):21–47, 2005. Jian-Xiong Dong, Adam Krzyzak, and Ching Y. Suen. A fast parallel optimization for training support vector machine. In P. Perner and A. Rosenfeld, editors, Proceedings of 3rd International Conference on Machine Learning and Data Mining, volume 17, pages 96–105. Springer Lecture Notes in Artiﬁcial Intelligence, Leipzig, Germany, 2003. Rong-En Fan, Pai-Hsuen Chen, and Chih-Jen Lin. Working set selection using second order information for training Support Vector Machines. Journal of Machine Learning Research, 6: 1889–1918, 2005. Hans Peter Graf, Eric Cosatto, L´ on Bottou, Igor Dourdanovic, and Vladimir N. Vapnik. Parallel e support vector machines: the Cascade SVM. In Lawrence Saul, Yair Weiss, and L´ on Bottou, e editors, Advances in Neural Information Processing Systems, volume 17. MIT Press, 2005. Chih-Wei Hsu and Chih-Jen Lin. A simple decomposition method for support vector machines. Machine Learning, 46:291–314, 2002. Don Hush and Clint Scovel. Polynomial-time decomposition algorithms for support vector machines. Machine Learning, 51:51–71, 2003. Throstem Joachims. Making large-scale SVM learning practical. In Bernard Sch¨ lkopf, C.J.C. o Burges, and Alex Smola, editors, Advances in Kernel Methods – Support Vector Learning. MIT Press, Cambridge, MA, 1998. S. Sathiya Keerthi and Elmer G. Gilbert. Convergence of a generalized SMO algorithm for SVM classiﬁer design. Machine Learning, 46:351–360, 2002. Chih-Jen Lin. On the convergence of the decomposition method for support vector machines. IEEE Transactions on Neural Networks, 12:1288–1298, 2001a. 1490  PARALLEL S OFTWARE FOR T RAINING L ARGE S CALE SVM S  Chih-Jen Lin. Linear convergence of a decomposition method for support vector machines. Technical report, Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan, 2001b. Chih-Jen Lin. Asymptotic convergence of an SMO algorithm without any assumptions. IEEE Transactions on Neural Networks, 13:248–250, 2002. Message Passing Interface Forum. MPI: A message-passing interface standard (version 1.2). International Journal of Supercomputing Applications, 8(3/4), 1995. URL http://www.mpi-forum. org. Also available as Technical Report CS-94-230, Computer Science Dept., University of Tennesse, Knoxville, TN. Bruce A. Murtagh and Michael A. Saunders. MINOS 5.5 user’s guide. Technical report, Department of Operation Research, Stanford University, Stanford CA, 1998. Edgar Osuna, Robert Freund, and Girosi Federico. Training support vector machines: an application to face detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR97), pages 130–136. IEEE Computer Society, New York, 1997. Laura Palagi and Marco Sciandrone. On the convergence of a modiﬁed version of SVMlight algorithm. Optimization Methods and Software, 20:317–334, 2005. Panos M. Pardalos and Naina Kovoor. An algorithm for a singly constrained class of quadratic programs subject to upper and lower bounds. Mathematical Programming, 46:321–328, 1990. John C. Platt. Fast training of support vector machines using sequential minimal optimization. In Bernard Sch¨ lkopf, C.J.C. Burges, and Alex Smola, editors, Advances in Kernel Methods – o Support Vector Learning. MIT Press, Cambridge, MA, 1998. Valeria Ruggiero and Luca Zanni. A modiﬁed projection algorithm for large strictly convex quadratic programs. Journal of Optimization Theory and Applications, 104(2):281–299, 2000a. Valeria Ruggiero and Luca Zanni. Variable projection methods for large convex quadratic programs. In Donato Trigiante, editor, Recent Trends in Numerical Analysis, volume 3 of Advances in the Theory of Computational Mathematics, pages 299–313. Nova Science Publisher, 2000b. Thomas Seraﬁni and Luca Zanni. On the working set selection in gradient projection-based decomposition techniques for support vector machines. Optimization Methods and Software, 20: 583–596, 2005. Thomas Seraﬁni, Gaetano Zanghirati, and Luca Zanni. Gradient projection methods for quadratic programs and applications in training support vector machines. Optimization Methods and Software, 20:353–378, 2005. Alex J. Smola. pr LOQO optimizer, 1997. URL http://www.kernel-machines.org/code/ prloqo.tar.gz. Ivor W. Tsang, James T. Kwok, and Pak-Ming Cheung. Core vector machines: fast SVM training on very large data sets. Journal of Machine Learning Research, 6(4):363–392, 2005. 1491  Z ANNI , S ERAFINI AND Z ANGHIRATI  Vladimir N. Vapnik. Statistical Learning Theory. John Wiley and Sons, New York, 1998. Gaetano Zanghirati and Luca Zanni. A parallel solver for large quadratic programs in training support vector machines. Parallel Computing, 29:535–551, 2003. Luca Zanni. An improved gradient projection-based decomposition technique for support vector machines. Computational Management Science, 3(2):131–145, 2006.  1492</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
