<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-75" href="../jmlr2013/jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">jmlr2013-75</a> <a title="jmlr-2013-75-reference" href="#">jmlr2013-75-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</h1>
<br/><p>Source: <a title="jmlr-2013-75-pdf" href="http://jmlr.org/papers/volume14/riihimaki13a/riihimaki13a.pdf">pdf</a></p><p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><br/>
<h2>reference text</h2><p>Kian Ming A. Chai. Variational multinomial logit Gaussian process. Journal of Machine Learning Research, 13:1745–1808, 2012. Botond Cseke and Tom Heskes. Approximate marginals in latent Gaussian models. Journal of Machine Learning Research, 12:417–454, 2011. Simon Duane, A. D. Kennedy, Brian J. Pendleton, and Duncan Roweth. Hybrid Monte Carlo. Physics Letters B, 195(2):216–222, 1987. Andrew Frank and Arthur Asuncion. UCI Machine Learning Repository. University of California, Irvine, School of Information and Computer Sciences, 2010. URL http://archive.ics.uci. edu/ml. Mark Girolami and Simon Rogers. Variational Bayesian multinomial probit regression with Gaussian process priors. Neural Computation, 18:1790–1817, 2006. Mark Girolami and Mingjun Zhong. Data integration for classiﬁcation problems employing Gaussian process priors. In Advances in Neural Information Processing Systems 19, pages 465–472. The MIT Press, 2007. Ricardo Henao and Ole Winther. PASS-GP: Predictive active set selection for Gaussian processes. In IEEE International Workshop on Machine Learning for Signal Processing, pages 148–153, 2010. Daniel Hern´ ndez-Lobato, Jos´ M. Hern´ ndez-Lobato, and Pierre Dupont. Robust multi-class a e a Gaussian process classiﬁcation. In Advances in Neural Information Processing Systems 24, pages 280–288, 2011. Jonathan J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(5):550–554, 1994. Hyun-Chul Kim and Zoubin Ghahramani. Bayesian Gaussian process classiﬁcation with the EM-EP algorithm. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(12):1948–1959, 2006. Malte Kuss and Carl E. Rasmussen. Assessing approximate inference for binary Gaussian process classiﬁcation. Journal of Machine Learning Research, 6:1679–1704, 2005. Thomas P. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, Massachusetts Institute of Technology, 2001a. Thomas P. Minka. Expectation Propagation for approximative Bayesian inference. In Proceedings of the 17th Conference on Uncertainty in Artiﬁcial Intelligence, pages 362–369. Morgan Kaufmann, San Francisco, CA, 2001b. Thomas P. Minka. Divergence measures and message passing. Technical report, Microsoft Research, Cambridge, 2005. 107  ¨ ¨ R IIHIM AKI , J YL ANKI AND V EHTARI  Thomas P. Minka and John Lafferty. Expectation-propagation for the generative aspect model. In Proceedings of the 18th Conference on Uncertainty in Artiﬁcial Intelligence, pages 352–359. Morgan Kaufmann, San Francisco, CA, 2002. Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, 1996. Radford M. Neal. Regression and classiﬁcation using Gaussian process priors (with discussion). In Bayesian Statistics 6, pages 475–501. Oxford University Press, 1998. Hannes Nickisch and Carl E. Rasmussen. Approximations for binary Gaussian process classiﬁcation. Journal of Machine Learning Research, 9:2035–2078, 2008. Manfred Opper and Ole Winther. Expectation consistent approximate inference. Journal of Machine Learning Research, 6:2177–2204, 2005. Yuan Qi, Thomas P. Minka, Rosalind W. Picard, and Zoubin Ghahramani. Predictive automatic relevance determination by expectation propagation. In Proceedings of the 21st International Conference on Machine Learning, pages 671–678, 2004. Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2006. H˚ vard Rue, Sara Martino, and Nicolas Chopin. Approximate Bayesian inference for latent Gausa sian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society (Series B), 71(2):319–392, 2009. Matthias Seeger. Expectation propagation for exponential families. Technical report, Max Planck Institute for Biological Cybernetics, T¨ bingen, Germany, 2005. u Matthias Seeger and Michael Jordan. Sparse Gaussian process classiﬁcation with multiple classes. Technical report, University of California, Berkeley, CA, 2004. Matthias Seeger, Neil Lawrence, and Ralf Herbrich. Efﬁcient nonparametric Bayesian modelling with sparse Gaussian process approximations. Technical report, Max Planck Institute for Biological Cybernetics, T¨ bingen, Germany, 2006. u Alexander Smola, Vishy Vishwanathan, and Eleazar Eskin. Laplace propagation. In Advances in Neural Information Processing Systems 16. The MIT Press, 2004. Edward Snelson and Zoubin Ghahramani. Compact approximations to Bayesian predictive distributions. In Proceedings of the 22nd International Conference on Machine Learning, pages 840–847, 2005. Luke Tierney and Joseph B. Kadane. Accurate approximations for posterior moments and marginal densities. Journal of the American Statistical Association, 81(393):82–86, 1986. Marcel van Gerven, Botond Cseke, Robert Oostenveld, and Tom Heskes. Bayesian source localization with the multivariate Laplace prior. In Advances in Neural Information Processing Systems 22, pages 1901–1909, 2009. 108  G AUSSIAN P ROCESS C LASSIFICATION WITH A M ULTINOMIAL P ROBIT L IKELIHOOD  Aki Vehtari and Jouko Lampinen. Bayesian model assessment and comparison using crossvalidation predictive densities. Neural Computation, 14(10):2439–2468, 2002. Christopher K. I. Williams and David Barber. Bayesian classiﬁcation with Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342–1351, 1998.  109</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
