<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-93" href="../jmlr2013/jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">jmlr2013-93</a> <a title="jmlr-2013-93-reference" href="#">jmlr2013-93-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</h1>
<br/><p>Source: <a title="jmlr-2013-93-pdf" href="http://jmlr.org/papers/volume14/urry13a/urry13a.pdf">pdf</a></p><p>Author: Matthew J. Urry, Peter Sollich</p><p>Abstract: We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance functions of Gaussian processes. In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to signiﬁcant variation in the prior variance across vertices, which is undesirable from a probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation show that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is signiﬁcantly more accurate than previous approximations and should become exact in the limit of large random graphs. Keywords: Gaussian process, generalisation error, learning curve, cavity method, belief propagation, graph, random walk kernel</p><br/>
<h2>reference text</h2><p>S. Amari, N. Fujita, and S. Shinomoto. Four types of learning curves. Neural Comput., 4(4): 605–618, 1992. C. Bishop. Pattern Recognition And Machine Learning. Springer, New York, NY, USA., 2nd edition, 2007. T. Britton, M. Deijfen, and A. Martin-Loeff. Generating simple random graphs with prescribed degree distribution. J. Stat. Phys., 124(6):1377–1397, 2006. F. K. Chung. Spectral Graph Theory, volume 92 of Regional Conference Series In Mathematics. American Mathematical Society, Dec 1996. F. K. Chung and S. T. Yau. Coverings, heat kernels and spanning trees. Electronic Journal Of Combinatorics, 6:R12, 1999. N. Cristianini and J. Shawe-Taylor. An Introduction To Support Vector Machines And Other KernelBased Learning Methods. Cambridge University Press, 2000. P. Erd˝ s and A. R´ nyi. On random graphs, I. Publicationes Mathematicae (Debrecen), 6:290–297, o e 1959. J. Freeman and D. Saad. Dynamics of on-line learning in radial basis networks. Phys. Rev. E, 56 (1):907–918, Jul 1997. M. G. Genton. Classes of kernels for machine learning: A statistics perspective. Journal Of Machine Learning Research, 2(2):299–312, 2002. 1832  G AUSSIAN P ROCESSES ON R ANDOM G RAPHS  W.W. Hager. Updating the inverse of a matrix. SIAM Rev., 31(2):221–239, 1989. M. S. Handcock and M. L. Stein. A Bayesian analysis of Kriging. Technometrics, 35(4):403–410, 1993. D. Haussler, M. Kearns, H. S. Seung, and N. Tishby. Rigorous learning curve bounds from statistical mechanics. Mach. Learn., 25(2–3):195–236, 1996. J. P. C. Kleijnen. Kriging metamodeling in simulation: A review. European Journal Of Operational Research, 192(3):707 – 716, 2009. R. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete structures. In C. Sammut and Hoffmann A. G., editors, Proceedings Of The 19th International Conference On Machine Learning (ICML), pages 315–322, San Francisco, CA, USA., 2002. Morgan Kaufmann. R. K¨ hn and J. van Mourik. Spectra of modular and small-world matrices. J. Phys. A Math. Gen., u 44(16):165205, 2011. D. Malzahn and M. Opper. Learning curves and bootstrap estimates for inference with Gaussian processes: A statistical mechanics study. Complexity, 8(4):57–63, 2003. D. Malzahn and M. Opper. A statistical physics approach for the analysis of machine learning algorithms on real data. J. Stat. Mech. Theor. Exp., 2005(11):P11001, 2005. B. D. McKay. The expected eigenvalue distribution of a large regular graph. Linear Algebra And Its Applications, 40:203–216, 1981. J. R. Meinhold and N. D. Singpurwalla. Understanding the Kalman ﬁlter. The American Statistician, 37(2):123–127, 1983. M. M´ zard and G. Parisi. The Bethe lattice spin glass revisited. Eur. Phys. J. B, 20(2):217–233, e 2001. M. M´ zard and G. Parisi. The cavity method at zero temperature. J. Stat. Phys., 111(1–2):1–34, e 2003. M. M´ zard, G. Parisi, and M. Virasoro. Spin Glass Theory And Beyond, volume 9 of World Scientiﬁc e Lecture Notes In Physics. World Scientiﬁc, Singapore, 1987. C. Monthus and C. Texier. Random walk on the Bethe lattice and hyperbolic Brownian motion. J. Phys. A Math. Gen., 29:2399–2409, 1996. K. R. M¨ ller, S. Mika, G. R¨ tsch, K. Tsuda, and B. Sch¨ lkopf. An introduction to kernel-based u a o learning algorithms. IEEE Transactions On Neural Networks, 12(2):181–201, 2001. R. M. Neal. Bayesian Learning For Neural Networks, volume 118 of Lecture Notes In Statistics. Springer-Verlag, Berlin, Germany, 1996. M. Opper and D. Haussler. Bounds for predictive errors in the statistical mechanics of supervised learning. Phys. Rev. Lett., 75(20):3772–3775, 1995. 1833  U RRY  AND  S OLLICH  M. Opper and D. Malzahn. A variational approach to learning curves. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances In Neural Information Processing Systems, volume 14, pages 463–469, Cambridge, MA, USA., 2002. The MIT Press. M. Opper and F. Vivarelli. General bounds on Bayes errors for regression with Gaussian processes. In M. S. Kearns, S. A. Solla, and D A Cohn, editors, Advances In Neural Information Processing Systems, volume 11, pages 302–308, Cambridge, MA, USA., 1999. The MIT Press. C. E. Rasmussen and C. K. I. Williams. Gaussian Processes For Machine Learning. MIT Press, Nov 2005. T. Rogers, C. Vicente, K. Takeda, and I. Castillo. Spectral density of random graphs with topological constraints. J. Phys. A Math. Gen., 43(19):195002, 2010. H. S. Seung, H. Sompolinsky, and N. Tishby. Statistical mechanics of learning from examples. Phys. Rev. A, 45(8):6056–6091, 1992. A. Smola and R. Kondor. Kernels and regularization on graphs. In B Sch¨ lkopf and M. K. Warmuth, o editors, Lect. Notes Artif. Int., volume 2777, pages 144–158, 2003. P. Sollich. Learning curves for Gaussian processes. In M. S. Kearns, S. A. Solla, and D A Cohn, editors, Advances In Neural Information Processing Systems, volume 11, pages 344–350, Cambridge, MA, USA., 1999a. The MIT Press. P. Sollich. Approximate learning curves for Gaussian processes. In Ninth International Conference On Artiﬁcial Neural Networks (ICANN99), pages 437–442, Edison, NJ, USA., 1999b. Institute of Electrical Engineers Inspec inc. P. Sollich. Gaussian process regression with mismatched models. In S Becker T G Dietterich and Z Ghahramani, editors, Advances In Neural Information Processing Systems, volume 14, pages 519–526, Cambridge, MA, USA., 2002. The MIT Press. P. Sollich and A. Halees. Learning curves for Gaussian process regression: Approximations and bounds. Neural Comput., 14(6):1393–1428, 2002. P. Sollich and C. K. I. Williams. Using the equivalent kernel to understand Gaussian process regression. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances In Neural Information Processing Systems, volume 17, pages 1313–1320, Cambridge, MA, USA., 2005. The MIT Press. P. Sollich, M. J. Urry, and C. Coti. Kernels and learning curves for Gaussian process regression on random graphs. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances In Neural Information Processing Systems, volume 22, pages 1723–1731, Red Hook, NY, USA., 2009. Curran Associates Inc. M. J. Urry and P. Sollich. Exact learning curves for Gaussian process regression on large random graphs. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances In Neural Information Processing Systems, volume 23, pages 2316–2324, Red Hook, NY, USA., 2010. Curran Associates Inc. 1834  G AUSSIAN P ROCESSES ON R ANDOM G RAPHS  M. J. Urry and P. Sollich. Replica theory for learning curves for gaussian processes on random graphs. J. Phys. A Math. Gen., 45(42):425005, 2012. T. L. H. Watkin, A. Rau, and M. Biehl. The statistical mechanics of learning a rule. Rev. Mod. Phys., 65(2):499–556, 1993. C. Williams and F. Vivarelli. Upper and lower bounds on the learning curve for Gaussian processes. Mach. Learn., 40:77–102, 2000.  1835</p>
<br/>
<br/><br/><br/></body>
</html>
