<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-52" href="../jmlr2013/jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">jmlr2013-52</a> <a title="jmlr-2013-52-reference" href="#">jmlr2013-52-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</h1>
<br/><p>Source: <a title="jmlr-2013-52-pdf" href="http://jmlr.org/papers/volume14/escalante13a/escalante13a.pdf">pdf</a></p><p>Author: Alberto N. Escalante-B., Laurenz Wiskott</p><p>Abstract: Supervised learning from high-dimensional data, for example, multimedia data, is a challenging task. We propose an extension of slow feature analysis (SFA) for supervised dimensionality reduction called graph-based SFA (GSFA). The algorithm extracts a label-predictive low-dimensional set of features that can be post-processed by typical supervised algorithms to generate the ﬁnal label or class estimation. GSFA is trained with a so-called training graph, in which the vertices are the samples and the edges represent similarities of the corresponding labels. A new weighted SFA optimization problem is introduced, generalizing the notion of slowness from sequences of samples to such training graphs. We show that GSFA computes an optimal solution to this problem in the considered function space and propose several types of training graphs. For classiﬁcation, the most straightforward graph yields features equivalent to those of (nonlinear) Fisher discriminant analysis. Emphasis is on regression, where four different graphs were evaluated experimentally with a subproblem of face detection on photographs. The method proposed is promising particularly when linear models are insufﬁcient as well as when feature selection is difﬁcult. Keywords: slow feature analysis, feature extraction, classiﬁcation, regression, pattern recognition, training graphs, nonlinear dimensionality reduction, supervised learning, implicitly supervised, high-dimensional data, image analysis</p><br/>
<h2>reference text</h2><p>T. Adali and S. Haykin. Adaptive Signal Processing: Next Generation Solutions. Adaptive and Learning Systems for Signal Processing, Communications and Control Series. John Wiley & Sons, 2010. P. Berkes. Pattern recognition with slow feature analysis. Cognitive Sciences EPrint Archive (CogPrints), February 2005a. URL http://cogprints.org/4104/. P. Berkes. Handwritten digit recognition with nonlinear ﬁsher discriminant analysis. In ICANN, volume 3697 of LNCS, pages 285–287. Springer Berlin/Heidelberg, 2005b. P. Berkes and L. Wiskott. Slow feature analysis yields a rich repertoire of complex cell properties. Journal of Vision, 5(6):579–602, 2005. W. B¨ hmer, S. Gr¨ new¨ lder, H. Nickisch, and K. Obermayer. Generating feature spaces for linear o u a algorithms with regularized sparse kernel slow feature analysis. Machine Learning, 89(1):67–86, 2012. A. Bray and D. Martinez. Kernel-based extraction of slow features: Complex cells learn disparity and translation invariance from natural images. In NIPS, volume 15, pages 253–260, Cambridge, MA, 2003. MIT Press. C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27, 2011. A. N. Escalante-B. and L. Wiskott. Gender and age estimation from synthetic face images with hierarchical slow feature analysis. In International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems, pages 240–249, 2010. A. N. Escalante-B. and L. Wiskott. Heuristic evaluation of expansions for Non-Linear Hierarchical Slow Feature Analysis. In Proc. The 10th International Conference on Machine Learning and Applications, pages 133–138, Los Alamitos, CA, USA, 2011. 3716  H OW TO S OLVE C LASSIFICATION AND R EGRESSION P ROBLEMS WITH SFA  A. N. Escalante-B. and L. Wiskott. Slow feature analysis: Perspectives for technical applications of a versatile learning algorithm. K¨ nstliche Intelligenz [Artiﬁcial Intelligence], 26(4):341–348, u 2012. M. Fink, R. Fergus, and A. Angelova. Caltech 10,000 web faces. URL http://www.vision. caltech.edu/Image_Datasets/Caltech_10K_WebFaces/. R. A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7: 179–188, 1936. P. F¨ ldi´ k. Learning invariance from transformation sequences. Neural Computation, 3(2):194– o a 200, 1991. M. Franzius, H. Sprekeler, and L. Wiskott. Slowness and sparseness lead to place, head-direction, and spatial-view cells. PLoS Computational Biology, 3(8):1605–1622, 2007. M. Franzius, N. Wilbert, and L. Wiskott. Invariant object recognition with slow feature analysis. In Proc. of the 18th ICANN, volume 5163 of LNCS, pages 961–970. Springer, 2008. M. Franzius, N. Wilbert, and L. Wiskott. Invariant object recognition and pose estimation with slow feature analysis. Neural Computation, 23(9):2289–2323, 2011. W. Gao, B. Cao, S. Shan, X. Chen, D. Zhou, X. Zhang, and D. Zhao. The CAS-PEAL largescale chinese face database and baseline evaluations. IEEE Transactions on Systems, Man, and Cybernetics, Part A, 38(1):149–161, 2008. G. Guo and G. Mu. Simultaneous dimensionality reduction and human age estimation via kernel partial least squares regression. In Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pages 657–664, 2011. X. He and P. Niyogi. Locality Preserving Projections. In Neural Information Processing Systems, volume 16, pages 153–160, 2003. G. E. Hinton. Connectionist learning procedures. Artiﬁcial Intelligence, 40(1-3):185–234, 1989. O. Jesorsky, K. J. Kirchberg, and R. Frischholz. Robust face detection using the hausdorff distance. In Proc. of Third International Conference on Audio- and Video-Based Biometric Person Authentication, pages 90–95. Springer-Verlag, 2001. S. Klampﬂ and W. Maass. Replacing supervised classiﬁcation learning by Slow Feature Analysis in spiking neural networks. In Proc. of NIPS 2009: Advances in Neural Information Processing Systems, volume 22, pages 988–996. MIT Press, 2010. P. Koch, W. Konen, and K. Hein. Gesture recognition on few training data using slow feature analysis and parametric bootstrap. In International Joint Conference on Neural Networks, pages 1 –8, 2010. T. Kuhnl, F. Kummert, and J. Fritsch. Monocular road segmentation using slow feature analysis. In Intelligent Vehicles Symposium, IEEE, pages 800–806, june 2011. 3717  E SCALANTE -B. AND W ISKOTT  N. Kumar, P. N. Belhumeur, and S. K. Nayar. FaceTracer: A search engine for large collections of images with faces. In European Conference on Computer Vision (ECCV), pages 340–353, 2008. G. Mitchison. Removing time variation with the anti-hebbian differential synapse. Neural Computation, 3(3):312–320, 1991. N. M. Mohamed and H. Mahdi. A simple evaluation of face detection algorithms using unpublished static images. In 10th International Conference on Intelligent Systems Design and Applications, pages 1–5, 2010. P. J. Phillips, P. J. Flynn, T. Scruggs, K. W. Bowyer, J. Chang, K. Hoffman, J. Marques, J. Min, and W. Worek. Overview of the face recognition grand challenge. In Proc. of IEEE Conference on Computer Vision and Pattern Recognition, volume 1, pages 947–954, Washington, DC, USA, 2005. IEEE Computer Society. E. M. Rehn. On the slowness principle and learning in hierarchical temporal memory. Master’s thesis, Bernstein Center for Computational Neuroscience, 2013. I. Rish, G. Grabarnik, G. Cecchi, F. Pereira, and G. J. Gordon. Closed-form supervised dimensionality reduction with generalized linear models. In Proc. of the 25th ICML, pages 832–839, New York, NY, USA, 2008. ACM. H. Sprekeler. On the relation of slow feature analysis and laplacian eigenmaps. Neural Computation, 23(12):3287–3302, 2011. H. Sprekeler and L. Wiskott. A theory of slow feature analysis for transformation-based input signals with an application to complex cells. Neural Computation, 23(2):303–335, 2011. J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The German Trafﬁc Sign Recognition Benchmark: A multi-class classiﬁcation competition. In International Joint Conference on Neural Networks, pages 1453–1460, 2011. M. Sugiyama. Local ﬁsher discriminant analysis for supervised dimensionality reduction. In Proc. of the 23rd ICML, pages 905–912, 2006. M. Sugiyama, T. Id´ , S. Nakajima, and J. Sese. Semi-supervised local ﬁsher discriminant analysis e for dimensionality reduction. Machine Learning, 78(1-2):35–61, 2010. W. Tang and S. Zhong. Computational Methods of Feature Selection, chapter Pairwise ConstraintsGuided Dimensionality Reduction. Chapman and Hall/CRC, 2007. R. Vollgraf and K. Obermayer. Sparse optimization for second order kernel methods. In International Joint Conference on Neural Networks, pages 145–152, 2006. L. Wiskott. Learning invariance manifolds. In Proc. of 5th Joint Symposium on Neural Computation, San Diego, CA, USA, volume 8, pages 196–203. Univ. of California, 1998. L. Wiskott. Slow feature analysis: A theoretical analysis of optimal free responses. Neural Computation, 15(9):2147–2177, 2003. 3718  H OW TO S OLVE C LASSIFICATION AND R EGRESSION P ROBLEMS WITH SFA  L. Wiskott and T. Sejnowski. Slow feature analysis: Unsupervised learning of invariances. Neural Computation, 14(4):715–770, 2002. D. Zhang, Z.-H. Zhou, and S. Chen. Semi-supervised dimensionality reduction. In Proc. of the 7th SIAM International Conference on Data Mining, 2007. T. Zhang, D. Tao, X. Li, and J. Yang. Patch alignment for dimensionality reduction. IEEE Transactions on Knowledge and Data Engineering, 21(9):1299–1313, 2009. Z. Zhang and D. Tao. Slow feature analysis for human action recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(3):436 –450, 2012.  3719</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
