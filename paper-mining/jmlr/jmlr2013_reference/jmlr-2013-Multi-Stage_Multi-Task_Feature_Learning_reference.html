<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-72" href="../jmlr2013/jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">jmlr2013-72</a> <a title="jmlr-2013-72-reference" href="#">jmlr2013-72-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</h1>
<br/><p>Source: <a title="jmlr-2013-72-pdf" href="http://jmlr.org/papers/volume14/gong13a/gong13a.pdf">pdf</a></p><p>Author: Pinghua Gong, Jieping Ye, Changshui Zhang</p><p>Abstract: Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an ℓ0 -type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel non-convex regularizer. To solve the non-convex optimization problem, we propose a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm; we also provide intuitive interpretations, detailed convergence and reproducibility analysis for the proposed algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms. Keywords: multi-task learning, multi-stage, non-convex, sparse learning</p><br/>
<h2>reference text</h2><p>A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243–272, 2008. A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009. 3006  M ULTI -S TAGE M ULTI -TASK F EATURE L EARNING  Algorithm 2: MSMTFL-CapL1: Multi-Stage Multi-Task Feature Learning for solving the capped-ℓ1 regularized feature learning problem in Equation (3) 1 2 3  (0)  Initialize λ j = λ; for ℓ = 1, 2, · · · do ˆ Let W (ℓ) be a solution of the following problem: d  min  W ∈Rd×m 4  5  (ℓ)  (ℓ−1)  l(W ) + ∑ λ ji j=1  |w ji | .  (ℓ)  (ℓ)  Let λ ji = λI(|w ji | < θ) ( j = 1, · · · , d, i = 1, · · · , m), where w ji is the ( j, i)-th entry of ˆ ˆ (ℓ) and I(·) denotes the {0, 1}-valued indicator function. ˆ W end  Algorithm 3: MSMTFL-CapL1,L2: Multi-Stage Multi-Task Feature Learning for solving the capped-ℓ1 , ℓ2 regularized multi-task feature learning problem in Equation (4) 1 2 3  (0)  Initialize λ j = λ; for ℓ = 1, 2, · · · do ˆ Let W (ℓ) be a solution of the following problem: d  min  W ∈Rd×m 4 5  (ℓ−1)  l(W ) + ∑ λ j  wj  .  j=1  (ℓ) ˆ ˆ ˆ Let λ j = λI( (w(ℓ) ) j < θ) ( j = 1, · · · , d), where (w(ℓ) ) j is the j-th row of W (ℓ) and I(·) denotes the {0, 1}-valued indicator function. end  D.P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 1999. J. Bi, T. Xiong, S. Yu, M. Dundar, and R. Rao. An improved multi-task learning approach with applications in medical diagnosis. Machine Learning and Knowledge Discovery in Databases, pages 117–132, 2008. E.J. Candes and T. Tao. Decoding by linear programming. IEEE Transactions on Information Theory, 51(12):4203–4215, 2005. R. Caruana. Multitask learning. Machine Learning, 28(1):41–75, 1997. J. Chen, J. Liu, and J. Ye. Learning incoherent sparse and low-rank patterns from multiple tasks. In SIGKDD, pages 1179–1188, 2010. D.L. Donoho, M. Elad, and V.N. Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Transactions on Information Theory, 52(1):6–18, 2006. T. Evgeniou and M. Pontil. Regularized multi–task learning. In SIGKDD, pages 109–117, 2004. 3007  G ONG , Y E AND Z HANG  J. Fan, L. Xue, and H. Zou. Strong oracle optimality of folded concave penalized estimation. ArXiv Preprint ArXiv:1210.5992, 2012. P. Gong, J. Ye, and C. Zhang. Robust multi-task feature learning. In SIGKDD, pages 895–903, 2012. P. Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye. A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems. In ICML, 2013a. P. Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye. GIST: General Iterative Shrinkage and Thresholding for Non-convex Sparse Learning. Tsinghua University, 2013b. URL http://www.public.asu. edu/˜jye02/Software/GIST. L. Grippo and M. Sciandrone. On the convergence of the block nonlinear gauss-seidel method under convex constraints. Operations Research Letters, 26(3):127–136, 2000. J. Huang and T. Zhang. The beneﬁt of group sparsity. The Annals of Statistics, 38(4):1978–2004, 2010. A. Jalali, P. Ravikumar, S. Sanghavi, and C. Ruan. A dirty model for multi-task learning. In NIPS, pages 964–972, 2010. S. Kim and E.P. Xing. Tree-guided group lasso for multi-task regression with structured sparsity. In ICML, pages 543–550, 2009. M. Kolar, J. Lafferty, and L. Wasserman. Union support recovery in multi-task learning. Journal of Machine Learning Research, 12:2415–2435, 2011. J. Liu, S. Ji, and J. Ye. Multi-task feature learning via efﬁcient ℓ2,1 -norm minimization. In UAI, pages 339–348, 2009. K. Lounici, M. Pontil, A.B. Tsybakov, and S. Van De Geer. Taking advantage of sparsity in multitask learning. In COLT, pages 73–82, 2009. S. Negahban and M.J. Wainwright. Joint support recovery under high-dimensional scaling: Beneﬁts and perils of ℓ1,∞ -regularization. In NIPS, pages 1161–1168, 2008. S. Negahban and M.J. Wainwright. Estimation of (near) low-rank matrices with noise and highdimensional scaling. The Annals of Statistics, 39(2):1069–1097, 2011. G. Obozinski, B. Taskar, and M.I. Jordan. Multi-task feature selection. Technical report, Statistics Department, UC Berkeley, 2006. G. Obozinski, M.J. Wainwright, and M.I. Jordan. Support union recovery in high-dimensional multivariate regression. Annals of Statistics, 39(1):1–47, 2011. S. Parameswaran and K. Weinberger. Large margin multi-task metric learning. In NIPS, pages 1867–1875, 2010. N. Quadrianto, A. Smola, T. Caetano, SVN Vishwanathan, and J. Petterson. Multitask learning without label correspondences. In NIPS, pages 1957–1965, 2010. 3008  M ULTI -S TAGE M ULTI -TASK F EATURE L EARNING  R.T. Rockafellar. Convex Analysis. Princeton University Press (Princeton, NJ), 1970. A. Schwaighofer, V. Tresp, and K. Yu. Learning gaussian process kernels via hierarchical bayes. In NIPS, pages 1209–1216, 2005. X. Shen, W. Pan, and Y. Zhu. Likelihood-based selection and sharp parameter estimation. Journal of the American Statistical Association, 107(497):223–232, 2012. R. Tibshirani. The lasso problem and uniqueness. Electronic Journal of Statistics, 7:1456–1490, 2013. J.F. Toland. A duality principle for non-convex optimisation and the calculus of variations. Archive for Rational Mechanics and Analysis, 71(1):41–61, 1979. P. Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization. Journal of Optimization Theory and Applications, 109(3):475–494, 2001. S.A. Van De Geer and P. B¨ hlmann. On the conditions used to prove oracle results for the lasso. u Electronic Journal of Statistics, 3:1360–1392, 2009. S. Wright, R. Nowak, and M. Figueiredo. Sparse reconstruction by separable approximation. IEEE Transactions on Signal Processing, 57(7):2479–2493, 2009. X. Yang, S. Kim, and E. Xing. Heterogeneous multitask learning with joint sparsity constraints. In NIPS, pages 2151–2159, 2009. K. Yu, V. Tresp, and A. Schwaighofer. Learning gaussian processes from multiple tasks. In ICML, pages 1012–1019, 2005. A.L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915– 936, 2003. C.H. Zhang and J. Huang. The sparsity and bias of the lasso selection in high-dimensional linear regression. The Annals of Statistics, 36(4):1567–1594, 2008. C.H. Zhang and T. Zhang. A general theory of concave regularization for high dimensional sparse estimation problems. Statistical Science, 2012. J. Zhang, Z. Ghahramani, and Y. Yang. Learning multiple related tasks using latent independent component analysis. In NIPS, pages 1585–1592, 2006. T. Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models. In NIPS, pages 1921–1928, 2008. T. Zhang. Some sharp performance bounds for least squares regression with ℓ1 regularization. The Annals of Statistics, 37:2109–2144, 2009. T. Zhang. Analysis of multi-stage convex relaxation for sparse regularization. Journal of Machine Learning Research, 11:1081–1107, 2010. T. Zhang. Multi-stage convex relaxation for feature selection. Bernoulli, 2012. 3009  G ONG , Y E AND Z HANG  Y. Zhang and D.Y. Yeung. Multi-task learning using generalized t process. Journal of Machine Learning Research - Proceedings Track, 9:964–971, 2010. Y. Zhang, D. Yeung, and Q. Xu. Probabilistic multi-task feature selection. In NIPS, pages 2559– 2567, 2010. J. Zhou, J. Chen, and J. Ye. Clustered multi-task learning via alternating structure optimization. In NIPS, pages 702–710, 2011. H. Zou and R. Li. One-step sparse estimates in nonconcave penalized likelihood models. Annals of Statistics, 36(4):1509, 2008.  3010</p>
<br/>
<br/><br/><br/></body>
</html>
