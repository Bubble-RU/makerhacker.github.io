<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>94 jmlr-2013-Ranked Bandits in Metric Spaces: Learning Diverse Rankings over Large Document Collections</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-94" href="../jmlr2013/jmlr-2013-Ranked_Bandits_in_Metric_Spaces%3A_Learning_Diverse_Rankings_over_Large_Document_Collections.html">jmlr2013-94</a> <a title="jmlr-2013-94-reference" href="#">jmlr2013-94-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>94 jmlr-2013-Ranked Bandits in Metric Spaces: Learning Diverse Rankings over Large Document Collections</h1>
<br/><p>Source: <a title="jmlr-2013-94-pdf" href="http://jmlr.org/papers/volume14/slivkins13a/slivkins13a.pdf">pdf</a></p><p>Author: Aleksandrs Slivkins, Filip Radlinski, Sreenivas Gollapudi</p><p>Abstract: Most learning to rank research has assumed that the utility of different documents is independent, which results in learned ranking functions that return redundant results. The few approaches that avoid this have rather unsatisfyingly lacked theoretical foundations, or do not scale. We present a learning-to-rank formulation that optimizes the fraction of satisﬁed users, with several scalable algorithms that explicitly takes document similarity and ranking context into account. Our formulation is a non-trivial common generalization of two multi-armed bandit models from the literature: ranked bandits (Radlinski et al., 2008) and Lipschitz bandits (Kleinberg et al., 2008b). We present theoretical justiﬁcations for this approach, as well as a near-optimal algorithm. Our evaluation adds optimizations that improve empirical performance, and shows that our algorithms learn orders of magnitude more quickly than previous approaches. Keywords: online learning, clickthrough data, diversity, multi-armed bandits, contextual bandits, regret, metric spaces</p><br/>
<h2>reference text</h2><p>Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efﬁcient algorithm for bandit linear optimization. In 21th Conf. on Learning Theory (COLT), pages 263–274, 2008. Rajeev Agrawal. The continuum-armed bandit problem. SIAM J. Control and Optimization, 33(6): 1926–1951, 1995. ´ ´e David J. Aldous. Exchangeability and related topics. In Ecole d’Et´ de Probabilit´ s de Saint-Flour e XIII, pages 1–198, 1985. Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. J. of Machine Learning Research (JMLR), 3:397–422, 2002. Preliminary version in 41st IEEE FOCS, 2000. Peter Auer, Nicol` Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit o problem. Machine Learning, 47(2-3):235–256, 2002a. Preliminary version in 15th ICML, 1998. Peter Auer, Nicol` Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multio armed bandit problem. SIAM J. Comput., 32(1):48–77, 2002b. Preliminary version in 36th IEEE FOCS, 1995. Peter Auer, Ronald Ortner, and Csaba Szepesv´ ri. Improved rates for the stochastic continuuma armed bandit problem. In 20th Conf. on Learning Theory (COLT), pages 454–468, 2007. Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. J. of Computer and System Sciences, 74(1):97–114, February 2008. Preliminary version in 36th ACM STOC, 2004. Yair Bartal. Probabilistic approximations of metric spaces and its algorithmic applications. In IEEE Symp. on Foundations of Computer Science (FOCS), 1996. Dirk Bergemann and Juuso V¨ lim¨ ki. Bandit problems. In Steven Durlauf and Larry Blume, editors, a a The New Palgrave Dictionary of Economics, 2nd ed. Macmillan Press, 2006. S´ bastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multie armed bandit problems. Foundations and Trends in Machine Learning (Draft under submission), 2012. Available at www.princeton.edu/∼sbubeck/pub.html. S´ bastien Bubeck and R´ mi Munos. Open loop optimistic planning. In 23rd Conf. on Learning e e Theory (COLT), pages 477–489, 2010. S´ bastien Bubeck, R´ mi Munos, Gilles Stoltz, and Csaba Szepesvari. Online optimization in xe e armed bandits. J. of Machine Learning Research (JMLR), 12:1587–1627, 2011. Preliminary version in NIPS 2008. Christopher J. C. Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Gregory N. Hullender. Learning to rank using gradient descent. In Intl. Conf. on Machine Learning (ICML), pages 89–96, 2005. 433  S LIVKINS , R ADLINSKI AND G OLLAPUDI  Jaime G. Carbonell and Jade Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In ACM Intl. Conf. on Research and Development in Information Retrieval (SIGIR), pages 335–336, 1998. Nicol` Cesa-Bianchi and G´ bor Lugosi. Prediction, Learning, and Games. Cambridge Univ. Press, o a 2006. Wei Chu and Zoubin Ghahramani. Gaussian processes for ordinal regression. J. of Machine Learning Research, 6:1019–1041, 2005. Wei Chu, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandits with linear payoff functions. In 14thIntl. Conf. on Artiﬁcial Intelligence and Statistics (AISTATS), 2011. Varsha Dani, Thomas P. Hayes, and Sham Kakade. The price of bandit information for online optimization. In 20th Advances in Neural Information Processing Systems (NIPS), 2007. Jittat Fakcharoenphol, Satish Rao, and Kunal Talwar. A tight bound on approximating arbitrary metrics by tree metrics. J. of Computer and System Sciences, 69(3):485–497, 2004. Abraham Flaxman, Adam Kalai, and H. Brendan McMahan. Online convex optimization in the bandit setting: Gradient descent without a gradient. In 16th ACM-SIAM Symp. on Discrete Algorithms (SODA), pages 385–394, 2005. Daniel Golovin, Andreas Krause, and Matthew Streeter. Online learning of assignments. In Advances in Neural Information Processing Systems (NIPS), 2009. Anupam Gupta, Robert Krauthgamer, and James R. Lee. Bounded geometries, fractals, and low– distortion embeddings. In IEEE Symp. on Foundations of Computer Science (FOCS), 2003. Elad Hazan and Satyen Kale. Better algorithms for benign bandits. In 20th ACM-SIAM Symp. on Discrete Algorithms (SODA), pages 38–47, 2009. Elad Hazan and Nimrod Megiddo. Online learning with prior information. In 20th Conf. on Learning Theory (COLT), pages 499–513, 2007. Thorsten Joachims. Optimizing search engines using clickthrough data. In 8th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD), pages 133–142, 2002. Satyen Kale, Lev Reyzin, and Robert E. Schapire. Non-stochastic bandit slate problems. In 24th Advances in Neural Information Processing Systems (NIPS), pages 1054–1062, 2010. Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In 18th Advances in Neural Information Processing Systems (NIPS), 2004. Robert Kleinberg and Aleksandrs Slivkins. Sharp dichotomies for regret minimization in metric spaces. In 21st ACM-SIAM Symp. on Discrete Algorithms (SODA), 2010. Robert Kleinberg, Alexandru Niculescu-Mizil, and Yogeshwer Sharma. Regret bounds for sleeping experts and bandits. In 21st Conf. on Learning Theory (COLT), pages 425–436, 2008a. 434  R ANKED BANDITS IN M ETRIC S PACES  Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Multi-armed bandits in metric spaces. In 40th ACM Symp. on Theory of Computing (STOC), pages 681–690, 2008b. Levente Kocsis and Csaba Szepesvari. Bandit based Monte-Carlo planning. In 17th European Conf. on Machine Learning (ECML), pages 282–293, 2006. Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied Mathematics, 6:4–22, 1985. John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In 21st Advances in Neural Information Processing Systems (NIPS), 2007. Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to personalized news article recommendation. In 19th Intl. World Wide Web Conf. (WWW), 2010. Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased ofﬂine evaluation of contextualbandit-based news article recommendation algorithms. In 4th ACM Intl. Conf. on Web Search and Data Mining (WSDM), 2011. Tyler Lu, D´ vid P´ l, and Martin P´ l. Showing relevant ads via Lipschitz context multi-armed a a a bandits. In 14thIntl. Conf. on Artiﬁcial Intelligence and Statistics (AISTATS), 2010. Odalric-Ambrym Maillard and R´ mi Munos. Online learning in adversarial lipschitz environments. e In European Conf. on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), pages 305–320, 2010. R´ mi Munos and Pierre-Arnaud Coquelin. Bandit algorithms for tree search. In 23rd Conf. on e Uncertainty in Artiﬁcial Intelligence (UAI), 2007. Sandeep Pandey, Deepak Agarwal, Deepayan Chakrabarti, and Vanja Josifovski. Bandits for taxonomies: A model-based approach. In SIAM Intl. Conf. on Data Mining (SDM), 2007. Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. Learning diverse rankings with multiarmed bandits. In 25th Intl. Conf. on Machine Learning (ICML), pages 784–791, 2008. Philippe Rigollet and Assaf Zeevi. Nonparametric bandits with covariates. In 23rd Conf. on Learning Theory (COLT), pages 54–66, 2010. Aleksandrs Slivkins. Contextual bandits with similarity information. http://arxiv.org/abs/0907.3986, 2009. Has been published in 24th COLT 2011. Aleksandrs Slivkins. Multi-armed bandits on implicit metric spaces. In 25th Advances in Neural Information Processing Systems (NIPS), 2011. Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In 27th Intl. Conf. on Machine Learning (ICML), pages 1015–1022, 2010. Matthew Streeter and Daniel Golovin. An online algorithm for maximizing submodular functions. In Advances in Neural Information Processing Systems (NIPS), pages 1577–1584, 2008. 435  S LIVKINS , R ADLINSKI AND G OLLAPUDI  Rangarajan K. Sundaram. Generalized bandit problems. In David Austen-Smith and John Duggan, editors, Social Choice and Strategic Decisions: Essays in Honor of Jeffrey S. Banks (Studies in Choice and Welfare), pages 131–162. Springer, 2005. First appeared as Working Paper, Stern School of Business, 2003. Michael J. Taylor, John Guiver, Stephen Robertson, and Tom Minka. Softrank: Optimizing nonsmooth rank metrics. In ACM Intl. Conf. on Web Search and Data Mining (WSDM), pages 77–86, 2008. Taishi Uchiya, Atsuyoshi Nakamura, and Mineichi Kudo. Algorithms for adversarial bandit problems with multiple plays. In 21st Intl. Conf. on Algorithmic Learning Theory (ALT), pages 375– 389, 2010. Chih-Chun Wang, Sanjeev R. Kulkarni, and H. Vincent Poor. Bandit problems with side observations. IEEE Trans. on Automatic Control, 50(3):338355, 2005. Yizao Wang, Jean-Yves Audibert, and R´ mi Munos. Algorithms for inﬁnitely many-armed bandits. e In Advances in Neural Information Processing Systems (NIPS), pages 1729–1736, 2008. Michael Woodroofe. A one-armed bandit problem with a concomitant variable. J. Amer. Statist. Assoc., 74(368), 1979.  436</p>
<br/>
<br/><br/><br/></body>
</html>
