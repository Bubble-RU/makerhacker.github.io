<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-10" href="../jmlr2013/jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">jmlr2013-10</a> <a title="jmlr-2013-10-reference" href="#">jmlr2013-10-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</h1>
<br/><p>Source: <a title="jmlr-2013-10-pdf" href="http://jmlr.org/papers/volume14/long13a/long13a.pdf">pdf</a></p><p>Author: Philip M. Long, Rocco A. Servedio</p><p>Abstract: We consider the problem of learning an unknown large-margin halfspace in the context of parallel computation, giving both positive and negative results. As our main positive result, we give a parallel algorithm for learning a large-margin halfspace, based on an algorithm of Nesterov’s that performs gradient descent with a momentum term. We show that this algorithm can learn an unknown γ-margin halfspace over n dimensions using ˜ n · poly(1/γ) processors and running in time O(1/γ) + O(log n). In contrast, naive parallel algorithms that learn a γ-margin halfspace in time that depends polylogarithmically on n have an inverse quadratic running time dependence on the margin parameter γ. Our negative result deals with boosting, which is a standard approach to learning large-margin halfspaces. We prove that in the original PAC framework, in which a weak learning algorithm is provided as an oracle that is called by the booster, boosting cannot be parallelized. More precisely, we show that, if the algorithm is allowed to call the weak learner multiple times in parallel within a single boosting stage, this ability does not reduce the overall number of successive stages of boosting needed for learning by even a single stage. Our proof is information-theoretic and does not rely on unproven assumptions. Keywords: PAC learning, parallel learning algorithms, halfspace learning, linear classiﬁers</p><br/>
<h2>reference text</h2><p>N. Alon and N. Megiddo. Parallel linear programming in ﬁxed dimension almost surely in constant time. J. ACM, 41(2):422–434, 1994. R. I. Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and random projection. Machine Learning, 63(2):161–182, 2006. P. Beame, S.A. Cook, and H.J. Hoover. Log depth circuits for division and related problems. SIAM J. on Computing, 15(4):994–1003, 1986. H. Block. The Perceptron: A model for brain functioning. Reviews of Modern Physics, 34:123–135, 1962. A. Blum. Random Projection, Margins, Kernels, and Feature-Selection. In Subspace, Latent Structure and Feature Selection, pages 52–68, 2006. A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. Learnability and the VapnikChervonenkis dimension. Journal of the ACM, 36(4):929–965, 1989. J. Bradley, A. Kyrola, D. Bickson, and C. Guestrin. Parallel coordinate descent for l1-regularized loss minimization. In Proc. 28th ICML, pages 321–328, 2011. J. K. Bradley and R. E. Schapire. Filterboost: Regression and classiﬁcation on large datasets. In Proc. 21st NIPS, 2007. N. Bshouty, S. Goldman, and H.D. Mathias. Noise-tolerant parallel learning of geometric concepts. Inf. and Comput., 147(1):89–110, 1998. ISSN 0890-5401. doi: DOI: 10.1006/inco.1998.2737. M. Collins, R. E. Schapire, and Y. Singer. Logistic regression, adaboost and bregman distances. Machine Learning, 48(1-3):253–285, 2002. A. d’Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Optimization, 19 (3): 1171–1183, 2008. 3126  PARALLEL L ARGE -M ARGIN L EARNING  O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction. In Proc. 28th ICML, pages 713–720, 2011. C. Domingo and O. Watanabe. MadaBoost: A modiﬁed version of AdaBoost. In Proc. 13th COLT, pages 180–189, 2000. Y. Freund. Boosting a weak learning algorithm by majority. Information and Computation, 121 (2): 256–285, 1995. Y. Freund. An adaptive version of the boost-by-majority algorithm. Machine Learning, 43(3):293– 318, 2001. Y. Freund and R. E. Schapire. Large margin classiﬁcation using the perceptron algorithm. Machine Learning, 37(3):277–296, 1999. R. Greenlaw, H.J. Hoover, and W.L. Ruzzo. Limits to Parallel Computation: P-Completeness Theory. Oxford University Press, New York, 1995. A. Kalai and R. Servedio. Boosting in the presence of noise. Journal of Computer & System Sciences, 71(3):266–290, 2005. N. Karmarkar. A new polynomial time algorithm for linear programming. Combinat., 4:373–395, 1984. M. Kearns and Y. Mansour. On the boosting ability of top-down decision tree learning algorithms. In Proc. 28th STOC, pages 459–468, 1996. M. Kearns and U. Vazirani. An Introduction to Computational Learning Theory. MIT Press, Cambridge, MA, 1994. N. Littlestone. From online to batch learning. In Proc. 2nd COLT, pages 269–284, 1989. P. Long and R. Servedio. Martingale boosting. In Proc. 18th COLT, pages 79–94, 2005. P. Long and R. Servedio. Adaptive martingale boosting. In Proc. 22nd NIPS, pages 977–984, 2008. P. Long and R. Servedio. Algorithms and hardness results for parallel large margin learning. In Proc. 25th NIPS, 2011. Y. Mansour and D. McAllester. Boosting using branching programs. Journal of Computer & System Sciences, 64(1):103–112, 2002. Y. Nesterov. Introductory lectures on Convex Optimization. Kluwer, 2004. Y. Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM J. Optimization, 16(1):235–249, 2005. A. Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on Mathematical Theory of Automata, volume XII, pages 615–622, 1962. F. Rosenblatt. The Perceptron: a probabilistic model for information storage and organization in the brain. Psychological Review, 65:386–407, 1958. 3127  L ONG AND S ERVEDIO  R. Schapire. The strength of weak learnability. Machine Learning, 5(2):197–227, 1990. R. Servedio. Smooth boosting and learning with malicious noise. JMLR, 4:633–648, 2003. S. Shalev-Shwartz and Y. Singer. On the equivalence of weak learnability and linear separability: New relaxations and efﬁcient boosting algorithms. Machine Learning, 80(2):141–163, 2010. N. Soheili and J. Pe˜ a. A smooth perceptron algorithm. SIAM J. Optimization, 22(2):728–737, n 2012. L. Valiant. A theory of the learnable. Communications of the ACM, 27 (11): 1134–1142, 1984. V. N. Vapnik and A. Y. Chervonenkis. Theory of Pattern Recognition. Nauka, 1974. In Russian. J. S. Vitter and J. Lin. Learning in parallel. Inf. Comput., 96(2):179–202, 1992. E. W. Weisstein. Newton’s iteration, 2011. http://mathworld.wolfram.com/NewtonsIteration.html. DIMACS 2011 Workshop. Parallelism: A 2020 Vision. 2011. NIPS 2009 Workshop. Large-Scale Machine Learning: Parallelism and Massive Datasets. 2009.  3128</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
