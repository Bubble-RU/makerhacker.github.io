<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>20 jmlr-2013-CODA: High Dimensional Copula Discriminant Analysis</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-20" href="../jmlr2013/jmlr-2013-CODA%3A_High_Dimensional_Copula_Discriminant_Analysis.html">jmlr2013-20</a> <a title="jmlr-2013-20-reference" href="#">jmlr2013-20-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>20 jmlr-2013-CODA: High Dimensional Copula Discriminant Analysis</h1>
<br/><p>Source: <a title="jmlr-2013-20-pdf" href="http://jmlr.org/papers/volume14/han13a/han13a.pdf">pdf</a></p><p>Author: Fang Han, Tuo Zhao, Han Liu</p><p>Abstract: We propose a high dimensional classiﬁcation method, named the Copula Discriminant Analysis (CODA). The CODA generalizes the normal-based linear discriminant analysis to the larger Gaussian Copula models (or the nonparanormal) as proposed by Liu et al. (2009). To simultaneously achieve estimation efﬁciency and robustness, the nonparametric rank-based methods including the Spearman’s rho and Kendall’s tau are exploited in estimating the covariance matrix. In high dimensional settings, we prove that the sparsity pattern of the discriminant features can be consistently recovered with the parametric rate, and the expected misclassiﬁcation error is consistent to the Bayes risk. Our theory is backed up by careful numerical experiments, which show that the extra ﬂexibility gained by the CODA method incurs little efﬁciency loss even when the data are truly Gaussian. These results suggest that the CODA method can be an alternative choice besides the normal-based high dimensional linear discriminant analysis. Keywords: high dimensional statistics, sparse nonlinear discriminant analysis, Gaussian copula, nonparanormal distribution, rank-based statistics</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
