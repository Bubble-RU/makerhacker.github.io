<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 jmlr-2013-Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-110" href="../jmlr2013/jmlr-2013-Sub-Local_Constraint-Based_Learning_of_Bayesian_Networks_Using_A_Joint_Dependence_Criterion.html">jmlr2013-110</a> <a title="jmlr-2013-110-reference" href="#">jmlr2013-110-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>110 jmlr-2013-Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion</h1>
<br/><p>Source: <a title="jmlr-2013-110-pdf" href="http://jmlr.org/papers/volume14/mahdi13a/mahdi13a.pdf">pdf</a></p><p>Author: Rami Mahdi, Jason Mezey</p><p>Abstract: Constraint-based learning of Bayesian networks (BN) from limited data can lead to multiple testing problems when recovering dense areas of the skeleton and to conﬂicting results in the orientation of edges. In this paper, we present a new constraint-based algorithm, light mutual min (LMM) for improved accuracy of BN learning from small sample data. LMM improves the assessment of candidate edges by using a ranking criterion that considers conditional independence on neighboring variables at both sides of an edge simultaneously. The algorithm also employs an adaptive relaxation of constraints that, selectively, allows some nodes not to condition on some neighbors. This relaxation aims at reducing the incorrect rejection of true edges connecting high degree nodes due to multiple testing. LMM additionally incorporates a new criterion for ranking v-structures that is used to recover the completed partially directed acyclic graph (CPDAG) and to resolve conﬂicting v-structures, a common problem in small sample constraint-based learning. Using simulated data, each of these components of LMM is shown to signiﬁcantly improve network inference compared to commonly applied methods when learning from limited data, including more accurate recovery of skeletons and CPDAGs compared to the PC, MaxMin, and MaxMin hill climbing algorithms. A proof of asymptotic correctness is also provided for LMM for recovering the correct skeleton and CPDAG. Keywords: Bayesian networks, skeleton, constraint-based learning, mutual min</p><br/>
<h2>reference text</h2><p>C. Aliferis, I. Tsamardinos, A. Statnikov, and L. Brown. Causal Explorer:Causal Probabilistic Network Learning Toolkit for Biomedical Discovery, 2003. URL http://www.dsl-lab.org/ causal_explorer. C.F. Aliferis, A. Statnikov, I. Tsamardinos, S. Mani, and X.D. Koutsoukos. Local causal and Markov blanket induction for causal discovery and feature selection for classiﬁcation part ii: Analysis and extensions. The Journal of Machine Learning Research, 11:235–284, 2010. A. Armen. Estimation and control of the false discovery rate in Bayesian network skeleton identiﬁcation, with application to biological data. Master’s thesis, Computer Science Department, University of Crete, Heraklion, Crete, Greece, 2011. 1599  M AHDI AND M EZEY  (B) TPR ROC Flat Curve  (A) SHD Curve  FN  FP  TP  |FP| + |FN| - |FP FN|  Figure 13: Illustration of A) SHD and B) TPR plots. In both plots, the X-Axis is the number of retrieved edges normalized by the number of correct edges. The metrics: FP, FN, TP, and SHD are also normalized by the correct number of edges. In the SHD plot, lower is better while in the TPR Plot, higher is better.  LMM_EO  LMM_EO −1  PCAlg  (B) 300 Edges  0.6  0.8  SHD  1  1.2  (A) 100 Edges  MMHC  0  0.5X  1X  1.4X  # of Retrieved Edges  0  0.5X  1X  1.3X  # of Retrieved Edges  Figure 14: Average SHD when recovering CPDAGs of networks of 100 nodes and A) ∼100 or B) ∼300 true edges when the number of samples is 100. X-Axis is the number of retrieved edges (X = Number of edges in the correct network). Y-Axis is the average normalized structural Hamming distance (SHD).  J. Cheng, R. Greiner, J. Kelly, D. Bell, and W. Liu. Learning Bayesian networks from data: An information-theory based approach. Artiﬁcial Intelligence, 137(1-2):43 – 90, 2002. ISSN 00043702. D. Chickering, D. Heckerman, and C. Meek. Large-sample learning of Bayesian networks is NPhard. Journal of Machine Learning Research, 5:1287–1330, 2004. ISSN 1532-4435. 1600  S UB -L OCAL L EARNING OF BN U SING A J OINT D EPENDENCE C RITERION  LMM_EO  MMHC  (B) 500 Nodes  PCAlg  (C) 2500 Nodes  1 0.7 1.3 1 0.7 0.4  SHD (Samples = 300)  0.4  SHD (Samples = 100)  1.3  (A) 100 Nodes  LMM_EO −1  0  0.5X  1X  1.4X  0  0.5X  1X  1.4X  0  0.5X  1X  1.4X  # of Retrieved Edges  Figure 15: Average SHD when recovering CPDAGs of networks of A) 100, B) 500, and C) 2500 nodes with ∼200, ∼1,000, and ∼5,000 true edges respectively, when the number of samples is 100 (1st row) and 300 (2nd row). X-Axis is the number of retrieved edges (X = Number of edges in the correct network). Y-Axis is the average normalized structural Hamming distance (SHD). Optimal  10k Samples  (B) 1000 Edges  0.5  1  1.3  100 Samples  0  0.5 0  SHD  1  1.3  (A) 500 Edges  1k Samples  0  0.5X  1X  1.4X  # of Retrieved Edges  0.5X  1X  1.3X  # of Retrieved Edges  Figure 16: Average SHD of CPDAG recovery by LMM EO as the number of observations grows large when recovering networks of 500 variables and A) ∼500 edges, and B) ∼1000 edges. X-Axis is the number of retrieved edges (X = Number of edges in the correct network). Y-Axis is the average normalized structural Hamming distance (SHD).  1601  M AHDI AND M EZEY  G. Cooper and E. Herskovits. Using Bayesian networks to analyze expression data. Machine Learning, 9:309–347, 10 1992. R. Cowell, S. Lauritzen, P. David, and D. Spiegelhalter. Probabilistic Networks and Expert Systems. Springer-Verlag New York, Inc., Secaucus, NJ, USA, 1999. ISBN 0387987673. A.S. Fast. Learning the Structure of Bayesian Networks with Constraint Satisfaction. PhD thesis, University of Massachusetts Amherst, Amherst, MA, USA, 2010. N. Friedman. Inferring cellular networks using probabilistic graphical models. Science Magazine, 303(5659):799–805, 2004. N. Friedman, I. Nachman, and D. Peer. Learning Bayesian network structure from massive datasets: The sparse candidate algorithm. In 5th Conference on Uncertainty in Artiﬁcial Intelligence, pages 206–215, 1999. N. Friedman, M. Linial, I. Nachman, and D. Pe’er. Using Bayesian networks to analyze expression data. Journal of Computational Biology, pages 601–620, 2000. doi: 10. H. Hotelling. New light on the correlation coefﬁcient and its transforms. Journal of the Royal Statistical Society. Series B (Methodological), 15(2):193–232, 1953. M. Kalisch and P. B¨ hlmann. Estimating high-dimensional directed acyclic graphs with the PCu algorithm. Journal of Machine Learning Research, 8:613–636, 2007. ISSN 1532-4435. M. Kalisch, M. Maechler, and D. Colombo. PCAlg: Estimation of CPDAG/PAG and causal inference using the IDA algorithm, 2010. URL http://CRAN.R-project.org/package=pcalg. R package version 1.0-2. W. Lam and F. Bacchus. Learning Bayesian belief networks: An approach based on the mdl principle. Computational Intelligence, 10:269–293, 1994. J. Li and Z.J. Wang. Controlling the false discovery rate of the association/causality structure learned with the PC algorithm. The Journal of Machine Learning Research, 10:475–514, 2009. D. Margaritis and S. Thrun. A Bayesian multiresolution independence test for continuous variables. In Proceedings of the 17th Conference on Uncertainty in Artiﬁcial Intelligence, pages 346–353. Morgan Kaufmann Publishers Inc., 2001. C. Meek. Causal inference and causal explanation with background knowledge. In Proceedings of the 11th Annual Conference on Uncertainty in Artiﬁcial Intelligence, pages 403–441, San Francisco, CA, USA, 1995. Morgan Kaufmann. R. Neapolitan. Learning Bayesian Networks. Pearson Printice Hall, London, UK, 2004. J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1988. ISBN 1558604790. E. Perrier, S. Imoto, and S. Miyano. Finding optimal Bayesian network given a super-structure. Journal of Machine Learning Research, 9:2251–2286, 2008. 1602  S UB -L OCAL L EARNING OF BN U SING A J OINT D EPENDENCE C RITERION  S.J. Russell and P. Norvig. Artiﬁcial Intelligence: A Modern Approach. Prentice hall, Englewood Cliffs, NJ, USA, 2009. J. Sch¨ fer and K. Strimmer. An empirical Bayes approach to inferring large-scale gene association a networks. Bioinformatics, 21(6):754, 2005. M. Scutari. Learning Bayesian networks with the BNlearn R package. Journal of Statistical Software, 35(3):1–22, 2010. P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. Springer, New York, NY, USA, 1993. P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search, 2nd Edition, volume 1. The MIT Press, Cambridge, MI, USA, 2001. I. Tsamardinos and L.E. Brown. Bounding the false discovery rate in local Bayesian network learning. In Proceedings of the 23rd National Conference on Artiﬁcial Intelligence, pages 1100–1105, 2008. I. Tsamardinos, L. Brown, and C. Aliferis. The max-min hill-climbing Bayesian network structure learning algorithm. Machine Learning, 65:31–78, 10 2006. ISSN 1532-4435. T.S. Verma and J. Pearl. Equivalence and synthesis of causal models. In Proceedings of the 6th Conference on Uncertainty in Artiﬁcial Intelligence, pages 220–227, 1990. J. Zhang and P. Spirtes. Strong faithfulness and uniform consistency in causal inference. In Proceedings of the 19th Conference on Uncertainty in Artiﬁcial Intelligence, pages 632–639. Citeseer, 2003.  1603</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
