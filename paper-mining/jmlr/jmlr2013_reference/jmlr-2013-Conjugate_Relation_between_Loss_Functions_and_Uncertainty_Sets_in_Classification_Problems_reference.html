<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-26" href="../jmlr2013/jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">jmlr2013-26</a> <a title="jmlr-2013-26-reference" href="#">jmlr2013-26-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</h1>
<br/><p>Source: <a title="jmlr-2013-26-pdf" href="http://jmlr.org/papers/volume14/kanamori13a/kanamori13a.pdf">pdf</a></p><p>Author: Takafumi Kanamori, Akiko Takeda, Taiji Suzuki</p><p>Abstract: There are two main approaches to binary classiÄ?Ĺš cation problems: the loss function approach and the uncertainty set approach. The loss function approach is widely used in real-world data analysis. Statistical decision theory has been used to elucidate its properties such as statistical consistency. Conditional probabilities can also be estimated by using the minimum solution of the loss function. In the uncertainty set approach, an uncertainty set is deÄ?Ĺš ned for each binary label from training samples. The best separating hyperplane between the two uncertainty sets is used as the decision function. Although the uncertainty set approach provides an intuitive understanding of learning algorithms, its statistical properties have not been sufÄ?Ĺš ciently studied. In this paper, we show that the uncertainty set is deeply connected with the convex conjugate of a loss function. On the basis of the conjugate relation, we propose a way of revising the uncertainty set approach so that it will have good statistical properties such as statistical consistency. We also introduce statistical models corresponding to uncertainty sets in order to estimate conditional probabilities. Finally, we present numerical experiments, verifying that the learning with revised uncertainty sets improves the prediction accuracy. Keywords: loss function, uncertainty set, convex conjugate, consistency</p><br/>
<h2>reference text</h2><p>S. Arora, L. Babai, J. Stern, and Z. Sweedyk. The hardness of approximate optima in lattices, codes, and systems of linear equations. J. Comput. Syst. Sci., 54(2):317Ă˘&euro;&ldquo;331, 1997. P. L. Bartlett and A. Tewari. Sparseness vs estimating conditional probabilities: Some asymptotic results. Journal of Machine Learning Research, 8:775Ă˘&euro;&ldquo;790, April 2007. P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classiÄ?Ĺš cation, and risk bounds. Journal of the American Statistical Association, 101:138Ă˘&euro;&ldquo;156, 2006. 1502  C ONJUGATE R ELATION IN C LASSIFICATION P ROBLEMS  A. Ben-Tal and A. Nemirovski. Robust optimization - methodology and applications. Math. Program., 92(3):453Ă˘&euro;&ldquo;480, 2002. A. Ben-Tal, L. El-Ghaoui, and A. Nemirovski. Robust Optimization. Princeton University Press, Princeton, 2009. K. P. Bennett and E. J. Bredensteiner. Duality and geometry in SVM classiÄ?Ĺš ers. In Proceedings of International Conference on Machine Learning, pages 57Ă˘&euro;&ldquo;64, 2000. A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics. Kluwer Academic, 2004. D. Bertsekas, A. Nedic, and A. Ozdaglar. Convex Analysis and Optimization. Athena ScientiÄ?Ĺš c, Belmont, MA, 2003. C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20:273Ă˘&euro;&ldquo;297, 1995. D. J. Crisp and C. J. C. Burges. A geometric interpretation of ĂŽË?-SVM classiÄ?Ĺš ers. In S. A. Solla, T. K. Leen, and K.-R. MĂ&sbquo;Â¨ ller, editors, Advances in Neural Information Processing Systems 12, u pages 244Ă˘&euro;&ldquo;250. MIT Press, 2000. F. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin of the American Mathematical Society, 39:1Ă˘&euro;&ldquo;49, 2002. T. Evgeniou, M. Pontil, and T. Poggio. A uniÄ?Ĺš ed framework for regularization networks and support vector machines. Laboratory, Massachusetts Institute of Technology, 1999. Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119Ă˘&euro;&ldquo;139, aug 1997. J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. Annals of Statistics, 28:2000, 1998. T. Hastie, R. Tibishirani, and J. Friedman. The Elements of Statistical Learning. Springer, New York, 2001. K. Huang, H. Yang, I. King, M. R. Lyu, and Laiwan Chan. The minimum error minimax probability machine. Journal of Machine Learning Research, 5:1253Ă˘&euro;&ldquo;1286, 2004. A. Karatzoglou, A. Smola, K. Hornik, and A. Zeileis. kernlab Ă˘&euro;&ldquo; an S4 package for kernel methods in R. Journal of Statistical Software, 11(9):1Ă˘&euro;&ldquo;20, 2004. G. R. G. Lanckriet, L. El Ghaoui, C. Bhattacharyya, and M. I. Jordan. A robust minimax approach to classiÄ?Ĺš cation. Journal of Machine Learning Research, 3:555Ă˘&euro;&ldquo;582, 2003. M. E. Mavroforakis and S. Theodoridis. A geometric approach to support vector machine (svm) classiÄ?Ĺš cation. IEEE Transactions on Neural Networks, 17(3):671Ă˘&euro;&ldquo;682, 2006. J. S. Nath and C. Bhattacharyya. Maximum margin classiÄ?Ĺš ers with speciÄ?Ĺš ed false positive and false negative error rates. In C. Apte, B. Liu, S. Parthasarathy, and D. Skillicorn, editors, Proceedings of the seventh SIAM International Conference on Data mining, pages 35Ă˘&euro;&ldquo;46. SIAM, 2007. 1503  K ANAMORI , TAKEDA AND S UZUKI  G. RĂ&sbquo;Â¨ tsch, B. SchĂ&sbquo;Â¨ lkopf, A.J. Smola, S. Mika, T. Onoda, and K.-R. MĂ&sbquo;Â¨ ller. Robust Ensemble a o u Learning., pages 207Ă˘&euro;&ldquo;220. MIT Press, Cambridge, MA, 2000. G. RĂ&sbquo;Â¨ tsch, T. Onoda, and K.-R. MĂ&sbquo;Â¨ ller. Soft margins for adaboost. Machine Learning, 42(3): a u 287Ă˘&euro;&ldquo;320, 2001. R. T. Rockafellar. Convex Analysis. Princeton University Press, Princeton, NJ, USA, 1970. R. T. Rockafellar and S. Uryasev. Conditional value-at-risk for general loss distributions. Journal of Banking & Finance, 26(7):1443Ă˘&euro;&ldquo;1472, 2002. R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. The Annals of Statistics, 26(5):1651Ă˘&euro;&ldquo;1686, 1998. B. SchĂ&sbquo;Â¨ lkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002. o B. SchĂ&sbquo;Â¨ lkopf, A. Smola, R. Williamson, and P. Bartlett. New support vector algorithms. Neural o Computation, 12(5):1207Ă˘&euro;&ldquo;1245, 2000. I. Steinwart. On the optimal parameter choice for ĂŽË?-support vector machines. IEEE Trans. Pattern Anal. Mach. Intell., 25(10):1274Ă˘&euro;&ldquo;1284, 2003. I. Steinwart. Consistency of support vector machines and other regularized kernel classiÄ?Ĺš ers. IEEE Transactions on Information Theory, 51(1):128Ă˘&euro;&ldquo;142, 2005. I. Steinwart and A. Christmann. Support Vector Machines. Springer Publishing Company, Incorporated, 1st edition, 2008. S. Sun and J. Shawe-Taylor. Sparse semi-supervised learning using conjugate functions. Journal of Machine Learning Research, 11:2423Ă˘&euro;&ldquo;2455, 2010. A. Takeda and M. Sugiyama. ĂŽË?-support vector machine as conditional value-at-risk minimization. In Proceedings of the 25th International Conference on Machine Learning, pages 1056Ă˘&euro;&ldquo;1063, 2008. A. Takeda, H. Mitsugi, and T. Kanamori. A uniÄ?Ĺš ed classiÄ?Ĺš cation model based on robust optimization. Neural Computation, 25(3):759Ă˘&euro;&ldquo;804, 2013. V. Vapnik. Statistical Learning Theory. Wiley, 1998. T. Zhang. Statistical behavior and consistency of classiÄ?Ĺš cation methods based on convex risk minimization. Annals of Statitics, 32(1):56Ă˘&euro;&ldquo;85, 2004. D.-X. Zhou. The covering number in learning theory. Journal of Complexity, 18(3):739Ă˘&euro;&ldquo;767, 2002.  1504</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
