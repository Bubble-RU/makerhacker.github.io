<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-103" href="../jmlr2013/jmlr-2013-Sparse_Robust_Estimation_and_Kalman_Smoothing_with_Nonsmooth_Log-Concave_Densities%3A_Modeling%2C_Computation%2C_and_Theory.html">jmlr2013-103</a> <a title="jmlr-2013-103-reference" href="#">jmlr2013-103-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</h1>
<br/><p>Source: <a title="jmlr-2013-103-pdf" href="http://jmlr.org/papers/volume14/aravkin13a/aravkin13a.pdf">pdf</a></p><p>Author: Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto</p><p>Abstract: We introduce a new class of quadratic support (QS) functions, many of which already play a crucial role in a variety of applications, including machine learning, robust statistical inference, sparsity promotion, and inverse problems such as Kalman smoothing. Well known examples of QS penalties include the ℓ2 , Huber, ℓ1 and Vapnik losses. We build on a dual representation for QS functions, using it to characterize conditions necessary to interpret these functions as negative logs of true probability densities. This interpretation establishes the foundation for statistical modeling with both known and new QS loss functions, and enables construction of non-smooth multivariate distributions with speciﬁed means and variances from simple scalar building blocks. The main contribution of this paper is a ﬂexible statistical modeling framework for a variety of learning applications, together with a toolbox of efﬁcient numerical methods for estimation. In particular, a broad subclass of QS loss functions known as piecewise linear quadratic (PLQ) penalties has a dual representation that can be exploited to design interior point (IP) methods. IP methods solve nonsmooth optimization problems by working directly with smooth systems of equations characterizing their optimality. We provide several numerical examples, along with a code that can be used to solve general PLQ problems. The efﬁciency of the IP approach depends on the structure of particular applications. We consider the class of dynamic inverse problems using Kalman smoothing. This class comprises a wide variety of applications, where the aim is to reconstruct the state of a dynamical system with known process and measurement models starting from noisy output samples. In the classical case, Gaus∗. The authors would like to thank Bradley M. Bell for insightful discussions and helpful suggestions. The research leading to these results has received funding from the European Union Seventh Framework Programme [FP7/2007-2013</p><br/>
<h2>reference text</h2><p>B. D. O. Anderson and J. B. Moore. Optimal Filtering. Prentice-Hall, Englewood Cliffs, N.J., USA, 1979. A. Y. Aravkin, J. V. Burke, and M. P. Friedlander. Variational properties of value functions. To Appear in Siam Journal of Optimization, 2013. A.Y. Aravkin. Robust Methods with Applications to Kalman Smoothing and Bundle Adjustment. PhD thesis, University of Washington, Seattle, WA, June 2010. A.Y. Aravkin, B.M. Bell, J.V. Burke, and G. Pillonetto. An ℓ1 -laplace robust kalman smoother. Automatic Control, IEEE Transactions on, 56(12):2898–2911, dec. 2011a. ISSN 0018-9286. doi: 10.1109/TAC.2011.2141430. A.Y. Aravkin, B.M. Bell, J.V. Burke, and G. Pillonetto. Learning using state space kernel machines. In Proc. IFAC World Congress 2011, Milan, Italy, 2011b. N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society, 68:337–404, 1950. B.M. Bell. The marginal likelihood for parameters in a discrete Gauss-Markov process. IEEE Transactions on Signal Processing, 48(3):626–636, August 2000. S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Found. Trends Mach. Learn., 3(1): 1–122, January 2011. ISSN 1935-8237. doi: 10.1561/2200000016. URL http://dx.doi.org/ 10.1561/2200000016. R. Brockett. Finite Dimensional Linear Systems. John Wiley and Sons, Inc., 1970. J. V. Burke. An exact penalization viewpoint of constrained optimization. Technical report, Argonne National Laboratory, ANL/MCS-TM-95, 1987. J.V. Burke. Descent methods for composite nondifferentiable optimization problems. Mathematical Programming, 33:260–279, 1985. 2724  N ONSMOOTH D ENSITIES : M ODELING , C OMPUTATION AND T HEORY  W. Chu, S. S. Keerthi, and O. C. Jin. A uniﬁed loss function in bayesian framework for support vector regression. In In Proceeding of the 18th International Conference on Machine Learning, pages 51–58, 2001. F. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin of the American Mathematical Society, 39:1–49, 2001. F. Dinuzzo. Analysis of ﬁxed-point and coordinate descent algorithms for regularized kernel methods. IEEE Transactions on Neural Networks, 22(10):1576 –1587, 2011. F. Dinuzzo, M. Neve, G. De Nicolao, and U. P. Gianazza. On the representer theorem and equivalent degrees of freedom of SVR. Journal of Machine Learning Research, 8:2467–2495, 2007. D. Donoho. Compressed sensing. IEEE Trans. on Information Theory, 52(4):1289–1306, 2006. B. Efron, T. Hastie, L. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32:407–499, 2004. T. Evgeniou, M. Pontil, and T. Poggio. Regularization networks and support vector machines. Advances in Computational Mathematics, 13:1–150, 2000. S. Farahmand, G.B. Giannakis, and D. Angelosante. Doubly robust smoothing of dynamical processes via outlier sparsity constraints. IEEE Transactions on Signal Processing, 59:4529–4543, 2011. M.C. Ferris and T.S. Munson. Interior-point methods for massive support vector machines. SIAM Journal on Optimization, 13(3):783 – 804, 2003. S. Fine and K. Scheinberg. Efﬁcient svm training using low-rank kernel representations. J. Mach. Learn. Res., 2:243 –264, 2001. J. Gao. Robust l1 principal component analysis and its Bayesian variational inference. Neural Computation, 20(2):555–572, February 2008. A. Gelb. Applied Optimal Estimation. The M.I.T. Press, Cambridge, MA, 1974. O. G¨ ler and R. Hauser. Self-scaled barrier functions on symmetric cones and their classiﬁcation. u Foundations of Computational Mathematics, 2:121–143, 2002. T. J. Hastie and R. J. Tibshirani. Generalized additive models. In Monographs on Statistics and Applied Probability, volume 43. Chapman and Hall, London, UK, 1990. T. J. Hastie, R. J. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Data Mining, Inference and Prediction. Springer, Canada, 2001. P.J. Huber. Robust Statistics. Wiley, 1981. A. Jazwinski. Stochastic Processes and Filtering Theory. Dover Publications, Inc, 1970. T. Joachims, editor. Making Large-Scale Support Vector Machine Learning Practical. MIT Press, Cambridge, MA, USA, 1998. 2725  A RAVKIN , B URKE AND P ILLONETTO  S. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky. An interior-point method for large-scale ℓ1 -regularized least squares. IEEE Journal of Selected Topics in Signal Processing, 1(4):606 – 617, 2007. M. Kojima, N. Megiddo, T. Noma, and A. Yoshise. A Uniﬁed Approach to Interior Point Algorithms for Linear Complementarity Problems, volume 538 of Lecture Notes in Computer Science. Springer Verlag, Berlin, Germany, 1991. C.J. Lin. On the convergence of the decomposition method for support vector machines. IEEE Transactions on Neural Networks, 12(12):1288 –1298, 2001. H. Liu, S. Shah, and W. Jiang. On-line outlier detection and data cleaning. Computers and Chemical Engineering, 28:1635–1647, 2004. S. Lucidi, L. Palagi, A. Risi, and M. Sciandrone. A convergent decomposition algorithm for support vector machines. Comput. Optim. Appl., 38(2):217 –234, 2007. D.J.C. MacKay. Bayesian interpolation. Neural Computation, 4:415–447, 1992. D.J.C. Mackay. Bayesian non-linear modelling for the prediction competition. ASHRAE Trans., 100(2):3704–3716, 1994. A. Nemirovskii and Y. Nesterov. Interior-Point Polynomial Algorithms in Convex Programming, volume 13 of Studies in Applied Mathematics. SIAM, Philadelphia, PA, USA, 1994. H. Ohlsson, F. Gustafsson, L. Ljung, and S. Boyd. Smoothed state estimates under abrupt changes using sum-of-norms regularization. Automatica, 48:595–605, 2012. B. Oksendal. Stochastic Differential Equations. Springer, sixth edition, 2005. J.A. Palmer, D.P. Wipf, K. Kreutz-Delgado, and B.D. Rao. Variational em algorithms for nongaussian latent variable models. In Proc. of NIPS, 2006. G. Pillonetto and B.M. Bell. Bayes and empirical Bayes semi-blind deconvolution using eigenfunctions of a prior covariance. Automatica, 43(10):1698–1712, 2007. J. Platt. Fast training of support vector machines using sequential minimal optimization. In Advances in Kernel Methods: Support Vector Learning, 1998. M. Pontil and A. Verri. Properties of support vector machines. Neural Computation, 10:955–974, 1998. C.E. Rasmussen and C.K.I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2006. B. Ristic, S. Arulampalam, and N. Gordon. Beyond the Kalman Filter: Particle Filters for Tracking Applications. Artech House Publishers, 2004. R.T. Rockafellar. Convex Analysis. Priceton Landmarks in Mathematics. Princeton University Press, 1970. 2726  N ONSMOOTH D ENSITIES : M ODELING , C OMPUTATION AND T HEORY  R.T. Rockafellar and R.J.B. Wets. Variational Analysis, volume 317. Springer, 1998. S. Roweis and Z. Ghahramani. A unifying review of linear gaussian models. Neural Computation, 11:305–345, 1999. S. Saitoh. Theory of Reproducing Kernels and Its Applications. Longman, 1988. H. H. Schaefer. Topological Vector Spaces. Springe-Verlag, 1970. B. Sch¨ lkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regularization, o Optimization, and Beyond. (Adaptive Computation and Machine Learning). The MIT Press, 2001. B. Sch¨ lkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett. New support vector algorithms. o Neural Computation, 12:1207–1245, 2000. B. Sch¨ lkopf, R. Herbrich, and A. J. Smola. A generalized representer theorem. Neural Networks o and Computational Learning Theory, 81:416–426, 2001. A. J. Smola and B. Sch¨ lkopf. Bayesian kernel methods. In S. Mendelson and A. J. Smola, editors, o Machine Learning, Proceedings of the Summer School, Australian National University, pages 65–117, Berlin, Germany, 2003. Springer-Verlag. R. Tibshirani. Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society, Series B., 58:267–288, 1996. M. Tipping. Sparse bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1:211–244, 2001. P. Tseng and S. Yun. A coordinate gradient descent method for linearly constrained smooth optimization and support vector machines training. Comput. Optim. Appl., 47(2):1 –28, 2008. V. Vapnik. Statistical Learning Theory. Wiley, New York, NY, USA, 1998. G. Wahba. Spline Models For Observational Data. SIAM, Philadelphia, 1990. G. Wahba. Support vector machines, reproducing kernel Hilbert spaces and randomized GACV. Technical Report 984, Department of Statistics, University of Wisconsin, 1998. D.P. Wipf, B.D. Rao, and S. Nagarajan. Latent variable bayesian models for promoting sparsity. IEEE Transactions on Information Theory, 57:6236–6255, 2011. S.J. Wright. Primal-Dual Interior-Point Methods. Siam, Englewood Cliffs, N.J., USA, 1997. √ Y. Ye and K. Anstreicher. On quadratic and o( nL) convergence of a predictor-corrector method for LCP. Mathematical Programming, 62(1-3):537–551, 1993. E. H. Zarantonello. Projections on Convex Sets in Hilbert Space and Spectral Theory. Academic Press, 1971. K. Zhang and J.T. Kwok. Clustered Nystrom method for large scale manifold learning and dimension reduction. IEEE Transactions on Neural Networks, 21(10):1576 –1587, 2010. 2727  A RAVKIN , B URKE AND P ILLONETTO  H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, Series B, 67:301–320, 2005.  2728</p>
<br/>
<br/><br/><br/></body>
</html>
