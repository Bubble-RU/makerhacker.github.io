<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>61 jmlr-2013-Learning Theory Analysis for Association Rules and Sequential Event Prediction</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-61" href="../jmlr2013/jmlr-2013-Learning_Theory_Analysis_for_Association_Rules_and_Sequential_Event_Prediction.html">jmlr2013-61</a> <a title="jmlr-2013-61-reference" href="#">jmlr2013-61-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>61 jmlr-2013-Learning Theory Analysis for Association Rules and Sequential Event Prediction</h1>
<br/><p>Source: <a title="jmlr-2013-61-pdf" href="http://jmlr.org/papers/volume14/rudin13a/rudin13a.pdf">pdf</a></p><p>Author: Cynthia Rudin, Benjamin Letham, David Madigan</p><p>Abstract: We present a theoretical analysis for prediction algorithms based on association rules. As part of this analysis, we introduce a problem for which rules are particularly natural, called “sequential event prediction.” In sequential event prediction, events in a sequence are revealed one by one, and the goal is to determine which event will next be revealed. The training set is a collection of past sequences of events. An example application is to predict which item will next be placed into a customer’s online shopping cart, given his/her past purchases. In the context of this problem, algorithms based on association rules have distinct advantages over classical statistical and machine learning methods: they look at correlations based on subsets of co-occurring past events (items a and b imply item c), they can be applied to the sequential event prediction problem in a natural way, they can potentially handle the “cold start” problem where the training set is small, and they yield interpretable predictions. In this work, we present two algorithms that incorporate association rules. These algorithms can be used both for sequential event prediction and for supervised classiﬁcation, and they are simple enough that they can possibly be understood by users, customers, patients, managers, etc. We provide generalization guarantees on these algorithms based on algorithmic stability analysis from statistical learning theory. We include a discussion of the strict minimum support threshold often used in association rule mining, and introduce an “adjusted conﬁdence” measure that provides a weaker minimum support condition that has advantages over the strict minimum support. The paper brings together ideas from statistical learning theory, association rule mining and Bayesian analysis. Keywords: statistical learning theory, algorithmic stability, association rules, sequence prediction, associative classiﬁcation c 2013 Cynthia Rudin, Benjamin Letham and David Madigan. RUDIN , L E</p><br/>
<h2>reference text</h2><p>Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proc. 20th Int’l Conf. Very Large Data Bases, pages 487–499. Morgan Kaufmann, 1994. 3489  RUDIN , L ETHAM AND M ADIGAN  Rakesh Agrawal, Tomasz Imieli´ ski, and Arun Swami. Mining association rules between sets of n items in large databases. In Proc. ACM SIGMOD Int’l Conference on Management of Data, pages 207–216, 1993. Martin Anthony. Generalization error bounds for threshold decision lists. Journal of Machine Learning Research, 5:189–217, 2004. Jay Ayres, Johannes Gehrke, Tomi Yiu, and Jason Flannick. Sequential PAttern Mining using a bitmap representation. In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2002. Kevin Bache and Moshe Lichman. UCI machine learning repository, 2013. URL http://archive. ics.uci.edu/ml. Andr´ Berchtold and Adrian E. Raftery. The mixture transition distribution model for high-order e markov chains and non-gaussian time series. Statistical Science, 17(3):328–356, 2002. Olivier Bousquet and Andr´ Elisseeff. Stability and generalization. Journal of Machine Learning e Research, 2:499–526, 2002. John S. Breese, David Heckerman, and Carl Myers Kadie. Empirical analysis of predictive algorithms for collaborative ﬁltering. In Proc. Uncertainty in Artiﬁcial Intelligence, pages 43–52, 1998. Edith Cohen, Mayur Datar, Shinji Fujiwara, Aristides Gionis, Piotr Indyk, Rajeev Motwani, Jeffrey D. Ullman, and Cheng Yang. Finding interesting associations without support pruning. IEEE Trans. Knowl. Data Eng., 13(1):64–78, 2001. Michelle Keim Condliff, David D. Lewis, David Madigan, and Christian Posse. Bayesian mixedeffects models for recommender systems. In ACM SIGIR Workshop on Recommender Systems: Algorithms and Evaluation, 1999. Luc P. Devroye and Terry J. Wagner. Distribution-free performance bounds for potential function rules. IEEE Transactions on Information Theory, 25(5):601–604, 1979. William DuMouchel and Daryl Pregibon. Empirical bayes screening for multi-item associations. In Proc. ACM SIGKDD Int’l Conf. on Knowl. Discovery and Data Mining, pages 67–76, 2001. Jerome H. Friedman and Bogdan E. Popescu. Predictive learning via rule ensembles. The Annals of Applied Statistics, 2(3):916–954, 2008. Liqiang Geng and Howard J. Hamilton. Choosing the right lens: Finding what is interesting in data mining. In Quality Measures in Data Mining, pages 3–24. Springer, 2007. Xiangji Huang, Aijun An, Nick Cercone, and Gary Promhouse. Discovery of interesting association rules from Livelink web log data. In Proc. IEEE Int’l Conf. on Data Mining, 2002. Peter Jackson. Introduction to Expert Systems (Third Edition). Addison-Wesley, 1998. 3490  L EARNING T HEORY A NALYSIS FOR A SSOCIATION RULES AND S EQUENTIAL E VENT P REDICTION  St´ phanie Jacquemont, Francois Jacquenet, and Marc Sebban. A lower bound on the sample size e ¸ needed to perform a signiﬁcant frequent pattern mining task. Pattern Recogn. Lett., 30:960–967, August 2009. Xiang-Rong Jiang and Le Gruenwald. Microarray gene expression data association rules mining based on BSC-tree and FIS-tree. Data & Knowl. Eng., 53(1):3–29, 2005. Norman Lloyd Johnson, Adrienne W. Kemp, and Samuel Kotz. Univariate Discrete Distributions. John Wiley & Sons, August 2005. Yun Sing Koh. Mining non-coincidental rules without a user deﬁned support threshold. In Advances in Knowl. Discovery and Data Mining, 12th Paciﬁc-Asia Conf., pages 910–915, 2008. Ron Kohavi, Llew Mason, Rajesh Parekh, and Zijian Zheng. Lessons and challenges from mining retail e-commerce data. Machine Learning, 57(1-2):83–113, 2004. Richard D. Lawrence, George S. Almasi, Viveros Kotlyar, Marisa S. Viveros, and Sastry S. Duri. Personalization of supermarket product recommendations. Data Mining and Knowledge Discovery, 5(1-2):11–32, 2001. Benjamin Letham. Similarity-weighted association rules for a name recommender system. In Proceedings of the ECML/PKDD Discovery Challenge Workshop, 2013. Benjamin Letham, Cynthia Rudin, and Katherine A. Heller. Growing a list. Data Mining and Knowledge Discovery, 27(3):372–395, 2013a. Benjamin Letham, Cynthia Rudin, and David Madigan. Sequential event prediction. Machine Learning, 93:357–380, 2013b. Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, and David Madigan. An interpretable stroke prediction model using rules and bayesian analysis. In Proceedings of AAAI Late Breaking Track, 2013c. Jinyan Li, Xiuzhen Zhang, Guozho Dong, Kotagiri Ramamohanarao, and Qun Sun. Efﬁcient mining of high conﬁdence association rules without support thresholds. In Proc. Principles of Data Mining and Knowledge Discovery, pages 406–411, 1999. Wenmin Li, Jiawei Han, and Jian Pei. CMAR: Accurate and efﬁcient classiﬁcation based on multiple class-association rules. IEEE International Conference on Data Mining, pages 369–376, 2001. Weiyang Lin, Sergio A. Alvarez, and Carolina Ruiz. Efﬁcient adaptive-support association rule mining for recommender systems. Data Mining and Knowledge Discovery, 6(1):83–105, 2002. Bing Liu, Wynne Hsu, and Yiming Ma. Integrating classiﬁcation and association rule mining. In Proceedings of the 4th International Conference on Knowledge Discovery and Data Mining, 1998. David Madigan, Krzysztof Mosurski, and Russell G Almond. Graphical explanation in belief networks. Journal of Computational and Graphical Statistics, 6:160–181, 1997. 3491  RUDIN , L ETHAM AND M ADIGAN  Mario Marchand and Marina Sokolova. Learning with decision lists of data-dependent features. Journal of Machine Learning Research, 2005. Tyler H. McCormick, Cynthia Rudin, and David Madigan. Bayesian hierarchical modeling for predicting medical conditions. The Annals of Applied Statistics, 6(2):652–668, 2012. Ken McGarry. A survey of interestingness measures for knowledge discovery. The Knowledge Engineering Review, 20:39–61, 2005. Grzegorz A. Rempala. Asymptotic factorial powers expansions for binomial and negative binomial reciprocals. In Proceedings of the American Mathematical Society, volume 132, pages 261–272, 2003. Ronald L. Rivest. Learning decision lists. Machine Learning, 2(3):229–246, 1987. William H. Rogers and Terry J. Wagner. A ﬁnite sample distribution-free performance bound for local discrimination rules. Annals of Statistics, 6(3):506–514, 1978. V. Romanovsky. Note on the moments of a binomial (p + q)n about its mean. Biometrika, 15: 410–412, 1923. Cynthia Rudin, Benjamin Letham, Ansaf Salleb-Aouissi, Eugene Kogan, and David Madigan. A framework for supervised learning with association rules. In Proceedings of the 24th Annual Conference on Learning Theory, 2011. Xiaotong Shen and Lifeng Wang. Generalization error for multi-class margin classiﬁcation. Electronic Journal of Statistics, pages 307–330, 2007. Gy¨ rgy J. Simon, Vipin Kumar, and Peter W. Li. A simple statistical model and association rule o ﬁltering for classiﬁcation. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 823–831, 2011. Pang N. Tan, Vipin Kumar, and Jaideep Srivastava. Selecting the right interestingness measure for association patterns. In Proc. ACM SIGKDD Int’l Conference on Knowledge Discovery and Data Mining, 2002. Fadi Thabtah. A review of associative classiﬁcation mining. The Knowledge Engineering Review, 22:37–65, March 2007. Koen Vanhoof and Benoˆt Depaire. Structure of association rule classiﬁers: a review. In Proceedings ı of the International Conference on Intelligent Systems and Knowledge Engineering, pages 9–12, 2010. Vladimir Vapnik. An overview of statistical learning theory. IEEE Transactions on Neural Networks, 10(5):988–999, September 1999. Ke Wang, Yu He, David Wai-Lok Cheung, and Francis Y. L. Chin. Mining conﬁdent rules without support requirement. In Proc. Conference on Information and Knowledge Management, pages 89–96, 2001. Xiaoxin Yin and Jiawei Han. CPAR: Classiﬁcation based on predictive association rules. In Proceedings of the 2003 SIAM International Conference on Data Mining, pages 331–335, 2003. 3492</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
