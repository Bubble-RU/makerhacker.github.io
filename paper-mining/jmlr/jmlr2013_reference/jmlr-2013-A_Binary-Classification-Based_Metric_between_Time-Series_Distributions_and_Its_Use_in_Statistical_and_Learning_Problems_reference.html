<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-2" href="../jmlr2013/jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">jmlr2013-2</a> <a title="jmlr-2013-2-reference" href="#">jmlr2013-2-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</h1>
<br/><p>Source: <a title="jmlr-2013-2-pdf" href="http://jmlr.org/papers/volume14/ryabko13a/ryabko13a.pdf">pdf</a></p><p>Author: Daniil Ryabko, Jérémie Mary</p><p>Abstract: A metric between time-series distributions is proposed that can be evaluated using binary classiﬁcation methods, which were originally developed to work on i.i.d. data. It is shown how this metric can be used for solving statistical problems that are seemingly unrelated to classiﬁcation and concern highly dependent time series. Speciﬁcally, the problems of time-series clustering, homogeneity testing and the three-sample problem are addressed. Universal consistency of the resulting algorithms is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data. Keywords: time series, reductions, stationary ergodic, clustering, metrics between probability distributions</p><br/>
<h2>reference text</h2><p>T. M. Adams and A. B. Nobel. Uniform approximation of Vapnik-Chervonenkis classes. Bernoulli, 18(4):1310–1319, 2012. M.-F. Balcan, N. Bansal, A. Beygelzimer, D. Coppersmith, J. Langford, and G. Sorkin. Robust reductions from ranking to classiﬁcation. In Nader Bshouty and Claudio Gentile, editors, Learning Theory, volume 4539 of Lecture Notes in Computer Science, pages 604–619. 2007. M.-F. Balcan, A. Blum, and S. Vempala. A discriminative framework for clustering via similarity functions. In Proceedings of the 40th Annual ACM Symposium on Theory of Computing, pages 671–680. ACM, 2008. P. Billingsley. Ergodic Theory and Information. Wiley, New York, 1965. D. Bosq. Nonparametric Statistics for Stochastic Processes. Estimation and Prediction. Springer, 1996. Ch.-Ch. Chang and Ch.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27, 2011. Software available at http://www. csie.ntu.edu.tw/˜cjlin/libsvm. C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995. R. Fortet and E. Mourier. Convergence de la r´ partition empirique vers la r´ partition th´ oretique. e e e Ann. Sci. Ec. Norm. Super., III. Ser, 70(3):267–285, 1953. R. Gray. Probability, Random Processes, and Ergodic Properties. Springer Verlag, 1988. M. Gutman. Asymptotically optimal classiﬁcation for multiple tests with empirically observed statistics. IEEE Transactions on Information Theory, 35(2):402–408, 1989. Z. Harchaoui, F. Bach, and E. Moulines. Kernel change-point analysis. In Advances in Neural Information Processing Systems 21, pages 609–616, 2008. L. V. Kantorovich and G. S. Rubinstein. On a function space in certain extremal problems. Dokl. Akad. Nauk USSR, 115(6):1058–1061, 1957. R.L. Karandikar and M. Vidyasagar. Rates of uniform convergence of empirical means with mixing processes. Statistics and Probability Letters, 58:297–307, 2002. 2854  F ROM C LASSIFICATION TO P ROBLEMS ON H IGHLY D EPENDENT T IME S ERIES  A. Khaleghi, D. Ryabko, J. Mary, and P. Preux. Online clustering of processes. In AISTATS, JMLR W&CP; 22, pages 601–609, 2012. A. Khaleghi and D. Ryabko. Locating changes in highly dependent data with unknown number of change points. In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 3095–3103. 2012. A. Khaleghi and D. Ryabko. Nonparametric multiple change point estimation in highly dependent time series. In Proc. 24th International Conf. on Algorithmic Learning Theory (ALT’13), Singapre, 2013. Springer. D. Kifer, Sh. Ben-David, and J. Gehrke. Detecting change in data streams. In Proc. the Thirtieth International Conference on Very Large Data Bases - Volume 30, VLDB’04, pages 180–191, 2004. A.N. Kolmogorov. Sulla determinazione empirica di una legge di distribuzione. G. Inst. Ital. Attuari, pages 83–91, 1933. J. Langford, R. Oliveira, and B. Zadrozny. Predicting conditional quantiles via reduction to classiﬁcation. In Proc. of the 22th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2006. J. del R. Mill´ n. On the need for on-line learning in brain-computer interfaces. In Proc. of the Int. a Joint Conf. on Neural Networks, 2004. D.S. Ornstein and B. Weiss. How sampling reveals a process. Annals of Probability, 18(3):905–930, 1990. D. Pollard. Convergence of Stochastic Processes. Springer, 1984. B. Ryabko. Prediction of random sequences and universal coding. Problems of Information Transmission, 24:87–96, 1988. B. Ryabko. Compression-based methods for nonparametric prediction and estimation of some characteristics of time series. IEEE Transactions on Information Theory, 55:4309–4315, 2009. D. Ryabko. Clustering processes. In Proc. the 27th International Conference on Machine Learning (ICML 2010), pages 919–926, Haifa, Israel, 2010a. D. Ryabko. Discrimination between B-processes is impossible. Journal of Theoretical Probability, 23(2):565–575, 2010b. D. Ryabko. On the relation between realizable and non-realizable cases of the sequence prediction problem. Journal of Machine Learning Research, 12:2161–2180, 2011. D. Ryabko. Testing composite hypotheses about discrete ergodic processes. Test, 21(2):317–329, 2012. D. Ryabko and B. Ryabko. Nonparametric statistical inference for ergodic processes. IEEE Transactions on Information Theory, 56(3):1430–1435, 2010. 2855  RYABKO AND M ARY  D. Ryabko and J. Mary. Reducing statistical time-series problems to binary classiﬁcation. In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 2069–2077. 2012. H. Sakoe and S. Chiba. Dynamic programming algorithm optimization for spoken word recognition. IEEE Transactions on Acoustics, Speech and Signal Processing, 26(1):43–49, 1978. P. Shields. The Ergodic Theory of Discrete Sample Paths. AMS Bookstore, 1996. R. J. Solomonoff. Complexity-based induction systems: comparisons and convergence theorems. IEEE Trans. Information Theory, IT-24:422–432, 1978. V. M. Zolotarev. Probability metrics. Theory of Probability and Its Applications., 28(2):264–287, 1983.  2856</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
