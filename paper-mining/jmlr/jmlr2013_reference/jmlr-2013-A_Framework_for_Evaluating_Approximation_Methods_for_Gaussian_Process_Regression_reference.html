<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-3" href="../jmlr2013/jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">jmlr2013-3</a> <a title="jmlr-2013-3-reference" href="#">jmlr2013-3-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</h1>
<br/><p>Source: <a title="jmlr-2013-3-pdf" href="http://jmlr.org/papers/volume14/chalupka13a/chalupka13a.pdf">pdf</a></p><p>Author: Krzysztof Chalupka, Christopher K. I. Williams, Iain Murray</p><p>Abstract: Gaussian process (GP) predictors are an important component of many Bayesian approaches to machine learning. However, even a straightforward implementation of Gaussian process regression (GPR) requires O(n2 ) space and O(n3 ) time for a data set of n examples. Several approximation methods have been proposed, but there is a lack of understanding of the relative merits of the different approximations, and in what situations they are most useful. We recommend assessing the quality of the predictions obtained as a function of the compute time taken, and comparing to standard baselines (e.g., Subset of Data and FITC). We empirically investigate four different approximation algorithms on four different prediction problems, and make our code available to encourage future comparisons. Keywords: Gaussian process regression, subset of data, FITC, local GP</p><br/>
<h2>reference text</h2><p>R. P. Adams, G. E. Dahl, and I. Murray. Incorporating side information into probabilistic matrix factorization using Gaussian processes. In Proceedings of the 26th Conference on Uncertainty in Artiﬁcial Intelligence, pages 1–9. AUAI Press, 2010. M. Blum, R. W. Floyd, V. Pratt, R. L. Rivest, and R. E. Tarjan. Time bounds for selection. Journal of Computer and System Sciences, 7:448–461, 1973. 347  C HALUPKA , W ILLIAMS AND M URRAY  K. Chalupka. Empirical evaluation of Gaussian process approximation algorithms. Master’s thesis, School of Informatics, University of Edinburgh, 2011. http://homepages.inf.ed.ac.uk/ ckiw/postscript/Chalupka2011diss.pdf. T. Feder and D. H. Greene. Optimal algorithms for approximate clustering. In Proceedings of the 20th ACM Symposium on Theory of Computing, pages 434–444. ACM Press, New York, USA, 1988. ISBN 0-89791-264-0. doi: http://doi.acm.org/10.1145/62212.62255. N. De Freitas, Y. Wang, M. Mahdaviani, and D. Lang. Fast Krylov methods for N-body learning. In Y. Weiss, B. Sch¨ lkopf, and J. Platt, editors, Advances in Neural Information Processing Systems o 18, pages 251–258. MIT Press, 2006. J. Fritz, I. Neuweiler, and W. Nowak. Application of FFT-based algorithms for large-scale universal Kriging problems. Mathematical Geosciences, 41:509–533, 2009. M. Gibbs. Bayesian Gaussian processes for Classiﬁcation and Regression. PhD thesis, University of Cambridge, 1997. G. H. Golub and C. F. Van Loan. Matrix Computations. The John Hopkins University Press, third edition, 1996. T. F. Gonzales. Clustering to minimize the maximum intercluster distance. Theoretical Computer Science, 38(2-3):293–306, 1985. A. Gray. Fast kernel matrix-vector multiplication with application to Gaussian process learning. Technical Report CMU-CS-04-110, School of Computer Science, Carnegie Mellon University, 2004. S. Keerthi and W. Chu. A matching pursuit approach to sparse Gaussian process regression. In Y. Weiss, B. Sch¨ lkopf, and J. Platt, editors, Advances in Neural Information Processing Systems o 18, pages 643–650. MIT Press, Cambridge, MA, 2006. N. Lawrence, M. Seeger, and R. Herbrich. Fast sparse Gaussian process methods: The informative vector machine. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 625–632. MIT Press, 2003. N. D. Lawrence. Gaussian process latent variable models for visualization of high dimensional data. In S. Thrun, L. Saul, and B. Sch¨ lkopf, editors, Advances in Neural Information Processing o Systems 16, pages 329–336. MIT Press, 2004. M. L´ zaro-Gredilla, J. Qui˜ onero-Candela, C. E. Rasmussen, and A. R. Figueiras-Vidal. Sparse a n spectrum Gaussian process regression. Journal of Machine Learning Research, 11:1865–1881, 2010. W. Li, K-H. Lee, and K-S. Leung. Large-scale RLSC learning without agony. In Proceedings of the 24th International Conference on Machine learning, pages 529–536. ACM Press New York, NY, USA, 2007. E. Liberty, F. Woolfe, P-G. Martinsson, V. Rokhlin, and M. Tygert. Randomized algorithms for the low-rank approximation of matrices. Proceedings of the National Academy of Sciences, 104(51): 20167–72, 2007. 348  E VALUATING A PPROXIMATION M ETHODS FOR GPR  M. Malshe, L. M. Raff, M. G. Rockey, M. Hagan, P. M. Agrawal, and R. Komanduri. Theoretical investigation of the dissociation dynamics of vibrationally excited vinyl bromide on an ab initio potential-energy surface obtained using modiﬁed novelty sampling and feedforward neural networks. II. Numerical application of the method. The Journal of Chemical Physics, 127(13): 134105, 2007. S. Manzhos and T. Carrington Jr. Using neural networks, optimized coordinates, and highdimensional model representations to obtain a vinyl bromide potential surface. The Journal of Chemical Physics, 129:224104–1–224104–8, 2008. V. I. Morariu, B. V. Srinivasan, V. C. Raykar, R. Duraiswami, and L. S. Davis. Automatic online tuning for fast Gaussian summation. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1113–1120, 2009. I. Murray. Gaussian processes and fast matrix-vector multiplies, 2009. Presented at the Numerical Mathematics in Machine Learning workshop at the 26th International Conference on Machine Learning (ICML 2009), Montreal, Canada. URL http://www.cs.toronto.edu/˜murray/ pub/09gp_eval/ (as of March 2011). R. M. Neal. Bayesian Learning for Neural Networks. Springer, New York, 1996. Lecture Notes in Statistics 118. C. J. Paciorek. Bayesian smoothing with Gaussian processes using Fourier basis functions in the spectralGP package. Journal of Statistical Software, 19(2):1–38, 2007. URL http://www. jstatsoft.org/v19/i02. J. Qui˜ onero-Candela and C. E. Rasmussen. A unifying view of sparse approximate Gaussian n process regression. Journal of Machine Learning Research, 6:1939–1959, 2005. J. Qui˜ onero-Candela, C. E. Rasmussen, and C. K. I. Williams. Approximation methods for Gausn sian process regression. In L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors, Large Scale Learning Machines, pages 203–223. MIT Press, 2007. C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, Cambridge, Massachusetts, 2006. V. C. Raykar and R. Duraiswami. Fast large scale Gaussian process regression using approximate matrix-vector products. In Learning Workshop 2007, 2007. Available from: http://www.umiacs.umd.edu/˜vikas/publications/raykar_learning_workshop_ 2007_full_paper.pdf. Y. Shen, A. Ng, and M. Seeger. Fast Gaussian process regression using KD-trees. In Y. Weiss, B. Sch¨ lkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages o 1225–1232. MIT Press, 2006. E. Snelson. Flexible and Efﬁcient Gaussian Process Models for Machine Learning. PhD thesis, Gatsby Computational Neuroscience Unit, University College London, 2007. 349  C HALUPKA , W ILLIAMS AND M URRAY  E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Y. Weiss, B. Sch¨ lkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages o 1257–1264, 2006. E. Snelson and Z. Ghahramani. Local and global sparse Gaussian process approximations. In M. Meila and X. Shen, editors, Artiﬁcial Intelligence and Statistics 11. Omnipress, 2007. E. Sudderth and M. Jordan. Shared segmentation of natural scenes using dependent Pitman-Yor processes. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1585–1592, 2009. M. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In Artiﬁcial Intelligence and Statistics 12, volume 5, pages 567–574. JMLR: W&CP;, 2009. C. K. Wikle, R. F. Milliff, D. Nychka, and L. M. Berliner. Spatiotemporal hierarchical Bayesian modeling: tropical ocean surface winds. Journal of the American Statistical Association, 96 (454):382–397, 2001. C. Yang, R. Duraiswami, and L. Davis. Efﬁcient kernel machines using the improved fast Gauss transform. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 1561–1568. MIT Press, 2005.  350</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
