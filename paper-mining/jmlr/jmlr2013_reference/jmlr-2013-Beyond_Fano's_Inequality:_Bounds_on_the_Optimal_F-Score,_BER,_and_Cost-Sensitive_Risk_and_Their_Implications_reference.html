<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>18 jmlr-2013-Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-18" href="../jmlr2013/jmlr-2013-Beyond_Fano%27s_Inequality%3A_Bounds_on_the_Optimal_F-Score%2C_BER%2C_and_Cost-Sensitive_Risk_and_Their_Implications.html">jmlr2013-18</a> <a title="jmlr-2013-18-reference" href="#">jmlr2013-18-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>18 jmlr-2013-Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications</h1>
<br/><p>Source: <a title="jmlr-2013-18-pdf" href="http://jmlr.org/papers/volume14/zhao13a/zhao13a.pdf">pdf</a></p><p>Author: Ming-Jie Zhao, Narayanan Edakunni, Adam Pocock, Gavin Brown</p><p>Abstract: Fano’s inequality lower bounds the probability of transmission error through a communication channel. Applied to classiﬁcation problems, it provides a lower bound on the Bayes error rate and motivates the widely used Infomax principle. In modern machine learning, we are often interested in more than just the error rate. In medical diagnosis, different errors incur different cost; hence, the overall risk is cost-sensitive. Two other popular criteria are balanced error rate (BER) and F-score. In this work, we focus on the two-class problem and use a general deﬁnition of conditional entropy (including Shannon’s as a special case) to derive upper/lower bounds on the optimal F-score, BER and cost-sensitive risk, extending Fano’s result. As a consequence, we show that Infomax is not suitable for optimizing F-score or cost-sensitive risk, in that it can potentially lead to low F-score and high risk. For cost-sensitive risk, we propose a new conditional entropy formulation which avoids this inconsistency. In addition, we consider the common practice of using a threshold on the posterior probability to tune performance of a classiﬁer. As is widely known, a threshold of 0.5, where the posteriors cross, minimizes error rate—we derive similar optimal thresholds for F-score and BER. Keywords: balanced error rate, F-score (Fβ -measure), cost-sensitive risk, conditional entropy, lower/upper bound</p><br/>
<h2>reference text</h2><p>C.D. Aliprantis and K.C. Border. Inﬁnite Dimensional Analysis: a Hitchhiker’s Guide. Springer, 2006. A.J. Bell and T.J. Sejnowski. An information-maximization approach to blind separation and blind deconvolution. Neural Computation, 7(6):1129–1159, 1995. M. Ben-Bassat. f-entropies, probability of error, and feature selection. Information and Control, 39 (3):227–242, 1978. S. Boyd and L. Vandenberghe. Convex Optimization. Camgridge University Press, 2004. R.P. Brent. Algorithms for Minimization with Derivatives. Prentice Hall, 1973. G. Brown, A. Pocock, M.-J. Zhao, and M. Lujan. Conditional likelihood maximisation: a unifying framework for information theoretic feature selection. Journal of Machine Learning Research, 13:27–66, 2012. R. Caruana and A. Niculescu-Mizil. Data mining in metric space: an empirical analysis of supervised learning performance criteria. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 69–78. ACM, 2004. T.M. Cover and J.A. Thomas. Elements of Information Theory. John Wiley & Sons, Inc., 2006. L. Devroye, L. Gyorﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition, volume 31 of Applications of Mathematics. Springer, 1996. W. Duch. Feature Extraction: Foundation and Applications, chapter 3, pages 89–117. Springer, 2006. ISBN 3-540-35487-5. R.O. Duda, P.E. Hart, and D.G. Stork. Pattern Classiﬁcation. John Wiley & Sons, Inc., 2001. C. Elkan. The foundations of cost-sensitive learning. In Proceedings of the 17th International Joint Conference on Artiﬁcial Intelligence, pages 973–978, 2001. D. Erdogmus and J.C. Principe. Lower and upper bounds for misclassiﬁcation probability based on R´ nyi’s information. Journal of VLSI Signal Processing, 37:305–317, 2004. e 1088  O N BER, F-S CORE , C OST-S ENSITIVE R ISK AND C ONDITIONAL E NTROPY  R.M. Fano. Transmission of Information: a Statistical Theory of Communications. MIT Press, 1961. M. Feder and N. Merhav. Relations between entropy and error probability. IEEE Transactions on Information Theory, 40:259–266, 1994. A. Garg and D. Roth. Understanding probabilistic classiﬁers. In 12th European Conference on Machine Learning (ECML), pages 179–191, 2001. J.D. Golic. On the relationship between the information measures and the bayes probability of error. IEEE Transactions on Information Theory, IT-33(5):681–693, 1987. S. Guiasu. Weighted entropy. Reports on Mathematical Physics, 2(3):165–179, 1971. I. Guyon and A. Elisseeff. An introduction to variable and feature selection. Journal of Machine Learning Research, 3:1157–1182, 2003. M.E. Hellman and J. Raviv. Probability of error, equivocation, and the Chernoff bound. IEEE Transactions on Infomation Theory, IT-16(4):368–372, 1970. K.E. Hild II, D. Erdogmus, K. Torkkola, and J.C. Principe. Feature extraction using informationtheoretic learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(9):1385– 1392, 2006. M. Jansche. A maximum expected utility framework for binary sequence labeling. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 736–743, 2007. T. Kailath. The divergence and Bhattacharyya distance measures in signal selection. IEEE Transactions on Communication Technology, 15(1):52–60, February 1967. R. Kohavi and G.H. John. Wrappers for feature subset selection. Artiﬁcial Intelligence, special issue on relevance:273–324, 1997. V.A. Kovalevsky. The problem of character recognition from the point of view of mathematical statistics. In V. A. Kovalevski, editor, Character Readers and Pattern Recognition, pages 3–30, New York, 1968. T.N. Lal, O. Chapelle, J. Weston, and A. Elisseeff. Feature Extraction: Foundation and Applications, chapter 5, pages 137–165. Springer, 2006. D. Lewis. Evaluating and optimizing autonomous text classiﬁcation systems. In SIGIR, pages 246–254, 1995. R. Linsker. Towards an organizing principle for a layered perceptual network. In Advances in Neural Information Processing Systems (NIPS), volume 0, pages 485–494, 1988. R. Linsker. An application of the priciple of maximum information preservation to linear systems. In Advances in Neural Information Processing Systems (NIPS), volume 1, pages 186–194, 1989. C.D. Manning, P. Raghavan, and H. Schuetze. An Introduction to Information Retrieval. Cambridge University Press, 2008. 1089  Z HAO , E DAKUNNI , P OCOCK AND B ROWN  G.H. Nguyen, A. Bouzerdoum, and S.L. Phung. Learning pattern classiﬁcation tasks with imbalanced data sets. In P. Yin, editor, Pattern recognition, chapter 10, pages 193–208. Vukovar, Croatia: In-Teh., 2009. J.C. Principe and D. Xu. Information-theoretic learning using Renyi’s quadratic entropy. In Proceedings of the First International Workshop on Independent Component Analysis and Signal Separation, pages 407–412, 1999. A. R´ nyi. On measures of entropy and information. In Proceedings of the Berkeley Symposium on e Mathematical Statistics and Probability, volume 1, pages 547–561, 1961. R.T. Rockafellar. Convex Analysis. Princeton University Press, 1970. C.E. Shannon. A mathematical theory of communication. The Bell System Technical Journal, 27: 379–423, 623–656, 1948. I. Steinwart. How to compare different loss functions and their risks. Constructive Approximation, 26(2):225–287, 2007. I.J. Taneja. Generalized Information Measures and Their Applications. on-line book, 2001. URL www.mtm.ufsc.br/ taneja/book/book.html. D.L. Tebbe and S.J. Dwyer III. Uncertainty and the probability of error. 14(3):516–518, May 1968. A. Tewari and P.L. Bartlett. On the consistency of multiclass classiﬁcation methods. Journal of Machine Learning Research, 8:1007–1025, 2007. K. Torkkola. Feature extraction by non-parametric mutual information maximization. Journal of Machine Learning Research, 3:1415–1438, 2003.  1090</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
