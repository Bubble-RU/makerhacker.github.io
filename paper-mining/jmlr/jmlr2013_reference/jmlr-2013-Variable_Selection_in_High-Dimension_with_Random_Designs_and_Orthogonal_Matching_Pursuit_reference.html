<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>119 jmlr-2013-Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-119" href="../jmlr2013/jmlr-2013-Variable_Selection_in_High-Dimension_with_Random_Designs_and_Orthogonal_Matching_Pursuit.html">jmlr2013-119</a> <a title="jmlr-2013-119-reference" href="#">jmlr2013-119-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>119 jmlr-2013-Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit</h1>
<br/><p>Source: <a title="jmlr-2013-119-pdf" href="http://jmlr.org/papers/volume14/joseph13a/joseph13a.pdf">pdf</a></p><p>Author: Antony Joseph</p><p>Abstract: The performance of orthogonal matching pursuit (OMP) for variable selection is analyzed for random designs. When contrasted with the deterministic case, since the performance is here measured after averaging over the distribution of the design matrix, one can have far less stringent sparsity constraints on the coefﬁcient vector. We demonstrate that for exact sparse vectors, the performance of the OMP is similar to known results on the Lasso algorithm (Wainwright, 2009). Moreover, variable selection under a more relaxed sparsity assumption on the coefﬁcient vector, whereby one has only control on the ℓ1 norm of the smaller coefﬁcients, is also analyzed. As consequence of these results, we also show that the coefﬁcient estimate satisﬁes strong oracle type inequalities. Keywords: high dimensional regression, greedy algorithms, Lasso, compressed sensing</p><br/>
<h2>reference text</h2><p>R. Baraniuk, M. Davenport, R. DeVore, and M. Wakin. The Johnson-Lindenstrauss lemma meets compressed sensing. Constructive Approximation, 2007. A.R. Barron and A. Joseph. Sparse superposition codes: Fast and reliable at rates approaching capacity with gaussian noise. Technical report, Yale University, 2010. A.R. Barron and A. Joseph. Least squares superposition codes of moderate dictionary size, reliable at rates up to channel capacity. IEEE Trans. Inform. Theory, 58:2541 – 2557, 2012. A.R. Barron, A. Cohen, W. Dahmen, and R.A. DeVore. Approximation and learning by greedy algorithms. Ann. Statist., 36(1):64–94, 2008. T.T. Cai and L. Wang. Orthogonal matching pursuit for sparse signal recovery with noise. IEEE Trans. Inform. Theory, 57(7):4680–4688, 2011. E. Cand` s and T. Tao. The Dantzig selector: Statistical estimation when p is much larger than n. e Ann. Statist., 35(6):2313–2351, 2007. E.J. Cand` s and Y. Plan. Near-ideal model selection by l1 minimization. Ann. Statist., 37(5A): e 2145–2177, 2009. E.J. Cand` s and T. Tao. Decoding by linear programming. IEEE Trans. Inform. Theory, 51(12): e 4203–4215, 2005. E.J. Cand` s and T. Tao. Near-optimal signal recovery from random projections: Universal encoding e strategies? IEEE Trans. Inform. Theory, 52(12):5406–5425, 2006. D.L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289–1306, 2006a. 1798  VARIABLE S ELECTION W ITH OMP  D.L. Donoho. For most large underdetermined systems of equations, the minimal l1-norm nearsolution approximates the sparsest near-solution. Communications On Pure And Applied Mathematics, 59(7):907–934, 2006b. D.L. Donoho, M. Elad, and V.N. Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Trans. Inform. Theory, 52(1):6–18, 2006. W. Feller. An Introduction To Probability Theory And Its Applications. Vol. i. 1950. A.K. Fletcher and S. Rangan. Orthogonal matching pursuit: A Brownian motion analysis. IEEE Trans. Sig. Processing, (99):1–1, 2011. C. Huang, G.H.L. Cheang, and A.R. Barron. Risk of penalized least squares, greedy selection and l1 penalization for ﬂexible function libraries. Technical Report, 2008. L. Jones. A simple lemma for optimization in a Hilbert space, with application to projection pursuit and neural net training. Ann. Statist., 20:608–613, 1992. W.S. Lee, P.L. Bartlett, and R.C. Williamson. Efﬁcient agnostic learning of neural networks with bounded fan-in. IEEE Trans. Inform. Theory, 42(6):2118–2132, 1996. S. Mallat and S.M.Z. Zhang. Matching pursuit with time-frequency dictionaries. IEEE Trans. Signal Processing, 41:3397–3415, 1993. N. Meinshausen and P. Buhlmann. High-dimensional graphs and variable selection with the lasso. Ann. Statist., 34(3):1436–1462, 2006. Y.C. Pati, R. Rezaiifar, and PS Krishnaprasad. Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition. In Conf. Rec. 27th Asilomar Conf. Sig., Sys. and Comput., pages 40–44. IEEE, 1993. S.J. Szarek. Condition numbers of random matrices. Journal of Complexity, 7(2):131–149, 1991. R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. Ser. B Stat. Methodol., pages 267–288, 1996. J.A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans. Inform. Theory, 50(10):2231–2242, 2004. J.A. Tropp. Just relax: Convex programming methods for identifying sparse signals in noise. IEEE Trans. Inform. Theory, 52(3):1030–1051, 2006. J.A. Tropp and A.C. Gilbert. Signal recovery from random measurements via orthogonal matching pursuit. IEEE Trans. Inform. Theory, 53(12):4655–4666, 2007. M.J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using ℓ1 constrained quadratic programming (lasso). IEEE Trans. Inform. Theory, 55(5):2183–2202, 2009. C.H. Zhang and J. Huang. The sparsity and bias of the lasso selection in high-dimensional linear regression. Ann. Statist., 36(4):1567–1594, 2008. 1799  J OSEPH  T. Zhang. On the consistency of feature selection using greedy least squares regression. J. Mach. Learn. Res., 10:555–568, 2009a. T. Zhang. Some sharp performance bounds for least squares regression with l1 regularization. Ann. Statist., 37(5A):2109–2144, 2009b. P. Zhao and B. Yu. On model selection consistency of lasso. J. Mach. Learn. Res., 7:2541–2563, 2006.  1800</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
