<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>107 jmlr-2013-Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-107" href="../jmlr2013/jmlr-2013-Stochastic_Dual_Coordinate_Ascent_Methods_for_Regularized_Loss_Minimization.html">jmlr2013-107</a> <a title="jmlr-2013-107-reference" href="#">jmlr2013-107-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>107 jmlr-2013-Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization</h1>
<br/><p>Source: <a title="jmlr-2013-107-pdf" href="http://jmlr.org/papers/volume14/shalev-shwartz13a/shalev-shwartz13a.pdf">pdf</a></p><p>Author: Shai Shalev-Shwartz, Tong Zhang</p><p>Abstract: Stochastic Gradient Descent (SGD) has become popular for solving large scale supervised machine learning optimization problems such as SVM, due to their strong theoretical guarantees. While the closely related Dual Coordinate Ascent (DCA) method has been implemented in various software packages, it has so far lacked good convergence analysis. This paper presents a new analysis of Stochastic Dual Coordinate Ascent (SDCA) showing that this class of methods enjoy strong theoretical guarantees that are comparable or better than SGD. This analysis justiﬁes the effectiveness of SDCA for practical applications. Keywords: stochastic dual coordinate ascent, optimization, computational complexity, regularized loss minimization, support vector machines, ridge regression, logistic regression</p><br/>
<h2>reference text</h2><p>L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In Advances in Neural Information Processing Systems (NIPS), pages 161–168, 2008. M. Collins, A. Globerson, T. Koo, X. Carreras, and P. Bartlett. Exponentiated gradient algorithms for conditional random ﬁelds and max-margin markov networks. Journal of Machine Learning Research, 9:1775–1822, 2008. Y. Le Cun and L. Bottou. Large scale online learning. In Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference, volume 16, page 217. MIT Press, 2004. C.J. Hsieh, K.W. Chang, C.J. Lin, S.S. Keerthi, and S. Sundararajan. A dual coordinate descent method for large-scale linear SVM. In Proceedings of the International Conference on Machine Learning (ICML), pages 408–415, 2008. D. Hush, P. Kelly, C. Scovel, and I. Steinwart. QP algorithms with guaranteed accuracy and run time for support vector machines. Journal of Machine Learning Research, 7:733–769, 2006. T. Joachims. Making large-scale support vector machine learning practical. In B. Schölkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT Press, 1998. S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Stochastic block-coordinate frank-wolfe optimization for structural svms. arXiv preprint:1207.4747, 2012. N. Le Roux, M. Schmidt, and F. Bach. A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with ﬁnite training sets. In Advances in Neural Information Processing Systems (NIPS): Proceedings of the 2012 Conference, 2012. arXiv preprint:1202.6258. Z.Q. Luo and P. Tseng. On the convergence of coordinate descent method for convex differentiable minimization. J. Optim. Theory Appl., 72:7–35, 1992. O. Mangasarian and D. Musicant. Successive overrelaxation for support vector machines. IEEE Transactions on Neural Networks, 10, 1999. N. Murata. A statistical study of on-line learning. Online Learning and Neural Networks. Cambridge University Press, Cambridge, UK, 1998. Y. Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341–362, 2012. 598  S TOCHASTIC D UAL C OORDINATE A SCENT M ETHODS FOR R EGULARIZED L OSS M INIMIZATION  J. C. Platt. Fast training of Support Vector Machines using sequential minimal optimization. In B. Schölkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT Press, 1998. H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pages 400–407, 1951. S. Shalev-Shwartz and N. Srebro. SVM optimization: Inverse dependence on training set size. In International Conference on Machine Learning (ICML), pages 928–935, 2008. S. Shalev-Shwartz and A. Tewari. Stochastic methods for l1 regularized loss minimization. In Proceedings of the International Conference on Machine Learning (ICML), page 117, 2009. S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal Estimated sub-GrAdient SOlver for SVM. In Proceedings of the International Conference on Machine Learning (ICML), pages 807–814, 2007. K. Sridharan, N. Srebro, and S. Shalev-Shwartz. Fast rates for regularized objectives. In Advances in Neural Information Processing Systems (NIPS), 2009. T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In Proceedings of the Twenty-First International Conference on Machine Learning (ICML), 2004.  599</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
