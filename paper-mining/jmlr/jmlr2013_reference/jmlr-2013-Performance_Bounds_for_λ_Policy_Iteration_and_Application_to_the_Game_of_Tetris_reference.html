<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>87 jmlr-2013-Performance Bounds for λ Policy Iteration and Application to the Game of Tetris</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-87" href="../jmlr2013/jmlr-2013-Performance_Bounds_for_%CE%BB_Policy_Iteration_and_Application_to_the_Game_of_Tetris.html">jmlr2013-87</a> <a title="jmlr-2013-87-reference" href="#">jmlr2013-87-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>87 jmlr-2013-Performance Bounds for λ Policy Iteration and Application to the Game of Tetris</h1>
<br/><p>Source: <a title="jmlr-2013-87-pdf" href="http://jmlr.org/papers/volume14/scherrer13a/scherrer13a.pdf">pdf</a></p><p>Author: Bruno Scherrer</p><p>Abstract: We consider the discrete-time inﬁnite-horizon optimal control problem formalized by Markov decision processes (Puterman, 1994; Bertsekas and Tsitsiklis, 1996). We revisit the work of Bertsekas and Ioffe (1996), that introduced λ policy iteration—a family of algorithms parametrized by a parameter λ—that generalizes the standard algorithms value and policy iteration, and has some deep connections with the temporal-difference algorithms described by Sutton and Barto (1998). We deepen the original theory developed by the authors by providing convergence rate bounds which generalize standard bounds for value iteration described for instance by Puterman (1994). Then, the main contribution of this paper is to develop the theory of this algorithm when it is used in an approximate form. We extend and unify the separate analyzes developed by Munos for approximate value iteration (Munos, 2007) and approximate policy iteration (Munos, 2003), and provide performance bounds in the discounted and the undiscounted situations. Finally, we revisit the use of this algorithm in the training of a Tetris playing controller as originally done by Bertsekas and Ioffe (1996). Our empirical results are different from those of Bertsekas and Ioffe (which were originally qualiﬁed as “paradoxical” and “intriguing”). We track down the reason to be a minor implementation error of the algorithm, which suggests that, in practice, λ policy iteration may be more stable than previously thought. Keywords: stochastic optimal control, reinforcement learning, Markov decision processes, analysis of algorithms</p><br/>
<h2>reference text</h2><p>A. Antos, Cs. Szepesv´ ri, and R. Munos. Value-iteration based ﬁtted policy iteration: Learning with a a single trajectory. In ADPRL, pages 330–337. IEEE, 2007. A. Antos, Cs. Szepesv´ ri, and R. Munos. Learning near-optimal policies with Bellman-residual a minimization based ﬁtted policy iteration and a single sample path. Machine Learning Journal, 71:89–129, 2008. D. Bertsekas. Lambda policy iteration: A review and a new implementation. Technical Report LIDS-2874, MIT, 2011. D. Bertsekas and S. Ioffe. Temporal differences-based policy iteration and applications in neurodynamic programming. Technical Report LIDS-P-2349, MIT, 1996. D.P. Bertsekas and J.N. Tsitsiklis. Neurodynamic Programming. Athena Scientiﬁc, 1996. S.J. Bradtke and A.G. Barto. Linear least-squares algorithms for temporal difference learning. Machine Learning, 22(1-3):33–57, 1996. H. Burgiel. How to Lose at Tetris. Mathematical Gazette, 81:194–200, 1997. C. P. Fahey. Tetris AI, computer plays tetris. http://colinfahey.com/tetris/tetris_en. html, 2003. A.M. Farahmand, R. Munos, and Cs. Szepesv´ ri. Error propagation for approximate policy and a value iteration. In Advances in Neural Information Processing Systems 23 (NIPS 2010), 2010. M.G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4:1107–1149, 2003. A. Lazaric, M. Ghavamzadeh, and R. Munos. Analysis of a classiﬁcation-based policy iteration algorithm. In International Conference on Machine Learning (ICML 2010), pages 607–614, 2010. R. Munos. Error bounds for approximate policy iteration. In International Conference on Machine Learning (ICML 2003), pages 560–567, 2003. R. Munos. Performance bounds in L p -norm for approximate value iteration. SIAM Journal on Control and Optimization, 46(2):541–561, 2007. R. Munos and Cs. Szepesv´ ri. Finite-time bounds for ﬁtted value iteration. Journal of Machine a Learning Research, 9:815–857, 2008. 1226  P ERFORMANCE B OUNDS FOR λ P OLICY I TERATION  A. Nedi´ and D. P. Bertsekas. Least squares policy evaluation algorithms with linear function c approximation. Discrete Event Dynamic Systems, 13:79–110, 2003. M. Puterman. Markov Decision Processes. Wiley, New York, 1994. M. Puterman and M. Shin. Modiﬁed policy iteration algorithms for discounted markov decision problems. Management Science, 24(11), 1978. B. Scherrer and C. Thi´ ry. Performance bound for approximate optimistic policy iteration. Technical e report, INRIA, 2010. B. Scherrer, M. Ghavamzadeh, V. Gabillon, and M. Geist. Approximate modiﬁed policy iteration. In International Conference on Machine Learning (ICML 2012), Edinburgh, Scotland, 2012. S. Singh and P. Dayan. Analytical mean squared error curves for temporal difference learning. Machine Learning Journal, 32(1):5–40, 1998. R.S. Sutton and A.G. Barto. Reinforcement Learning, An introduction. BradFord Book. The MIT Press, 1998. C. Thi´ ry and B. Scherrer. Improvements on learning Tetris with cross entropy. International e Computer Games Association Journal, 32, 2009. C. Thi´ ry and B. Scherrer. Least-squares λ policy iteration: Bias-variance trade-off in control e problems. In International Conference on Machine Learning (ICML 2010), Haifa, Isra¨ l, 2010. e  1227</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
