<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-105" href="../jmlr2013/jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">jmlr2013-105</a> <a title="jmlr-2013-105-reference" href="#">jmlr2013-105-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</h1>
<br/><p>Source: <a title="jmlr-2013-105-pdf" href="http://jmlr.org/papers/volume14/gerchinovitz13a/gerchinovitz13a.pdf">pdf</a></p><p>Author: Sébastien Gerchinovitz</p><p>Abstract: We consider the problem of online linear regression on arbitrary deterministic sequences when the ambient dimension d can be much larger than the number of time rounds T . We introduce the notion of sparsity regret bound, which is a deterministic online counterpart of recent risk bounds derived in the stochastic setting under a sparsity scenario. We prove such regret bounds for an online-learning algorithm called SeqSEW and based on exponential weighting and data-driven truncation. In a second part we apply a parameter-free version of this algorithm to the stochastic setting (regression model with random design). This yields risk bounds of the same ﬂavor as in Dalalyan and Tsybakov (2012a) but which solve two questions left open therein. In particular our risk bounds are adaptive (up to a logarithmic factor) to the unknown variance of the noise if the latter is Gaussian. We also address the regression model with ﬁxed design. Keywords: sparsity, online linear regression, individual sequences, adaptive regret bounds</p><br/>
<h2>reference text</h2><p>F. Abramovich, Y. Benjamini, D.L. Donoho, and I.M. Johnstone. Adapting to unknown sparsity by controlling the false discovery rate. Ann. Statist., 34(2):584–653, 2006. P. Alquier and K. Lounici. PAC-Bayesian bounds for sparse regression estimation with exponential weights. Electron. J. Stat., 5:127–145, 2011. J.-Y. Audibert. Fast learning rates in statistical inference through aggregation. Ann. Statist., 37(4): 1591–1646, 2009. ISSN 0090-5364. P. Auer, N. Cesa-Bianchi, and C. Gentile. Adaptive and self-conﬁdent on-line learning algorithms. J. Comp. Sys. Sci., 64:48–75, 2002. K. S. Azoury and M. K. Warmuth. Relative loss bounds for on-line density estimation with the exponential family of distributions. Mach. Learn., 43(3):211–246, 2001. ISSN 0885-6125. G. Biau, K. Bleakley, L. Gy¨ rﬁ, and G. Ottucs´ k. Nonparametric sequential prediction of time o a series. J. Nonparametr. Stat., 22(3–4):297–317, 2010. P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Ann. Statist., 37(4):1705–1732, 2009. ISSN 0090-5364. L. Birg´ and P. Massart. Gaussian model selection. J. Eur. Math. Soc., 3:203–268, 2001. e L. Birg´ and P. Massart. Minimal penalties for Gaussian model selection. Probab. Theory Relat. e Fields, 138:33–73, 2007. P. B¨ hlmann and S. van de Geer. Statistics for High-Dimensional Data: Methods, Theory and u Applications. Springer Series in Statistics. Springer, Heidelberg, 2011. F. Bunea and A. Nobel. Sequential procedures for aggregating arbitrary estimators of a conditional mean. IEEE Trans. Inform. Theory, 54(4):1725–1735, 2008. ISSN 0018-9448. F. Bunea, A. B. Tsybakov, and M. H. Wegkamp. Aggregation for regression learning. Technical report, 2004. Available at http://arxiv.org/abs/math/0410214. F. Bunea, A. B. Tsybakov, and M. H. Wegkamp. Aggregation for Gaussian regression. Ann. Statist., 35(4):1674–1697, 2007a. ISSN 0090-5364. F. Bunea, A. B. Tsybakov, and M. H. Wegkamp. Sparsity oracle inequalities for the Lasso. Electron. J. Stat., 1:169–194, 2007b. ISSN 1935-7524. E. Candes and T. Tao. The Dantzig selector: statistical estimation when p is much larger than n. Ann. Statist., 35(6):2313–2351, 2007. O. Catoni. Universal aggregation rules with exact bias bounds. Technical Report PMA-510, Laboratoire de Probabilit´ s et Mod` les Al´ atoires, CNRS, Paris, 1999. e e e O. Catoni. Statistical Learning Theory and Stochastic Optimization. Springer, New York, 2004. 767  G ERCHINOVITZ  N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006. N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms. IEEE Trans. Inform. Theory, 50(9):2050–2057, 2004. ISSN 0018-9448. N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with expert advice. Mach. Learn., 66(2/3):321–352, 2007. A. Dalalyan and A. B. Tsybakov. Aggregation by exponential weighting and sharp oracle inequalities. In Proceedings of the 20th Annual Conference on Learning Theory (COLT’07), pages 97–111, 2007. ISBN 978-3-540-72925-9. A. Dalalyan and A. B. Tsybakov. Aggregation by exponential weighting, sharp PAC-Bayesian bounds and sparsity. Mach. Learn., 72(1-2):39–61, 2008. ISSN 0885-6125. A. Dalalyan and A. B. Tsybakov. Mirror averaging with sparsity priors. Bernoulli, 18(3):914–944, 2012a. A. Dalalyan and A. B. Tsybakov. Sparse regression learning by aggregation and Langevin MonteCarlo. J. Comput. System Sci., 78:1423–1443, 2012b. D. L. Donoho and I. M. Johnstone. Ideal spatial adaptation by wavelet shrinkage. Biometrika, 81 (3):425–455, 1994. ISSN 0006-3444. J. C. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT’10), pages 14–26, 2010. Y. Freund, R. E. Schapire, Y. Singer, and M. K. Warmuth. Using and combining predictors that specialize. In Proceedings of the 29th Annual ACM Symposium on Theory of Computing (STOC’97), pages 334–343, 1997. S. Gerchinovitz. Sparsity regret bounds for individual sequences in online linear regression. JMLR Workshop and Conference Proceedings, 19 (COLT 2011 Proceedings):377–396, 2011. S. Gerchinovitz and J.Y. Yu. Adaptive and optimal online linear regression on ℓ1 -balls. In J. Kivinen, C. Szepesv´ ri, E. Ukkonen, and T. Zeugmann, editors, Algorithmic Learning Theory, volume a 6925 of Lecture Notes in Computer Science, pages 99–113. Springer Berlin/Heidelberg, 2011. L. Gy¨ rﬁ and G. Ottucs´ k. Sequential prediction of unbounded stationary time series. IEEE Trans. o a Inform. Theory, 53(5):1866–1872, 2007. L. Gy¨ rﬁ, M. Kohler, A. Krzy˙ ak, and H. Walk. A Distribution-Free Theory of Nonparametric o z Regression. Springer Series in Statistics. Springer-Verlag, New York, 2002. ISBN 0-387-954414. M. Hebiri and S. van de Geer. The Smooth-Lasso and other ℓ1 + ℓ2 -penalized methods. Electron. J. Stat., 5:1184–1226, 2011. A. Juditsky, P. Rigollet, and A. B. Tsybakov. Learning by mirror averaging. Ann. Statist., 36(5): 2183–2206, 2008. ISSN 0090-5364. 768  S PARSITY R EGRET B OUNDS FOR I NDIVIDUAL S EQUENCES  J. Kivinen and M. K. Warmuth. Averaging expert predictions. In Proceedings of the 4th European Conference on Computational Learning Theory (EuroCOLT’99), pages 153–167, 1999. V. Koltchinskii. Sparse recovery in convex hulls via entropy penalization. Ann. Statist., 37(3): 1332–1359, 2009a. ISSN 0090-5364. V. Koltchinskii. Sparsity in penalized empirical risk minimization. Ann. Inst. Henri Poincar´ e Probab. Stat., 45(1):7–57, 2009b. ISSN 0246-0203. V. Koltchinskii, K. Lounici, and A.B. Tsybakov. Nuclear norm penalization and optimal rates for noisy low-rank matrix completion. Ann. Statist., 39(5):2302–2329, 2011. J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated gradient. J. Mach. Learn. Res., 10:777–801, 2009. ISSN 1532–4435. N. Littlestone. From on-line to batch learning. In Proceedings of the 2nd Annual Conference on Learning Theory (COLT’89), pages 269–284, 1989. K. Lounici, M. Pontil, S. van de Geer, and A.B. Tsybakov. Oracle inequalities and optimal inference under group sparsity. Ann. Statist., 39(4):2164–2204, 2011. P. Massart. Concentration Inequalities and Model Selection, volume 1896 of Lecture Notes in Mathematics. Springer, Berlin, 2007. P. Rigollet and A. B. Tsybakov. Exponential Screening and optimal rates of sparse estimation. Ann. Statist., 39(2):731–771, 2011. M. W. Seeger. Bayesian inference and optimal design for the sparse linear model. J. Mach. Learn. Res., 9:759–813, 2008. S. Shalev-Shwartz and A. Tewari. Stochastic methods for ℓ1 -regularized loss minimization. J. Mach. Learn. Res., 12:1865–1892, 2011. R. Tibshirani. Regression shrinkage and selection via the Lasso. J. Roy. Statist. Soc. Ser. B, 58(1): 267–288, 1996. S. A. van de Geer. High-dimensional generalized linear models and the Lasso. Ann. Statist., 36(2): 614–645, 2008. ISSN 0090-5364. S. A. van de Geer and P. B¨ hlmann. On the conditions used to prove oracle results for the Lasso. u Electron. J. Stat., 3:1360–1392, 2009. ISSN 1935-7524. V. Vovk. Competitive on-line statistics. Internat. Statist. Rev., 69:213–248, 2001. L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. J. Mach. Learn. Res., 11:2543–2596, 2010.  769</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
