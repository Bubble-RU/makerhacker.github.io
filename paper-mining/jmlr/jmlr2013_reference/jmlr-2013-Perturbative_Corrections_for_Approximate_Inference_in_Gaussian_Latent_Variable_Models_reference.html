<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-88" href="../jmlr2013/jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">jmlr2013-88</a> <a title="jmlr-2013-88-reference" href="#">jmlr2013-88-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</h1>
<br/><p>Source: <a title="jmlr-2013-88-pdf" href="http://jmlr.org/papers/volume14/opper13a/opper13a.pdf">pdf</a></p><p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian ﬁeld, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model’s partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP’s local matching of moments. Through the expansion, we see that EP is correct to ﬁrst order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution. Keywords: expectation consistent inference, expectation propagation, perturbation correction, Wick expansions, Ising model, Gaussian process</p><br/>
<h2>reference text</h2><p>D. Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press, 2012. C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. S. Blinnikov and R. Moessner. Expansions for nearly Gaussian distributions. Astronomy and Astrophysics Supplement Series, 130:193–205, 1998. J. P. Boyd. The devil’s invention: Asymptotic, superasymptotic and hyperasymptotic series. Acta Applicandae Mathematicae, 56:1–98, 1999. M. Chertkov and V. Y. Chernyak. Loop series for discrete statistical models on graphs. Journal of Statistical Mechanics: Theory and Experiment, 2006:P06009, 2006. B. Cseke and T. Heskes. Approximate marginals in latent Gaussian models. Journal of Machine Learning Research, 12:417–457, 2011. M. Kuss and C. E. Rasmussen. Assessing approximate inference for binary Gaussian process classiﬁcation. Journal of Machine Learning Research, 6:1679–1704, 2005. M. M´ zard, G. Parisi, and M. A. Virasoro. Spin Glass Theory and Beyond, volume 9 of Lecture e Notes in Physics. World Scientiﬁc, 1987. T. P. Minka. Expectation propagation for approximate Bayesian inference. In UAI 2001, pages 362–369, 2001a. T. P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, MIT Media Lab, 2001b. T. P. Minka. Power EP. Technical Report MSR-TR-2004-149, Microsoft Research Ltd, 2004. 2897  O PPER , PAQUET AND W INTHER  T. P. Minka and Y. Qi. Tree-structured approximations by expectation propagation. In Advances in Neural Information Processing Systems 16. 2004. K. P. Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012. M. Opper and O. Winther. Gaussian processes for classiﬁcation: Mean ﬁeld algorithms. Neural Computation, 12:2655–2684, 2000. M. Opper and O. Winther. Expectation consistent approximate inference. Journal of Machine Learning Research, 6:2177–2204, 2005. M. Opper, U. Paquet, and O. Winther. Improving on expectation propagation. In Advances in Neural Information Processing Systems 21, pages 1241–1248. 2009. U. Paquet, M. Opper, and O. Winther. Perturbation corrections in approximate inference: Mixture modelling applications. Journal of Machine Learning Research, 10:935–976, 2009. C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2005. H. Rue, S. Martino, and N. Chopin. Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(2):319–392, 2009. M. W. Seeger and H. Nickisch. Fast convergent algorithms for expectation propagation approximate Bayesian inference. Arxiv preprint arXiv:1012.3584, 2010. D. Sherrington and S. Kirckpatrick. Solvable model of a spin-glass. Phys. Rev. Lett., 35(26):1792– 1796, December 1975. E. Sudderth, M. Wainwright, and A. Willsky. Loop series and Bethe variational bounds in attractive graphical models. In Advances in Neural Information Processing Systems 20, pages 1425–1432. 2008. D. J. Thouless, P. W. Anderson, and R. G. Palmer. Solution of a ‘solvable model of a spin glass’. Phil. Mag., 35:593, 1977. M. A. J. van Gerven, B. Cseke, F. P. de Lange, and T. Heskes. Efﬁcient Bayesian multivariate fMRI analysis using a sparsifying spatio-temporal prior. NeuroImage, 50(1):150–161, 2010. M. J. Wainwright and M. I. Jordan. Log-determinant relaxation for approximate inference in discrete Markov random ﬁelds. IEEE Transactions on Signal Processing, 54(6):2099–2109, 2006. M. Welling, A. Gelfand, and A. Ihler. A cluster-cumulant expansion at the ﬁxed points of belief propagation. In Uncertainty in Artiﬁcial Intelligence (UAI). 2012.  2898</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
