<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-111" href="../jmlr2013/jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">jmlr2013-111</a> <a title="jmlr-2013-111-reference" href="#">jmlr2013-111-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</h1>
<br/><p>Source: <a title="jmlr-2013-111-pdf" href="http://jmlr.org/papers/volume14/mairal13a/mairal13a.pdf">pdf</a></p><p>Author: Julien Mairal, Bin Yu</p><p>Abstract: We consider supervised learning problems where the features are embedded in a graph, such as gene expressions in a gene network. In this context, it is of much interest to automatically select a subgraph with few connected components; by exploiting prior knowledge, one can indeed improve the prediction performance or obtain results that are easier to interpret. Regularization or penalty functions for selecting features in graphs have recently been proposed, but they raise new algorithmic challenges. For example, they typically require solving a combinatorially hard selection problem among all connected subgraphs. In this paper, we propose computationally feasible strategies to select a sparse and well-connected subset of features sitting on a directed acyclic graph (DAG). We introduce structured sparsity penalties over paths on a DAG called “path coding” penalties. Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. The penalties and their proximal operators involve path selection problems, which we efﬁciently solve by leveraging network ﬂow optimization. We experimentally show on synthetic, image, and genomic data that our approach is scalable and leads to more connected subgraphs than other regularization functions for graphs. Keywords: convex and non-convex optimization, network ﬂow optimization, graph sparsity</p><br/>
<h2>reference text</h2><p>N. Ahmed, T. Natarajan, and K.R. Rao. Discrete cosine transform. IEEE Transactions on Computers, C-23(1):90–93, 1974. R.K. Ahuja, T.L. Magnanti, and J.B. Orlin. Network Flows. Prentice Hall, 1993. H. Akaike. Information theory and an extension of the maximum likelihood principle. In Second International Symposium on Information Theory, volume 1, pages 267–281, 1973. F. Bach. Exploring large feature spaces with hierarchical multiple kernel learning. In Advances in Neural Information Processing Systems (NIPS), 2008. F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization with sparsity-inducing penalties. Foundation and Trends in Machine Learning, 4:1–106, 2012. A. Barron, J. Rissanen, and B. Yu. The minimum description length principle in coding and modeling. IEEE Transactions on Information Theory, 44(6):2743–2760, 1998. A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009. E. Bernard, L. Jacob, J. Mairal, and J.-P. Vert. Efﬁcient RNA isoform identiﬁcation and quantiﬁcation from RNA-Seq data with network ﬂows. Technical Report hal-00803134, 2013. D.P. Bertsekas. Network Optimization: Continuous and Discrete Models. Athena Scientiﬁc, 1998. T. Blumensath and M.E. Davies. Iterative hard thresholding for compressed sensing. Applied and Computational Harmonic Analysis, 27(3):265–274, 2009. M. Bogu˜ a, R. Pastor-Satorras, A. D´az-Guilera, and A. Arenas. Models of social networks based n´ ı on social distance attachment. Physical Review E, 70(5):056122, 2004. S. P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004. 2482  F EATURE S ELECTION IN G RAPHS WITH PATH C ODING P ENALTIES  Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(11):1222–1239, 2001. A. Buades, B. Coll, and J.M. Morel. A review of image denoising algorithms, with a new one. SIAM Multiscale Modelling and Simulation, 4(2):490, 2005. V. Cehver, M. Duarte, C. Hedge, and R. G. Baraniuk. Sparse signal recovery using Markov random ﬁelds. In Advances in Neural Information Processing Systems (NIPS), 2008. A. Chambolle and J. Darbon. On total variation minimization and surface evolution using parametric maximal ﬂows. International Journal of Computer Vision, 84(3):288–307, 2009. S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientiﬁc Computing, 20(1):33–61, 1999. X. Chen, Q. Lin, S. Kim, J. Pena, J.G. Carbonell, and E.P. Xing. Smoothing proximal gradient method for general structured sparse learning. In Proceedings of the Twenty-Seven Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2011. H.Y. Chuang, E. Lee, Y.T. Liu, D. Lee, and T. Ideker. Network-based classiﬁcation of breast cancer metastasis. Molecular Systems Biology, 3(140), 2007. T.H. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein. Introduction to Algorithms. MIT Press, 2001. C. Couprie, L. Grady, H. Talbot, and L. Najman. Combinatorial continuous maximum ﬂow. SIAM Journal on Imaging Sciences, 4:905–930, 2011. T.M. Cover and J.A Thomas. Elements of Information Theory. Wiley, 2006. 2nd edition. K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3D transformdomain collaborative ﬁltering. IEEE Transactions on Image Processing, 16(8):2080–2095, 2007. I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Communications on Pure and Applied Mathematics, 57(11): 1413–1457, 2004. M. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned dictionaries. IEEE Transactions on Image Processing, 54(12):3736–3745, 2006. J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348–1360, 2001. L.R. Ford and D.R. Fulkerson. Maximal ﬂow through a network. Canadian Journal of Mathematics, 8(3):399–404, 1956. P. Gleiser and L. Danon. Community structure in jazz. Advances in Complex Systems, 6(4):565– 573, 2003. A.V. Goldberg. An Efﬁcient Implementation of a Scaling Minimum-Cost Flow Algorithm. Journal of Algorithms, 22(1):1–29, 1997. 2483  M AIRAL AND Y U  A.V. Goldberg and R.E. Tarjan. A new approach to the maximum ﬂow problem. In Proceedings of the ACM Symposium on Theory of Computing, 1986. R. Guimer` , L. Danon, A. D´az Guilera, F. Giralt, and A. Arenas. Self-similar community structure a ı in a network of human interactions. Physical Review E, 68(6):065103, 2003. D.S. Hochbaum. Complexity and algorithms for nonlinear optimization problems. Annals of Operations Research, 153(1):257–296, 2007. H. Hoeﬂing. A path algorithm for the fused lasso signal approximator. Journal of Computational and Graphical Statistics, 19(4):984–1006, 2010. J. Huang, T. Zhang, and D. Metaxas. Learning with structured sparsity. Journal of Machine Learning Research, 12:3371–3412, 2011. D.R. Hunter and K. Lange. A tutorial on MM algorithms. The American Statistician, 58(1):30–37, 2004. L. Jacob, G. Obozinski, and J.-P. Vert. Group Lasso with overlap and graph Lasso. In Proceedings of the International Conference on Machine Learning (ICML), 2009. R. Jenatton, J-Y. Audibert, and F. Bach. Structured variable selection with sparsity-inducing norms. Journal of Machine Learning Research, 12:2777–2824, 2011. D.J.C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University Press, 2003. J. Mairal. Sparse Coding for Machine Learning, Image Processing and Computer Vision. PhD thesis, Ecole Normale Sup´ rieure de Cachan, 2010. http://tel.archives-ouvertes.fr/ e tel-00595312. J. Mairal. Optimization with ﬁrst-order surrogate functions. In Proceedings of the International Conference on Machine Learning (ICML), 2013. J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Non-local sparse models for image restoration. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2009. J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse coding. Journal of Machine Learning Research, 11:19–60, 2010. J. Mairal, R. Jenatton, G. Obozinski, and F. Bach. Convex and network ﬂow optimization for structured sparsity. Journal of Machine Learning Research, 12:2649–2689, 2011. S. Mallat and Z. Zhang. Matching pursuit in a time-frequency dictionary. IEEE Transactions on Signal Processing, 41(12):3397–3415, 1993. N. Meinshausen and P. B¨ hlmann. Stability selection. Journal of the Royal Statistical Society: u Series B (Statistical Methodology), 72(4):417–473, 2010. 2484  F EATURE S ELECTION IN G RAPHS WITH PATH C ODING P ENALTIES  A. Nemirovsky and D. Yudin. Problem Complexity and Method Efﬁciency in Optimization. Wiley, 1983. Y. Nesterov. Gradient methods for minimizing composite objective function. Technical report, CORE Discussion paper, 2007. J. Nocedal and S.J. Wright. Numerical Optimization. Springer Verlag, 2006. 2nd edition. G. Obozinski and F. Bach. Convex relaxations for combinatorial penalties. Technical Report arXiv:1205.1240v1, 2012. M. R. Osborne, B. Presnell, and B. A. Turlach. On the Lasso and its dual. Journal of Computational and Graphical Statistics, 9(2):319–37, 2000. J. Portilla, V. Strela, M. J. Wainwright, and E. P. Simoncelli. Image denoising using scale mixtures of Gaussians in the wavelet domain. IEEE Transactions on Image Processing, 12(11):1338–1351, 2003. F. Rapaport, A. Zinovyev, M. Dutreix, E. Barillot, and J.-P. Vert. Classiﬁcation of microarray data using gene networks. BMC Bioinformatics, 8(1):35, 2007. J. Rissanen. Modeling by shortest data description. Automatica, 14(5):465–471, 1978. M. Schmidt, N. Le Roux, and F. Bach. Convergence rates of inexact proximal-gradient methods for convex optimization. In Advances in Neural Information Processing Systems (NIPS), 2011. G. Schwarz. Estimating the dimension of a model. Annals of Statistics, 6(2):461–464, 1978. R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society: Series B, 58(1):267–288, 1996. B.A. Turlach, W.N. Venables, and S.J. Wright. Simultaneous variable selection. Technometrics, 47 (3):349–363, 2005. M.H. Van De Vijver et al. A gene-expression signature as a predictor of survival in breast cancer. The New England Journal of Medicine, 347(25):1999–2009, 2002. S. Wright, R. Nowak, and M. Figueiredo. Sparse reconstruction by separable approximation. IEEE Transactions on Signal Processing, 57(7):2479–2493, 2009. M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B, 68:49–67, 2006. P. Zhao, G. Rocha, and B. Yu. The composite absolute penalties family for grouped and hierarchical variable selection. Annals of Statistics, 37(6A):3468–3497, 2009. H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B, 67(2):301–320, 2005.  2485</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
