<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-48" href="../jmlr2013/jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">jmlr2013-48</a> <a title="jmlr-2013-48-reference" href="#">jmlr2013-48-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</h1>
<br/><p>Source: <a title="jmlr-2013-48-pdf" href="http://jmlr.org/papers/volume14/hernandez-lobato13a/hernandez-lobato13a.pdf">pdf</a></p><p>Author: Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Pierre Dupont</p><p>Abstract: We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efﬁciently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about speciﬁc groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy. Keywords: group feature selection, generalized spike-and-slab priors, expectation propagation, sparse linear model, approximate inference, sequential experimental design, signal reconstruction</p><br/>
<h2>reference text</h2><p>H. Attias. A variational Bayesian framework for graphical models. In In Advances in Neural Information Processing Systems, volume 12, pages 209–215. MIT Press, 2000. F. R. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine Learning Research, 9:1179–1225, 2008. ISSN 1532-4435. J. A. Bazerque, G. Mateos, and G. B. Giannakis. Group-lasso on splines for spectrum cartography. Transactions on Signal Proccesing, 59(10):4648–4663, October 2011. ISSN 1053-587X. C. M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer, August 2006. ISBN 0387310738. J. Blitzer, M. Dredze, and F. Pereira. Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classiﬁcation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–407, Prague, Czech Republic, 2007. E. J. Candes and M. B. Wakin. An introduction to compressive sampling. IEEE Signal Processing Magazine, 25(2):21 –30, march 2008. ISSN 1053-5888. C. M. Carvalho, N. G. Polson, and J. G. Scott. Handling sparsity via the horseshoe. Journal of Machine Learning Research W&CP;, 5:73–80, 2009. K. Chaloner and I. Verdinelli. Bayesian experimental design: A review. Statistical Science, 10: 273–304, 1995. P. Damien, J. Wakeﬁeld, and S. Walker. Gibbs sampling for Bayesian non-conjugate and hierarchical models by using auxiliary variables. Journal of the Royal Statistical Society Series B, 61(2): 331–344, 1999. J. Demˇar. Statistical comparisons of classiﬁers over multiple data sets. Journal of Machine Learns ing Research, 7:1–30, 2006. ISSN 1533-7928. D. L. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289–1306, 2006. A. C. Faul and M. E. Tipping. Analysis of sparse Bayesian learning. In Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 383–389. MIT Press, 2001. V. Fedorov. Theory of Optimal Experiments. Academic Press, 1972. E. I. George and R. E. McCulloch. Approaches for Bayesian variable selection. Statistica Sinica, 7 (2):339–373, 1997. 1941  ´ ´ H ERN ANDEZ -L OBATO , H ERN ANDEZ -L OBATO AND D UPONT  M. Van Gerven, B. Cseke, R. Oostenveld, and T. Heskes. Bayesian source localization with the multivariate Laplace prior. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 1901–1909, 2009. J. Geweke. Variable selection and model comparison in regression. Bayesian Statistics, 5:609–620, 1996. P. E. Gill, G. H. Golub, W. Murray, and M. A. Saunders. Methods for modifying matrix factorizations. Mathematics of Computation, 28(126):505–535, 1974. D. Hern´ ndez-Lobato. Prediction Based on Averages over Automatically Induced Learna ers: Ensemble Methods and Bayesian Techniques. PhD thesis, Computer Science Department, Universidad Aut´ noma de Madrid, 2009. o Online available at: ˜ http://arantxa.ii.uam.es/dhernan/docs/Thesis color links.pdf. D. Hern´ ndez-Lobato, J. M. Hern´ ndez-Lobato, T. Helleputte, and P. Dupont. Expectation propaa a gation for Bayesian multi-task feature selection. In Jos´ L. Balc´ zar, Francesco Bonchi, Aristides e a Gionis, and Mich` le Sebag, editors, Proceedings of the European Conference on Machine Learne ing, volume 6321, pages 522–537. Springer, 2010. J. M. Hern´ ndez-Lobato. Balancing Flexibility and Robustness in Machine Learning: Semia parametric Methods and Sparse Linear Models. PhD thesis, Computer Science Department, Universidad Aut´ noma de Madrid, 2010. o J. M. Hern´ ndez-Lobato and T. Dijkstra. Hub gene selection methods for the reconstruction of a transcription networks. In Jos´ L. Balc´ zar, Francesco Bonchi, Aristides Gionis, and Mich` le See a e bag, editors, Machine Learning and Knowledge Discovery in Databases, European Conference, Proceedings, Part I, volume 6321, pages 506–521, 2010. J. M. Hern´ ndez-Lobato, T. Dijkstra, and T. Heskes. Regulator discovery from gene expression a time series of malaria parasites: A hierarchical approach. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 649–656. The MIT Press, 2008. J. M. Hern´ ndez-Lobato, D. Hern´ ndez-Lobato, and A. Su´ rez. Network-based sparse Bayesian a a a classiﬁcation. Pattern Recognition, 44:886–900, 2011. J. Huang and T. Zhang. The beneﬁt of group sparsity. The Annals of Statistics, 38:1978–2004, 2010. T. S. Jaakkola. Tutorial on variational approximation methods. In M. Opper and D. Saad, editors, Advances in Mean Field Methods: Theory and Practice, pages 129–159. MIT Press, 2001. S. Ji and L. Carin. Bayesian compressive sensing and projection optimization. In Proceedings of the 24th International Conference on Machine Learning, pages 377–384. ACM, 2007. ISBN 978-1-59593-793-3. S. Ji, Y. Xue, and L. Carin. Bayesian compressive sensing. IEEE Transactions on Signal Processing, 56(6):2346 –2356, june 2008. ISSN 1053-587X. 1942  G ENERALIZED S PIKE - AND -S LAB P RIORS FOR BAYESIAN G ROUP F EATURE S ELECTION  S. Ji, D. Dunson, and L. Carin. Multitask compressive sensing. IEEE Transactions on Signal Processing, 57(1):92 –106, January 2009. ISSN 1053-587X. I. M. Johnstone and D. M. Titterington. Statistical challenges of high-dimensional data. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 367 (1906):4237, 2009. H. J. Kappen and Vicenc G´ mez. The variational garrote. Machine Learning, pages 1–17, 2013. ¸ o Submitted. S. Kim and E. P. Xing. Feature selection via block-regularized regression. In David A. McAllester and Petri Myllym¨ ki, editors, Proceedings of the 24th Conference in Uncertainty in Artiﬁcial a Intelligence, pages 325–332. AUAI Press, 2008. Y. Kim, J. Kim, and Y. Kim. Blockwise sparse regression. Statistica Sinica, 16(2):375, 2006. ISSN 1017-0405. N. Lawrence, M. W. Seeger, and R. Herbrich. Fast sparse Gaussian process methods: The informative vector machine. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 609–616. MIT Press, Cambridge, MA, 2003. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. K. E. Lee, N. Sha, E. R. Dougherty, M. Vannucci, and B. K. Mallick. Gene selection: A Bayesian variable selection approach. Bioinformatics, 19(1):90–97, 2003. Y. Li, C. Campbell, and M. Tipping. Bayesian automatic relevance determination algorithms for classifying gene expression data. Bioinformatics, 18(10):1332–1339, 2002. D. J. C. MacKay. Information-based objective functions for active data selection. Neural Computation, 4:590–604, 1991. D. J. C. MacKay. Information Theory, Inference & Learning Algorithms. Cambridge University Press, Cambridge, UK., 2003. ISBN 0521642981. D. M. Malioutov, M. Cetin, and A. S. Willsky. A sparse signal reconstruction perspective for source ¸ localization with sensor arrays. IEEE Transactions on Signal Processing, 53(8-2):3010–3022, 2005. L. Meier, S. Van De Geer, and P. B¨ hlmann. The group lasso for logistic regression. Journal u of the Royal Statistical Society: Series B (Statistical Methodology), 70(1):53–71, 2008. ISSN 1467-9868. T. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, Massachusetts Institute of Technology, 2001. T. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In Adnan Darwiche and Nir Friedman, editors, Proceedings of the 18th Conference on Uncertainty in Artiﬁcial Intelligence, pages 352–359. Morgan Kaufmann, 2002. 1943  ´ ´ H ERN ANDEZ -L OBATO , H ERN ANDEZ -L OBATO AND D UPONT  T. J. Mitchell and J. J. Beauchamp. Bayesian variable selection in linear regression. Journal of the American Statistical Association, 83(404):1023–1032, 1988. R. M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical Report CRG-TR-93-1, Dept. of Computer Science, University of Toronto, 1993. H. Nickisch and C. E. Rasmussen. Approximations for binary Gaussian process classiﬁcation. Journal of Machine Learning Research, 9:2035–2078, 10 2008. B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? Sentiment classiﬁcation using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 79–86, 2002. A. T. Puig, A. Wiesel, G. F., and A. O. Hero. Multidimensional shrinkage-thresholding operator and group lasso penalties. IEEE Signal Processing Letters, 18(6):363–366, 2011. R Development Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2011. URL http://www.R-project.org/. ISBN 3-900051-07-0. S. Raman, T. J. Fuchs, P. J. Wild, E. Dahl, and V. Roth. The Bayesian group-lasso for analyzing contingency tables. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 881–888, New York, NY, USA, 2009. ACM. ISBN 978-1-60558516-1. V. Roth and B. Fischer. The group-lasso for generalized linear models: uniqueness of solutions and efﬁcient algorithms. In A. McCallum and S. Roweis, editors, Proceedings of the 25th International Conference on Machine Learning, pages 848–855, 2008. T. Sandler, J. Blitzer, P. P. Talukdar, and L. H. Ungar. Regularized learning with networks of features. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1401–1408. 2009. F. Scheipl, L. Fahrmeir, and T. Kneib. Spike-and-slab priors for function selection in structured additive regression models. Journal of the American Statistical Association, 107:1518–1532, 2012. J. G. Scott. Parameter expansion in local-shrinkage models. arXiv:1010.5265v1.  ArXiv E-prints, 2010.  M. W. Seeger. Bayesian inference and optimal design for the sparse linear model. Journal of Machine Learning Research, 9:759–813, 2008. M. W. Seeger, H. Nickisch, R. Pohmann, and B. Sch¨ lkopf. Optimization of k-space trajectories for o compressed sensing by Bayesian experimental design. Magnetic Resonance in Medicine, 63(1): 116–126, 2009. R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1):267–288, 1996. 1944  G ENERALIZED S PIKE - AND -S LAB P RIORS FOR BAYESIAN G ROUP F EATURE S ELECTION  M. E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1:211–244, 2001. M. E. Tipping and A. Faul. Fast marginal likelihood maximisation for sparse Bayesian models. In Proceedings of the Ninth International Workshop on Artiﬁcial Intelligence and Statistics, pages 3–6, 2003. J. E. Vogt and V. Roth. The group-lasso: ℓ1,∞ regularization versus ℓ1,2 regularization. In Goesele et al., editor, 32nd Anual Symposium of the German Association for Pattern Recognition, volume 6376, pages 252–261. Springer, 2010. M. Wakin, M. Duarte, S. Sarvotham, D. Baron, and R. Baraniuk. Recovery of jointly sparse signals from few random projections. In Y. Weiss, B. Sch¨ lkopf, and J. Platt, editors, Advances in Neural o Information Processing Systems 18, pages 1433–1440. MIT Press, Cambridge, MA, 2006. M. West. Bayesian factor regression models in the ”large p, small n” paradigm. In Bayesian Statistics 7, pages 723–732. Oxford University Press, 2003. T. J. Yen and Y. M. Yen. Grouped variable selection via nested spike and slab priors. ArXiv E-prints, 2011. arXiv:1106.5837v1. M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49–67, 2006. ISSN 1467-9868. H. Zhu and R. Rohwer. Bayesian invariant measurements of generalization. Neural Processing Letters, 2:28–31, 1995. ISSN 1370-4621.  1945</p>
<br/>
<br/><br/><br/></body>
</html>
