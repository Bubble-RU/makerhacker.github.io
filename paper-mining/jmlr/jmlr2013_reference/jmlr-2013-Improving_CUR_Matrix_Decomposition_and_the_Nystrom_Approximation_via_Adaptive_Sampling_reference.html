<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 jmlr-2013-Improving CUR Matrix Decomposition and the Nystrom Approximation via Adaptive Sampling</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-53" href="../jmlr2013/jmlr-2013-Improving_CUR_Matrix_Decomposition_and_the_Nystrom_Approximation_via_Adaptive_Sampling.html">jmlr2013-53</a> <a title="jmlr-2013-53-reference" href="#">jmlr2013-53-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>53 jmlr-2013-Improving CUR Matrix Decomposition and the Nystrom Approximation via Adaptive Sampling</h1>
<br/><p>Source: <a title="jmlr-2013-53-pdf" href="http://jmlr.org/papers/volume14/wang13c/wang13c.pdf">pdf</a></p><p>Author: Shusen Wang, Zhihua Zhang</p><p>Abstract: The CUR matrix decomposition and the Nystr¨ m approximation are two important low-rank matrix o approximation techniques. The Nystr¨ m method approximates a symmetric positive semideﬁnite o matrix in terms of a small number of its columns, while CUR approximates an arbitrary data matrix by a small number of its columns and rows. Thus, CUR decomposition can be regarded as an extension of the Nystr¨ m approximation. o In this paper we establish a more general error bound for the adaptive column/row sampling algorithm, based on which we propose more accurate CUR and Nystr¨ m algorithms with expected o relative-error bounds. The proposed CUR and Nystr¨ m algorithms also have low time complexity o and can avoid maintaining the whole data matrix in RAM. In addition, we give theoretical analysis for the lower error bounds of the standard Nystr¨ m method and the ensemble Nystr¨ m method. o o The main theoretical results established in this paper are novel, and our analysis makes no special assumption on the data matrices. Keywords: large-scale matrix computation, CUR matrix decomposition, the Nystr¨ m method, o randomized algorithms, adaptive sampling</p><br/>
<h2>reference text</h2><p>A. Ben-Israel and T. N. E. Greville. Generalized Inverses: Theory and Applications. Second Edition. Springer, 2003. M. W. Berry, S. A. Pulatova, and G. W. Stewart. Algorithm 844: computing sparse reduced-rank approximations to sparse matrices. ACM Transactions on Mathematical Software, 31(2):252– 269, 2005. J. Bien, Y. Xu, and M. W. Mahoney. CUR from a sparse optimization viewpoint. In Advances in Neural Information Processing Systems (NIPS). 2010. C. H. Bischof and P. C. Hansen. Structure-preserving and rank-revealing QR-factorizations. SIAM Journal on Scientiﬁc and Statistical Computing, 12(6):1332–1350, 1991. C. Boutsidis, P. Drineas, and M. Magdon-Ismail. Near-optimal column-based matrix reconstruction. CoRR, abs/1103.0995, 2011. T. F. Chan. Rank revealing QR factorizations. Linear Algebra and Its Applications, 88:67–82, 1987. S. Chandrasekaran and I. C. F. Ipsen. On rank-revealing factorisations. SIAM Journal on Matrix Analysis and Applications, 15(2):592–622, 1994. P. Cortez, A. Cerdeira, F. Almeida, T. Matos, and J. Reis. Modeling wine preferences by data mining from physicochemical properties. Decision Support Systems, 47(4):547–553, 2009. S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent semantic analysis. Journal of The American Society for Information Science, 41(6):391–407, 1990. A. Deshpande and L. Rademacher. Efﬁcient volume sampling for row/column subset selection. In Proceedings of the 51st IEEE Annual Symposium on Foundations of Computer Science (FOCS), pages 329–338, 2010. A. Deshpande, L. Rademacher, S. Vempala, and G. Wang. Matrix approximation and projective clustering via volume sampling. Theory of Computing, 2(2006):225–247, 2006. P. Drineas and R. Kannan. Pass-efﬁcient algorithms for approximating large matrices. In Proceeding of the 14th Annual ACM-SIAM Symposium on Dicrete Algorithms (SODA), pages 223–232, 2003. P. Drineas and M. W. Mahoney. On the Nystr¨ m method for approximating a gram matrix for o improved kernel-based learning. Journal of Machine Learning Research, 6:2153–2175, 2005. P. Drineas, R. Kannan, and M. W. Mahoney. Fast Monte Carlo algorithms for matrices III: computing a compressed approximate matrix decomposition. SIAM Journal on Computing, 36(1): 184–206, 2006. 2767  WANG AND Z HANG  P. Drineas, M. W. Mahoney, and S. Muthukrishnan. Relative-error CUR matrix decompositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844–881, September 2008. P. Drineas, M. Magdon-Ismail, M. W. Mahoney, and D. P. Woodruff. Fast approximation of matrix coherence and statistical leverage. In International Conference on Machine Learning (ICML), 2012. L. V. Foster. Rank and null space calculations using matrix decomposition without column interchanges. Linear Algebra and its Applications, 74:47–71, 1986. C. Fowlkes, S. Belongie, F. Chung, and J. Malik. Spectral grouping using the Nystr¨ m method. o IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2):214–225, 2004. A. Frank and A. Asuncion. UCI machine learning repository, http://archive.ics.uci.edu/ml.  2010.  URL  A. Frieze, R. Kannan, and S. Vempala. Fast Monte Carlo algorithms for ﬁnding low-rank approximations. Journal of the ACM, 51(6):1025–1041, November 2004. ISSN 0004-5411. A. Gittens and M. W. Mahoney. Revisiting the Nystr¨ m method for improved large-scale machine o learning. arXiv preprint arXiv:1303.1849, 2013. S. A. Goreinov, E. E. Tyrtyshnikov, and N. L. Zamarashkin. A theory of pseudoskeleton approximations. Linear Algebra and Its Applications, 261:1–21, 1997a. S. A. Goreinov, N. L. Zamarashkin, and E. E. Tyrtyshnikov. Pseudo-skeleton approximations by matrices of maximal volume. Mathematical Notes, 62(4):619–623, 1997b. M. Gu and S. C. Eisenstat. Efﬁcient algorithms for computing a strong rank-revealing QR factorization. SIAM Journal on Scientiﬁc Computing, 17(4):848–869, 1996. V. Guruswami and A. K. Sinop. Optimal column-based low-rank matrix reconstruction. In Proceedings of the 23rd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2012. I. Guyon, S. Gunn, A. Ben-Hur, and G. Dror. Result analysis of the NIPS 2003 feature selection challenge. Advances in Neural Information Processing Systems (NIPS), 2004. N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review, 53(2):217–288, 2011. Y. P. Hong and C. T. Pan. Rank-revealing QR factorizations and the singular value decomposition. Mathematics of Computation, 58(197):213–232, 1992. R. Jin, T. Yang, and M. Mahdavi. Improved bound for the Nystr¨ m method and its application to o kernel classiﬁcation. CoRR, abs/1111.2262, 2011. S. Kumar, M. Mohri, and A. Talwalkar. Ensemble Nystr¨ m method. In Advances in Neural Inforo mation Processing Systems (NIPS), 2009. 2768  ¨ I MPROVING CUR M ATRIX D ECOMPOSITION AND THE N YSTR OM A PPROXIMATION  S. Kumar, M. Mohri, and A. Talwalkar. Sampling methods for the Nystr¨ m method. Journal of o Machine Learning Research, 13:981–1006, 2012. F. G. Kuruvilla, P. J. Park, and S. L. Schreiber. Vector algebra in the analysis of genome-wide expression data. Genome Biology, 3:research0011–research0011.1, 2002. M. Li, J. T. Kwok, and B.-L. Lu. Making large-scale Nystr¨ m approximation possible. In Internao tional Conference on Machine Learning (ICML), 2010. L. Mackey, A. Talwalkar, and M. I. Jordan. Divide-and-conquer matrix factorization. In Advances in Neural Information Processing Systems (NIPS). 2011. M. W. Mahoney. Randomized algorithms for matrices and data. Foundations and Trends in Machine Learning, 3(2):123–224, 2011. M. W. Mahoney and P. Drineas. CUR matrix decompositions for improved data analysis. Proceedings of the National Academy of Sciences, 106(3):697–702, 2009. M. W. Mahoney, M. Maggioni, and P. Drineas. Tensor-CUR decompositions for tensor-based data. SIAM Journal on Matrix Analysis and Applications, 30(3):957–987, 2008. C. Mesterharm and M. J. Pazzani. Active learning using on-line algorithms. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2011. D. Michie, D. J. Spiegelhalter, and C. C. Taylor. Machine learning, neural and statistical classiﬁcation. 1994. L. Sirovich and M. Kirby. Low-dimensional procedure for the characterization of human faces. Journal of the Optical Society of America A, 4(3):519–524, Mar 1987. G. W. Stewart. Four algorithms for the the efﬁcient computation of truncated pivoted QR approximations to a sparse matrix. Numerische Mathematik, 83(2):313–323, 1999. A. Talwalkar and A. Rostamizadeh. Matrix coherence and the Nystr¨ m method. arXiv preprint o arXiv:1004.2008, 2010. A. Talwalkar, S. Kumar, and H. Rowley. Large-scale manifold learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008. M. Turk and A. Pentland. Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1): 71–86, 1991. E. E. Tyrtyshnikov. Incomplete cross approximation in the mosaic-skeleton method. Computing, 64:367–380, 2000. C. Williams and M. Seeger. Using the Nystr¨ m method to speed up kernel machines. In Advances o in Neural Information Processing Systems (NIPS), 2001. K. Zhang and J. T. Kwok. Clustered Nystr¨ m method for large scale manifold learning and dimeno sion reduction. IEEE Transactions on Neural Networks, 21(10):1576–1587, 2010. K. Zhang, I. W. Tsang, and J. T. Kwok. Improved Nystr¨ m low-rank approximation and error o analysis. In International Conference on Machine Learning (ICML), 2008. 2769</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
