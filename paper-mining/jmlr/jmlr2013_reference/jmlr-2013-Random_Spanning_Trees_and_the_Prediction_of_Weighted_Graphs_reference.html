<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-92" href="../jmlr2013/jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">jmlr2013-92</a> <a title="jmlr-2013-92-reference" href="#">jmlr2013-92-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</h1>
<br/><p>Source: <a title="jmlr-2013-92-pdf" href="http://jmlr.org/papers/volume14/cesa-bianchi13a/cesa-bianchi13a.pdf">pdf</a></p><p>Author: Nicol√≤ Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella</p><p>Abstract: We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world data sets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice. Keywords: online learning, learning on graphs, graph prediction, random spanning trees</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
