<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 jmlr-2013-Learning Theory Approach to Minimum Error Entropy Criterion</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-62" href="../jmlr2013/jmlr-2013-Learning_Theory_Approach_to_Minimum_Error_Entropy_Criterion.html">jmlr2013-62</a> <a title="jmlr-2013-62-reference" href="#">jmlr2013-62-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>62 jmlr-2013-Learning Theory Approach to Minimum Error Entropy Criterion</h1>
<br/><p>Source: <a title="jmlr-2013-62-pdf" href="http://jmlr.org/papers/volume14/hu13a/hu13a.pdf">pdf</a></p><p>Author: Ting Hu, Jun Fan, Qiang Wu, Ding-Xuan Zhou</p><p>Abstract: We consider the minimum error entropy (MEE) criterion and an empirical risk minimization learning algorithm when an approximation of R´ nyi’s entropy (of order 2) by Parzen windowing is e minimized. This learning algorithm involves a Parzen windowing scaling parameter. We present a learning theory approach for this MEE algorithm in a regression setting when the scaling parameter is large. Consistency and explicit convergence rates are provided in terms of the approximation ability and capacity of the involved hypothesis space. Novel analysis is carried out for the generalization error associated with R´ nyi’s entropy and a Parzen windowing function, to overcome e technical difﬁculties arising from the essential differences between the classical least squares problems and the MEE setting. An involved symmetrized least squares error is introduced and analyzed, which is related to some ranking algorithms. Keywords: minimum error entropy, learning theory, R´ nyi’s entropy, empirical risk minimization, e approximation error</p><br/>
<h2>reference text</h2><p>S. Agarwal and P. Niyogi. Generalization bounds for ranking algorithms via algorithmic stability. Journal of Machine Learning Research, 10:441–474, 2009. M. Anthony and P. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, 1999. J. Y. Audibert and O. Catoni. Robust linear least squares regression. Annals of Statistics, 39:2766– 2794, 2011. B. Chen and J. C. Principe. Some further results on the minimum error entropy estimation. Entropy, 14:966–977, 2012. S. Clemencon, G. Lugosi, and N. Vayatis. Ranking and scoring using empirical risk minimization. Proceedings of COLT 2005, in LNCS Computational Learning Theory, Springer-Verlag, Berlin, Heidelberg, 3559:1–15, 2005. F. Cucker and D. X. Zhou. Learning Theory: An Approximation Theory Viewpoint. Cambridge University Press, 2007. D. Erdogmus and J. C. Principe. An error-entropy minimization algorithm for supervised training of nonlinear adaptive systems. IEEE Transactions on Signal Processing, 50:1780–1786, 2002. D. Erdogmus and J. C. Principe. Convergence properties and data efﬁciency of the minimum error entropy criterion in adaline training. IEEE Transactions on Signal Processing, 51:1966–1978, 2003. D. Haussler, M. Kearns, and R. Schapire. Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. Machine Learning, 14:83–114, 1994. T. Hu. Online regression with varying Gaussians and non-identical distributions. Analysis and Applications, 9:395–408, 2011. W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:13–30, 1963. V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. Annals of Statistics, 34:2593–2656, 2006. E. Parzen. On the estimation of a probability density function and the mode. Annals of Mathematical Statistics, 33:1049–1051, 1962. 396  M INIMUM E RROR E NTROPY  J. C. Principe. Information Theoretic Learning: R´ nyi’s Entropy and Kernel Perspectives. Springer, e New York, 2010. L. M. Silva, J. M. de S´ , and L. A. Alexandre. The MEE principle in data classiﬁcation: a a perceptrop-based analysis. Neural Computation, 22:2698–2728, 2010. S. Smale and D. X. Zhou. Estimating the approximation error in learning theory. Analysis and Applications, 1:17–41, 2003. S. Smale and D. X. Zhou. Online learning with Markov sampling. Analysis and Applications, 7:87– 113, 2009. V. Vapnik. Statistical Learning Theory. John Wiley & Sons, 1998. Y. Yao. On complexity issue of online learning algorithms. IEEE Transactions on Information Theory, 56:6470–6481, 2010. Y. Ying. Convergence analysis of online algorithms. Advances in Computational Mathematics, 27:273–291, 2007. D. X. Zhou. The covering number in learning theory. Journal of Complexity, 18:739–767, 2002. D. X. Zhou. Capacity of reproducing kernel spaces in learning theory. IEEE Transactions on Information Theory, 49:1743-1752, 2003.  397</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
