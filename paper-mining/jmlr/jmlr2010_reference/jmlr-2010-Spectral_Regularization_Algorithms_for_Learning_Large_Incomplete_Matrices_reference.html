<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-105" href="../jmlr2010/jmlr-2010-Spectral_Regularization_Algorithms_for_Learning_Large_Incomplete_Matrices.html">jmlr2010-105</a> <a title="jmlr-2010-105-reference" href="#">jmlr2010-105-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</h1>
<br/><p>Source: <a title="jmlr-2010-105-pdf" href="http://jmlr.org/papers/volume11/mazumder10a/mazumder10a.pdf">pdf</a></p><p>Author: Rahul Mazumder, Trevor Hastie, Robert Tibshirani</p><p>Abstract: We use convex relaxation techniques to provide a sequence of regularized low-rank solutions for large-scale matrix completion problems. Using the nuclear norm as a regularizer, we provide a simple and very efﬁcient convex algorithm for minimizing the reconstruction error subject to a bound on the nuclear norm. Our algorithm S OFT-I MPUTE iteratively replaces the missing elements with those obtained from a soft-thresholded SVD. With warm starts this allows us to efﬁciently compute an entire regularization path of solutions on a grid of values of the regularization parameter. The computationally intensive part of our algorithm is in computing a low-rank SVD of a dense matrix. Exploiting the problem structure, we show that the task can be performed with a complexity of order linear in the matrix dimensions. Our semideﬁnite-programming algorithm is readily scalable to large matrices; for example S OFT-I MPUTE takes a few hours to compute low-rank approximations of a 106 × 106 incomplete matrix with 107 observed entries, and ﬁts a rank-95 approximation to the full Netﬂix training set in 3.3 hours. Our methods achieve good training and test errors and exhibit superior timings when compared to other competitive state-of-the-art techniques. Keywords: collaborative ﬁltering, nuclear norm, spectral regularization, netﬂix prize, large scale convex optimization</p><br/>
<h2>reference text</h2><p>J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. A new approach to collaborative ﬁltering: operator estimation with spectral regularization. Journal of Machine Learning Research, 10:803–826, 2009. A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Advances in Neural Information Processing Systems 19. MIT Press, 2007. A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243–272, 2008. F. Bach. Consistency of trace norm minimization. Journal of Machine Learning Research, 9: 1019–1048, 2008. R. M. Bell and Y. Koren. Lessons from the Netﬂix prize challenge. Technical report, AT&T; Bell Laboratories, 2007. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004. S. Burer and R. D.C. Monteiro. Local minima and convergence in low-rank semideﬁnite programming. Mathematical Programming, 103(3):427–631, 2005. J. Cai, E. J. Candes, and Z. Shen. A singular value thresholding algorithm for matrix completion, 2008. Available at http://www.citebase.org/abstract?id=oai:arXiv.org:0810.3286. E. Cand` s and B. Recht. Exact matrix completion via convex optimization. Foundations of Come putational Mathematics, 9:717–772, 2008. 2320  M ATRIX C OMPLETION BY S PECTRAL R EGULARIZATION  E. J. Cand` s and T. Tao. The power of convex relaxation: near-optimal matrix completion. IEEE e Transactions on Information Theory, 56(5):2053–2080, 2009. D. DeCoste. Collaborative prediction using ensembles of maximum margin matrix factorizations. In Proceedings of the 23rd International Conference on Machine Learning, pages 249–256. ACM, 2006. A. Dempster, N.. Laird, and D. Rubin. Maximum likelihood from incomplete data via the EM algorithm (with discussion). Journal of the Royal Statistical Society Series B, 39:1–38, 1977. D. Donoho, I. Johnstone, G. Kerkyachairan, and D. Picard. Wavelet shrinkage; asymptopia? (with discussion). Journal of the Royal Statistical Society: Series B, 57:201–337, 1995. M. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford University, 2002. J. Friedman. Fast sparse regression and classiﬁcation. Technical report, Department of Statistics, Stanford University, 2008. J. Friedman, T. Hastie, H. Hoeﬂing, and R. Tibshirani. Pathwise coordinate optimization. Annals of Applied Statistics, 2(1):302–332, 2007. M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, 2009. Web page and software available at http://stanford.edu/∼boyd/cvx. T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Prediction, Inference and Data Mining (Second Edition). Springer Verlag, New York, 2009. S. Ji and J. Ye. An accelerated gradient method for trace norm minimization. In Proceedings of the 26th International Conference on Machine Learning, pages 457–464, 2009. R. H. Keshavan, S. Oh, and A. Montanari. Matrix completion from a few entries. IEEE Transactions on Information Theory, 56(6):2980–2998, 2009. R. M. Larsen. Lanczos bidiagonalization with partial reorthogonalization. Technical Report DAIMI PB-357, Department of Computer Science, Aarhus University, 1998. R.M. Larsen. Propack-software for large and sparse svd calculations, 2004. http://sun.stanford.edu/∼rmunk/PROPACK.  Available at  J. Liu, S. Ji, and J. Ye. SLEP: Sparse Learning with Efﬁcient Projections. Arizona State University, 2009. Available at http://www.public.asu.edu/∼jye02/Software/SLEP. Z. Liu and L. Vandenberghe. Interior-point method for nuclear norm approximation with application to system identﬁcation. SIAM Journal on Matrix Analysis and Applications, 31(3):1235–1256, 2009. S. Ma, D. Goldfarb, and L. Chen. Fixed point and Bregman iterative methods for matrix rank minimization. Mathematical Programming Series A, forthcoming. R. Mazumder, J. Friedman, and T. Hastie. Sparsenet: coordinate descent with non-convex penalties. Technical report, Stanford University, 2009. 2321  M AZUMDER , H ASTIE AND T IBSHIRANI  Y. Nesterov. Introductory Lectures on Convex Optimization: Basic course. Kluwer, Boston, 2003. Y. Nesterov. Gradient methods for minimizing composite objective function. Technical Report 76, Center for Operations Research and Econometrics (CORE), Catholic University of Louvain, 2007. B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization, 2007. Available at http://www.citebase.org/abstract?id=oai:arXiv.org:0706.4138. J. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22nd International Conference on Machine Learning, pages 713–719. ACM, 2005. R. Salakhutdinov, A. Mnih, and G. E. Hinton. Restricted Boltzmann machines for collaborative ﬁltering. In Proceedings of the 24th International Conference on Machine Learning, pages 791– 798. AAAI Press, 2007. ACM SIGKDD and Netﬂix. Soft modelling by latent variables: the nonlinear iterative partial least squares (NIPALS) approach. In Proceedings of KDD Cup and Workshop, 2007. Available at http://www.cs.uic.edu/∼liub/KDD-cup-2007/proceedings.html. N. Srebro and T. Jaakkola. Weighted low-rank approximations. In Proceedings of the 20th International Conference on Machine Learning, pages 720–727. AAAI Press, 2003. N. Srebro, N. Alon, and T. Jaakkola. Generalization error bounds for collaborative prediction with low-rank matrices. In Advances in Neural Information Processing Systems 17, pages 5–27. MIT Press, 2005a. N. Srebro, J. Rennie, and T. Jaakkola. Maximum-margin matrix factorization. In Advances in Neural Information Processing Systems 17, pages 1329–1336. MIT Press, 2005b. G. Takacs, I. Pilaszy, B. Nemeth, and D. Tikk. Scalable collaborative ﬁltering approaches for large recommender systems. Journal of Machine Learning Research, 10:623–656, 2009. R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, 58:267–288, 1996. O. Troyanskaya, M. Cantor, G. Sherlock, P. Brown, T. Hastie, R. Tibshirani, D. Botstein, and R. B. Altman. Missing value estimation methods for DNA microarrays. Bioinformatics, 17(6):520– 525, 2001. C. H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Annals of Statistics, 38(2):894–942, 2010.  2322</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
