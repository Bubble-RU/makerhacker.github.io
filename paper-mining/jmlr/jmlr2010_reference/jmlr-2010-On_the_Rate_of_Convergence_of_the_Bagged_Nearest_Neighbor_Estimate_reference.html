<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>86 jmlr-2010-On the Rate of Convergence of the Bagged Nearest Neighbor Estimate</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-86" href="../jmlr2010/jmlr-2010-On_the_Rate_of_Convergence_of_the_Bagged_Nearest_Neighbor_Estimate.html">jmlr2010-86</a> <a title="jmlr-2010-86-reference" href="#">jmlr2010-86-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>86 jmlr-2010-On the Rate of Convergence of the Bagged Nearest Neighbor Estimate</h1>
<br/><p>Source: <a title="jmlr-2010-86-pdf" href="http://jmlr.org/papers/volume11/biau10a/biau10a.pdf">pdf</a></p><p>Author: Gérard Biau, Frédéric Cérou, Arnaud Guyader</p><p>Abstract: Bagging is a simple way to combine estimates in order to improve their performance. This method, suggested by Breiman in 1996, proceeds by resampling from the original data set, constructing a predictor from each subsample, and decide by combining. By bagging an n-sample, the crude nearest neighbor regression estimate is turned into a consistent weighted nearest neighbor regression estimate, which is amenable to statistical analysis. Letting the resampling size kn grows appropriately with n, it is shown that this estimate may achieve optimal rate of convergence, independently from the fact that resampling is done with or without replacement. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, adaptation results by data-splitting are presented. Keywords: bagging, resampling, nearest neighbor estimate, rates of convergence</p><br/>
<h2>reference text</h2><p>G. Biau and L. Devroye. On the Layered Nearest Neighbour Estimate, the Bagged Nearest Neighbour Estimate and the Random Forest Method in Regression and Classiﬁcation. Technical Report, Universit´ Pierre et Marie Curie, Paris, 2008. e URL http://www.lsta.upmc.fr/BIAU/bd4.pdf. L. Breiman. Random forests. Machine Learning, 45:5–32, 2001. L. Breiman. Bagging predictors. Machine Learning, 24:123–140, 1996. P. B¨ hlmann and B. Yu. Analyzing bagging. The Annals of Statistics, 30:927–961, 2002. u A. Buja and W. Stuetzle. Observations on bagging. Statistica Sinica, 16:323–352, 2006. T.M. Cover. Estimation by the nearest neighbor rule. IEEE Transactions on Information Theory, 14:50–55, 1968a. T.M. Cover. Rates of convergence for nearest neighbor procedures. In Proceedings of the Hawaii International Conference on Systems Sciences, pages 413–415, Honolulu, 1968b. T.M. Cover and P.E. Hart. Nearest neighbor pattern classiﬁcation. IEEE Transactions on Information Theory, 13:21–27, 1967. L. Devroye. On the almost everywhere convergence of nonparametric regression function estimates. The Annals of Statistics, 9:1310–1319, 1981. L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springero Verlag, New York, 1996. T.G. Dietterich. Ensemble methods in machine learning. In J. Kittler and F. Roli, editors, First International Workshop on Multiple Classiﬁer Systems, Lecture Notes in Computer Science, pages 1–15, New York, 2000. Springer-Verlag. E. Fix and J.L. Hodges. Discriminatory Analysis. Nonparametric Discrimination: Consistency Properties. Technical Report 4, Project Number 21-49-004, USAF School of Aviation Medicine, Randolph Field, Texas, 1951. E. Fix and J.L. Hodges. Discriminatory analysis: Small sample performance. Technical Report 11, Project Number 21-49-004, USAF School of Aviation Medicine, Randolph Field, Texas, 1952. J.H. Friedman and P. Hall. On bagging and nonlinear estimation. Journal of Statistical Planning and Inference, 137:669–683, 2000. 711  ´ B IAU , C E ROU AND G UYADER  I.S. Gradshteyn and I.M. Ryzhik. Table of Integrals, Series, and Products. Academic Press, New York, 2007. L. Gy¨ rﬁ. On the rate of convergence of nearest neighbor rules. IEEE Transactions on Information o Theory, 29:509–512, 1978. L. Gy¨ rﬁ, M. Kohler, A. Krzy˙ ak, and H. Walk. A Distribution-Free Theory of Nonparametric o z Regression. Springer-Verlag, New York, 2002. P. Hall and R.J. Samworth. Properties of bagged nearest neighbour classiﬁers. Journal of the Royal Statistical Society B, 67:363–379, 2005. I.A. Ibragimov and R.Z. Khasminskii. On nonparametric estimation of regression. Akademii Nauk SSSR, 252:780–784, 1980.  Doklady  I.A. Ibragimov and R.Z. Khasminskii. Statistical Estimation: Asymptotic Theory. Springer-Verlag, New York, 1981. I.A. Ibragimov and R.Z. Khasminskii. On the bounds for quality of nonparametric regression function estimation. Theory of Probability and its Applications, 27:81–94, 1982. A.N. Kolmogorov and V.M. Tihomirov. ε-entropy and ε-capacity of sets in functional spaces. American Mathematical Society Translations, 17:277–364, 1961. S.R. Kulkarni and S.E. Posner. Rates of convergence of nearest neighbor estimation under arbitrary sampling. IEEE Transactions on Information Theory, 41:1028–1039, 1995. Y. Lin and Y. Jeon. Random forests and adaptive nearest neighbours. Journal of the American Statistical Association, 101:578–590, 2006. D. Psaltis, R.R. Snapp, and S.S. Venkatesh. On the ﬁnite sample performance of the nearest neighbor classiﬁer. IEEE Transactions on Information Theory, 40:820–837, 1994. B.M. Steele. Exact bootstrap k-nearest neighbor learners. Machine Learning, 74:235–255, 2009. C.J. Stone. Consistent nonparametric regression. The Annals of Statistics, 5:595–645, 1977. S.S. Venkatesh, R.R. Snapp, and D. Psaltis. Bellman strikes again! The growth rate of sample complexity with dimension for the nearest neighbor classiﬁer. In J. Kittler and F. Roli, editors, Proceedings of the Fifth Annual Workshop on Computational Learning Theory, pages 93–102, Pittsburgh, 1992.  712</p>
<br/>
<br/><br/><br/></body>
</html>
