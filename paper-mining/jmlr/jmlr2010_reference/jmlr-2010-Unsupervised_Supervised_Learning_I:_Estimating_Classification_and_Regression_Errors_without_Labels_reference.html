<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-114" href="../jmlr2010/jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">jmlr2010-114</a> <a title="jmlr-2010-114-reference" href="#">jmlr2010-114-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</h1>
<br/><p>Source: <a title="jmlr-2010-114-pdf" href="http://jmlr.org/papers/volume11/donmez10a/donmez10a.pdf">pdf</a></p><p>Author: Pinar Donmez, Guy Lebanon, Krishnakumar Balasubramanian</p><p>Abstract: Estimating the error rates of classiﬁers or regression models is a fundamental task in machine learning which has thus far been studied exclusively using supervised learning techniques. We propose a novel unsupervised framework for estimating these error rates using only unlabeled data and mild assumptions. We prove consistency results for the framework and demonstrate its practical applicability on both synthetic and real world data. Keywords: classiﬁcation and regression, maximum likelihood, latent variable models</p><br/>
<h2>reference text</h2><p>Y. Bishop, S. Fienberg, and P. Holland. Discrete Multivariate Analysis: Theory and Practice. MIT press, 1975. J. Blitzer, M. Dredze, and F. Pereira. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classiﬁcation. In Proc. of ACL ’07, 2007. L. Breiman. Bias, variance, and arcing classiﬁers. Technical Report 460, Statistics department, University of California, 1996. T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, second edition, 2005. D. Cox, J. Little, and D. O’Shea. Ideals, Varieties, and Algorithms: An Introduction to Computational Algebraic Geometry and Commutative Algebra. Springer, 2006. R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁcation. Wiley New York, 2001. B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap. Chapman & Hall, 1997. T. S. Ferguson. A Course in Large Sample Theory. Chapman & Hall, 1996. D. J. Hand. Recent advances in error rate estimation. Pattern Recognition Letters, 4(5):335–346, 1986. T. Joachims. Making large-scale svm learning practical. In B. Sch¨ lkopf, C. Burges, and A. Smola, o editors, Advances in Kernel Methods - Support Vector Learning. MIT Press, 1999. K. Lang. Newsweeder: Learning to ﬁlter netnews. In International Conference on Machine Learning, 1995. A. Papoulis. Probability, Random Variables, and Stochastic Processes. McGraw-Hill, 1984. V. S. Sheng, F. Provost, and P. G. Ipeirotis. Get another label? improving data quality and data mining using multiple, noisy labelers. In Proc. of the 14th ACM SIGKDD Internation Conference on Knowledge Discovery and Data Mining, pages 614–622, 2008. P. Smyth, U. Fayyad, M. Burl, P. Perona, and P. Baldi. Inferring ground truth from subjective labelling of venus images. In Advances in Neural Information Processing Systems 7, 1995. R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng. Cheap and fast-but is it good? evaluating non-expert annotations for natural language tasks. In Proc. of EMNLP, 2008. B. Sturmfels. Solving Systems of Polynomial Equations. American Mathematical Society, 2002. V. N. Vapnik. The Nature of Statistical Learning Theory. Springer, second edition, 2000. 1351</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
