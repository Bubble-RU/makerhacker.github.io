<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>16 jmlr-2010-Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-16" href="../jmlr2010/jmlr-2010-Asymptotic_Equivalence_of_Bayes_Cross_Validation_and_Widely_Applicable_Information_Criterion_in_Singular_Learning_Theory.html">jmlr2010-16</a> <a title="jmlr-2010-16-reference" href="#">jmlr2010-16-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>16 jmlr-2010-Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory</h1>
<br/><p>Source: <a title="jmlr-2010-16-pdf" href="http://jmlr.org/papers/volume11/watanabe10a/watanabe10a.pdf">pdf</a></p><p>Author: Sumio Watanabe</p><p>Abstract: In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2λ/n, where λ is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion. Keywords: cross-validation, information criterion, singular learning machine, birational invariant</p><br/>
<h2>reference text</h2><p>H. Akaike. A new look at the statistical model identiﬁcation. IEEE Trans. on Automatic Control, 19:716-723, 1974. S. Amari. A universal theorem on learning curves, Neural Networks, 6(2):161-166, 1993. M. Aoyagi. Stochastic complexity and generalization error of a restricted Boltzmann machine in Bayesian estimation. Journal of Machine Learning Research, 11:1243-1272, 2010. M. Aoyagi, S. Watanabe. Stochastic complexities of reduced rank regression in Bayesian estimation. Neural Networks, 18(7):924-933, 2005. M.F. Atiyah. Resolution of singularities and division of distributions. Communications of Pure and Applied Mathematics, 13:145-150. 1970. M.W. Browne. Cross-Validation Methods. Journal of Mathematical Psychology, 44:108-132, 2000. H. Cramer. Mathematical Methods of Statistics. Princeton University Press, 1949. M. Drton, B. Sturmfels, and S. Sullivant. Lecures on Algebraic Statistics. Birkh¨ user, Berlin, 2009. a S. Geisser. The predictive sample reuse method with applications. Journal of the American Statistical Association, 70:320-328, 1975. I.M. Gelfand and G.E. Shilov. Generalized Functions. Academic Press, San Diego, 1964. 3592  S INGULAR C ROSS -VALIDATION  A.E. Gelfand, D.K. Dey, H. Chang. Model determination using predictive distributions with implementation via sampling-based method. Bayesian Statistics, 4:147-167, Oxford University Press, Oxford, 1992. A. Gelman, J.B. Carlin, H.S. Stern, D.B. Rubin. Bayesian Data Analysis. Chapman & Hall CRC, Boca Raton, 2004. K. Hagiwara. On the problem in model selection of neural network regression in overrealizable scenario. Neural Computation, 14:1979-2002, 2002. J. A. Hartigan. A failure of likelihood asymptotics for normal mixtures. In Proceedings of the Barkeley Conference in Honor of J. Neyman and J. Kiefer, Vol. 2, pages 807–810, 1985. H. Hironaka. Resolution of singularities of an algebraic variety over a ﬁeld of characteristic zero. Annals of Mathematics, 79:109-326, 1964. M. Kashiwara. B-functions and holonomic systems. Inventiones Mathematicae, 38:33-53, 1976. J. Koll´ r, S.Mori, C.H.Clemens, A.Corti. Birational Geometry of Algebraic Varieties. Cambridge o Tract in Mathematics Cambridge University Press, Cambridge, 1998. C.I. Mosier. Problems and designs of cross-validation. Educational and Psychological Measurement, 11:5-11, 1951. M. Mustata. Singularities of pairs via jet schemes. Journal of the American Mathematical Society, 15:599-615. 2002. E. Levin, N. Tishby, S.A. Solla. A statistical approaches to learning and generalization in layered neural networks. Proceedings of IEEE, 78(10):1568-1574. 1990. S. Lin. Asymptotic approximation of marginal likelihood integrals. arXiv:1003.5338, 2010. H. Linhart, W. Zucchini. Model Selection. John Wiley and Sons, New York, 1986. K. Nagata and S. Watanabe, Asymptotic behavior of exchange ratio in exchange Monte Carlo method. International Journal of Neural Networks, 21(7):980-988, 2008. T. Oaku. Algorithms for the b-function and D-modules associated with a polynomial. Journal of Pure Applied Algebra, 117:495-518, 1997. M. Peruggia. On the variability of case-detection importance sampling weights in the Bayesian linear model. Journal of American Statistical Association, 92:199-207, 1997. A.E. Raftery, M.A. Newton, J.M. Satagopan, P.N. Krivitsly. Estimating the integrated likelihood via posterior simulation using the harmonic mean identity. Bayesian Statistics, 8:1-45, Oxford University Press, Oxford, 2007. D. Rusakov, D. Geiger. Asymptotic model selection for naive Bayesian network. Journal of Machine Learning Research. 6:1-35, 2005. M. Saito. On real log canonical thresholds, arXiv:0707.2308v1, 2007. 3593  WATANABE  G. Schwarz. Estimating the dimension of a model. Annals of Statistics, 6(2):461-464. 1978. D.J. Spiegelhalter, N.G. Best, B.P. Carlin, A. Linde. Bayesian measures of model complexity and ﬁt. Journal of Royal Statistical Society, Series B, 64(4):583-639, 2002. M. Stone. An asymptotic equivalence of choice of model by cross-validation and Akaike’s criterion. Journal of the Royal Statistical Society. 39(B):44-47, 1977. A. Takemura, T.Kuriki. On the equivalence of the tube and Euler characteristic methods for the distribution of the maximum of the gaussian ﬁelds over piecewise smooth domains. Annals of Applied Probability, 12(2):768-796, 2002. A. W. van der Vaart, J. A. Wellner. Weak Convergence and Empirical Processes. Springer, 1996. A. Vehtari, J. Lampinen. Bayesian Model Assessment and Comparison Using Cross-Validation Predictive Densities. Neural Computation, 14(10):2439-2468, 2002. S. Watanabe. Generalized Bayesian framework for neural networks with singular Fisher information matrices. In the Proceedings of the International Symposium on Nonlinear Theory and Its applications, pages 207–210, 1995. S. Watanabe. Algebraic analysis for nonidentiﬁable learning machines. Neural Computation, 13(4):899-933, 2001a. S. Watanabe. Algebraic geometrical methods for hierarchical learning machines. Neural Networks. 14(8):1049-1060, 2001b. S. Watanabe. Almost all learning machines are singular. In the Proceedings of the IEEE Int. Conf. FOCI, pages 383–388, 2007. S. Watanabe. Algebraic Geometry and Statistical Learning Theory. Cambridge University Press, Cambridge, UK, 2009. S. Watanabe. Equations of states in singular statistical estimation. Neural Networks. 23(1):20-34, 2010a. S. Watanabe. Equations of states in statistical learning for an unrealizable and regular case. IEICE Transactions. E93A(3):617-626, 2010b. S. Watanabe. A limit theorem in singular regression problem. Advanced Studies of Pure Mathematics. 57:473-492, 2010c. S. Watanabe. Asymptotic learning curve and renormalizable condition in statistical learning theory. Journal of Physics Conference Series, 233, No.012014, 2010d. K. Yamazaki, S. Watanabe. Singularities in mixture models and upper bounds of stochastic complexity. Neural Networks. 16(7):1029-1038, 2003. K. Yamazaki, S. Watanabe. Algebraic geometry and stochastic complexity of hidden Markov models. Neurocomputing, 69:62-84, 2005. P. Zwiernik. An asymptotic approximation of the marginal likelihood for general Markov models. arXiv:1012.0753v1, 2010. 3594</p>
<br/>
<br/><br/><br/></body>
</html>
