<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-87" href="../jmlr2010/jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">jmlr2010-87</a> <a title="jmlr-2010-87-reference" href="#">jmlr2010-87-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</h1>
<br/><p>Source: <a title="jmlr-2010-87-pdf" href="http://jmlr.org/papers/volume11/mairal10a/mairal10a.pdf">pdf</a></p><p>Author: Julien Mairal, Francis Bach, Jean Ponce, Guillermo Sapiro</p><p>Abstract: Sparse coding—that is, modelling data vectors as sparse linear combinations of basis elements—is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to speciﬁc data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets. Keywords: basis pursuit, dictionary learning, matrix factorization, online learning, sparse coding, sparse principal component analysis, stochastic approximations, stochastic optimization, nonnegative matrix factorization</p><br/>
<h2>reference text</h2><p>M. Aharon and M. Elad. Sparse and redundant modeling of image content using an image-signaturedictionary. SIAM Journal on Imaging Sciences, 1(3):228–247, July 2008. M. Aharon, M. Elad, and A. M. Bruckstein. The K-SVD: An algorithm for designing of overcomplete dictionaries for sparse representations. IEEE Transactions on Signal Processing, 54(11): 4311–4322, November 2006. F. Bach. Consistency of the group Lasso and multiple kernel learning. Journal of Machine Learning Research, 9:1179–1224, 2008. F. Bach, J. Mairal, and J. Ponce. Convex sparse matrix factorizations. Technical report, 2008. Preprint arXiv:0812.1869. D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc Belmont, 1999. P. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Annals of statistics, 37(4):1705–1732, 2009. J. F. Bonnans and A. Shapiro. Optimization problems with perturbations: A guided tour. SIAM Review, 40(2):202–227, 1998. J. F. Bonnans and A. Shapiro. Perturbation Analysis of Optimization Problems. Springer, 2000. J. M. Borwein and A. S. Lewis. Convex Analysis and Nonlinear Optimization: Theory and Examples. Springer, 2006. L. Bottou. Online algorithms and stochastic approximations. In David Saad, editor, Online Learning and Neural Networks. 1998. L. Bottou and O. Bousquet. The trade-offs of large scale learning. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, volume 20, pages 161–168. MIT Press, 2008. D. M. Bradley and J. A. Bagnell. Differentiable sparse coding. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems, volume 21, pages 113–120. 2009. S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientiﬁc Computing, 20:33–61, 1999. K. Chin, S. DeVries, J. Fridlyand, P.T. Spellman, R. Roydasgupta, W. L. Kuo, A. Lapuk, R. M. Neve, Z. Qian, T. Ryder, et al. Genomic and transcriptional aberrations linked to breast cancer pathophysiologies. Cancer Cell, 10(6):529–541, 2006. S. F. Cotter, B. D. Rao, K. Engan, and K. Kreutz-Delgado. Sparse solutions to linear inverse problems with multiple measurement vectors. IEEE Transactions on Signal Processing, 53(7):2477– 2488, 2005. 55  M AIRAL , BACH , P ONCE AND S APIRO  J. M. Danskin. The theory of max-min, and its application to weapons allocation problems. ¨ Okonometrie und Unternehmensforschung, 1967. A. d’Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. G. Lanckriet. A direct formulation for sparse PCA using semideﬁnite programming. SIAM Review, 49(3):434–448, 2007. A. d’Aspremont, F. Bach, and L. El Ghaoui. Optimal solutions for sparse principal component analysis. Journal of Machine Learning Research, 9:1269–1294, 2008. J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efﬁcient projections onto the ℓ1 -ball for learning in high dimensions. In Proceedings of the International Conference on Machine Learning (ICML), 2008. B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32 (2):407–499, 2004. M. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned dictionaries. IEEE Transactions on Image Processing, 54(12):3736–3745, December 2006. K. Engan, S. O. Aase, and J. H. Husoy. Frame based signal compression using method of optimal directions (MOD). In Proceedings of the 1999 IEEE International Symposium on Circuits Systems, volume 4, 1999. M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2006 (VOC2006) Results, 2006. C. F´ votte, N. Bertin, and J. L. Durrieu. Nonnegative matrix factorization with the itakura-saito e divergence: With application to music analysis. Neural Computation, 21(3):793–830, 2009. D. L. Fisk. Quasi-martingales. Transactions of the American Mathematical Society, 120(3):359– 388, 1965. J. Friedman, T. Hastie, H. H¨ lﬂing, and R. Tibshirani. Pathwise coordinate optimization. Annals of o Applied Statistics, 1(2):302–332, 2007. W. J. Fu. Penalized regressions: The bridge versus the Lasso. Journal of Computational and Graphical Statistics, 7:397–416, 1998. J. J. Fuchs. Recovery of exact sparse representations in the presence of bounded noise. IEEE Transactions on Information Theory, 51(10):3601–3608, 2005. A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman. From few to many: Illumination cone models for face recognition under variable lighting and pose. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(6):643–660, 2001. G. H. Golub and C. F. Van Loan. Matrix computations. John Hopkins University Press, 1996. R. Grosse, R. Raina, H. Kwong, and A. Y. Ng. Shift-invariant sparse coding for audio classiﬁcation. In Proceedings of the Twenty-third Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2007. 56  O NLINE L EARNING FOR M ATRIX FACTORIZATION AND S PARSE C ODING  Z. Harchaoui. M´ thodes a Noyaux pour la D´ tection. PhD thesis, T´ l´ com ParisTech, 2008. e e ee ` Z. Harchaoui and C. L´ vy-Leduc. Catching change-points with Lasso. In J.C. Platt, D. Koller, e Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, volume 20, pages 161–168. MIT Press, 2008. H. Hotelling. Relations between two sets of variates. Biometrika, 28:321–377, 1936. P. O. Hoyer. Non-negative sparse coding. In Proc. IEEE Workshop on Neural Networks for Signal Processing, 2002. P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of Machine Learning Research, 5:1457–1469, 2004. L. Jacob, G. Obozinski, and J.-P. Vert. Group Lasso with overlap and graph Lasso. In Proceedings of the International Conference on Machine Learning (ICML), 2009. R. Jenatton, J-Y. Audibert, and F. Bach. Structured variable selection with sparsity-inducing norms. Technical report, 2009a. Preprint arXiv:0904.3523v1. R. Jenatton, G. Obozinski, and F. Bach. Structured sparse principal component analysis. Technical report, 2009b. Preprint arXiv:0909.1440v1. I. T. Jolliffe, N. T. Trendaﬁlov, and M. Uddin. A modiﬁed principal component technique based on the Lasso. Journal of Computational and Graphical Statistics, 12(3):531–547, 2003. K. Kavukcuoglu, M. Ranzato, and Y. LeCun. Fast inference in sparse coding algorithms with applications to object recognition. Technical report, Computational and Biological Learning Lab, Courant Institute, NYU, 2008. Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. IEEE Computer, 42(8):30–37, 2009. H. J. Kushner and G. Yin. Stochastic Approximation and Recursive Algorithms and Applications. Springer, 2003. D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In Advances in Neural Information Processing Systems, pages 556–562, 2001. H. Lee, A. Battle, R. Raina, and A. Y. Ng. Efﬁcient sparse coding algorithms. In B. Sch¨ lkopf, o J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems, volume 19, pages 801–808. MIT Press, 2007. K. C. Lee, J. Ho, and D. Kriegman. Acquiring linear subspaces for face recognition under variable lighting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(5):684–698, 2005. M. S. Lewicki and T. J. Sejnowski. Learning overcomplete representations. Neural Computation, 12(2):337–365, 2000. C.J. Lin. Projected gradient methods for nonnegative matrix factorization. Neural Computation, 19 (10):2756–2779, 2007. 57  M AIRAL , BACH , P ONCE AND S APIRO  N. Maculan and J. R. G. Galdino de Paula. A linear-time median-ﬁnding algorithm for projecting a vector on the simplex of Rn. Operations Research Letters, 8(4):219–222, 1989. J. R. Magnus and H. Neudecker. Matrix Differential Calculus with Applications in Statistics and Econometrics, revised edition. John Wiley, Chichester, 1999. J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Discriminative learned dictionaries for local image analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008a. J. Mairal, M. Elad, and G. Sapiro. Sparse representation for color image restoration. IEEE Transactions on Image Processing, 17(1):53–69, January 2008b. J. Mairal, G. Sapiro, and M. Elad. Learning multiscale sparse representations for image and video restoration. SIAM Multiscale Modelling and Simulation, 7(1):214–241, April 2008c. J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary learning for sparse coding. In Proceedings of the International Conference on Machine Learning (ICML), 2009a. J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Supervised dictionary learning. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems, volume 21, pages 1033–1040. MIT Press, 2009b. J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Non-local sparse models for image restoration. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2009c. S. Mallat. A Wavelet Tour of Signal Processing, Second Edition. Academic Press, New York, September 1999. M. M´ tivier. Semi-martingales. Walter de Gruyter, 1983. e R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other variants. Learning in Graphical Models, 89:355–368, 1998. Y. Nesterov. Gradient methods for minimizing composite objective function. Technical report, Center for Operations Research and Econometrics (CORE), Catholic University of Louvain, 2007. G. Obozinski, M. J. Wainwright, and M. I. Jordan. Union support recovery in high-dimensional multivariate regression. UC Berkeley Technical Report 761, August 2008. G. Obozinski, B. Taskar, and M. I. Jordan. Joint covariate selection and joint subspace selection for multiple classiﬁcation problems. Statistics and Computing, 2009. Published online. B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research, 37:3311–3325, 1997. M. R. Osborne, B. Presnell, and B. A. Turlach. A new approach to variable selection in least squares problems. IMA Journal of Numerical Analysis, 20(3):389–403, 2000. 58  O NLINE L EARNING FOR M ATRIX FACTORIZATION AND S PARSE C ODING  G. Peyr´ . Sparse modeling of textures. Journal of Mathematical Imaging and Vision, 34(1):17–31, e May 2009. M. Protter and M. Elad. Image sequence denoising via sparse and redundant representations. IEEE Transactions on Image Processing, 18(1):27–36, 2009. R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng. Self-taught learning: transfer learning from unlabeled data. In Proceedings of the International Conference on Machine Learning (ICML), 2007. V. Roth and B. Fischer. The Group-Lasso for generalized linear models: uniqueness of solutions and efﬁcient algorithms. In Proceedings of the International Conference on Machine Learning (ICML), 2008. S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization. In 22nd Annual Conference on Learning Theory (COLT), 2009. K.-K. Sung. Learning and Example Selection for Object and Pattern Recognition. PhD thesis, MIT, Artiﬁcial Intelligence Laboratory and Center for Biological and Computational Learning, 1996. R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society. Series B, 58(1):267–288, 1996. R. Tibshirani and P. Wang. Spatial smoothing and hot spot detection for CGH data using the fused Lasso. Biostatistics, 9(1):18–29, 2008. R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight. Sparsity and smoothness via the fused lasso. Journal of the Royal Statistical Society Series B, 67(1):91–108, 2005. J. A. Tropp. Algorithms for simultaneous sparse approximation. part ii: Convex relaxation. Signal Processing, Special Issue ”Sparse Approximations in Signal and Image Processing”, 86:589– 602, April 2006. J. A. Tropp, A. C. Gilbert, and M. J. Strauss. Algorithms for simultaneous sparse approximation. part i: Greedy pursuit. Signal Processing, Special Issue ”Sparse Approximations in Signal and Image Processing”, 86:572–588, April 2006. B. A. Turlach, W. N. Venables, and S. J. Wright. Simultaneous variable selection. Technometrics, 47(3):349–363, 2005. A. W. Van der Vaart. Asymptotic Statistics. Cambridge University Press, 1998. D. M. Witten, R. Tibshirani, and T. Hastie. A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis. Biostatistics, 10(3):515–534, 2009. T. T. Wu and K. Lange. Coordinate descent algorithms for Lasso penalized regression. Annals of Applied Statistics, 2(1):224–244, 2008. 59  M AIRAL , BACH , P ONCE AND S APIRO  J. Yang, K. Yu, Y. Gong, and T. Huang. Linear spatial pyramid matching using sparse coding for image classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009. M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society Series B, 68:49–67, 2006. R. Zass and A. Shashua. Nonnegative sparse PCA. In B. Sch¨ lkopf, J. Platt, and T. Hoffman, o editors, Advances in Neural Information Processing Systems, volume 19, pages 1561–1568. MIT Press, 2007. H. H. Zhang, Y. Liu, Y. Wu, and J. Zhu. Selection for the multicategory svm via adaptive sup-norm regularization. Electronic Journal of Statistics, 2:149–167, 2008. M. Zibulevsky and B. A. Pearlmutter. Blind source separation by sparse decomposition in a signal dictionary. Neural Computation, 13(4):863–882, 2001. H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society Series B, 67(2):301–320, 2005. H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. Journal of Computational and Graphical Statistics, 15(2):265–286, 2006.  60</p>
<br/>
<br/><br/><br/></body>
</html>
