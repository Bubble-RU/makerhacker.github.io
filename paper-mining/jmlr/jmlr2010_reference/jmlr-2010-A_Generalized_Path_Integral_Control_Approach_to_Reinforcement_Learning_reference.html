<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>4 jmlr-2010-A Generalized Path Integral Control Approach to Reinforcement Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-4" href="../jmlr2010/jmlr-2010-A_Generalized_Path_Integral_Control_Approach_to_Reinforcement_Learning.html">jmlr2010-4</a> <a title="jmlr-2010-4-reference" href="#">jmlr2010-4-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>4 jmlr-2010-A Generalized Path Integral Control Approach to Reinforcement Learning</h1>
<br/><p>Source: <a title="jmlr-2010-4-pdf" href="http://jmlr.org/papers/volume11/theodorou10a/theodorou10a.pdf">pdf</a></p><p>Author: Evangelos Theodorou, Jonas Buchli, Stefan Schaal</p><p>Abstract: With the goal to generate more scalable algorithms with higher efﬁciency and fewer open parameters, reinforcement learning (RL) has recently moved towards combining classical techniques from optimal control and dynamic programming with modern learning techniques from statistical estimation theory. In this vein, this paper suggests to use the framework of stochastic optimal control with path integrals to derive a novel approach to RL with parameterized policies. While solidly grounded in value function estimation and optimal control based on the stochastic Hamilton-JacobiBellman (HJB) equations, policy improvements can be transformed into an approximation problem of a path integral which has no open algorithmic parameters other than the exploration noise. The resulting algorithm can be conceived of as model-based, semi-model-based, or even model free, depending on how the learning problem is structured. The update equations have no danger of numerical instabilities as neither matrix inversions nor gradient learning rates are required. Our new algorithm demonstrates interesting similarities with previous RL research in the framework of probability matching and provides intuition why the slightly heuristically motivated probability matching approach can actually perform well. Empirical evaluations demonstrate signiﬁcant performance improvements over gradient-based policy learning and scalability to high-dimensional control problems. Finally, a learning experiment on a simulated 12 degree-of-freedom robot dog illustrates the functionality of our algorithm in a complex robot learning scenario. We believe that Policy Improvement with Path Integrals (PI2 ) offers currently one of the most efﬁcient, numerically robust, and easy to implement algorithms for RL based on trajectory roll-outs. Keywords: stochastic optimal control, reinforcement learning, parameterized policies</p><br/>
<h2>reference text</h2><p>S. Amari. Natural gradient learning for over- and under-complete bases in ica. Neural Computation, 11(8):1875–83, 1999. A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that can solve difﬁcult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, SMC-13(5): 115–133, 1983. J. Baxter and P. L. Bartlett. Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial Intelligence Research, 15:319–350, 2001. R. Bellman and R. Kalaba. Selected Papers On mathematical trends in Control Theory. Dover Publications, 1964. B. Van Den Broek, W. Wiegerinck, and H. J. Kappen. Graphical model inference in optimal control of stochastic multi-agent systems. Journal of Artiﬁcial Intelligence Research, 32(1):95–122, 2008. J. Buchli, E. Theodorou, F. Stulp, and S. Schaal. Variable impedance control - a reinforcement learning approach. In Robotics Science and Systems, 2010. P. Dayan and G. Hinton. Using em for reinforcement learning. Neural Computation, 9, 1997. M. P. Deisenroth, C. E. Rasmussen, and J. Peters. Gaussian Process Dynamic Programming. Neurocomputing, 72(7–9):1508–1524, March 2009. W. H. Fleming and H. M. Soner. Controlled Markov Processes and Viscosity Solutions. Applications of aathematics. Springer, New York, 2nd edition, 2006. 3178  A G ENERALIZED PATH I NTEGRAL C ONTROL A PPROACH TO R EINFORCEMENT L EARNING  M. Ghavamzadeh and E. Yaakov. Bayesian actor-critic algorithms. In ICML ’07: Proceedings of The 24th International Conference on Machine Learning, pages 297–304, 2007. V. Gullapalli. A stochastic reinforcement learning algorithm for learning real-valued functions. Neural Networks, 3:671–692, 1990. A. Ijspeert, J. Nakanishi, and S. Schaal. Learning attractor landscapes for learning motor primitives. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 1547–1554. Cambridge, MA: MIT Press, 2003. D. H. Jacobson and D. Q. Mayne. Differential dynamic programming. American Elsevier Pub. Co., New York,, 1970. N. Jetchev and M Toussaint. Trajectory prediction: learning to map situations to robot trajectories. In ICML ’09: Proceedings of the 26th Annual International Conference on Machine Learning, pages 449–456, 2009. H. J. Kappen. Linear theory for control of nonlinear stochastic systems. Phys. Rev. Lett., 95:200201, Nov 2005a. H. J. Kappen. Path integrals and symmetry breaking for optimal control theory. Journal of Statistical Mechanics: Theory and Experiment, (11):P11011, 2005b. H. J. Kappen. An introduction to stochastic control theory, path integrals and reinforcement learning. In J. Marro, P. L. Garrido, and J. J. Torres, editors, Cooperative Behavior in Neural Systems, volume 887 of American Institute of Physics Conference Series, pages 149–181, February 2007. H. J. Kappen, W. Wiegerinck, and B. van den Broek. A path integral approach to agent planning. In AAMAS, 2007. H. J. Kappen, Gmez V., and Opper M. Optimal control as a graphical model inference problem. Journal for Machine Learning Research (JMLR), arXiv:0901.0633v, 2009. Submitted. J. Koeber and J. Peters. Policy search for motor primitives. In D. Schuurmans, J. Benigio, and D. Koller, editors, Advances in Neural Information Processing Systems 21 (NIPS 2008), pages 297–304, Vancouver, BC, Dec. 8-11, 2008. Cambridge, MA: MIT Press. W. Thomas Miller, Richard S., and Paul J. Werbos. Neural Networks for Control. Neural network modeling and connectionism. MIT Press, Cambridge, Mass., 1990. B. K. Øksendal. Stochastic Differential Equations : An Introduction with Applications. Springer, Berlin; New York, 6th edition, 2003. J. Peters. Machine Learning of Motor Skills for Robotics. PhD thesis, University of Southern California, 2007. J. Peters and S. Schaal. Policy gradient methods for robotics. In Proceedings of the IEEE International Conference on Intelligent Robotics Systems (IROS 2006), 2006a. J. Peters and S. Schaal. Reinforcement learning for parameterized motor primitives. In Proceedings of the 2006 International Joint Conference on Neural Networks (IJCNN 2006), 2006b. 3179  T HEODOROU , B UCHLI AND S CHAAL  J. Peters and S. Schaal. Reinforcement learning of motor skills with policy gradients. Neural Networks, 21(4):682–97, 2008a. J. Peters and S. Schaal. Natural actor critic. Neurocomputing, 71(7-9):1180–1190, 2008b. J. Peters and S. Schaal. Learning to control in operational space. International Journal of Robotics Research, 27:197–212, 2008c. T. Rueckstiess, M. Felder, and J. Schmidhuber. State-dependent exploration for policy gradient methods. In ECML PKDD ’08: Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases - Part II, pages 234–249, 2008. S. Schaal and C. G. Atkeson. Constructive incremental learning from only local information. Neural Computation, 10(8):2047–2084, 1998. L. Sciavicco and B. Siciliano. Modelling and Control of Robot Manipulators. Advanced textbooks in control and signal processing. Springer, London ; New York, 2000. R. F. Stengel. Optimal Control and Estimation. Dover books on advanced mathematics. Dover Publications, New York, 1994. R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. Adaptive computation and machine learning. MIT Press, Cambridge, 1998. R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 12, pages 1057–1063. MIT Press, 2000. E. Todorov. Stochastic optimal control and estimation methods adapted to the noise characteristics of the sensorimotor system. Neural Computation, 17(5):1084, 2005. E. Todorov. Linearly-solvable markov decision problems. In B. Scholkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19 (NIPS 2007), Vancouver, BC, 2007. Cambridge, MA: MIT Press. E. Todorov. General duality between optimal control and estimation. In In Proceedings of the 47th IEEE Conference on Decision and Control, 2008. E. Todorov. Classic maximum principles and estimation-control dualities for nonlinear stochastic systems. 2009a. (Submitted). E. Todorov. Efﬁcient computation of optimal actions. Proceedings National Academy of Science USA, 106(28):11478–83, 2009b. M. Toussaint and A. Storkey. Probabilistic inference for solving discrete and continuous state markov decision processes, 2006. N. Vlassis, M. Toussaint, G. Kontes, and Piperidis. S. Learning model-free control by a monte-carlo em algorithm. Autonomous Robots, 27(2):123–130, 2009. 3180  A G ENERALIZED PATH I NTEGRAL C ONTROL A PPROACH TO R EINFORCEMENT L EARNING  W. Wiegerinck, B. van den Broek, and H. J. Kappen. Stochastic optimal control in continuous space-time multi-agent system. In UAI, 2006. R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229–256, 1992. J. Yong. Relations among odes, pdes, fsdes, bsdes, and fbsdes. In Proceedings of the 36th IEEE Conference on Decision and Control, volume 3, pages 2779–2784, Dec 1997.  3181</p>
<br/>
<br/><br/><br/></body>
</html>
