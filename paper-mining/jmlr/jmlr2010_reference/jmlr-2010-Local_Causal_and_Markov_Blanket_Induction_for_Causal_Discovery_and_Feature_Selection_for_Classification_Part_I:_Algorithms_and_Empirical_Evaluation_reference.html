<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-67" href="../jmlr2010/jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_I%3A_Algorithms_and_Empirical_Evaluation.html">jmlr2010-67</a> <a title="jmlr-2010-67-reference" href="#">jmlr2010-67-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</h1>
<br/><p>Source: <a title="jmlr-2010-67-pdf" href="http://jmlr.org/papers/volume11/aliferis10a/aliferis10a.pdf">pdf</a></p><p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: We present an algorithmic framework for learning local causal structure around target variables of interest in the form of direct causes/effects and Markov blankets applicable to very large data sets with relatively small samples. The selected feature sets can be used for causal discovery and classiÃ„?Ä¹Å¡ cation. The framework (Generalized Local Learning, or GLL) can be instantiated in numerous ways, giving rise to both existing state-of-the-art as well as novel algorithms. The resulting algorithms are sound under well-deÃ„?Ä¹Å¡ ned sufÃ„?Ä¹Å¡ cient conditions. In a Ã„?Ä¹Å¡ rst set of experiments we evaluate several algorithms derived from this framework in terms of predictivity and feature set parsimony and compare to other local causal discovery methods and to state-of-the-art non-causal feature selection methods using real data. A second set of experimental evaluations compares the algorithms in terms of ability to induce local causal neighborhoods using simulated and resimulated data and examines the relation of predictivity with causal induction performance. Our experiments demonstrate, consistently with causal feature selection theory, that local causal feature selection methods (under broad assumptions encompassing appropriate family of distribuc 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS tions, types of classiÃ„?Ä¹Å¡ ers, and loss functions) exhibit strong feature set parsimony, high predictivity and local causal interpretability. Although non-causal feature selection methods are often used in practice to shed light on causal relationships, we Ã„?Ä¹Å¡ nd that they cannot be interpreted causally even when they achieve excellent predictivity. Therefore we conclude that only local causal techniques should be used when insight into causal structure is sought. In a companion paper we examine in depth the behavior of GLL algorithms, provide extensions, and show</p><br/>
<h2>reference text</h2><p>C. F. Aliferis and G. F. Cooper. An evaluation of an algorithm for inductive learning of Bayesian belief networks using simulated data sets. Proceedings of the Tenth Conference on Uncertainty in ArtiÃ„?Ä¹Å¡ cial Intelligence (UAI), 1994. C. F. Aliferis and I. Tsamardinos. Algorithms for large-scale local causal discovery and feature selection in the presence of small sample or large causal neighborhoods. Technical Report DSL 02-08, 2002a. C. F. Aliferis and I. Tsamardinos. Using local causal induction to improve global causal discovery: Enhancing the sparse candidate set. Technical Report DSL 02-04, 2002b. C. F. Aliferis, I. Tsamardinos, and A. Statnikov. Large-scale feature selection using Markov blanket induction for the prediction of protein-drug binding. Technical Report DSL 02-06, 2002. C. F. Aliferis, I. Tsamardinos, and A. Statnikov. HITON: a novel Markov blanket algorithm for optimal variable selection. AMIA 2003 Annual Symposium Proceedings, pages 21Ä‚Ë˜&euro;&ldquo;25, 2003a. C. F. Aliferis, I. Tsamardinos, A. Statnikov, and L. E. Brown. Causal explorer: a causal probabilistic network learning toolkit for biomedical discovery. Proceedings of the 2003 International Conference on Mathematics and Engineering Techniques in Medicine and Biological Sciences (METMBS), 2003b. C. F. Aliferis, A. Statnikov, E. Kokkotou, P. P. Massion, and I. Tsamardinos. Local regulatorynetwork inducing algorithms for biomarker discovery from mass-throughput datasets. Technical Report DSL 06-05, 2006a. C. F. Aliferis, A. Statnikov, and P. P. Massion. Pathway induction and high-Ã„?Ä¹Å¡ delity simulation for molecular signature and biomarker discovery in lung cancer using microarray gene expression data. Proceedings of the 2006 American Physiological Society Conference Ä‚Ë˜&euro;&oelig;Physiological Genomics and Proteomics of Lung DiseaseÄ‚Ë˜&euro;?, 2006b. C. F. Aliferis, A. Statnikov, I. Tsamardinos, S. Mani, and X. D. Koutsoukos. Local causal and Markov blanket induction for causal discovery and feature selection for classiÃ„?Ä¹Å¡ cation. Part II: Analysis and extensions. Journal of Machine Learning Research, 11:235Ä‚Ë˜&euro;&ldquo;284, 2010. Y. Aphinyanaphongs and C. F. Aliferis. Learning boolean queries for article quality Ã„?Ä¹Å¡ ltering. Medinfo 2004., 11(Pt 1):263Ä‚Ë˜&euro;&ldquo;267, 2004. Y. Aphinyanaphongs, A. Statnikov, and C. F. Aliferis. A comparison of citation metrics to machine learning Ã„?Ä¹Å¡ lters for the identiÃ„?Ä¹Å¡ cation of high quality medline documents. J.Am.Med.Inform.Assoc., 13(4):446Ä‚Ë˜&euro;&ldquo;455, Jul 2006. X. Bai, C. Glymour, R. Padman, J. Ramsey, P. Spirtes, and F. Wimberly. PCX: Markov blanket classiÃ„?Ä¹Å¡ cation for large data sets with few cases. Technical Report, Center for Automated Learning and Discovery, 2004. G. E. A. P. A. Batista and M. C. Monard. An analysis of four missing data treatment methods for supervised learning. Applied ArtiÃ„?Ä¹Å¡ cial Intelligence, 17(5-6):519Ä‚Ë˜&euro;&ldquo;533, 2003. 225  &ODVVLILHU;  ,QI D 0R QWB UWD O 2K LW\ VXP H $& G 3(WL B R OR J\ /\ PS KR PD *L VHW WH 'H [WH U 6\ OYD 2Y DUL &D; DQB QFH U 7K URP ELQ %U HD V &D; WB QFH U +L YD  )HDWXUH VXEVHW  1R YD %D QN UX SWF \  A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS  /$56(1 690 ZR VWDW /$56(1 FRPS                                /$56(1 690 ZLWK VWDW /$56(1 FRPS                                690                /               7  /  7  5)96 ZR VWDW FRPS  690    7  7   7      7     7    5)    7  7    7      7      7    5)96 ZLWK VWDW FRPS  690    7  7   7      7     7    5)    7  7        7    7    7    Table 10: ClassiÃ„?Ä¹Å¡ cation performance (AUC) for polynomial SVMs and classiÃ„?Ä¹Å¡ ers native to LARSEN, L0, and RFVS feature selection algorithms induced with features selected by the latter three methods. In cells marked with Ä‚Ë˜&euro;&oelig;TÄ‚Ë˜&euro;?, the corresponding feature selection method did not terminate within the allotted time.  %D\HVLDQ QHWZRUN  1XPEHU RI YDULDEOHV  7UDLQLQJ VDPSOHV  1XPEHU RI VHOHFWHG WDUJHWV  &KLOG;     [   [   [     ,QVXUDQFH     [   [   [     $ODUP     [   [   [     +DLOILQGHU     [   [   [     0XQLQ     [   [     3LJV     [   [   [     /LQN     [   [   [     /XQJB&DQFHU;     [   [   [     *HQH     [   [   [     Table 11: Simulated and resimulated data sets used for experiments. Lung Cancer network is resimulated from human lung cancer gene expression data (Bhattacharjee et al., 2001) using SCA algorithm (Friedman et al., 1999b). Gene network is resimulated from yeast cell cycle gene expression data (Spellman et al., 1998) using SCA algorithm. More details about data sets are provided in Tsamardinos et al. (2006).  226  L OCAL C AUSAL AND M ARKOV B LANKET I NDUCTION PART I  +,7213& PD[ N  +,7213& PD[ N  +,7213& PD[ N  +,7213& PD[ N  ,QWHUOHDYHG +,7213& PD[ N ,QWHUOHDYHG +,7213& PD[ N ,QWHUOHDYHG +,7213& PD[ N ,QWHUOHDYHG +,7213& PD[ N 003& PD[ N  003& PD[ N  003& PD[ N  003& PD[ N  ,QWHUOHDYHG 003& PD[ N  ,QWHUOHDYHG 003& PD[ N  ,QWHUOHDYHG 003& PD[ N  ,QWHUOHDYHG 003& PD[ N        +,7213&)'5 PD[ N  +,7213&)'5 PD[ N  +,7213&)'5 PD[ N  +,7213&)'5 PD[ N  +,7210% PD[ N  000% PD[ N  5)( UHGXFWLRQ RI IHDWXUHV E\   5)( UHGXFWLRQ RI IHDWXUHV E\   8$).UXVNDO:DOOLV690   8$).UXVNDO:DOOLV690   8$)6LJQDO1RLVH690   8$)6LJQDO1RLVH690   / /$56(1 IRU PXOWLFODVV UHVSRQVH /$56(1 RQHYHUVXVUHVW  Table 12: Algorithms used in local causal discovery experiments with simulated and resimulated data. Y. Benjamini and Y. Hochberg. Controlling the false discovery rate: A practical and powerful approach to multiple testing. Journal of the Royal Statistical Society.Series B (Methodological), 57(1):289Ä‚Ë˜&euro;&ldquo;300, 1995. Y. Benjamini and D. Yekutieli. The control of the false discovery rate in multiple testing under dependency. Ann.Statist, 29(4):1165Ä‚Ë˜&euro;&ldquo;1188, 2001. A. Bhattacharjee, W. G. Richards, J. Staunton, C. Li, S. Monti, P. Vasa, C. Ladd, J. Beheshti, R. Bueno, M. Gillette, M. Loda, G. Weber, E. J. Mark, E. S. Lander, W. Wong, B. E. Johnson, T. R. Golub, D. J. Sugarbaker, and M. Meyerson. ClassiÃ„?Ä¹Å¡ cation of human lung carcinomas by mrna expression proÃ„?Ä¹Å¡ ling reveals distinct adenocarcinoma subclasses. Proc.Natl.Acad.Sci.U.S.A, 98(24):13790Ä‚Ë˜&euro;&ldquo;13795, Nov 2001. L. Breiman. Random forests. Machine Learning, 45(1):5Ä‚Ë˜&euro;&ldquo;32, 2001. L. E. Brown, I. Tsamardinos, and C. F. Aliferis. A comparison of novel and state-of-the-art polynomial Bayesian network learning algorithms. Proceedings of the Twentieth National Conference on ArtiÃ„?Ä¹Å¡ cial Intelligence (AAAI), 2005. R. Caruana and D. Freitag. Greedy attribute selection. Proceedings of the Eleventh International Conference on Machine Learning, pages 28Ä‚Ë˜&euro;&ldquo;36, 1994. J. Cheng and R. Greiner. Comparing Bayesian network classiÃ„?Ä¹Å¡ ers. Proceedings of the 15th Conference on Uncertainty in ArtiÃ„?Ä¹Å¡ cial Intelligence (UAI), pages 101Ä‚Ë˜&euro;&ldquo;107, 1999. J. Cheng and R. Greiner. Learning Bayesian belief network classiÃ„?Ä¹Å¡ ers: Algorithms and system. Proceedings of 14th Biennial Conference of the Canadian Society for Computational Studies of Intelligence, 2001. 227  A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS  J. Cheng, R. Greiner, J. Kelly, D. Bell, and W. Liu. Learning Bayesian networks from data: an information-theory based approach. ArtiÃ„?Ä¹Å¡ cial Intelligence, 137(1):43Ä‚Ë˜&euro;&ldquo;90, 2002a. J. Cheng, C. Hatzis, H. Hayashi, M. A. Krogel, S. Morishita, D. Page, and J. Sese. Kdd cup 2001 report. ACM SIGKDD Explorations Newsletter, 3(2):47Ä‚Ë˜&euro;&ldquo;64, 2002b. D. M. Chickering. Learning equivalence classes of bayesian-network structures. Journal of Machine Learning Research, 2:445Ä‚Ë˜&euro;&ldquo;498, 2002. D. M. Chickering. Optimal structure identiÃ„?Ä¹Å¡ cation with greedy search. Journal of Machine Learning Research, 3(3):507Ä‚Ë˜&euro;&ldquo;554, 2003. D. M. Chickering, D. Geiger, and D. Heckerman. Learning Bayesian networks is NP-hard. Technical Report MSR-TR-94-17, 1994. C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. IEEE Transactions on Information Theory, 14(3):462Ä‚Ë˜&euro;&ldquo;467, 1968. T. P. Conrads, V. A. Fusaro, S. Ross, D. Johann, V. Rajapakse, B. A. Hitt, S. M. Steinberg, E. C. Kohn, D. A. Fishman, G. Whitely, J. C. Barrett, L. A. Liotta, E. F. I. I. I. Petricoin, and T. D. Veenstra. High-resolution serum proteomic features for ovarian cancer detection. Endocr.Relat Cancer, 11(2):163Ä‚Ë˜&euro;&ldquo;178, Jun 2004. G. F. Cooper. A simple constraint-based algorithm for efÃ„?Ä¹Å¡ ciently mining observational databases for causal relationships. Data Mining and Knowledge Discovery, 1(2):203Ä‚Ë˜&euro;&ldquo;224, 1997. G. F. Cooper and E. Herskovits. A Bayesian method for the induction of probabilistic networks from data. Machine Learning, 9(4):309Ä‚Ë˜&euro;&ldquo;347, 1992. G. F. Cooper and C. Yoo. Causal discovery from a mixture of experimental and observational data. Proceedings of Uncertainty in ArtiÃ„?Ä¹Å¡ cial Intelligence, pages 116Ä‚Ë˜&euro;&ldquo;125, 1999. G. F. Cooper, C. F. Aliferis, R. Ambrosino, J. Aronis, B. G. Buchanan, R. Caruana, M. J. Fine, C. Glymour, G. Gordon, and B. H. Hanusa. An evaluation of machine-learning methods for predicting pneumonia mortality. ArtiÃ„?Ä¹Å¡ cial Intelligence in Medicine, 9(2):107Ä‚Ë˜&euro;&ldquo;138, 1997. D. Dash and G. F. Cooper. Exact model averaging with naive Bayesian classiÃ„?Ä¹Å¡ ers. Proc.19th Int.Conf.Machine Learning (ICML 2002), pages 91Ä‚Ë˜&euro;&ldquo;98, 2002. E. R. DeLong, D. M. DeLong, and D. L. Clarke-Pearson. Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach. Biometrics, 44(3): 837Ä‚Ë˜&euro;&ldquo;845, Sep 1988. R. Diaz-Uriarte and S. Alvarez de Andres. Gene selection and classiÃ„?Ä¹Å¡ cation of microarray data using random forest. BMC Bioinformatics, 7:3, 2006. R. O. Duda and P. E. Hart. Pattern ClassiÃ„?Ä¹Å¡ cation and Scene Analysis. Wiley, New York, 1973. S. Duda, C. F. Aliferis, R. Miller, A. Statnikov, and K. Johnson. Extracting drug-drug interaction articles from medline to improve the content of drug databases. AMIA 2005 Annual Symposium Proceedings, pages 216Ä‚Ë˜&euro;&ldquo;220, 2005. 228  L OCAL C AUSAL AND M ARKOV B LANKET I NDUCTION PART I  S. Dudoit and M. J. van der Laan. Asymptotics of cross-validated risk estimation in model selection and performance assessment. UC Berkeley Division of Biostatistics Working Paper Series, 126, 2003. F. Eberhardt, C. Glymour, and R. Scheines. On the number of experiments sufÃ„?Ä¹Å¡ cient and in the worst case necessary to identify all causal relations among n variables. Proceedings of the 21st Conference on Uncertainty in ArtiÃ„?Ä¹Å¡ cial Intelligence (UAI), pages 178Ä‚Ë˜&euro;&ldquo;183, 2005. F. Eberhardt, C. Glymour, and R. Scheines. N-1 experiments sufÃ„?Ä¹Å¡ ce to determine the causal relations among n variables. Innovations in Machine Learning: Theory And Applications, 2006. M. B. Eisen, P. T. Spellman, P. O. Brown, and D. Botstein. Cluster analysis and display of genomewide expression patterns. Proc.Natl.Acad.Sci.U.S.A, 95(25):14863Ä‚Ë˜&euro;&ldquo;14868, Dec 1998. J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348Ä‚Ë˜&euro;&ldquo;1361, 2001. R. E. Fan, P. H. Chen, and C. J. Lin. Working set selection using second order information for training support vector machines. Journal of Machine Learning Research, 6(1889):1918, 2005. N. Fananapazir, M. Li, D. Spentzos, and C. F. Aliferis. Formative evaluation of a prototype system for automated analysis of mass spectrometry data. AMIA 2005 Annual Symposium Proceedings, pages 241Ä‚Ë˜&euro;&ldquo;245, 2005. T. Fawcett. Roc graphs: Notes and practical considerations for researchers. Technical Report, HPL-2003-4, HP Laboratories, 2003. D. P. Foster and R. A. Stine. Variable selecion in data mining: Building a predictive model for bankruptcy. Journal of the American Statistical Association, 99(466):303Ä‚Ë˜&euro;&ldquo;314, 2004. L. Frey, D. Fisher, I. Tsamardinos, C. F. Aliferis, and A. Statnikov. Identifying Markov blankets with decision tree induction. Proceedings of the Third IEEE International Conference on Data Mining (ICDM), 2003. N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian network classiÃ„?Ä¹Å¡ ers. Machine Learning, 29 (2):131Ä‚Ë˜&euro;&ldquo;163, 1997. N. Friedman, M. Goldszmidt, and A. Wyner. Data analysis with Bayesian networks: A bootstrap approach. Proceedings of Uncertainty in ArtiÃ„?Ä¹Å¡ cial Intelligence (UAI), pages 206Ä‚Ë˜&euro;&ldquo;215, 1999a. N. Friedman, I. Nachman, and D. PeÄ‚Ë˜&euro;&trade;er. Learning Bayesian network structure from massive datasets: the Ä‚Ë˜&euro;&oelig;sparse candidateÄ‚Ë˜&euro;? algorithm. Proceedings of the Fifteenth Conference on Uncertainty in ArtiÃ„?Ä¹Å¡ cial Intelligence (UAI), 1999b. N. Friedman, M. Linial, I. Nachman, and D. PeÄ‚Ë˜&euro;&trade;er. Using Bayesian networks to analyze expression data. J Comput.Biol., 7(3-4):601Ä‚Ë˜&euro;&ldquo;620, 2000. G. M. Fung and O. L. Mangasarian. A feature selection newton method for support vector machine classiÃ„?Ä¹Å¡ cation. Computational Optimization and Applications, 28(2):185Ä‚Ë˜&euro;&ldquo;202, 2004. 229  A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS  T. S. Furey, N. Cristianini, N. Duffy, D. W. Bednarski, M. Schummer, and D. Haussler. Support vector machine classiÃ„?Ä¹Å¡ cation and validation of cancer tissue samples using microarray expression data. Bioinformatics, 16(10):906Ä‚Ë˜&euro;&ldquo;914, Oct 2000. O. Gevaert, Smet F. De, D. Timmerman, Y. Moreau, and Moor B. De. Predicting the prognosis of breast cancer by integrating clinical and microarray data with Bayesian networks. Bioinformatics, 22(14):e184Ä‚Ë˜&euro;&ldquo;e190, Jul 2006. C. N. Glymour and G. F. Cooper. Computation, Causation, and Discovery. AAAI Press, Menlo Park, Calif, 1999. P. I. Good. Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses, volume 2nd. Springer, New York, 2000. I. Guyon and A. Elisseeff. An introduction to variable and feature selection. Journal of Machine Learning Research, 3(1):1157Ä‚Ë˜&euro;&ldquo;1182, 2003. I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for cancer classiÃ„?Ä¹Å¡ cation using support vector machines. Machine Learning, 46(1):389Ä‚Ë˜&euro;&ldquo;422, 2002. I. Guyon, S. Gunn, M. Nikravesh, and L. A. Zadeh. Feature Extraction: Foundations and Applications. Springer-Verlag, Berlin, 2006a. I. Guyon, J. Li, T. Mader, P. A. Pletscher, G. Schneider, and M. Uhr. Feature selection with the clop package. Technical report, http://clopinet.com/isabelle/Projects/ETH/TM-fextract-class.pdf, 2006b. I. Guyon, C. F. Aliferis, and A. Elisseeff. Computational Methods of Feature Selection, chapter Causal Feature Selection. Chapman and Hall, 2007. D. Hardin, I. Tsamardinos, and C. F. Aliferis. A theoretical characterization of linear SVM-based feature selection. Proceedings of the Twenty First International Conference on Machine Learning (ICML), 2004. D. Heckerman. A tutorial on learning with Bayesian networks. Technical Report MSR-TR-95-06, 1995. D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20(3):197Ä‚Ë˜&euro;&ldquo;243, 1995. M. Hollander and D. Wolfe. Nonparametric Statistical Methods, volume 2nd. Wiley, New York, NY, USA, 1999. J. H. Holmes, D. R. Durbin, and F. K. Winston. The learning classiÃ„?Ä¹Å¡ er system: an evolutionary computation approach to knowledge discovery in epidemiologic surveillance. Artif.Intell.Med., 19(1):53Ä‚Ë˜&euro;&ldquo;74, May 2000. N. Hoot, I. Feurer, C. W. Pinson, and C. F. Aliferis. Modelling liver transplant survival: comparing techniques of deriving predictor sets. Journal of Gastrointestinal Surgery, 9(4):563, Apr 2005. 230  L OCAL C AUSAL AND M ARKOV B LANKET I NDUCTION PART I  T. Joachims. Learning to Classify Text Using Support Vector Machines. Kluwer Academic Publishers, Boston, 2002. K. Kira and L. A. Rendell. A practical approach to feature selection. Proceedings of the Ninth International Workshop on Machine Learning, pages 249Ä‚Ë˜&euro;&ldquo;256, 1992. R. Kohavi and G. H. John. Wrappers for feature subset selection. ArtiÃ„?Ä¹Å¡ cial Intelligence, 97(1-2): 273Ä‚Ë˜&euro;&ldquo;324, 1997. D. Koller and M. Sahami. Toward optimal feature selection. Proceedings of the International Conference on Machine Learning, 1996, 1996. I. Kononenko. Estimating attributes: Analysis and extensions of relief. Proceedings of the European Conference on Machine Learning, pages 171Ä‚Ë˜&euro;&ldquo;182, 1994. L. Li, C. R. Weinberg, T. A. Darden, and L. G. Pedersen. Gene selection for sample classiÃ„?Ä¹Å¡ cation based on gene expression data: study of sensitivity to choice of parameters of the ga/knn method. Bioinformatics, 17(12):1131Ä‚Ë˜&euro;&ldquo;1142, Dec 2001. H. Liu and H. Motoda. Feature Extraction, Construction and Selection: A Data Mining Perspective. Kluwer Academic, Boston, 1998. H. Liu, F. Hussain, C. L. Tan, and M. Dash. Discretization: an enabling technique. Data Mining and Knowledge Discovery, 6(4):393Ä‚Ë˜&euro;&ldquo;423, 2002. S. Mani and G. F. Cooper. A study in causal discovery from population-based infant birth and death records. Proceedings of the AMIA Annual Fall Symposium, 319, 1999. S. Mani and G. F. Cooper. Causal discovery using a Bayesian local causal discovery algorithm. Medinfo 2004., 11(Pt 1):731Ä‚Ë˜&euro;&ldquo;735, 2004. D. Margaritis and S. Thrun. Bayesian network induction via local neighborhoods. Advances in Neural Information Processing Systems, 12:505Ä‚Ë˜&euro;&ldquo;511, 1999. S. Meganck, P. Leray, and B. Manderick. Learning causal Bayesian networks from observations and experiments: A decision theoretic approach. Modeling Decisions in ArtiÃ„?Ä¹Å¡ cial Intelligence, LNCS, pages 58Ä‚Ë˜&euro;&ldquo;69, 2006. A. Moore and W. K. Wong. Optimal reinsertion: a new search operator for accelerated and more accurate Bayesian network structure learning. Proceedings of the Twentieth International Conference on Machine Learning (ICML), pages 552Ä‚Ë˜&euro;&ldquo;559, 2003. K. P. Murphy. Active learning of causal Bayes net structure. Technical Report, University of California, Berkeley, 2001. R. E. Neapolitan. Probabilistic Reasoning in Expert Systems: Theory and Algorithms. Wiley, New York, 1990. R. E. Neapolitan. Learning Bayesian networks. Pearson Prentice Hall, Upper Saddle River, NJ, 2004. 231  A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS  J. PeÄ‚&lsaquo;&oelig; a, J. Bjorkegren, and J. Tegner. Growing Bayesian network models of gene networks from n seed genes. Bioinformatics, 21(2):224Ä‚Ë˜&euro;&ldquo;229, 2005a. J. PeÄ‚&lsaquo;&oelig; a, J. Bjorkegren, and J. Tegner. Scalable, efÃ„?Ä¹Å¡ cient and correct learning of Markov boundaries n under the faithfulness assumption. Proceedings of the Eighth European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty, 2005b. J. PeÄ‚&lsaquo;&oelig; a, R. Nilsson, J. Bjorkegren, and J. TegnÄ‚&sbquo;Ã‚Â´ r. Towards scalable and data efÃ„?Ä¹Å¡ cient learning of n e Markov boundaries. International Journal of Approximate Reasoning, 45(2):211Ä‚Ë˜&euro;&ldquo;232, 2007. J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers, San Mateo, California, 1988. J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, Cambridge, U.K, 2000. J. Pearl and T. Verma. A theory of inferred causation. Principles of Knowledge Representation and Reasoning: Proceedings of Second International Conference, pages 441Ä‚Ë˜&euro;&ldquo;452, 1991. J. Pearl and T. S. Verma. Equivalence and synthesis of causal models. Proceedings of the Sixth Conference on Uncertainty in ArtiÃ„?Ä¹Å¡ cial Intelligence, pages 220Ä‚Ë˜&euro;&ldquo;227, 1990. I. Pournara and L. Wernisch. Reconstruction of gene networks using Bayesian learning and manipulation experiments. Bioinformatics, 20(17):2934Ä‚Ë˜&euro;&ldquo;2942, Nov 2004. A. Rakotomamonjy. Variable selection using SVM-based criteria. Journal of Machine Learning Research, 3(7-8):1357Ä‚Ë˜&euro;&ldquo;1370, 2003. J. Ramsey. A pc-style Markov blanket search for high-dimensional datasets. Technical Report, CMU-PHIL-177, Carnegie Mellon University, Department of Philosophy, 2006. J. Ramsey, J. Zhang, and P. Spirtes. Adjacency-faithfulness and conservative causal inference. Proceedings of the 22nd Annual Conference on Uncertainty in ArtiÃ„?Ä¹Å¡ cial Intelligence (UAI-06), 2006. A. Rosenwald, G. Wright, W. C. Chan, J. M. Connors, E. Campo, R. I. Fisher, R. D. Gascoyne, H. K. Muller-Hermelink, E. B. Smeland, J. M. Giltnane, E. M. Hurt, H. Zhao, L. Averett, L. Yang, W. H. Wilson, E. S. Jaffe, R. Simon, R. D. Klausner, J. Powell, P. L. Duffey, D. L. Longo, T. C. Greiner, D. D. Weisenburger, W. G. Sanger, B. J. Dave, J. C. Lynch, J. Vose, J. O. Armitage, E. Montserrat, A. Lopez-Guillermo, T. M. Grogan, T. P. Miller, M. LeBlanc, G. Ott, S. Kvaloy, J. Delabie, H. Holte, P. Krajci, T. Stokke, and L. M. Staudt. The use of molecular proÃ„?Ä¹Å¡ ling to predict survival after chemotherapy for diffuse large-b-cell lymphoma. N.Engl.J Med., 346(25): 1937Ä‚Ë˜&euro;&ldquo;1947, Jun 2002. A. Sboner and C. F. Aliferis. Modeling clinical judgment and implicit guideline compliance in the diagnosis of melanomas using machine learning. AMIA 2005 Annual Symposium Proceedings, pages 664Ä‚Ë˜&euro;&ldquo;668, 2005. T. Scheffer. Error Estimation and Model Selection. PhD thesis, Ph.D.Thesis, Technischen Universitet Berlin, School of Computer Science, 1999. 232  L OCAL C AUSAL AND M ARKOV B LANKET I NDUCTION PART I  C. Silverstein, S. Brin, R. Motwani, and J. Ullman. Scalable techniques for mining causal structures. Data Mining and Knowledge Discovery, 4(2):163Ä‚Ë˜&euro;&ldquo;192, 2000. P. T. Spellman, G. Sherlock, M. Q. Zhang, V. R. Iyer, K. Anders, M. B. Eisen, P. O. Brown, D. Botstein, and B. Futcher. Comprehensive identiÃ„?Ä¹Å¡ cation of cell cycle-regulated genes of the yeast saccharomyces cerevisiae by microarray hybridization. Mol.Biol Cell, 9(12):3273Ä‚Ë˜&euro;&ldquo;3297, Dec 1998. P. Spirtes, C. N. Glymour, and R. Scheines. Causation, Prediction, and Search, volume 2nd. MIT Press, Cambridge, Mass, 2000. A. Statnikov. Algorithms for discovery of multiple Markov boundaries: Application to the molecular signature multiplicity problem. Ph.D.Thesis, Department of Biomedical Informatics, Vanderbilt University, 2008. A. Statnikov, C. F. Aliferis, I. Tsamardinos, D. Hardin, and S. Levy. A comprehensive evaluation of multicategory classiÃ„?Ä¹Å¡ cation methods for microarray gene expression cancer diagnosis. Bioinformatics, 21(5):631Ä‚Ë˜&euro;&ldquo;643, Mar 2005a. A. Statnikov, I. Tsamardinos, Y. Dosbayev, and C. F. Aliferis. Gems: a system for automated cancer diagnosis and biomarker discovery from microarray gene expression data. Int.J.Med.Inform., 74 (7-8):491Ä‚Ë˜&euro;&ldquo;503, Aug 2005b. A. Statnikov, D. Hardin, and C. F. Aliferis. Using SVM weight-based methods to identify causally relevant and non-causally relevant variables. Proceedings of the NIPS 2006 Workshop on Causality and Feature Selection, 2006. J. Tian and J. Pearl. Causal discovery from changes: A bayesian approach. UCLA Cognitive Systems Laboratory, Technical Report (R-285), 2001. R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society.Series B (Methodological), 58(1):267Ä‚Ë˜&euro;&ldquo;288, 1996. S. Tong and D. Koller. Active learning for structure in bayesian networks. Proceedings of the International Joint Conference on ArtiÃ„?Ä¹Å¡ cial Intelligence, 17:863Ä‚Ë˜&euro;&ldquo;869, 2001. I. Tsamardinos and C. F. Aliferis. Towards principled feature selection: relevancy, Ã„?Ä¹Å¡ lters and wrappers. Proceedings of the Ninth International Workshop on ArtiÃ„?Ä¹Å¡ cial Intelligence and Statistics (AI & Stats), 2003. I. Tsamardinos and L. E. Brown. Bounding the false discovery rate in local Bayesian network learning. Proceedings of the Twenty Third National Conference on ArtiÃ„?Ä¹Å¡ cial Intelligence (AAAI), 2008a. I. Tsamardinos and L. E. Brown. Markov blanket-based variable selection in feature space. Technical report DSL-08-01, 2008b. I. Tsamardinos, C. F. Aliferis, and A. Statnikov. Algorithms for large scale Markov blanket discovery. Proceedings of the Sixteenth International Florida ArtiÃ„?Ä¹Å¡ cial Intelligence Research Society Conference (FLAIRS), pages 376Ä‚Ë˜&euro;&ldquo;381, 2003a. 233  A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS  I. Tsamardinos, C. F. Aliferis, and A. Statnikov. Time and sample efÃ„?Ä¹Å¡ cient discovery of Markov blankets and direct causal relations. Proceedings of the Ninth International Conference on Knowledge Discovery and Data Mining (KDD), pages 673Ä‚Ë˜&euro;&ldquo;678, 2003b. I. Tsamardinos, C. F. Aliferis, A. Statnikov, and L. E. Brown. Scaling-up Bayesian network learning to thousands of variables using local learning technique. Technical Report DSL 03-02, 12, 2003c. I. Tsamardinos, Brown L.E., and C. F. Aliferis. The max-min hill-climbing Bayesian network structure learning algorithm. Technical report DSL-05-01, 2005. I. Tsamardinos, L. E. Brown, and C. F. Aliferis. The max-min hill-climbing Bayesian network structure learning algorithm. Machine Learning, 65(1):31Ä‚Ë˜&euro;&ldquo;78, 2006. L. Wang, J. Zhu, and H. Zou. The doubly regularized support vector machine. Statistica Sinica, 16: 589Ä‚Ë˜&euro;&ldquo;615, 2006. Y. Wang, J. G. Klijn, Y. Zhang, A. M. Sieuwerts, M. P. Look, F. Yang, D. Talantov, M. Timmermans, M. E. Meijer-van Gelder, J. Yu, T. Jatkoe, E. M. Berns, D. Atkins, and J. A. Foekens. Geneexpression proÃ„?Ä¹Å¡ les to predict distant metastasis of lymph-node-negative primary breast cancer. Lancet, 365(9460):671Ä‚Ë˜&euro;&ldquo;679, Feb 2005. J. Weston, A. Elisseeff, B. Scholkopf, and M. Tipping. Use of the zero-norm with linear models and kernel methods. Journal of Machine Learning Research, 3(7):1439Ä‚Ë˜&euro;&ldquo;1461, 2003. S. Yaramakala and D. Margaritis. Speculative Markov blanket discovery for optimal feature selection. Proceedings of the Fifth IEEE International Conference on Data Mining, pages 809Ä‚Ë˜&euro;&ldquo;812, 2005. C. Yoo and G. F. Cooper. An evaluation of a system that recommends microarray experiments to perform to discover gene-regulation pathways. Artif.Intell.Med., 31(2):169Ä‚Ë˜&euro;&ldquo;182, Jun 2004. X. Zhou, M. C. J. Kao, and W. H. Wong. Transitive functional annotation by shortest-path analysis of gene expression data. Proceedings of the National Academy of Sciences, 99(20):12783Ä‚Ë˜&euro;&ldquo;12788, 2002. J. Zhu, S. Rosset, T. Hastie, and R. Tibshirani. 1-norm support vector machines. Advances in Neural Information Processing Systems (NIPS), 16, 2004. H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society Series B(Statistical Methodology), 67(2):301Ä‚Ë˜&euro;&ldquo;320, 2005.  234</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
