<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-107" href="../jmlr2010/jmlr-2010-Stacked_Denoising_Autoencoders%3A_Learning_Useful_Representations_in_a_Deep_Network_with_a_Local_Denoising_Criterion.html">jmlr2010-107</a> <a title="jmlr-2010-107-reference" href="#">jmlr2010-107-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</h1>
<br/><p>Source: <a title="jmlr-2010-107-pdf" href="http://jmlr.org/papers/volume11/vincent10a/vincent10a.pdf">pdf</a></p><p>Author: Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol</p><p>Abstract: We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classiﬁcation problems to yield signiﬁcantly lower classiﬁcation error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classiﬁers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations. Keywords: deep learning, unsupervised feature learning, deep belief networks, autoencoders, denoising</p><br/>
<h2>reference text</h2><p>G. An. The effects of adding noise during backpropagation training on a generalization performance. Neural Computation, 8(3):643–674, 1996. H. Baird. Document image defect models. In IAPR Workshop on Syntactic and Structural Pattern Recognition, pages 38–46, Murray Hill, NJ., 1990. P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural Networks, 2:53–58, 1989. A. Bell and T.J. Sejnowski. The independent components of natural scenes are edge ﬁlters. Vision Research, 37:3327–3338, 1997. A.J. Bell and T.J. Sejnowski. An information maximisation approach to blind separation and blind deconvolution. Neural Computation, 7(6):1129–1159, 1995. Y. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1): 1–127, 2009. Also published as a book. Now Publishers, 2009. Y. Bengio and O. Delalleau. Justifying and generalizing contrastive divergence. Neural Computation, 21(6):1601–1621, June 2009. Y. Bengio and Y. LeCun. Scaling learning algorithms towards AI. In L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors, Large Scale Kernel Machines. MIT Press, 2007. Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In Bernhard Sch¨ lkopf, John Platt, and Thomas Hoffman, editors, Advances in Neural o Information Processing Systems 19 (NIPS’06), pages 153–160. MIT Press, 2007. J. Bergstra. Algorithms for classifying recorded music by genre. Master’s thesis, Universit´ de e Montreal, 2006. J. Besag. Statistical analysis of non-lattice data. The Statistician, 24(3):179–195, 1975. C.M. Bishop. Training with noise is equivalent to Tikhonov regularization. Neural Computation, 7 (1):108–116, 1995. H. Bourlard and Y. Kamp. Auto-association by multilayer perceptrons and singular value decomposition. Biological Cybernetics, 59:291–294, 1988. O. Chapelle, B. Sch¨ lkopf, and A. Zien, editors. Semi-Supervised Learning. MIT Press, Cambridge, o MA, 2006. Y. Cho and L. Saul. Kernel methods for deep learning. In Y. Bengio, D. Schuurmans, C. Williams, J. Lafferty, and A. Culotta, editors, Advances in Neural Information Processing Systems 22 (NIPS’09), pages 342–350. NIPS Foundation, 2010. D. Erhan, Y. Bengio, A. Courville, P.A. Manzagol, P. Vincent, and S. Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11:625–660, February 2010. 3405  V INCENT, L AROCHELLE , L AJOIE , B ENGIO AND M ANZAGOL  P. Gallinari, Y. LeCun, S. Thiria, and F. Fogelman-Soulie. Memoires associatives distribuees. In Proceedings of COGNITIVA 87, Paris, La Villette, 1987. Y. Grandvalet, S. Canu, and S. Boucheron. Noise injection: Theoretical prospects. Neural Computation, 9(5):1093–1108, 1997. J. H˚ stad. Almost optimal lower bounds for small depth circuits. In Proceedings of the 18th annual a ACM Symposium on Theory of Computing, pages 6–20, Berkeley, California, 1986. ACM Press. J. H˚ stad and M. Goldmann. On the power of small-depth threshold circuits. Computational Coma plexity, 1:113–129, 1991. D. Heckerman, D.M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. Dependency networks for inference, collaborative ﬁltering, and data visualization. Journal of Machine Learning Research, 1:49–75, 2000. G.E. Hinton. Connectionist learning procedures. Artiﬁcial Intelligence, 40:185–234, 1989. G.E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14:1771–1800, 2002. G.E. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, July 2006. G.E. Hinton, S. Osindero, and Y.W. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527–1554, 2006. L. Holmstrm and P. Koistinen. Using additive noise in back-propagation training. IEEE Transactions on Neural Networks, 3(1):24–38, 1992. J.J. Hopﬁeld. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences, USA, 79, 1982. D.H. Hubel and T.N. Wiesel. Receptive ﬁelds of single neurons in the cat’s striate cortex. Journal of Physiology, 148:574–591, 1959. V. Jain and S.H. Seung. Natural image denoising with convolutional networks. In Daphne Koller, Dale Schuurmans, Yoshua Bengio, and Leon Bottou, editors, Advances in Neural Information Processing Systems 21 (NIPS’08), 2008. N. Japkowicz, S.J. Hanson, and M.A. Gluck. Nonlinear autoassociation is not equivalent to PCA. Neural Computation, 12(3):531–545, 2000. H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Z. Ghahramani, editor, Proceedings of the Twenty-fourth International Conference on Machine Learning (ICML’07), pages 473–480. ACM, 2007. H. Larochelle, Y. Bengio, J. Louradour, and P. Lamblin. Exploring strategies for training deep neural networks. Journal of Machine Learning Research, 10:1–40, January 2009a. 3406  S TACKED D ENOISING AUTOENCODERS  H. Larochelle, D. Erhan, and P. Vincent. Deep learning using robust interdependent codes. In Proceedings of the Twelfth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2009), pages 312–319, April 2009b. Y. LeCun. Mod` les connexionistes de l’apprentissage. PhD thesis, Universit´ de Paris VI, 1987. e e Y. LeCun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989. H. Lee, C. Ekanadham, and A. Ng. Sparse deep belief net model for visual area V2. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20 (NIPS’07), pages 873–880, Cambridge, MA, 2008. MIT Press. R. Linsker. An application of the principle of maximum information preservation to linear systems. In D.S. Touretzky, editor, Advances in Neural Information Processing Systems 1 (NIPS’88). Morgan Kaufmann, 1989. J.L. McClelland, D.E. Rumelhart, and the PDP Research Group. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 2. MIT Press, Cambridge, 1986. B.A. Olshausen and D.J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381:607–609, 1996. B.A. Olshausen and D.J. Field. Sparse coding with an overcomplete basis set: a strategy employed by V1? Vision Research, 37:3311–3325, December 1997. T. Poggio and T. Vetter. Recognition and structure from one 2d model view: Observations on prototypes, object classes and symmetries. Technical Report A.I. Memo No. 1347, Artiﬁcial Intelligence Laboratory, Massachusetts Institute of Technology, 1992. M. Ranzato, C.S. Poultney, S. Chopra, and Y. LeCun. Efﬁcient learning of sparse representations with an energy-based model. In B. Sch¨ lkopf, J. Platt, and T. Hoffman, editors, Advances in o Neural Information Processing Systems 19 (NIPS’06), pages 1137–1144. MIT Press, 2007. M. Ranzato, Y. Boureau, and Y. LeCun. Sparse feature learning for deep belief networks. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20 (NIPS’07), pages 1185–1192, Cambridge, MA, 2008. MIT Press. R. Scalettar and A. Zee. Emergence of grandmother memory in feed forward networks: Learning with noise and forgetfulness. In D. Waltz and J. A. Feldman, editors, Connectionist Models and Their Implications: Readings from Cognitive Science, pages 309–332. Ablex, Norwood, 1988. B. Sch¨ lkopf, C.J.C. Burges, and V. Vapnik. Incorporating invariances in support vector learning o machines. In C. von der Malsburg, W. von Seelen, J. C. Vorbrggen, and B. Sendhoff, editors, Lecture Notes in Computer Science (Vol 112), Artiﬁcial Neural Netowrks ICANN’96, pages 47– 52. Springer, 1996. S.H. Seung. Learning continuous attractors in recurrent networks. In M.I. Jordan, M.J. Kearns, and S.A. Solla, editors, Advances in Neural Information Processing Systems 10 (NIPS’97), pages 654–660. MIT Press, 1998. 3407  V INCENT, L AROCHELLE , L AJOIE , B ENGIO AND M ANZAGOL  J. Sietsma and R. Dow. Creating artiﬁcial neural networks that generalize. Neural Networks, 4(1): 67–79, 1991. P. Simard, B. Victorri, Y. LeCun, and J. Denker. Tangent prop - A formalism for specifying selected invariances in an adaptive network. In J.E. Moody S.J. Hanson and R.P. Lippmann, editors, Advances in Neural Information Processing Systems 4 (NIPS’91), pages 895–903, San Mateo, CA, 1992. Morgan Kaufmann. P. Smolensky. Information processing in dynamical systems: Foundations of harmony theory. In D.E. Rumelhart and J.L. McClelland, editors, Parallel Distributed Processing, volume 1, chapter 6, pages 194–281. MIT Press, Cambridge, 1986. P.E. Utgoff and D.J. Stracuzzi. Many-layered learning. Neural Computation, 14:2497–2539, 2002. P. Vincent, H. Larochelle, Y. Bengio, and P.A. Manzagol. Extracting and composing robust features with denoising autoencoders. In W.W. Cohen, A. McCallum, and S.T. Roweis, editors, Proceedings of the Twenty-ﬁfth International Conference on Machine Learning (ICML’08), pages 1096–1103. ACM, 2008. A. von Lehman, E.G. Paek, P.F. Liao, A. Marrakchi, and J.S. Patel. Factors inﬂuencing learning by back-propagation. In IEEE International Conference on Neural Networks, volume 1, pages 335–341, San Diego 1988, 1988. IEEE, New York. J. Weston, F. Ratle, and R. Collobert. Deep learning via semi-supervised embedding. In William W. Cohen, Andrew McCallum, and Sam T. Roweis, editors, Proceedings of the Twenty-ﬁfth International Conference on Machine Learning (ICML’08), pages 1168–1175, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390303.  3408</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
