<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 jmlr-2010-A Fast Hybrid Algorithm for Large-Scalel1-Regularized Logistic Regression</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-3" href="../jmlr2010/jmlr-2010-A_Fast_Hybrid_Algorithm_for_Large-Scalel1-Regularized_Logistic_Regression.html">jmlr2010-3</a> <a title="jmlr-2010-3-reference" href="#">jmlr2010-3-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3 jmlr-2010-A Fast Hybrid Algorithm for Large-Scalel1-Regularized Logistic Regression</h1>
<br/><p>Source: <a title="jmlr-2010-3-pdf" href="http://jmlr.org/papers/volume11/shi10a/shi10a.pdf">pdf</a></p><p>Author: Jianing Shi, Wotao Yin, Stanley Osher, Paul Sajda</p><p>Abstract: ℓ1 -regularized logistic regression, also known as sparse logistic regression, is widely used in machine learning, computer vision, data mining, bioinformatics and neural signal processing. The use of ℓ1 regularization attributes attractive properties to the classiﬁer, such as feature selection, robustness to noise, and as a result, classiﬁer generality in the context of supervised learning. When a sparse logistic regression problem has large-scale data in high dimensions, it is computationally expensive to minimize the non-differentiable ℓ1 -norm in the objective function. Motivated by recent work (Koh et al., 2007; Hale et al., 2008), we propose a novel hybrid algorithm based on combining two types of optimization iterations: one being very fast and memory friendly while the other being slower but more accurate. Called hybrid iterative shrinkage (HIS), the resulting algorithm is comprised of a ﬁxed point continuation phase and an interior point phase. The ﬁrst phase is based completely on memory efﬁcient operations such as matrix-vector multiplications, while the second phase is based on a truncated Newton’s method. Furthermore, we show that various optimization techniques, including line search and continuation, can signiﬁcantly accelerate convergence. The algorithm has global convergence at a geometric rate (a Q-linear rate in optimization terminology). We present a numerical comparison with several existing algorithms, including an analysis using benchmark data from the UCI machine learning repository, and show our algorithm is the most computationally efﬁcient without loss of accuracy. Keywords: logistic regression, ℓ1 regularization, ﬁxed point continuation, supervised learning, large scale c 2010 Jianing Shi, Wotao Yin, Stanley Osher and Paul Sajda. S HI , Y IN , O SHER AND S AJDA</p><br/>
<h2>reference text</h2><p>C.M. Bishop. Pattern Recognition and Machine Learning. Springer, 2007. S. Boyd, S.-J. Kim, L. Vandenberghe, and A. Hassibi. A tutorial on geometric programming. Optimization and Engineering, 8(1):67–127, 2007. M. Burger, G. Gilboa, S. Osher, and J. Xu. Nonlinear inverse scale space methods. Communications in Mathematical Sciences, 4:179–212, 2006. E.J. Cand´ s and T. Tao. Near optimal signal recovery from random projections: Universal encoding e strategies? IEEE Trans. Inform. Theory, 52(2):5406–5425, 2006. E.J. Cand´ s, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction e from highly incomplete frequency information. IEEE Trans. Inform. Theory, 52(2):489–509, 2006. S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM J. Scientiﬁc Computing, 20:33–61, 1998. J.F. Claerbout and F. Muir. Robust modeling with erratic data. Geophysics, 38(5):826–844, 1973. A. d’Aspremont, L. El Ghaoui, M. Jordan, and G. Lanckriet. A direct formulation for sparse pca using semideﬁnite programming. In Advances in Neural Information Processing Systems, pages 41–48. MIT Press, 2005. D.L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52:1289–1306, 2006. D.L. Donoho and M. Elad. Optimally sparse representations in general nonorthogonal dictionaries by ℓ1 minimization. Proc. Nat’l Academy of Science, 100(5):2197–2202, 2003. D.L. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE. Trans. Inform. Theory, 48(9):2845–2862, 2001. D.L. Donoho and B.F. Logan. Signal recovery and the large sieve. SIAM J. Appl. Math., 52(2): 577–591, 1992. D.L. Donoho and P.B. Stark. Uncertainty principle and signal recovery. SIAM J. Appl. Math., 49(3): 906–931, 1989. 738  H YBRID I TERATIVE S HRINKAGE - HIS  D.L. Donoho, I. Johnstone, G. Kerkyacharian, and D. Picard. Wavelet shrinkage: Asymptopia? J. Roy. Stat. Soc. B, 57(2):301–337, 1995. B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407–499, 2004. S. Eyheramendy, A. Genkin, W. Ju, D. Lewis, and D. Madigan. Sparse bayesian classiﬁers for text categorization. Technical report, J. Intelligence Community Research and Development, 2003. M. Figueiredo. Adaptive sparseness for supervised learning. IEEE Trans. Pattern Analysis and Machine Intelligence, 25:1150–1159, 2003. M. Figueiredo and A. Jain. Bayesian learning of sparse classiﬁers. In IEEE Conf. Computer Vision and Pattern Recognition, pages 35–41, 2001. M. Figueiredo, R. Nowak, and S. Wright. Gradient projection for sparse reconstruction: application to compressed sensing and other inverse problems. IEEE J. Selected Topics in Signal Processing: Special Issue on Convex Optimization Methods for Signal Processing, 1(4):586–598, 2007. A. Genkin, D.D. Lewis, and D. Madigan. Large-scale bayesian logistic regression for text categorization. Technometrics, 49(3):291–304, 2007. A.D. Gerson, L.C. Parra, and P. Sajda. Cortical origins of response time variability during rapid discrimination of visual objects. Neuroimage, 28(2):342–353, 2005. A. Ghosh and S. Boyd. Growing well-connected graphs. In 45th IEEE Conference on Decision and Control, pages 6605–6611, 2006. J. Goodman. Exponential priors for maximum entropy models. In Proceedings of the Annual Meetings of the Association for Computational Linguistics, 2004. E. Hale, W. Yin, and Y. Zhang. Fixed-point continuation for ℓ1 -minimization: methodology and convergence. SIAM J. Optimization, 19(3):1107–1130, 2008. A. Hassibi, J. How, and S. Boyd. Low-authority controller design via convex optimization. AIAA Journal of Guidance, Control and Dynamics, 22(6):862–872, 1999. K. Koh, S.-J. Kim, and S. Boyd. An interior-point method for large-scale ℓ1 -regularized logistic regression. J. Machine Learning Research, 8:1519–1555, 2007. B. Krishnapuram and A. Hartemink. Sparse multinomial logistic regression: fast algorithms and generalization bounds. IEEE Trans. Pattern Analysis and Machine Intelligence, 27(6):957–968, 2005. B. Krishnapuram, L. Carin, and M. Figueiredo. Sparse multinomial logistic regression: fast algorithms and generalization bounds. IEEE Trans. Pattern Analysis and Machine Intelligence, 27(6): 957–968, 2005. S. Lee, H. Lee, P. Abbeel, and A. Ng. Efﬁcient ℓ1 -regularized logistic regression. In 21th National Conference on Artiﬁcial Intelligence (AAAI), 2006. 739  S HI , Y IN , O SHER AND S AJDA  D.D. Lewis, Y. Yang, T.G. Rose, and F. Li. Rcv1: A new benchmark collection for text categorization research. J. Machine Learning Research, 5:361–397, 2004. J.G. Liao and K.V. Chin. Logistic regression for disease classiﬁcation using microarray data: model selection in a large p and small n case. Bioinformatics, 23(15):1945–51, 2007. N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2:285–318, 1988. M. Lobo, M. Fazel, and S. Boyd. Portfolio optimization with linear and ﬁxed transaction costs. Annals of Operations Research, 152(1):376–394, 2007. J. Lokhorst. The lasso and generalised linear models. Technical report, Honors Project, Department of Statistics, University of Adelaide, South Australia, Australia, 1999. D. Madigan, A. Genkin, D. Lewis, and D Fradkin. Bayesian multinomial logistic regression for author identiﬁcation. In Maxent Conference, pages 509–516, 2005. B.K. Natarajan. Sparse approximate solutions to linear system. SIAM J. Computing, 24(2):227–234, 1995. A. Ng. Feature selection, ℓ1 vs ℓ2 regularization, and rotational invariance. In International Conference on Machine Learning (ICML), pages 78–85. ACM Press, New York, 2004. A. Ng. On feature selection: Learning with exponentially many irrelevant features as training examples. In International Conference on Machine Learning (ICML), pages 404–412, 1998. S. Osher, Y. Mao, B. Dong, and W. Yin. Fast linearized bregman iteration for compressive sensing and sparse denoising. Communications in Mathematical Sciences, 8(1):93–111, 2010. M.Y. Park and T. Hastie. ℓ1 regularized path algorithm for generalized linear models. J. R. Statist. Soc. B, 69:659–677, 2007. L.C. Parra, C. Spence, and P. Sajda. Higher-order statistical properties arising from the nonstationarity of natural signals. In Advances in Neural Information Processing Systems, volume 13, pages 786–792, 2001. L.C. Parra, C.D. Spence, A.D. Gerson, and P. Sajda. Recipes for the linear analysis of EEG. Neuroimage, 28(2):326–341, 2005. S. Perkins and J. Theiler. Online feature selection using grafting. In Proceedings of the Twenty-First International Conference on Machine Learning (ICML), pages 592–599. ACM Press, 2003. M.G. Philiastides and P. Sajda. Temporal characterization of the neural correlates of perceptual decision making in the human brain. Cereb Cortex, 16(4):509–518, 2006. B. Polyak. Introduction to Optimization. Optimization Software, 1987. V. Roth. The generalized lasso. IEEE Tran. Neural Networks, 15(1):16–28, 2004. L. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica D, 60(1-4):259–268, 1992. 740  H YBRID I TERATIVE S HRINKAGE - HIS  M. Schmidt, G. Fung, and R. Rosales. Fast optimization methods for l1 regularization: a comparative study and two new approaches. In European Conference on Machine Learning (ECML), pages 286–297, 2007. J. Shi and S. Osher. A nonlinear inverse scale space method for a convex multiplicative noise model. SIAM J. Imaging Sciences, 1(3):294–321, 2008. J. Shi, J. Wielaard, R.T. Smith, and P. Sajda. Perceptual decision making investigated via sparse decoding of a spiking neuron model of V1. In 4th International IEEE/EMBS Conference on Neural Engineering, pages 558–561, 2009. N.Z. Shor. Minimization Methods for Non-differentiable functions. Springer Series in Computational Mathematics. Springer, 1985. H.L. Taylor, S.C. Banks, and J.F. McCoy. Deconvolution with the ℓ1 norm. Geophysics, 44(1): 39–52, 1979. R. Tibshirani. Regression shrinkage and selection via the lasso. J. Roy. Stat. Soc. B, 58(1):267–288, 1996. Y. Tsuruoka, J. McNaught, J. Tsujii, and S. Ananiadou. Learning string similarity measures for gene/protein name dictionary look-up using logistic regression. Bioinformatics, 23(20):2768–74, 2007. L. Vandenberghe, S. Boyd, and A. El Gamal. Optimal wire and transistor sizing for circuits with non-tree topology. In IEEE/ACM International Conference on Computer Aided Design, pages 252–259, 1997. L. Vandenberghe, S. Boyd, and A. El Gamal. Optimizing dominant time constant in RC circuits. IEEE Trans. Computer-Aided Design, 17(2):110–125, 1998. V. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-Verlag, 1982. V. Vapnik. Statistical Learning Theory. John Wiley & Sons, 1988. Z. Wen, W. Yin, D. Goldfarb, and Y. Zhang. A fast algorithm for sparse reconstruction based on shrinkage, subspace optimization and continuation. Technical report, Rice University CAAM TR09-01, 2009. W. Yin, S. Osher, J. Darbon, and D. Goldfarb. Bregman iterative algorithm for ℓ1 -minimization with applications to compressed sensing. SIAM J. Imaging Science, 1(1):143–168, 2008. P. Zhao and B. Yu. On model selection consistency of lasso. J. Machine Learning Research, 7: 2541–2567, 2007. J. Zhu, S. Rosset, T. Hastie, and R. Tibshirani. 1-norm support vector machines. In Advances in Neural Information Processing Systems, volume 16, pages 49–56. MIT Press, 2004. H. Zou, T. Hastie, and R. Tibshirani. Sparse principle component analysis. J. Computational and Graphical Statistics, 15(2):262–286, 2006. 741</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
