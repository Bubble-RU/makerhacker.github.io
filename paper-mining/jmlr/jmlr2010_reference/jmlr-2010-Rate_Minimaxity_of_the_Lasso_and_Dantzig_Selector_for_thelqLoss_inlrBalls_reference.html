<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-96" href="../jmlr2010/jmlr-2010-Rate_Minimaxity_of_the_Lasso_and_Dantzig_Selector_for_thelqLoss_inlrBalls.html">jmlr2010-96</a> <a title="jmlr-2010-96-reference" href="#">jmlr2010-96-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</h1>
<br/><p>Source: <a title="jmlr-2010-96-pdf" href="http://jmlr.org/papers/volume11/ye10a/ye10a.pdf">pdf</a></p><p>Author: Fei Ye, Cun-Hui Zhang</p><p>Abstract: We consider the estimation of regression coefﬁcients in a high-dimensional linear model. For regression coefﬁcients in ℓr balls, we provide lower bounds for the minimax ℓq risk and minimax quantiles of the ℓq loss for all design matrices. Under an ℓ0 sparsity condition on a target coefﬁcient vector, we sharpen and unify existing oracle inequalities for the Lasso and Dantzig selector. We derive oracle inequalities for target coefﬁcient vectors with many small elements and smaller threshold levels than the universal threshold. These oracle inequalities provide sufﬁcient conditions on the design matrix for the rate minimaxity of the Lasso and Dantzig selector for the ℓq risk and loss in ℓr balls, 0 ≤ r ≤ 1 ≤ q ≤ ∞. By allowing q = ∞, our risk bounds imply the variable selection consistency of threshold Lasso and Dantzig selectors. Keywords: variable selection, estimation, oracle inequality, minimax, linear regression, penalized least squares, linear programming</p><br/>
<h2>reference text</h2><p>P. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous analysis of lasso and dantzig selector. Annals of Statistics, 37:1705–1732, 2009. F. Bunea, A. Tsybakov, and M. Wegkamp. Sparsity oracle inequalities for the lasso. Electronic Journal of Statistics, 1:169–194, 2007. T. Cai, L. Wang, and G. Xu. Shifting inequality and recovery of sparse signals. IEEE Transactions on Signal Processing, 58:1300–1308, 2010. 3538  R ATE M INIMAXITY OF ℓ1 M ETHODS  E. Candes and Y. Plan. Near-ideal model selection by ℓ1 minimization. Annals of Statistics, 37: 2145–2177, 2009. E. Candes and T. Tao. The dantzig selector: statistical estimation when p is much larger than n (with discussion). Annals of Statistics, 35:2313–2404, 2007. S. Chen and D. L. Donoho. On basis pursuit. Technical report, Department of Statistics, Stanford University, 1994. D. L. Donoho and I. Johnstone. Minimax risk over ℓ p –balls for ℓq –error. Probability Theory and Related Fields, 99:277–303, 1994. B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression (with discussion). Annals of Statistics, 32:407–499, 2004. B. Efron, T. Hastie, and R. Tibshirani. Discussion: The dantzig selector: statistical estimation when p is much larger than n. Annals of Statistics, 35:2358–2364, 2007. J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96:1348–1360, 2001. Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm. In Machine Learning: Proceedings of the Thirteenth International Conference, pages 148–156. Morgan Kauffmann, San Francisco, 1996. J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting (with discussion). Annals of Statistics, 28:337–407, 2000. E. Greenshtein and Y. Rotiv. Persistence in high–dimensional linear predictor selection and the virtue of overparametrization. Bernoulli, 10:971–988, 2004. J. Huang, S. Ma, and C.-H. Zhang. Adaptive lasso for sparse high–dimensional regression models. Statistica Sinica, 18:1603–1618, 2008. V. Koltchinskii. The dantzig selector and sparsity oracle inequalities. Bernoulli, 15:799–828, 2009. K. Lounici. Sup–norm convergence rate and sign concentration property of lasso and dantzig estimators. Electronic Journal of Statistics, 2:90–102, 2008. N. Meinshausen and P. B¨ hlmann. High dimensional graphs and variable selection with the lasso. u Annals of Statistics, 34:1436–1462, 2006. N. Meinshausen and B. Yu. Lasso–type recovery of sparse representations for high–dimensional data. Annals of Statistics, 37:246–270, 2009. M. Osborne, B. Presnell, and B. Turlach. A new approach to variable selection in least squares problems. IMA Journal of Numerical Analysis, 20:389–404, 2000a. M. Osborne, B. Presnell, and B. Turlach. On the lasso and its dual. Journal of Computational and Graphical Statistics, 9(2):319–337, 2000b. 3539  Y E AND Z HANG  G. Raskutti, M. J. Wainwright, and B. Yu. Minimax rates of estimation for high–dimensional linear regression over ℓq –balls. Technical report, University of California, Berkeley, 2009. R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 58:267–288, 1996. J. A. Tropp. Just relax: convex programming methods for identifying sparse signals in noise. IEEE Transactions on Information Theory, 52:1030–1051, 2006. S. van de Geer. The deterministic lasso. Technical Report 140, ETH Zurich, Switzerland, 2007. S. van de Geer. High–dimensional generalized linear models and the lasso. Annals of Statistics, 36: 614–645, 2008. S. van de Geer and P. B¨ hlmann. On the conditions used to prove oracle results for the lasso. u Electronic Journal of Statistics, 3:1360–1392, 2009. M. J. Wainwright. Sharp thresholds for noisy and high–dimensional recovery of sparsity using ℓ1 –constrained quadratic programming (lasso). IEEE Transactions on Information Theory, 55: 2183–2202, 2009. F. Ye and C.-H. Zhang. Rate minimaxity of the lasso and dantzig estimators. Technical report, Department of Statistics and Biostatistics, Rutgers University, 2009. C.-H. Zhang. Least squares estimation and variable selection under minimax concave penalty. In Mathematisches Forschungsintitut Oberwolfach: Sparse Recovery Problems in High Dimensions, 3 2009a. C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Annals of Statistics, 38:894–942, 2010. C.-H. Zhang and J. Huang. The sparsity and bias of the lasso selection in high–dimensional linear regression. Annals of Statistics, 36:1567–1594, 2008. T. Zhang. Some sharp performance bounds for least squares regression with l1 regularization. Annals of Statistics, 37:2109–2144, 2009b. P. Zhao and B. Yu. On model selection consistency of lasso. Journal of Machine Learning Research, 7:2541–2567, 2006. H. Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical Association, 101:1418–1429, 2006.  3540</p>
<br/>
<br/><br/><br/></body>
</html>
