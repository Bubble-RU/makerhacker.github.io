<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-45" href="../jmlr2010/jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">jmlr2010-45</a> <a title="jmlr-2010-45-reference" href="#">jmlr2010-45-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</h1>
<br/><p>Source: <a title="jmlr-2010-45-pdf" href="http://jmlr.org/papers/volume11/omidiran10a/omidiran10a.pdf">pdf</a></p><p>Author: Dapo Omidiran, Martin J. Wainwright</p><p>Abstract: We consider the problem of high-dimensional variable selection: given n noisy observations of a k-sparse vector β∗ ∈ R p , estimate the subset of non-zero entries of β∗ . A signiﬁcant body of work has studied behavior of ℓ1 -relaxations when applied to random measurement matrices that are dense (e.g., Gaussian, Bernoulli). In this paper, we analyze sparsiﬁed measurement ensembles, and consider the trade-off between measurement sparsity, as measured by the fraction γ of nonzero entries, and the statistical efﬁciency, as measured by the minimal number of observations n required for correct variable selection with probability converging to one. Our main result is to prove that it is possible to let the fraction on non-zero entries γ → 0 at some rate, yielding measurement matrices with a vanishing fraction of non-zeros per row, while retaining the same statistical efﬁciency as dense ensembles. A variety of simulation results conﬁrm the sharpness of our theoretical predictions. Keywords: variable selection, sparse random projections, high-dimensional statistics, Lasso, consistency, ℓ1 -regularization</p><br/>
<h2>reference text</h2><p>D. Achlioptas. Database-friendly random projections. In Proc. ACM Symp. Princ. Database Systems (PODS), pages 274–281, New York, USA, 2001. ACM. N. Alon, Y. Matias, and M. Szegedy. The space complexity of approximating the frequency moments. In STOC ’96: Proceedings of the twenty-eighth annual ACM symposium on Theory of computing, pages 20–29, New York, NY, USA, 1996. ACM Press. R. Baraniuk, M. Davenport, R. Devore, and M. Wakin. A simple proof of the restricted isometry property for random matrices. Constr. Approx, 2007. S. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press, Cambridge, UK, 2004. E. Candes and T. Tao. Decoding by linear programming. IEEE Trans. Info Theory, 51(12):4203– 4215, December 2005. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM J. Sci. Computing, 20(1):33–61, 1998. G. Cormode and S. Muthukrishnan. Towards an algorithmic theory of compressed sensing. Technical report, Rutgers University, July 2005. 2384  H IGH - DIMENSIONAL VARIABLE S ELECTION  S. Dasgupta. Learning mixtures of Gaussians. In IEEE Symp. Foundations of Computer Science (FOCS), September 1999. S. Dasgupta and A. Gupta. An elementary proof of a theorem of Johnson and Lindenstrauss. Random Structures and Algorithms, 22(1):60–65, 2003. D. Donoho. For most large underdetermined systems of linear equations, the minimal ℓ1 -norm solution is also the sparsest solution. Communications on Pure and Applied Mathematics, 59(6): 797–829, June 2006. J. Feldman, T. Malkin, R. A. Servedio, C. Stein, and M. J. Wainwright. LP decoding corrects a constant fraction of errors. IEEE Trans. Information Theory, 53(1):82–89, January 2007. A. Gilbert, M. Strauss, J. Tropp, and R. Vershynin. Algorithmic linear dimension reduction in the ℓ1 -norm for sparse vectors. In Proc. Allerton Conference on Communication, Control and Computing, Allerton, IL, September 2006. W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:13–30, 1963. P. Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream computation. J. ACM, 53(3):307–323, May 2006. W. B. Johnson and J. Lindenstrauss. Extensions of lipschitz mapping into hilbert space. Contemporary Mathematics, 26:189–206, 1984. I. M. Johnstone. Chi-square oracle inequalities. In M. de Gunst, C. Klaassen, and A. van der Vaart, editors, State of the Art in Probability and Statistics, number 37 in IMS Lecture Notes, pages 399–418. Institute of Mathematical Statistics, 2001. I. M. Johnstone and A. Lu. On consistency and sparsity for principal components analysis in high dimensions. Journal of the American Statistical Association, 104(486):682–693, 2009. M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. SpringerVerlag, New York, NY, 1991. P. Li, T. J. Hastie, and K. W. Church. Very sparse random projections. In KDD ’06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 287–296, New York, NY, USA, 2006. ACM. P. Li, T. J. Hastie, and K. W. Church. Nonlinear estimators and tail bounds for dimension reduction in l1 using cauchy random projections. Journal of Machine Learning Research, 8:2497–2532, 2007. J. Matousek. Lectures on Discrete Geometry. Springer-Verlag, New York, 2002. N. Meinshausen and P. Buhlmann. High dimensional graphs and variable selection with the Lasso. Annals of Statistics, 34:1436–1462, 2006. D. Paul. Asymptotics of sample eigenstructure for a large-dimensional spiked covariance model. Statistica Sinica, 17:1617–1642, 2007. 2385  O MIDIRAN AND WAINWRIGHT  P. Ravikumar, M. J. Wainwright, and J. Lafferty. High-dimensional ising model selection using ℓ1 -regularized logistic regression. Annals of Statistics, 38(3):1287–1319, 2010. G. Rockafellar. Convex Analysis. Princeton University Press, Princeton, 1970. S. Sarvotham, D. Baron, and R. G. Baraniuk. Sudocodes: Fast measurement and reconstruction of sparse signals. In Int. Symposium on Information Theory, Seattle, WA, July 2006. R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58(1):267–288, 1996. J. Tropp. Just relax: Convex programming methods for identifying sparse signals in noise. IEEE Trans. Info Theory, 52(3):1030–1051, March 2006. R. Vershynin. On large random almost euclidean bases. Acta. Math. Univ. Comenianae, LXIX: 137–144, 2000. R. Vershynin. Random matrix theory. Technical report, UC Davis, 2006. Lecture Notes. M. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using ℓ1 constrained quadratic programming (Lasso). IEEE Trans. Inf. Theor., 55(5):2183–2202, 2009. M. B. Wakin, J. N. Laska, M. F. Duarte, D. Baron, S. Sarvotham, D. Takhar, K. F. Kelly, and R. G. Baraniuk. An architecture for compressive imaging. IEEE Int. Conf. Image Proc., 2006. W. Wang, M. Garofalakis, and K. Ramchandran. Distributed sparse random projections for reﬁnable approximation. In Proc. International Conference on Information Processing in Sensor Networks, Nashville, TN, April 2007. W. Wang, M.J. Wainwright, and K. Ramchandran. Information-theoretic limits on sparse signal recovery: Dense versus sparse measurement matrices. IEEE Trans. Inf. Theor., 56(6):2967–2979, 2010. W. Xu and B. Hassibi. Efﬁcient compressive sensing with deterministic guarantees using expander graphs. Information Theory Workshop, 2007. ITW ’07. IEEE, 2007. P. Zhao and B. Yu. On model selection consistency of lasso. Journal of Machine Learning Research, 7:2541–2563, 2006. S. Zhou, J. Lafferty, and L. Wasserman. Compressed regression. In Neural Information Processing Systems, December 2007.  2386</p>
<br/>
<br/><br/><br/></body>
</html>
