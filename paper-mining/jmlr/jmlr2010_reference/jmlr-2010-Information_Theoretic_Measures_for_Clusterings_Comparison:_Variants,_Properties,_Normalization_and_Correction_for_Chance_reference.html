<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>55 jmlr-2010-Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-55" href="../jmlr2010/jmlr-2010-Information_Theoretic_Measures_for_Clusterings_Comparison%3A_Variants%2C_Properties%2C_Normalization_and_Correction_for_Chance.html">jmlr2010-55</a> <a title="jmlr-2010-55-reference" href="#">jmlr2010-55-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>55 jmlr-2010-Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance</h1>
<br/><p>Source: <a title="jmlr-2010-55-pdf" href="http://jmlr.org/papers/volume11/vinh10a/vinh10a.pdf">pdf</a></p><p>Author: Nguyen Xuan Vinh, Julien Epps, James Bailey</p><p>Abstract: Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and inter-relationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing popular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then highlight to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0, 1] range better than other normalized variants. Keywords: clustering comparison, information theory, adjustment for chance, normalized information distance</p><br/>
<h2>reference text</h2><p>A. N. Albatineh, M. Niewiadomska-Bugaj, and D. Mihalko. On similarity indices and correction for chance agreement. Journal of Classiﬁcation, 23(2):301–313, 2006. S. Asur, D. Ucar, and S. Parthasarathy. An ensemble framework for clustering protein-protein interaction networks. Bioinformatics, 23(13):i29–i40, 2007. 2852  I NFORMATION T HEORETIC M EASURES FOR C LUSTERINGS C OMPARISON  A. Banerjee, I. S. Dhillon, J. Ghosh, and S. Sra. Clustering on the unit hypersphere using von mises-ﬁsher distributions. J. Mach. Learn. Res., 6:1345–1382, 2005. S. Ben-David, U. von Luxburg, and D. Pal. A sober look at clustering stability. In 19th Annual Conference on Learning Theory (COLT 2006), pages 5–19, 2006. M. Charikar, V. Guruswami, and A. Wirth. Clustering with qualitative information. In FOCS ’03: Procs. IEEE Symposium on Foundations of Computer Science, 2003. T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley, 1991. B. E. Dom. An information-theoretic external cluster-validity measure. Technical report, Research Report RJ 10219, IBM, 2001. X. Z. Fern and C. E. Brodley. Random projection for high dimensional data clustering: A cluster ensemble approach. In Procs. ICML’03, pages 186–193, 2003. Z. He, X. Xu, and S. Deng. k-anmi: A mutual information based clustering algorithm for categorical data. Inf. Fusion, 9(2):223–233, 2008. L. Hubert and P. Arabie. Comparing partitions. Journal of Classif., 2(1):193–218, 1985. A. Kraskov, H. Stogbauer, R. G. Andrzejak, and P. Grassberger. Hierarchical clustering using mutual information. EPL (Europhysics Letters), 70(2):278–284, 2005. T. O. Kvalseth. Entropy and correlation: Some comments. Systems, Man and Cybernetics, IEEE Transactions on, 17(3):517–519, 1987. H.O Lancaster. The chi-squared distribution. New York, 1969. John Wiley. M. Li, X. Chen, X. Li, B. Ma, and P. Vit´ nyi. The similarity metric. Information Theory, IEEE a Transactions on, 50(12):3250–3264, 2004. Z. Liu, Z. Guo, and M. Tan. Constructing tumor progression pathways and biomarker discovery with fuzzy kernel kmeans and dna methylation data. Cancer Inform, 6:1–7, 2008. P. Luo, H. Xiong, G. Zhan, J. Wu, and Z. Shi. Information-theoretic distance measures for clustering validation: Generalization and normalization. IEEE Trans. on Knowl. and Data Eng., 21(9): 1249–1262, 2009. M. Meil˘ . Comparing clusterings by the variation of information. In COLT ’03, pages 173–187, a 2003. M. Meil˘ . Comparing clusterings: an axiomatic view. In ICML ’05: Proceedings of the 22nd a international conference on Machine learning, pages 577–584, 2005. ISBN 1-59593-180-5. M. Meil˘ . Comparing clusterings—an information based distance. J. Multivar. Anal., 98(5):873– a 895, 2007. S. Monti, P. Tamayo, J. Mesirov, and T. Golub. Consensus clustering: A resampling-based method for class discovery and visualization of gene expression microarray data. Mach. Learn., 52(1-2): 91–118, 2003. 2853  V INH , E PPS AND BAILEY  W. M. Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66(336):846–850, 1971. O. Shamir and N. Tishby. Model selection and stability in k-means clustering. In 21th Annual Conference on Learning Theory (COLT 2008), 2008. V. Singh, L. Mukherjee, J. Peng, and J. Xu. Ensemble clustering using semideﬁnite programming with applications. Mach. Learn., 2009. doi: 10.1007/s10994-009-5158-y. D. Steinley. Properties of the Hubert-Arabie adjusted Rand index. Psychol Methods, 9(3):386–96, 2004. A. Strehl and J. Ghosh. Cluster ensembles - a knowledge reuse framework for combining multiple partitions. Journal of Machine Learning Research, 3:583–617, 2002. K. Tumer and A.K. Agogino. Ensemble clustering with voting active clusters. Pattern Recognition Letters, 29(14):1947–1953, 2008. N. X. Vinh and J. Epps. A novel approach for automatic number of clusters detection in microarray data based on consensus clustering. In BIBE’09: Procs. IEEE Int. Conf. on BioInformatics and BioEngineering, 2009. N. X. Vinh, J. Epps, and J. Bailey. Information theoretic measures for clusterings comparison: Is a correction for chance necessary? In ICML ’09, 2009. M. Warrens. On similarity coefﬁcients for 2x2 tables and correction for chance. Psychometrika, 73 (3):487–502, 2008. J. Wu, H. Xiong, and J. Chen. Adapting the right measures for k-means clustering. In KDD ’09, 2009. Y. Y. Yao. Information-theoretic measures for knowledge discovery and data mining. In Entropy Measures, Maximum Entropy Principle and Emerging Applications, pages 115–136. Karmeshu (ed.), Springer, 2003. Z. Yu, H-S. Wong, and H. Wang. Graph-based consensus clustering for class discovery from gene expression data. Bioinformatics, 23(21):2888–2896, 2007.  2854</p>
<br/>
<br/><br/><br/></body>
</html>
