<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-99" href="../jmlr2010/jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">jmlr2010-99</a> <a title="jmlr-2010-99-reference" href="#">jmlr2010-99-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</h1>
<br/><p>Source: <a title="jmlr-2010-99-pdf" href="http://jmlr.org/papers/volume11/raskutti10a/raskutti10a.pdf">pdf</a></p><p>Author: Garvesh Raskutti, Martin J. Wainwright, Bin Yu</p><p>Abstract: Methods based on ℓ1 -relaxation, such as basis pursuit and the Lasso, are very popular for sparse regression in high dimensions. The conditions for success of these methods are now well-understood: (1) exact recovery in the noiseless setting is possible if and only if the design matrix X satisﬁes the restricted nullspace property, and (2) the squared ℓ2 -error of a Lasso estimate decays at the minimax optimal rate k log p , where k is the sparsity of the p-dimensional regression problem with additive n Gaussian noise, whenever the design satisﬁes a restricted eigenvalue condition. The key issue is thus to determine when the design matrix X satisﬁes these desirable properties. Thus far, there have been numerous results showing that the restricted isometry property, which implies both the restricted nullspace and eigenvalue conditions, is satisﬁed when all entries of X are independent and identically distributed (i.i.d.), or the rows are unitary. This paper proves directly that the restricted nullspace and eigenvalue conditions hold with high probability for quite general classes of Gaussian matrices for which the predictors may be highly dependent, and hence restricted isometry conditions can be violated with high probability. In this way, our results extend the attractive theoretical guarantees on ℓ1 -relaxations to a much broader class of problems than the case of completely independent or unitary designs. Keywords: Lasso, basis pursuit, random matrix theory, Gaussian comparison inequality, concentration of measure</p><br/>
<h2>reference text</h2><p>R. Adamczak, A. Litvak, N. Tomczak-Jaegermann, and A. Pajor. Restricted isometry property of matrices with independent columns and neighborly polytopes by random sampling. Technical report, University of Alberta, 2009. K. S. Alexander. Rates of growth for weighted empirical processes. In Proceedings of the Berkeley Conference in Honor of Jerzy Neyman and Jack Kiefer, pages 475–493. UC Press, Berkeley, 1985. P. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous analysis of lasso and dantzig selector. Annals of Statistics, 37(4):1705–1732, 2009. F. Bunea, A. Tsybakov, and M. Wegkamp. Sparsity oracle inequalities for the Lasso. Electronic Journal of Statistics, pages 169–194, 2007. E. Candes and T. Tao. Decoding by linear programming. IEEE Trans. Info Theory, 51(12):4203– 4215, December 2005. E. Candes and T. Tao. The Dantzig selector: Statistical estimation when p is much larger than n. Annals of Statistics, 35(6):2313–2351, 2007. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM J. Sci. Computing, 20(1):33–61, 1998. A. Cohen, W. Dahmen, and R. DeVore. Compressed sensing and best k-term approximation. J. of. American Mathematical Society, 22(1):211–231, January 2009. K. R. Davidson and S. J. Szarek. Local operator theory, random matrices, and Banach spaces. In Handbook of Banach Spaces, volume 1, pages 317–336. Elsevier, Amsterdan, NL, 2001. D. Donoho. Compressed sensing. IEEE Trans. Info. Theory, 52(4):1289–1306, April 2006. D. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE Trans. Info Theory, 47(7):2845–2862, 2001. 2257  R ASKUTTI , WAINWRIGHT AND Y U  M. Elad and A. M. Bruckstein. A generalized uncertainty principle and sparse representation in pairs of bases. IEEE Trans. Info Theory, 48(9):2558–2567, September 2002. A. Feuer and A. Nemirovski. On sparse representation in pairs of bases. IEEE Trans. Info Theory, 49(6):1579–1581, 2003. Y. Gordon. Some inequalities for Gaussian processes and applications. Israel Journal of Mathematics, 50(4):265–289, 1985. R. M. Gray. Toeplitz and Circulant Matrices: A Review. Technical report, Stanford University, Information Systems Laboratory, 1990. O. Gu´ don, S. Mendelson, A. Pajor, and N. Tomczak-Jaegermann. Subspaces and orthogonal dee compositions generated by bounded orthogonal systems. Journal of Positivity, 11(2):269–283, 2007. O. Gu´ don, S. Mendelson, A. Pajor, and N. Tomczak-Jaegermann. Majorizing measures and propore tional subsets of bounded orthonormal systems. Journal of Rev. Mat. Iberoam, 24(3):1075–1095, 2008. J. Haupt, W. U. Bajwa, G. Raz, and R. Nowak. Toeplitz compressed sensing matrices with applications to sparse channel estimation. Technical report, University of Wisconsin-Madison, 2010. R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, 1985. M. Ledoux. The Concentration of Measure Phenomenon. Mathematical Surveys and Monographs. American Mathematical Society, Providence, RI, 2001. M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. SpringerVerlag, New York, NY, 1991. P. Massart. Concentration Inequalties and Model Selection. Ecole d’Et´ de Probabilit´ s, Sainte e Flour. Springer, New York, 2003. N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional data. Annals of Statistics, 37(1):246–270, 2009. S. Mendelson, A. Pajor, and N. Tomczak-Jaegermann. Uniform uncertainty principle for bernoulli and subgaussian ensembles. Journal of Constr. Approx., 28(3):277–289, 2008. S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A uniﬁed framework for highdimensional analysis of M-estimators with decomposable regularizers. In Proceedings of NIPS, December 2009. G. Raskutti, M. J. Wainwright, and B. Yu. Minimax rates of estimation for high-dimensional linear regression over ℓq -balls. Technical report, U. C. Berkeley, October 2009. Posted as http://arxiv.org/abs/0910.2042. Justin Romberg. Compressive sensing by random convolution. SIAM Journal of Imaging Science, 2(4):1098–1128, 2009. 2258  R ESTRICTED E IGENVALUE P ROPERTIES FOR C ORRELATED G AUSSIAN D ESIGNS  M. Rudelson and R. Vershynin. On sparse reconstruction from fourier and gaussian measurements. Comm. Pure and Appl. Math., 61(8):1025–1045, 2008. R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58(1):267–288, 1996. S. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000. S. van de Geer. The deterministic lasso. In Proc. of Joint Statistical Meeting, 2007. S. van de Geer and P. Buhlmann. On the conditions used to prove oracle results for the lasso. Electronic Journal of Statistics, 3:1360–1392, 2009. C. H. Zhang and J. Huang. The sparsity and bias of the lasso selection in high-dimensional linear regression. Annals of Statistics, 36(4):1567–1594, 2008. S. Zhou. Restricted eigenvalue conditions on subgaussian random matrices. Technical report, Department of Mathematics, ETH Z¨ rich, December 2009. u  2259</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
