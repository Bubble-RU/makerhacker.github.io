<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-57" href="../jmlr2010/jmlr-2010-Iterative_Scaling_and_Coordinate_Descent_Methods_for_Maximum_Entropy_Models.html">jmlr2010-57</a> <a title="jmlr-2010-57-reference" href="#">jmlr2010-57-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</h1>
<br/><p>Source: <a title="jmlr-2010-57-pdf" href="http://jmlr.org/papers/volume11/huang10a/huang10a.pdf">pdf</a></p><p>Author: Fang-Lan Huang, Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin</p><p>Abstract: Maximum entropy (Maxent) is useful in natural language processing and many other areas. Iterative scaling (IS) methods are one of the most popular approaches to solve Maxent. With many variants of IS methods, it is difﬁcult to understand them and see the differences. In this paper, we create a general and uniﬁed framework for iterative scaling methods. This framework also connects iterative scaling and coordinate descent methods. We prove general convergence results for IS methods and analyze their computational complexity. Based on the proposed framework, we extend a coordinate descent method for linear SVM to Maxent. Results show that it is faster than existing iterative scaling methods. Keywords: maximum entropy, iterative scaling, coordinate descent, natural language processing, optimization</p><br/>
<h2>reference text</h2><p>Galen Andrew and Jianfeng Gao. Scalable training of L1-regularized log-linear models. In Proceedings of the Twenty Fourth International Conference on Machine Learning (ICML), 2007. Tom M. Apostol. Mathematical Analysis. Addison-Wesley, second edition, 1974. Jason Baldridge, Tom Morton, and Gann Bierner. //opennlp.sourceforge.net/.  OpenNLP package, 2001.  URL http:  Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71, 1996. Dimitri P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, Belmont, MA 02178-9998, second edition, 1999. 845  H UANG , H SIEH , C HANG AND L IN  L´ on Bottou. Stochastic learning. In Olivier Bousquet and Ulrike von Luxburg, editors, Advanced e Lectures on Machine Learning, Lecture Notes in Artiﬁcial Intelligence, LNAI 3176, pages 146– 168. Springer Verlag, 2004. Kai-Wei Chang, Cho-Jui Hsieh, and Chih-Jen Lin. Coordinate descent method for large-scale L2loss linear SVM. Journal of Machine Learning Research, 9:1369–1398, 2008. URL http: //www.csie.ntu.edu.tw/˜cjlin/papers/cdl2.pdf. Stanley F. Chen and Ronald Rosenfeld. A survey of smoothing techniques for ME models. IEEE Transactions on Speech and Audio Processing, 8(1):37–50, January 2000. Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, AdaBoost and Bregman distances. Machine Learning, 48(1–3):253–285, 2002. Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, and Peter Bartlett. Exponentiated gradient algorithms for conditional random ﬁelds and max-margin Markov networks. Journal of Machine Learning Research, 9:1775–1822, 2008. John N. Darroch and Douglas Ratcliff. Generalized iterative scaling for log-linear models. The Annals of Mathematical Statistics, 43(5):1470–1480, 1972. Hal Daum´ , III. Notes on CG and LM-BFGS optimization of logistic regression. 2004. URL e http://www.cs.utah.edu/˜hal/megam/. Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. Inducing features of random ﬁelds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393, 1997. Miroslav Dud´k, Steven J. Phillips, and Robert E. Schapire. Performance guarantees for reguları ized maximum entropy density estimation. In Proceedings of the 17th Annual Conference on Computational Learning Theory, pages 655–662, New York, 2004. ACM press. Roger Fletcher. Practical Methods of Optimization. John Wiley and Sons, 1987. Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Regularization paths for generalized linear models via coordinate descent. 2008. Jianfeng Gao, Galen Andrew, Mark Johnson, and Kristina Toutanova. A comparative study of parameter estimation methods statistical natural language processing. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL), pages 824–831, 2007. Alexandar Genkin, David D. Lewis, and David Madigan. Large-scale Bayesian logistic regression for text categorization. Technometrics, 49(3):291–304, 2007. Joshua Goodman. Sequential conditional generalized iterative scaling. In Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics (ACL), pages 9–16, 2002. Luigi Grippo and Marco Sciandrone. Globally convergent block-coordinate techniques for unconstrained optimization. Optimization Methods and Software, 10:587–637, 1999. 846  I TERATIVE S CALING AND C OORDINATE D ESCENT M ETHODS FOR M AXIMUM E NTROPY M ODELS  Fang-Lan Huang, Cho-Jui Hsieh, Kai-Wei Chang, and Chih-Jen Lin. Iterative scaling and coordinate descent methods for maximum entropy. In Proceedings of the 47th Annual Meeting of the Association of Computational Linguistics (ACL), 2009. Short paper. Rong Jin, Rong Yan, Jian Zhang, and Alex G. Hauptmann. A faster iterative scaling algorithm for conditional exponential model. In Proceedings of the Twentieth International Conference on Machine Learning (ICML), 2003. S. Sathiya Keerthi, Kaibo Duan, Shirish Shevade, and Aun Neow Poo. A fast dual algorithm for kernel logistic regression. Machine Learning, 61:151–165, 2005. Kwangmoo Koh, Seung-Jean Kim, and Stephen Boyd. An interior-point method for large-scale l1-regularized logistic regression. Journal of Machine Learning Research, 8:1519–1555, 2007. URL http://www.stanford.edu/˜boyd/l1_logistic_reg.html. Kenneth Lange, David R. Hunter, and Ilsoon Yang. Optimization transfer using surrogate objective functions. Journal of Computational and Graphical Statistics, 9(1):1–20, March 2000. Chih-Jen Lin, Ruby C. Weng, and S. Sathiya Keerthi. Trust region Newton method for largescale logistic regression. Journal of Machine Learning Research, 9:627–650, 2008. URL http: //www.csie.ntu.edu.tw/˜cjlin/papers/logistic.pdf. Dong C. Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(1):503–528, 1989. Zhi-Quan Luo and Paul Tseng. On the convergence of coordinate descent method for convex differentiable minimization. Journal of Optimization Theory and Applications, 72(1):7–35, 1992. Robert Malouf. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the 6th conference on Natural language learning, pages 1–7. Association for Computational Linguistics, 2002. Ryan McDonald and Fernando Pereira. Online learning of approximate dependency parsing algorithms. In Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 81–88, 2006. Thomas P. Minka. A comparison of numerical optimizers for logistic regression, 2003. URL http://research.microsoft.com/˜minka/papers/logreg/. Adwait Ratnaparkhi. Maximum Entropy Models For Natural Language Ambiguity Resolution. PhD thesis, University of Pennsylvania, 1998. Nicol N. Schraudolph, Jin Yu, and Simon Gunter. A stochastic quasi-Newton method for online convex optimization. In Proceedings of the 11th International Conference Artiﬁcial Intelligence and Statistics (AISTATS), pages 433–440, 2007. S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W. Schmidt, and Kevin Murphy. Accelerated training of conditional random ﬁelds with stochastic gradient methods. In Proceedings of the 23rd International Conference on Machine Learning (ICML), pages 969–976, 2006. 847  H UANG , H SIEH , C HANG AND L IN  Zhihua Zhang, James T. Kwok, and Dit-Yan Yeung. Surrogate maximization/minimization algorithms and extensions. Machine Learning, 69(1):1–33, October 2007.  848</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
