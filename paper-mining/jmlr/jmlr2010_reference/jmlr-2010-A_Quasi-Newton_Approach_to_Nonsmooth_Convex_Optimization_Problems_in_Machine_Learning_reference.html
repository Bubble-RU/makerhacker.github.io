<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 jmlr-2010-A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-5" href="../jmlr2010/jmlr-2010-A_Quasi-Newton_Approach_to_Nonsmooth_Convex_Optimization_Problems_in_Machine_Learning.html">jmlr2010-5</a> <a title="jmlr-2010-5-reference" href="#">jmlr2010-5-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 jmlr-2010-A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning</h1>
<br/><p>Source: <a title="jmlr-2010-5-pdf" href="http://jmlr.org/papers/volume11/yu10a/yu10a.pdf">pdf</a></p><p>Author: Jin Yu, S.V.N. Vishwanathan, Simon Güunter, Nicol N. Schraudolph</p><p>Abstract: We extend the well-known BFGS quasi-Newton method and its memory-limited variant LBFGS to the optimization of nonsmooth convex objectives. This is done in a rigorous fashion by generalizing three components of BFGS to subdifferentials: the local quadratic model, the identiﬁcation of a descent direction, and the Wolfe line search conditions. We prove that under some technical conditions, the resulting subBFGS algorithm is globally convergent in objective function value. We apply its memory-limited variant (subLBFGS) to L2 -regularized risk minimization with the binary hinge loss. To extend our algorithm to the multiclass and multilabel settings, we develop a new, efﬁcient, exact line search algorithm. We prove its worst-case time complexity bounds, and show that our line search can also be used to extend a recently developed bundle method to the multiclass and multilabel settings. We also apply the direction-ﬁnding component of our algorithm to L1 -regularized risk minimization with logistic loss. In all these contexts our methods perform comparable to or better than specialized state-of-the-art solvers on a number of publicly available data sets. An open source implementation of our algorithms is freely available. Keywords: BFGS, variable metric methods, Wolfe conditions, subgradient, risk minimization, hinge loss, multiclass, multilabel, bundle methods, BMRM, OCAS, OWL-QN</p><br/>
<h2>reference text</h2><p>N. Abe, J. Takeuchi, and M. K. Warmuth. Polynomial Learnability of Stochastic Rules with Respect to the KL-Divergence and Quadratic Distance. IEICE Transactions on Information and Systems, 84(3):299–316, 2001. P. K. Agarwal and M. Sharir. Davenport-Schinzel sequences and their geometric applications. In J. Sack and J. Urrutia, editors, Handbook of Computational Geometry, pages 1–47. NorthHolland, New York, 2000. G. Andrew and J. Gao. Scalable training of L1 -regularized log-linear models. In Proc. Intl. Conf. Machine Learning, pages 33–40, New York, NY, USA, 2007. ACM. J. Basch. Kinetic Data Structures. PhD thesis, Stanford University, June 1999. D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, Belmont, MA, 1999. J. R. Birge, L. Qi, and Z. Wei. A general approach to convergence properties of some methods for nonsmooth convex optimization. Applied Mathematics and Optimization, 38(2):141–158, 1998. A. Bordes, L. Bottou, P. Gallinari, and J. Weston. Solving multiclass support vector machines with LaRank. In Proc. Intl. Conf. Machine Learning, pages 89–96, New York, NY, USA, 2007. ACM. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, England, 2004. K. Crammer and Y. Singer. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951–991, January 2003a. K. Crammer and Y. Singer. A family of additive online algorithms for category ranking. J. Mach. Learn. Res., 3:1025–1058, February 2003b. V. Franc and S. Sonnenburg. Optimized cutting plane algorithm for support vector machines. In A. McCallum and S. Roweis, editors, ICML, pages 320–327. Omnipress, 2008. 1198  Q UASI -N EWTON A PPROACH TO N ONSMOOTH C ONVEX O PTIMIZATION  V. Franc and S. Sonnenburg. Optimized cutting plane algorithm for large-scale risk minimization. Journal of Machine Learning Research, 10:2157–2192, 2009. M. Haarala. Large-Scale Nonsmooth Optimization. PhD thesis, University of Jyv¨ skyl¨ , 2004. a a J. Hershberger. Finding the upper envelope of n line segments in O(n log n) time. Information Processing Letters, 33(4):169–174, December 1989. J. B. Hiriart-Urruty and C. Lemar´ chal. Convex Analysis and Minimization Algorithms, I and II, e volume 305 and 306. Springer-Verlag, 1993. T. Joachims. Training linear SVMs in linear time. In Proc. ACM Conf. Knowledge Discovery and Data Mining (KDD). ACM, 2006. Y. J. Lee and O. L. Mangasarian. SSVM: A smooth support vector machine for classiﬁcation. Computational optimization and Applications, 20(1):5–22, 2001. C. Lemarechal. Numerical experiments in nonsmooth optimization. Progress in Nondifferentiable Optimization, 82:61–84, 1982. A. S. Lewis and M. L. Overton. Nonsmooth optimization via BFGS. Technical report, Optimization Online, 2008a. URL http://www.optimization-online.org/DB_FILE/2008/12/ 2172.pdf. Submitted to SIAM J. Optimization. A. S. Lewis and M. L. Overton. Behavior of BFGS with an exact line search on nonsmooth examples. Technical report, Optimization Online, 2008b. URL http://www. optimization-online.org/DB_FILE/2008/12/2173.pdf. Submitted to SIAM J. Optimization. D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(3):503–528, 1989. L. Lukˇan and J. Vlˇ ek. Globally convergent variable metric method for convex nonsmooth uns c constrained minimization. Journal of Optimization Theory and Applications, 102(3):593–613, 1999. F. Maes, L. Denoyer, and P. Gallinari. XML structure mapping application to the PASCAL/INEX 2006 XML document mining track. In Advances in XML Information Retrieval and Evaluation: Fifth Workshop of the INitiative for the Evaluation of XML Retrieval (INEX’06), Dagstuhl, Germany, 2007. A. Nedi´ and D. P. Bertsekas. Convergence rate of incremental subgradient algorithms. In S. Uryac sev and P. M. Pardalos, editors, Stochastic Optimization: Algorithms and Applications, pages 263–304. Kluwer Academic Publishers, 2000. A. Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM J. on Optimization, 15(1):229–251, 2005. ISSN 1052-6234. Y. Nesterov. Smooth minimization of non-smooth functions. Math. Program., 103(1):127–152, 2005. 1199  ¨ Y U , V ISHWANATHAN , G UNTER AND S CHRAUDOLPH  J. Nocedal and S. J. Wright. Numerical Optimization. Springer Series in Operations Research. Springer, 1999. S. Shalev-Shwartz and Y. Singer. On the equivalence of weak learnability and linear separability: New relaxations and efﬁcient boosting algorithms. In Proceedings of COLT, 2008. A. J. Smola, S. V. N. Vishwanathan, and Q. V. Le. Bundle methods for machine learning. In D. Koller and Y. Singer, editors, Advances in Neural Information Processing Systems 20, Cambridge MA, 2007. MIT Press. B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In S. Thrun, L. Saul, and B. Sch¨ lkopf, editors, Advances in Neural Information Processing Systems 16, pages 25–32, o Cambridge, MA, 2004. MIT Press. C.-H. Teo, S. V. N. Vishwanthan, A. J. Smola, and Q. V. Le. Bundle methods for regularized risk minimization. Journal of Machine Learning Research, 11:311–365, 2010. I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453–1484, 2005. M. K. Warmuth, K. A. Glocer, and S. V. N. Vishwanathan. Entropy regularized LPBoost. In Y. Freund, Y. L` szl` Gy¨ rﬁ, and G. Tur` n, editors, Proc. Intl. Conf. Algorithmic Learning Theory, a o o a number 5254 in Lecture Notes in Artiﬁcial Intelligence, pages 256 – 271, Budapest, October 2008. Springer-Verlag. P. Wolfe. Convergence conditions for ascent methods. SIAM Review, 11(2):226–235, 1969. P. Wolfe. A method of conjugate subgradients for minimizing nondifferentiable functions. Mathematical Programming Study, 3:145–173, 1975. J. Yu, S. V. N. Vishwanathan, S. G¨ nter, and N. N. Schraudolph. A quasi-Newton approach to u nonsmooth convex optimization. In A. McCallum and S. Roweis, editors, ICML, pages 1216– 1223. Omnipress, 2008. T. Zhang and F. J. Oles. Text categorization based on regularized linear classiﬁcation methods. Information Retrieval, 4:5–31, 2001.  1200</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
