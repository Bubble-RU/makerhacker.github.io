<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 jmlr-2010-Expectation Truncation and the Benefits of Preselection In Training Generative Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-38" href="../jmlr2010/jmlr-2010-Expectation_Truncation_and_the_Benefits_of_Preselection_In_Training_Generative_Models.html">jmlr2010-38</a> <a title="jmlr-2010-38-reference" href="#">jmlr2010-38-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>38 jmlr-2010-Expectation Truncation and the Benefits of Preselection In Training Generative Models</h1>
<br/><p>Source: <a title="jmlr-2010-38-pdf" href="http://jmlr.org/papers/volume11/lucke10a/lucke10a.pdf">pdf</a></p><p>Author: Jörg Lücke, Julian Eggert</p><p>Abstract: We show how a preselection of hidden variables can be used to efﬁciently train generative models with binary hidden variables. The approach is based on Expectation Maximization (EM) and uses an efﬁciently computable approximation to the sufﬁcient statistics of a given model. The computational cost to compute the sufﬁcient statistics is strongly reduced by selecting, for each data point, the relevant hidden causes. The approximation is applicable to a wide range of generative models and provides an interpretation of the beneﬁts of preselection in terms of a variational EM approximation. To empirically show that the method maximizes the data likelihood, it is applied to different types of generative models including: a version of non-negative matrix factorization (NMF), a model for non-linear component extraction (MCA), and a linear generative model similar to sparse coding. The derived algorithms are applied to both artiﬁcial and realistic data, and are compared to other models in the literature. We ﬁnd that the training scheme can reduce computational costs by orders of magnitude and allows for a reliable extraction of hidden causes. Keywords: maximum likelihood, deterministic approximations, variational EM, generative models, component extraction, multiple-cause models</p><br/>
<h2>reference text</h2><p>P. Berkes, R. E. Turner, and M. Sahani. A structured model of video reproduces primary visual cortical organisation. PLoS Computational Biology, 5(9):e1000495, 2009. C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. N. J. Butko and J. Triesch. Learning sensory representations with intrinsic plasticity. Neurocomputing, 70(7-9):1130–1138, 2007. D. Charles, C. Fyfe, D. MacDonald, and J. Koetsier. Unsupervised neural networks for the identiﬁcation of minimum overcomplete basis in visual data. Neurocomputing, 47(1-4):119–143, 2002. P. Comon. Independent component analysis, a new concept? Signal Processing, 36(3):287–314, 1994. P. Dayan and R. S. Zemel. Competition and multiple cause models. Neural Computation, 7:565 – 579, 1995. A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm (with discussion). Journal of the Royal Statistical Society B, 39:1–38, 1977. P. F¨ ldi´ k. Forming sparse representations by local anti-Hebbian learning. Biological Cybernetics, o a 64:165 – 170, 1990. M. Fromer and A. Globerson. An LP View of the M-best MAP problem. In Advances in Neural Information Processing Systems 22, pages 567–575, 2009. G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal. The ‘wake-sleep’ algorithm for unsupervised neural networks. Science, 268:1158 – 1161, 1995. S. Hochreiter and J. Schmidhuber. Feature extraction through LOCOCODE. Neural Computation, 11:679 – 714, 1999. P. O. Hoyer. Non-negative sparse coding. In Neural Networks for Signal Processing XII: Proceedings of the IEEE Workshop on Neural Networks for Signal Processing, pages 557 – 565. IEEE, 2002. A. Hyv¨ rinen, J. Hurri, and P. O. Hoyer. Natural Image Statistics. Springer, Heidelberg London a New York, 2009. T. Jaakkola. Tutorial on variational approximation methods. In M. Opper and D. Saad, editors, Advanced mean ﬁeld methods: theory and practice. MIT Press, 2000. M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. Machine Learning, 37(2):183–233, 1999. 2898  E XPECTATION T RUNCATION  E. K¨ rner, M. O. Gewaltig, U. K¨ rner, A. Richter, and T. Rodemann. A model of computation in o o neocortical architecture. Neural Networks, 12:989 – 1005, 1999. U. K¨ ster and A. Hyv¨ rinen. A two-layer ICA-like model estimated by score matching. In Proc. o a International Conference on Artiﬁcial Neural Networks, LNCS 4669, pages 798–807. Springer, 2007. T. W. Lam and H.-F. Ting. Selecting the k largest elements with parity tests. Discrete Appl. Math., 101(1-3):187–196, 2000. D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755):788–91, 1999. D. D. Lee and H. S. Seung. Algorithm for non-negative matrix factorization. In Advances in Neural Information Processing Systems, volume 13, 2001. T. S. Lee and D. Mumford. Hierarchical Bayesian inference in the visual cortex. J Opt Soc Am A Opt Image Sci Vis, 20(7):1434–1448, 2003. J. L¨ cke. Hierarchical self-organization of minicolumnar receptive ﬁelds. Neural Networks, 17/8–9: u 1377 – 1389, 2004. J. L¨ cke and M. Sahani. Generalized softmax networks for non-linear component extraction. In u Proc. International Conference on Artiﬁcial Neural Networks, LNCS 4668, pages 657 – 667. Springer, 2007. J. L¨ cke and M. Sahani. Maximal causes for non-linear component extraction. Journal of Machine u Learning Research, 9:1227 – 1267, 2008. J. L¨ cke and C. von der Malsburg. Rapid processing and unsupervised learning in a model of the u cortical macrocolumn. Neural Computation, 16:501 – 533, 2004. J. L¨ cke, R. Turner, M. Sahani, and M. Henniges. Occlusive components analysis. In Y. Benu gio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 1069–1077, 2009. D. J. C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University Press, 2003. Available from http://www.inference.phy.cam.ac.uk/mackay/itila/. T. P. Minka. Expectation propagation for approximate Bayesian inference. In UAI ’01: Proceedings of the 17th Conference in Uncertainty in Artiﬁcial Intelligence, pages 362–369, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc. ISBN 1-55860-800-1. R. Neal and G. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models. Kluwer, 1998. B. A. Olshausen. Probabilistic Models of the Brain: Perception and Neural Function, chapter Sparse Codes and Spikes, pages 257 – 272. MIT Press, 2002. B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381:607 – 609, 1996. 2899  ¨ L UCKE AND E GGERT  M. Riesenhuber and T. Poggio. Hierarchical models of object recognition in cortex. Nature Neuroscience, 211(11):1019 – 1025, 1999. M. Sahani. Latent variable models for neural data analysis, 1999. PhD Thesis, Caltech. E. Saund. A multiple cause mixture model for unsupervised learning. Neural Computation, 7:51 – 71, 1995. M. W. Spratling. Learning image components for object recognition. Journal of Machine Learning Research, 7:793 – 815, 2006. Y. W. Teh, M. Welling, S. Osindero, and G. E. Hinton. Energy-based models for sparse overcomplete representations. The Journal of Machine Learning Research, 4(7), 2003. N. Ueda and R. Nakano. Deterministic annealing EM algorithm. Neural Networks, 11(2):271–282, 1998. R. van Rullen and S. J. Thorpe. Rate coding versus temporal order coding: What the retinal ganglion cells tell the visual cortex. Neural Computation, 13(6):1255–1283, 2001. G. Westphal and R. P. W¨ rtz. Combining feature- and correspondence-based methods for visual u object recognition. Neural Computation, 21(7):1952–1989, 2009. A. Yuille and D. Kersten. Vision as Bayesian inference: analysis by synthesis? Trends in Cognitive Sciences, 10(7):301–308, 2006.  2900</p>
<br/>
<br/><br/><br/></body>
</html>
