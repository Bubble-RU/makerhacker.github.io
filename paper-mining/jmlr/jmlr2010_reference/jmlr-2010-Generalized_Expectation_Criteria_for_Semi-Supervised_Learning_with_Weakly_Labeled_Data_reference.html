<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-42" href="../jmlr2010/jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">jmlr2010-42</a> <a title="jmlr-2010-42-reference" href="#">jmlr2010-42-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</h1>
<br/><p>Source: <a title="jmlr-2010-42-pdf" href="http://jmlr.org/papers/volume11/mann10a/mann10a.pdf">pdf</a></p><p>Author: Gideon S. Mann, Andrew McCallum</p><p>Abstract: In this paper, we present an overview of generalized expectation criteria (GE), a simple, robust, scalable method for semi-supervised training using weakly-labeled data. GE ﬁts model parameters by favoring models that match certain expectation constraints, such as marginal label distributions, on the unlabeled data. This paper shows how to apply generalized expectation criteria to two classes of parametric models: maximum entropy models and conditional random ﬁelds. Experimental results demonstrate accuracy improvements over supervised training and a number of other stateof-the-art semi-supervised learning methods for these models. Keywords: generalized expectation criteria, semi-supervised learning, logistic regression, conditional random ﬁelds</p><br/>
<h2>reference text</h2><p>S. Abney. Understanding the yarowsky algorithm. Computational Linguistics, 30:3, 2004. Y. Altun, D. McAllester, and M. Belkin. Maximum margin semi-supervised learning for structured variables. In NIPS, 2005. R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. JMLR, 6, 2005. S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik, S. Kumar, D. Ravichandran, and M. Aly. Video suggestion and discovery for youtube: Taking random walks through the view graph. In WWW, 2008. K. Bellare, G. Druck, and A. McCallum. Alternating projections for learning with expectation constraints. In UAI, 2009. Y. Bengio, O. Dellalleau, and N. Le Roux. Label propagation and quadratic criterion. In O. Chapelle, B. Schlkopf, and A. Zien, editors, Semi-Supervised Learning. MIT Press, 2006. A. L. Berger, V. J. Della Pietra, and S. A. Della Pietra. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1), 1996. 980  G ENERALIZED E XPECTATION C RITERIA  A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincut. In ICML, 2001. A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In COLT, 1998. P. F. Brown, V. J. D. Pietra, P. V. DeSouza, J. C. Lai, and R. L. Mercer. Class-based n-gram models of natural language. Computational Linguistics, pages 467–479, 1992. C.J.C Burges and J.C. Platt. Semi-supervised learning with conditional harmonic mixing. In O. Chapelle, B. Schlkopf, and A. Zien, editors, Semi-Supervised Learning. MIT Press, 2006. M.-W. Chang, L. Ratinov, and D. Roth. Guiding semi-supervision with constraint-driven learning. In ACL, 2007. O. Chapelle, B. Scholkopf, and A. Zien. Analysis of Benchmarks. In O. Chapelle, A. Zien, and B. Scholkopf, editors, Semi-Supervised Learning. MIT Press, 2006. M. Chen, I-H. Lee, G. Wu, Y. Wu, and E. Chang. Manifold learning, a promised land or work in progress? In IEEE/International Conference on Multimedia and Expo, 2005. A. Corduneanu and T. Jaakkola. On information regularization. In UAI, 2003. F. Cozman and I. Cohen. Risks of Semi-Supervised Learning. In O. Chapelle, A. Zien, and B. Scholkopf, editors, Semi-Supervised Learning. MIT Press, 2006. O. Delalleau, Y. Bengio, and N. Le Roux. Large-scale algorithms. In Olivier Chapelle, Bernhard Sch¨ lkopf, and Alexander Zien, editors, Semi-Supervised Learning, 2006. o A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. J. Royal Stat. Soc., 39:1–38, 1977. G. Druck, C. Pal, X. Zhu, and A. McCallum. Semi-supervised classiﬁcation with hybrid generative discriminative methods. In KDD, 2007. G. Druck, G. S. Mann, and A. McCallum. Learning from labeled features using generalized expectation criteria. In SIGIR, 2008. G. Druck, G. Mann, and A. McCallum. Semi-supervised learning of dependency parsers using generalized expectation criteria. In ACL/IJCNLP, 2009a. G. Druck, B. Settles, and A. McCallum. Active learning by labeling features. In EMNLP, 2009b. D. Freitag. Trained named entity recognition using distributional clusters. In EMNLP, 2004. K. Ganchev, K. Crammer, F. Pereira, G. Mann, A. McCallum, S. Carroll, Y. Jin, and P. White. Penn/umass/chop biocreativeii systems. In BioCreativeII, 2007. K. Ganchev, J. Gillenwater, and B. Taskar. Dependency grammar induction via bitext projection constraints. In ACL/IJCNLP, 2009. J. Graca, K. Ganchev, and B. Taskar. Expectation maximization and posterior constraints. In NIPS, 2008. 981  M ANN AND M C C ALLUM  Y. Grandvalet and Y. Bengio. Semi-supervised learning by entropy minimization. In NIPS, 2004. T. Grenager, D. Klein, and C. Manning. Unsupervised learning of ﬁeld segmentation models for information extraction. In ACL, 2005. A. Haghighi and D. Klein. Prototype-driven grammar induction. In COLING-ACL, 2006a. A. Haghighi and D. Klein. Prototype-driven learning for sequence models. In NAACL, 2006b. G. Ifrim and G. Weikum. Transductive learning for text classiﬁcation using explicit knowledge models. In PKDD, 2006. F. Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schuurmans. Semi-supervised conditional random ﬁelds for improved sequence segmentation and labeling. In COLING/ACL, 2006. R. Jin and Y. Liu. A framework for incorporating class priors into discriminative classiﬁcation. In PAKDD, 2005. T. Joachims. Transductive inference for text classiﬁcation using support vector machines. In ICML, 1999. URL citeseer.ist.psu.edu/joachims99transductive.html. S. Kakade, Y-W. Teg, and S.Roweis. An alternate objective function for markovian ﬁelds. In ICML, 2002. D. Klein and C. Manning. Corpus-based induction of syntactic structure: Models of dependency and constituency. In ACL, 2004. M. Kockelkorn, A. Luneburg, and T. Scheffer. Using transduction and multi-view learning to answer emails. In PKDD, 2003. M. Krogel and T. Scheffer. Multi-relational learning, text mining, and semi-supervised learing for functional genomics. Machine Learning, 57, 2004. J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In ICML, 2001. J. Lafferty, Y. Liu, and X. Zhu. Kernel conditional random ﬁelds:repsentation, clique selection, and semi-supervised learning. In ICML, 2004. W. Li and A. McCallum. A note on semi-supervised learning using markov random ﬁelds. Computer science technical note, University of Massachusetts, Amherst, MA, 2004. W. Li and A. McCallum. Semi-supervised sequence modeling with syntactic topic models. In AAAI, 2005. P. Liang, M. Jordan, and D. Klein. Lerning from measurements in exponential families. In ICML, 2009. S. Macskassy and F. Provost. Classiﬁcation in networked data. Technical Report CeDER-04-08, New York University, 2006. 982  G ENERALIZED E XPECTATION C RITERIA  R. Malouf. A comparison of algorithms for maximum enotrpy parameter estimation. In COLING, 2002. G. Mann and A. McCallum. Simple, robust, scalable semi-supervised learning via expectation regularization. In ICML, 2007. G. Mann and A. McCallum. Generalized expectation criteria for semi-supervised learning of conditional random ﬁelds. In ACL, 2008. A. McCallum, K. Bellare, and F. Pereira. A conditional random ﬁeld for discriminatively-trained ﬁnite-state string edit distance. In UAI, 2005. P. Merialdo. Tagging english text with a probabilistic model. Computational Linguistics, 1994. S. Miller, J. Guinness, and A. Zamanian. Name tagging with word clusters and discriminative training. In ACL, 2004. V. Ng and C. Cardie. Weakly supervised natural language learning without redundant views. In HLT-NAACL, 2003. K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Learning to classify text from labeled and unlabeled documents. In AAAI, 1998. K. Nigam, A. McCallum, and T. Mitchell. Semi-supervised Text Classiﬁcation Using EM. In O. Chapelle, A. Zien, and B. Scholkopf, editors, Semi-Supervised Learning. MIT Press, 2006. Z.-Y. Niu, D.-H. Ji, and C. L. Tam. Word sense disambiguation using label propagation based semi-supervised learning. In ACL, 2005. N. Quadrianto, A. J. Smola, T.S. Caetano, and Q.V. Le. Estimating labels from label proportions. In ICML, 2008. E. Riloff and J. Shepherd. A Corpus-based Bootstrapping Algorithm for Semi-Automated Semantic Lexicon Construction. Journal for Natural Language Engineering, 2000. forthcoming. R. Salakhutdinov, S. Roweis, and Z. Ghahramani. Optimization with em and expectation-conjugategradient. In ICML, 2003. R. Schapire, M. Rochery, M. Rahim, and N. Gupta. Incorporating prior knowledge into boosting. In ICML, 2002. D. Schuurmans. A new metric-based approach to model selection. In AAAI, 1997. V. Sindhwani and S. S. Keerthi. Large scale semi-supervised linear svms. In SIGIR, 2006. N. Smith and J. Eisner. Contrastive estimation: Training log-linear models on unlabeled data. In ACL, 2005. J. Suzuki and H. Isozaki. Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. In ACL, 2008. 983  M ANN AND M C C ALLUM  J. Suzuki, A. Fujino, and H. Isozaki. Semi-supervised structured output learning based on a hybrid generative and discriminative approach. In EMNLP-CoNLL, 2007. Martin Szummer and Tommi Jaakkola. Partially labeled classiﬁcation with markov random walks. In NIPS, volume 14, 2002. L. Wang, P. Xue, and K. Chan. Incorporating prior knowledge into svm for image retrieval. In ICPR, 2004. S. Wang, R. Rosenfeld, Y. Zhao, and D. Schuurmans. The latent maximum entropy principle. In IEEE ISIT, 2002. J. Weston, C. Leslie, E. Ie, and W. S. Noble. Semi-supervised protein classiﬁcation using cluster kernels. In Olivier Chapelle, Bernhard Sch¨ lkopf, and Alexander Zien, editors, Semi-Supervised o Learning, 2006. D. Yarowsky. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proceedings of ACL, 1995. T. Zhang and F.J. Oles. A probability analysis on the value of unlabeled data for classiﬁcation problems. In ICML, 2000. X. Zhu and Z. Ghahramani. Learning from labeled and unlabeled data with label propagation. Technical Report CMU-CALD-02-107, CMU, 2002. X. Zhu and J. Lafferty. Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning. In ICML, 2005. X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic mixtures. In ICML, 2003.  984</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
