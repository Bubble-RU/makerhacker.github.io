<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-68" href="../jmlr2010/jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_II%3A_Analysis_and_Extensions.html">jmlr2010-68</a> <a title="jmlr-2010-68-reference" href="#">jmlr2010-68-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>68 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions</h1>
<br/><p>Source: <a title="jmlr-2010-68-pdf" href="http://jmlr.org/papers/volume11/aliferis10b/aliferis10b.pdf">pdf</a></p><p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: In part I of this work we introduced and evaluated the Generalized Local Learning (GLL) framework for producing local causal and Markov blanket induction algorithms. In the present second part we analyze the behavior of GLL algorithms and provide extensions to the core methods. SpeciÃ„?Ä¹Å¡ cally, we investigate the empirical convergence of GLL to the true local neighborhood as a function of sample size. Moreover, we study how predictivity improves with increasing sample size. Then we investigate how sensitive are the algorithms to multiple statistical testing, especially in the presence of many irrelevant features. Next we discuss the role of the algorithm parameters and also show that Markov blanket and causal graph concepts can be used to understand deviations from optimality of state-of-the-art non-causal algorithms. The present paper also introduces the following extensions to the core GLL framework: parallel and distributed versions of GLL algorithms, versions with false discovery rate control, strategies for constructing novel heuristics for speciÃ„?Ä¹Å¡ c domains, and divide-and-conquer local-to-global learning (LGL) strategies. We test the generality of the LGL approach by deriving a novel LGL-based algorithm that compares favorably c 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS to the state-of-the-art global learning algorithms. In addition, we investigate the use of non-causal feature selection methods to facilitate global learning. Open problems and future research paths related to local and local-to-global causal learning are discussed. Keywords: local causal discovery, Markov blanket induction, feature selection, classiÃ„?Ä¹Å¡ cation, causal structure learning, learning of Bayesian networks</p><br/>
<h2>reference text</h2><p>C. F. Aliferis and G. F. Cooper. Aspects of modeling with mtbnÄ‚Ë˜&euro;&trade;s. Technical report CBMI 1998-3, Center for Biomedical Informatics, University of Pittsburgh, 1998. C. F. Aliferis and A. Statnikov. Dynamic ordering-based global learning. Technical report DSL-0802, 2008. 281  A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS  ;  < 7  37 _ ; < 7  7  3; ;  ;   ;  <   ;  <   ;  <   ;  <               3< <  <         Figure 20: In this example, T = XOR(X,Y ). The priors of X and Y are given in the table. Both X and Y have very strong univariate association with T despite being XOR parents and in the absence of connectivity. C. F. Aliferis, I. Tsamardinos, and A. Statnikov. Large-scale feature selection using markov blanket induction for the prediction of protein-drug binding. Technical Report DSL 02-06, 2002. C. F. Aliferis, A. Statnikov, I. Tsamardinos, S. Mani, and X. D. Koutsoukos. Local causal and markov blanket induction for causal discovery and feature selection for classiÃ„?Ä¹Å¡ cation. part i: Algorithms and empirical evaluation. Journal of Machine Learning Research, 11:171Ä‚Ë˜&euro;&ldquo;234, 2010. Y. Benjamini and Y. Hochberg. Controlling the false discovery rate: A practical and powerful approach to multiple testing. Journal of the Royal Statistical Society.Series B (Methodological), 57(1):289Ä‚Ë˜&euro;&ldquo;300, 1995. Y. Benjamini and D. Yekutieli. The control of the false discovery rate in multiple testing under dependency. Ann.Statist, 29(4):1165Ä‚Ë˜&euro;&ldquo;1188, 2001. A. Bhattacharjee, W. G. Richards, J. Staunton, C. Li, S. Monti, P. Vasa, C. Ladd, J. Beheshti, R. Bueno, M. Gillette, M. Loda, G. Weber, E. J. Mark, E. S. Lander, W. Wong, B. E. Johnson, T. R. Golub, D. J. Sugarbaker, and M. Meyerson. ClassiÃ„?Ä¹Å¡ cation of human lung carcinomas by mrna expression proÃ„?Ä¹Å¡ ling reveals distinct adenocarcinoma subclasses. Proc.Natl.Acad.Sci.U.S.A, 98(24):13790Ä‚Ë˜&euro;&ldquo;13795, Nov 2001. L. E. Brown, I. Tsamardinos, and C. F. Aliferis. A comparison of novel and state-of-the-art polynomial bayesian network learning algorithms. Proceedings of the Twentieth National Conference on ArtiÃ„?Ä¹Å¡ cial Intelligence (AAAI), 2005. G. Casella and R. L. Berger. Statistical Inference. Thomson Learning, Australia, 2nd edition, 2002. N. Friedman, I. Nachman, and D. PeÄ‚Ë˜&euro;&trade;er. Learning bayesian network structure from massive datasets: the Ä‚Ë˜&euro;&oelig;sparse candidateÄ‚Ë˜&euro;? algorithm. Proceedings of the Fifteenth Conference on Uncertainty in ArtiÃ„?Ä¹Å¡ cial Intelligence (UAI), 1999. L. D. Fu. A comparison of state-of-the-art algorithms for learning bayesian network structure from continuous data. MasterÄ‚Ë˜&euro;&trade;s thesis, Vanderbilt University, 2005. I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for cancer classiÃ„?Ä¹Å¡ cation using support vector machines. Machine Learning, 46(1):389Ä‚Ë˜&euro;&ldquo;422, 2002. 282  L OCAL C AUSAL AND M ARKOV B LANKET I NDUCTION PART II  I. Guyon, S. Gunn, M. Nikravesh, and L. A. Zadeh. Feature Extraction: Foundations and Applications. Springer-Verlag, Berlin, 2006. D. Hardin, I. Tsamardinos, and C. F. Aliferis. A theoretical characterization of linear svm-based feature selection. Proceedings of the Twenty First International Conference on Machine Learning (ICML), 2004. I. S. Kohane, A. T. Kho, and A. J. Butte. Microarrays for an Integrative Genomics. MIT Press, Cambridge, Mass, 2003. R. Kohavi and G. H. John. Wrappers for feature subset selection. ArtiÃ„?Ä¹Å¡ cial Intelligence, 97(1-2): 273Ä‚Ë˜&euro;&ldquo;324, 1997. R. E. Lenski, C. Ofria, R. T. Pennock, and C. Adami. The evolutionary origin of complex features. Nature, 423(6936):139Ä‚Ë˜&euro;&ldquo;144, May 2003. D. Margaritis and S. Thrun. Bayesian network induction via local neighborhoods. Advances in Neural Information Processing Systems, 12:505Ä‚Ë˜&euro;&ldquo;511, 1999. C. Meek. Strong completeness and faithfulness in bayesian networks. Proceedings of the Eleventh Conference on Uncertainty in ArtiÃ„?Ä¹Å¡ cial Intelligence (UAI), pages 411Ä‚Ë˜&euro;&ldquo;418, 1995. J. Ramsey, J. Zhang, and P. Spirtes. Adjacency-faithfulness and conservative causal inference. Proceedings of the 22nd Annual Conference on Uncertainty in ArtiÃ„?Ä¹Å¡ cial Intelligence (UAI-06), 2006. M. Schmidt, A. Niculescu-Mizil, and K. Murphy. Learning graphical model structure using l1regularization paths. Proceedings of the Twenty-Second National Conference on ArtiÃ„?Ä¹Å¡ cial Intelligence (AAAI), 2007. O. J. Sharpe. Towards a Rational Methodology for Using Evolutionary Search Algorithms. PhD thesis, University of Sussex, 2000. P. T. Spellman, G. Sherlock, M. Q. Zhang, V. R. Iyer, K. Anders, M. B. Eisen, P. O. Brown, D. Botstein, and B. Futcher. Comprehensive identiÃ„?Ä¹Å¡ cation of cell cycle-regulated genes of the yeast saccharomyces cerevisiae by microarray hybridization. Mol.Biol Cell, 9(12):3273Ä‚Ë˜&euro;&ldquo;3297, Dec 1998. P. Spirtes, C. N. Glymour, and R. Scheines. Causation, Prediction, and Search, volume 2nd. MIT Press, Cambridge, Mass, 2000. A. Statnikov. Algorithms for discovery of multiple markov boundaries: Application to the molecular signature multiplicity problem. Ph.D.Thesis, Department of Biomedical Informatics, Vanderbilt University, 2008. A. Statnikov, D. Hardin, and C. F. Aliferis. Using svm weight-based methods to identify causally relevant and non-causally relevant variables. Proceedings of the NIPS 2006 Workshop on Causality and Feature Selection, 2006. 283  A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS  A. Statnikov, J. Feig, E. Fisher, and C.F. Aliferis. Novel bioinformatics methods for discovery of complex molecular signatures, pathways, and biomarkers in very small sample situations. Submitted, 2010. I. Tsamardinos and C. F. Aliferis. Towards principled feature selection: relevancy, Ã„?Ä¹Å¡ lters and wrappers. Proceedings of the Ninth International Workshop on ArtiÃ„?Ä¹Å¡ cial Intelligence and Statistics (AI & Stats), 2003. I. Tsamardinos, C. F. Aliferis, and A. Statnikov. Algorithms for large scale markov blanket discovery. Proceedings of the Sixteenth International Florida ArtiÃ„?Ä¹Å¡ cial Intelligence Research Society Conference (FLAIRS), pages 376Ä‚Ë˜&euro;&ldquo;381, 2003a. I. Tsamardinos, C. F. Aliferis, and A. Statnikov. Time and sample efÃ„?Ä¹Å¡ cient discovery of markov blankets and direct causal relations. Proceedings of the Ninth International Conference on Knowledge Discovery and Data Mining (KDD), pages 673Ä‚Ë˜&euro;&ldquo;678, 2003b. I. Tsamardinos, L. E. Brown, and C. F. Aliferis. The max-min hill-climbing bayesian network structure learning algorithm. Machine Learning, 65(1):31Ä‚Ë˜&euro;&ldquo;78, 2006.  284</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
