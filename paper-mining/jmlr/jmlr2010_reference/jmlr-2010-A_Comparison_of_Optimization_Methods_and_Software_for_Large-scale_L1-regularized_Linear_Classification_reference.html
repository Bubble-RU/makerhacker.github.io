<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 jmlr-2010-A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-1" href="../jmlr2010/jmlr-2010-A_Comparison_of_Optimization_Methods_and_Software_for_Large-scale_L1-regularized_Linear_Classification.html">jmlr2010-1</a> <a title="jmlr-2010-1-reference" href="#">jmlr2010-1-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 jmlr-2010-A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</h1>
<br/><p>Source: <a title="jmlr-2010-1-pdf" href="http://jmlr.org/papers/volume11/yuan10c/yuan10c.pdf">pdf</a></p><p>Author: Guo-Xun Yuan, Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Large-scale linear classiﬁcation is widely used in many areas. The L1-regularized form can be applied for feature selection; however, its non-differentiability causes more difﬁculties in training. Although various optimization methods have been proposed in recent years, these have not yet been compared suitably. In this paper, we ﬁrst broadly review existing methods. Then, we discuss state-of-the-art software packages in detail and propose two efﬁcient implementations. Extensive comparisons indicate that carefully implemented coordinate descent methods are very suitable for training large document data. Keywords: L1 regularization, linear classiﬁcation, optimization methods, logistic regression, support vector machines, document classiﬁcation</p><br/>
<h2>reference text</h2><p>Galen Andrew and Jianfeng Gao. Scalable training of L1-regularized log-linear models. In Proceedings of the Twenty Fourth International Conference on Machine Learning (ICML), 2007. Suhrid Balakrishnan and David Madigan. Algorithms for sparse linear classiﬁers in the massive data setting. 2005. URL http://www.stat.rutgers.edu/˜madigan/PAPERS/sm.pdf. Steven Benson and Jorge J. Mor´ . A limited memory variable metric method for bound constrained e minimization. Preprint MCS-P909-0901, Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Illinois, 2001. Paul S. Bradley and Olvi L. Mangasarian. Feature selection via concave minimization and support vector machines. In Proceedings of the Twenty Fifth International Conference on Machine Learning (ICML), 1998. Richard H. Byrd, Peihung Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientiﬁc Computing, 16:1190–1208, 1995. Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm. 3229  Y UAN , C HANG , H SIEH AND L IN  Kai-Wei Chang, Cho-Jui Hsieh, and Chih-Jen Lin. Coordinate descent method for large-scale L2loss linear SVM. Journal of Machine Learning Research, 9:1369–1398, 2008. URL http: //www.csie.ntu.edu.tw/˜cjlin/papers/cdl2.pdf. David L. Donoho and Yaakov Tsaig. Fast solution of l1 minimization problems when the solution may be sparse. IEEE Transactions on Information Theory, 54:4789–4812, 2008. John Duchi and Yoram Singer. Boosting with structural sparsity. In Proceedings of the Twenty Sixth International Conference on Machine Learning (ICML), 2009. John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efﬁcient projections onto the L1-ball for learning in high dimensions. In Proceedings of the Twenty Fifth International Conference on Machine Learning (ICML), 2008. Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. Annals of Statistics, 32:407–499, 2004. Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. LIBLINEAR: A library for large linear classiﬁcation. Journal of Machine Learning Research, 9:1871–1874, 2008. URL http://www.csie.ntu.edu.tw/˜cjlin/papers/liblinear.pdf. Mario Figueiredo, Robert Nowak, and Stephen Wright. Gradient projection for sparse reconstruction: Applications to compressed sensing and other inverse problems. IEEE Journal on Selected Topics in Signal Processing, 1:586–598, 2007. M´ rio A. T. Figueiredo. Adaptive sparseness for supervised learning. IEEE Transactions on Pattern a Analysis and Machine Intelligence, 25:1150–1159, 2003. Jerome Friedman, Trevor Hastie, Holger H¨ ﬂing, and Robert Tibshirani. Pathwise coordinate optio mization. Annals of Applied Statistics, 1(2):302–332, 2007. Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1):1–22, 2010. Wenjiang J. Fu. Penalized regressions: The bridge versus the lasso. Journal of Computational and Graphical Statistics, 7(3):397–416, 1998. Glenn M. Fung and Olvi L. Mangasarian. A feature selection Newton method for support vector machine classiﬁcation. Computational optimization and applications, 28:185–202, 2004. Eli M. Gafni and Dimitri P. Bertsekas. Two-metric projection methods for constrained optimization. SIAM Journal on Control and Optimization, 22:936–964, 1984. Alexandar Genkin, David D. Lewis, and David Madigan. Large-scale Bayesian logistic regression for text categorization. Technometrics, 49(3):291–304, 2007. Joshua Goodman. Exponential priors for maximum entropy models. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2004. Elaine T. Hale, Wotao Yin, and Yin Zhang. Fixed-point continuation for l1-minimization: Methodology and convergence. SIAM Journal on Optimization, 19(3):1107–1130, 2008. 3230  C OMPARING M ETHODS AND S OFTWARE FOR L1- REGULARIZED L INEAR C LASSIFICATION  Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and Sellamanickam Sundararajan. A dual coordinate descent method for large-scale linear SVM. In Proceedings of the Twenty Fifth International Conference on Machine Learning (ICML), 2008. URL http://www.csie.ntu. edu.tw/˜cjlin/papers/cddual.pdf. Fang-Lan Huang, Cho-Jui Hsieh, Kai-Wei Chang, and Chih-Jen Lin. Iterative scaling and coordinate descent methods for maximum entropy. Journal of Machine Learning Research, 11:815– 848, 2010. URL http://www.csie.ntu.edu.tw/˜cjlin/papers/maxent_journal.pdf. Thorsten Joachims. Making large-scale SVM learning practical. In Bernhard Sch¨ lkopf, Christoo pher J. C. Burges, and Alexander J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, Cambridge, MA, 1998. MIT Press. Jun’ichi Kazama and Jun’ichi Tsujii. Evaluation and extension of maximum entropy models with inequality constraints. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 137–144, 2003. S. Sathiya Keerthi and Shrish Shevade. A fast tracking algorithm for generalized LARS/LASSO. IEEE Transactions on Neural Networks, 18(6):1826–1830, 2007. Jinseog Kim, Yuwon Kim, and Yongdai Kim. A gradient-based optimization algorithm for LASSO. Journal of Computational and Graphical Statistics, 17(4):994–1009, 2008. Seung-Jean Kim, Kwangmoo Koh, Michael Lustig, Stephen Boyd, and Dimitry Gorinevsky. An interior point method for large-scale l1-regularized least squares. IEEE Journal on Selected Topics in Signal Processing, 1:606–617, 2007. Yongdai Kim and Jinseog Kim. Gradient LASSO for feature selection. In Proceedings of the 21st International Conference on Machine Learning (ICML), 2004. Jyrki Kivinen and Manfred K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132:1–63, 1997. Kwangmoo Koh, Seung-Jean Kim, and Stephen Boyd. An interior-point method for large-scale l1-regularized logistic regression. Journal of Machine Learning Research, 8:1519–1555, 2007. URL http://www.stanford.edu/˜boyd/l1_logistic_reg.html. Balaji Krishnapuram, Alexander J. Hartemink, Lawrence Carin, and Mario A. T. Figueiredo. A Bayesian approach to joint feature selection and classiﬁer design. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(6):1105–1111, 2004. Balaji Krishnapuram, Lawrence Carin, Mario A. T. Figueiredo, and Alexander J. Hartemink. Sparse multinomial logistic regression: fast algorithms and generalization bounds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(6):957–968, 2005. Jussi Kujala, Timo Aho, and Tapio Elomaa. A walk from 2-norm SVM to 1-norm SVM. Preprint available from http://www.cs.tut.fi/˜kujala2/papers/1norm2norm.pdf, 2009. John Langford, Lihong Li, and Tong Zhang. Sparse online learning via truncated gradient. Journal of Machine Learning Research, 10:771–801, 2009. 3231  Y UAN , C HANG , H SIEH AND L IN  Cheng-Yu Lee. A comparison of optimization methods for large-scale l1-regularized logistic regression. Master’s thesis, Department of Computer Science and Information Engineering, National Taiwan University, 2008. Su-In Lee, Honglak Lee, Pieter Abbeel, and Andrew Y. Ng. Efﬁcient l1 regularized logistic regression. In Proceedings of the Twenty-ﬁrst National Conference on Artiﬁcial Intelligence (AAAI-06), pages 1–9, Boston, MA, USA, July 2006. Chih-Jen Lin and Jorge J. Mor´ . Newton’s method for large-scale bound constrained problems. e SIAM Journal on Optimization, 9:1100–1127, 1999. Chih-Jen Lin, Ruby C. Weng, and S. Sathiya Keerthi. Trust region Newton method for largescale logistic regression. Journal of Machine Learning Research, 9:627–650, 2008. URL http: //www.csie.ntu.edu.tw/˜cjlin/papers/logistic.pdf. Dong C. Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(1):503–528, 1989. Jun Liu and Jieping Ye. Efﬁcient euclidean projections in linear time. In Proceedings of the Twenty Sixth International Conference on Machine Learning (ICML), 2009. Jun Liu, Jianhui Chen, and Jieping Ye. Large-scale sparse logistic regression. In Proceedings of The 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 547–556, 2009. Olvi L. Mangasarian. A ﬁnite Newton method for classiﬁcation. Optimization Methods and Software, 17(5):913–929, 2002. Olvi L. Mangasarian. Exact 1-norm support vector machines via unconstrained convex differentiable minimization. Journal of Machine Learning Research, 7:1517–1530, 2006. Yurii E. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic Publishers, 2003. Michael R. Osborne, Brett Presnell, and Berwin A. Turlach. A new approach to variable selection in least squares problems. IMA Journal of Numerical Analysis, 20:389–404, 2000a. Michael R. Osborne, Brett Presnell, and Berwin A. Turlach. On the LASSO and its dual. Journal of Computational and Graphical Statistics, 9(2):319–337, 2000b. Mee Young Park and Trevor Hastie. L1 regularization path algorithm for generalized linear models. Journal of the Royal Statistical Society Series B, 69:659–677, 2007. Simon Perkins, Kevin Lacker, and James Theiler. Grafting: Fast, incremental feature selection by gradient descent in function space. Journal of Machine Learning Research, 3:1333–1356, 2003. Saharon Rosset. Following curved regularized optimization solution paths. In Lawrence K. Saul, Yair Weiss, and L´ on Bottou, editors, Advances in Neural Information Processing Systems 17, e pages 1153–1160, Cambridge, MA, 2005. MIT Press. Volker Roth. The generalized LASSO. IEEE Transactions on Neural Networks, 15(1):16–28, 2004. 3232  C OMPARING M ETHODS AND S OFTWARE FOR L1- REGULARIZED L INEAR C LASSIFICATION  Sylvain Sardy, Andrew G. Bruce, and Paul Tseng. Block coordinate relaxation methods for nonparametric signal denoising with wavelet dictionaries. Journal of Computational and Graphical Statistics, 9:361–379, 2000. Mark Schmidt, Glenn Fung, and Romer Rosales. Fast optimization methods for l1 regularization: A comparative study and two new approaches. In Proceedings of European Conference on Machine Learning, pages 286–297, 2007. Mark Schmidt, Glenn Fung, and Romer Rosales. Optimization methods for l1-regularization. Technical Report TR-2009-19, University of British Columbia, 2009. Shai Shalev-Shwartz and Ambuj Tewari. Stochastic methods for l1 regularized loss minimization. In Proceedings of the Twenty Sixth International Conference on Machine Learning (ICML), 2009. Shirish Krishnaj Shevade and S. Sathiya Keerthi. A simple and efﬁcient algorithm for gene selection using sparse logistic regression. Bioinformatics, 19(17):2246–2253, 2003. Jianing Shi, Wotao Yin, Stanley Osher, and Paul Sajda. A fast hybrid algorithm for large scale l1-regularized logistic regression. Journal of Machine Learning Research, 11:713–741, 2010. Choon Hui Teo, S.V.N. Vishwanathan, Alex Smola, and Quoc V. Le. Bundle methods for regularized risk minimization. Journal of Machine Learning Research, 11:311–365, 2010. Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B, 58:267–288, 1996. Ryota. Tomioka and Masashi Sugiyama. Dual augmented Lagrangian method for efﬁcient sparse reconstruction. IEEE Signal Processing Letters, 16(12):1067–1070, 2009. Paul Tseng and Sangwoon Yun. A coordinate gradient descent method for nonsmooth separable minimization. Mathematical Programming, 117:387–423, 2007. Stephen J. Wright, Robert D. Nowak, and Mario A.T. Figueiredo. Sparse reconstruction by separable approximation. IEEE Transactions on Signal Processing, 57:2479–2493, 2009. Tong Tong Wu and Kenneth Lange. Coordinate descent algorithms for lasso penalized regression. The Annals of Applied Statistics, 2(1):224–244, 2008. Jin Yu, S.V.N. Vishwanathan, Simon Gunter, and Nicol N. Schraudolph. A quasi-Newton approach to nonsmooth convex optimization problems in machine learning. Journal of Machine Learning Research, 11:1–57, 2010. Sangwoon Yun and Kim-Chuan Toh. A coordinate gradient descent method for l1-regularized convex minimization. 2009. To appear in Computational Optimizations and Applications. Tong Zhang and Frank J. Oles. Text categorization based on regularized linear classiﬁcation methods. Information Retrieval, 4(1):5–31, 2001. Peng Zhao and Bin Yu. On model selection consistency of lasso. Journal of Machine Learning Research, 7:2541–2563, 2006. 3233  Y UAN , C HANG , H SIEH AND L IN  Peng Zhao and Bin Yu. Stagewise lasso. Journal of Machine Learning Research, 8:2701–2726, 2007. Ji Zhu, Saharon Rosset, Trevor Hastie, and Rob Tibshirani. 1-norm support vector machines. In Sebastian Thrun, Lawrence Saul, and Bernhard Sch¨ lkopf, editors, Advances in Neural Information o Processing Systems 16. MIT Press, Cambridge, MA, 2004.  3234</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
