<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-112" href="../jmlr2010/jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">jmlr2010-112</a> <a title="jmlr-2010-112-reference" href="#">jmlr2010-112-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</h1>
<br/><p>Source: <a title="jmlr-2010-112-pdf" href="http://jmlr.org/papers/volume11/chang10a/chang10a.pdf">pdf</a></p><p>Author: Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, Chih-Jen Lin</p><p>Abstract: Kernel techniques have long been used in SVM to handle linearly inseparable problems by transforming data to a high dimensional space, but training and testing large data sets is often time consuming. In contrast, we can efﬁciently train and test much larger data sets using linear SVM without kernels. In this work, we apply fast linear-SVM methods to the explicit form of polynomially mapped data and investigate implementation issues. The approach enjoys fast training and testing, but may sometimes achieve accuracy close to that of using highly nonlinear kernels. Empirical experiments show that the proposed method is useful for certain large-scale data sets. We successfully apply the proposed method to a natural language processing (NLP) application by improving the testing accuracy under some training/testing speed requirements. Keywords: decomposition methods, low-degree polynomial mapping, kernel functions, support vector machines, dependency parsing, natural language processing</p><br/>
<h2>reference text</h2><p>Bernhard E. Boser, Isabelle Guyon, and Vladimir Vapnik. A training algorithm for optimal margin classiﬁers. In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, pages 144–152. ACM Press, 1992. Leon Bottou. Stochastic gradient descent examples, 2007. http://leon.bottou.org/projects/ sgd. Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm. Corina Cortes and Vladimir Vapnik. Support-vector network. Machine Learning, 20:273–297, 1995. Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. LIBLINEAR: A library for large linear classiﬁcation. Journal of Machine Learning Research, 9:1871–1874, 2008. URL http://www.csie.ntu.edu.tw/˜cjlin/papers/liblinear.pdf. Michael Ferris and Todd Munson. Interior point methods for massive support vector machines. SIAM Journal on Optimization, 13(3):783–804, 2003. E. Michael Gertz and Joshua D. Grifﬁn. Support vector machine classiﬁers for large data sets. Technical Report ANL/MCS-TM-289, Argonne National Laboratory, 2005. 1488  T RAINING AND T ESTING L OW- DEGREE P OLYNOMIAL DATA M APPINGS VIA L INEAR SVM  Yoav Goldberg and Michael Elhadad. splitSVM: Fast, space-efﬁcient, non-heuristic, polynomial kernel computation for NLP applications. In Proceedings of the 46st Annual Meeting of the Association of Computational Linguistics (ACL), 2008. Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and Sellamanickam Sundararajan. A dual coordinate descent method for large-scale linear SVM. In Proceedings of the Twenty Fifth International Conference on Machine Learning (ICML), 2008. URL http://www.csie.ntu. edu.tw/˜cjlin/papers/cddual.pdf. Chih-Wei Hsu and Chih-Jen Lin. A comparison of methods for multi-class support vector machines. IEEE Transactions on Neural Networks, 13(2):415–425, 2002. Georgiana Ifrim, G¨ khan Bakır, and Gerhard Weikum. Fast logistic regression for text categoo rization with variable-length n-grams. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 354–362, 2008. Hideki Isozaki and Hideto Kazawa. Efﬁcient support vector classiﬁers for named entity recognition. In Proceedings of COLING, pages 390–396, 2002. Thorsten Joachims. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006. Thorsten Joachims. Making large-scale SVM learning practical. In Bernhard Sch¨ lkopf, Christoo pher J. C. Burges, and Alexander J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, Cambridge, MA, 1998. MIT Press. Jin Hyuk Jung, Dianne P. O’Leary, and Andr´ L. Tits. Adaptive constraint reduction for training e support vector machines. Electronic Transactions on Numerical Analysis, 31:156–177, 2008. S. Sathiya Keerthi and Chih-Jen Lin. Asymptotic behaviors of support vector machines with Gaussian kernel. Neural Computation, 15(7):1667–1689, 2003. S. Sathiya Keerthi, Shirish Krishnaj Shevade, Chiranjib Bhattacharyya, and Karuturi Radha Krishna Murthy. Improvements to Platt’s SMO algorithm for SVM classiﬁer design. Neural Computation, 13:637–649, 2001. S. Sathiya Keerthi, Olivier Chapelle, and Dennis DeCoste. Building support vector machines with reduced classiﬁer complexity. Journal of Machine Learning Research, 7:1493–1515, 2006. Taku Kudo and Yuji Matsumoto. Japanese dependency structure analysis based on support vector machines. In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora, 2000. Taku Kudo and Yuji Matsumoto. Fast methods for kernel-based text analysis. In Proceedings of the 41st Annual Meeting of the Association of Computational Linguistics (ACL), 2003. John Langford, Lihong Li, and Tong Zhang. Sparse online learning via truncated gradient. Journal of Machine Learning Research, 10:771–801, 2009. Yuh-Jye Lee and Olvi L. Mangasarian. RSVM: Reduced support vector machines. In Proceedings of the First SIAM International Conference on Data Mining, 2001. 1489  C HANG , H SIEH , C HANG , R INGGAARD AND L IN  Ross A. Lippert and Ryan M. Rifkin. Inﬁnite-σ limits for Tikhonov regularization. Journal of Machine Learning Research, 7:855–876, 2006. Olvi L. Mangasarian and David R. Musicant. Successive overrelaxation for support vector machines. IEEE Transactions on Neural Networks, 10(5):1032–1037, 1999. Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330, 1993. Ryan McDonald and Joakim Nivre. Characterizing the errors of data-driven dependency parsing models. In Proceedings of the Joint Conferences on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), 2007. Ryan McDonald and Fernando Pereira. Online learning of approximate dependency parsing algorithms. In Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 81–88, 2006. Yvonne Moh and Joachim M. Buhmann. Kernel expansion for online preference tracking. In Proceedings of The International Society for Music Information Retrieval (ISMIR), pages 167– 172, 2008. Joakim Nivre. An efﬁcient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03), 2003. Joakim Nivre, Johan Hall, Jens Nilsson, G¨ lsen Eryiˇ it, and Svetoslav Marinov. Labeled pseudou¸ g projective dependency parsing with support vector machines. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL), pages 221–225, 2006. Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, Gulsen Eryigit, Sandra Kubler, Svetoslav Marinov, and Erwin Marsi. MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135, 2007. Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: primal estimated sub-gradient solver for SVM. In Proceedings of the Twenty Fourth International Conference on Machine Learning (ICML), 2007. Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, and S.V.N. Vishwanathan. Hash kernels for structured data. Journal of Machine Learning Research, 10:2615–2637, 2009. Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. Feature hashing for large scale multitask learning. In Proceedings of the Twenty Sixth International Conference on Machine Learning (ICML), pages 1113–1120, 2009. Hiroyasu Yamada and Yuji Matsumoto. Statistical dependency analysis with support vector machines. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03), 2003. Guo-Xun Yuan, Kai-Wei Chang, Cho-Jui Hsieh, and Chih-Jen Lin. A comparison of optimization methods and software for large-scale l1-regularized linear classiﬁcation. 2009. URL http: //www.csie.ntu.edu.tw/˜cjlin/papers/l1.pdf. Under revision for Journal of Machine Learning Research. 1490</p>
<br/>
<br/><br/><br/></body>
</html>
