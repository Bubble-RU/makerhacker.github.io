<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 jmlr-2010-Learning Gradients: Predictive Models that Infer Geometry and Statistical Dependence</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-62" href="../jmlr2010/jmlr-2010-Learning_Gradients%3A_Predictive_Models_that_Infer_Geometry_and_Statistical_Dependence.html">jmlr2010-62</a> <a title="jmlr-2010-62-reference" href="#">jmlr2010-62-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 jmlr-2010-Learning Gradients: Predictive Models that Infer Geometry and Statistical Dependence</h1>
<br/><p>Source: <a title="jmlr-2010-62-pdf" href="http://jmlr.org/papers/volume11/wu10a/wu10a.pdf">pdf</a></p><p>Author: Qiang Wu, Justin Guinney, Mauro Maggioni, Sayan Mukherjee</p><p>Abstract: The problems of dimension reduction and inference of statistical dependence are addressed by the modeling framework of learning gradients. The models we propose hold for Euclidean spaces as well as the manifold setting. The central quantity in this approach is an estimate of the gradient of the regression or classiﬁcation function. Two quadratic forms are constructed from gradient estimates: the gradient outer product and gradient based diffusion maps. The ﬁrst quantity can be used for supervised dimension reduction on manifolds as well as inference of a graphical model encoding dependencies that are predictive of a response variable. The second quantity can be used for nonlinear projections that incorporate both the geometric structure of the manifold as well as variation of the response variable on the manifold. We relate the gradient outer product to standard statistical quantities such as covariances and provide a simple and precise comparison of a variety of supervised dimensionality reduction methods. We provide rates of convergence for both inference of informative directions as well as inference of a graphical model of variable dependencies. Keywords: gradient estimates, manifold learning, graphical models, inverse regression, dimension reduction, gradient diffusion maps</p><br/>
<h2>reference text</h2><p>R.J. Adcock. A problem in least squares. The Analyst, 5:53–54, 1878. M. Belkin and P. Niyogi. Laplacian Eigenmaps for Dimensionality Reduction and Data representation. Neural Computation, 15(6):1373–1396, 2003. M. Belkin and P. Niyogi. Semi-Supervised Learning on Riemannian Manifolds. Machine Learning, 56(1-3):209–239, 2004. M. Belkin and P. Niyogi. Towards a theoretical foundation for Laplacian-based manifold methods. In Learning Theory, volume 3559 of Lecture Notes in Comput. Sci., pages 486–500. Springer, Berlin, 2005. C. Carvalho, J. Lucas, Q. Wang, J. Chang, J. Nevins, and M. West. High-dimensional sparse factor modelling - applications in gene expression genomics. Journal of the American Statistical Association, 103(484):1438–1456, 2008. O. Chapelle, B. Sch¨ lkopf, and A. Zien, editors. Semi-Supervised Learning. MIT Press, Cambridge, o MA, 2006. URL http://www.kyb.tuebingen.mpg.de/ssl-book. R.R. Coifman and S. Lafon. Diffusion maps. Applied and Computational Harmonic Analysis, 21 (1):5–30, 2006. R.R. Coifman and M. Maggioni. Diffusion wavelets. Applied and Computational Harmonic Analysis, 21(1):53–94, 2006. R.R. Coifman, S. Lafon, A.B. Lee, M. Maggioni, B. Nadler, F. Warner, and S.W. Zucker. Geometric diffusions as a tool for harmonic analysis and structure deﬁnition of data: Diffusion maps. Proceedings of the National Academy of Sciences, 102(21):7426–7431, 2005a. R.R. Coifman, S. Lafon, A.B. Lee, M. Maggioni, B. Nadler, F. Warner, and S.W. Zucker. Geometric diffusions as a tool for harmonic analysis and structure deﬁnition of data: Multiscale methods. Proceedings of the National Academy of Sciences, 102(21):7432–7437, 2005b. R.D. Cook. Fisher lecture: Dimension reduction in regression. Statistical Science, 22(1):1–26, 2007. R.D. Cook and S. Weisberg. Discussion of “Sliced inverse regression for dimension reduction”. J. Amer. Statist. Assoc., 86:328–332, 1991. 2195  W U , G UINNEY, M AGGIONI AND M UKHERJEE  M. P. do Carmo. Riemannian Geometry. Birkh¨ user, Boston, MA, 1992. a D. Donoho and C. Grimes. Hessian eigenmaps: new locally linear embedding techniques for highdimensional data. Proceedings of the National Academy of Sciences, 100:5591–5596, 2003. N. Duan and K.C. Li. Slicing regression: a link-free regression method. Ann. Statist., 19(2):505– 530, 1991. E.J. Edelman, J. Guinney, J-T. Chi, P.G. Febbo, and S. Mukherjee. Modeling cancer progression via pathway dependencies. PLoS Comput. Biol., 4(2):e28, 2008. F.Y. Edgeworth. On the reduction of observations. Philosophical Magazine, pages 135–141, 1884. J. Fan and I. Gijbels. Local Polynomial Modelling and its Applications. Chapman and Hall, London, 1996. E.R. Fearon and B. Vogelstein. A genetic model for colorectal tumorigenesis. Cell, 61:759–767, 1990. R.A. Fisher. On the mathematical foundations of theoretical statistics. Philosophical Transactions of the Royal Statistical Society A, 222:309–368, 1922. K. Fukumizu, F.R. Bach, and M.I. Jordan. Dimensionality reduction in supervised learning with reproducing kernel Hilbert spaces. Journal of Machine Learning Research, 5:73–99, 2005. E. Gin´ and V. Koltchinskii. Empirical graph Laplacian approximation of Laplace-Beltrami opere ators: large sample results. In High dimensional probability, volume 51 of IMS Lecture Notes Monogr. Ser., pages 238–259. Inst. Math. Statist., Beachwood, OH, 2006. G.H. Golub and C.F. Va Loan. Matrix Computations. The Johns Hopkins University Press; 3rd edition, 1996. T. Hastie and R. Tibshirani. Discriminant analysis by Gaussian mixtures. J. Roy.Statist. Soc. Ser. B, 58(1):155–176, 1996. C-H. Hsiang, T. Tunoda, Y.E. Whang, D.R. Tyson, and D.K. Ornstein. The impact of altered annexin i protein levels on apoptosis and signal transduction pathways in prostate cancer cells. The Prostate, 66(13):1413–1424, 2004. Z. Jiang, B.A. Woda BA, C.L. Wu, and X.J. Yang. Discovery and clinical application of a novel prostate cancer marker: alpha-methylacyl CoA racemase (P504S). Am. J. Clin. Pathol, 122(2): 275–8941, 2004. G . Kramer, G. Steiner, D. Fodinger, E. Fiebiger, C. Rappersberger, S. Binder, J. Hofbauer, and M. Marberger. High expression of a CD38-like molecule in normal prostatic epithelium and its differential loss in benign and malignant disease. The Journal of Urology, 154(5):1636–1641, 1995. S.L. Lauritzen. Graphical Models. Oxford: Clarendon Press, 1996. 2196  L EARNING G RADIENTS : P REDICTIVE M ODELS THAT I NFER G EOMETRY  K.C. Li. Sliced inverse regression for dimension reduction. J. Amer. Statist. Assoc., 86:316–342, 1991. K.C. Li. On principal Hessian directions for data visualization and dimension reduction: another application of Stein’s lemma. Ann. Statist., 97:1025–1039, 1992. N. Meinshausen and P. Buhlmann. High-dimensional graphs and variable selection with the Lasso. Annals of Statistics, 34(2):1436–1462, 2006. S. Mukherjee and Q. Wu. Estimation of gradients and coordinate covariation in classiﬁcation. J. Mach. Learn. Res., 7:2481–2514, 2006. S. Mukherjee and DX. Zhou. Learning coordinate covariances via gradients. J. Mach. Learn. Res., 7:519–549, 2006. S. Mukherjee, D-X. Zhou, and Q. Wu. Learning gradients and feature selection on manifolds. Bernoulli, 16(1):181–207, 2010. S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323–2326, 2000. T. Speed and H. Kiiveri. Gaussian Markov distributions over ﬁnite graphs. Ann. Statist., 14:138– 150, 1986. M. Sugiyama. Dimensionality reduction of multimodal labeled data by local ﬁsher discriminant analysis. J. Mach. Learn. Res., 8:1027–1061, 2007. A. Szlam, M. Maggioni, and R. R. Coifman. Regularization on graphs with function-adapted diffusion process. J. Mach. Learn. Res., 9:1711–1739, 2008. M. Szummer and T. Jaakkola. Partially labeled classiﬁcation with markov random walks. In T. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems, volume 14, pages 945–952, 2001. J. Tenenbaum, V. de Silva, and J. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290:2319–2323, 2000. S.A. Tomlins, R. Mehra, D.R. Rhodes, X. Cao, L. Wang, S.M. Dhanasekaran, S. KalyanaSundaram, J.T. Wei, M.A. Rubin, K.J. Pienta, R.B. Shah, and A.M. Chinnaiyan. Integrative molecular concept modeling of prostate cancer progression. Nature Genetics, 39(1):41–51, 2007. G. Wahba. Splines Models for Observational Data. Series in Applied Mathematics, Vol. 59, SIAM, Philadelphia, 1990. Q. Wu, F. Liang, and S. Mukherjee. Regularized sliced inverse regression for kernel models. Technical Report 07-25, ISDS, Duke Univ., 2007. Y. Xia, H. Tong, W. Li, and L-X. Zhu. An adaptive estimation of dimension reduction space. J. Roy.Statist. Soc. Ser. B, 64(3):363–410, 2002. 2197  W U , G UINNEY, M AGGIONI AND M UKHERJEE  L. Zwald and G. Blanchard. On the convergence of eigenspaces in kernel principal component analysi s. In Y. Weiss, B. Sch¨ lkopf, and J. Platt, editors, Advances in Neural Information Proo cessing Systems 18, pages 1649–1656. MIT Press, Cambridge, MA, 2006.  2198</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
