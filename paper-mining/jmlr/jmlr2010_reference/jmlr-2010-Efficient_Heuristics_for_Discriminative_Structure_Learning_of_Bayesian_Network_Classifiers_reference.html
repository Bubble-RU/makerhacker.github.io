<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-33" href="../jmlr2010/jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">jmlr2010-33</a> <a title="jmlr-2010-33-reference" href="#">jmlr2010-33-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</h1>
<br/><p>Source: <a title="jmlr-2010-33-pdf" href="http://jmlr.org/papers/volume11/pernkopf10a/pernkopf10a.pdf">pdf</a></p><p>Author: Franz Pernkopf, Jeff A. Bilmes</p><p>Abstract: We introduce a simple order-based greedy heuristic for learning discriminative structure within generative Bayesian network classiﬁers. We propose two methods for establishing an order of N features. They are based on the conditional mutual information and classiﬁcation rate (i.e., risk), respectively. Given an ordering, we can ﬁnd a discriminative structure with O N k+1 score evaluations (where constant k is the tree-width of the sub-graph over the attributes). We present results on 25 data sets from the UCI repository, for phonetic classiﬁcation using the TIMIT database, for a visual surface inspection task, and for two handwritten digit recognition tasks. We provide classiﬁcation performance for both discriminative and generative parameter learning on both discriminatively and generatively structured networks. The discriminative structure found by our new procedures signiﬁcantly outperforms generatively produced structures, and achieves a classiﬁcation accuracy on par with the best discriminative (greedy) Bayesian network learning approach, but does so with a factor of ∼10-40 speedup. We also show that the advantages of generative discriminatively structured Bayesian network classiﬁers still hold in the case of missing features, a case where generative classiﬁers have an advantage over discriminative classiﬁers. Keywords: Bayesian networks, classiﬁcation, discriminative learning, structure learning, graphical model, missing feature</p><br/>
<h2>reference text</h2><p>S. Acid, L.M. de Campos, and J.G. Castellano. Learning Bayesian network classiﬁers: Searching in a space of partially directed acyclic graphs. Machine Learning, 59:213–235, 2005. S. Arnborg, D.G. Corneil, and A. Proskurowski. Complexity of ﬁnding embeddings in a k-tree. SIAM Journal of Algebraic and Discrete Methods, 8(2):277–284, 1987. L.R. Bahl, P.F. Brown, P.V. de Souza, and R.L. Mercer. Maximum Mutual Information estimation of HMM parameters for speech recognition. In IEEE Intern. Conf. on Acoustics, Speech, and Signal Processing, pages 49–52, 1986. 2356  D ISCRIMINATIVE L EARNING FOR BAYESIAN N ETWORK C LASSIFIERS  P.L. Bartlett, M.I. Jordan, and J.D. McAuliffe. Convexity, classiﬁcation, and risk bounds. Journal of the American Statistical Association, 101(473):138–156, 2006. J. Bilmes. Natural Statistical Models for Automatic Speech Recognition. PhD thesis, U.C. Berkeley, 1999. J. Bilmes. Dynamic Bayesian multinets. In 16th Inter. Conf. of Uncertainty in Artiﬁcial Intelligence (UAI), pages 38–45, 2000. J. Bilmes, G. Zweig, T. Richardson, K. Filali, K. Livescu, P. Xu, K. Jackson, Y. Brandman, E. Sandness, E. Holtz, J. Torres, and B. Byrne. Discriminatively structured graphical models for speech recognition: JHU-WS-2001. Technical report, Johns Hopkins University, 2001. C.M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995. C.M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. C.M. Bishop and J. Lasserre. Generative or discriminative? Getting the best of both worlds. Bayesian Statistics, 8:3–23, 2007. N. Brenner, S. Strong, R. Koberle, and W. Bialek. Synergy in a neural code. Neural Computation, 12:1531–1552, 2000. W.L. Buntine. Theory reﬁnement on Bayesian networks. In 7th Conference on Uncertainty in AI (UAI), pages 52–60, 1991. C.J.C. Burges. A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery, 2(2):121–167, 1998. C.K. Chow and C.N. Liu. Approximating discrete probability distributions with dependence trees. IEEE Transaction on Information Theory, 14:462–467, 1968. G. Cooper and E. Herskovits. A Bayesian method for the induction of probabilistic networks from data. Machine Learning, 9:309–347, 1992. T. Cover and J. Thomas. Elements of Information Theory. John Wiley & Sons, 1991. R.G Cowell, A.P. Dawid, S.L. Lauritzen, and D.J. Spiegelhalter. Probabilistic Networks and Expert Systems. Springer Verlag, 1999. S. Dasgupta. The sample complexity of learning ﬁxed-structure Bayesian networks. Machine Learning, 29(2):165–180, 1997. L.M. de Campos. A scoring function for learning Bayesian networks based on mutual information and conditional independence tests. Journal of Machine Learning Research, 7:2149–2187, 2006. P. Domingos and M.J. Pazzani. On the optimality of the simple Bayesian classiﬁer under zero-one loss. Machine Learning, 29(2-3):103–130, 1997. J. Dougherty, R. Kohavi, and M. Sahami. Supervised and unsupervised discretization of continuous features. In 12th International Conference on Machine Learning (ICML), pages 194–202, 1995. 2357  P ERNKOPF AND B ILMES  Y. Ephraim and L.R. Rabiner. On the releations between modeling approaches for speech recognition. IEEE Transactions on Information Theory, 36(2):372–380, 1990. Y. Ephraim, A. Dembo, and L.R. Rabiner. A minimum discrimination information approach for Hidden Markov Models. IEEE Transactions on Information Theory, 35(5):1001–1013, 1989. U.M. Fayyad and K.B. Irani. Multi-interval discretization of continuous-valued attributes for classiﬁcation learning. In 13th International Joint Conference on Artiﬁcial Intelligence, pages 1022– 1027, 1993. N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian network classiﬁers. Machine Learning, 29: 131–163, 1997. N. Friedman, I. Nachman, and D. Peer. Learning Bayesian network structure form massive datasets: The sparse candidate algorithm. In 15th Conference on Uncertainty in AI (UAI), pages 196–205, 1999. D. Geiger and D. Heckerman. Knowledge representation and inference in similarity networks and Bayesian multinets. Artiﬁcial Intelligence, 82:45–74, 1996. R. Greiner and W. Zhou. Structural extension to logistic regression: Discriminative parameter learning of belief net classiﬁers. In 18th Conf. of the AAAI, pages 167–173, 2002. R. Greiner, X. Su, S. Shen, and W. Zhou. Structural extension to logistic regression: Discriminative parameter learning of belief net classiﬁers. Machine Learning, 59:297–322, 2005. A. Gretton and L. G¨ orﬁ. Nonparametric independence tests: Space partitioning and kernel apy proaches. In Algorithmic Learning Theory: 19th International Conference (ALT08), pages 183– 198, 2008. D. Grossman and P. Domingos. Learning Bayesian network classiﬁers by maximizing conditional likelihood. In 21st Inter. Conf. of Machine Lerning (ICML), pages 361–368, 2004. A.K. Halberstadt and J. Glass. Heterogeneous measurements for phonetic classiﬁcation. In Proceedings of EUROSPEECH, pages 401–404, 1997. D. Heckerman. A tutorial on learning Bayesian networks. Technical Report MSR-TR-95-06, Microsoft Research, 1995. G. Heigold, T. Deselaers, R. Schl¨ ter, and H. Ney. Modiﬁed MMI/MPE: A direct evaluation of u the margin in speech recognition. In Intern. Conf. on Machine learning (ICML), pages 384–391, 2008. T. Jebara. Discriminative, Generative and Imitative Learning. PhD thesis, Media Laboratory, MIT, 2001. M.I. Jordan. Learning in Graphical Models. MIT Press, 1999. B.-H. Juang and S. Katagiri. Discriminative learning for minimum error classiﬁcation. IEEE Transactions on Signal Processing, 40(12):3043–3054, 1992. 2358  D ISCRIMINATIVE L EARNING FOR BAYESIAN N ETWORK C LASSIFIERS  B.-H. Juang, W. Chou, and C.-H. Lee. Minimum classiﬁcation error rate methods for speech recognition. IEEE Transactions on Speech and Audio Processing, 5(3):257–265, 1997. D.R. Karger and N. Srebro. Learning Markov networks: Maximum bounded tree-width graphs. In Symposium on Discrete Algorithms, pages 302–401, 2001. E.J. Keogh and M.J. Pazzani. Learning augmented Bayesian classiﬁers: A comparison of distribution-based and classiﬁcation-based approaches. In 7th International Workshop on Artiﬁcial Intelligence and Statistics, pages 225–230, 1999. R. Kohavi and G.H. John. Wrappers for feature subset selection. Artiﬁcial Intelligence, 97:273–324, 1997. J.B. Kruskal. On the shortest spanning subtree and the traveling salesman problem. In Proceedings of the American Mathematical Society, volume 7, pages 48–50, 1956. L. Lamel, R. Kassel, and S. Seneff. Speech database development: Design and analysis of the acoustic-phonetic corpus. In Proceedings of the DARPA Speech Recognition Workshop, Report No. SAIC-86/1546, 1986. S.L. Lauritzen. Graphical Models. Oxford Science Publications, 1996. Y. LeCun and C. Cortes. The MNIST database of handwritten digits. http://yann.lecun.com/ exdb/mnist/. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. C. Meek. Causal inference and causal explanation with background knowledge. In 11th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 403–410, 1995. C. Merz, P. Murphy, and D. Aha. UCI repository of machine learning databases. Department of Information and Computer Science, University of California, Irvine, 1997. www.ics.uci.edu/ \˜{}mlearn/MLRepository.html. T. Mitchell. Machine Learning. McGraw Hill, 1997. K.P. Murphy. Dynamic Bayesian networks: Representation, Iference and Learning. PhD Thesis, University of California, Berkeley, 2002. N. Narasimhan and J. Bilmes. A supermodular-submodular procedure with applications to discriminative structure learning. In 21st Conf. on Uncertainty in Artiﬁcial Intelligence (UAI), 2005. A.Y. Ng and M. Jordan. On discriminative vs. generative classiﬁers: A comparison of logistic regression and naive bayes. In Advances in Neural Information Processing Systems 14, 2002. M. Pazzani. Searching for dependencies in Bayesian classiﬁers. In Learning from Data: Artiﬁcial Intelligence and Statistics V, pages 239–248, 1996. J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, 1988. 2359  P ERNKOPF AND B ILMES  F. Pernkopf. Detection of surface defects on raw steel blocks using Bayesian network classiﬁers. Pattern Analysis and Applications, 7(3):333–342, 2004. F. Pernkopf. Bayesian network classiﬁers versus selective k-NN classiﬁer. Pattern Recognition, 38 (3):1–10, 2005. F. Pernkopf and J. Bilmes. Discriminative versus generative parameter and structure learning of Bayesian network classiﬁers. In Intern. Conf. on Machine Learning (ICML), pages 657 – 664, 2005. F. Pernkopf and J. Bilmes. Efﬁcient heuristics for discriminative structure learning of Bayesian network classiﬁers. Technical report, Laboratory of Signal Processing and Speech Communication, Graz University of Technology, 2008a. F. Pernkopf and J.A. Bilmes. Order-based discriminative structure learning for Bayesian network classiﬁers. In International Symposium on Artiﬁcial Intelligence and Mathematics, 2008b. F. Pernkopf and M. Wohlmayr. On discriminative parameter learning of bayesian network classiﬁers. In European Conference on Machine Learning (ECML), pages 221–237, 2009. F. Pernkopf, T. Van Pham, and J.A. Bilmes. Broad phonetic classiﬁcation using discriminative Bayesian networks. Speech Communication, 143(1):123–138, 2008. W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery. Numerical recipes in C. Cambridge Univ. Press, 1992. R. Raina, Y. Shen, A.Y Ng, and A. McCallum. Classiﬁcation with hybrid generative/discriminative models. In Advances in Neural Information Processing Systems 16, 2004. T. Roos, H. Wettig, P. Gr¨ nwald, P. Myllym¨ ki, and H. Tirri. On discriminative Bayesian network u a classiﬁers and logistic regression. Machine Learning, 59:267–296, 2005. B. Sch¨ lkopf and A.J. Smola. Learning with kernels: Support Vector Machines, Regularization, o Optimization, and Beyond. MIT Press, 2001. N. Srebro. Maximum likelihood bounded tree-width Markov networks. Artiﬁcial Intelligence, 143 (1):123–138, 2003. M. Teyssier and D. Koller. Ordering-based search: A simple and effective algorithm for learning Bayesian networks. In 21th Conference on Uncertainty in AI (UAI), pages 584 – 590, 2005. R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B., 58(1): 267–288, 1996. V. Vapnik. Statistical Learning Theory. Wiley & Sons, 1998. X. Zhang, L. Song, A. Gretton, and A. Smola. Kernel measures of independence for non-iid data. In Advances on Neural Information Processing Systems 22, 2009.  2360</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
