<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-2" href="../jmlr2010/jmlr-2010-A_Convergent_Online_Single_Time_Scale_Actor_Critic_Algorithm.html">jmlr2010-2</a> <a title="jmlr-2010-2-reference" href="#">jmlr2010-2-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</h1>
<br/><p>Source: <a title="jmlr-2010-2-pdf" href="http://jmlr.org/papers/volume11/dicastro10a/dicastro10a.pdf">pdf</a></p><p>Author: Dotan Di Castro, Ron Meir</p><p>Abstract: Actor-Critic based approaches were among the ﬁrst to address reinforcement learning in a general setting. Recently, these algorithms have gained renewed interest due to their generality, good convergence properties, and possible biological relevance. In this paper, we introduce an online temporal difference based actor-critic algorithm which is proved to converge to a neighborhood of a local maximum of the average reward. Linear function approximation is used by the critic in order estimate the value function, and the temporal difference signal, which is passed from the critic to the actor. The main distinguishing feature of the present convergence proof is that both the actor and the critic operate on a similar time scale, while in most current convergence proofs they are required to have very different time scales in order to converge. Moreover, the same temporal difference signal is used to update the parameters of both the actor and the critic. A limitation of the proposed approach, compared to results available for two time scale convergence, is that convergence is guaranteed only to a neighborhood of an optimal value, rather to an optimal value itself. The single time scale and identical temporal difference signal used by the actor and the critic, may provide a step towards constructing more biologically realistic models of reinforcement learning in the brain. Keywords: actor critic, single time scale convergence, temporal difference</p><br/>
<h2>reference text</h2><p>T. Archibald, K. McKinnon, and L. Thomas. On the generation of markov decision processes. Journal of the Operational Research Society, 46:354–361, 1995. D. Baras and R. Meir. Reinforcement learning, spike time dependent plasticity and the bcm rule. Neural Comput., 19(8):2245–2279, Aug 2007. J. Baxter and P.L. Bartlett. Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial Intelligence Research, 15:319–350, 2001. D.P. Bertsekas. Dynamic Programming and Optimal Control, Vol I & II, 3rd Ed. Athena Scinetiﬁc, 2006. D.P. Bertsekas and J. Tsitsiklis. Neuro-dynamic Programming. Athena Scinetiﬁc, 1996. S. Bhatnagar, R.S. Sutton, M. Ghavamzadeh, and M. Lee. Incremental natural actor-critic algorithms. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 105–112, Cambridge, MA, 2008. MIT Press. S. Bhatnagar, R.S. Sutton, M. Ghavamzadeh, and M. Lee. Natural actor–critic algorithms. Automatica, 45(11):2471–2482, 2009. V.S. Borkar. Stochastic approximation with two time scales. Syst. Control Lett., 29(5):291–294, 1997. P. Br´ maud. Markov Chains: Gibbs Fields, Monte Carlo Simulation, and Queues. Springer, 1999. e X. Cao. Stochastic Learning and Optimization: A Sensitivity-Based Approach (International Series on Discrete Event Dynamic Systems). Springer-Verlag New York, Inc. Secaucus, NJ, USA, 2007. X.R. Cao and H.F. Chen. Pertubation realization, potentials, and sensitivity analysis of markov processes. IEEE Trans. Automat. Contr, 42:1382–1393, 1997. N.D. Daw, Y. Niv, , and P. Dayan. Actions, Policies, Values, and the Basal Ganglia - In: Bezard, E editor, Recent Breakthroughs in Basal Ganglia Research. Nova Science Publishers Inc., 2006. D. DiCastro, D. Volkinstein, and R. Meir. Temporal difference based actor critic algorithms single time scale convergence and neural implementation. In Advances in Neural Information Processing Systems, accepted, 2008. R.V. Florian. Reinforcement learning through modulation of spike-timing-dependent synaptic plasticity. Neural Computation, 19:1468–1502, 2007. R.G. Gallager. Discrete Stochastic Processes. Kluwer Academic Publishers, 1995. E. Greensmith, P.L. Bartlett, and J. Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5:1471–1530, 2004. R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 1985. H. K. Khalil. Nonlinear Systems, 3rd Ed. Prentice Hall, 2002. 409  D I C ASTRO AND M EIR  V.R. Konda and V.S. Borkar. Actor-critic like learning algorithms for markov decision processes. SIAM Journal on Control and Optimization, pages 94–123, 1999. V.R. Konda and J. Tsitsiklis. On actor critic algorithms. SIAM J. Control Optim., 42(4):1143–1166, 2003. H.J. Kushner and G.G. Yin. Stochastic Approximation Algorithms and Applications. Springer, 1997. P. Marbach and J. Tsitsiklis. Simulation-based optimization of markov reward processes. IEEE. Trans. Auto. Cont., 46(2):191–209, 1998. A. Mokkadem and M. Pelletier. Convergence rate and averaging of nonlinear two-time-scale stochastic approximation algorithms. Annals of Applied Probability, 16(3):1671, 2006. J. Peters and S. Schaal. Natural actor-critic. Neurocomputing, 71:1180–1190, 2008. B.T. Polyak. New method of stochastic approximation type. Automat. Remote Control, 51:937–946, 1990. M.L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc, 1994. W. Schultz. Getting formal with dopamine and reward. Neuron, 36(2):241–63, 2002. S. Singh and P. Dayan. Analytical mean squared error curves for temporal difference learning. Machine Learning, 32:5–40, 1998. R.S. Sutton and A.G. Barto. Reinforcement Learning. MIT Press, 1998. G. Tesauro. Temporal difference learning and the td-gammon. Communication of the ACM, 38(3), March 1995. J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674–690, May 1997.  410</p>
<br/>
<br/><br/><br/></body>
</html>
