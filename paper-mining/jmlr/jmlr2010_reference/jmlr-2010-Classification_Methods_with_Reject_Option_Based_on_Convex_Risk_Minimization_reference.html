<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-21" href="../jmlr2010/jmlr-2010-Classification_Methods_with_Reject_Option_Based_on_Convex_Risk_Minimization.html">jmlr2010-21</a> <a title="jmlr-2010-21-reference" href="#">jmlr2010-21-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</h1>
<br/><p>Source: <a title="jmlr-2010-21-pdf" href="http://jmlr.org/papers/volume11/yuan10a/yuan10a.pdf">pdf</a></p><p>Author: Ming Yuan, Marten Wegkamp</p><p>Abstract: In this paper, we investigate the problem of binary classiﬁcation with a reject option in which one can withhold the decision of classifying an observation at a cost lower than that of misclassiﬁcation. Since the natural loss function is non-convex so that empirical risk minimization easily becomes infeasible, the paper proposes minimizing convex risks based on surrogate convex loss functions. A necessary and sufﬁcient condition for inﬁnite sample consistency (both risks share the same minimizer) is provided. Moreover, we show that the excess risk can be bounded through the excess surrogate risk under appropriate conditions. These bounds can be tightened by a generalized margin condition. The impact of the results is illustrated on several commonly used surrogate loss functions. Keywords: classiﬁcation, convex surrogate loss, empirical risk minimization, generalized margin condition, reject option</p><br/>
<h2>reference text</h2><p>P.L. Bartlett, M.I. Jordan and J. McAuliffe. Convexity, classiﬁcation, and risk bounds. Journal of the American Statistical Association, 101:138-156, 2006. P.L. Bartlett and M.H. Wegkamp. Classiﬁcation with a reject option using a hinge loss. Journal of Machine Learning Research, 9:1823-1840, 2008. J. Friedman, T. Hastie and R. Tibshirani. Additive logistic regression: a statistical view of boosting. Annals of Statistics, 28(2):337-407, 2000. R. Herbei and M.H. Wegkamp. Classiﬁcation with reject option. Canadian Journal of Statistics, 34(4):709-721, 2006. Y. Lin. Support vector machines and the Bayes rule in classiﬁcation. Machine Learning and Knowledge Discovery, 6:259-275, 2002. 129  Y UAN AND W EGKAMP  E. Mammen and A.B. Tsybakov. Smooth discrimination analysis. Annals of Statistics, 27(6):18081829, 1999. J.S. Marron, M. Todd and J. Ahn. Distance weighted discrimination. Journal of the American Statistical Association, 102:1267-1271, 2007. B.D. Ripley. Pattern Recognition and Neural Networks. Cambridge University Press, Cambridge, 1996. A.B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Annals of Statistics, 32:135-166, 2004. M.H. Wegkamp. Lasso type classiﬁers with a reject option. Electronic Journal of Statistics, 1:155168, 2007. T. Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. Annals of Statistics, 32:56-134, 2004.  130</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
