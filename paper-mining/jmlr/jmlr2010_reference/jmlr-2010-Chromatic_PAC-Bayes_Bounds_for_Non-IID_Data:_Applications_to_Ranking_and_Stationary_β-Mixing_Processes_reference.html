<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-20" href="../jmlr2010/jmlr-2010-Chromatic_PAC-Bayes_Bounds_for_Non-IID_Data%3A_Applications_to_Ranking_and_Stationary_%CE%B2-Mixing_Processes.html">jmlr2010-20</a> <a title="jmlr-2010-20-reference" href="#">jmlr2010-20-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</h1>
<br/><p>Source: <a title="jmlr-2010-20-pdf" href="http://jmlr.org/papers/volume11/ralaivola10a/ralaivola10a.pdf">pdf</a></p><p>Author: Liva Ralaivola, Marie Szafranski, Guillaume Stempfel</p><p>Abstract: PAC-Bayes bounds are among the most accurate generalization bounds for classiﬁers learned from independently and identically distributed (IID) data, and it is particularly so for margin classiﬁers: there have been recent contributions showing how practical these bounds can be either to perform model selection (Ambroladze et al., 2007) or even to directly guide the learning of linear classiﬁers (Germain et al., 2009). However, there are many practical situations where the training data show some dependencies and where the traditional IID assumption does not hold. Stating generalization bounds for such frameworks is therefore of the utmost interest, both from theoretical and practical standpoints. In this work, we propose the ﬁrst—to the best of our knowledge—PAC-Bayes generalization bounds for classiﬁers trained on data exhibiting interdependencies. The approach undertaken to establish our results is based on the decomposition of a so-called dependency graph that encodes the dependencies within the data, in sets of independent data, thanks to graph fractional covers. Our bounds are very general, since being able to ﬁnd an upper bound on the fractional chromatic number of the dependency graph is sufﬁcient to get new PAC-Bayes bounds for speciﬁc settings. We show how our results can be used to derive bounds for ranking statistics (such as AUC) and classiﬁers trained on data distributed according to a stationary β-mixing process. In the way, we show how our approach seamlessly allows us to deal with U-processes. As a side note, we also provide a PAC-Bayes generalization bound for classiﬁers learned on data from stationary ϕ-mixing distributions. Keywords: PAC-Bayes bounds, non IID data, ranking, U-statistics, mixing processes c 2010 Liva Ralaivola, Marie Szafranski and Guillaume Stempfel. R ALAIVOLA , S ZAFRANSKI AND S TEMPFEL</p><br/>
<h2>reference text</h2><p>S. Agarwal and P. Niyogi. Generalization bounds for ranking algorithms via algorithmic stability. Journal of Machine Learning Research, 10:441–474, 2009. S. Agarwal, T. Graepel, R. Herbrich, S. Har-Peled, and D. Roth. Generalization bounds for the area under the ROC curve. Journal of Machine Learning Research, 6:393–425, 2005. A. Ambroladze, E. Parrado-Hernandez, and J. Shawe-Taylor. Tighter PAC-Bayes bounds. In Adv. in Neural Information Processing Systems 19, pages 9–16, 2007. K. Ataman, W. Nick, and Y. Zhang. Learning to rank by maximizing auc with linear programming. In In IEEE International Joint Conference on Neural Networks (IJCNN 2006), pages 123–129, 2006. J.-Y. Audibert and O. Bousquet. Combining PAC-bayesian and generic chaining bounds. Journal of Machine Learning Research, 8:863–889, 2007. ISSN 1533-7928. P. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. Annals of Statistics, 33(4):1497–1537, 2005. P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463–482, 2002. G. Blanchard and F. Fleuret. Occam’s hammer. In COLT, pages 112–126, 2007. O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research, 2:499–526, March 2002. U. Brefeld and T. Scheffer. AUC maximizing support vector learning. In Proc. of the ICML Workshop on ROC Analysis in Machine Learning, 2005. O. Catoni. PAC-Bayesian Supervised Classiﬁcation: The Thermodynamics of Statistical Learning, volume 56 of Lecture Notes–Monograph Series. Beachwood, Ohio, USA: Institute of Mathematical Statistics, 2007. S. Cl´ mencon, G. Lugosi, and N. Vayatis. Ranking and empirical minimization of U-statistics. The e ¸ Annals of Statistics, 36(2):844–874, April 2008. ISSN 0090-5364. C. Cortes and M. Mohri. AUC optimization vs. error rate minimization. In Adv. in Neural Information Processing Systems 16, 2004. Y. Freund, R. Iyer, R.E. Schapire, and Y. Singer. An efﬁcient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933–969, 2003. P. Germain, A. Lacasse, F. Laviolette, and M. Marchand. PAC-Bayesian learning of linear classiﬁers. In Proc. of the 26th Annual International Conference on Machine Learning, pages 353–360, 2009. R. Herbrich and T. Graepel. A PAC-Bayesian margin bound for linear classiﬁers: Why svms work. In Advances in Neural Information Processing Systems 13, pages 224–230, 2001. 1954  C HROMATIC PAC -BAYES B OUNDS AND N ON -IID DATA  W. Hoeffding. A class of statistics with asymptotically normal distribution. Annals of Mathematical Statistics, 19(3):293–325, 1948. W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58(301):13–30, 1963. S. Janson. Large deviations for sums of partly dependent random variables. Random Structures Algorithms, 24:234–248, 2004. L. Kontorovich and K. Ramanan. Concentration inequalities for dependent random variables via the martingale method. The Annals of Probability, 36(6):2126–2158, 2008. A. Lacasse, F. Laviolette, M. Marchand, P. Germain, and N. Usunier. PAC-Bayes bounds for the risk of the majority vote and the variance of the Gibbs classiﬁer. In Advances in Neural Information Processing Systems 19, pages 769–776, 2006. J. Langford. Tutorial on Practical Theory for Classiﬁcation. Journal of Machine Learning Research, pages 273–306, 2005. J. Langford and J. Shawe-taylor. PAC-Bayes and margins. In Adv. in Neural Information Processing Systems 15, pages 439–446, 2002. D. McAllester. Some PAC-Bayesian Theorems. Machine Learning, 37:355–363, 1999. D. McAllester. Simpliﬁed pac-bayesian margin bounds. In Proc. of the 16th Annual Conference on Computational Learning Theory, pages 203–215, 2003. C. McDiarmid. On the method of bounded differences. Survey in Combinatorics, pages 148–188, 1989. M. Mohri and A. Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1097–1104, 2009. S. V. Pemmaraju. Equitable coloring extends chernoff-hoeffding bounds. In RANDOM-APPROX, pages 285–296, 2001. A. Rakotomamonjy. Optimizing the area under the ROC curve with SVMs. In ROC Analysis in Artiﬁcial Intelligence, pages 71–80, 2004. E.R. Schreinerman and D.H. Ullman. Fractional Graph Theory: A Rational Approach to the Theory of Graphs. Wiley Interscience Series in Discrete Math., 1997. M. Seeger. PAC-Bayesian generalization bounds for Gaussian processes. Journal of Machine Learning Research, 3:233–269, 2002a. M. Seeger. The proof of McAllester’s PAC-Bayesian theorem. Technical report, Institute for ANC, Edinburgh, UK, 2002b. N. Usunier, M.-R. Amini, and P. Gallinari. A data-dependent generalisation error bound for the AUC. In Proc. of the ICML Workshop on ROC Analysis in Machine Learning, 2005. 1955  R ALAIVOLA , S ZAFRANSKI AND S TEMPFEL  N. Usunier, M.-R. Amini, and P. Gallinari. Generalization error bounds for classiﬁers trained with interdependent data. In Adv. in Neural Information Processing Systems 18, pages 1369–1376, 2006. B. Yu. Rates of convergence for empirical processes of stationary mixing sequences. Annals of Probability, 22(1):94–116, 1994.  1956</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
