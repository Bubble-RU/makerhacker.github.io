<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 jmlr-2010-Importance Sampling for Continuous Time Bayesian Networks</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-51" href="../jmlr2010/jmlr-2010-Importance_Sampling_for_Continuous_Time_Bayesian_Networks.html">jmlr2010-51</a> <a title="jmlr-2010-51-reference" href="#">jmlr2010-51-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>51 jmlr-2010-Importance Sampling for Continuous Time Bayesian Networks</h1>
<br/><p>Source: <a title="jmlr-2010-51-pdf" href="http://jmlr.org/papers/volume11/fan10a/fan10a.pdf">pdf</a></p><p>Author: Yu Fan, Jing Xu, Christian R. Shelton</p><p>Abstract: A continuous time Bayesian network (CTBN) uses a structured representation to describe a dynamic system with a ﬁnite number of states which evolves in continuous time. Exact inference in a CTBN is often intractable as the state space of the dynamic system grows exponentially with the number of variables. In this paper, we ﬁrst present an approximate inference algorithm based on importance sampling. We then extend it to continuous-time particle ﬁltering and smoothing algorithms. These three algorithms can estimate the expectation of any function of a trajectory, conditioned on any evidence set constraining the values of subsets of the variables over subsets of the time line. We present experimental results on both synthetic networks and a network learned from a real data set on people’s life history events. We show the accuracy as well as the time efﬁciency of our algorithms, and compare them to other approximate algorithms: expectation propagation and Gibbs sampling. Keywords: continuous time Bayesian networks, importance sampling, approximate inference, ﬁltering, smoothing</p><br/>
<h2>reference text</h2><p>Gunter Bolch, Stefan Greiner, Hermann de Meer, and Kishor S. Trivedi. Queueing Networks and Markov Chains. John Wiley & Sons, Inc., 1998. Thomas Dean and Keiji Kanazawa. A model for reasoning about persistence and causation. Computational Intelligence, 5(3):142–150, 1989. Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39:1–38, 1977. Arnaud Doucet, Nando de Freitas, and Neil Gordon, editors. Sequential Monte Carlo Methods in Practice. Springer-Verlag Telos, 2001. Tal El-Hay, Nir Friedman, and Raz Kupferman. Gibbs sampling in factorized continuous-time Markov processes. In Proceedings of the Twenty-Fourth Conference on Uncertainty in Artiﬁcial Intelligence, 2008. ESRC Research Centre on Micro-social Change. British household panel survey. Computer Data File and Associated Documentation. http://www.iser.essex.ac.uk/bhps, 2003. Colchester: The Data Archive. Yu Fan and Christian R. Shelton. Sampling for approximate inference in continuous time Bayesian networks. In Proceedings of Tenth International Symposium on Artiﬁcial Intelligence and Mathematics, 2008. Robert M. Fung and Kuo-Chu Chang. Weighing and integrating evidence for stochastic simulation in Bayesian networks. In Proceedings of the Fifth Annual Conference on Uncertainty in Artiﬁcial Intelligence, pages 209–220, 1989. Simon Godsill, Arnaud Doucet, and Mike West. Monte Carlo smoothing for non-linear time series. Journal of the American Statistical Association, 99:156–168, 2004. Ralf Herbrich, Thore Graepel, and Brendan Murphy. Structure from failure. In Proceedings of the 2nd USENIX workshop on Tackling computer systems problems with machine learning techniques, pages 1–6, 2007. Tim Hesterberg. Weighted average importance sampling and defensive mixture distributions. Technometrics, 37(2):185–194, 1995. Kin Fai Kan and Christian R. Shelton. Solving structured continuous-time Markov decision processes. In Proceedings of Tenth International Symposium on Artiﬁcial Intelligence and Mathematics, 2008. Thomas P. Minka. Expectation propagation for approximate Bayesian inference. In Proceedings of the Seventeenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 362–369, 2001. 2139  FAN , X U AND S HELTON  Brenda Ng, Avi Pfeffer, and Richard Dearden. Continuous time particle ﬁltering. In Proceedings of the Nineteenth International Joint Conference on Artiﬁcial Intelligence, pages 1360–1365, 2005. Uri Nodelman and Eric Horvitz. Continuous time Bayesian networks for inferring users’ presence and activities with extensions for modeling and evaluation. Technical Report MSR-TR-2003-97, Microsoft Research, December 2003. Uri Nodelman, Christian R. Shelton, and Daphne Koller. Continuous time Bayesian networks. In Proceedings of the Eighteenth International Conference on Uncertainty in Artiﬁcial Intelligence, pages 378–387, 2002. Uri Nodelman, Christian R. Shelton, and Daphne Koller. Learning continuous time Bayesian networks. In Proceedings of the Nineteenth International Conference on Uncertainty in Artiﬁcial Intelligence, pages 451–458, 2003. Uri Nodelman, Daphne Koller, and Christian R. Shelton. Expectation propagation for continuous time Bayesian networks. In Proceedings of the Twenty-First International Conference on Uncertainty in Artiﬁcial Intelligence, pages 431–440, 2005a. Uri Nodelman, Christian R. Shelton, and Daphne Koller. Expectation maximization and complex duration distributions for continuous time Bayesian networks. In Proceedings of the Twenty-First International Conference on Uncertainty in Artiﬁcial Intelligence, pages 421–430, 2005b. James R. Norris. Markov Chains. Cambridge University Press, 1997. Carl Adam Petri. Kommunikation mit Automaten. PhD thesis, University of Bonn, 1962. Suchi Saria, Uri Nodelman, and Daphne Koller. Reasoning at the right time granularity. In Proceedings of the Twenty-third Conference on Uncertainty in AI, pages 421–430, 2007. Ross D. Shachter and Mark A. Peot. Simulation approaches to general probabilistic inference on belief networks. In Proceedings of the Fifth International Conference on Uncertainty in Artiﬁcial Intelligence, pages 221–234, 1989. Christian R. Shelton, Yu Fan, William Lam, Joon Lee, and Jing Xu. Continuous time Bayesian network reasoning and learning engine. Journal of Machine Learning Research, 11(Mar):1137–1140, 2010. Charles A. Sutton and Michael I. Jordan. Probabilistic inference in queueing networks. In Armando Fox and Sumit Basu, editors, Proceedings of Third Workshop on Tackling Computer Systems Problems with Machine Learning Techniques, 2008. Greg C. G. Wei and Martin A. Tanner. A Monte Carlo implementation of the EM algorithm and the poor man’s data augmentation algorithms. Journal of the American Statistical Association, 85(411):699–704, 1990. Jing Xu and Christian R. Shelton. Continuous time Bayesian networks for host level network intrusion detection. In European Conference on Machine Learning, pages 613–627, 2008.  2140</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
