<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>24 jmlr-2010-Collective Inference for  Extraction MRFs Coupled with Symmetric Clique Potentials</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-24" href="../jmlr2010/jmlr-2010-Collective_Inference_for__Extraction_MRFs_Coupled_with_Symmetric_Clique_Potentials.html">jmlr2010-24</a> <a title="jmlr-2010-24-reference" href="#">jmlr2010-24-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>24 jmlr-2010-Collective Inference for  Extraction MRFs Coupled with Symmetric Clique Potentials</h1>
<br/><p>Source: <a title="jmlr-2010-24-pdf" href="http://jmlr.org/papers/volume11/gupta10a/gupta10a.pdf">pdf</a></p><p>Author: Rahul Gupta, Sunita Sarawagi, Ajit A. Diwan</p><p>Abstract: Many structured information extraction tasks employ collective graphical models that capture interinstance associativity by coupling them with various clique potentials. We propose tractable families of such potentials that are invariant under permutations of their arguments, and call them symmetric clique potentials. We present three families of symmetric potentials—MAX, SUM, and MAJORITY . We propose cluster message passing for collective inference with symmetric clique potentials, and present message computation algorithms tailored to such potentials. Our ﬁrst message computation algorithm, called α-pass, is sub-quadratic in the clique size, outputs exact messages for 13 MAX , and computes 15 -approximate messages for Potts, a popular member of the SUM family. Empirically, it is upto two orders of magnitude faster than existing algorithms based on graph-cuts or belief propagation. Our second algorithm, based on Lagrangian relaxation, operates on MAJORITY potentials and provides close to exact solutions while being two orders of magnitude faster. We show that the cluster message passing framework is more principled, accurate and converges faster than competing approaches. We extend our collective inference framework to exploit associativity of more general intradomain properties of instance labelings, which opens up interesting applications in domain adaptation. Our approach leads to signiﬁcant error reduction on unseen domains without incurring any overhead of model retraining. Keywords: passing graphical models, collective inference, clique potentials, cluster graphs, message</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
