<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>83 jmlr-2010-On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-83" href="../jmlr2010/jmlr-2010-On_Over-fitting_in_Model_Selection_and_Subsequent_Selection_Bias_in_Performance_Evaluation.html">jmlr2010-83</a> <a title="jmlr-2010-83-reference" href="#">jmlr2010-83-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>83 jmlr-2010-On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</h1>
<br/><p>Source: <a title="jmlr-2010-83-pdf" href="http://jmlr.org/papers/volume11/cawley10a/cawley10a.pdf">pdf</a></p><p>Author: Gavin C. Cawley, Nicola L. C. Talbot</p><p>Abstract: Model selection strategies for machine learning algorithms typically involve the numerical optimisation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneﬁcial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a nonnegligible variance introduces the potential for over-ﬁtting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-ﬁtting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-ﬁtting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-ﬁtting and hence are unreliable. We discuss methods to avoid over-ﬁtting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the ﬁndings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a ﬁnite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds. Keywords: model selection, performance evaluation, bias-variance trade-off, selection bias, overﬁtting</p><br/>
<h2>reference text</h2><p>D. M. Allen. The relationship between variable selection and prediction. Technometrics, 16:125– 127, 1974. C. Ambroise and G. J. McLachlan. Selection bias in gene extraction on the basis of microarray geneexpression data. Proceedings of the National Academy of Sciences, 99(10):6562–6566, May 14 2002. doi: 10.1073/pnas.102102699. S. An, W. Liu, and S. Venkatesh. Fast cross-validation algorithms for least squares support vector machines and kernel ridge regression. Pattern Recognition, 40(8):2154–2162, August 2007. doi: 10.1016/j.patcog.2006.12.015. E. Andeli´ , M. Schaff¨ ner, M. Katz, S. E. Kr¨ ger, and A. Wendermuth. Kernel least-squares models c o u using updates of the pseudoinverse. Neural Computation, 18(12):2928–2935, December 2006. doi: 10.1162/neco.2006.18.12.2928. Y. Bengio and Y. Grandvalet. No unbiased estimator of the variance of k-fold cross-validation. Journal of Machine Learning Research, 5:1089–1105, 2004. S. A. Billings and K. L. Lee. Nonlinear Fisher discriminant analysis using a minimum squared error cost function and the orthogonal least squares algorithm. Neural Networks, 15(2):263–270, March 2002. doi: 10.1016/S0893-6080(01)00142-3. C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995. L. Bo, L. Wang, and L. Jiao. Feature scaling for kernel Fisher discriminant analysis using leaveone-out cross validation. Neural Computation, 18(4):961–978, April 2006. doi: 10.1162/neco. 2006.18.4.961. O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research, 2:499–526, 2002. L. Breiman. Random forests. Machine Learning, 45(1):5–32, October 2001. doi: 10.1023/A: 1010933404324. G. C. Cawley. Leave-one-out cross-validation based model selection criteria for weighted LSSVMs. In Proceedings of the IEEE/INNS International Joint Conference on Neural Networks (IJCNN-06), pages 1661–1668, Vancouver, BC, Canada, July 16–21 2006. doi: 10.1109/IJCNN. 2006.246634. G. C. Cawley and N. L. C. Talbot. Efﬁcient leave-one-out cross-validation of kernel Fisher discriminant classiﬁers. Pattern Recognition, 36(11):2585–2592, November 2003. doi: 10.1016/ S0031-3203(03)00136-5. G. C. Cawley and N. L. C. Talbot. Preventing over-ﬁtting during model selection via Bayesian regularisation of the hyper-parameters. Journal of Machine Learning Research, 8:841–861, April 2007. 2104  OVERFITTING IN M ODEL S ELECTION AND S ELECTION B IAS IN P ERFORMANCE E VALUATION  G. C. Cawley and N. L. C. Talbot. Efﬁcient approximate leave-one-out cross-validation for kernel logistic regression. Machine Learning, 71(2–3):243–264, June 2008. doi: 10.1007/ s10994-008-5055-9. G. C. Cawley, G. J. Janacek, and N. L. C. Talbot. Generalised kernel machines. In Proceedings of the IEEE/INNS International Joint Conference on Neural Networks (IJCNN-07), pages 1720–1725, Orlando, Florida, USA, August 12–17 2007. doi: 10.1109/IJCNN.2007.4371217. O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector machines. Machine Learning, 46(1–3):131–159, January 2002. doi: 10.1023/A: 1012450327387. H. Chen, P. Tino, and X. Yao. Probabilistic classiﬁcation vector machines. IEEE Transactions on Neural Networks, 20(6):901–914, June 2009. doi: 10.1109/TNN.2009.2014161. W. Chu, S. S. Keerthi, and C. J. Ong. Bayesian trigonometric support vector classiﬁer. Neural Computation, 15(9):2227–2254, September 2003. doi: 10.1162/089976603322297368. J. Demˇar. Statistical comparisons of classiﬁers over multiple data sets. Journal of Machine Learns ing Research, 7:1–30, 2006. R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁcation. John Wiley and Sons, second edition, 2001. B. Efron and R. J. Tibshirani. Introduction to the Bootstrap. Monographs on Statistics and Applied Probability. Chapman & Hall, 1994. S. Geman, E. Bienenstock, and R. Doursat. Neural networks and the bias/variance dilemma. Neural Computation, 4(1):1–58, January 1992. doi: 10.1162/neco.1992.4.1.1. C. Gold, A. Holub, and P. Sollich. Bayesian approach to feature selection and parameter tuning for support vector machine classiﬁers. Neural Networks, 18(5):693–701, July/August 2005. doi: 10.1016/j.neunet.2005.06.044. I. Guyon, A. Saffari, G. Dror, and G. Cawley. Model selection: Beyond the Bayesian/frequentist divide. Journal of Machine Learning Research, 11:61–87, 2009. P. Hall and A. P. Robinson. Reducing the variability of crossvalidation for smoothing parameter choice. Biometrika, 96(1):175–186, March 2009. doi: doi:10.1093/biomet/asn068. M. Kearns and D. Ron. Algorithmic stability and sanity-check bounds for leave-one-out cross-validation. Neural Computation, 11(6):1427–1453, August 1999. doi: 10.1162/ 089976699300016304. G. S. Kimeldorf and G. Wahba. Some results on Tchebychefﬁan spline functions. Journal of Mathematical Analysis and Applications, 33:82–95, 1971. S. R. Kulkarni, G. Lugosi, and S. S. Venkatesh. Learning pattern classiﬁcation — a survey. IEEE Transactions on Information Theory, 44(6):2178–2206, October 1998. 2105  C AWLEY AND TALBOT  P. A. Lachenbruch and M. R. Mickey. Estimation of error rates in discriminant analysis. Technometrics, 10(1):1–12, February 1968. A. Luntz and V. Brailovsky. On estimation of characters obtained in statistical procedure of recognition (in Russian). Techicheskaya Kibernetica, 3, 1969. D. J. C. MacKay. Bayesian interpolation. Neural Computation, 4(3):415–447, May 1992. doi: 10.1162/neco.1992.4.3.415. J. Mercer. Functions of positive and negative type and their connection with the theory of integral equations. Philosophical Transactions of the Royal Society of London, Series A, 209:415–446, 1909. S. Mika, G. R¨ tsch, J. Weston, B. Sch¨ lkopf, and K.-R. M¨ ller. Fisher discriminant analysis with a o u kernels. In Neural Networks for Signal Processing IX, Proceedings of the 1999 IEEE Signal Processing Society Workshop, pages 41–48, Maddison, WI, USA, 21–25 August 1999. doi: 10.1109/NNSP.1999.788121. S. Mika, G. R¨ tsch, J. Weston, B. Sch¨ lkpf, and K.-R. M¨ ller. Contructing descriptive and disa o u criminative nonlinear features: Rayleigh coefﬁcients in kernel feature spaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(5):623–628, May 2003. doi: 10.1109/TPAMI. 2003.1195996. J. A. Nelder and R. Mead. A simplex method for function minimization. Computer Journal, 7: 308–313, 1965. T. Pe˜ a Centeno and Lawrence N. D. Optimising kernel parameters and regularisation coefﬁcients n for non-linear discriminant analysis. Journal of Machine Learning Research, 7:455–491, February 2006. T. Poggio and F. Girosi. Networks for approximation and learning. Proceedings of the IEEE, 78(9): 1481–1497, September 1990. doi: 10.1109/5.58326. Y. Qi, T. P. Minka, R. W. Picard, and Z. Ghahramani. Predictive automatic relevance determination by expectation propagation. In Proceedings of the Twenty First International Conference on Machine Learning (ICML-04), pages 671–678, Banff, Alberta, Canada, July 4–8 2004. C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. Adaptive Computation and Machine Learning. MIT Press, 2006. G. R¨ tsch, T. Onoda, and K.-R. M¨ ller. Soft margins for AdaBoost. Machine Learning, 42(3): a u 287–320, March 2001. doi: 10.1023/A:1007618119488. R. M. Rifkin and R. A. Lippert. Notes on regularized least squares. Technical Report MIT-CSAILTR-2007-025, Computer Science and Artiﬁcial Intelligence Laboratory, MIT, May 2007. B. D. Ripley. Pattern Recognition and Neural Networks. Cambridge University Press, 1996. C. Saunders, A. Gammerman, and V. Vovk. Ridge regression learning algorithm in dual variables. In Proceedings of the Fifteenth International Conference on Machine Learning (ICML-98), pages 515–521. Morgan Kaufmann, 1998. 2106  OVERFITTING IN M ODEL S ELECTION AND S ELECTION B IAS IN P ERFORMANCE E VALUATION  J. Shao. Linear model selection by cross-validation. Journal of the American Statistical Society, 88:486–494, 1993. I. Stewart. On the optimal parameter choice for ν-support vector machines. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(10):1274–1284, October 2003. doi: 10.1109/ TPAMI.2003.1233901. M. Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the Royal Statistical Society, Series B (Statistical Methodology), 36(2):111–147, 1974. M. Stone. Asymptotics for and against cross-validation. Biometrika, 64(1):29–35, April 1977. doi: 10.1093/biomet/64.1.29. J. A. K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, and J. Vanderwalle. Least Squares Support Vector Machine. World Scientiﬁc Publishing Company, Singapore, 2002. ISBN 981238-151-1. A. N. Tikhonov and V. Y. Arsenin. Solutions of Ill-posed Problems. John Wiley, New York, 1977. G. Toussaint. Bibliography on estimation of misclassiﬁcation. IEEE Transactions on Information Theory, IT-20(4):472–479, July 1974. V. N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer, 1982. V. N. Vapnik. Statistical Learning Theory. Adaptive and learning systems for signal processing, communications and control series. Wiley, 1998. S. Weisberg. Applied Linear Regression. Probability and Mathematical Statistics. John Wiley & Sons, second edition, 1985. J. Weston. Leave-one-out support vector machines. In Proceedings of the Sixteenth International Joint Conference on Artiﬁcial Intelligence (IJCAI-99), pages 727–733, San Fransisco, CA, USA, 1999. Morgan Kaufmann. C. K. I. Williams and D. Barber. Bayesian classiﬁcation with Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342–1351, December 1998. doi: 10.1109/34.735807. P. M. Williams. A Marquardt algorithm for choosing the step size in backpropagation learning with conjugate gradients. Technical Report CSRP-229, University of Sussex, February 1991. T. Zhang. Leave-one-out bounds for kernel machines. Neural Computation, 15(6):1397–1437, June 2003. doi: 10.1162/089976603321780326.  2107</p>
<br/>
<br/><br/><br/></body>
</html>
