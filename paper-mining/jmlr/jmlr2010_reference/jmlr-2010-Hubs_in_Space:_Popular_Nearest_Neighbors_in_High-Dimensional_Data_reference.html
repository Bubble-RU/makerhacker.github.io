<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-49" href="../jmlr2010/jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">jmlr2010-49</a> <a title="jmlr-2010-49-reference" href="#">jmlr2010-49-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</h1>
<br/><p>Source: <a title="jmlr-2010-49-pdf" href="http://jmlr.org/papers/volume11/radovanovic10a/radovanovic10a.pdf">pdf</a></p><p>Author: Miloš Radovanović, Alexandros Nanopoulos, Mirjana Ivanović</p><p>Abstract: Different aspects of the curse of dimensionality are known to present serious challenges to various machine-learning methods and tasks. This paper explores a new aspect of the dimensionality curse, referred to as hubness, that affects the distribution of k-occurrences: the number of times a point appears among the k nearest neighbors of other points in a data set. Through theoretical and empirical analysis involving synthetic and real data sets we show that under commonly used assumptions this distribution becomes considerably skewed as dimensionality increases, causing the emergence of hubs, that is, points with very high k-occurrences which effectively represent “popular” nearest neighbors. We examine the origins of this phenomenon, showing that it is an inherent property of data distributions in high-dimensional vector space, discuss its interaction with dimensionality reduction, and explore its inﬂuence on a wide range of machine-learning tasks directly or indirectly based on measuring distances, belonging to supervised, semi-supervised, and unsupervised learning families. Keywords: nearest neighbors, curse of dimensionality, classiﬁcation, semi-supervised learning, clustering</p><br/>
<h2>reference text</h2><p>Milton Abramowitz and Irene A. Stegun, editors. Handbook of Mathematical Functions With Formulas, Graphs and Mathematical Tables. National Bureau of Standards, USA, 1964. Charu C. Aggarwal and Philip S. Yu. Outlier detection for high dimensional data. In Proceedings of the 27th ACM SIGMOD International Conference on Management of Data, pages 37–46, 2001. 2525  ´ ´ R ADOVANOVI C , NANOPOULOS AND I VANOVI C  Charu C. Aggarwal, Alexander Hinneburg, and Daniel A. Keim. On the surprising behavior of distance metrics in high dimensional spaces. In Proceedings of the 8th International Conference on Database Theory (ICDT), volume 1973 of Lecture Notes in Computer Science, pages 420– 434. Springer, 2001. R´ ka Albert and Albert-L´ szl´ Barab´ si. Statistical mechanics of complex networks. Reviews of e a o a Modern Physics, 74(1):47–97, 2002. Jean-Julien Aucouturier and Francois Pachet. A scale-free distribution of false positives for a large class of audio similarity measures. Pattern Recognition, 41(1):272–284, 2007. Richard E. Bellman. Adaptive Control Processes: A Guided Tour. Princeton University Press, 1961. Adam Berenzweig. Anchors and Hubs in Audio-based Music Similarity. PhD thesis, Columbia University, New York, USA, 2007. Kevin S. Beyer, Jonathan Goldstein, Raghu Ramakrishnan, and Uri Shaft. When is “nearest neighbor” meaningful? In Proceedings of the 7th International Conference on Database Theory (ICDT), volume 1540 of Lecture Notes in Computer Science, pages 217–235. Springer, 1999. Christopher M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1996. Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. Classiﬁcation and Regression Trees. Chapman & Hall, 1984. S´ bastien Bubeck and Ulrike von Luxburg. Nearest neighbor clustering: A baseline method for e consistent clustering with arbitrary objective functions. Journal of Machine Learning Research, 10:657–698, 2009. Rich Caruana, Nikos Karampatziakis, and Ainur Yessenalina. An empirical evaluation of supervised learning in high dimensions. In Proceedings of the 25th International Conference on Machine Learning (ICML), pages 96–103, 2008. Olivier Chapelle, Bernhard Sch¨ lkopf, and Alexander Zien, editors. Semi-Supervised Learning. o The MIT Press, 2006. Jie Chen, Haw ren Fang, and Yousef Saad. Fast approximate kNN graph construction for high dimensional data via recursive Lanczos bisection. Journal of Machine Learning Research, 10: 1989–2012, 2009. Pierre Demartines. Analyse de Donn´ es par R´ seaux de Neurones Auto-Organis´ s. PhD thesis, e e e Institut national polytechnique de Grenoble, Grenoble, France, 1994. George Doddington, Walter Liggett, Alvin Martin, Mark Przybocki, and Douglas Reynolds. SHEEP, GOATS, LAMBS and WOLVES: A statistical analysis of speaker performance in the NIST 1998 speaker recognition evaluation. In Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP), 1998. Paper 0608. 2526  H UBS IN S PACE  Robert J. Durrant and Ata Kab´ n. When is ‘nearest neighbour’ meaningful: A converse theorem a and implications. Journal of Complexity, 25(4):385–397, 2009. Paul Erd˝ s and Alfr´ d R´ nyi. On random graphs. Publicationes Mathematicae Debrecen, 6:290– o e e 297, 1959. Paul F. Evangelista, Mark J. Embrechts, and Boleslaw K. Szymanski. Taming the curse of dimensionality in kernels and novelty detection. In A. Abraham, B. Baets, M. Koppen, and B. Nickolay, editors, Applied Soft Computing Technologies: The Challenge of Complexity, volume 34 of Advances in Soft Computing, pages 425–438. Springer, 2006. Damien Francois. High-dimensional Data Analysis: Optimal Metrics and Feature Selection. PhD ¸ thesis, Universit´ catholique de Louvain, Louvain, Belgium, 2007. e Damien Francois, Vincent Wertz, and Michel Verleysen. The concentration of fractional distances. ¸ IEEE Transactions on Knowledge and Data Engineering, 19(7):873–886, 2007. Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: A statistical view of boosting. The Annals of Statistics, 28(2):337–374, 2000. Yasunori Fujikoshi. Computable error bounds for asymptotic expansions of the hypergeometric function 1 F1 of matrix argument and their applications. Hiroshima Mathematical Journal, 37(1): 13–23, 2007. Hugo G¨ vert, Jarmo Hurri, Jaakko S¨ rel¨ , and Aapo Hyv¨ rinen. The FastICA package for Matlab. a a a a http://www.cis.hut.ﬁ/projects/ica/fastica/, 2005. Xin Geng, De-Chuan Zhan, and Zhi-Hua Zhou. Supervised nonlinear dimensionality reduction for visualization and classiﬁcation. IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 35(6):1098–1107, 2005. David Gleich. MatlabBGL: A Matlab graph library. http://www.stanford.edu/∼dgleich/programs/matlab bgl/, 2008. Uffe Haagerup. The best constants in the Khintchine inequality. Studia Mathematica, 70:231–283, 1982. Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, 2nd edition, 2009. Austin Hicklin, Craig Watson, and Brad Ulery. The myth of goats: How many people have ﬁngerprints that are hard to match? Internal Report 7271, National Institute of Standards and Technology (NIST), USA, 2005. Alexander Hinneburg, Charu C. Aggarwal, and Daniel A. Keim. What is the nearest neighbor in high dimensional spaces? In Proceedings of the 26th International Conference on Very Large Data Bases (VLDB), pages 506–515, 2000. Geoffrey Hinton and Sam Roweis. Stochastic neighbor embedding. In Advances in Neural Information Processing Systems 15, pages 833–840, 2003. 2527  ´ ´ R ADOVANOVI C , NANOPOULOS AND I VANOVI C  Chih-Ming Hsu and Ming-Syan Chen. On the design and applicability of distance functions in high-dimensional data space. IEEE Transactions on Knowledge and Data Engineering, 21(4): 523–536, 2009. Aapo Hyv¨ rinen and Erkki Oja. Independent component analysis: Algorithms and applications. a Neural Networks, 13(4–5):411–430, 2000. Khandoker Tarik-Ul Islam, Kamrul Hasan, Young-Koo Lee, and Sungyoung Lee. Enhanced 1-NN time series classiﬁcation using badness of records. In Proceedings of the 2nd International Conference on Ubiquitous Information Management and Communication, pages 108–113, 2008. Kiyosi Itˆ , editor. Encyclopedic Dictionary of Mathematics. The MIT Press, 2nd edition, 1993. o Tony Jebara, Jun Wang, and Shih-Fu Chang. Graph construction and b-matching for semisupervised learning. In Proceedings of the 26th International Conference on Machine Learning (ICML), pages 441–448, 2009. Norman L. Johnson, Samuel Kotz, and N. Balakrishnan. Continuous Univariate Distributions, volume 1. Wiley, 2nd edition, 1994. Ian T. Jolliffe. Principal Component Analysis. Springer, 2nd edition, 2002. Ioannis Karydis, Miloˇ Radovanovi´ , Alexandros Nanopoulos, and Mirjana Ivanovi´ . Looking s c c through the “glass ceiling”: A conceptual framework for the problems of spectral similarity. In Proceedings of the 11th International Society for Music Information Retrieval Conference (ISMIR), 2010. S. Sathiya Keerthi, Shirish Krishnaj Shevade, Chiranjib Bhattacharyya, and K. R. Krishna Murthy. Improvements to Platt’s SMO algorithm for SVM classiﬁer design. Neural Computation, 13(3): 637–649, 2001. Teuvo Kohonen. Self-Organizing Maps. Springer, 3rd edition, 2001. Filip Korn, Bernd-Uwe Pagel, and Christos Faloutsos. On the “dimensionality curse” and the “selfsimilariy blessing”. IEEE Transactions on Knowledge and Data Engineering, 13(1):96–111, 2001. Ch. Aswani Kumar. Analysis of unsupervised dimensionality reduction techniques. Computer Science and Information Systems, 6(2):217–227, 2009. St´ phane Lafon and Ann B. Lee. Diffusion maps and coarse-graining: A uniﬁed framework for e dimensionality reduction, graph partitioning, and data set parameterization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(9):1393–1403, 2006. Elizaveta Levina and Peter J. Bickel. Maximum likelihood estimation of intrinsic dimension. In Advances in Neural Information Processing Systems 17, pages 777–784, 2005. Lun Li, David Alderson, John C. Doyle, and Walter Willinger. Towards a theory of scale-free graphs: Deﬁnition, properties, and implications. Internet Mathematics, 2(4):431–523, 2005. 2528  H UBS IN S PACE  Laurence T. Maloney. Nearest neighbor analysis of point processes: Simulations and evaluations. Journal of Mathematical Psychology, 27(3):251–260, 1983. Marina Meil˘ and Jianbo Shi. Learning segmentation by random walks. In Advances in Neural a Information Processing Systems 13, pages 873–879, 2001. Boaz Nadler, St´ phane Lafon, Ronald R. Coifman, and Ioannis G. Kevrekidis. Diffusion maps, e spectral clustering and reaction coordinates of dynamical systems. Applied and Computational Harmonic Analysis, 21(1):113–127, 2006. Alexandros Nanopoulos, Miloˇ Radovanovi´ , and Mirjana Ivanovi´ . How does high dimensionality s c c affect collaborative ﬁltering? In Proceedings of the 3rd ACM Conference on Recommender Systems (RecSys), pages 293–296, 2009. Charles M. Newman and Yosef Rinott. Nearest neighbors and Voronoi volumes in high-dimensional point processes with various distance functions. Advances in Applied Probability, 17(4):794–809, 1985. Charles M. Newman, Yosef Rinott, and Amos Tversky. Nearest neighbors and Voronoi regions in certain point processes. Advances in Applied Probability, 15(4):726–751, 1983. Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. In Advances in Neural Information Processing Systems 14, pages 849–856, 2002. Luca Oberto and Francesca Pennecchi. Estimation of the modulus of a complex-valued quantity. Metrologia, 43(6):531–538, 2006. Andrew M. Odlyzko and Neil J. A. Sloane. New bounds on the number of unit spheres that can touch a unit sphere in n dimensions. Journal of Combinatorial Theory, Series A, 26(2):210–214, 1979. Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 271–278, 2004. Mathew Penrose. Random Geometric Graphs. Oxford University Press, 2003. John C. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Sch¨ lkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods—Support o Vector Learning, pages 185–208. MIT Press, 1999. Miloˇ Radovanovi´ and Mirjana Ivanovi´ . Document representations for classiﬁcation of short s c c Web-page descriptions. In Proceedings of the 8th International Conference on Data Warehousing and Knowledge Discovery (DaWaK), volume 4081 of Lecture Notes in Computer Science, pages 544–553. Springer, 2006. Miloˇ Radovanovi´ , Alexandros Nanopoulos, and Mirjana Ivanovi´ . Nearest neighbors in highs c c dimensional data: The emergence and inﬂuence of hubs. In Proceedings of the 26th International Conference on Machine Learning (ICML), pages 865–872, 2009. 2529  ´ ´ R ADOVANOVI C , NANOPOULOS AND I VANOVI C  Miloˇ Radovanovi´ , Alexandros Nanopoulos, and Mirjana Ivanovi´ . On the existence of obstinate s c c results in vector space models. In Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 186–193, 2010a. Miloˇ Radovanovi´ , Alexandros Nanopoulos, and Mirjana Ivanovi´ . Time-series classiﬁcation in s c c many intrinsic dimensions. In Proceedings of the 10th SIAM International Conference on Data Mining (SDM), pages 677–688, 2010b. Gunnar R¨ tsch, Takashi Onoda, and Klaus-Robert M¨ ller. Soft margins for AdaBoost. Machine a u Learning, 42(3):287–320, 2001. Robert E. Schapire and Yoram Singer. Improved boosting algorithms using conﬁdence-rated predictions. Machine Learning, 37(3):297–336, 1999. John P. Scott. Social Network Analysis: A Handbook. Sage Publications, 2nd edition, 2000. Amit Singh, Hakan Ferhatosmano˘ lu, and Ali Saman Tosun. High dimensional reverse nearest g ¸ neighbor queries. In Proceedings of the 12th International Conference on Information and Knowledge Management (CIKM), pages 91–98, 2003. Pang-Ning Tan, Michael Steinbach, and Vipin Kumar. Introduction to Data Mining. Addison Wesley, 2005. Yufei Tao, Dimitris Papadias, Xiang Lian, and Xiaokui Xiao. Multidimensional reverse kNN search. VLDB Journal, 16(3):293–316, 2007. Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. Amos Tversky and John Wesley Hutchinson. Nearest neighbor analysis of psychological spaces. Psychological Review, 93(1):3–22, 1986. Amos Tversky, Yosef Rinott, and Charles M. Newman. Nearest neighbor analysis of point processes: Applications to multidimensional scaling. Journal of Mathematical Psychology, 27(3): 235–250, 1983. Laurens van der Maaten. An introduction to dimensionality reduction using Matlab. Technical Report MICC 07-07, Maastricht University, Maastricht, The Netherlands, 2007. Laurens van der Maaten, Eric Postma, and Jaap van den Herik. Dimensionality reduction: A comparative review. Technical Report TiCC-TR 2009-005, Tilburg University, Tilburg, The Netherlands, 2009. Deepak Verma. SpectraLIB – package for symmetric spectral clustering. http://www.stat.washington.edu/spectral/, 2003. Alexander Vezhnevets. GML AdaBoost Matlab Toolbox. MSU Graphics & Media Lab, Computer Vision Group, http://graphics.cs.msu.ru/, 2006. Nikos Vlassis, Yoichi Motomura, and Ben Kr¨ se. Supervised dimension reduction of intrinsically o low-dimensional data. Neural Computation, 14(1):191–215, 2002. 2530  H UBS IN S PACE  Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation. Journal of Machine Learning Research, 10:207–244, 2009. Ian H. Witten and Eibe Frank. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann Publishers, 2nd edition, 2005. Yi-Ching Yao and Gordon Simons. A large-dimensional independent and identically distributed property for nearest neighbor counts in Poisson processes. Annals of Applied Probability, 6(2): 561–571, 1996. Kenneth Zeger and Allen Gersho. Number of nearest neighbors in a Euclidean code. IEEE Transactions on Information Theory, 40(5):1647–1649, 1994. Daoqiang Zhang, Zhi-Hua Zhou, and Songcan Chen. Semi-supervised dimensionality reduction. In Proceedings of the 7th SIAM International Conference on Data Mining (SDM), pages 629–634, 2007. Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using Gaussian ﬁelds and harmonic functions. In Proceedings of the 20th International Conference on Machine Learning (ICML), pages 912–919, 2003.  2531</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
