<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-54" href="../jmlr2010/jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization.html">jmlr2010-54</a> <a title="jmlr-2010-54-reference" href="#">jmlr2010-54-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</h1>
<br/><p>Source: <a title="jmlr-2010-54-pdf" href="http://jmlr.org/papers/volume11/venna10a/venna10a.pdf">pdf</a></p><p>Author: Jarkko Venna, Jaakko Peltonen, Kristian Nybo, Helena Aidos, Samuel Kaski</p><p>Abstract: Nonlinear dimensionality reduction methods are often used to visualize high-dimensional data, although the existing methods have been designed for other related tasks such as manifold learning. It has been difﬁcult to assess the quality of visualizations since the task has not been well-deﬁned. We give a rigorous deﬁnition for a speciﬁc visualization task, resulting in quantiﬁable goodness measures and new visualization methods. The task is information retrieval given the visualization: to ﬁnd similar data based on the similarities shown on the display. The fundamental tradeoff between precision and recall of information retrieval can then be quantiﬁed in visualizations as well. The user needs to give the relative cost of missing similar points vs. retrieving dissimilar points, after which the total cost can be measured. We then introduce a new method NeRV (neighbor retrieval visualizer) which produces an optimal visualization by minimizing the cost. We further derive a variant for supervised visualization; class information is taken rigorously into account when computing the similarity relationships. We show empirically that the unsupervised version outperforms existing unsupervised dimensionality reduction methods in the visualization task, and the supervised version outperforms existing supervised methods. Keywords: information retrieval, manifold learning, multidimensional scaling, nonlinear dimensionality reduction, visualization</p><br/>
<h2>reference text</h2><p>Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 585–591. MIT Press, Cambridge, MA, 2002a. Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Technical Report TR-2002-01, Department of Computer Science,The University of Chicago, 2002b. Mira Bernstein, Vin de Silva, John C. Langford, and Joshua B. Tenenbaum. Graph approximations to geodesics on embedded manifolds. Technical report, Department of Psychology, Stanford University, 2000. Catherine L. Blake and C. J. Merz. UCI repository of machine learning databases. http://www.ics.uci.edu/∼mlearn/MLRepository.html, 1998. Ingwer Borg and Patrick Groenen. Modern Multidimensional Scaling. Springer, New York, 1997. Hong Chang and Dit-Yan Yeung. Locally linear metric adaptation for semi-supervised clustering. In Proceedings of the Twenty-ﬁrst International Conference on Machine Learning 2004, pages 153–160. Omnipress, Madison, WI, 2004. Lisha Chen and Andreas Buja. Local multidimensional scaling for nonlinear dimension reduction, graph drawing and proximity analysis. Journal of the American Statistical Association, 104(485): 209–219, 2009. Pierre Demartines and Jeanny H´ rault. Curvilinear component analysis: A self-organizing neural e network for nonlinear mapping of data sets. IEEE Transactions on Neural Networks, 8(1):148– 154, 1997. David L. Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data. Proceedings of the National Academy of Sciences, 100:5591–5596, 2003. Xin Geng, De-Chuan Zhan, and Zhi-Hua Zhou. Supervised nonlinear dimensionality reduction for visualization and classiﬁcation. IEEE Transactions on Systems, Man, and Cybernetics–Part B: Cybernetics, 35:1098–1107, 2005. Amir Globerson and Sam Roweis. Metric learning by collapsing classes. In Y. Weiss, B. Sch¨ lkopf, o and J. Platt, editors, Advances in Neural Information Processing 18, pages 451–458. MIT Press, Cambridge, MA, 2006. Jacob Goldberger, Sam Roweis, Geoffrey Hinton, and Ruslan Salakhutdinov. Neighbourhood components analysis. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 513–520. MIT Press, Cambridge, MA, 2005. John C. Gower. Some distance properties of latent root and vector methods used in multivariate analysis. Biometrika, 53:325–338, 1966. 487  V ENNA , P ELTONEN , N YBO , A IDOS AND K ASKI  Rui-jun Gu and Wen-bo Xu. Weighted kernel Isomap for data visualization and pattern classiﬁcation. In Y. Wang, Y.-m. Cheung, and H. Liu, editors, Computational Intelligence and Security (CIS 2006), pages 1050–1057. Springer-Verlag, Berlin Heidelberg, 2007. Geoffrey Hinton and Sam T. Roweis. Stochastic neighbor embedding. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural information processings systems 14, pages 833– 840. MIT Press, Cambridge, MA, 2002. Harold Hotelling. Analysis of a complex of statistical variables into principal components. Journal of Educational Psychology, 24:417–441,498–520, 1933. Tomoharu Iwata, Kazumi Saito, Naonori Ueda, Sean Stromsten, Thomas L. Grifﬁths, and Joshua B. Tenenbaum. Parametric embedding for class visualization. Neural Computation, 19:2536–2556, 2007. Samuel Kaski and Jaakko Peltonen. Informative discriminant analysis. In Proceedings of the Twentieth International Conference on Machine Learning (ICML-2003), pages 329–336. AAAI Press, Menlo Park, CA, 2003. Samuel Kaski and Janne Sinkkonen. Principle of learning metrics for exploratory data analysis. Journal of VLSI Signal Processing, special issue on Machine Learning for Signal Processing, 37: 177–188, 2004. Samuel Kaski, Janne Sinkkonen, and Jaakko Peltonen. Bankruptcy analysis with self-organizing maps in learning metrics. IEEE Transactions on Neural Networks, 12:936–947, 2001. Samuel Kaski, Janne Nikkil¨ , Merja Oja, Jarkko Venna, Petri T¨ r¨ nen, and Eero Castr´ n. Trusta oo e worthiness and metrics in visualizing similarity of gene expression. BMC Bioinformatics, 4:48, 2003. Teuvo Kohonen, Jussi Hynninen, Jari Kangas, Jorma Laaksonen, and Kari Torkkola. LVQ PAK: The learning vector quantization program package. Technical Report A30, Helsinki University of Technology, Laboratory of Computer and Information Science, FIN-02150 Espoo, Finland, 1996. Joseph B. Kruskal. Multidimensional scaling by optimizing goodness of ﬁt to a nonmetric hypothesis. Psychometrica, 29(1):1–27, 1964. John Aldo Lee, Amaury Lendasse, Nicolas Donckers, and Michel Verleysen. A robust nonlinear projection method. In M. Verleysen, editor, ESANN’2000, Eighth European Symposium on Artiﬁcial Neural Networks, pages 13–20. D-Facto Publications, Bruges, Belgium, 2000. John Aldo Lee, Amaury Lendasse, and Michel Verleysen. Nonlinear projection with curvilinear distances: Isomap versus curvilinear distance analysis. Neurocomputing, 57:49–76, 2004. Chun-Guang Li and Jun Guo. Supervised Isomap with explicit mapping. In J.-S. Pan, P. Shi, and Y. Zhao, editors, Proceedings of the First International Conference on Innovative Computing, Information and Control (ICICIC’06), volume 3, pages 345–348. IEEE, 2006. 488  D IMENSIONALITY R EDUCTION FOR V ISUALIZATION  E. Liiti¨ inen and A. Lendasse. Variable scaling for time series prediction: Application to the a ESTSP’07 and the NN3 forecasting competitions. In IJCNN 2007, International Joint Conference on Neural Networks, pages 2812–2816. IEEE, Piscataway, NJ, 2007. Ning Liu, Fengshan Bai, Jun Yan, Benyu Zhang, Zheng Chen, and Wei-Ying Ma. Supervised semideﬁnite embedding for email data cleaning and visualization. In Y. Zhang, K. Tanaka, J. Xu Yu, S. Wang, and M. Li, editors, Web Technologies Research and Development–APWeb 2005, pages 972–982. Springer-Verlag, Berlin Heidelberg, 2005. Roland Memisevic and Geoffrey Hinton. Multiple relational embedding. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 913–920. MIT Press, Cambridge, MA, 2005. Jaakko Peltonen and Samuel Kaski. Discriminative components of data. IEEE Transactions on Neural Networks, 16(1):68–83, 2005. Jaakko Peltonen, Arto Klami, and Samuel Kaski. Improved learning of Riemannian metrics for exploratory analysis. Neural Networks, 17:1087–1100, 2004. Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323–2326, 2000. Eran Segal, Nir Friedman, Daphne Koller, and Aviv Regev. A module map showing conditional activity of expression modules in cancer. Nature Genetics, 36:1090–1098, 2004. Blake Shaw and Tony Jebara. Minimum volume embedding. In M. Meila and X. Shen, editors, Proceedings of AISTATS*07, the 11th International Conference on Artiﬁcial Intelligence and Statistics (JMLR Workshop and Conference Proceedings Volume 2), pages 460-467, 2007. Le Song, Alex Smola, Kersten Borgwardt, and Arthur Gretton. Colored maximum variance unfolding. In J. C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1385–1392. MIT Press, Cambridge, MA, 2008. Andrew I. Su, Michael P. Cooke, Keith A. Ching, Yaron Hakak, John R. Walker, Tim Wiltshire, Anthony P. Orth, Raquel G. Vega, Lisa M. Sapinoso, Aziz Moqrich, Ardem Patapoutian, Garret M. Hampton, Peter G. Schultz, and John B. Hogenesch. Large-scale analysis of the human and mouse transcriptomes. Proceedings of the National Academy of Sciences, 99:4465–4470, 2002. Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290:2319–2323, December 2000. TIMIT. CD-ROM prototype version of the DARPA TIMIT acoustic-phonetic speech database, 1998. Warren S. Torgerson. Multidimensional scaling: I. theory and method. Psychometrika, 17:401–419, 1952. Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9:2579–2605, 2008. 489  V ENNA , P ELTONEN , N YBO , A IDOS AND K ASKI  Laurens van der Maaten, Eric Postma, and Jaap van der Herik. Dimensionality reduction: A comparative review. Technical report 2009–005, Tilburg centre for Creative Computing, Tilburg University, Tilburg, The Netherlands, 2009. Jarkko Venna. Dimensionality Reduction for Visual Exploration of Similarity Structures. PhD thesis, Helsinki University of Technology, Espoo, Finland, 2007. Jarkko Venna and Samuel Kaski. Local multidimensional scaling. Neural Networks, 19:889–99, 2006. Jarkko Venna and Samuel Kaski. Comparison of visualization methods for an atlas of gene expression data sets. Information Visualization, 6:139–154, 2007a. Jarkko Venna and Samuel Kaski. Nonlinear dimensionality reduction as information retrieval. In M. Meila and X. Shen, editors, Proceedings of AISTATS*07, the 11th International Conference on Artiﬁcial Intelligence and Statistics (JMLR Workshop and Conference Proceedings Volume 2), pages 572–579, 2007b. Kilian Weinberger, Benjamin Packer, and Lawrence Saul. Nonlinear dimensionality reduction by semideﬁnite programming and kernel matrix factorization. In R. G. Cowell and Z. Ghahramani, editors, Proceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics (AISTATS 2005), pages 381–388. Society for Artiﬁcial Intelligence and Statistics, 2005. (Available electronically at http://www.gatsby.ucl.ac.uk/aistats/). Kilian Weinberger, John Blitzer, and Lawrence Saul. Distance metric learning for large margin nearest neighbor classiﬁcation. In Y. Weiss, B. Sch¨ lkopf, and J. Platt, editors, Advances in o Neural Information Processing Systems 18, pages 1473–1480. MIT Press, Cambridge, MA, 2006. Kilian Q. Weinberger and Lawrence K. Saul. Unsupervised learning of image manifolds by semidefinite programming. International Journal of Computer Vision, 70:77–90, 2006. Kilian Q. Weinberger, Fei Sha, Qihui Zhu, and Lawrence K. Saul. Graph Laplacian regularization for large-scale semideﬁnite programming. In B. Sch¨ lkopf, J. Platt, and T. Hoffman, editors, Ado vances in Neural Information Processing Systems 19, pages 1489–1496. MIT Press, Cambridge, MA, 2007. Shifeng Weng, Changshui Zhang, and Zhonglin Lin. Exploring the structure of supervised data by Discriminant Isometric Mapping. Pattern Recognition, 38:599–601, 2005. Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stuart Russell. Distance metric learning, with application to clustering with side information. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15. MIT Press, Cambridge, MA, 2003.  490</p>
<br/>
<br/><br/><br/></body>
</html>
