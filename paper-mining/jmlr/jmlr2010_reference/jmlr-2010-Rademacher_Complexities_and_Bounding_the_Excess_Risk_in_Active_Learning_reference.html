<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-95" href="../jmlr2010/jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">jmlr2010-95</a> <a title="jmlr-2010-95-reference" href="#">jmlr2010-95-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</h1>
<br/><p>Source: <a title="jmlr-2010-95-pdf" href="http://jmlr.org/papers/volume11/koltchinskii10a/koltchinskii10a.pdf">pdf</a></p><p>Author: Vladimir Koltchinskii</p><p>Abstract: Sequential algorithms of active learning based on the estimation of the level sets of the empirical risk are discussed in the paper. Localized Rademacher complexities are used in the algorithms to estimate the sample sizes needed to achieve the required accuracy of learning in an adaptive way. Probabilistic bounds on the number of active examples have been proved and several applications to binary classiﬁcation problems are considered. Keywords: active learning, excess risk, Rademacher complexities, capacity function, disagreement coefﬁcient</p><br/>
<h2>reference text</h2><p>M.–F. Balcan, S. Hanneke and J. Wortman. The true sample complexity of active learning. In: Proc. 21st Annual Conference on Learning Theory (COLT 2008), pages 45–56, 2008. M.–F. Balcan, A. Beygelzimer and J. Langford. Agnostic active learning. J. of Computer and System Sciences, 75(1):78–89, 2009. P. Bartlett, S. Boucheron and G. Lugosi. Model selection and error estimation. Machine Learning, 48:85–113, 2002. P. Bartlett, O. Bousquet and S. Mendelson. Local Rademacher complexities. Annals of Statistics, 33(4):1497–1537, 2005. A. Beygelzimer, S. Dasgupta and J. Langford. Importance weighted active learning. In Proceedings of the 26th International Conference on Machine Learning ICML 2009, pages 49–56, Montreal, Canada, 2009. R. Castro, R. Willett and R. Nowak. Faster rates in regression via active learning. In: Advances in Neural Information Processing Systems 18 (NIPS 2005), pages 179–186, 2005. R. Castro and R. Nowak. Minimax bounds for active learning. IEEE Transactions on Information Theory, 54(5):2339–2353, 2008. S. Dasgupta, D. Hsu and C. Monteleoni. A general agnostic active learning algorithm. In: Advances in Neural Information Processing Systems 20 (NIPS 2007), pages 353–360, 2007. L. Devroye, L. Gy¨ rﬁ and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, New o York, 1996. R.M. Dudley. Uniform Central Limit Theorems. Cambridge University Press, 1999. E. Gin´ and V. Koltchinskii. Concentration inequalities and asymptotic results for ratio type empire ical processes. Annals of Probability, 34(3):1143–1216, 2006. E.J. Friedman. Active learning for smooth problems, In Proceeding of the 22nd Annual Conference on Learning Theory, COLT 2009, pages 343–352, Montreal, Canada, 2009. S. Hanneke. Adaptive rates of convergence in active learning. In: Proc. 22nd Annual Conference on Learning Theory, COLT 2009, pages 353–364, Montreal, Canada, 2009a. S. Hanneke. Rates of convergence in active learning. Preprint, 2009b. 2484  E XCESS R ISK IN ACTIVE L EARNING  V. Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions on Information Theory, 47(5):1902–1914, 2001. V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. Annals of Statistics, 34(6):2593–2656, 2006. V. Koltchinskii. Oracle inequalities in empirical risk minimization and sparse recovery problems. Lecture Notes. Ecole d’ete de Probabilit´ s de Saint-Flour 2008, Preprint, 2008. e V. Koltchinskii and D. Panchenko. Rademacher processes and bounding the risk of function learning. In: E. Gin´ , D. Mason and J. Wellner (Eds). High Dimensional Probability II, pages 443–459, e Birkh¨ user, Boston, 2000. a P. Massart and E. Nedelec. Risk bounds for statistical learning. Annals of Statistics, 34(5):2326– 2366, 2006. A. Tsybakov. Optimal aggregation in statistical learning. Annals of Statistics, 32(1):135–166, 2004. A. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes. Springer, New York, 1996.  2485</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
