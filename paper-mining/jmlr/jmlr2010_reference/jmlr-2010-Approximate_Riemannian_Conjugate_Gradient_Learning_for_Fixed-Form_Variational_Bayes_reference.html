<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 jmlr-2010-Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-14" href="../jmlr2010/jmlr-2010-Approximate_Riemannian_Conjugate_Gradient_Learning_for_Fixed-Form_Variational_Bayes.html">jmlr2010-14</a> <a title="jmlr-2010-14-reference" href="#">jmlr2010-14-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 jmlr-2010-Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes</h1>
<br/><p>Source: <a title="jmlr-2010-14-pdf" href="http://jmlr.org/papers/volume11/honkela10a/honkela10a.pdf">pdf</a></p><p>Author: Antti Honkela, Tapani Raiko, Mikael Kuusela, Matti Tornio, Juha Karhunen</p><p>Abstract: Variational Bayesian (VB) methods are typically only applied to models in the conjugate-exponential family using the variational Bayesian expectation maximisation (VB EM) algorithm or one of its variants. In this paper we present an efﬁcient algorithm for applying VB to more general models. The method is based on specifying the functional form of the approximation, such as multivariate Gaussian. The parameters of the approximation are optimised using a conjugate gradient algorithm that utilises the Riemannian geometry of the space of the approximations. This leads to a very efﬁcient algorithm for suitably structured approximations. It is shown empirically that the proposed method is comparable or superior in efﬁciency to the VB EM in a case where both are applicable. We also apply the algorithm to learning a nonlinear state-space model and a nonlinear factor analysis model for which the VB EM is not applicable. For these models, the proposed algorithm outperforms alternative gradient-based methods by a signiﬁcant margin. Keywords: variational inference, approximate Riemannian conjugate gradient, ﬁxed-form approximation, Gaussian approximation</p><br/>
<h2>reference text</h2><p>S. Amari. Differential-Geometrical Methods in Statistics, volume 28 of Lecture Notes in Statistics. Springer-Verlag, Berlin, 1985. S. Amari. Information geometry of the EM and em algorithms for neural networks. Neural Networks, 8(9):1379–1408, 1995. doi: 10.1016/0893-6080(95)00003-8. S. Amari. Natural gradient works efﬁciently in learning. Neural Computation, 10(2):251–276, 1998. doi: 10.1162/089976698300017746. ¯ ˜ 2. We will use the notation γ for the mean and γ for the variance of γ in the approximation for all variables.  3265  H ONKELA , R AIKO , K UUSELA , T ORNIO AND K ARHUNEN  S. Amari and H. Nagaoka. Methods of Information Geometry, volume 191 of Translations of Mathematical Monographs. American Mathematical Society, Providence, RI, USA, 2000. C. Archambeau, M. Opper, Y. Shen, D. Cornford, and J. Shawe-Taylor. Variational inference for diffusion processes. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 17–24. MIT Press, Cambridge, MA, USA, 2008. H. Attias. A variational Bayesian framework for graphical models. In S. Solla, T. Leen, and K.-R. M¨ ller, editors, Advances in Neural Information Processing Systems 12, pages 209–215. MIT u Press, Cambridge, MA, USA, 2000. D. Barber and C. Bishop. Ensemble learning for multi-layer networks. In M. Jordan, M. Kearns, and S. Solla, editors, Advances in Neural Information Processing Systems 10, pages 395–401. The MIT Press, Cambridge, MA, USA, 1998. C. M. Bishop. Pattern Recognition and Machince Learning. Springer, New York, NY, USA, 2006. R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientiﬁc Computing, 16(5):1190–1208, 1995. doi: 10.1137/ 0916069. P. Carbonetto. A MATLAB interface for L-BFGS-B. http://people.cs.ubc.ca/˜pcarbo/ lbfgsb-for-matlab.html, March 2007. A. I. Cohen. Rate of convergence of several conjugate gradient algorithms. SIAM Journal of Numerical Analysis, 9(2):248–259, 1972. doi: 10.1137/0709024. A. Edelman, T. A. Arias, and S. T. Smith. The geometry of algorithms with orthogonality constraints. SIAM Journal on Matrix Analysis and Applications, 20(2):303–353, 1998. doi: 10.1137/S0895479895290954. Z. Ghahramani and M. Beal. Propagation algorithms for variational Bayesian learning. In T. Leen, T. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 507–513. The MIT Press, Cambridge, MA, USA, 2001. M. Girolami and B. Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods. J. of the Royal Statistical Society, Series B (Methodological), 2011. In press. G. H. Golub and C. F. Van Loan. Matrix Computations. Johns Hopkins University Press, Baltimore, MD, USA, 3rd edition, 1996. A. Gonz´ lez and J. R. Dorronsoro. Natural conjugate gradient training of multilayer perceptrons. a Neurocomputing, 71(13–15):2499–2506, 2008. doi: 10.1016/j.neucom.2007.11.035. A. Honkela and H. Valpola. Unsupervised variational Bayesian learning of nonlinear models. In L. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 593–600. MIT Press, Cambridge, MA, USA, 2005. A. Honkela, H. Valpola, and J. Karhunen. Accelerating cyclic update algorithms for parameter estimation by pattern searches. Neural Processing Letters, 17(2):191–203, 2003. doi: 10.1023/A: 1023655202546. 3266  R IEMANNIAN C ONJUGATE G RADIENT FOR VB  A. Honkela, H. Valpola, A. Ilin, and J. Karhunen. Blind separation of nonlinear mixtures by variational Bayesian learning. Digital Signal Processing, 17(5):914–934, 2007. doi: 10.1016/j.dsp. 2007.02.009. A. Honkela, M. Tornio, T. Raiko, and J. Karhunen. Natural conjugate gradient in variational inference. In Proceedings of the 14th International Conference on Neural Information Processing (ICONIP 2007), volume 4985 of Lecture Notes in Computer Science, pages 305–314, Kitakyushu, Japan, 2008. Springer-Verlag, Berlin. doi: 10.1007/978-3-540-69162-4 32. M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. In M. Jordan, editor, Learning in Graphical Models, pages 105–161. The MIT Press, Cambridge, MA, USA, 1999. M. Kuusela, T. Raiko, A. Honkela, and J. Karhunen. A gradient-based algorithm competitive with variational Bayesian EM for mixture of Gaussians. In Proceedings of the International Joint Conference on Neural Networks, IJCNN 2009, pages 1688–1695, Atlanta, GA, USA, June 2009. doi: 10.1109/IJCNN.2009.5178726. H. Lappalainen and A. Honkela. Bayesian nonlinear independent component analysis by multilayer perceptrons. In M. Girolami, editor, Advances in Independent Component Analysis, pages 93–121. Springer-Verlag, Berlin, 2000. H. Lappalainen and J. Miskin. Ensemble learning. In M. Girolami, editor, Advances in Independent Component Analysis, pages 75–92. Springer-Verlag, Berlin, 2000. M. K. Murray and J. W. Rice. Differential Geometry and Statistics. Chapman & Hall, London, 1993. J. Nocedal. Theory of algorithms for unconstrained optimization. Acta Numerica, 1:199–242, 1992. doi: 10.1017/S0962492900002270. M. Opper and C. Archambeau. The variational Gaussian approximation revisited. Neural Computation, 21(3):786–792, 2009. doi: 10.1162/neco.2008.08-07-592. M. J. D. Powell. Restart procedures for the conjugate gradient method. Mathematical Programming, 12(1):241–254, 1977. doi: 10.1007/BF01593790. T. Raiko, H. Valpola, M. Harva, and J. Karhunen. Building blocks for variational Bayesian learning of latent variable models. Journal of Machine Learning Research, 8(Jan):155–201, January 2007. R. Salakhutdinov and S. T. Roweis. Adaptive overrelaxed bound optimization methods. In Proc. 20th International Conference on Machine Learning (ICML 2003), pages 664–671. AAAI Press, 2003. M. Sato. Online model selection based on the variational Bayes. Neural Computation, 13(7):1649– 1681, 2001. doi: 10.1162/089976601750265045. M. Seeger. Bayesian model selection for support vector machines, Gaussian processes and other kernel classiﬁers. In S. Solla, T. Leen, and K.-R. M¨ ller, editors, Advances in Neural Information u Processing Systems 12, pages 603–609. MIT Press, Cambridge, MA, USA, 2000. 3267  H ONKELA , R AIKO , K UUSELA , T ORNIO AND K ARHUNEN  S. T. Smith. Geometric Optimization Methods for Adaptive Filtering. PhD thesis, Harvard University, Cambridge, MA, USA, 1993. T. Tanaka. Information geometry of mean-ﬁeld approximation. In Manfred Opper and David Saad, editors, Advanced Mean Field Methods: Theory and Practice, pages 259–273. The MIT Press, Cambridge, MA, USA, 2001. M. K. Titsias and N. D. Lawrence. Bayesian Gaussian process latent variable model. In Y. W. Teh and M. Titterington, editors, Proceedings of the Thirteenth International Workshop on Artiﬁcial Intelligence and Statistics, volume 9, pages 844–851, Chia Laguna Resort, Sardinia, Italy, 2010. JMLR W&CP; 9. H. Valpola. Bayesian Ensemble Learning for Nonlinear Factor Analysis. PhD thesis, Helsinki University of Technology, Espoo, Finland, 2000. Published in Acta Polytechnica Scandinavica, Mathematics and Computing Series No. 108. H. Valpola and J. Karhunen. An unsupervised ensemble learning method for nonlinear dynamic state-space models. Neural Computation, 14(11):2647–2692, 2002. doi: 10.1162/ 089976602760408017. H. Valpola, M. Harva, and J. Karhunen. Hierarchical models of variance sources. Signal Processing, 84(2):267–282, 2004. doi: 10.1016/j.sigpro.2003.10.014. J. Winn and C. M. Bishop. Variational message passing. Journal of Machine Learning Research, 6: 661–694, April 2005.  3268</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
