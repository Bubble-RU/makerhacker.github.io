<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-92" href="../jmlr2010/jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">jmlr2010-92</a> <a title="jmlr-2010-92-reference" href="#">jmlr2010-92-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</h1>
<br/><p>Source: <a title="jmlr-2010-92-pdf" href="http://jmlr.org/papers/volume11/ilin10a/ilin10a.pdf">pdf</a></p><p>Author: Alexander Ilin, Tapani Raiko</p><p>Abstract: Principal component analysis (PCA) is a classical data analysis technique that Ä?Ĺš nds linear transformations of data that retain the maximal amount of variance. We study a case where some of the data values are missing, and show that this problem has many features which are usually associated with nonlinear models, such as overÄ?Ĺš tting and bad locally optimal solutions. A probabilistic formulation of PCA provides a good foundation for handling missing values, and we provide formulas for doing that. In case of high dimensional and very sparse data, overÄ?Ĺš tting becomes a severe problem and traditional algorithms for PCA are very slow. We introduce a novel fast algorithm and extend it to variational Bayesian learning. Different versions of PCA are compared in artiÄ?Ĺš cial experiments, demonstrating the effects of regularization and modeling of posterior variance. The scalability of the proposed algorithm is demonstrated by applying it to the NetÄ?Ĺš&sbquo;ix problem. Keywords: principal component analysis, missing values, overÄ?Ĺš tting, regularization, variational Bayes</p><br/>
<h2>reference text</h2><p>T. W. Anderson. Maximum likelihood estimates for a multivariate normal distribution when some observations are missing. Journal of the American Statistical Association, 52:200Ă˘&euro;&ldquo;203, 1957. R. Bell and Y. Koren. Scalable collaborative Ä?Ĺš ltering with jointly derived neighborhood interpolation weights. In Proceedings of the IEEE International Conference on Data Mining (ICDM 2007), 2007. R. Bell, Y. Koren, and C. Volinsky. The BellKor solution to the NetÄ?Ĺš&sbquo;ix Prize. Available at http: //www.netflixprize.com/, 2007. C. M. Bishop. Variational principal components. In Proceedings of the 9th International Conference on ArtiÄ?Ĺš cial Neural Networks (ICANN99), pages 509Ă˘&euro;&ldquo;514, 1999. C. M. Bishop. Pattern Recognition and Machince Learning. Springer, Cambridge, 2006. W. J. Boscardin and X. Zhang. Modeling the covariance and correlation matrix of repeated measures. In A. Gelman and X.-L. Meng, editors, Applied Bayesian Modeling and Causal Inference from an Incomplete-Data Perspective. John Wiley & Sons, New York, 2004. A. Christoffersson. The One Component Model with Incomplete Data. PhD thesis, Uppsala University, 1970. A. Cichocki and S. Amari. Adaptive Blind Signal and Image Processing - Learning Algorithms and Applications. Wiley, 2002. C. L. de Ligny, G. H. E. Nieuwdorp, W. K. Brederode, W. E. Hammers, and J. C. van Houwelingen. An application of factor analysis with missing data. Technometrics, 23(1):91Ă˘&euro;&ldquo;95, 1981. R. E. Dear. A principal components missing data method for multiple regression models. Technical Report SP-86, Santa Monica: Systems Development Corporation, 1959. A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B (Methodological), 39(1):1Ă˘&euro;&ldquo;38, 1977. 1997  I LIN AND R AIKO  K. Diamantaras and S. Kung. Principal Component Neural Networks - Theory and Application. Wiley, 1996. S. Funk. NetÄ?Ĺš&sbquo;ix update: Try this at home. Available at http://sifter.org/Ă&lsaquo;&oelig;simon/journal/ 20061211.html, December 2006. Z. Ghahramani and M. I. Jordan. Learning from incomplete data. Technical report CBCL-108, Massachusetts Institute of Technology, 1994. B. Grung and R. Manne. Missing values in principal component analysis. Chemometrics and Intelligent Laboratory Systems, 42(1):125Ă˘&euro;&ldquo;139, 1998. S. Haykin. Modern Filters. Macmillan, 1989. G. E. Hinton and D. van Camp. Keeping neural networks simple by minimizing the description length of the weights. In Proceedings of the 6th Annual ACM Conference on Computational Learning Theory, pages 5Ă˘&euro;&ldquo;13, Santa Cruz, CA, USA, 1993. P. Hoff. Model averaging and dimension selection for the singular value decomposition. Journal of the American Statistical Association, 102(478):674Ă˘&euro;&ldquo;685, 2008. T. Hofmann. Latent semantic models for collaborative Ä?Ĺš ltering. ACM Transactions on Information Systems, 22(1):89Ă˘&euro;&ldquo;115, 2004. A. Honkela, H. Valpola, and J. Karhunen. Accelerating cyclic update algorithms for parameter estimation by pattern searches. Neural Processing Letters, 17(2):191Ă˘&euro;&ldquo;203, 2003. A. Ilin and H. Valpola. On the effect of the form of the posterior approximation in variational learning of ICA models. Neural Processing Letters, 22(2):183Ă˘&euro;&ldquo;204, 2005. I. T. Jolliffe. Principal Component Analysis. Springer-Verlag, 2nd edition, 2002. A. Kaplan, Y. Kushnir, M. Cane, and M. Blumenthal. Reduced space optimal analysis for historical datasets: 136 years of Atlantic sea surface temperatures. Journal of Geophysical Research, 102: 27835Ă˘&euro;&ldquo;27860, 1997. Y. J. Lim and Y. W. Teh. Variational Bayesian approach to movie rating prediction. In Proceedings of of KDD Cup and Workshop 2007, held during the Thirteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Jose, California, 2007. R. J. A. Little and D. B. Rubin. Statistical Analysis with Missing Data. J. Wiley & Sons, 1987. J. Luttinen and A. Ilin. Transformations in variational Bayesian factor analysis to speed up learning. Neurocomputing, 73(7Ă˘&euro;&ldquo;9):1093Ă˘&euro;&ldquo;1102, 2010. T. Minka. Automatic choice of dimensionality for PCA. In V. Tresp T. Leen, T. Dietterich, editor, Advances in Neural Information Processing Systems 13, pages 598Ă˘&euro;&ldquo;604. MIT Press, Cambridge, MA, USA, 2001. R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiÄ?Ĺš es incremental, sparse, and other variants. In Michael I. Jordan, editor, Learning in Graphical Models, pages 355Ă˘&euro;&ldquo;368. The MIT Press, Cambridge, MA, USA, 1999. 1998  P RINCIPAL C OMPONENT A NALYSIS WITH M ISSING VALUES  NetÄ?Ĺš&sbquo;ix. NetÄ?Ĺš&sbquo;ix prize webpage, 2007. http://www.netflixprize.com/. S. Oba, M. Sato, I. Takemasa, M. Monden, K. Matsubara, and S. Ishii. A Bayesian missing value estimation method for gene expression proÄ?Ĺš le data. Bioinformatics, 19(16):2088Ă˘&euro;&ldquo;2096, 2003. E. Oja. Subspace Methods of Pattern Recognition. Research Studies Press and J. Wiley, 1983. A. Paterek. Improving regularized singular value decomposition for collaborative Ä?Ĺš ltering. In Proceedings of KDD Cup and Workshop, 2007. K. Pearson. On lines and planes of closest Ä?Ĺš t to systems of points in space. Philosophical Magazine, 2(6):559Ă˘&euro;&ldquo;572, 1901. T. Raiko and H. Valpola. Missing values in nonlinear factor analysis. In Proceedings of the 8th International Conference on Neural Information Processing (ICONIPĂ˘&euro;&trade;01), pages 822Ă˘&euro;&ldquo;827, Shanghai, 2001. T. Raiko, A. Ilin, and J. Karhunen. Principal component analysis for large scale problems with lots of missing values. In Proceedings of the 18th European Conference on Machine Learning (ECML 2007), pages 691Ă˘&euro;&ldquo;698, Warsaw, Poland, 2007a. T. Raiko, H. Valpola, M. Harva, and J. Karhunen. Building blocks for variational Bayesian learning of latent variable models. Journal of Machine Learning Research, 8:155Ă˘&euro;&ldquo;201, 2007b. T. Raiko, A. Ilin, and J. Karhunen. Principal component analysis for sparse high-dimensional data. In Proceedings of the 14th International Conference on Neural Information Processing (ICONIP 2007), pages 566Ă˘&euro;&ldquo;575, Kitakyushu, Japan, 2008. S. Roweis. EM algorithms for PCA and SPCA. In Advances in Neural Information Processing Systems 10, pages 626Ă˘&euro;&ldquo;632, Cambridge, MA, 1998. MIT Press. R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In Proceedings of the 25th International Conference on Machine Learning (ICML2008), Helsinki, Finland, 2008a. R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In Advances in Neural Information Processing Systems 20, 2008b. R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted Boltzmann machines for collaborative Ä?Ĺš ltering. In Proceedings of the 24th International Conference on Machine Learning (ICML-2007), 2007. N. Srebro and T. Jaakkola. Weighted low-rank approximations. In Proceedings of the 20th International Conference on Machine Learning (ICML-2003), Washington DC, 2003. The Ensemble. Website of the runner-up in the NetÄ?Ĺš&sbquo;ix competition, 2009. the-ensemble.com/.  http://www.  M. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611Ă˘&euro;&ldquo;622, 1999. 1999  I LIN AND R AIKO  C. S. Wallace. ClassiÄ?Ĺš cation by minimum-message-length inference. In S. G. Aki, F. Fiala, and W. W. Koczkodaj, editors, Advances in Computing and Information Ă˘&euro;&ldquo; ICCI Ă˘&euro;&trade;90, volume 468 of Lecture Notes in Computer Science, pages 72Ă˘&euro;&ldquo;81. Springer, Berlin, 1990. T. Wiberg. Computation of principal components when data are missing. In Johannes Gordesch and Peter Naeve, editors, COMPSTAT 1976: Proceedings in Computational Statistics, 2nd symposium held in Berlin (West), pages 229Ă˘&euro;&ldquo;236. Wien: Physica-Verlag, 1976. G. Young. Maximum likelihood estimation and factor analysis. Psychometrika, 6(1):49Ă˘&euro;&ldquo;53, 1941.  2000</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
