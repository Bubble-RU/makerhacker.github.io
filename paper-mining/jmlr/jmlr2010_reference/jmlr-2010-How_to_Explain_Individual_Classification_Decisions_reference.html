<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>48 jmlr-2010-How to Explain Individual Classification Decisions</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-48" href="../jmlr2010/jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">jmlr2010-48</a> <a title="jmlr-2010-48-reference" href="#">jmlr2010-48-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>48 jmlr-2010-How to Explain Individual Classification Decisions</h1>
<br/><p>Source: <a title="jmlr-2010-48-pdf" href="http://jmlr.org/papers/volume11/baehrens10a/baehrens10a.pdf">pdf</a></p><p>Author: David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, Klaus-Robert Müller</p><p>Abstract: After building a classiﬁer with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most inﬂuential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classiﬁcation method. Keywords: explaining, nonlinear, black box model, kernel methods, Ames mutagenicity</p><br/>
<h2>reference text</h2><p>B. N. Ames, E. G. Gurney, J. A. Miller, and H. Bartsch. Carcinogens as frameshift mutagens: Metabolites and derivatives of 2-acetylaminoﬂuorene and other aromatic amine carcinogens. Proceedings of the National Academy of Sciences of the United States of America, 69(11):3128–3132, 1972. C.M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995. L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Number 31 o in Applications of Mathematics. Springer, New York, 1996. R.A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7: 179–188, 1936. R. Fraud and F. Clrot. A methodology to explain neural network classiﬁcation. Neural Networks, 15(2):237 – 246, 2002. doi: 10.1016/S0893-6080(01)00127-7. H. Glatt, R. Jung, and F. Oesch. Bacterial mutagenicity investigation of epoxides: drugs, drug metabolites, steroids and pesticides. Mutation Research/Fundamental and Molecular Mechanisms of Mutagenesis, 111(2):99–118, 1983. doi: 10.1016/0027-5107(83)90056-8. I. Guyon and A. Elisseeff. An introduction to variable and feature selection. The Journal of Machine Learning Research, 3:1157–1182, 2003. F. R. Hampel, E. M. Ronchetti, P. J. Rousseeuw, and W. A. Stahel. Robust Statistics: The Approach Based on Inﬂuence Functions. Wiley, New York, 1986. K. Hansen, S. Mika, T. Schroeter, A. Sutter, A. Ter Laak, T. Steger-Hartmann, N. Heinrich, and K.-R. M¨ ller. A benchmark data set for in silico prediction of ames mutagenicity. Journal of u Chemical Information and Modelling, 49(9):2077–2081, 2009. T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer, 2001. E. J. Horvitz, J. S. Breese, and M. Henrion. Decision theory in expert systems and artiﬁcial inteligence. Journal of Approximation Reasoning, 2:247–302, 1988. Special Issue on Uncertainty in Artiﬁcial Intelligence. D. H. Johnson and S. Sinanovic. Symmetrizing the Kullback-Leibler distance. Technical report, IEEE Transactions on Information Theory, 2000. J. Kazius, R. McGuire, and R. Bursi. Derivation and validation of toxicophores for mutagenicity prediction. J. Med. Chem., 48:312–320, 2005. W. Kienzle, M. O. Franz, B. Sch¨ lkopf, and F. A. Wichmann. Center-surround patterns emerge as o optimal predictors for human saccade targets. Journal of Vision, 9(5):1–15, 2009. 1829  ¨ BAEHRENS , S CHROETER , H ARMELING , K AWANABE , H ANSEN AND M ULLER  M. Kuss and C. E. Ramussen. Assesing approximate inference for binary gaussian process classiﬁcation. Journal of Machine Learning Research, 6:1679–1704, 2005. Y. LeCun, L. Bottou, G.B. Orr, and K.-R. M¨ ller. Efﬁcient backprop. In G.B. Orr and K.-R. M¨ ller, u u editors, Neural Networks: Tricks of the Trade, pages 9–53. Springer, 1998. V. Lemaire and R. Feraud. Une m´ thode d’interpr´ tation de scores. In EGC, pages 191–192, 2007. e e K.-R. M¨ ller, S. Mika, G. R¨ tsch, K. Tsuda, and B. Sch¨ lkopf. An introduction to kernel-based u a o learning algorithms. Neural Networks, IEEE Transactions on, 12(2):181–201, 2001. Olga Obrezanova and Matthew D. Segall. Gaussian processes for classiﬁcation: QSAR modeling of ADMET and target activity. Journal of Chemical Information and Modeling, April 2010. ISSN 1549-9596. doi: doi:10.1021/ci900406x. URL http://dx.doi.org/10.1021/ci900406x. O. Obrezanova, G. Cs´ nyi, J. M. R. Gola, and M. D. Segall. Gaussian processes: A method for a automatic QSAR modelling of adme properties. J. Chem. Inf. Model. O. Obrezanova, J. M. R. Gola, E. J. Champness, and M. D. Segall. Automatic QSAR modeling of adme properties: blood-brain barrier penetration and aqueous solubility. J. Comput.-Aided Mol. Des., 22:431–440, 2008. J. C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classiﬁers, pages 61–74. MIT Press, 1999. J. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset Shift in Machine Learning. The MIT Press, 2009. C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. Springer, 2006. M. Robnik-Sikonja and I. Kononenko. Explaining classiﬁcations for individual instances. IEEE TKDE, 20(5):589–600, 2008. B. Sch¨ lkopf and A. Smola. Learning with Kernels. MIT, 2002. o T. Schroeter, A. Schwaighofer, S. Mika, A. Ter Laak, D. Suelzle, U. Ganzer, N. Heinrich, and K.-R. M¨ ller. Estimating the domain of applicability for machine learning QSAR models: A study on u aqueous solubility of drug discovery molecules. Journal of Computer Aided Molecular Design, 21(9):485–498, 2007a. T. Schroeter, A. Schwaighofer, S. Mika, A. Ter Laak, D. Suelzle, U. Ganzer, N. Heinrich, and K.-R. M¨ ller. Machine learning models for lipophilicity and their domain of applicability. Mol. Pharm., u 4(4):524–538, 2007b. T. Schroeter, A. Schwaighofer, S. Mika, A. Ter Laak, D. S¨ lzle, U. Ganzer, N. Heinrich, and K.u R. M¨ ller. Predicting lipophilicity of drug discovery molecules using gaussian process models. u ChemMedChem, 2(9):1265–1267, 2007c. A. Schwaighofer. SVM Toolbox for Matlab, Jan 2002. URL http://ida.first.fraunhofer. de/˜anton/software.html. 1830  H OW TO E XPLAIN I NDIVIDUAL C LASSIFICATION D ECISIONS  A. Schwaighofer, T. Schroeter, S. Mika, J. Laub, A. Ter Laak, D. S¨ lzle, U. Ganzer, N. Heinrich, u and K.-R. M¨ ller. Accurate solubility prediction with error bars for electrolytes: A machine u learning approach. Journal of Chemical Information and Modelling, 47(2):407–424, 2007. A. Schwaighofer, T. Schroeter, S. Mika, K. Hansen, A. Ter Laak, P. Lienau, A. Reichel, N. Heinrich, and K.-R. M¨ ller. A probabilistic approach to classifying metabolic stability. Journal of Chemical u Information and Modelling, 48(4):785–796, 2008. S. Sonnenburg, A. Zien, P. Philips, and G. R¨ tsch. POIMs: positional oligomer importance matrices a — understanding support vector machine based signal detectors. Bioinformatics, 2008. E. Strumbelj and I. Kononenko. Towards a model independent method for explaining classiﬁcation for individual instances. In I.-Y. Song, J. Eder, and T.M. Nguyen, editors, Data Warehousing and Knowledge Discovery, volume 5182 of Lecture Notes in Computer Science, pages 273–282. Springer, 2008. H. Suermondt. Explanation in Bayesian Belief Networks. PhD thesis, Department of Computer Science and Medicine, Stanford University, Stanford, CA, 1992. M. Sugiyama, M. Krauledat, and K.-R. M¨ ller. Covariate shift adaptation by importance weighted u cross validation. Journal of Machine Learning Research, 8:985–1005, May 2007a. M. Sugiyama, S. Nakajima, H. Kashima, P. von Buenau, and M. Kawanabe. Direct importance estimation with model selection and its application to covariate shift adaptation. In Advances in Neural Information Processing Systems 20. MIT Press, 2007b. R. Todeschini, V. Consonni, A. Mauri, and M. Pavan. D RAGON for Windows and Linux 2006. http://www.talete.mi.it/help/dragon_help/ (accessed 27 March 2009), 2006. V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995. P. von B¨ nau, F. C. Meinecke, F. J. Kir´ ly, and K.-R. M¨ ller. Finding stationary subspaces in u a u multivariate time series. Physical Review Letters, 103(21):214101, 2009.  1831</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
