<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-12" href="../jmlr2010/jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">jmlr2010-12</a> <a title="jmlr-2010-12-reference" href="#">jmlr2010-12-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</h1>
<br/><p>Source: <a title="jmlr-2010-12-pdf" href="http://jmlr.org/papers/volume11/zhang10a/zhang10a.pdf">pdf</a></p><p>Author: Tong Zhang</p><p>Abstract: We consider learning formulations with non-convex objective functions that often occur in practical applications. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to a sub-optimal solution in reality. This paper tries to remedy the above gap between theory and practice. In particular, we present a multi-stage convex relaxation scheme for solving problems with non-convex objective functions. For learning formulations with sparse regularization, we analyze the behavior of a speciﬁc multistage relaxation scheme. Under appropriate conditions, we show that the local solution obtained by this procedure is superior to the global solution of the standard L1 convex relaxation for learning sparse targets. Keywords: sparsity, non-convex optimization, convex relaxation, multi-stage convex relaxation</p><br/>
<h2>reference text</h2><p>Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil, and Yiming Ying. A spectral regularization framework for multi-task structure learning. In NIPS’07, 2008. Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classiﬁcation, and risk bounds. Journal of the American Statistical Association, 101(473):138–156, 2006. Peter Bickel, Yaacov Ritov, and Alexandre Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Annals of Statistics, 37(4):1705–1732, 2009. Florentina Bunea, Alexandre Tsybakov, and Marten H. Wegkamp. Sparsity oracle inequalities for the Lasso. Electronic Journal of Statistics, 1:169–194, 2007. Emmanuel J. Candes and Terence Tao. Decoding by linear programming. IEEE Trans. on Information Theory, 51:4203–4215, 2005. Emmanuel J. Candes, Michael B. Wakin, and Stephen P. Boyd. Enhancing sparsity by reweighted l1 minimization. Journal of Fourier Analysis and Applications, 14(5):877–905, 2008. Runze Li Jianqing Fan. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96:1348–1360, 2001. 1106  M ULTISTAGE C ONVEX R ELAXATION  Vladimir Koltchinskii. Sparsity in penalized empirical risk minimization. Annales de l’Institut Henri Poincar´ , 2008. e Gilles Pisier. The Volume of Convex Bodies and Banach Space Geometry. Cambridge University Press, 1989. R. Tyrrell Rockafellar. Convex Analysis. Princeton University Press, Princeton, NJ, 1970. Alan L. Yuille and Anand Rangarajan. The concave-convex procedure. Neural Computation, 15: 915–936, 2003. Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 2010. to appear. Cun-Hui Zhang and Jian Huang. The sparsity and bias of the Lasso selection in high-dimensional linear regression. Annals of Statistics, 36(4):1567–1594, 2008. Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. The Annals of Statistics, 32:56–85, 2004. with discussion. Tong Zhang. Some sharp performance bounds for least squares regression with L1 regularization. Ann. Statist., 37(5A):2109–2144, 2009a. ISSN 0090-5364. doi: 10.1214/08-AOS659. Tong Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models. In NIPS’08, 2009b. Peng Zhao and Bin Yu. On model selection consistency of Lasso. Journal of Machine Learning Research, 7:2541–2567, 2006. Hui Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical Association, 101:1418–1429, 2006. Hui Zou and Runze Li. One-step sparse estimates in nonconcave penalized likelihood models. Annals of Statistics, 36(4):1509–1533, 2008.  1107</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
