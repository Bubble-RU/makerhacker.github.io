<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>41 jmlr-2012-Exploration in Relational Domains for Model-based Reinforcement Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-41" href="../jmlr2012/jmlr-2012-Exploration_in_Relational_Domains_for_Model-based_Reinforcement_Learning.html">jmlr2012-41</a> <a title="jmlr-2012-41-reference" href="#">jmlr2012-41-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>41 jmlr-2012-Exploration in Relational Domains for Model-based Reinforcement Learning</h1>
<br/><p>Source: <a title="jmlr-2012-41-pdf" href="http://jmlr.org/papers/volume13/lang12a/lang12a.pdf">pdf</a></p><p>Author: Tobias Lang, Marc Toussaint, Kristian Kersting</p><p>Abstract: A fundamental problem in reinforcement learning is balancing exploration and exploitation. We address this problem in the context of model-based reinforcement learning in large stochastic relational domains by developing relational extensions of the concepts of the E 3 and R- MAX algorithms. Efﬁcient exploration in exponentially large state spaces needs to exploit the generalization of the learned model: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be a well-known context in which exploitation is promising. To address this we introduce relational count functions which generalize the classical notion of state and action visitation counts. We provide guarantees on the exploration efﬁciency of our framework using count functions under the assumption that we had a relational KWIK learner and a near-optimal planner. We propose a concrete exploration algorithm which integrates a practically efﬁcient probabilistic rule learner and a relational planner (for which there are no guarantees, however) and employs the contexts of learned relational rules as features to model the novelty of states and actions. Our results in noisy 3D simulated robot manipulation problems and in domains of the international planning competition demonstrate that our approach is more effective than existing propositional and factored exploration techniques. Keywords: reinforcement learning, statistical relational learning, exploration, relational transition models, robotics</p><br/>
<h2>reference text</h2><p>Yoshua Bengio, J´ rˆ me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In eo Proc. of the Int. Conf. on Machine Learning (ICML), pages 41–48, 2009. 3764  E XPLORATION IN R ELATIONAL D OMAINS  Mustafa Bilgic, Lilyana Mihalkova, and Lise Getoor. Active learning for networked data. In Proc. of the Int. Conf. on Machine Learning (ICML), 2010. Hendrik Blockeel and Luc de Raedt. Top-down induction of ﬁrst order local decision trees. Artiﬁcial Intelligence Journal, 101:185–297, 1998. Craig Boutilier, Ray Reiter, and Bob Price. Symbolic dynamic programming for ﬁrst-order MDPs. In Proc. of the Int. Conf. on Artiﬁcial Intelligence (IJCAI), pages 690–700, 2001. Ronen I. Brafman and Moshe Tennenholtz. R-max - a general polynomial time algorithm for nearoptimal reinforcement learning. Journal of Machine Learning Research (JMLR), 3:213–231, 2002. Henrik Christensen. From internet to robotics – a roadmap for US robotics, May 2009. http: //www.us-robotics.us/reports/CCC\%20Report.pdf. David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan. Active learning with statistical models. Journal of Artiﬁcial Intelligence Research (JAIR), 4(1):129–145, 1996. Tom Croonenborghs, Jan Ramon, Hendrik Blockeel, and Maurice Bruynooghe. Online learning and exploiting relational models in reinforcement learning. In Proc. of the Int. Conf. on Artiﬁcial Intelligence (IJCAI), pages 726–731, 2007. Luc de Raedt, P. Frasconi, Kristian Kersting, and S.H. Muggleton, editors. Probabilistic Inductive Logic Programming, volume 4911 of Lecture Notes in Computer Science. Springer, 2008. Carlos Diuk. An Object-Oriented Representation for Efﬁcient Reinforcement Learning. PhD thesis, Rutgers, The State University of New Jersey, New Brunswick, NJ, 2010. Carlos Diuk, Andre Cohen, and Michael Littman. An object-oriented representation for efﬁcient reinforcement learning. In Proc. of the Int. Conf. on Machine Learning (ICML), 2008. Carlos Diuk, Lihong Li, and Bethany R. Lefﬂer. The adaptive k-meteorologists problem and its application to structure learning and feature selection in reinforcement learning. In Proc. of the Int. Conf. on Machine Learning (ICML), 2009. Kurt Driessens and Saˇo Dˇ eroski. Integrating guidance into relational reinforcement learning. s z Machine Learning Journal, 57(3):271–304, 2004. Kurt Driessens, Jan Ramon, and Thomas G¨ rtner. Graph kernels and Gaussian processes for relaa tional reinforcement learning. Machine Learning Journal, 64(1-3):91–119, 2006. Saˇo Dˇ eroski, L. de Raedt, and Kurt Driessens. Relational reinforcement learning. Machine s z Learning Journal, 43:7–52, 2001. Arkady Epshteyn, Adam Vogel, and Gerald DeJong. Active reinforcement learning. In Proc. of the Int. Conf. on Machine Learning (ICML), pages 296–303, 2008. Lise Getoor and Ben Taskar, editors. Introduction to Statistical Relational Learning. MIT Press, 2007. 3765  L ANG , T OUSSAINT AND K ERSTING  Carlos Guestrin, Relu Patrascu, and Dale Schuurmans. Algorithm-directed exploration for modelbased reinforcement learning in factored MDPs. In Proc. of the Int. Conf. on Machine Learning (ICML), pages 235–242, 2002. Carlos Guestrin, Daphne Koller, Chris Gearhart, and Neal Kanodia. Generalizing plans to new environments in relational MDPs. In Proc. of the Int. Conf. on Artiﬁcial Intelligence (IJCAI), pages 1003–1010, 2003. Florian Halbritter and Peter Geibel. Learning models of relational MDPs using graph kernels. In Proc. of the Mexican Conf. on AI (MICAI), pages 409–419, 2007. Steffen H¨ lldobler, Eldar Karabaev, and Olga Skvortsova. FluCaP: a heuristic search planner for o ﬁrst-order MDPs. Journal of Artiﬁcial Intelligence Research (JAIR), 27:419–439, 2006. IPPC. Sixth International Planning Competition, Uncertainty Part, 2008. ippc-2008.loria.fr/wiki/index.php/Main\_Page.  URL http://  Saket Joshi, Kristian Kersting, and Roni Khardon. Self-taught decision theoretic planning with ﬁrst order decision diagrams. In Proc. of the Int. Conf. on Automated Planning and Scheduling (ICAPS), 2010. Leslie Pack Kaelbling, Michael Littman, and Andrew Moore. Reinforcement learning: a survey. Journal of Artiﬁcial Intelligence Research (JAIR), 4:237–285, 1996. Sham Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, Gatsby Computational Neuroscience Unit, University College London, 2003. Sham Kakade, Michael Kearns, and John Langford. Exploration in metric state spaces. In Proc. of the Int. Conf. on Machine Learning (ICML), 2003. Michael Kearns and Daphne Koller. Efﬁcient reinforcement learning in factored MDPs. In Proc. of the Int. Conf. on Artiﬁcial Intelligence (IJCAI), pages 740–747, 1999. Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning Journal, 49(2-3):209–232, 2002. Kristian Kersting and Kurt Driessens. Non–parametric policy gradients: A uniﬁed treatment of propositional and relational domains. In Proc. of the Int. Conf. on Machine Learning (ICML), 2008. Kristian Kersting, Martijn van Otterlo, and Luc de Raedt. Bellman goes relational. In Proc. of the Int. Conf. on Machine Learning (ICML), pages 465–472, 2004. J. Zico Kolter and Andrew Ng. Near-Bayesian exploration in polynomial time. In Proc. of the Int. Conf. on Machine Learning (ICML), pages 513–520, 2009. Tobias Lang. Planning and Exploration in Stochastic Relational Worlds. PhD thesis, Fachbereich Mathematik und Informatik, Freie Universit¨ t Berlin, 2011. a Tobias Lang and Marc Toussaint. Relevance grounding for planning in relational domains. In Proc. of the European Conf. on Machine Learning (ECML), 2009. 3766  E XPLORATION IN R ELATIONAL D OMAINS  Tobias Lang and Marc Toussaint. Planning with noisy probabilistic relational rules. Journal of Artiﬁcial Intelligence Research (JAIR), 39:1–49, 2010. Lihong Li. A Unifying Framework for Computational Reinforcement Learning Theory. PhD thesis, Department of Computer Science, Rutgers University, New Brunswick, NJ, USA, 2009. Lihong Li, Michael Littman, Thomas Walsh, and Alexander Strehl. Knows what it knows: a framework for self-aware learning. Machine Learning Journal, 82(3):568–575, 2011. Stephen Muggleton. Learning from positive data. In Selected Papers from the 6th International Workshop on Inductive Logic Programming, pages 358–376, London, UK, 1997. SpringerVerlag. Sriraam Natarajan, Tushar Khot, Kristian Kersting, Bernd Gutmann, and Jude Shavlik. Boosting relational dependency networks. In Proc. of the Int. Conf. on Inductive Logic Programming (ILP), 2010. Jennifer Neville and David Jensen. Relational dependency networks. Journal of Machine Learning Research (JMLR), 8:653–692, 2007. Shan-Hwei Nienhuys-Cheng and Ronald de Wolf, editors. Foundations of Inductive Logic Programming, volume 1228 of Lecture Notes in Computer Science. Springer, 1997. Ali Nouri and Michael L. Littman. Dimension reduction and its application to model-based exploration in continuous spaces. Machine Learning Journal, 81:85–98, October 2010. Hanna M. Pasula, Luke S. Zettlemoyer, and Leslie Pack Kaelbling. Learning symbolic models of stochastic domains. Journal of Artiﬁcial Intelligence Research (JAIR), 29:309–352, 2007. Pascal Poupart, Nikos Vlassis, Jesse Hoey, and Kevin Regan. An analytic solution to discrete Bayesian reinforcement learning. In Proc. of the Int. Conf. on Machine Learning (ICML), pages 697–704, 2006. Jan Ramon. Clustering and Instance-Based Learning in First Order Logic. PhD thesis, Department of Computer Science, K.U.Leuven, Leuven, Belgium, 2002. Jan Ramon, Kurt Driessens, and T. Croonenborghs. Transfer learning in reinforcement learning problems through partial policy recycling. In Proc. of the European Conf. on Machine Learning (ECML), pages 699–707, 2007. Scott Sanner. Simultaneous learning of structure and value in relational reinforcement learning. In Proc. of the ICML-05 Workshop on ”Rich Representations for Relational Reinforcement Learning”, 2005. Scott Sanner. Online feature discovery in relational reinforcement learning. In Proc. of the ICML-06 Workshop on ”Open Problems in Statistical Relational Learning”, 2006. Scott Sanner and Craig Boutilier. Practical solution techniques for ﬁrst-order MDPs. Artiﬁcial Intelligence Journal, 173(5-6):748–788, 2009. 3767  L ANG , T OUSSAINT AND K ERSTING  J¨ rgen Schmidhuber. Curious model-building control systems. In Proc. of Int. Joint Conf. on Neural u Networks, volume 2, pages 1458–1463, 1991. Alexander L. Strehl and Michael L. Littman. Online linear regression and its application to modelbased reinforcement learning. In Proc. of the Conf. on Neural Information Processing Systems (NIPS), pages 737–744, 2007. Alexander L. Strehl, Lihong Li, and Michael Littman. Reinforcement learning in ﬁnite MDPs: PAC analysis. Journal of Machine Learning Research (JMLR), 2009. Sebastian Thrun. The role of exploration in learning control. In Handbook for Intelligent Control: Neural, Fuzzy and Adaptive Approaches. Van Nostrand Reinhold, 1992. Marc Toussaint, Nils Plath, Tobias Lang, and Nikolay Jetchev. Integrated motor control, planning, grasping and high-level reasoning in a blocks world using probabilistic inference. In Proc. of the IEEE Int. Conf. on Robotics & Automation (ICRA), 2010. Thomas J. Walsh. Efﬁcient Learning of Relational Models for Sequential Decision Making. PhD thesis, Rutgers, The State University of New Jersey, New Brunswick, NJ, 2010. Thomas J. Walsh and Michael L. Littman. Efﬁcient learning of action schemas and web-service descriptions. In Proc. of the Nat. Conf. on Artiﬁcial Intelligence (AAAI), pages 714–719, 2008. Thomas J. Walsh, Istvan Szita, Carlos Diuk, and Michael L. Littman. Exploring compact reinforcement-learning representations with linear regression. In Proc. of the Conf. on Uncertainty in Artiﬁcial Intelligence (UAI), 2009. Chenggang Wang, Saket Joshi, and Roni Khardon. First order decision diagrams for relational MDPs. Journal of Artiﬁcial Intelligence Research (JAIR), 31:431–472, 2008. David Windridge and Josef Kittler. Perception-action learning as an epistemologically-consistent model for self-updating cognitive representation. Brain Inspired Cognitive Systems (special issue), Advances in Experimental Medicine and Biology, 657, 2010. Zhao Xu, Kristian Kersting, and Thorsten Joachims. Fast active exploration for link–based preference learning using Gaussian processes. In Proc. of the European Conf. on Machine Learning (ECML), 2010.  3768</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
