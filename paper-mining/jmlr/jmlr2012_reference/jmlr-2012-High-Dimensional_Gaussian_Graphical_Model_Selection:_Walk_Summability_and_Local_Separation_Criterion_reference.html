<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-48" href="../jmlr2012/jmlr-2012-High-Dimensional_Gaussian_Graphical_Model_Selection%3A_Walk_Summability_and_Local_Separation_Criterion.html">jmlr2012-48</a> <a title="jmlr-2012-48-reference" href="#">jmlr2012-48-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>48 jmlr-2012-High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion</h1>
<br/><p>Source: <a title="jmlr-2012-48-pdf" href="http://jmlr.org/papers/volume13/anandkumar12a/anandkumar12a.pdf">pdf</a></p><p>Author: Animashree Anandkumar, Vincent Y.F. Tan, Furong Huang, Alan S. Willsky</p><p>Abstract: We consider the problem of high-dimensional Gaussian graphical model selection. We identify a set of graphs for which an efﬁcient estimation algorithm exists, and this algorithm is based on thresholding of empirical conditional covariances. Under a set of transparent conditions, we establish structural consistency (or sparsistency) for the proposed algorithm, when the number of −2 samples n = Ω(Jmin log p), where p is the number of variables and Jmin is the minimum (absolute) edge potential of the graphical model. The sufﬁcient conditions for sparsistency are based on the notion of walk-summability of the model and the presence of sparse local vertex separators in the underlying graph. We also derive novel non-asymptotic necessary conditions on the number of samples required for sparsistency. Keywords: Gaussian graphical model selection, high-dimensional learning, local-separation property, walk-summability, necessary conditions for model selection</p><br/>
<h2>reference text</h2><p>P. Abbeel, D. Koller, and A.Y. Ng. Learning factor graphs in polynomial time and sample complexity. The Journal of Machine Learning Research, 7:1743–1788, 2006. A. Anandkumar, A. Hassidim, and J. Kelner. Topology discovery of sparse random graphs with few participants. Accepted to J. of Random Structures and Algorithms, Jan. 2012a. A. Anandkumar, V. Y. F. Tan, F. Huang, and A. S. Willsky. High-dimensional structure learning of Ising models: Local separation criterion. Accepted to Annals of Statistics, Jan. 2012b. P.J. Bickel and E. Levina. Covariance regularization by thresholding. The Annals of Statistics, 36 (6):2577–2604, 2008. A. Bogdanov, E. Mossel, and S. Vadhan. The complexity of distinguishing Markov random ﬁelds. Approximation, Randomization and Combinatorial Optimization. Algorithms and Techniques, pages 331–342, 2008. B. Bollob´ s. Random graphs. Academic Press, 1985. a G. Bresler, E. Mossel, and A. Sly. Reconstruction of Markov random ﬁelds from samples: Some observations and algorithms. In Intl. Workshop APPROX Approximation, Randomization and Combinatorial Optimization, pages 343–356. Springer, 2008. J. Cheng, R. Greiner, J. Kelly, D. Bell, and W. Liu. Learning bayesian networks from data: an information-theory based approach. Artiﬁcial Intelligence, 137(1-2):43–90, 2002. M.J. Choi, V.Y.F. Tan, A. Anandkumar, and A. Willsky. Learning latent tree graphical models. J. of Machine Learning Research, 12:1771–1812, May 2011. 2334  H IGH -D IMENSIONAL G AUSSIAN G RAPHICAL M ODEL S ELECTION  C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. IEEE Tran. on Information Theory, 14(3):462–467, 1968. F.R.K. Chung. Spectral Graph Theory. Amer Mathematical Society, 1997. F.R.K. Chung and L. Lu. Complex Graphs and Network. Amer. Mathematical Society, 2006. T. Cover and J. Thomas. Elements of Information Theory. John Wiley & Sons, Inc., 2006. A. d’Aspremont, O. Banerjee, and L. El Ghaoui. First-order methods for sparse covariance selection. SIAM. J. Matrix Anal. & Appl., 30(56), 2008. S. Dommers, C. Giardin` , and R. van der Hofstad. Ising models on power-law random graphs. a Journal of Statistical Physics, pages 1–23, 2010. I. Dumitriu and S. Pal. Sparse regular random graphs: Spectral density and eigenvectors. Arxiv preprint arXiv:0910.5306, 2009. R. Foygel and M. Drton. Extended bayesian information criteria for gaussian graphical models. In Proc. of NIPS, 2010. A. Gamburd, S. Hoory, M. Shahshahani, A. Shalev, and B. Virag. On the girth of random cayley graphs. Random Structures & Algorithms, 35(1):100–117, 2009. J.Z. Huang, N. Liu, M. Pourahmadi, and L. Liu. Covariance matrix selection and estimation via penalised normal likelihood. Biometrika, 93(1), 2006. M. Kalisch and P. B¨ hlmann. Estimating high-dimensional directed acyclic graphs with the pcu algorithm. J. of Machine Learning Research, 8:613–636, 2007. D. Karger and N. Srebro. Learning Markov networks: maximum bounded tree-width graphs. In Proc. of ACM-SIAM Symposium on Discrete Algorithms, pages 392–401, 2001. Y.-H. Kim, A. Sutivong, and T. M. Cover. State ampliﬁcation. IEEE Transactions on Information Theory, 54(5):1850 – 1859, May 2008. M. Krivelevich and B. Sudakov. The largest eigenvalue of sparse random graphs. Combinatorics, Probability and Computing, 12(01):61–72, 2003. C. Lam and J. Fan. Sparsistency and rates of convergence in large covariance matrix estimation. Annals of Statistics, 37(6B):4254, 2009. S. L. Lauritzen. Graphical Models. Clarendon Press, 1996. H. Liu, K. Roeder, and L. Wasserman. Stability approach to regularization selection (stars) for high dimensional graphical models. Journal of Machine Learning Research (JMLR), 10:2295–2328, 2009. H. Liu, M. Xu, H. Gu, A. Gupta, J. Lafferty, and L. Wasserman. Forest density estimation. J. of Machine Learning Research, 12:907–951, 2011. 2335  A NANDKUMAR , TAN , H UANG AND W ILLSKY  L. Lov´ sz, V. Neumann-Lara, and M. Plummer. Mengerian theorems for paths of bounded length. a Periodica Mathematica Hungarica, 9(4):269–276, 1978. D.M. Malioutov, J.K. Johnson, and A.S. Willsky. Walk-sums and belief propagation in Gaussian graphical models. J. of Machine Learning Research, 7:2031–2064, 2006. B.D. McKay, N.C. Wormald, and B. Wysocka. Short cycles in random regular graphs. The Electronic Journal of Combinatorics, 11(R66):1, 2004. N. Meinshausen and P. B¨ hlmann. High dimensional graphs and variable selection with the lasso. u Annals of Statistics, 34(3):1436–1462, 2006. P. Netrapalli, S. Banerjee, S. Sanghavi, and S. Shakkottai. Greedy learning of Markov network structure . In Proc. of Allerton Conf. on Communication, Control and Computing, Monticello, USA, Sept. 2010. J. Pearl. Probabilistic Reasoning in Intelligent Systems—Networks of Plausible Inference. Morgan Kaufmann, 1988. P. Ravikumar, M.J. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estimation by minimizing ℓ1 -penalized log-determinant divergence. Electronic Journal of Statistics, (4): 935–980, 2011. A.J. Rothman, P.J. Bickel, E. Levina, and J. Zhu. Sparse permutation invariant covariance estimation. Electronic Journal of Statistics, 2:494–515, 2008. H. Rue and L. Held. Gaussian Markov Random Fields: Theory and Applications. Chapman and Hall, London, 2005. N.P. Santhanam and M.J. Wainwright. Information-theoretic Limits of high-dimensional model selection. In International Symposium on Information Theory, Toronto, Canada, July 2008. G. Schwarz. Estimating the dimension of a model. Annals of Statistics, 6(2):461–464, 1978. P. Spirtes and C. Meek. Learning bayesian networks with discrete variables from data. In Proc. of Intl. Conf. on Knowledge Discovery and Data Mining, pages 294–299, 1995. V.Y.F. Tan, A. Anandkumar, and A. Willsky. Learning Gaussian tree models: analysis of error exponents and extremal structures. IEEE Tran. on Signal Processing, 58(5):2701–2714, May 2010. V.Y.F. Tan, A. Anandkumar, and A. Willsky. A large-deviation analysis for the maximum likelihood learning of tree structures. IEEE Tran. on Information Theory, 57(3):1714–1735, March 2011a. V.Y.F. Tan, A. Anandkumar, and A. Willsky. Learning Markov forest models: Analysis of error rates. J. of Machine Learning Research, 12:1617–1653, May 2011b. W. Wang, M.J. Wainwright, and K. Ramchandran. Information-theoretic bounds on model selection for Gaussian Markov random ﬁelds. In IEEE International Symposium on Information Theory Proceedings (ISIT), Austin, Tx, June 2010. 2336  H IGH -D IMENSIONAL G AUSSIAN G RAPHICAL M ODEL S ELECTION  D.J. Watts and S.H. Strogatz. Collective dynamics of small-worldnetworks. Nature, 393(6684): 440–442, 1998. X. Xie and Z. Geng. A recursive method for structural learning of directed acyclic graphs. J. of Machine Learning Research, 9:459–483, 2008.  2337</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
