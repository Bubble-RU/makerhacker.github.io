<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-10" href="../jmlr2012/jmlr-2012-A_Unified_View_of_Performance_Metrics%3A_Translating_Threshold_Choice_into_Expected_Classification_Loss.html">jmlr2012-10</a> <a title="jmlr-2012-10-reference" href="#">jmlr2012-10-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>10 jmlr-2012-A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss</h1>
<br/><p>Source: <a title="jmlr-2012-10-pdf" href="http://jmlr.org/papers/volume13/hernandez-orallo12a/hernandez-orallo12a.pdf">pdf</a></p><p>Author: José Hernández-Orallo, Peter Flach, Cèsar Ferri</p><p>Abstract: Many performance metrics have been introduced in the literature for the evaluation of classiﬁcation performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into reﬁnement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassiﬁcation costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: ﬁxed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the reﬁnement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be summarised as follows: given a model, apply the threshold choice methods that correspond with the available information about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibra</p><br/>
<h2>reference text</h2><p>N. M. Adams and D. J. Hand. Comparing classiﬁers when the misallocation costs are uncertain. Pattern Recognition, 32(7):1139–1147, 1999. T. A. Alonzo, M. S. Pepe, and T. Lumley. Estimating disease prevalence in two-phase studies. Biostatistics, 4(2):313–326, 2003. M. Ayer, H.D. Brunk, G.M. Ewing, W.T. Reid, and E. Silverman. An empirical distribution function for sampling with incomplete information. Annals of Mathematical Statistics, 5:641–647, 1955. A. Bella, C. Ferri, J. Hernandez-Orallo, and M.J. Ramirez-Quintana. Calibration of machine learning models. In Handbook of Research on Machine Learning Applications, pages 128–146. IGI Global, 2009. A. Bella, C. Ferri, J. Hern´ ndez-Orallo, and M.J. Ram´rez-Quintana. Quantiﬁcation via probability a ı estimators. In 2010 IEEE International Conference on Data Mining, pages 737–742. IEEE, 2010. G. W. Brier. Veriﬁcation of forecasts expressed in terms of probability. Monthly Weather Review, 78(1):1–3, 1950. N. Br¨ mmer. Measuring, reﬁning and calibrating speaker and language information extracted from u speech. Ph.D. Dissertation, Department of Electrical and Electronic Engineering, University of Stellenbosch, 2010. A. Buja, W. Stuetzle, and Y. Shen. Loss functions for binary class probability estimation: structure and applications. http://www-stat.wharton.upenn.edu/∼buja/PAPERS/paper-proper-scoring.pdf, 2005. 2865  ´ H ERN ANDEZ -O RALLO , F LACH AND F ERRI  I. Cohen and M. Goldszmidt. Properties and beneﬁts of calibrated classiﬁers. Knowledge Discovery in Databases: PKDD 2004, pages 125–136, 2004. C. Drummond and R. C. Holte. Explicitly representing expected cost: an alternative to ROC representation. In Knowledge Discovery and Data Mining, pages 198–207, 2000. C. Drummond and R. C. Holte. Cost curves: an improved method for visualizing classiﬁer performance. Machine Learning, 65(1):95–130, 2006. C. Elkan. The foundations of cost-sensitive searning. In Proceedings of the Seventeenth International Conference on Artiﬁcial Intelligence (IJCAI-01), pages 973–978, San Francisco, CA, 2001. T. Fawcett. Using rule sets to maximize ROC performance. In 2001 IEEE International Conference on Data Mining (ICDM-01), pages 131–138, 2001. T. Fawcett. An introduction to ROC analysis. Pattern Recognition Letters, 27(8):861–874, 2006. T. Fawcett and A. Niculescu-Mizil. PAV and the ROC convex hull. Machine Learning, 68(1): 97–106, July 2007. T. Fawcett and F. Provost. Adaptive fraud detection. Data Mining and Knowledge Discovery, 1(3): 291–316, 1997. C. Ferri, P.A. Flach, J. Hern´ ndez-Orallo, and A. Senad. Modifying ROC curves to incorporate a predicted probabilities. In Second Workshop on ROC Analysis in ML, ROCML, pages 33–40, 2005. C. Ferri, J. Hern´ ndez-Orallo, and R. Modroiu. An experimental comparison of performance meaa sures for classiﬁcation. Pattern Recognition Letters, 30(1):27–38, 2009. ISSN 0167-8655. P. A. Flach. The geometry of ROC space: understanding machine learning metrics through ROC isometrics. In Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003), pages 194–201, 2003. P. A. Flach. Machine Learning: The Art and Science of Algorithms That Make Sense of Data. Cambridge University Press, to appear, 2012. P. A. Flach and E. T. Matsubara. A simple lexicographic ranker and probability estimator. In 18th European Conference on Machine Learning, pages 575–582. Springer, 2007. P. A. Flach, J. Hern´ ndez-Orallo, and C. Ferri. A coherent interpretation of AUC as a measure of a aggregated classiﬁcation performance. In Proceedings of the 28th International Conference on Machine Learning, ICML2011, 2011. G. Forman. Quantifying counts and costs via classiﬁcation. Data Mining and Knowledge Discovery, 17(2):164–206, 2008. J. H. Friedman. On bias, variance, 0/1loss, and the curse-of-dimensionality. Data Mining and Knowledge Discovery, 1(1):55–77, 1997. 2866  A U NIFIED V IEW OF P ERFORMANCE M ETRICS  M. Gebel. Multivariate Calibration of Classiﬁer Scores into the Probability Space. PhD thesis, University of Dortmund, 2009. T. Gneiting and A.E. Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359–378, 2007. ISSN 0162-1459. I. J. Good. Rational decisions. Journal of the Royal Statistical Society, Series B, 14:107–114, 1952. D. J. Hand. Construction and Assessment of Classiﬁcation Rules. John Wiley & Sons Inc, 1997. D. J. Hand. Measuring classiﬁer performance: a coherent alternative to the area under the ROC curve. Machine Learning, 77(1):103–123, 2009. D. J. Hand. Evaluating diagnostic tests: the area under the ROC curve and the balance of errors. Statistics in Medicine, 29(14):1502–1510, 2010. D. J. Hand and C. Anagnostopoulos. When is the area under the ROC curve an appropriate measure of classiﬁer performance? Technical report, Department of Mathematics, Imperial College, London, 2011. D. J. Hand and C. Anagnostopoulos. A better Beta for the H measure of classiﬁcation performance. arXiv:1202.2564v1 [stat] 12 Feb 2012, page 9, 2012. J. Hern´ ndez-Orallo, P. A. Flach, and C. Ferri. Brier curves: a new cost-based visualisation of clasa siﬁer performance. In Proceedings of the 28th International Conference on Machine Learning, ICML2011, 2011. J. Hern´ ndez-Orallo, P. Flach, and C. Ferri. ROC curves in cost space. In Submitted, 2012. a N. Lachiche and P. Flach. Improving accuracy and cost of two-class and multi-class probabilistic classiﬁers using roc curves. In International Conference on Machine Learning, pages 416–423, 2003. G. Lebanon and J. D. Lafferty. Cranking: combining rankings using conditional probability models on permutations. In Machine Learning, Proceedings of the Nineteenth International Conference (ICML 2002), pages 363–370, 2002. A. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki. The DET curve in assessment of detection task performance. In Fifth European Conference on Speech Communication and Technology, 1997. A. H. Murphy. A note on the utility of probabilistic predictions and the probability score in the cost-loss ratio decision situation. Journal of Applied Meteorology, 5:534–536, 1966. ISSN 08948763. A. H. Murphy. Measures of the utility of probabilistic predictions in cost-loss ratio decision situations in which knowledge of the cost-loss ratios is incomplete. Journal of Applied Meteorology, 8:863–873, 1969. ISSN 0894-8763. A. H. Murphy. A new vector partition of the probability score. Journal of Applied Meteorology, 12: 595–600, 1973. 2867  ´ H ERN ANDEZ -O RALLO , F LACH AND F ERRI  A. H. Murphy and R. L. Winkler. Scoring rules in probability assessment and evaluation. Acta Psychologica, 34:273–286, 1970. J. M. Murphy, D. M. Berwick, M. C. Weinstein, J. F. Borus, S. H. Budman, and G. L. Klerman. Performance of screening and diagnostic tests: application of receiver operating characteristic analysis. Archives of General Psychiatry, 44(6):550, 1987. J. Neyman. Contribution to the theory of sampling human populations. Journal of the American Statistical Association, 33(201):101–116, 1938. A. Niculescu-Mizil and R. Caruana. Obtaining calibrated probabilities from boosting. In The 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI 05), AUAI Press, pages 413–420. AUAI Press, 2005. A. Niculescu-Mizil and R. Caruana. Predicting good probabilities with supervised learning. In Proceedings of the 22nd International Conference on Machine learning, pages 625–632. ACM, 2005. G. Piatetsky-Shapiro and B. Masand. Estimating campaign beneﬁts and modeling lift. In Proceedings of the ﬁfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, page 193. ACM, 1999. J. C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classiﬁers, pages 61–74. MIT Press, Boston, 1999. R. C. Prati, G. E. A. P. A. Batista, and M. C. Monard. A survey on graphical methods for classiﬁcation predictive performance evaluation. IEEE Transactions on Knowledge and Data Engineering, 23:1601–1618, 2011. F. Provost and T. Fawcett. Robust classiﬁcation for imprecise environments. Machine Learning, 42 (3):203–231, 2001. M. D. Reid and R. C. Williamson. Composite binary losses. The Journal of Machine Learning Research, 11:2387–2422, 2010. M. D. Reid and R. C. Williamson. Information, divergence and risk for binary experiments. The Journal of Machine Learning Research, 12:731–817, 2011. T. Robertson, F. Wright, and R. Dykstra. Order Restricted Statistical Inference. John Wiley and Sons, New York, 1988. S. R¨ ping. Robust probabilistic calibration. Machine Learning: ECML 2006, pages 743–750, 2006. u J. A. Swets, R. M. Dawes, and J. Monahan. Better decisions through science. Scientiﬁc American, 283(4):82–87, October 2000. A. Tenenbein. A double sampling scheme for estimating from binomial data with misclassiﬁcations. Journal of the American Statistical Association, pages 1350–1361, 1970. 2868  A U NIFIED V IEW OF P ERFORMANCE M ETRICS  P. Turney. Types of cost in inductive concept learning. Canada National Research Council Publications Archive, 2000. S. Wieand, M.H. Gail, B.R. James, and K.L. James. A family of nonparametric statistics for comparing diagnostic markers with paired or unpaired data. Biometrika, 76(3):585–592, 1989. B. Zadrozny and C. Elkan. Obtaining calibrated probability estimates from decision trees and naive bayesian classiﬁers. In Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001), pages 609–616, 2001. B. Zadrozny and C. Elkan. Transforming classiﬁer scores into accurate multiclass probability estimates. In The 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 694–699, 2002.  2869</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
