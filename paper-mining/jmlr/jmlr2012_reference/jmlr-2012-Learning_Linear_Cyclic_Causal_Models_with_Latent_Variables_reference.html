<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 jmlr-2012-Learning Linear Cyclic Causal Models with Latent Variables</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-56" href="../jmlr2012/jmlr-2012-Learning_Linear_Cyclic_Causal_Models_with_Latent_Variables.html">jmlr2012-56</a> <a title="jmlr-2012-56-reference" href="#">jmlr2012-56-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 jmlr-2012-Learning Linear Cyclic Causal Models with Latent Variables</h1>
<br/><p>Source: <a title="jmlr-2012-56-pdf" href="http://jmlr.org/papers/volume13/hyttinen12a/hyttinen12a.pdf">pdf</a></p><p>Author: Antti Hyttinen, Frederick Eberhardt, Patrik O. Hoyer</p><p>Abstract: Identifying cause-effect relationships between variables of interest is a central problem in science. Given a set of experiments we describe a procedure that identiﬁes linear models that may contain cycles and latent variables. We provide a detailed description of the model family, full proofs of the necessary and sufﬁcient conditions for identiﬁability, a search algorithm that is complete, and a discussion of what can be done when the identiﬁability conditions are not satisﬁed. The algorithm is comprehensively tested in simulations, comparing it to competing algorithms in the literature. Furthermore, we adapt the procedure to the problem of cellular network inference, applying it to the biologically realistic data of the DREAM challenges. The paper provides a full theoretical foundation for the causal discovery procedure ﬁrst presented by Eberhardt et al. (2010) and Hyttinen et al. (2010). Keywords: causality, graphical models, randomized experiments, structural equation models, latent variables, latent confounders, cycles</p><br/>
<h2>reference text</h2><p>K. A. Bollen. Structural Equations with Latent Variables. John Wiley & Sons, 1989. D. M. Chickering. Learning equivalence classes of Bayesian-network structures. Journal of Machine Learning Research, 2:445–498, 2002a. D. M. Chickering. Optimal structure identiﬁcation with greedy search. Journal of Machine Learning Research, 3:507–554, 2002b. T. Claassen and T. Heskes. Causal discovery in multiple models from different experiments. In Advances in Neural Information Processing Systems 23, pages 415–423, 2010. G. Cooper and C. Yoo. Causal discovery from a mixture of experimental and observational data. In Proceedings of the Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 116–125, 1999. D. Eaton and K. Murphy. Exact Bayesian structure learning from uncertain interventions. In Proceedings of the Eleventh International Conference on Artiﬁcial Intelligence and Statistics, 2007. F. Eberhardt and R. Scheines. Interventions and causal inference. Philosophy of Science, 74:5: 981–995, 2007. F. Eberhardt, C. Glymour, and R. Scheines. On the number of experiments sufﬁcient and in the worst case necessary to identify all causal relations among n variables. In Proceedings of the Twenty-First Conference Conference on Uncertainty in Artiﬁcial Intelligence, pages 178–184, 2005. F. Eberhardt, P. O. Hoyer, and R. Scheines. Combining experiments to discover linear cyclic models with latent variables. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics, 2010. F. M. Fisher. A correspondence principle for simultaneous equation models. Econometrica, 38(1): pp. 73–92, 1970. R. A. Fisher. The Design of Experiments. Hafner, 1935. D. Geiger and D. Heckerman. Learning Gaussian networks. Technical Report MSR-TR-94-10, Microsoft Research, 1994. 3437  H YTTINEN , E BERHARDT AND H OYER  C. Glymour, R. Scheines, P. Spirtes, and K. Kelly. Discovering Causal Structure: Artiﬁcial Intelligence, Philosophy of Science and Statistical Modeling. Academic Press, 1987. G. H. Hardy. Divergent Series. Oxford: Clarendon Press, 1949. A. Hyttinen, F. Eberhardt, and P. O. Hoyer. Causal discovery for linear cyclic models with latent variables. In Proceedings of the Fifth European Workshop on Probabilistic Graphical Models, 2010. A. Hyttinen, F. Eberhardt, and P. O. Hoyer. Noisy-or models with latent confounding. In Proceedings of the Twenty-Seventh Conference Conference on Uncertainty in Artiﬁcial Intelligence, 2011. A. Hyttinen, F. Eberhardt, and P. O. Hoyer. Experiment selection for causal discovery. Submitted, 2012. S. Itani, M. Ohannessian, K. Sachs, G. P. Nolan, and M. A. Dahleh. Structure learning in causal cyclic networks. In JMLR Workshop & Conference Proceedings, volume 6, pages 165–176, 2008. M. Koivisto and K. Sood. Exact Bayesian structure discovery in Bayesian networks. Journal of Machine Learning Research, 5:549–573, 2004. S. L. Lauritzen and T. S. Richardson. Chain graph models and their causal interpretations. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64:321–348, 2002. D. Marbach, T. Schaffter, C. Mattiussi, and D. Floreano. Generating realistic in silico gene networks for performance assessment of reverse engineering methods. Journal of Computational Biology, 16(2):229–239, 2009. S. J. Mason. Feedback theory : further properties of signal ﬂow graphs. Technical Report 303, Research Laboratory of Electronics, Massachusetts Institute of Technology, 1956. S. Meganck, B. Manderick, and P. Leray. A decision theoretic approach to learning Bayesian networks. Technical report, Vrije Universiteit Brussels, 2005. K. P. Murphy. Active learning of causal Bayes net structure. Technical report, U.C. Berkeley, 2001. E. Nyberg and K. Korb. Informative interventions. In Causality and Probability in the Sciences. College Publications, London, 2006. J. Pearl. Causality. Oxford University Press, 2000. R. J. Prill, D. Marbach, J. Saez-Rodriguez, P. K. Sorger, L. G. Alexopoulos, X. Xue, N. D. Clarke, G. Altan-Bonnet, and G. Stolovitzky. Towards a rigorous assessment of systems biology models: The DREAM 3 challenges. PLoS ONE, 5(2):e9202, 2010. T. S. Richardson. Feedback Models: Interpretation and Discovery. PhD thesis, Carnegie Mellon, 1996. M. Schmidt and K. Murphy. Modeling discrete interventional data using directed cyclic graphical models. In Proceedings of the Twenty-Fifth Conference Conference on Uncertainty in Artiﬁcial Intelligence, 2009. 3438  L EARNING L INEAR C YCLIC C AUSAL M ODELS WITH L ATENT VARIABLES  S. Shimizu, P. O. Hoyer, A. Hyv¨ rinen, and A. J. Kerminen. A linear non-Gaussian acyclic model a for causal discovery. Journal of Machine Learning Research, 7:2003–2030, 2006. J. Spencer. Minimal completely separating systems. Journal of Combinatorial Theory, 8(4):446 – 447, 1970. P. Spirtes. Directed cyclic graphical representation of feedback models. In Proceedings of the 11th Conference on Uncertainty in Artiﬁcial Intelligence, 1995. P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction and Search. MIT Press, 2nd edition, 2000. G. Stolovitzky, R. J. Prill, and A. Califano. Lessons from the DREAM 2 challenges. Annals of the New York Academy of Sciences, 1158(1):159–195, 2009. S. Tong and D. Koller. Active learning for structure in Bayesian networks. In Proceedings of the Seventeenth International Joint Conference on Artiﬁcial Intelligence, pages 863–869, 2001. T. Verma and J. Pearl. Causal networks: Semantics and expressiveness. In Proceedings of the Fourth Conference Conference on Uncertainty in Artiﬁcial Intelligence, pages 352–359, 1988. S. Wright. The method of path coefﬁcients. The Annals of Mathematical Statistics, 5(3):pp. 161– 215, 1934.  3439</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
