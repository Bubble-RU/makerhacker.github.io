<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-40" href="../jmlr2012/jmlr-2012-Exact_Covariance_Thresholding_into_Connected_Components_for_Large-Scale_Graphical_Lasso.html">jmlr2012-40</a> <a title="jmlr-2012-40-reference" href="#">jmlr2012-40-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>40 jmlr-2012-Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso</h1>
<br/><p>Source: <a title="jmlr-2012-40-pdf" href="http://jmlr.org/papers/volume13/mazumder12a/mazumder12a.pdf">pdf</a></p><p>Author: Rahul Mazumder,  Trevor Hastie</p><p>Abstract: We consider the sparse inverse covariance regularization problem or graphical lasso with regularization parameter λ. Suppose the sample covariance graph formed by thresholding the entries of the sample covariance matrix at λ is decomposed into connected components. We show that the vertex-partition induced by the connected components of the thresholded sample covariance graph (at λ) is exactly equal to that induced by the connected components of the estimated concentration graph, obtained by solving the graphical lasso problem for the same λ. This characterizes a very interesting property of a path of graphical lasso solutions. Furthermore, this simple rule, when used as a wrapper around existing algorithms for the graphical lasso, leads to enormous performance gains. For a range of values of λ, our proposal splits a large graphical lasso problem into smaller tractable problems, making it possible to solve an otherwise infeasible large-scale problem. We illustrate the graceful scalability of our proposal via synthetic and real-life microarray examples. Keywords: sparse inverse covariance selection, sparsity, graphical lasso, Gaussian graphical models, graph connected components, concentration graph, large scale covariance estimation</p><br/>
<h2>reference text</h2><p>U. Alon, N. Barkai, D. A. Notterman, K. Gish, S. Ybarra, D. Mack, and A. J. Levine. Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays. Proceedings of the National Academy of Sciences of the United States of America, 96(12):6745–6750, June 1999. ISSN 0027-8424. doi: 10.1073/pnas.96.12.6745. URL http://dx.doi.org/10.1073/pnas.96.12.6745. O. Banerjee, L. El Ghaoui, and A. d’Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data. Journal of Machine Learning Research, 9:485–516, 2008. B. Bollobas. Modern Graph Theory. Springer, New York, 1998. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004. 792  E XACT T HRESHOLDING FOR G RAPHICAL L ASSO  D.R Cox and N. Wermuth. Multivariate Dependencies. Chapman and Hall, London, 1996. J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9:432–441, 2007. H. Gazit. An optimal randomized parallel algorithm for ﬁnding connected components in a graph. SIAM Journal on Computing, 20(6):1046–1067, 1991. S. Lauritzen. Graphical Models. Oxford University Press, 1996. Z. Lu. Smooth optimization approach for sparse covariance selection. SIAM Journal on Optimization, 19:1807–1827, February 2009. ISSN 1052-6234. doi: 10.1137/070695915. URL http://portal.acm.org/citation.cfm?id=1654243.1654257. Z. Lu. Adaptive ﬁrst-order methods for general sparse inverse covariance selection. SIAM Journal on Matrix Analysis and Applications, 31:2000–2016, May 2010. ISSN 0895-4798. doi: http: //dx.doi.org/10.1137/080742531. URL http://dx.doi.org/10.1137/080742531. N. Meinshausen and P. B¨ hlmann. High-dimensional graphs and variable selection with the lasso. u Annals of Statistics, 34:1436–1462, 2006. Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, Series A, 103:127–152, 2005. A.J. Rothman, P.J. Bickel, E. Levina, and J. Zhu. Sparse permutation invariant covariance estimation. Electronic Journal of Statistics, 2:494–515, 2008. K. Scheinberg, S. Ma, and D. Goldfarb. Sparse inverse covariance selection via alternating linearization methods. In Neural Information Processing Systems, pages 2101–2109, 2010. R. E. Tarjan. Depth-ﬁrst search and linear graph algorithms. SIAM Journal on Computing, 1(2): 146160, 1972. M. J. Van-De-Vijver, Y. D. He, L. J. van’t Veer, H. Dai, A. A. Hart, D. W. Voskuil, G. J. Schreiber, J. L. Peterse, C. Roberts, M. J. Marton, M. Parrish, D. Atsma, A. Witteveen, A. Glas, L. Delahaye, T. van der Velde, H. Bartelink, S. Rodenhuis, E. T. Rutgers, S. H. Friend, and R. Bernards. A gene-expression signature as a predictor of survival in breast cancer. The New England Journal of Medicine, 347:1999–2009, Dec 2002. D. Witten and J. Friedman. A fast screening rule for the graphical lasso. accepted for publication in Journal of Computational and Graphical Statistics, 2011. Report dated 5-12-2011. D. Witten, J. Friedman, and N. Simon. New insights and faster computations for the graphical lasso. Journal of Computational and Graphical Statistics, 20(4):892–900, 2011. (Reference draft dated 9/8/2011). M. Yuan and Y. Lin. Model selection and estimation in the gaussian graphical model. Biometrika, 94(1):19–35, 2007. X. Yuan. Alternating direction methods for sparse covariance selection. Methods, (August):1–12, 2009. URL http://www.optimization-online.org/DB-FILE/2009/09/2390.pdf. 793  M AZUMDER AND H ASTIE  S. Zhou, P. R¨ timann, M. Xu, and P. B¨ hlmann. High-dimensional covariance estimation based on u u gaussian graphical models. Journal of Machine Learning Research, 999888:2975–3026, November 2011. ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id=2078183.2078201.  794</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
