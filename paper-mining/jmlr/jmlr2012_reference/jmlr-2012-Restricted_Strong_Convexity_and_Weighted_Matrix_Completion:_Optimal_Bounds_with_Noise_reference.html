<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-99" href="../jmlr2012/jmlr-2012-Restricted_Strong_Convexity_and_Weighted_Matrix_Completion%3A_Optimal_Bounds_with_Noise.html">jmlr2012-99</a> <a title="jmlr-2012-99-reference" href="#">jmlr2012-99-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>99 jmlr-2012-Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise</h1>
<br/><p>Source: <a title="jmlr-2012-99-pdf" href="http://jmlr.org/papers/volume13/negahban12a/negahban12a.pdf">pdf</a></p><p>Author: Sahand Negahban, Martin J. Wainwright</p><p>Abstract: We consider the matrix completion problem under a form of row/column weighted entrywise sampling, including the case of uniform entrywise sampling as a special case. We analyze the associated random observation operator, and prove that with high probability, it satisﬁes a form of restricted strong convexity with respect to weighted Frobenius norm. Using this property, we obtain as corollaries a number of error bounds on matrix completion in the weighted Frobenius norm under noisy sampling and for both exact and near low-rank matrices. Our results are based on measures of the “spikiness” and “low-rankness” of matrices that are less restrictive than the incoherence conditions imposed in previous work. Our technique involves an M-estimator that includes controls on both the rank and spikiness of the solution, and we establish non-asymptotic error bounds in weighted Frobenius norm for recovering matrices lying with ℓq -“balls” of bounded spikiness. Using information-theoretic methods, we show that no algorithm can achieve better estimates (up to a logarithmic factor) over these same sets, showing that our conditions on matrices and associated rates are essentially optimal. Keywords: matrix completion, collaborative ﬁltering, convex optimization</p><br/>
<h2>reference text</h2><p>A. Agarwal, S. Negahban, and M. J. Wainwright. Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions. Annals of Statistics, 2011. To appear. R. Ahlswede and A. Winter. Strong converse for identiﬁcation via quantum channels. IEEE Transactions on Information Theory, 48(3):569–579, March 2002. F. Bach. Consistency of trace norm minimization. Journal of Machine Learning Research, 9: 1019–1048, June 2008. D.P. Bertsekas. Nonlinear programming. Athena Scientiﬁc, Belmont, MA, 1995. E. Cand` s and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):925–936, e 2010. E. J. Cand` s and B. Recht. Exact matrix completion via convex optimization. Found. Comput. e Math., 9(6):717–772, 2009. E. J. Candes and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Transactions on Information Theory, 56(5):2053–2080, 2010. K. R. Davidson and S. J. Szarek. Local operator theory, random matrices, and Banach spaces. In Handbook of Banach Spaces, volume 1, pages 317–336. Elsevier, Amsterdam, NL, 2001. M. Deza and M. Laurent. Geometry of Cuts and Metric Embeddings. Springer-Verlag, New York, 1997. 1695  N EGAHBAN AND WAINWRIGHT  M. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford, 2002. Available online: http://faculty.washington.edu/mfazel/thesis-ﬁnal.pdf. D. Gross. Recovering low-rank matrices from few coefﬁcients in any basis. IEEE Transactions on Information Theory, 57(3):1548–1566, March 2011. R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, 1985. R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from few entries. IEEE Transactions on Information Theory, 56(6):2980–2998, June 2010a. R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. Journal of Machine Learning Research, 11:2057–2078, July 2010b. M. Laurent. Matrix completion problems. In The Encyclopedia of Optimization, pages 221—229. Kluwer Academic, 2001. M. Ledoux. The Concentration of Measure Phenomenon. Mathematical Surveys and Monographs. American Mathematical Society, Providence, RI, 2001. M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. SpringerVerlag, New York, NY, 1991. Z. Lin, A. Ganesh, J. Wright, L. Wu, M. Chen, and Y. Ma. Fast convex optimization algorithms for exact recovery of a corrupted low-rank matrix. Technical Report UILU-ENG-09-2214, Univ. Illinois, Urbana-Champaign, July 2009. R. Mazumber, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research, 11:2287–2322, August 2010. S. Negahban and M. J. Wainwright. Estimation of (near) low-rank matrices with noise and highdimensional scaling. Annals of Statistics, 39(2):1069–1097, 2011. S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A uniﬁed framework for highdimensional analysis of M-estimators with decomposable regularizers. In Neural Information Processing Systems (NIPS), Vancouver, Canada, December 2009. To appear in Statistical Science. Y. Nesterov. Gradient methods for minimizing composite objective function. Technical Report 2007/76, CORE, Universit’e catholique de Louvain, 2007. G. Pisier. The Volume of Convex Bodies and Banach Space Geometry, volume 94 of Cambridge Tracts in Mathematics. Cambridge University Press, Cambridge, UK, 1989. B. Recht. A simpler approach to matrix completion. Journal of Machine Learning Research, 12: 3413–3430, 2011. B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471–501, 2010. 1696  R ESTRICTED S TRONG C ONVEXITY AND W EIGHTED M ATRIX C OMPLETION  A. Rohde and A. Tsybakov. Estimation of high-dimensional low-rank matrices. Annals of Statistics, 39(2):887–930, 2011. R. Salakhutdinov and N. Srebro. Collaborative ﬁltering in a non-uniform world: Learning with the weighted trace norm. Technical Report abs/1002.2780v1, Toyota Institute of Technology, 2010. N. Srebro. Learning with Matrix Factorizations. PhD thesis, MIT, 2004. Available online: http://ttic.uchicago.edu/ nati/Publications/thesis.pdf. N. Srebro, J. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. In Neural Information Processing Systems (NIPS), Vancouver, Canada, December 2004. N. Srebro, N. Alon, and T. S. Jaakkola. Generalization error bounds for collaborative prediction with low-rank matrices. In Neural Information Processing Systems (NIPS), Vancouver, Canada, December 2005. S. J. Szarek. The ﬁnite dimensional basis problem with an appendix on nets of grassmann manifolds. Acta Mathematica, 151:153–179, 1983. J. Tropp. User-friendly tail bounds for matrix martingales. Technical report, Caltech, April 2010. R. Vershynin. A note on sums of independent random matrices after Ahlswede-Winter. Technical report, Univ. Michigan, December 2009. R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Compressed Sensing: Theory and Applications, 2012. Available at http://www-personal.umich.edu/ ˜romanv/papers/non-asymptotic-rmt-plain.pdf. Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. Annals of Statistics, 27(5):1564–1599, 1999. B. Yu. Assouad, Fano and Le Cam. In Festschrift for Lucien Le Cam, pages 423–435. SpringerVerlag, Berlin, 1997.  1697</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
