<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-64" href="../jmlr2012/jmlr-2012-Manifold_Identification_in_Dual_Averaging_for_Regularized_Stochastic_Online_Learning.html">jmlr2012-64</a> <a title="jmlr-2012-64-reference" href="#">jmlr2012-64-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>64 jmlr-2012-Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning</h1>
<br/><p>Source: <a title="jmlr-2012-64-pdf" href="http://jmlr.org/papers/volume13/lee12a/lee12a.pdf">pdf</a></p><p>Author: Sangkyun Lee, Stephen J. Wright</p><p>Abstract: Iterative methods that calculate their steps from approximate subgradient directions have proved to be useful for stochastic learning problems over large and streaming data sets. When the objective consists of a loss function plus a nonsmooth regularization term, the solution often lies on a lowdimensional manifold of parameter space along which the regularizer is smooth. (When an ℓ1 regularizer is used to induce sparsity in the solution, for example, this manifold is deﬁned by the set of nonzero components of the parameter vector.) This paper shows that a regularized dual averaging algorithm can identify this manifold, with high probability, before reaching the solution. This observation motivates an algorithmic strategy in which, once an iterate is suspected of lying on an optimal or near-optimal manifold, we switch to a “local phase” that searches in this manifold, thus converging rapidly to a near-optimal point. Computational results are presented to verify the identiﬁcation property and to illustrate the effectiveness of this approach. Keywords: regularization, dual averaging, partly smooth manifold, manifold identiﬁcation</p><br/>
<h2>reference text</h2><p>M. Anitescu. Degenerate nonlinear programming with a quadratic growth condition. SIAM Journal on Optimization, 10(4):1116–1135, 2000. K. Azuma. Weighted sums of certain dependent random variables. Tohoku Mathematical Journal, 19(3):357–367, 1967. P. L. Bartlett, E. Hazan, and A. Rakhlin. Adaptive online gradient descent. In Advances in Neural Information Processing Systems 20, pages 65–72, 2008. 1742  M ANIFOLD I DENTIFICATION FOR R EGULARIZED S TOCHASTIC O NLINE L EARNING  L. Bottou. Stochastic learning. In Advanced Lectures on Machine Learning, volume 3176 of Lecture Notes in Artiﬁcial Intelligence, pages 146–168. Springer Verlag, 2004. J. V. Burke and J. J. Moré. Exposing constraints. SIAM Journal on Optimization, 4(3):573–595, 1994. K. L. Chung. On a stochastic approximation method. Annals of Mathematical Statistics, 25(3): 463–483, 1954. O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using mini-batches. Journal of Machine Learning Research, 13:165–202, 2012. J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward backward splitting. Journal of Machine Learning Research, 10:2899–2934, 2009. J. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In Proceedings of the 23rd Annual Conference on Learning Theory, 2010. W. L. Hare and A. S. Lewis. Identifying active constraints via partial smoothness and proxregularity. Journal of Convex Analysis, 11(2):251–266, 2004. E. Hazan, A. Kalai, S. Kale, and A. Agarwal. Logarithmic regret algorithms for online convex optimization. In Proceedings of the 19th Annual Conference on Learning Theory, pages 499– 513, 2006. J. Kiefer and J. Wolfowitz. Stochastic estimation of the maximum of a regression function. Annals of Mathematical Statistics, 23(3):462–466, 1952. K. Koh, S.-J. Kim, and S. Boyd. An interior-point method for large-scale ℓ1 -regularized logistic regression. Journal of Machine Learning Research, 8:1519–1555, 2007. J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated gradient. Journal of Machine Learning Research, 10:777–801, 2009. A. S. Lewis. Active sets, nonsmoothness, and sensitivity. SIAM Journal on Optimization, 13: 702–725, 2003. A. S. Lewis and S. J. Wright. A proximal method for composite minimization. Technical report, University of Wisconsin-Madison, August 2008. Q. Lin, X. Chen, and J. Peña. A sparsity preserving stochastic gradient method for composite optimization. Working paper, Tepper School of Business, Carnegie Mellon University, March 2011. Revised May 2012. A. Nemirovski and D. Yudin. On Cezari’s convergence of the steepest descent method for approximating saddle point of convex-concave functions. Soviet Mathematics-Doklady, 19, 1978. A. Nemirovski and D. Yudin. Problem Complexity and Method Efﬁciency in Optimization. John Wiley, 1983. 1743  L EE AND W RIGHT  A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009. Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic Publishers, 2004. Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming, 120:221–259, 2009. C. Oberlin and S. J. Wright. Active set identiﬁcation in nonlinear programming. SIAM Journal on Optimization, 17(2):577–605, 2006. B. T. Polyak. New stochastic approximation type procedures. Avtomatica i Telemekhanika, 7: 98–107, 1990. B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838–855, 1992. H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics, 22(3):400–407, 1951. J. Sacks. Asymptotic distribution of stochastic approximation procedures. Annals of Mathematical Statistics, 29(2):373–405, 1958. S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal Estimated sub-GrAdient SOlver for SVM. In Proceedings of the 24th International Conference on Machine Learning, pages 807– 814, 2007. W. Shi, G. Wahba, S. J. Wright, K. Lee, R. Klein, and B. Klein. LASSO-Patternsearch algorithm with application to opthalmology data. Statistics and its Interface, 1:137–153, 2008. I. Vaisman. A First Course in Differential Geometry. Monographs and Textbooks in Pure and Applied Mathematics. Marcel Dekker, 1984. S. J. Wright. Identiﬁable surfaces in constrained optimization. SIAM Journal on Control and Optimization, 31(4):1063–1079, 1993. S. J. Wright. Accelerated block-coordinate relaxation for regularized optimization. SIAM Journal on Optimization, 22(1):159–186, 2012. S. J. Wright, R. D. Nowak, and M. A. T. Figueiredo. Sparse reconstruction by separable approximation. IEEE Transactions on Signal Processing, 57:2479–2493, 2009. L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine Learning Research, 11:2543–2596, 2010. T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In Proceedings of the 21st International Conference on Machine Learning, 2004. M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning, pages 928–936, 2003. 1744</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
