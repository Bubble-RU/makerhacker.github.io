<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-105" href="../jmlr2012/jmlr-2012-Selective_Sampling_and_Active_Learning_from_Single_and_Multiple_Teachers.html">jmlr2012-105</a> <a title="jmlr-2012-105-reference" href="#">jmlr2012-105-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>105 jmlr-2012-Selective Sampling and Active Learning from Single and Multiple Teachers</h1>
<br/><p>Source: <a title="jmlr-2012-105-pdf" href="http://jmlr.org/papers/volume13/dekel12b/dekel12b.pdf">pdf</a></p><p>Author: Ofer Dekel, Claudio Gentile, Karthik Sridharan</p><p>Abstract: We present a new online learning algorithm in the selective sampling framework, where labels must be actively queried before they are revealed. We prove bounds on the regret of our algorithm and on the number of labels it queries when faced with an adaptive adversarial strategy of generating the instances. Our bounds both generalize and strictly improve over previous bounds in similar settings. Additionally, our selective sampling algorithm can be converted into an efﬁcient statistical active learning algorithm. We extend our algorithm and analysis to the multiple-teacher setting, where the algorithm can choose which subset of teachers to query for each label. Finally, we demonstrate the effectiveness of our techniques on a real-world Internet search problem. Keywords: online learning, regret, label-efﬁcient, crowdsourcing</p><br/>
<h2>reference text</h2><p>B. D. Argall, B. Browning, and M. Veloso. Automatic weight learning for multiple data sources when learning from demonstration. In Proc. of the 2009 IEEE International Conference on Robotics and Automation, pages 226–231, 2009. K. S. Azoury and M. K. Warmuth. Relative loss bounds for online density estimation with the exponential family of distributions. Machine Learning, 43(3):211–246, 2001. F. Bach. Active learning for misspeciﬁed generalized linear models. Technical Report N15/06/MM, Ecole des mines de Paris, June 2006. M. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In Proc. of the 23th International Conference on Machine Learning, pages 65–72, 2006. M. Balcan, A. Broder, and T. Zhang. Margin-based active learning. In Proc. of the 20th Annual Conference on Learning Theory, pages 35–50, 2007. M. Balcan, S. Hanneke, and J. Wortman. The true sample complexity of active learning. In Proc. of the 21th Annual Conference on Learning Theory, pages 45–56, 2008. A. Beygelzimer, D. Hsu, J. Langford, and T. Zhang. Agnostic active learning without constraints. In Advances in Neural Information Processing Systems 24, pages 199–207, 2010. A. Beygelzimer, D. Hsu, N. Karampatziakis, J. Langford, and T. Zhang. Efﬁcient active learning. In ICML 2011 Workshop on On-line Trading of Exploration and Exploitation, 2011. R. Castro and R. D. Nowak. Minimax bounds for active learning. IEEE Transactions on Information Theory, 54(5):2339–2353, 2008. G. Cavallanti, N. Cesa-Bianchi, and C. Gentile. Linear classiﬁcation and selective sampling under low noise conditions. In Advances in Neural Information Processing Systems 21, pages 249–256, 2009. N. Cesa-Bianchi and C. Gentile. Improved risk tail bounds for on-line algorithms. IEEE Transactions on Information Theory, 54(1):386–390, 2008. N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006. N. Cesa-Bianchi, A. Conconi, and C. Gentile. Learning probabilistic linear-threshold classiﬁers via selective sampling. In Proc. of the 16th Annual Conference on Learning Theory, pages 373–387, 2003. N. Cesa-Bianchi, A. Conconi, and C. Gentile. A second-order Perceptron algorithm. SIAM Journal on Computing, 43(3):640–668, 2005a. 2694  S ELECTIVE S AMPLING AND ACTIVE L EARNING FROM S INGLE AND M ULTIPLE T EACHERS  N. Cesa-Bianchi, G. Lugosi, and G. Stoltz. Minimizing regret with label efﬁcient prediction. IEEE Transactions on Information Theory, 51(6):2152–2162, 2005b. N. Cesa-Bianchi, C. Gentile, and L. Zaniboni. Worst-case analysis of selective sampling for linear classiﬁcation. Journal of Machine Learning Research, 7:1025–1230, 2006. N. Cesa-Bianchi, C. Gentile, and F. Orabona. Robust bounds for classiﬁcation via selective sampling. In Proc. of the 26th International Conference on Machine Learning, pages 121–128, 2009. S. Chen, J. Zhang, G. Chen, and C Zhang. What if the irresponsible teachers are dominating? A method of training on samples and clustering on teachers. In Proc. of the Twenty-fourth AAAI Conference on Artiﬁcial Intelligence, pages 419–424, 2010. R. Cohn, L. Atlas, and R. Ladner. Training connectionist networks with queries and selective sampling. In Advances in Neural Information Processing Systems 2, pages 566–573, 1990. K. Crammer and C. Gentile. Multiclass classiﬁcation with bandit feedback using adaptive regularization. In Proc. of the 28th International Conference on Machine Learning, pages 273–280, 2011. K. Crammer, M. Kearns, and J. Wortman. Learning from multiple sources. Journal of Machine Learning Research, 9:1757–1774, 2008. V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under bandit feedback. In Proc. of the 21th Annual Conference on Learning Theory, pages 355–366, 2008. S. Dasgupta, A. T. Kalai, and C. Monteleoni. Analysis of perceptron-based active learning. In Proc. of the 18th Annual Conference on Learning Theory, pages 249–263, 2005. S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In Advances in Neural Information Processing Systems 21, pages 353–360, 2008. A.P. Dawid and A.M. Skeene. Maximum likelihood estimation of observed error-rates using the em algorithm. Applied statistics, 28:20–28, 1979. O. Dekel and O. Shamir. Vox populi: Collecting high-quality labels from a crowd. In Proc. of the 22nd Annual Conference on Learning Theory, 2009a. O. Dekel and O. Shamir. Good learners for evil teachers. In Proc. of the Twenty-Sixth International Conference on Machine Learning, pages 216–223, 2009b. P. Domnez. Proactive Learning: Towards Learning with Multiple Imperfect Predictors. PhD thesis, Carnegie Mellon University, 2010. P. Donmez and J. G. Carbonell. Proactive learning: Cost-sensitive active learning with multiple imperfect oracles. In Proc. of the 17th ACM Conference on Information and Knowledge Management, pages 619–628, 2008. P. Donmez, J.G. Carbonell, and J. Schneider. Efﬁciently learning the accuracy of labeling sources for selective sampling. In Proc. of the 15th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 259–268, 2009. 2695  D EKEL , G ENTILE AND S RIDHARAN  P. Donmez, J.G. Carbonell, and J. Schneider. A probabilistic framework to learn from multiple annotators with time-varying accuracy. In Proc. of the SIAM International Conference on Data Mining, pages 826–837, 2010. Y. Freund, S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by committee algorithm. Machine Learning, 28:133–168, 1997. P. Groot, A. Birlutiu, and T. Heskes. Learning from multiple annotators with Gaussian processes. In Proc. of the 21st International Conference on Artiﬁcial Neural Networks, pages 159–164, 2011. S. Hanneke. A bound on the label complexity of agnostic active learning. In Proc. of the 24th International Conference on Machine Learning, pages 353–360, 2007. S. Hanneke. Adaptive rates of convergence in active learning. In Proc. of the 22th Annual Conference on Learning Theory, 2009. E. Hazan, A. Kalai, S. Kale, and A. Agarwal. Logarithmic regret algorithms for online convex optimization. Machine Learning, 2:169–192, 2007. D. Helmbold and S. Panizza. Some label efﬁcient learning results. In Proc. of the 10th Conference of Computationa Learning Theory, pages 218–230, 1997. A. Hoerl and R. Kennard. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12:55–67, 1970. S.L. Hui and X.H. Zhou. Evaluation of diagnostic tests without a gold standard. Statistical Methods in Medical Research, 7:354–370, 1998. S. Kakade and A. Tewari. On the generalization ability of online strongly convex programming algorithms. In Advances in Neural Information Processing Systems, pages 801–808, 2008. V. Koltchinskii. Rademacher complexities and bounding the excess risk in active learning. Journal of Machine Learning Research, 11:2457–2485, 2010. T. L. Lai and C. Z. Wei. Least squares estimates in stochastic regression models with applications to identiﬁcation and control of dynamic systems. The Annals of Statistics, pages 154–166, 1982. L. Li, M. Littman, and T. Walsh. Knows what it knows: a framework for self-aware learning. In Proc. of the 25th International Conference on Machine Learning, pages 568–575, 2008. N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Computation, 108(2):212–261, 1994. Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation with multiple sources. In Advances in Neural Information Processing Systems 21, pages 1041–1048, 2009a. Y. Mansour, M. Mohri, and A. Rostamizadeh. Multiple source adaptation and the Renyi divergence. In Proc. of the 25th Conference on Uncertainty in Artiﬁcial Intelligence, pages 367–374, 2009b. P. Melville, M. Saar-Tsechansky, F. Provost, and R. Mooney. Economical active feature-value acquisition through expected utility estimation. In KDD Workshop on Utility-based data mining, 2005. 2696  S ELECTIVE S AMPLING AND ACTIVE L EARNING FROM S INGLE AND M ULTIPLE T EACHERS  F. Orabona and N. Cesa-Bianchi. Better algorithms for selective sampling. In Proc. of the 28th International Conference on Machine Learning, pages 433–440, 2011. V.C. Raykar, S. Yu, L.H. Zhao, G.H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning from crowds. Journal of Machine Learning, 11:1297–1322, 2010. V.S. Sheng, F. Provost, and P.G. Ipeirotis. Get another label? Improving data quality and data mining using multiple noisy labelers. In 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 614–622, 2008. P. Smyth, U. Fayyad, M. Burl, P. Perona, and P. Baldi. Inferring ground truth from subjective labelling of venus images. In Advances in neural information processing systems, pages 1085– 1092, 1995. R. Snow, B. O’Connor, D. Jurafsky, and A. Ng. Cheap and fast - but is it good? evaluating nonexpert annotations for natural language tasks. In Proc. of the Conference on Empirical Methods in Natural Language Processing, pages 254–263, 2008. D.J. Spiegelhalter and P. Stovin. An analysis of repeated biopsies following cardiac transplantation. Statistics in Medicine, 2(1):33–40, 1983. A. Strehl and M. Littman. Online linear regression and its application to model-based reinforcement learning. In Advances in Neural Information Processing Systems 20, pages 1417–1424, 2008. A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Annals of Statistics, 32(1): 135–166, 2004. V. Vovk. Competitive on-line statistics. International Statistical Review, 69:213–248, 2001. Y. Yan, R. Rosales, L. Bogoni, G. Fung, L. Moy, M. Schmidt, and J.G. Dy. Modeling annotator expertise: Learning when everybody knows a bit of something. In Proc. of the 13th International Conference on Artiﬁcial Intelligence and Statistics, pages 932–939, 2010. Y. Yan, R. Rosales, G. Fung, and J. Dy. Active learning from crowds. In Proc. of the 28th International Conference on Machine Learning, pages 1161–1168, 2011. L. Yang and J. Carbonell. Cost complexity of proactive learning via a reduction to realizable active learning. Technical Report CMU-ML-09-113, Carnegie Mellon University, 2009a. L. Yang and J. Carbonell. Adaptive proactive learning with cost-reliability tradeoff. Technical Report CMU-ML-09-114, Carnegie Mellon University, 2009b.  2697</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
