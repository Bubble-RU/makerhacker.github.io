<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-7" href="../jmlr2012/jmlr-2012-A_Multi-Stage_Framework_for_Dantzig_Selector_and_LASSO.html">jmlr2012-7</a> <a title="jmlr-2012-7-reference" href="#">jmlr2012-7-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>7 jmlr-2012-A Multi-Stage Framework for Dantzig Selector and LASSO</h1>
<br/><p>Source: <a title="jmlr-2012-7-pdf" href="http://jmlr.org/papers/volume13/liu12a/liu12a.pdf">pdf</a></p><p>Author: Ji Liu, Peter Wonka, Jieping Ye</p><p>Abstract: We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ Rn×m (m ≫ n) and a noisy observation vector y ∈ Rn satisfying y = Xβ∗ + ε where ε is the noise vector following a Gaussian distribution N(0, σ2 I), how to recover the signal (or parameter vector) β∗ when the signal is sparse? The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively reﬁnes the target signal β∗ . We show that if X obeys a certain condition, then with a large probability ˆ the difference between the solution β estimated by the proposed method and the true solution β∗ measured in terms of the ℓ p norm (p ≥ 1) is bounded as ˆ β − β∗ p ≤ C(s − N)1/p log m + ∆ σ, where C is a constant, s is the number of nonzero entries in β∗ , the risk of the oracle estimator ∆ is independent of m and is much smaller than the ﬁrst term, and N is the number of entries of β∗ larger √ than a certain value in the order of O (σ log m). The proposed √ method improves the estimation √ bound of the standard Dantzig selector approximately from Cs1/p log mσ to C(s − N)1/p log mσ where the value N depends on the number of large entries in β∗ . When N = s, the proposed algorithm achieves the oracle solution with a high probability, where the oracle solution is the projection of the observation vector y onto true features. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector. Finally, we extend this multi-stage procedure to the LASSO case. Keywords: multi-stage, Dantzig selector, LASSO, sparse signal recovery</p><br/>
<h2>reference text</h2><p>P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Annals of Statistics, 37(4):1705–1732, 2009. F. Bunea, A. Tsybakov, and M. Wegkamp. Sparsity oracle inequalities for the Lasso. Electronic Journal of Statistics, 1:169–194, 2007. T. Cai and L. Wang. Orthogonal matching pursuit for sparse signal recovery. IEEE Transactions on Information Theory, 57(7):4680–4688, 2011. T. Cai, G. Xu, and J. Zhang. On recovery of sparse signals via ℓ1 minimization. IEEE Transactions on Information Theory, 55(7):3388–3397, 2009. E. J. Cand` s and Y. Plan. Near-ideal model selection by ℓ1 minimization. Annals of Statistics, 37 e (5A):2145–2177, 2009. E. J. Cand` s and T. Tao. Decoding by linear programming. IEEE Transactions on Information e Theory, 51(12):4203–4215, 2005. E. J. Cand` s and T. Tao. The Dantzig selector: Statistical estimation when p is much larger than n. e Annals of Statistics, 35(6):2313–2351, 2007. D. L. Donoho, M. Elad, and V. N. Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Transactions on Information Theory, 52(1):6–18, 2006. J. Fan and J. Lv. A selective overview of variable selection in high dimensional feature space. (invited review article) Statistica Sinica, 20:101–148, 2010. J. Fan and J. Lv. Nonconcave penalized likelihood with np-dimensionality. IEEE Transactions on Information Theory, 57(8):5467–5484, 2011. G. M. James, P. Radchenko, and J. Lv. DASSO: connections between the Dantzig selector and Lasso. Journal of The Royal Statistical Society Series B, 71(1):127–142, 2009. 1217  L IU , W ONKA AND Y E  V. Koltchinskii and M. Yuan. Sparse recovery in large ensembles of kernel machines on-line learning and bandits. In Proceedings of the Twenty-First Annual Conference on Learning Theory (COLT), pages 229–238, Helsinki, Finland, 2008. K. Lounici. Sup-norm convergence rate and sign concentration property of Lasso and Dantzig estimators. Electronic Journal of Statistics, 2:90–102, 2008. J. Lv and Y. Fan. A uniﬁed approach to model selection and sparse recovery using regularized least squares. Annals of Statistics, 37(6A):3498–3528, 2009. N. Meinshausen, P. Bhlmann, and E. Zrich. High dimensional graphs and variable selection with the Lasso. Annals of Statistics, 34(3):1436–1462, 2006. P. Ravikumar, G. Raskutti, M. J. Wainwright, and B. Yu. Model selection in gaussian graphical models: High-dimensional consistency of ℓ1 -regularized MLE. In Proceedings of the TwentySecond Annual Conference on Neural Information Processing Systems (NIPS), pages 1329–1336, Vancouver, British Columbia, Canada, 2008. J. Romberg. The Dantzig selector and generalized thresholding. In Proceedings of the Forty-Second Annual Conference on Information Sciences and Systems (CISS), pages 22–25, Princeton, New Jersey, USA, 2008. R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society: Series B, 58(1):267–288, 1996. J. A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information Theory, 50(10):2231–2242, 2004. M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using ℓ1 constrained quadratic programming (Lasso). IEEE Transactions on Information Theory, 55(5): 2183–2202, 2009. C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Annals of Statistics, 38(2):894–942, 2010a. C.-H. Zhang and T. Zhang. A general theory of concave regularization for high dimensional sparse estimation problems. Technical report, Department of Statistics, Rutgers University, Piscataway, New Jersey, USA, 2012. T. Zhang. Some sharp performance bounds for least squares regression with ℓ1 regularization. Annals of Statistics, 37(5A):2109–2114, 2009a. T. Zhang. On the consistency of feature selection using greedy least squares regression. Journal of Machine Learning Research, 10:555–568, 2009b. T. Zhang. Analysis of multi-stage convex relaxation for sparse regularization. Journal of Machine Learning Research, 11:1081–1107, 2010b. T. Zhang. Sparse recovery with orthogonal matching pursuit under RIP. IEEE Transactions on Information Theory, 57(9):5215–6221, 2011a. 1218  M ULTI -S TAGE DANTZIG S ELECTOR  T. Zhang. Adaptive forward-backward greedy algorithm for learning sparse representations. IEEE Transactions on Information Theory, 57(7):4689–4708, 2011b. T. Zhang. Multi-stage convex relaxation for feature selection. Technical report, Department of Statistics, Rutgers University, Piscataway, New Jersey, USA, 2011c. P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning Research, 7:2541–2563, 2006. S. Zhou. Thresholding procedures for high dimensional variable selection and statistical estimation. In Proceedings of the Twenty-Third Annual Conference on Neural Information Processing Systems (NIPS), pages 2304–2312, Vancouver, British Columbia, Canada, 2009. H. Zou. The adaptive Lasso and its oracle properties. Journal of the American Statistical Association, 101(476):1418–1429, 2006.  1219</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
