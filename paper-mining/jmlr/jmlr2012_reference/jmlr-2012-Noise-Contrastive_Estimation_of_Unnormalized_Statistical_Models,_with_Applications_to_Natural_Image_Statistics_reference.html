<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-76" href="../jmlr2012/jmlr-2012-Noise-Contrastive_Estimation_of_Unnormalized_Statistical_Models%2C_with_Applications_to_Natural_Image_Statistics.html">jmlr2012-76</a> <a title="jmlr-2012-76-reference" href="#">jmlr2012-76-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>76 jmlr-2012-Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</h1>
<br/><p>Source: <a title="jmlr-2012-76-pdf" href="http://jmlr.org/papers/volume13/gutmann12a/gutmann12a.pdf">pdf</a></p><p>Author: Michael U. Gutmann, Aapo Hyvärinen</p><p>Abstract: We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a ﬁnite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only speciﬁed up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artiﬁcially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities. Keywords: statistics unnormalized models, partition function, computation, estimation, natural image</p><br/>
<h2>reference text</h2><p>C. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995. C.J. Geyer. On the convergence of Monte Carlo maximum likelihood calculations. Journal of the Royal Statistical Society, Series B (Methodological), 56(1):261–274, 1994. M. Gutmann and A. Hyv¨ rinen. Learning features by contrasting natural images with noise. In Proa ceedings of the 19th International Conference on Artiﬁcial Neural Networks (ICANN), volume 5769 of Lecture Notes in Computer Science, pages 623–632. Springer Berlin / Heidelberg, 2009. M. Gutmann and A. Hyv¨ rinen. Noise-contrastive estimation: A new estimation principle for una normalized statistical models. In Proceedings of the 13th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), volume 9 of JMLR W&CP;, pages 297–304, 2010. T. Hastie, R. Tibshirani, and J.H. Friedman. The Elements of Statistical Learning. Springer, 2009. G. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–1800, 2002. A. Hyv¨ rinen. Estimation of non-normalized statistical models using score matching. Journal of a Machine Learning Research, 6:695–709, 2005. A. Hyv¨ rinen. Optimal approximation of signal priors. Neural Computation, 20:3087–3110, 2008. a A. Hyv¨ rinen, P.O. Hoyer, and M. Inki. Topographic independent component analysis. Neural a Computation, 13(7):1527–1558, 2001a. A. Hyv¨ rinen, J. Karhunen, and E. Oja. Independent Component Analysis. Wiley-Interscience, a 2001b. A. Hyv¨ rinen, J. Hurri, and P.O. Hoyer. Natural Image Statistics. Springer, 2009. a Y. Karklin and M. Lewicki. A hierarchical Bayesian model for learning nonlinear statistical regularities in nonstationary natural signals. Neural Computation, 17:397–423, 2005. D. Koller and N. Friedman. Probabilistic Graphical Models. MIT Press, 2009. U. K¨ ster and A. Hyv¨ rinen. A two-layer model of natural stimuli estimated with score matching. o a Neural Computation, 22(9):2308–2333, 2010. J. L¨ cke and M. Sahani. Maximal causes for non-linear component extraction. Journal of Machine u Learning Research, 9:1227–1267, 2008. R.M. Neal. Handbook of Markov Chain Monte Carlo, chapter MCMC using Hamiltonian Dynamics. Chapman & Hall /CRC Press, 2010. B.A. Olshausen and D.J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381(6583):607–609, 1996. 360  N OISE -C ONTRASTIVE E STIMATION  S. Osindero and G. Hinton. Modeling image patches with a directed hierarchy of Markov random ﬁelds. In Advances in Neural Information Processing Systems 20, pages 1121–1128. MIT Press, 2008. S. Osindero, M. Welling, and G. E. Hinton. Topographic product models applied to natural scene statistics. Neural Computation, 18 (2):381–414, 2006. M. Pihlaja, M. Gutmann, and A. Hyv¨ rinen. A family of computationally efﬁcient and simple estia mators for unnormalized statistical models. In Proceedings of the 26th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 442–449. AUAI Press, 2010. M.A. Ranzato and G. Hinton. Modeling pixel means and covariances using factorized third-order Boltzmann machines. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2551–2558, 2010. C.E. Rasmussen. Conjugate gradient algorithm, Matlab code version 2006-09-08. Downloaded from http://learning.eng.cam.ac.uk/carl/code/minimize/minimize.m. 2006. C.P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer, 2nd edition, 2004. N.N. Schraudolph and T. Graepel. Towards stochastic conjugate gradient methods. In Proceedings of the 9th International Conference on Neural Information Processing (ICONIP), volume 2, pages 853–856, 2002. W. Sun and Y. Yuan. Optimization Theory and Methods: Nonlinear Programming. Springer, 2006. Y. Teh, M. Welling, S. Osindero, and G. Hinton. Energy-based models for sparse overcomplete representations. Journal of Machine Learning Research, 4:1235–1260, 2004. T. Tieleman. Training restricted Boltzmann machines using approximations to the likelihood gradient. In Proceedings of the 25th International Conference on Machine Learning, pages 1064– 1071, 2008. J. H. van Hateren and A. van der Schaaf. Independent component ﬁlters of natural images compared with simple cells in primary visual cortex. Proceedings of the Royal Society of London. Series B: Biological Sciences, 265(1394):359–366, 1998. Z. Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600–612, 2004. L. Wasserman. All of Statistics. Springer, 2004. L. Younes. Parametric inference for imperfectly observed Gibbsian ﬁelds. Probability Theory and Related Fields, 82(4):625–645, 1989.  361</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
