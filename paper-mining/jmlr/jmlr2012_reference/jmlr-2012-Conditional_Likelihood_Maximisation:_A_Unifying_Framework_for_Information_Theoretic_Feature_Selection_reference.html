<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-27" href="../jmlr2012/jmlr-2012-Conditional_Likelihood_Maximisation%3A_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection.html">jmlr2012-27</a> <a title="jmlr-2012-27-reference" href="#">jmlr2012-27-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>27 jmlr-2012-Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection</h1>
<br/><p>Source: <a title="jmlr-2012-27-pdf" href="http://jmlr.org/papers/volume13/brown12a/brown12a.pdf">pdf</a></p><p>Author: Gavin Brown, Adam Pocock, Ming-Jie Zhao, Mikel Luján</p><p>Abstract: We present a unifying framework for information theoretic feature selection, bringing almost two decades of research on heuristic ﬁlter criteria under a single theoretical interpretation. This is in response to the question: “what are the implicit statistical assumptions of feature selection criteria based on mutual information?”. To answer this, we adopt a different strategy than is usual in the feature selection literature—instead of trying to deﬁne a criterion, we derive one, directly from a clearly speciﬁed objective function: the conditional likelihood of the training labels. While many hand-designed heuristic criteria try to optimize a deﬁnition of feature ‘relevancy’ and ‘redundancy’, our approach leads to a probabilistic framework which naturally incorporates these concepts. As a result we can unify the numerous criteria published over the last two decades, and show them to be low-order approximations to the exact (but intractable) optimisation problem. The primary contribution is to show that common heuristics for information based feature selection (including Markov Blanket algorithms as a special case) are approximate iterative maximisers of the conditional likelihood. A large empirical study provides strong evidence to favour certain classes of criteria, in particular those that balance the relative size of the relevancy/redundancy terms. Overall we conclude that the JMI criterion (Yang and Moody, 1999; Meyer et al., 2008) provides the best tradeoff in terms of accuracy, stability, and ﬂexibility with small data samples. Keywords: feature selection, mutual information, conditional likelihood</p><br/>
<h2>reference text</h2><p>K. S. Balagani and V. V. Phoha. On the feature selection criterion based on an approximation of multidimensional mutual information. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(7):1342–1343, 2010. ISSN 0162-8828. R. Battiti. Using mutual information for selecting features in supervised neural net learning. IEEE Transactions on Neural Networks, 5(4):537–550, 1994. G. Brown. A new perspective for information theoretic feature selection. In International Conference on Artiﬁcial Intelligence and Statistics, volume 5, pages 49–56, 2009. H. Cheng, Z. Qin, C. Feng, Y. Wang, and F. Li. Conditional mutual information-based feature selection analyzing for synergy and redundancy. Electronics and Telecommunications Research Institute (ETRI) Journal, 33(2), 2011. D. M. Chickering, D. Heckerman, and C. Meek. Large-sample learning of bayesian networks is np-hard. Journal of Machine Learning Research, 5:1287–1330, 2004. T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley-Interscience New York, 1991. 63  ´ B ROWN , P OCOCK , Z HAO AND L UJ AN  J. Demˇar. Statistical comparisons of classiﬁers over multiple data sets. Journal of Machine Learns ing Research, 7:1–30, 2006. J. P. Dmochowski, P. Sajda, and L. C. Parra. Maximum likelihood in cost-sensitive learning: model speciﬁcation, approximations, and upper bounds. Journal of Machine Learning Research, 11: 3313–3332, 2010. W. Duch. Feature Extraction: Foundations and Applications, chapter 3, pages 89–117. Studies in Fuzziness & Soft Computing. Springer, 2006. ISBN 3-540-35487-5. A. El Akadi, A. El Ouardighi, and D. Aboutajdine. A powerful feature selection approach based on mutual information. International Journal of Computer Science and Network Security, 8(4):116, 2008. R. M. Fano. Transmission of Information: Statistical Theory of Communications. New York: Wiley, 1961. F. Fleuret. Fast binary feature selection with conditional mutual information. Journal of Machine Learning Research, 5:1531–1555, 2004. C. Fonseca and P. Fleming. On the performance assessment and comparison of stochastic multiobjective optimizers. Parallel Problem Solving from Nature, pages 584–593, 1996. D. Grossman and P. Domingos. Learning bayesian network classiﬁers by maximizing conditional likelihood. In International Conference on Machine Learning. ACM, 2004. G. Gulgezen, Z. Cataltepe, and L. Yu. Stable and accurate feature selection. Machine Learning and Knowledge Discovery in Databases, pages 455–468, 2009. B. Guo and M. S. Nixon. Gait feature subset selection by mutual information. IEEE Trans Systems, Man and Cybernetics, 39(1):36–46, January 2009. I. Guyon. Design of experiments for the NIPS 2003 variable selection benchmark. http://www.nipsfsc.ecs.soton.ac.uk/papers/NIPS2003-Datasets.pdf, 2003. I. Guyon, S. Gunn, M. Nikravesh, and L. Zadeh, editors. Feature Extraction: Foundations and Applications. Springer, 2006. ISBN 3-540-35487-5. M. Hellman and J. Raviv. Probability of error, equivocation, and the chernoff bound. IEEE Transactions on Information Theory, 16(4):368–372, 1970. A. Jakulin. Machine Learning Based on Attribute Interactions. PhD thesis, University of Ljubljana, Slovenia, 2005. A. Kalousis, J. Prados, and M. Hilario. Stability of feature selection algorithms: a study on highdimensional spaces. Knowledge and information systems, 12(1):95–116, 2007. ISSN 0219-1377. R. Kohavi and G. H. John. Wrappers for feature subset selection. Artiﬁcial intelligence, 97(1-2): 273–324, 1997. ISSN 0004-3702. 64  F EATURE S ELECTION VIA C ONDITIONAL L IKELIHOOD  D. Koller and M. Sahami. Toward optimal feature selection. In International Conference on Machine Learning, 1996. K. Korb. Encyclopedia of Machine Learning, chapter Learning Graphical Models, page 584. Springer, 2011. L. I. Kuncheva. A stability index for feature selection. In IASTED International Multi-Conference: Artiﬁcial Intelligence and Applications, pages 390–395, 2007. N. Kwak and C. H. Choi. Input feature selection for classiﬁcation problems. IEEE Transactions on Neural Networks, 13(1):143–159, 2002. D. D. Lewis. Feature selection and feature extraction for text categorization. In Proceedings of the workshop on Speech and Natural Language, pages 212–217. Association for Computational Linguistics Morristown, NJ, USA, 1992. D. Lin and X. Tang. Conditional infomax learning: An integrated framework for feature extraction and fusion. In European Conference on Computer Vision, 2006. P. Meyer and G. Bontempi. On the use of variable complementarity for feature selection in cancer classiﬁcation. In Evolutionary Computation and Machine Learning in Bioinformatics, pages 91– 102, 2006. P. E. Meyer, C. Schretter, and G. Bontempi. Information-theoretic feature selection in microarray data using variable complementarity. IEEE Journal of Selected Topics in Signal Processing, 2(3): 261–274, 2008. T. Minka. Discriminative models, not discriminative training. Microsoft Research Cambridge, Tech. Rep. TR-2005-144, 2005. L. Paninski. Estimation of entropy and mutual information. Neural Computation, 15(6):1191–1253, 2003. ISSN 0899-7667. H. Peng, F. Long, and C. Ding. Feature selection based on mutual information: Criteria of maxdependency, max-relevance, and min-redundancy. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(8):1226–1238, 2005. C. E. Shannon. A mathematical theory of communication. Bell Systems Technical Journal, 27(3): 379–423, 1948. M. Tesmer and P. A. Estevez. Amifs: Adaptive feature selection by using mutual information. In IEEE International Joint Conference on Neural Networks, volume 1, 2004. I. Tsamardinos and C. F. Aliferis. Towards principled feature selection: Relevancy, ﬁlters and wrappers. In Proceedings of the Ninth International Workshop on Artiﬁcial Intelligence and Statistics (AISTATS), 2003. I. Tsamardinos, C. F. Aliferis, and A. Statnikov. Algorithms for large scale markov blanket discovery. In 16th International FLAIRS Conference, volume 103, 2003. 65  ´ B ROWN , P OCOCK , Z HAO AND L UJ AN  M. Vidal-Naquet and S. Ullman. Object recognition with informative features and linear classiﬁcation. IEEE Conference on Computer Vision and Pattern Recognition, 2003. J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Poggio, and V. Vapnik. Feature selection for svms. Advances in Neural Information Processing Systems, pages 668–674, 2001. ISSN 10495258. H. Yang and J. Moody. Data visualization and feature selection: New algorithms for non-gaussian data. Advances in Neural Information Processing Systems, 12, 1999. L. Yu and H. Liu. Efﬁcient feature selection via analysis of relevance and redundancy. Journal of Machine Learning Research, 5:1205–1224, 2004. L. Yu, C. Ding, and S. Loscalzo. Stable feature selection via dense feature groups. In Proceeding of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 803–811, 2008.  66</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
