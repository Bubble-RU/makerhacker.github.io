<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>23 jmlr-2012-Breaking the Curse of Kernelization: Budgeted Stochastic Gradient Descent for Large-Scale SVM Training</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-23" href="../jmlr2012/jmlr-2012-Breaking_the_Curse_of_Kernelization%3A_Budgeted_Stochastic_Gradient_Descent_for_Large-Scale_SVM_Training.html">jmlr2012-23</a> <a title="jmlr-2012-23-reference" href="#">jmlr2012-23-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>23 jmlr-2012-Breaking the Curse of Kernelization: Budgeted Stochastic Gradient Descent for Large-Scale SVM Training</h1>
<br/><p>Source: <a title="jmlr-2012-23-pdf" href="http://jmlr.org/papers/volume13/wang12b/wang12b.pdf">pdf</a></p><p>Author: Zhuang Wang, Koby Crammer, Slobodan Vucetic</p><p>Abstract: Online algorithms that process one example at a time are advantageous when dealing with very large data or with data streams. Stochastic Gradient Descent (SGD) is such an algorithm and it is an attractive choice for online Support Vector Machine (SVM) training due to its simplicity and effectiveness. When equipped with kernel functions, similarly to other SVM learning algorithms, SGD is susceptible to the curse of kernelization that causes unbounded linear growth in model size and update time with data size. This may render SGD inapplicable to large data sets. We address this issue by presenting a class of Budgeted SGD (BSGD) algorithms for large-scale kernel SVM training which have constant space and constant time complexity per update. Speciﬁcally, BSGD keeps the number of support vectors bounded during training through several budget maintenance strategies. We treat the budget maintenance as a source of the gradient error, and show that the gap between the BSGD and the optimal SVM solutions depends on the model degradation due to budget maintenance. To minimize the gap, we study greedy budget maintenance methods based on removal, projection, and merging of support vectors. We propose budgeted versions of several popular online SVM algorithms that belong to the SGD family. We further derive BSGD algorithms for multi-class SVM training. Comprehensive empirical results show that BSGD achieves higher accuracy than the state-of-the-art budgeted online algorithms and comparable to non-budget algorithms, while achieving impressive computational efﬁciency both in time and space during training and prediction. Keywords: SVM, large-scale learning, online learning, stochastic gradient descent, kernel methods</p><br/>
<h2>reference text</h2><p>A. Bordes, S. Ertekin, J. Weston, and L. Bottou. Fast kernel classiﬁers for online and active learning. Journal of Machine Learning Research, 2005. A. Bordes, L. Bottou, and P. Gallinari. Sgd-qn: careful quasi-newton stochastic gradient descent. Journal of Machine Learning Research, 2009. C. J. C. Burges. Simpliﬁed support vector decision rules. In Advances in Neural Information Processing Systems, 1996. G. Cauwenberghs and T. Poggio. Incremental and decremental support vector machine learning. In Advances in Neural Information Processing Systems, 2000. N. Cesa-Bianchi and C. Gentile. Tracking the best hyperplane with a simple budget perceptron. In Annual Conference on Learning Theory, 2006. C.-C. Chang and C.-J. Lin. Libsvm: a library for support vector machines, http://www.csie.ntu.edu. tw/˜cjlin/libsvm. 2001. F. Chang, C.-Y. Guo, X.-R. Lin, and C.-J. Lu. Tree decomposition for large-scale svm problems. Journal of Machine Learning Research, 2010a. Y.-W. Chang, C.-J. Hsie, K.-W. Chang, M. Ringgaard, and C.-J. Lin. Training and testing low-degree polynomial data mappings via linear svm. Journal of Machine Learning Research, 2010b. L. Cheng, S. V. N. Vishwanathan, D. Schuurmans, S. Wang, and T. Caelli. Implicit online earning with kernels. In Advances in Neural Information Processing Systems, 2007. R. Collobert, F. Sinz, J. Weston, and L. Bottou. Trading convexity for scalability. In International Conference on Machine Learning, 2006. C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 1995. K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning Research, 2001. K. Crammer and Y. Singer. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 2003. K. Crammer, J. Kandola, and Y. Singer. Online classiﬁcation on a budget. In Advances in Neural Information Processing Systems, 2004. 3128  B UDGETED S TOCHASTIC G RADIENT D ESCENT  K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online passive-aggressive algorithms. Journal of Machine Learning Research, 2006. L. Csat´ and M. Opper. Sparse representation for gaussian process models. In Advances in Neural Informao tion Processing Systems, 2001. O. Dekel and Y. Singer. Support vector machines on a budget. In Advances in Neural Information Processing Systems, 2006. O. Dekel, S. Shalev-Shwartz, and Y. Singer. The forgetron: a kernel-based perceptron on a budget. SIAM Journal on Computing, 2008. R. Duda and P. Hart. Pattern Classiﬁcation and Scene Analysis. New York: Wiley, 1973. Y. Engel, S. Mannor, and R. Meir. Sparse online greedy support vector regression. In European Conference on Machine Learning, 2002. C. Gentile. A new approximate maximal margin classiﬁcation algorithm. Journal of Machine Learning Research, 2001. H.-P. Graf, E. Cosatto, L. Bottou, I. Dourdanovic, and V. Vapnik. Parallel support vector machines: the cascade svm. In Advances in Neural Information Processing Systems, 2005. C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and S. Sundararajan. A dual coordinate descent method for large-scale linear svm. In International Conference on Machine Learning, 2008. T. Joachims. Training linear svms in linear time. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2006. S. S. Keerthi, O. Chapelle, , and D. DeCoste. Building support vector machines with reduced classiﬁer complexity. Journal of Machine Learning Research, 2006. J. Kivinen, A. J. Smola, and R. C. Williamson. Online learning with kernels. IEEE Transactions on Signal Processing, 2002. Y.-J. Lee and O. L. Mangasarian. Rsvm: reduced support vector machines. In SIAM Conference on Data Mining, 2001. B. Li, M. Chi, J. Fan, , and X. Xue. Support cluster machine. In International Conference on Machine Learning, 2007. Y. Li and P. Long. The relaxed online maximum margin algorithm. Machine Learning, 2002. D. Nguyen and T. Ho. An efﬁcient method for simplifying support vector machines. In International Conference on Machine Learning, 2005. F. Orabona, J. Keshet, and B. Caputo. Bounded kernel-based online learning. Journal of Machine Learning Research, 2009. J. Platt. Fast training of support vector machines using sequential minimal optimization. Advances in Kernel Methods - Support Vector Learning, MIT Press, 1998. A. Rahimi and B. Rahimi. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems, 2007. F. Rosenblatt. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological Review, 1958. 3129  WANG , C RAMMER AND V UCETIC  B. Sch¨ lkopf, S. Mika, C. J. C. Burges, P. Knirsch, K. M¨ ller, G. R¨ tsch, and A. J. Smola. Input space versus o u a feature space in kernel-based methods. IEEE Transactions on Neural Networks, 1999. S. Shalev-Shwartz and Y. Singer. Logarithmic regret algorithms for strongly convex repeated games (technical report). The Hebrew University, 2007. S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: primal estimated sub-gradient solver for svm. Mathematical Programming, 2011. S. Sonnenburg and V. Franc. Cofﬁn: a computational framework for linear svms. In International Conference on Machine Learning, 2010. I. Steinwart. Sparseness of support vector machines. Journal of Machine Learning Research, 2003. I. Sutskever. A simpler uniﬁed analysis of budget perceptrons. In International Conference on Machine Learning, 2009. C.H. Teo, S. V. N. Vishwanathan, A. J. Smola, and Q. V. Le. Bundle methods for regularized risk minimization. Journal of Machine Learning Research, 2010. I. W. Tsang, J. T. Kwok, and P.-M. Cheung. Core vector machines: fast svm training on very large data sets. Journal of Machine Learning Research, 2005. I. W. Tsang, A. Kocsor, and J. T. Kwok. Simpler core vector machines with enclosing balls. In International Conference on Machine Learning, 2007. S. V. N. Vishwanathan, A. J. Smola, and M. N.Murty. Simplesvm. In International Conference on Machine Learning, 2003. Z. Wang and S. Vucetic. Tighter perceptron with improved dual use of cached data for model representation and validation. In International Joint Conference on Neutral Netweok, 2009. Z. Wang and S. Vucetic. Online passive-aggressive algorithms on a budget. In International Conference on Artiﬁcial Intelligence and Statistics, 2010a. Z. Wang and S. Vucetic. Online training on a budget of support vector machines using twin prototypes. Statisitcal Analysis and Data Mining Journal, 2010b. Z. Wang, N. Djuric, K. Crammer, and S. Vucetic. Trading representability for scalability: adaptive multihyperplane machine for nonlinear classiﬁcation. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2011. J. Weston, A. Bordes, and L. Bottou. Online (and ofﬂine) on an even tighter budget. In International Workshop on Artiﬁcial Intelligence and Statistics, 2005. M.-R. Wu, B. Sch¨ lkopf, , and G. Barik. Building sparse large margin classiﬁers. In International Conference o on Machine Learning, 2005. H.-F. Yu, C.-J. Hsieh, K.-W. Chang, and C.-J. Lin. Large linear classiﬁcation when data cannot ﬁt in memory. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2010. K. Zhang, L. Lan, Z. Wang, and F. Moerchen. Scaling up kernel svm on limited resources: a low-rank linearization approach. In International Conference on Artiﬁcial Intelligence and Statistics, 2012. T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent. In International Conference on Machine Learning, 2004. 3130  B UDGETED S TOCHASTIC G RADIENT D ESCENT  Z. A. Zhu, W. Chen, G. Wang, C. Zhu, and Z. Chen. P-packsvm: parallel primal gradient descent kernel svm. In IEEE International Conference on Data Mining, 2009. M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In International Conference on Machine Learning, 2003.  3131</p>
<br/>
<br/><br/><br/></body>
</html>
