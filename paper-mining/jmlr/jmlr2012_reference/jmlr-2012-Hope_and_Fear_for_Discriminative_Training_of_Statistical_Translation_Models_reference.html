<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>49 jmlr-2012-Hope and Fear for Discriminative Training of Statistical Translation Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-49" href="../jmlr2012/jmlr-2012-Hope_and_Fear_for_Discriminative_Training_of_Statistical_Translation_Models.html">jmlr2012-49</a> <a title="jmlr-2012-49-reference" href="#">jmlr2012-49-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>49 jmlr-2012-Hope and Fear for Discriminative Training of Statistical Translation Models</h1>
<br/><p>Source: <a title="jmlr-2012-49-pdf" href="http://jmlr.org/papers/volume13/chiang12a/chiang12a.pdf">pdf</a></p><p>Author: David Chiang</p><p>Abstract: In machine translation, discriminative models have almost entirely supplanted the classical noisychannel model, but are standardly trained using a method that is reliable only in low-dimensional spaces. Two strands of research have tried to adapt more scalable discriminative training methods to machine translation: the ﬁrst uses log-linear probability models and either maximum likelihood or minimum risk, and the other uses linear models and large-margin methods. Here, we provide an overview of the latter. We compare several learning algorithms and describe in detail some novel extensions suited to properties of the translation task: no single correct output, a large space of structured outputs, and slow inference. We present experimental results on a large-scale ArabicEnglish translation task, demonstrating large gains in translation accuracy. Keywords: machine translation, structured prediction, large-margin methods, online learning, distributed computing</p><br/>
<h2>reference text</h2><p>Abhishek Arun and Philipp Koehn. Online learning methods for discriminative training of phrase based statistical machine translation. In Proceedings of MT Summit XI, 2007. Abhishek Arun, Barry Haddow, and Philipp Koehn. A uniﬁed approach to minimum risk training and decoding. In Proceedings of the Fifth Workshop on Statistical Machine Translation, 2010. Phil Blunsom, Trevor Cohn, and Miles Osborne. A discriminative latent variable model for statistical machine translation. In Proceedings of ACL, 2008. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19: 263–311, 1993. Nicol` Cesa-Bianchi, Alex Conconi, and Claudio Gentile. A second-order perceptron algorithm. o SIAM Journal on Computing, 34(3):640–668, 2005. David Chiang. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL, 2005. David Chiang. Hierarchical phrase-based translation. Computational Linguistics, 33(2), 2007. David Chiang. Learning to translate with source and target syntax. In Proceedings of ACL, 2010. David Chiang, Steve DeNeefe, Yee Seng Chan, and Hwee Tou Ng. Decomposability of translation metrics for improved evaluation and efﬁcient algorithms. In Proceedings of EMNLP, 2008a. David Chiang, Yuval Marton, and Philip Resnik. Online large-margin training of syntactic and structural translation features. In Proceedings of EMNLP, 2008b. David Chiang, Wei Wang, and Kevin Knight. 11,001 new features for statistical machine translation. In Proceedings of NAACL HLT, 2009. David Chiang, Steve DeNeefe, and Michael Pust. Two easy improvements to lexical weighting. In Proceedings of ACL HLT, 2011. Michael Collins. Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP, 2002. 1184  D ISCRIMINATIVE T RAINING OF S TATISTICAL T RANSLATION M ODELS  Koby Crammer and Yoram Singer. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951–991, 2003. Koby Crammer, Alex Kulesza, and Mark Dredze. Adaptive regularization of weight vectors. In Advances in Neural Information Processing Systems 22, 2009. Harold Charles Daum´ III. Practical Structured Learning Techniques for Natural Language Proe cessing. PhD thesis, University of Southern California, 2006. Mark Dredze, Koby Crammer, and Fernando Pereira. Conﬁdence-weighted linear classiﬁcation. In Proceedings of ICML, 2008. Markus Dreyer, Keith Hall, and Sanjeev Khudanpur. Comparing reordering constraints for SMT using efﬁcient B LEU oracle computation. In Proceedings of the Workshop on Syntax and Structure in Statistical Translation, 2007. Jason Eisner. Parameter estimation for probabilistic ﬁnite-state transducers. In Proceedings of ACL, 2002. Yoav Freund and Robert E. Schapire. Large margin classiﬁcation using the perceptron algorithm. Machine Learning, 37:277–296, 1999. Kevin Gimpel, Dipanjan Das, and Noah A. Smith. Distributed asynchronous online learning for natural language processing. In Proceedings of CoNLL, 2010. Liang Huang. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL, 2008. Jyrki Kivinen and Manfred K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1–64, 1996. Philipp Koehn. Statistical signiﬁcance tests for machine translation evaluation. In Proceedings of EMNLP, 2004. Philipp Koehn, Franz Josef Och, and Daniel Marcu. Statistical phrase-based translation. In Proceedings of HLT-NAACL, 2003. John Langford, Alexander J. Smola, and Martin Zinkevich. Slow learners are fast. In Advances in Neural Information Processing Systems 22, 2009. Zhifei Li and Jason Eisner. First- and second-order expectation semirings with applications to minimum-risk training on translation forests. In Proceedings of EMNLP, 2009. Percy Liang, Alexandre Bouchard-Cˆ t´ , Dan Klein, and Ben Taskar. An end-to-end discriminative oe approach to machine translation. In Proceedings of COLING-ACL, 2006. Chin-Yew Lin and Franz Josef Och. ORANGE: A method for evaluating automatic evaluation metrics for machine translation. In Proceedings of COLING, 2004. William N. Locke and A. Donald Booth, editors. Machine Translation of Languages: Fourteen Essays. Technology Press of MIT, Cambridge, MA, 1955. 1185  C HIANG  Gideon Mann, Ryan McDonald, Mehryar Mohri, Nathan Silberman, and Daniel D. Walker. Efﬁcient large-scale distributed training of conditional maximum entropy models. In Advances in Neural Information Processing Systems 22, 2009. David McAllester, Tamir Hazan, and Joseph Keshet. Direct loss minimization for structured prediction. In Advances in Neural Information Processing Systems 23, 2010. Ryan McDonald, Koby Crammer, and Fernando Pereira. Online large-margin training of dependency parsers. In Proceedings of ACL, 2005. Ryan McDonald, Keith Hall, and Gideon Mann. Distributed training strategies for the structured perceptron. In Proceedings of NAACL HLT, 2010. Franz Josef Och. Minimum error rate training in statistical machine translation. In Proceedings of ACL, 2003. Franz Josef Och and Hermann Ney. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of ACL, 2002. Franz Josef Och, Christoph Tillmann, and Hermann Ney. Improved alignment models for statistical machine translation. In Proceedings of EMNLP, 1999. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. B LEU: a method for automatic evaluation of machine translation. In Proceedings of ACL, 2002. Kaare Brandt Petersen and Michael Syskind Pedersen. The Matrix Cookbook. 2008. http:// matrixcookbook.com. John C. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Sch¨ lkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods: Support o Vector Learning, pages 195–208. MIT Press, 1998. Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. Subgradient methods for maximum margin structured learning. In Proceedings of the ICML Workshop on Learning in Structured Output Spaces, 2006. Frank Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65:386–408, 1958. Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal Estimated sub-GrAdient SOlver for SVM. In Proceedings of ICML, 2007. David A. Smith and Jason Eisner. Minimum risk annealing for training log-linear models. In Proceedings of COLING/ACL, 2006. Poster Sessions. Ben Taskar. Learning Structured Prediction Models: A Large Margin Approach. PhD thesis, Stanford University, 2004. Christoph Tillmann and Tong Zhang. A discriminative global training algorithm for statistical MT. In Proceedings of COLING-ACL, 2006. 1186  D ISCRIMINATIVE T RAINING OF S TATISTICAL T RANSLATION M ODELS  Roy W. Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. Lattice minimum Bayesrisk decoding for statistical machine translation. In Proceedings of EMNLP, 2008. Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. Support vector machine learning for interdependent and structured output spaces. In Proceedings of ICML, 2004. Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and Hideki Isozaki. Online large-margin training for statistical machine translation. In Proceedings of EMNLP, 2007. Richard Zens, Saˇa Hasan, and Hermann Ney. A systematic comparison of training criteria for s statistical machine translation. In Proceedings of EMNLP, 2008. Ying Zhang, Stephan Vogel, and Alex Waibel. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC), 2004. Martin A. Zinkevich, Markus Weimer, Alex Smola, and Lihong Li. Parallelized stochastic gradient descent. In Advances in Neural Information Processing Systems 23, 2010.  1187</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
