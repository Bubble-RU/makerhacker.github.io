<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-11" href="../jmlr2012/jmlr-2012-A_Unifying_Probabilistic_Perspective_for_Spectral_Dimensionality_Reduction%3A_Insights_and_New_Models.html">jmlr2012-11</a> <a title="jmlr-2012-11-reference" href="#">jmlr2012-11-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>11 jmlr-2012-A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models</h1>
<br/><p>Source: <a title="jmlr-2012-11-pdf" href="http://jmlr.org/papers/volume13/lawrence12a/lawrence12a.pdf">pdf</a></p><p>Author: Neil D. Lawrence</p><p>Abstract: We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random ﬁelds (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter ﬁtting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random ﬁeld via the graphical lasso.</p><br/>
<h2>reference text</h2><p>Onureena Banerjee, Laurent El Ghaoui, and Alexandre d’Aspremont. Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data. Journal of Machine Learning Research, 2007. 1636  U NIFYING S PECTRAL D IMENSIONALITY R EDUCTION  Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373–1396, 2003. doi: 10.1162/089976603321780317. Yoshua Bengio, Olivier Delalleau, Jean-Francois Palement, Nicolas Le Roux, Marie Ouimet, and Pascal Vincent. Learning eigenfunctions links spectral embedding and kernel PCA. Neural Computation, 16(10): 2197–2219, 2004a. Yoshua Bengio, Jean-Francois Paiement, Pascal Vincent, Olivier Delalleau, Nicolas Le Roux, and Marie Ouimet. Out-of-sample extensions for LLE, isomap, MDS, eigenmaps, and spectral clustering. In Sebastian Thrun, Lawrence Saul, and Bernhard Sch¨ lkopf, editors, Advances in Neural Information Processing o Systems, volume 16, pages 177–184, Cambridge, MA, 2004b. MIT Press. Julian Besag. Statistical analysis of non-lattice data. The Statistician, 24(3):179–195, 1975. George E. P. Box and Norman R. Draper. Empirical Model-Building and Response Surfaces. John Wiley and Sons, 1987. ISBN 0471810339. Brian D. Ferris, Dieter Fox, and Neil D. Lawrence. WiFi-SLAM using Gaussian process latent variable models. In Manuela M. Veloso, editor, Proceedings of the 20th International Joint Conference on Artiﬁcial Intelligence (IJCAI 2007), pages 2480–2485, 2007. Brendan J. Frey, Geoffrey E. Hinton, and Peter Dayan. Does the wake-sleep algorithm learn good density estimators? In David Touretzky, Michael Mozer, and Mark Hasselmo, editors, Advances in Neural Information Processing Systems, volume 8, pages 661–670, Cambridge, MA, 1996. MIT Press. Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3):432–441, Jul 2008. doi: 10.1093/biostatistics/kxm045. Jihun Ham, Daniel D. Lee, Sebastian Mika, and Bernhard Sch¨ lkopf. A kernel view of dimensionality o reduction of manifolds. In Russell Greiner and Dale Schuurmans, editors, Proceedings of the International Conference in Machine Learning, volume 21. Omnipress, 2004. John M. Hammersley and Peter Clifford. Markov ﬁelds on ﬁnite graphs and lattices. Technical report, 1971. URL http://www.statslab.cam.ac.uk/˜grg/books/hammfest/hamm-cliff.pdf. Stefan Harmeling. Exploring model selection techniques for nonlinear dimensionality reduction. Technical Report EDI-INF-RR-0960, University of Edinburgh, 2007. Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. SpringerVerlag, 2nd edition, 2009. Edwin T. Jaynes. Bayesian methods: General background. In J. H. Justice, editor, Maximum Entropy and Bayesian Methods in Applied Statistics, pages 1–25. Cambridge University Press, 1986. Charles Kemp and Joshua B. Tenenbaum. The discovery of structural form. Proc. Natl. Acad. Sci. USA, 105 (31), 2008. Daphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press, 2009. ISBN 978-0-262-01319-2. Solomon Kullback and Richard A. Leibler. On information and sufﬁciency. Annals of Mathematical Statistics, 22:79–86, 1951. Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. JMLR: W&CP;, 15: 29–37, 2011. 1637  L AWRENCE  Neil D. Lawrence. Gaussian process models for visualisation of high dimensional data. In Sebastian Thrun, Lawrence Saul, and Bernhard Sch¨ lkopf, editors, Advances in Neural Information Processing Systems, o volume 16, pages 329–336, Cambridge, MA, 2004. MIT Press. Neil D. Lawrence. Probabilistic non-linear principal component analysis with Gaussian process latent variable models. Journal of Machine Learning Research, 6:1783–1816, 11 2005. Na Li and Matthew Stephens. Modeling linkage disequilibrium and identifying recombination hotspots using single-nucleotide polymorphism data. Genetics, 165:2213–2233, 2003. URL http://www.genetics. org/cgi/content/abstract/165/4/2213. Kantilal V. Mardia, John T. Kent, and John M. Bibby. Multivariate Analysis. Academic Press, London, 1979. ISBN 0-12-471252-5. Radford M. Neal. Connectionist learning of belief networks. Artiﬁcial Intelligence, 56:71–113, 1992. Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000. doi: 10.1126/science.290.5500.2323. Bernhard Sch¨ lkopf, Alexander Smola, and Klaus-Robert M¨ ller. Nonlinear component analysis as a kernel o u eigenvalue problem. Neural Computation, 10:1299–1319, 1998. doi: 10.1162/089976698300017467. Joshua B. Tenenbaum, Virginia de Silva, and John C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. doi: 10.1126/science.290.5500.2319. Michael E. Tipping and Christopher M. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society, B, 6(3):611–622, 1999. doi: doi:10.1111/1467-9868.00196. Larry A. Wasserman. All of Statistics. Springer-Verlag, New York, 2003. ISBN 9780387402727. Kilian Q. Weinberger, Fei Sha, and Lawrence K. Saul. Learning a kernel matrix for nonlinear dimensionality reduction. In Russell Greiner and Dale Schuurmans, editors, Proceedings of the International Conference in Machine Learning, volume 21, pages 839–846. Omnipress, 2004. Christopher K. I. Williams. On a connection between kernel PCA and metric multidimensional scaling. In Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, Advances in Neural Information Processing Systems, volume 13, pages 675–681, Cambridge, MA, 2001. MIT Press. Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani. Semi-supervised learning: From Gaussian ﬁelds to Gaussian processes. Technical Report CMU-CS-03-175, Carnegie Mellon University, 2003.  1638</p>
<br/>
<br/><br/><br/></body>
</html>
