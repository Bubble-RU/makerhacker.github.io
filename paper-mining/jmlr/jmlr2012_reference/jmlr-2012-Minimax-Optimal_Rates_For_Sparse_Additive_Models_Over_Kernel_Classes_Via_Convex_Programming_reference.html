<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-67" href="../jmlr2012/jmlr-2012-Minimax-Optimal_Rates_For_Sparse_Additive_Models_Over_Kernel_Classes_Via_Convex_Programming.html">jmlr2012-67</a> <a title="jmlr-2012-67-reference" href="#">jmlr2012-67-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>67 jmlr-2012-Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming</h1>
<br/><p>Source: <a title="jmlr-2012-67-pdf" href="http://jmlr.org/papers/volume13/raskutti12a/raskutti12a.pdf">pdf</a></p><p>Author: Garvesh Raskutti, Martin J. Wainwright, Bin Yu</p><p>Abstract: Sparse additive models are families of d-variate functions with the additive decomposition f ∗ = ∑ j∈S f j∗ , where S is an unknown subset of cardinality s ≪ d. In this paper, we consider the case where each univariate component function f j∗ lies in a reproducing kernel Hilbert space (RKHS), and analyze a method for estimating the unknown function f ∗ based on kernels combined with ℓ1 -type convex regularization. Working within a high-dimensional framework that allows both the dimension d and sparsity s to increase with n, we derive convergence rates in the L2 (P) and L2 (Pn ) norms over the class Fd,s,H of sparse additive models with each univariate function f j∗ in the unit ball of a univariate RKHS with bounded kernel function. We complement our upper bounds by deriving minimax lower bounds on the L2 (P) error, thereby showing the optimality of our method. Thus, we obtain optimal minimax rates for many interesting classes of sparse additive models, including polynomials, splines, and Sobolev classes. We also show that if, in contrast to our univariate conditions, the d-variate function class is assumed to be globally bounded, then much √ faster estimation rates are possible for any sparsity s = Ω( n), showing that global boundedness is a signiﬁcant restriction in the high-dimensional setting. Keywords: sparsity, kernel, non-parametric, convex, minimax</p><br/>
<h2>reference text</h2><p>K. S. Alexander. Rates of Growth and Sample Moduli for Weighted Empirical Processes Indexed by Sets. Probability Theory and Related Fields, 75:379–423, 1987. N. Aronszajn. Theory of Reproducing Kernels. Transactions of the American Mathematical Society, 68:337–404, 1950. F. Bach. Consistency of the Group Lasso and Multiple Kernel Learning. Journal of Machine Learning Research, 9:1179–1225, 2008. 424  M INIMAX -O PTIMAL R ATES FOR S PARSE A DDITIVE M ODELS  P. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher Complexities. Annals of Statistics, 33:1497–1537, 2005. P. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous Analysis of Lasso and Dantzig Selector. Annals of Statistics, 37(4):1705–1732, 2009. M. S. Birman and M. Z. Solomjak. Piecewise-polynomial Approximations of Functions of the α Classes Wp . Math. USSR-Sbornik, 2(3):295–317, 1967. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, UK, 2004. L. Breiman. Better Subset Regression Using the Nonnegative Garrote. Technometrics, 37:373–384, 1995. V. V. Buldygin and Y. V. Kozachenko. Metric Characterization of Random Variables and Random Processes. American Mathematical Society, Providence, RI, 2000. B. Carl and I. Stephani. Entropy, Compactness and the Approximation of Operators. Cambridge Tracts in Mathematics. Cambridge University Press, Cambridge, UK, 1990. B. Carl and H. Triebel. Inequalities Between Eigenvalues, Entropy Numbers and Related Quantities of Compact Operators in Banach Spaces. Annals of Mathematics, 251:129–133, 1980. F. Chung and L. Lu. Concentration Inequalities and Martingale Inequalities. Internet Mathematics, 3:79–127, 2006. T.M. Cover and J.A. Thomas. Elements of Information Theory. John Wiley and Sons, New York, 1991. U. Einmahl and D. M. Mason. Some Universal Results on the Behavior of the Increments of Partial Sums. Annals of Probability, 24:1388–1407, 1996. C. Gu. Smoothing Spline ANOVA Models. Springer Series in Statistics. Springer, New York, NY, 2002. R. Z. Has’minskii. A Lower Bound on the Risks of Nonparametric Estimates of Densities in the Uniform Metric. Theory Prob. Appl., 23:794–798, 1978. T. Hastie and R. Tibshirani. Generalized Additive Models. Statistical Science, 1(3):297–310, 1986. G. Kimeldorf and G. Wahba. Some Results on Tchebychefﬁan Spline Functions. Jour. Math. Anal. Appl., 33:82–95, 1971. V. Koltchinskii and M. Yuan. Sparse Recovery in Large Ensembles of Kernel Machines. In Proceedings of COLT, 2008. V. Koltchinskii and M. Yuan. Sparsity in Multiple Kernel Learning. Annals of Statistics, 38:3660– 3695, 2010. M. Ledoux. The Concentration of Measure Phenomenon. Mathematical Surveys and Monographs. American Mathematical Society, Providence, RI, 2001. 425  R ASKUTTI , WAINWRIGHT AND Y U  Y. Lin and H. H. Zhang. Component Selection and Smoothing in Multivariate Nonparametric Regression. Annals of Statistics, 34:2272–2297, 2006. P. Massart. About the Constants in Talagrand’s Concentration Inequalities for Empirical Processes. Annals of Probability, 28(2):863–884, 2000. L. Meier, S. van de Geer, and P. Buhlmann. High-dimensional Additive Modeling. Annals of Statistics, 37:3779–3821, 2009. S. Mendelson. Geometric Parameters of Kernel Machines. In Proceedings of COLT, pages 29–43, 2002. J. Mercer. Functions of Positive and Negative Type and Their Connection With the Theory of Integral Equations. Philosophical Transactions of the Royal Society A, 209:415–446, 1909. S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A Uniﬁed Framework for Highdimensional Analysis of M-estimators with Decomposable Regularizers. In NIPS Conference, 2009. G. Pisier. The Volume of Convex Bodies and Banach Space Geometry, volume 94 of Cambridge Tracts in Mathematics. Cambridge University Press, Cambridge, UK, 1989. G. Raskutti, M. J. Wainwright, and B. Yu. Minimax Rates of Estimation for High-dimensional Linear Regression Over ℓq -balls. IEEE Trans. Information Theory, 57(10):6976—6994, October 2011. P. Ravikumar, H. Liu, J. Lafferty, and L. Wasserman. SpAM: Sparse Additive Models. Journal of the Royal Statistical Society, Series B, 71(5):1009–1030, 2009. S. Saitoh. Theory of Reproducing Kernels and its Applications. Longman Scientiﬁc & Technical, Harlow, UK, 1988. B. Sch¨ lkopf and A. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002. o C. J. Stone. Additive Regression and Other Nonparametric Models. Annals of Statistics, 13(2): 689–705, 1985. T. Suzuki and M. Sugiyama. Fast Learning Rate of Multiple Kernel Learning: Trade-off Between Sparsity and Smoothness. In AISTATS Conference, 2012. S. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000. A. W. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes. Springer-Verlag, New York, NY, 1996. G. Wahba. Spline Models for Observational Data. CBMS-NSF Regional Conference Series in Applied Mathematics. SIAM, Philadelphia, PN, 1990. Y. Yang and A. Barron. Information-theoretic Determination of Minimax Rates of Convergence. Annals of Statistics, 27(5):1564–1599, 1999. 426  M INIMAX -O PTIMAL R ATES FOR S PARSE A DDITIVE M ODELS  B. Yu. Assouad, Fano and Le Cam. Research Papers in Probability and Statistics: Festschrift in Honor of Lucien Le Cam, pages 423–435, 1996. M. Yuan. Nonnegative Garrote Component Selection in Functional ANOVA Models. In Conference on Artiﬁcial Intelligence and Statistics, pages 660–666, 2007.  427</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
