<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 jmlr-2012-Integrating a Partial Model into Model Free Reinforcement Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2012" href="../home/jmlr2012_home.html">jmlr2012</a> <a title="jmlr-2012-51" href="../jmlr2012/jmlr-2012-Integrating_a_Partial_Model_into_Model_Free_Reinforcement_Learning.html">jmlr2012-51</a> <a title="jmlr-2012-51-reference" href="#">jmlr2012-51-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>51 jmlr-2012-Integrating a Partial Model into Model Free Reinforcement Learning</h1>
<br/><p>Source: <a title="jmlr-2012-51-pdf" href="http://jmlr.org/papers/volume13/tamar12a/tamar12a.pdf">pdf</a></p><p>Author: Aviv Tamar, Dotan Di Castro, Ron Meir</p><p>Abstract: In reinforcement learning an agent uses online feedback from the environment in order to adaptively select an effective policy. Model free approaches address this task by directly mapping environmental states to actions, while model based methods attempt to construct a model of the environment, followed by a selection of optimal actions based on that model. Given the complementary advantages of both approaches, we suggest a novel procedure which augments a model free algorithm with a partial model. The resulting hybrid algorithm switches between a model based and a model free mode, depending on the current state and the agent’s knowledge. Our method relies on a novel deﬁnition for a partially known model, and an estimator that incorporates such knowledge in order to reduce uncertainty in stochastic approximation iterations. We prove that such an approach leads to improved policy evaluation whenever environmental knowledge is available, without compromising performance when such knowledge is absent. Numerical simulations demonstrate the effectiveness of the approach on policy gradient and Q-learning algorithms, and its usefulness in solving a call admission control problem. Keywords: reinforcement learning, temporal difference, stochastic approximation, markov decision processes, hybrid model based model free algorithms</p><br/>
<h2>reference text</h2><p>P. Abbeel, M. Quigley, and A.Y. Ng. Using inaccurate models in reinforcement learning. In Proceedings of the 23rd International Conference on Machine Learning, pages 1–8. ACM, 2006. J. Abounadi, D. Bertsekas, and V.S. Borkar. Learning algorithms for markov decision processes with average cost. SIAM Journal on Control and Optimization, 40(3):681–698, 2001. A.G. Barto, S.J. Bradtke, and S.P. Singh. Learning to act using real-time dynamic programming. Artiﬁcial Intelligence, 72(1-2):81–138, 1995. D.P. Bertsekas. Dynamic Programming and Optimal Control, Vol I & II. Athena Scientiﬁc, third edition, 2006. D.P. Bertsekas and J. Tsitsiklis. Neuro-dynamic Programming. Athena Scientiﬁc, 1996. S. Bhatnagar, R. Sutton, M. Ghavamzadeh, and M. Lee. Natural actor–critic algorithms. Technical Report TR09-10, Univ. of Alberta, 2007. V.S. Borkar. Stochastic Approximation: a Dynamical Systems Viewpoint. Cambridge University Press, 2008. R.I. Brafman and M. Tennenholtz. R-max a general polynomial time algorithm for near-optimal reinforcement learning. The Journal of Machine Learning Research, 3:213–231, 2003. R. Crites and A. Barto. Improving elevator performance using reinforcement learning. In Advances in Neural Information Processing Systems 8, pages 1017–1023. MIT Press, 1996. N.D. Daw, Y. Niv, and P. Dayan. Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control. Nature Neuroscience, 8(12):1704–1711, 2005. 1965  TAMAR , D I C ASTRO AND M EIR  P. Dayan and T.J. Sejnowski. TD(λ) converges with probability 1. Machine Learning, 14:295–301, 1994. R.G. Gallager. Discrete Stochastic Processes. Kluwer Academic Publishers, 1995. R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 1985. M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning, 49(2):209–232, 2002. V. Konda. Actor-Critic Algorithms. PhD thesis, Massachusetts Institute of Technology, 2002. P.R. Kumar. A survey of some results in stochastic adaptive control. SIAM Journal on Control and Optimization, 23:329–380, 1985. H.J. Kushner and G. Yin. Stochastic Approximation and Recursive Algorithms and Applications. Springer Verlag, 2003. L. Ljung. Analysis of recursive stochastic algorithms. IEEE Transactions on Automatic Control, 22 (4):551 – 575, 1977. P. Marbach and J. Tsitsiklis. Simulation-based optimization of markov reward processes. IEEE Transactions on Automatic Control, 46(2):191–209, 1998. P. Marbach, O. Mihatsch, M. Schulte, and J.N. Tsitsiklis. Reinforcement learning for call admission control and routing in integrated service networks. In Advances in Neural Information Processing Systems 10, pages 922–928. MIT Press, 1998. A. Papoulis and S.U. Pillai. Probability, Random Variables, and Stochastic Processes. McGraw Hill, fourth edition, 2002. M.J. Schervish. Theory of Statistics. Springer, 1995. S. Singh and P. Dayan. Analytical mean squared error curves for temporal difference learning. Machine Learning, 32:5–40, 1998. R.S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Proceedings of the Seventh International Conference on Machine Learning, pages 216–224. Morgan Kaufmann, 1990. R.S. Sutton and A.G. Barto. Reinforcement Learning. MIT Press, 1998. G. Tesauro. Temporal difference learning and td-gammon. Commun. ACM, 38, March 1995.  1966</p>
<br/>
<br/><br/><br/></body>
</html>
