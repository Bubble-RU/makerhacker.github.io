<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-74" href="#">jmlr2010-74</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</h1>
<br/><p>Source: <a title="jmlr-2010-74-pdf" href="http://jmlr.org/papers/volume11/shivaswamy10a/shivaswamy10a.pdf">pdf</a></p><p>Author: Pannagadatta K. Shivaswamy, Tony Jebara</p><p>Abstract: Leading classiﬁcation methods such as support vector machines (SVMs) and their counterparts achieve strong generalization performance by maximizing the margin of separation between data classes. While the maximum margin approach has achieved promising performance, this article identiﬁes its sensitivity to afﬁne transformations of the data and to directions with large data spread. Maximum margin solutions may be misled by the spread of data and preferentially separate classes along large spread directions. This article corrects these weaknesses by measuring margin not in the absolute sense but rather only relative to the spread of data in any projection direction. Maximum relative margin corresponds to a data-dependent regularization on the classiﬁcation function while maximum absolute margin corresponds to an ℓ2 norm constraint on the classiﬁcation function. Interestingly, the proposed improvements only require simple extensions to existing maximum margin formulations and preserve the computational efﬁciency of SVMs. Through the maximization of relative margin, surprising performance gains are achieved on real-world problems such as digit, text classiﬁcation and on several other benchmark data sets. In addition, risk bounds are derived for the new formulation based on Rademacher averages. Keywords: support vector machines, kernel methods, large margin, Rademacher complexity</p><p>Reference: <a title="jmlr-2010-74-reference" href="../jmlr2010_reference/jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 While the maximum margin approach has achieved promising performance, this article identiﬁes its sensitivity to afﬁne transformations of the data and to directions with large data spread. [sent-7, score-0.297]
</p><p>2 Maximum margin solutions may be misled by the spread of data and preferentially separate classes along large spread directions. [sent-8, score-0.467]
</p><p>3 This article corrects these weaknesses by measuring margin not in the absolute sense but rather only relative to the spread of data in any projection direction. [sent-9, score-0.474]
</p><p>4 Maximum relative margin corresponds to a data-dependent regularization on the classiﬁcation function while maximum absolute margin corresponds to an ℓ2 norm constraint on the classiﬁcation function. [sent-10, score-0.541]
</p><p>5 Interestingly, the proposed improvements only require simple extensions to existing maximum margin formulations and preserve the computational efﬁciency of SVMs. [sent-11, score-0.239]
</p><p>6 Support vector machines (SVMs) and maximum margin classiﬁers (Vapnik, 1995; Sch¨ lkopf and Smola, 2002; Shawe-Taylor and Cristianini, 2004) have o been a particularly successful approach both in theory and in practice. [sent-16, score-0.239]
</p><p>7 The parameters of the hyperplane (w, b) are estimated by maximizing the margin (e. [sent-20, score-0.239]
</p><p>8 In practice, the margin is maximized by minimizing 1 w⊤ w plus an upper bound on the misclassiﬁcation rate. [sent-23, score-0.263]
</p><p>9 S HIVASWAMY AND J EBARA  While maximum margin classiﬁcation works well in practice, its solution can easily be perturbed by an (invertible) afﬁne or scaling transformation of the input space. [sent-28, score-0.325]
</p><p>10 This article will explore such shortcomings in maximum margin solutions (or equivalently, SVMs in the context of this article) which exclusively measure margin by the points near the classiﬁcation boundary regardless of how spread the remaining data is away from the separating hyperplane. [sent-32, score-0.651]
</p><p>11 The key is to recover a large margin solution while normalizing the margin by the spread of the data. [sent-35, score-0.611]
</p><p>12 Thus, margin is measured in a relative sense rather than in the absolute sense. [sent-36, score-0.302]
</p><p>13 The resulting classiﬁer will be referred to as the relative margin machine (RMM) and was ﬁrst introduced by Shivaswamy and Jebara (2009a) with this longer article serving to provide more details, more thorough empirical evaluation and more theoretical support. [sent-38, score-0.318]
</p><p>14 The estimation of spread should not make second-order assumptions about the data and should be tied to the margin criterion (Vapnik, 1995). [sent-44, score-0.353]
</p><p>15 In this prior work, a feature’s contribution to margin is compared to its effect on the radius of the data by computing bounding hyper-spheres rather than simple second-order statistics. [sent-48, score-0.275]
</p><p>16 While these previous methods showed performance improvements, they relied on multiple-step locally optimal algorithms for interleaving spread information with margin estimation. [sent-55, score-0.353]
</p><p>17 Alternatively, one may consider distributions over classiﬁer solutions which provide a different estimate than the maximum margin setting and have also shown empirical improvements over SVMs (Jaakkola et al. [sent-63, score-0.239]
</p><p>18 These distribution assumptions permit update rules that resemble whitening of the data, thus alleviating adversarial afﬁne transformations and producing changes to the basic maximum margin formulation that are similar in spirit to those the RMM provides. [sent-69, score-0.333]
</p><p>19 In addition, recently, a new batch algorithm called the Gaussian margin machine (GMM) (Crammer et al. [sent-70, score-0.239]
</p><p>20 In principle, these methods also change the way margin is measured and the way regularization is applied to the learning problem. [sent-80, score-0.239]
</p><p>21 The argument is that, without any additional assumptions beyond the simple classiﬁcation problem, maximizing margin in the absolute sense may be suboptimal and that maximizing relative margin is a promising alternative. [sent-82, score-0.541]
</p><p>22 Further, large margin methods have been successfully applied to a variety of tasks such as parsing (Collins and Roark, 2004; Taskar et al. [sent-83, score-0.239]
</p><p>23 The relative margin machine formulation is detailed in Section 3 and several variants and implementations are proposed. [sent-91, score-0.301]
</p><p>24 Motivation This section provides three different (an intuitive, a probabilistic and an afﬁne transformation based) motivations for maximizing the margin relative to the data spread. [sent-102, score-0.315]
</p><p>25 The ﬁgure depicts three scaled versions of the two dimensional problem to illustrate potential problems with the large margin solution. [sent-105, score-0.239]
</p><p>26 The red (or dark shade) solution is the SVM estimate while the green (or light shade) solution is the proposed maximum relative margin alternative. [sent-107, score-0.337]
</p><p>27 Clearly, the SVM solution achieves the largest margin possible while separating both classes, yet is this necessarily the best solution? [sent-108, score-0.258]
</p><p>28 With progressive scaling, the SVM increasingly deviates from the maximum relative margin solution (green), clearly indicating that the SVM decision boundary is sensitive to afﬁne transformations of the data. [sent-111, score-0.352]
</p><p>29 Meanwhile, an algorithm producing the maximum relative margin (green) decision boundary could remain resilient to adversarial scaling. [sent-115, score-0.315]
</p><p>30 Unlike the maximum margin solution, this solution accounts for the spread of the data in various directions. [sent-117, score-0.372]
</p><p>31 This permits it to recover a solution which has a large margin relative to the spread in that direction. [sent-118, score-0.411]
</p><p>32 Such a solution would otherwise be overlooked by a maximum margin criterion. [sent-119, score-0.258]
</p><p>33 A small margin in a correspondingly smaller spread of the data might be better than a large absolute margin with correspondingly larger data spread. [sent-120, score-0.616]
</p><p>34 This particular weakness in large margin estimation has only received limited attention in previous work. [sent-121, score-0.239]
</p><p>35 In this case, the maximum relative margin (green) decision boundary should obtain zero test error even if it is estimated from a ﬁnite number of examples. [sent-124, score-0.32]
</p><p>36 The above arguments show that large margin on its own is not enough; it is also necessary to control the spread of the data after projection. [sent-130, score-0.353]
</p><p>37 Therefore, maximum margin should be traded-off or 750  M AXIMUM R ELATIVE M ARGIN AND DATA -D EPENDENT R EGULARIZATION  balanced with the goal of simultaneously minimizing the spread of the projected data, for instance, by bounding the spread |w⊤ x + b|. [sent-131, score-0.485]
</p><p>38 This will allow the linear classiﬁer to recover large margin solutions not in the absolute sense but rather relative to the spread of the data in that projection direction. [sent-132, score-0.434]
</p><p>39 No matter how they are mapped initially, a large margin solution still projects these points to the real line where the margin of separation is maximized. [sent-137, score-0.497]
</p><p>40 Given the above motivation, it is important to achieve a large margin relative to the spread of the projections even in such situations. [sent-139, score-0.421]
</p><p>41 2 Probabilistic Motivation In this subsection, an informal motivation is provided to illustrate why maximizing relative margin may be helpful. [sent-142, score-0.302]
</p><p>42 Instead of precisely ﬁnding low variance and high mean projections, this paper implements this intuition by trading off between large margin and small projections of the data while correctly classifying most of the examples with a hinge loss. [sent-154, score-0.289]
</p><p>43 3 Motivation From an Afﬁne Invariance Perspective Another motivation for maximum relative margin can be made by reformulating the classiﬁcation problem altogether. [sent-156, score-0.302]
</p><p>44 The data will be mapped by an afﬁne transformation such that it is separated with large margin while it also produces a small radius. [sent-158, score-0.276]
</p><p>45 Recall that maximum margin classiﬁcation and SVMs are motivated by generalization bounds based on Vapnik-Chervonenkis complexity arguments. [sent-159, score-0.288]
</p><p>46 These generalization bounds depend on the 751  S HIVASWAMY AND J EBARA  Figure 1: Left: As the data is scaled, the maximum margin SVM solution (red or dark shade) deviates from the maximum relative margin solution (green or light shade). [sent-160, score-0.622]
</p><p>47 The absolute margins for the maximum margin solution (red) are 1. [sent-164, score-0.282]
</p><p>48 For the maximum relative margin solution (green) the absolute margin is merely 0. [sent-168, score-0.591]
</p><p>49 However, the relative margin (the ratio of absolute margin to the spread of the projections) is 41%, 28%, and 21% for the maximum margin solution (red) and 100% for the relative margin solution (green). [sent-170, score-1.21]
</p><p>50 752  M AXIMUM R ELATIVE M ARGIN AND DATA -D EPENDENT R EGULARIZATION  ratio of the margin to the radius of the data (Vapnik, 1995). [sent-172, score-0.257]
</p><p>51 Instead of learning a classiﬁcation rule, the optimization problem considered in this section will recover an afﬁne transformation which achieves a large margin from a ﬁxed decision rule while also achieving small radius. [sent-175, score-0.276]
</p><p>52 Assume the classiﬁcation hyperplane is given a priori via the decision boundary w⊤ x+b0 = 0 with the two supporting margin hyperplanes w⊤ x+b0 = ±ρ. [sent-176, score-0.258]
</p><p>53 The following Lemma shows that this afﬁne transformation learning problem is basically equivalent to learning a large margin solution with a small spread. [sent-185, score-0.295]
</p><p>54 ˜ Learning a transformation matrix A so as to maximize the margin while minimizing the radius given an a priori hyperplane (w0 , b0 ) is no different from learning a classiﬁcation hyperplane (w, b) with ˜ a large margin as well as a small spread. [sent-194, score-0.533]
</p><p>55 This is because the rank of the afﬁne transformation A∗ ˜ ∗ merely maps all the points xi onto a line achieving a certain margin ρ but also lim˜ is one; thus, A iting the output or spread. [sent-195, score-0.36]
</p><p>56 This means that ﬁnding an afﬁne transformation which achieves a large margin and small radius is equivalent to ﬁnding a w and b with a large margin and with projections constrained to remain close to the origin. [sent-196, score-0.562]
</p><p>57 From Absolute Margin to Relative Margin This section will provide an upgrade path from the maximum margin classiﬁer (or SVM) to a maximum relative margin formulation. [sent-200, score-0.517]
</p><p>58 The above is an easily solvable quadratic program (QP) and maximizes the margin by minimizing w 2 . [sent-204, score-0.239]
</p><p>59 Thus, the above formulation maximizes the margin while minimizing an upper bound on the number of classiﬁcation errors. [sent-206, score-0.286]
</p><p>60 1 The Whitened SVM One way of limiting sensitivity to afﬁne transformations while recovering a large margin solution is to whiten the data with the covariance matrix prior to estimating the SVM solution. [sent-222, score-0.276]
</p><p>61 A parameter will also be introduced that helps trade off between large margin and small spread of the projection of the data. [sent-245, score-0.371]
</p><p>62 Consider the following formulation called the relative margin machine (RMM): min w,b  1 w 2  2  n  +C ∑ ξi  (8)  i=1  s. [sent-248, score-0.301]
</p><p>63 Speciﬁcally, with a smaller B, the RMM still ﬁnds a large margin solution but with a smaller projection of the training examples. [sent-259, score-0.308]
</p><p>64 By trying different B values (within the aforementioned thresholds), different large relative margin solutions are explored. [sent-260, score-0.278]
</p><p>65 Differentiating with respect to the primal variables and equating to zero produces: n  n  n  i=1  i=1  i=1  (I + ∑ λi xi x⊤ )w + b ∑ λi xi = ∑ αi yi xi , i n n 1 ( ∑ αi yi − ∑ λi w⊤ xi ) = b, λ⊤ 1 i=1 i=1  αi + βi = C  ∀1 ≤ i ≤ n. [sent-263, score-0.305]
</p><p>66 The RMM maximizes the margin while also limiting the spread of projections on the training data. [sent-381, score-0.414]
</p><p>67 2 2 ¯ := 1 − D and 0 < D < 1 trades off between large margin and small spread on the Above, take D projections. [sent-383, score-0.353]
</p><p>68 With these landmark examples, the modiﬁed RMM function class can be written as: ¯  V Deﬁnition 5 HE,D := {x → w⊤ x| D w⊤ w + D (w⊤ vi )2 ≤ E ∀1 ≤ i ≤ nv }. [sent-390, score-0.406]
</p><p>69 i=1 2  V U Note that the parameter E is ﬁxed in HE,D but nv may be different from n. [sent-398, score-0.294]
</p><p>70 λnv ≥ 0, the Lagrangian of (15) is: L (w, λ) = −w⊤ ∑n σi xi + i=1 nv 1 ¯ ∑i=1 λi 2 Dw⊤ w + D(w⊤ vi )2 − E . [sent-446, score-0.371]
</p><p>71 Substituti i=1 λ,D i=1 ing this w in L (w, λ) gives the dual of (15): min λ≥0  nv n 1 n ∑ σi x⊤ Σ−1 ∑ σ j x j + E ∑ λi . [sent-448, score-0.318]
</p><p>72 When an already deﬁned set, such as V (with a known number nv of elements) is an argument to T2 , λ will be subscripted with i or j. [sent-454, score-0.294]
</p><p>73 767  S HIVASWAMY AND J EBARA  √ S V Theorem 17 For nv = O ( n), with probability at least 1 − 2δ, HE,D ⊆ HE,D . [sent-550, score-0.294]
</p><p>74 2n  (24)  Similarly, applying (19) on the set V, with probability at least 1 − δ, 1 nv ≤ EDx [I[gw (x)>E] ] + 2 ∑I w nv i=1 [g (vi )>E]  2E ¯ nD  1 tr(Kv ) + 3 nv  ln(2/δ) . [sent-557, score-0.882]
</p><p>75 This is because, if there is a w ∈ HE,D that is not in HE,D , for such a 1 1 1 w, nv ∑nv I[gw (vi )>E] > nv and n ∑n I[gw (xi )>E] = 0. [sent-561, score-0.588]
</p><p>76 Thus, equating the right hand side of (26) to i=1 i=1 1 and solving for nv , the result follows. [sent-562, score-0.294]
</p><p>77 Both an exact value and the asymptotic behavior of nv are nv derived in Appendix C. [sent-563, score-0.588]
</p><p>78 For the SVM bound, the quantity of interest is 4 nγ SVM bound, the quantity of interest is  √ 4 2E nˆ γ  ¯ ∑n x⊤ DI + D ∑n u j u⊤ i=1 i j=1 j n  the RMM bound, the quantity of interest is:  nv nv 2 1 n ¯ min ∑ x⊤ D ∑ λ j I + D ∑ λ j v j v⊤ j ˆ γ λ≥0 n i=1 i j=1 j=1 769  −1  nv  xi +  −1  xi . [sent-580, score-0.988]
</p><p>79 However, in both the cases, the absolute margin of separation γ remains the same. [sent-591, score-0.263]
</p><p>80 2  This merely recovers the absolute margin γ which is shown in the ﬁgure. [sent-597, score-0.294]
</p><p>81 S ˆ Thus, when a linear function is selected from the function class GD,E , the margin γ is estimated from S , the margin is estimated from a a whitened version of the data. [sent-619, score-0.502]
</p><p>82 On this whitened ˆ data set, the margin γ appears much larger in toy example 2 since it is large compared to the spread. [sent-625, score-0.295]
</p><p>83 This gives further ﬂexibility and rescales data not only along principal eigen-directions but in any direction where the margin is large relative to the spread of the data. [sent-629, score-0.392]
</p><p>84 By exploring D values, margin can be measured relative to the spread of the data rather than in the absolute sense. [sent-630, score-0.416]
</p><p>85 However, the ﬁrst term in the bound is the average slack variables divided by the margin which does not go to zero asymptotically with increasing n and eventually dominates the bound. [sent-696, score-0.285]
</p><p>86 Clearly, as B decreases, the absolute margin is decreased however the test error rate drops drastically. [sent-788, score-0.286]
</p><p>87 This empirically suggests that maximizing the relative margin can have a beneﬁcial effect on the performance of a classiﬁer. [sent-789, score-0.278]
</p><p>88 This experiment once again demonstrates that an absolute margin does not always result in a small test error. [sent-901, score-0.286]
</p><p>89 Conclusions The article showed that support vector machines and maximum margin classiﬁers can be sensitive to afﬁne transformations of the input data and are biased in favor of separating data along directions with large spread. [sent-1215, score-0.297]
</p><p>90 The relative margin machine was proposed to overcome such problems and optimizes the projection direction such that the margin is large only relative to the spread of the data. [sent-1216, score-0.688]
</p><p>91 Empirically, the RMM and maximum relative margin approach showed signiﬁcant improvements in classiﬁcation accuracy. [sent-1219, score-0.278]
</p><p>92 Directions of future work include exploring the connections between maximum relative margin and generalization bounds based on margin distributions (Schapire et al. [sent-1237, score-0.566]
</p><p>93 By bounding outputs, the RMM is potentially ﬁnding a better margin distribution on the training examples. [sent-1239, score-0.289]
</p><p>94 Furthermore, the maximization of relative margin is a fairly promising and general concept which may be compatible with other popular problems that have recently been tackled by the maximum margin paradigm. [sent-1241, score-0.517]
</p><p>95 n i=1 Apply the Woodbury matrix inversion identity to the term inside the square root: n  D ¯ ∑ x⊤ DI + n uk u⊤ i k i=1  −1  1 n xi = ¯ ∑ x⊤ I − D i=1 i  uk u⊤ k  xi ¯ nD ⊤ D + u k uk n ∑n (x⊤ uk )2 x⊤ xi − ni=1 i i ¯ D ⊤ i=1 D + u k uk  1 = ¯ D  ∑  . [sent-1294, score-0.354]
</p><p>96 , unv : nv nv 1 n ¯ min ∑ x⊤ D ∑ λ j I + D ∑ λ j u j u⊤ i j λ≥0 n i=1 j=1 j=1  −1  2 nv xi + E ∑ λi . [sent-1308, score-0.935]
</p><p>97 Solving for nv Let x = √1 v , c = 4R 2E and b = 3 ln (2/δ) . [sent-1317, score-0.331]
</p><p>98 Thus, nv = 1/(b + √ b2 + (c + 2b)/ n) which gives an exact expression for nv . [sent-1322, score-0.588]
</p><p>99 Dropping terms from the denominator √ √ √ n produces the simpler expression: nv ≤ 1/ (c + 2b)/ n. [sent-1323, score-0.294]
</p><p>100 Empirical margin distributions and bounding the generalization error of combined classiﬁers. [sent-1463, score-0.279]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rmm', 0.728), ('nv', 0.294), ('margin', 0.239), ('svm', 0.158), ('universum', 0.151), ('ebara', 0.144), ('hivaswamy', 0.144), ('argin', 0.137), ('ependent', 0.137), ('rademacher', 0.122), ('elative', 0.117), ('spread', 0.114), ('aximum', 0.106), ('gw', 0.088), ('landmark', 0.088), ('af', 0.08), ('egularization', 0.074), ('mnist', 0.073), ('digits', 0.06), ('shivaswamy', 0.059), ('hw', 0.055), ('whitening', 0.053), ('xi', 0.053), ('tr', 0.049), ('klda', 0.048), ('classi', 0.046), ('landmarks', 0.042), ('article', 0.04), ('uk', 0.039), ('relative', 0.039), ('fe', 0.039), ('transformation', 0.037), ('weston', 0.037), ('ln', 0.037), ('yi', 0.036), ('cristianini', 0.036), ('yq', 0.035), ('digit', 0.034), ('rm', 0.033), ('training', 0.032), ('iid', 0.032), ('shade', 0.032), ('lda', 0.032), ('toy', 0.032), ('merely', 0.031), ('validation', 0.031), ('scaling', 0.03), ('bayes', 0.029), ('projections', 0.029), ('er', 0.028), ('edx', 0.027), ('rmms', 0.027), ('pr', 0.027), ('bounds', 0.027), ('kernel', 0.027), ('di', 0.026), ('crammer', 0.024), ('qp', 0.024), ('whitened', 0.024), ('dual', 0.024), ('rbf', 0.024), ('lipschitz', 0.024), ('vi', 0.024), ('motivation', 0.024), ('absolute', 0.024), ('bound', 0.024), ('sinz', 0.023), ('explored', 0.023), ('test', 0.023), ('paired', 0.023), ('perceptron', 0.023), ('mcdiarmid', 0.023), ('formulation', 0.023), ('slack', 0.022), ('generalization', 0.022), ('raetsch', 0.021), ('primal', 0.021), ('green', 0.021), ('examples', 0.021), ('synthetic', 0.021), ('labellings', 0.021), ('jebara', 0.02), ('svms', 0.02), ('kernels', 0.02), ('optical', 0.019), ('usps', 0.019), ('inequality', 0.019), ('boundary', 0.019), ('solution', 0.019), ('transformations', 0.018), ('radius', 0.018), ('projection', 0.018), ('bounding', 0.018), ('gram', 0.018), ('eu', 0.018), ('yw', 0.018), ('kernelized', 0.018), ('resilient', 0.018), ('gmm', 0.018), ('deviates', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="74-tfidf-1" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>Author: Pannagadatta K. Shivaswamy, Tony Jebara</p><p>Abstract: Leading classiﬁcation methods such as support vector machines (SVMs) and their counterparts achieve strong generalization performance by maximizing the margin of separation between data classes. While the maximum margin approach has achieved promising performance, this article identiﬁes its sensitivity to afﬁne transformations of the data and to directions with large data spread. Maximum margin solutions may be misled by the spread of data and preferentially separate classes along large spread directions. This article corrects these weaknesses by measuring margin not in the absolute sense but rather only relative to the spread of data in any projection direction. Maximum relative margin corresponds to a data-dependent regularization on the classiﬁcation function while maximum absolute margin corresponds to an ℓ2 norm constraint on the classiﬁcation function. Interestingly, the proposed improvements only require simple extensions to existing maximum margin formulations and preserve the computational efﬁciency of SVMs. Through the maximization of relative margin, surprising performance gains are achieved on real-world problems such as digit, text classiﬁcation and on several other benchmark data sets. In addition, risk bounds are derived for the new formulation based on Rademacher averages. Keywords: support vector machines, kernel methods, large margin, Rademacher complexity</p><p>2 0.092791595 <a title="74-tfidf-2" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>Author: Shiliang Sun, John Shawe-Taylor</p><p>Abstract: In this paper, we propose a general framework for sparse semi-supervised learning, which concerns using a small portion of unlabeled data and a few labeled data to represent target functions and thus has the merit of accelerating function evaluations when predicting the output of a new example. This framework makes use of Fenchel-Legendre conjugates to rewrite a convex insensitive loss involving a regularization with unlabeled data, and is applicable to a family of semi-supervised learning methods such as multi-view co-regularized least squares and single-view Laplacian support vector machines (SVMs). As an instantiation of this framework, we propose sparse multi-view SVMs which use a squared ε-insensitive loss. The resultant optimization is an inf-sup problem and the optimal solutions have arguably saddle-point properties. We present a globally optimal iterative algorithm to optimize the problem. We give the margin bound on the generalization error of the sparse multi-view SVMs, and derive the empirical Rademacher complexity for the induced function class. Experiments on artiﬁcial and real-world data show their effectiveness. We further give a sequential training approach to show their possibility and potential for uses in large-scale problems and provide encouraging experimental results indicating the efﬁcacy of the margin bound and empirical Rademacher complexity on characterizing the roles of unlabeled data for semi-supervised learning. Keywords: semi-supervised learning, Fenchel-Legendre conjugate, representer theorem, multiview regularization, support vector machine, statistical learning theory</p><p>3 0.084809422 <a title="74-tfidf-3" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>Author: Nicola Segata, Enrico Blanzieri</p><p>Abstract: A computationally efﬁcient approach to local learning with kernel methods is presented. The Fast Local Kernel Support Vector Machine (FaLK-SVM) trains a set of local SVMs on redundant neighbourhoods in the training set and an appropriate model for each query point is selected at testing time according to a proximity strategy. Supported by a recent result by Zakai and Ritov (2009) relating consistency and localizability, our approach achieves high classiﬁcation accuracies by dividing the separation function in local optimisation problems that can be handled very efﬁciently from the computational viewpoint. The introduction of a fast local model selection further speeds-up the learning process. Learning and complexity bounds are derived for FaLK-SVM, and the empirical evaluation of the approach (with data sets up to 3 million points) showed that it is much faster and more accurate and scalable than state-of-the-art accurate and approximated SVM solvers at least for non high-dimensional data sets. More generally, we show that locality can be an important factor to sensibly speed-up learning approaches and kernel methods, differently from other recent techniques that tend to dismiss local information in order to improve scalability. Keywords: locality, kernel methods, local learning algorithms, support vector machines, instancebased learning</p><p>4 0.080861531 <a title="74-tfidf-4" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>Author: Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, Chih-Jen Lin</p><p>Abstract: Kernel techniques have long been used in SVM to handle linearly inseparable problems by transforming data to a high dimensional space, but training and testing large data sets is often time consuming. In contrast, we can efﬁciently train and test much larger data sets using linear SVM without kernels. In this work, we apply fast linear-SVM methods to the explicit form of polynomially mapped data and investigate implementation issues. The approach enjoys fast training and testing, but may sometimes achieve accuracy close to that of using highly nonlinear kernels. Empirical experiments show that the proposed method is useful for certain large-scale data sets. We successfully apply the proposed method to a natural language processing (NLP) application by improving the testing accuracy under some training/testing speed requirements. Keywords: decomposition methods, low-degree polynomial mapping, kernel functions, support vector machines, dependency parsing, natural language processing</p><p>5 0.065236963 <a title="74-tfidf-5" href="./jmlr-2010-A_Streaming_Parallel_Decision_Tree_Algorithm.html">7 jmlr-2010-A Streaming Parallel Decision Tree Algorithm</a></p>
<p>Author: Yael Ben-Haim, Elad Tom-Tov</p><p>Abstract: We propose a new algorithm for building decision tree classiﬁers. The algorithm is executed in a distributed environment and is especially designed for classifying large data sets and streaming data. It is empirically shown to be as accurate as a standard decision tree classiﬁer, while being scalable for processing of streaming data on multiple processors. These ﬁndings are supported by a rigorous analysis of the algorithm’s accuracy. The essence of the algorithm is to quickly construct histograms at the processors, which compress the data to a ﬁxed amount of memory. A master processor uses this information to ﬁnd near-optimal split points to terminal tree nodes. Our analysis shows that guarantees on the local accuracy of split points imply guarantees on the overall tree accuracy. Keywords: decision tree classiﬁers, distributed computing, streaming data, scalability</p><p>6 0.064356081 <a title="74-tfidf-6" href="./jmlr-2010-Learning_Translation_Invariant_Kernels_for_Classification.html">65 jmlr-2010-Learning Translation Invariant Kernels for Classification</a></p>
<p>7 0.058425441 <a title="74-tfidf-7" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>8 0.047688417 <a title="74-tfidf-8" href="./jmlr-2010-Consensus-Based_Distributed_Support_Vector_Machines.html">26 jmlr-2010-Consensus-Based Distributed Support Vector Machines</a></p>
<p>9 0.046542402 <a title="74-tfidf-9" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>10 0.045796666 <a title="74-tfidf-10" href="./jmlr-2010-On_Over-fitting_in_Model_Selection_and_Subsequent_Selection_Bias_in_Performance_Evaluation.html">83 jmlr-2010-On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</a></p>
<p>11 0.045668386 <a title="74-tfidf-11" href="./jmlr-2010-Tree_Decomposition_for_Large-Scale_SVM_Problems.html">113 jmlr-2010-Tree Decomposition for Large-Scale SVM Problems</a></p>
<p>12 0.045406777 <a title="74-tfidf-12" href="./jmlr-2010-Posterior_Regularization_for_Structured_Latent_Variable_Models.html">91 jmlr-2010-Posterior Regularization for Structured Latent Variable Models</a></p>
<p>13 0.044818748 <a title="74-tfidf-13" href="./jmlr-2010-Classification_Methods_with_Reject_Option_Based_on_Convex_Risk_Minimization.html">21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</a></p>
<p>14 0.04448488 <a title="74-tfidf-14" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>15 0.042251665 <a title="74-tfidf-15" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>16 0.039236881 <a title="74-tfidf-16" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>17 0.039049305 <a title="74-tfidf-17" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>18 0.037611969 <a title="74-tfidf-18" href="./jmlr-2010-Composite_Binary_Losses.html">25 jmlr-2010-Composite Binary Losses</a></p>
<p>19 0.037511688 <a title="74-tfidf-19" href="./jmlr-2010-Maximum_Likelihood_in_Cost-Sensitive_Learning%3A_Model_Specification%2C_Approximations%2C_and_Upper_Bounds.html">73 jmlr-2010-Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds</a></p>
<p>20 0.036652882 <a title="74-tfidf-20" href="./jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">48 jmlr-2010-How to Explain Individual Classification Decisions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.18), (1, -0.033), (2, -0.003), (3, 0.136), (4, -0.002), (5, 0.105), (6, -0.043), (7, 0.047), (8, -0.006), (9, -0.071), (10, 0.044), (11, -0.081), (12, 0.084), (13, 0.067), (14, -0.056), (15, 0.019), (16, 0.042), (17, -0.093), (18, 0.003), (19, -0.08), (20, 0.11), (21, 0.058), (22, -0.008), (23, 0.016), (24, 0.183), (25, -0.148), (26, 0.073), (27, 0.074), (28, 0.216), (29, -0.079), (30, -0.037), (31, -0.006), (32, 0.039), (33, 0.114), (34, -0.062), (35, -0.083), (36, 0.01), (37, 0.04), (38, -0.042), (39, -0.096), (40, 0.0), (41, 0.093), (42, -0.084), (43, -0.027), (44, 0.069), (45, 0.117), (46, -0.026), (47, 0.054), (48, 0.06), (49, -0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8803274 <a title="74-lsi-1" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>Author: Pannagadatta K. Shivaswamy, Tony Jebara</p><p>Abstract: Leading classiﬁcation methods such as support vector machines (SVMs) and their counterparts achieve strong generalization performance by maximizing the margin of separation between data classes. While the maximum margin approach has achieved promising performance, this article identiﬁes its sensitivity to afﬁne transformations of the data and to directions with large data spread. Maximum margin solutions may be misled by the spread of data and preferentially separate classes along large spread directions. This article corrects these weaknesses by measuring margin not in the absolute sense but rather only relative to the spread of data in any projection direction. Maximum relative margin corresponds to a data-dependent regularization on the classiﬁcation function while maximum absolute margin corresponds to an ℓ2 norm constraint on the classiﬁcation function. Interestingly, the proposed improvements only require simple extensions to existing maximum margin formulations and preserve the computational efﬁciency of SVMs. Through the maximization of relative margin, surprising performance gains are achieved on real-world problems such as digit, text classiﬁcation and on several other benchmark data sets. In addition, risk bounds are derived for the new formulation based on Rademacher averages. Keywords: support vector machines, kernel methods, large margin, Rademacher complexity</p><p>2 0.65764785 <a title="74-lsi-2" href="./jmlr-2010-Tree_Decomposition_for_Large-Scale_SVM_Problems.html">113 jmlr-2010-Tree Decomposition for Large-Scale SVM Problems</a></p>
<p>Author: Fu Chang, Chien-Yang Guo, Xiao-Rong Lin, Chi-Jen Lu</p><p>Abstract: To handle problems created by large data sets, we propose a method that uses a decision tree to decompose a given data space and train SVMs on the decomposed regions. Although there are other means of decomposing a data space, we show that the decision tree has several merits for large-scale SVM training. First, it can classify some data points by its own means, thereby reducing the cost of SVM training for the remaining data points. Second, it is efﬁcient in determining the parameter values that maximize the validation accuracy, which helps maintain good test accuracy. Third, the tree decomposition method can derive a generalization error bound for the classiﬁer. For data sets whose size can be handled by current non-linear, or kernel-based, SVM training techniques, the proposed method can speed up the training by a factor of thousands, and still achieve comparable test accuracy. Keywords: binary tree, generalization error ¨bound, margin-based theory, pattern classiﬁcation, ı tree decomposition, support vector machine, VC theory</p><p>3 0.6219731 <a title="74-lsi-3" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>Author: Nicola Segata, Enrico Blanzieri</p><p>Abstract: A computationally efﬁcient approach to local learning with kernel methods is presented. The Fast Local Kernel Support Vector Machine (FaLK-SVM) trains a set of local SVMs on redundant neighbourhoods in the training set and an appropriate model for each query point is selected at testing time according to a proximity strategy. Supported by a recent result by Zakai and Ritov (2009) relating consistency and localizability, our approach achieves high classiﬁcation accuracies by dividing the separation function in local optimisation problems that can be handled very efﬁciently from the computational viewpoint. The introduction of a fast local model selection further speeds-up the learning process. Learning and complexity bounds are derived for FaLK-SVM, and the empirical evaluation of the approach (with data sets up to 3 million points) showed that it is much faster and more accurate and scalable than state-of-the-art accurate and approximated SVM solvers at least for non high-dimensional data sets. More generally, we show that locality can be an important factor to sensibly speed-up learning approaches and kernel methods, differently from other recent techniques that tend to dismiss local information in order to improve scalability. Keywords: locality, kernel methods, local learning algorithms, support vector machines, instancebased learning</p><p>4 0.57928163 <a title="74-lsi-4" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>Author: Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, Chih-Jen Lin</p><p>Abstract: Kernel techniques have long been used in SVM to handle linearly inseparable problems by transforming data to a high dimensional space, but training and testing large data sets is often time consuming. In contrast, we can efﬁciently train and test much larger data sets using linear SVM without kernels. In this work, we apply fast linear-SVM methods to the explicit form of polynomially mapped data and investigate implementation issues. The approach enjoys fast training and testing, but may sometimes achieve accuracy close to that of using highly nonlinear kernels. Empirical experiments show that the proposed method is useful for certain large-scale data sets. We successfully apply the proposed method to a natural language processing (NLP) application by improving the testing accuracy under some training/testing speed requirements. Keywords: decomposition methods, low-degree polynomial mapping, kernel functions, support vector machines, dependency parsing, natural language processing</p><p>5 0.47151509 <a title="74-lsi-5" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>Author: Shiliang Sun, John Shawe-Taylor</p><p>Abstract: In this paper, we propose a general framework for sparse semi-supervised learning, which concerns using a small portion of unlabeled data and a few labeled data to represent target functions and thus has the merit of accelerating function evaluations when predicting the output of a new example. This framework makes use of Fenchel-Legendre conjugates to rewrite a convex insensitive loss involving a regularization with unlabeled data, and is applicable to a family of semi-supervised learning methods such as multi-view co-regularized least squares and single-view Laplacian support vector machines (SVMs). As an instantiation of this framework, we propose sparse multi-view SVMs which use a squared ε-insensitive loss. The resultant optimization is an inf-sup problem and the optimal solutions have arguably saddle-point properties. We present a globally optimal iterative algorithm to optimize the problem. We give the margin bound on the generalization error of the sparse multi-view SVMs, and derive the empirical Rademacher complexity for the induced function class. Experiments on artiﬁcial and real-world data show their effectiveness. We further give a sequential training approach to show their possibility and potential for uses in large-scale problems and provide encouraging experimental results indicating the efﬁcacy of the margin bound and empirical Rademacher complexity on characterizing the roles of unlabeled data for semi-supervised learning. Keywords: semi-supervised learning, Fenchel-Legendre conjugate, representer theorem, multiview regularization, support vector machine, statistical learning theory</p><p>6 0.39137775 <a title="74-lsi-6" href="./jmlr-2010-Consensus-Based_Distributed_Support_Vector_Machines.html">26 jmlr-2010-Consensus-Based Distributed Support Vector Machines</a></p>
<p>7 0.37160811 <a title="74-lsi-7" href="./jmlr-2010-Linear_Algorithms_for_Online_Multitask_Classification.html">66 jmlr-2010-Linear Algorithms for Online Multitask Classification</a></p>
<p>8 0.36442915 <a title="74-lsi-8" href="./jmlr-2010-A_Streaming_Parallel_Decision_Tree_Algorithm.html">7 jmlr-2010-A Streaming Parallel Decision Tree Algorithm</a></p>
<p>9 0.3617374 <a title="74-lsi-9" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>10 0.34963107 <a title="74-lsi-10" href="./jmlr-2010-Chromatic_PAC-Bayes_Bounds_for_Non-IID_Data%3A_Applications_to_Ranking_and_Stationary_%CE%B2-Mixing_Processes.html">20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</a></p>
<p>11 0.30559808 <a title="74-lsi-11" href="./jmlr-2010-Maximum_Likelihood_in_Cost-Sensitive_Learning%3A_Model_Specification%2C_Approximations%2C_and_Upper_Bounds.html">73 jmlr-2010-Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds</a></p>
<p>12 0.30444098 <a title="74-lsi-12" href="./jmlr-2010-Classification_Methods_with_Reject_Option_Based_on_Convex_Risk_Minimization.html">21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</a></p>
<p>13 0.30146161 <a title="74-lsi-13" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>14 0.2946077 <a title="74-lsi-14" href="./jmlr-2010-On_Spectral_Learning.html">84 jmlr-2010-On Spectral Learning</a></p>
<p>15 0.27623674 <a title="74-lsi-15" href="./jmlr-2010-Learning_Translation_Invariant_Kernels_for_Classification.html">65 jmlr-2010-Learning Translation Invariant Kernels for Classification</a></p>
<p>16 0.27520925 <a title="74-lsi-16" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>17 0.27417424 <a title="74-lsi-17" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>18 0.27261129 <a title="74-lsi-18" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>19 0.26429084 <a title="74-lsi-19" href="./jmlr-2010-Quadratic_Programming_Feature_Selection.html">94 jmlr-2010-Quadratic Programming Feature Selection</a></p>
<p>20 0.25915489 <a title="74-lsi-20" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.012), (3, 0.019), (4, 0.015), (8, 0.023), (21, 0.018), (24, 0.01), (32, 0.08), (36, 0.033), (37, 0.071), (71, 0.303), (75, 0.17), (81, 0.023), (85, 0.093), (96, 0.022), (97, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82922351 <a title="74-lda-1" href="./jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">48 jmlr-2010-How to Explain Individual Classification Decisions</a></p>
<p>Author: David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, Klaus-Robert Müller</p><p>Abstract: After building a classiﬁer with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most inﬂuential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classiﬁcation method. Keywords: explaining, nonlinear, black box model, kernel methods, Ames mutagenicity</p><p>same-paper 2 0.72063923 <a title="74-lda-2" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>Author: Pannagadatta K. Shivaswamy, Tony Jebara</p><p>Abstract: Leading classiﬁcation methods such as support vector machines (SVMs) and their counterparts achieve strong generalization performance by maximizing the margin of separation between data classes. While the maximum margin approach has achieved promising performance, this article identiﬁes its sensitivity to afﬁne transformations of the data and to directions with large data spread. Maximum margin solutions may be misled by the spread of data and preferentially separate classes along large spread directions. This article corrects these weaknesses by measuring margin not in the absolute sense but rather only relative to the spread of data in any projection direction. Maximum relative margin corresponds to a data-dependent regularization on the classiﬁcation function while maximum absolute margin corresponds to an ℓ2 norm constraint on the classiﬁcation function. Interestingly, the proposed improvements only require simple extensions to existing maximum margin formulations and preserve the computational efﬁciency of SVMs. Through the maximization of relative margin, surprising performance gains are achieved on real-world problems such as digit, text classiﬁcation and on several other benchmark data sets. In addition, risk bounds are derived for the new formulation based on Rademacher averages. Keywords: support vector machines, kernel methods, large margin, Rademacher complexity</p><p>3 0.58149773 <a title="74-lda-3" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>Author: Pinar Donmez, Guy Lebanon, Krishnakumar Balasubramanian</p><p>Abstract: Estimating the error rates of classiﬁers or regression models is a fundamental task in machine learning which has thus far been studied exclusively using supervised learning techniques. We propose a novel unsupervised framework for estimating these error rates using only unlabeled data and mild assumptions. We prove consistency results for the framework and demonstrate its practical applicability on both synthetic and real world data. Keywords: classiﬁcation and regression, maximum likelihood, latent variable models</p><p>4 0.58139759 <a title="74-lda-4" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>Author: Shiliang Sun, John Shawe-Taylor</p><p>Abstract: In this paper, we propose a general framework for sparse semi-supervised learning, which concerns using a small portion of unlabeled data and a few labeled data to represent target functions and thus has the merit of accelerating function evaluations when predicting the output of a new example. This framework makes use of Fenchel-Legendre conjugates to rewrite a convex insensitive loss involving a regularization with unlabeled data, and is applicable to a family of semi-supervised learning methods such as multi-view co-regularized least squares and single-view Laplacian support vector machines (SVMs). As an instantiation of this framework, we propose sparse multi-view SVMs which use a squared ε-insensitive loss. The resultant optimization is an inf-sup problem and the optimal solutions have arguably saddle-point properties. We present a globally optimal iterative algorithm to optimize the problem. We give the margin bound on the generalization error of the sparse multi-view SVMs, and derive the empirical Rademacher complexity for the induced function class. Experiments on artiﬁcial and real-world data show their effectiveness. We further give a sequential training approach to show their possibility and potential for uses in large-scale problems and provide encouraging experimental results indicating the efﬁcacy of the margin bound and empirical Rademacher complexity on characterizing the roles of unlabeled data for semi-supervised learning. Keywords: semi-supervised learning, Fenchel-Legendre conjugate, representer theorem, multiview regularization, support vector machine, statistical learning theory</p><p>5 0.57851893 <a title="74-lda-5" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>Author: Ming Yuan</p><p>Abstract: This paper considers the problem of estimating a high dimensional inverse covariance matrix that can be well approximated by “sparse” matrices. Taking advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix, we propose an estimating procedure that can effectively exploit such “sparsity”. The proposed method can be computed using linear programming and therefore has the potential to be used in very high dimensional problems. Oracle inequalities are established for the estimation error in terms of several operator norms, showing that the method is adaptive to different types of sparsity of the problem. Keywords: covariance selection, Dantzig selector, Gaussian graphical model, inverse covariance matrix, Lasso, linear programming, oracle inequality, sparsity</p><p>6 0.57586873 <a title="74-lda-6" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>7 0.57320988 <a title="74-lda-7" href="./jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</a></p>
<p>8 0.57204789 <a title="74-lda-8" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>9 0.57128012 <a title="74-lda-9" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>10 0.5706439 <a title="74-lda-10" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>11 0.56880742 <a title="74-lda-11" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>12 0.56791055 <a title="74-lda-12" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>13 0.56773192 <a title="74-lda-13" href="./jmlr-2010-Bundle_Methods_for_Regularized_Risk_Minimization.html">18 jmlr-2010-Bundle Methods for Regularized Risk Minimization</a></p>
<p>14 0.56581038 <a title="74-lda-14" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>15 0.56437236 <a title="74-lda-15" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>16 0.56427807 <a title="74-lda-16" href="./jmlr-2010-Tree_Decomposition_for_Large-Scale_SVM_Problems.html">113 jmlr-2010-Tree Decomposition for Large-Scale SVM Problems</a></p>
<p>17 0.56365275 <a title="74-lda-17" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>18 0.56273454 <a title="74-lda-18" href="./jmlr-2010-Stacked_Denoising_Autoencoders%3A_Learning_Useful_Representations_in_a_Deep_Network_with_a_Local_Denoising_Criterion.html">107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</a></p>
<p>19 0.56247765 <a title="74-lda-19" href="./jmlr-2010-Linear_Algorithms_for_Online_Multitask_Classification.html">66 jmlr-2010-Linear Algorithms for Online Multitask Classification</a></p>
<p>20 0.56116354 <a title="74-lda-20" href="./jmlr-2010-A_Quasi-Newton_Approach_to_Nonsmooth_Convex_Optimization_Problems_in_Machine_Learning.html">5 jmlr-2010-A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
