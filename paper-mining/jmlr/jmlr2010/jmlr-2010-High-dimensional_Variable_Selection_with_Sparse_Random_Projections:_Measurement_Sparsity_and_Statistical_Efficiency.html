<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-45" href="#">jmlr2010-45</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</h1>
<br/><p>Source: <a title="jmlr-2010-45-pdf" href="http://jmlr.org/papers/volume11/omidiran10a/omidiran10a.pdf">pdf</a></p><p>Author: Dapo Omidiran, Martin J. Wainwright</p><p>Abstract: We consider the problem of high-dimensional variable selection: given n noisy observations of a k-sparse vector β∗ ∈ R p , estimate the subset of non-zero entries of β∗ . A signiﬁcant body of work has studied behavior of ℓ1 -relaxations when applied to random measurement matrices that are dense (e.g., Gaussian, Bernoulli). In this paper, we analyze sparsiﬁed measurement ensembles, and consider the trade-off between measurement sparsity, as measured by the fraction γ of nonzero entries, and the statistical efﬁciency, as measured by the minimal number of observations n required for correct variable selection with probability converging to one. Our main result is to prove that it is possible to let the fraction on non-zero entries γ → 0 at some rate, yielding measurement matrices with a vanishing fraction of non-zeros per row, while retaining the same statistical efﬁciency as dense ensembles. A variety of simulation results conﬁrm the sharpness of our theoretical predictions. Keywords: variable selection, sparse random projections, high-dimensional statistics, Lasso, consistency, ℓ1 -regularization</p><p>Reference: <a title="jmlr-2010-45-reference" href="../jmlr2010_reference/jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A signiﬁcant body of work has studied behavior of ℓ1 -relaxations when applied to random measurement matrices that are dense (e. [sent-7, score-0.54]
</p><p>2 Our main result is to prove that it is possible to let the fraction on non-zero entries γ → 0 at some rate, yielding measurement matrices with a vanishing fraction of non-zeros per row, while retaining the same statistical efﬁciency as dense ensembles. [sent-11, score-0.796]
</p><p>3 Introduction Recent years have witnessed a ﬂurry of research on the recovery of high-dimensional models satisfying some type of sparsity constraint. [sent-14, score-0.294]
</p><p>4 These types of sparse recovery problems arise in a variety of domains, including variable selection in regression (Tibshirani, 1996), graphical model selection (Meinshausen and Buhlmann, 2006; Ravikumar et al. [sent-15, score-0.262]
</p><p>5 , 2010), sparse principal components analysis (Johnstone and Lu, 2009; Paul, 2007), sparse approximation (Tropp, 2006), and compressed sensing (Candes and Tao, 2005; Donoho, 2006). [sent-16, score-0.233]
</p><p>6 A particular instance of high-dimensional sparse recovery involves the linear regression model Y = Xβ∗ +W , where Y ∈ Rn is the observation vector, W ∈ Rn is observation noise, and X ∈ Rn×p is the measurement matrix. [sent-19, score-0.605]
</p><p>7 The problem of high-dimensional regression can be studied either for deterministic designs, for which the measurement matrix X is ﬁxed, or for random designs in which X is drawn randomly from some ensemble. [sent-31, score-0.423]
</p><p>8 Past work on random designs has focused on the behavior of various ℓ1 -relaxations when applied to measurement matrices drawn from the standard Gaussian ensemble (e. [sent-32, score-0.562]
</p><p>9 A measurement matrix X drawn from such a standard random ensemble is dense, in that each row of X has p non-zero entries with high probability. [sent-35, score-0.564]
</p><p>10 In various applications of the sparse regression problem, the measurement matrix is itself a design variable, and dense measurement matrices are undesirable. [sent-36, score-0.979]
</p><p>11 , 2006), it would be preferable to take measurements of the signal β∗ based on sparse inner products, using measurement matrices X in which each row has a relatively small fraction of non-zero entries. [sent-40, score-0.64]
</p><p>12 , 2006; Xu and Hassibi, 2007) has studied compressed sensing methods based on sparse measurement matrices, using constructions motivated by group testing and coding theory. [sent-61, score-0.546]
</p><p>13 Whereas this past work focuses on the noiseless recovery problem, our primary interest in this paper is the noisy linear observation model which, as we show, exhibits qualitatively different behavior than the noiseless case. [sent-62, score-0.256]
</p><p>14 At a high level, our primary goal in this paper is not to design sparse measurement matrices, but rather to gain a theoretical understanding of the trade-off between the 2362  H IGH - DIMENSIONAL VARIABLE S ELECTION  degree of measurement sparsity, and statistical efﬁciency. [sent-63, score-0.815]
</p><p>15 We assess measurement sparsity in terms of the fraction γ of non-zero entries in any particular row of the measurement matrix, and we deﬁne statistical efﬁciency in terms of the minimal number of measurements n required to recover the correct support with probability converging to one. [sent-64, score-1.076]
</p><p>16 Our interest can be viewed in terms of experimental design: more precisely we ask, what degree of measurement sparsity can be permitted without increasing the number of observations required for correct variable selection or subset recovery? [sent-65, score-0.537]
</p><p>17 , 1998; Tibshirani, 1996), where past work on dense Gaussian measurement ensembles (Wainwright, 2009) provides a precise characterization of its success/failure. [sent-67, score-0.614]
</p><p>18 We characterize the density of our measurement ensembles with a positive parameter γ ∈ (0, 1], corresponding to the fraction of nonzero entries per row. [sent-68, score-0.633]
</p><p>19 We ﬁrst show that for all ﬁxed γ ∈ (0, 1], the statistical efﬁciency of the Lasso remains the same as with dense measurement matrices. [sent-69, score-0.483]
</p><p>20 In general, in contrast to the noiseless setting (Xu and Hassibi, 2007), our theory still requires that the average number of non-zeroes per column of the measurement matrix (i. [sent-71, score-0.421]
</p><p>21 , γn) tend to inﬁnity, however, under the loss function considered here (exact signed support recovery), we prove that no method can succeed with probability one if this condition does not hold. [sent-73, score-0.192]
</p><p>22 , 2010) analyzes the information-theoretic limitations of sparse measurement matrices for exact support recovery. [sent-80, score-0.496]
</p><p>23 In addition, we use log (x) to denote the natural logarithm of x. [sent-83, score-0.181]
</p><p>24 , p} | βi = 0}, 2363  O MIDIRAN AND WAINWRIGHT  and consider the class of k-sparse signals of length p:  C (p, k, β∗ ) := min  β ∈ R p | |S(β)| = k ≤  p , min |βi | ≥ β∗ . [sent-90, score-0.214]
</p><p>25 min 2 i∈S  (1)  Let β∗ ∈ R p be a ﬁxed but unknown vector in C (p, k, β∗ ), and suppose that we make a set min {Y1 , . [sent-91, score-0.214]
</p><p>26 ) observations of the unknown vector β∗ , each of the form Yi := xiT β∗ +Wi ,  (2)  where Wi ∼ N (0, 1) is observation noise, and xi ∈ R p is a measurement vector. [sent-97, score-0.376]
</p><p>27 Note that there is no loss in generality in assuming that the noise variance is one, since the observation model with signal class C (p, k, β∗ ) and Wi ∼ N (0, 1) is equivalent to the observation model with signal class min ∗ C (p, k, βmin ) with noise variance Wi ∼ N (0, σ2 ). [sent-98, score-0.279]
</p><p>28 A large body of past work has focused on the model selection behavior of the Lasso for both deterministic and random measurement matrices (e. [sent-124, score-0.433]
</p><p>29 Tropp (2006) demonstrates that under some technical conditions the Lasso produces an estimate with support contained in the true support set, while Zhao and Yu (2006) prove the sign consistency of the Lasso for certain sequences (n, kn , pn ) and measurement ensemble covariances. [sent-127, score-0.579]
</p><p>30 The main contribution of Wainwright (2009) is to identify the smallest n required for sign consistency of the Lasso when applied to measurement matrices X drawn randomly from Gaussian ensembles, with the standard Gaussian ensemble (i. [sent-128, score-0.593]
</p><p>31 3 Statement of Main Result A measurement matrix X ∈ Rn×p drawn randomly from a Gaussian ensemble is dense, in that each row has Θ(p) non-zero entries. [sent-140, score-0.494]
</p><p>32 The main focus of this paper is the observation model (2), using measurement ensembles that are designed to be sparse. [sent-141, score-0.507]
</p><p>33 To formalize the notion of sparsity, we let γ ∈ (0, 1] represent a measurement sparsity parameter, corresponding to the (average) fraction of non-zero entries per row. [sent-142, score-0.63]
</p><p>34 For a given choice of γ, we consider measurement matrices X with i. [sent-144, score-0.433]
</p><p>35 In fact, the analysis of this paper establishes exactly the same control parameter threshold (5) for γ-sparsiﬁed measurement ensembles, for any ﬁxed γ ∈ (0, 1), as the completely dense case (γ = 1). [sent-149, score-0.525]
</p><p>36 In particular, we state the following result on conditions under which the Lasso applied to sparsiﬁed ensembles has the same sample complexity as when applied to the dense (standard Gaussian) ensemble: Theorem 1 Suppose that the measurement matrix X ∈ Rn×p is drawn with i. [sent-150, score-0.657]
</p><p>37 (b) However, Theorem 1 also allows for general scalings of the measurement sparsity γ along with the triplet (n, p, k). [sent-162, score-0.504]
</p><p>38 (c) Of course, the conditions of Theorem 1 do not allow the measurement sparsity γ to approach zero arbitrarily quickly. [sent-165, score-0.547]
</p><p>39 ) A natural question is whether exact recovery is possible using measurement matrices, either randomly drawn or deterministically designed, with the average number of non-zeros per column (namely γn) remaining bounded. [sent-168, score-0.542]
</p><p>40 In fact, under the criterion of exactly recovering the signed support (3), as shown by the following result, if β∗ = O (1), then no method can succeed with probability converging to one min unless γn tends to inﬁnity. [sent-169, score-0.245]
</p><p>41 2366  H IGH - DIMENSIONAL VARIABLE S ELECTION  Proposition 1 If γn[β∗ ]2 does not tend to inﬁnity, then no method can recover the signed support min with probability one. [sent-170, score-0.236]
</p><p>42 The average Y = n1 ∑n1 [Yi − ∑ j∈T (i) Xi j β∗ ] is a j 1 1 min min i=1 1 sufﬁcient statistic, following the distribution Y ∼ N (β∗ , n1 ). [sent-185, score-0.214]
</p><p>43 Unless the effective signal-to-noise min ratio, which is of the order n1 [β∗ ]2 , goes to inﬁnity, there will always be a constant probability of min error in distinguishing β∗ = β∗ from β∗ = −β∗ . [sent-186, score-0.214]
</p><p>44 Under the γ-sparsiﬁed random ensemble, we 1 1 min min have n1 ≤ (1 + o(1)) γn with high probability, so that no method can succeed unless γn[β∗ ]2 goes min to inﬁnity, as claimed. [sent-187, score-0.364]
</p><p>45 In particular, condition (9) immin plies that ρ2 = o([β∗ ]2 ), and condition (8) implies that nγρ2 → +∞, which veriﬁes the condition n n min of Proposition 1. [sent-189, score-0.269]
</p><p>46 We begin with a high-level outline of the proof; as with previous work on dense Gaussian ensembles (Wainwright, 2009), the key is the notion of a primal-dual witness for exact signed support recovery. [sent-192, score-0.333]
</p><p>47 The proof itself involves a number of additional steps not needed in this past work, in order to gain good control on sparse matrices as opposed to generic Gaussian matrices (see Appendix D). [sent-193, score-0.262]
</p><p>48 The foundation of our proof is the following lemma: it provides sufﬁcient conditions for the Lasso (4) to recover the signed support set. [sent-205, score-0.215]
</p><p>49 2 for details) shows that ρn z j = V ja + V jb , and that Ui = βi − β∗ . [sent-217, score-0.437]
</p><p>50 Lemma 3 (Control of V a ) Under the conditions of Theorem 1, there exists a ﬁxed positive value δ (dependent on ε) such that P[max |V ja | ≥ (1 − δ)ρn ] → 0. [sent-220, score-0.324]
</p><p>51 c j∈S  Lemma 4 (Control of V b ) Under the conditions of Theorem 1, there exists a ﬁxed positive value δ (dependent on ε) P[max |V jb | ≥ δρn ] → 0. [sent-221, score-0.199]
</p><p>52 If we deﬁne the n-dimensional vector h := XS (ΣSS )−1 1,  (23)  then the variable V ja can be written compactly as V ja ρn  = X jT h =  n  ∑ hℓ Xℓ j . [sent-225, score-0.595]
</p><p>53 With these deﬁnitions, by construction, we have that γ V ja ρn  n  ∑ Hℓj Zℓ j . [sent-235, score-0.281]
</p><p>54 ) Consequently, we may condition on H j without affecting Z, and since Z is Gaussian, we / V ja Hj 2 have ( ρn | H j ) ∼ N (0, γ 2 ). [sent-241, score-0.335]
</p><p>55 Therefore, if we can obtain good control on the norm H j 2 , then we can use standard Gaussian tail bounds (see Appendix A) to control the maximum max j∈Sc V ja /ρn . [sent-242, score-0.49]
</p><p>56 n 2k The primary implication of the above bound is that each V ja /ρn variable is (essentially) no larger k than a N (0, n ) variable. [sent-244, score-0.352]
</p><p>57 We can then use standard techniques for bounding the tails of Gaussian variables to obtain good control over the random variable max j∈Sc |V ja |/ρn . [sent-245, score-0.409]
</p><p>58 In particular, by the union bound, we have P  Hj  2 2  ≤  n  P[max |V ja | ≥ (1 − δ)ρn ] ≤ (p − k) P[ ∑ Hℓ Zℓ j ≥ (1 − δ)]. [sent-246, score-0.281]
</p><p>59 c j∈S  ℓ=1  For any δ > 0, deﬁne the event T  j (δ)  :=  P[max |V ja | ≥ (1 − δ)ρn ] ≤ (p − k) c j∈S  j  { H j 2 ≤ kγ(1+δ) }. [sent-247, score-0.326]
</p><p>60 Finally, let us assume the condition n > (2 + ε)k log (p − k) for some ﬁxed ε > 0. [sent-249, score-0.235]
</p><p>61 P[max |V ja | ≥ (1 − δ)ρn ] ≤ (p − k) 2 exp − c j∈S  Note that the above inequality holds for all values of ε. [sent-251, score-0.413]
</p><p>62 S n  P[max |V jb | ≥ δρn ] = P[max X jT Π⊥ ( S c c j∈S  j∈S  (24)  Now since Π⊥ is an orthogonal projection matrix, and the column vector X j , noise vector W and S randomness in Π⊥ are all independent, we have the bound S ≤ P X jT (W /n) ≥ δρn ,  P X jT Π⊥ (W /n) ≥ δρn S  (25)  For each ℓ = 1, . [sent-256, score-0.228]
</p><p>63 (26) 2 k 16 Finally, putting together the pieces from Equations (26), (25), and (24) we obtain that P[max j∈Sc |V jb | ≥ δρn ] is upper bounded by (p − k) 2 exp(−  1 3δ2 nγ2 (δ2 ρ2 ) n n + exp − n(γ + √ ) 1 ) + 2 exp − 1 2k 2(1 + δ1 )(γ + 2√k ) 2 k 16  . [sent-272, score-0.42]
</p><p>64 Once this occurs, we have the 2 γ inequality γ+γ √ > 2 , and hence 1 2 k  (p − k) exp −  nγ2 (δ2 ρ2 ) n 1 2(1 + δ1 )(γ + 2√k )  ≤ (p − k) exp −  nγδ2 ρ2 n 4(1 + δ1 )  = (p − k) exp − log (p − k)  nγδ2 ρ2 n . [sent-276, score-0.577]
</p><p>65 4(1 + δ1 ) log (p − k)  Recalling that the terms δ and δ1 are ﬁxed constants, condition (10) implies that eventually (p − k) exp − log (p − k)  nγδ2 ρ2 n ≤ (p − k) exp − 3 log (p − k) , 4(1 + δ1 ) log (p − k)  showing that the middle term of Equation (27) goes to zero. [sent-277, score-1.042]
</p><p>66 Finally, using the condition n ≥ (2 + ε)k log (p − k), we obtain 1 3δ2 (p − k) exp − n(γ + √ ) 1 2 k 16  1 3δ2 ≤ (p − k) exp − n( √ ) 1 2 k 16 √ (2 + ε) k 3δ2 ≤ (p − k) exp − log (p − k) 1 . [sent-278, score-0.812]
</p><p>67 Let us deﬁne the function T (γ, k, p, θ,t) :=  1 γ  max  log[θ log(p − k)] log (t) , , θk log (p − k) θ log(p − k)  (28)  as well as the the upper bounds √ m∗ := ρn (1 +C kT (γ, k, p, 1, k)),  and ψ∗ :=  1 (1 +CT (γ, k, p, 1, k)). [sent-283, score-0.415]
</p><p>68 i∈S  i∈S  Conditioning on T and its complement, we have 1 max Ui | > 1] β∗ i∈S min 1 ≤ P[ ∗ max |Ui | > 1 | T (m∗ , ψ∗ )] + P[(T (m∗ , ψ∗ ))c ]. [sent-285, score-0.213]
</p><p>69 For sufﬁciently large p and k, we have T (γ, k, p, 1, k) =  1 γ  ≤  1 γ  max  log[log(p − k)] log (k) , k log (p − k) log(p − k)  log[log(p − k)] , log(p − k)  using the facts that logk(k) → 0 and log[log(p − k)] → ∞, so that the maximum is dominated by the ∗ second term. [sent-295, score-0.415]
</p><p>70 Letting Y ∗ ∼ N (0, ψ∗ ), we ∗ min have β∗ 1 1 ≤ E k P[|Y ∗ | ≥ min | XS , T ] E P ∗ max |Yi | > | XS , T βmin i∈S 2 2 ≤ 2k exp −  [β∗ ]2 min , 8ψ∗  where the last inequality follows from Gaussian tail bounds (see Appendix A). [sent-297, score-0.578]
</p><p>71 Taking logarithms and ignoring constant terms, we have log (k)(1 −  [β∗ ]2 γn [β∗ ]2 min min . [sent-299, score-0.395]
</p><p>72 ) = log (k) 1 − log(k) 8ψ∗ 8 log (k) (1 +CT (γ, k, p, 1, k))  Our goal is to show that that this quantity diverges to −∞. [sent-300, score-0.362]
</p><p>73 Condition (10) implies that T (γ, k, p, 1, k) =  1 γ  max  Hence, it sufﬁces to show that log k 1 − log(k) 1 −  [β∗ ]2 γn min 16 log(k)  log (k) log log(p − k) → 0. [sent-301, score-0.703]
</p><p>74 , k log (p − k) log(p − k)  [β∗ ]2 γn min 16 log k  diverges to −∞. [sent-302, score-0.469]
</p><p>75 = log(k) (1 − min 2 ρn 16 log(p − k) log (k)  = log(k) (1 −  γnρn Condition (9) implies that min → ∞, whereas condition (8) ensures that log(p−k) → ∞. [sent-304, score-0.449]
</p><p>76 We consider two different sparsity regimes, namely linear sparsity (k = αp) and polynomial sparsity √ (k = p), and we show simulations in which the fraction γ of non-zero entries in each row of the measurement matrix X converges to zero. [sent-309, score-0.922]
</p><p>77 Although it is possible to solve the Lasso using a variety of methods, our theory (in particular, Lemma 2) shows that it sufﬁces to simulate the random variables {V ja ,V jb , j ∈ Sc } and {Ui , i ∈ S}, and then check the equivalent conditions (21) and (22). [sent-313, score-0.48]
</p><p>78 (These necessary and sufﬁcient conditions give the same result for support recovery as solving the Lasso; however, they are much faster to simulate. [sent-314, score-0.209]
</p><p>79 5  2  (b)  n Figure 1: Plots of the success probability P[S = S] versus the control parameter θ(n, p, k) = k log(p−k) √ (p−k) for γ-sparsiﬁed ensembles, with decaying measurement sparsity γ = . [sent-346, score-0.644]
</p><p>80 Discussion In this paper, we have studied the problem of recovery the support set of a sparse vector β∗ based on noisy observations. [sent-351, score-0.229]
</p><p>81 The main result is to show that it is possible to “sparsify” standard dense measurement matrices, so that they have a vanishing fraction of non-zeroes per row, while retaining the same sample complexity (number of observations n) required for exact recovery. [sent-352, score-0.613]
</p><p>82 We also showed that under the support recovery metric and in the presence of noise, no method can succeed without the number of non-zeroes per column tending to inﬁnity, so that our results cannot be improved substantially. [sent-353, score-0.209]
</p><p>83 Thus, our results show that it is possible to use sparse measurement matrices while retaining the same guarantees regarding the recovery of the support. [sent-354, score-0.693]
</p><p>84 Although this paper focused on sparsiﬁed Gaussian measurement matrices, it is possible to obtain qualitatively similar results for sparsiﬁed sub-Gaussian ensembles (for instance, the discrete uniform distribution on {−1, 1}). [sent-356, score-0.507]
</p><p>85 , 2006), it may be preferable to make the measurement ensembles even sparser at the cost of taking more measurements n, thereby decreasing statistical efﬁciency relative to dense random matrices. [sent-359, score-0.614]
</p><p>86 Lemma 7 (Hoeffding bound—Hoeffding, 1963) Given a binomial variate Z ∼ Bin(n, γ), we have for any δ > 0 P[|Z − γn| ≥ δn] ≤ 2 exp − 2nδ2 . [sent-372, score-0.184]
</p><p>87 2 Derivation of {V ja ,V jb ,Ui } In this appendix, we derive the form of the {V ja ,V jb } and {Ui } variables deﬁned in Equations (18) through (20). [sent-401, score-0.874]
</p><p>88 One function that sufﬁces is f2 (p, k, γ) :=  1 1+ √ 2 kγ  1 +C  1 γ  max  1 log [(γ + 2√k ) log(p − k)] 1 , 1 1 k(γ − 2√k ) (γ − 2√k ) log(p − k)  . [sent-417, score-0.234]
</p><p>89 Since assumption (10) ensures that γ k → ∞, it sufﬁces to study the simpler function f3 (p, k, γ) :=  1 +C  1 γ  max  1 log [γ log(p − k)] , kγ γ log(p − k)  ,  which has the same asymptotic behavior as f2 (p, k, γ). [sent-424, score-0.234]
</p><p>90 Observe that f3 (p, k, γ) satisﬁes the sandwich relation 1 ≤ f3 (p, k, γ) ≤ 1 +C  max  1 log [log(p − k)] , kγ3 γ3 log(p − k)  ,  By assumption (10), this upper bound converges to one, showing that f3 and hence f2 converge to 2 one, as desired. [sent-425, score-0.272]
</p><p>91 Lemma 10 Suppose that n ≥ (2 + ν)k log (p − k) for some ν > 0. [sent-439, score-0.181]
</p><p>92 Letting Sd−1 denote the ℓ2 unit ball in d dimensions, we begin with the variational representation smax (X) =  max Xu  u∈Sk−1  max max vT Xu. [sent-465, score-0.218]
</p><p>93 2  By the union bound, we have P  max  max vT Xuα > t β  ≤ Mk (ε)Mθn (ε) exp −  uα ∈Ck (ε) vβ ∈Cθn (ε)  γt 2 2  ≤ exp (k + θn) log(3/ε) − By choosing ε =  1 2  and t =  4 γ (k + θn) log 6,  √ smax (X)/ θn = X  γt 2 . [sent-473, score-0.61]
</p><p>94 2  we can conclude that √  2/  θn ≤ C  1 γ  1+  k , θn  with probability at least 1 − exp(−(k + θn) log 6). [sent-474, score-0.181]
</p><p>95 Although this bound is essentially correct for a N (0, 1 ) ensemble with γ γ ﬁxed, it is very crude for the sparsiﬁed case with γ → 0, but will useful in obtaining tighter control on smax (X) and smin (X) := minu∈Sk−1 Xu in the sequel. [sent-479, score-0.221]
</p><p>96 Consequently, for any δ > 0 less than 16/γ (we will in fact take δ → 0), we have Xu θn  P  2 2  −1 > δ  ≤ 2 exp −  δ2 (θn)2 256θn/γ2  = 2 exp −  γ2 δ2 θn . [sent-490, score-0.264]
</p><p>97 ) For any element u ∈ Sk−1 , we have some ui in the cover, and moreover Xu  2  − Xui  2  = ≤  Xu − Xui  ≤ ( X  Xu − Xui  Xu + Xui (2 X )  u − ui ) (2 X ) ≤ 2 X  2382  2  ε. [sent-499, score-0.186]
</p><p>98 Putting together the pieces, we have there is a universal constant C2 > 0, independent of ((θn), k, γ), such that the bound 1 inf Xu θn u∈Sk−1  2  ≥ 1 − δ −C3 ε/γ = 1 −  2 γ  32 f (k, p)k log(3/ε) C2 − ε, θn γ  holds with probability at least 1 − exp(−k f (k, p) log(3/ε)) − exp(− log 6(k + θn)). [sent-501, score-0.25]
</p><p>99 Sharp thresholds for high-dimensional and noisy sparsity recovery using ℓ1 constrained quadratic programming (Lasso). [sent-708, score-0.294]
</p><p>100 Information-theoretic limits on sparse signal recovery: Dense versus sparse measurement matrices. [sent-744, score-0.554]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('measurement', 0.376), ('xs', 0.288), ('ja', 0.281), ('midiran', 0.225), ('lasso', 0.191), ('log', 0.181), ('igh', 0.178), ('wainwright', 0.174), ('ss', 0.168), ('recovery', 0.166), ('xu', 0.157), ('jb', 0.156), ('exp', 0.132), ('ensembles', 0.131), ('sparsity', 0.128), ('election', 0.123), ('dense', 0.107), ('min', 0.107), ('lemma', 0.101), ('sk', 0.099), ('sparsi', 0.098), ('sc', 0.096), ('signed', 0.095), ('ui', 0.093), ('xui', 0.087), ('ensemble', 0.082), ('sign', 0.078), ('jw', 0.074), ('tail', 0.072), ('entries', 0.07), ('sarvotham', 0.069), ('zii', 0.067), ('sparse', 0.063), ('jt', 0.062), ('decaying', 0.059), ('smax', 0.059), ('zsc', 0.059), ('matrices', 0.057), ('sensing', 0.056), ('fraction', 0.056), ('condition', 0.054), ('tropp', 0.053), ('max', 0.053), ('signal', 0.052), ('baraniuk', 0.052), ('dapo', 0.052), ('variate', 0.052), ('compressed', 0.051), ('gaussian', 0.049), ('designs', 0.047), ('noiseless', 0.045), ('event', 0.045), ('hassibi', 0.044), ('wakin', 0.044), ('achlioptas', 0.044), ('vt', 0.044), ('conditions', 0.043), ('vanishing', 0.043), ('candes', 0.043), ('dasgupta', 0.043), ('succeed', 0.043), ('meinshausen', 0.043), ('proof', 0.043), ('control', 0.042), ('zi', 0.041), ('ct', 0.04), ('success', 0.039), ('mi', 0.038), ('bound', 0.038), ('hoeffding', 0.037), ('xit', 0.037), ('projections', 0.037), ('row', 0.036), ('baron', 0.035), ('ncrit', 0.035), ('omidiran', 0.035), ('sharpness', 0.035), ('sparsities', 0.035), ('xsc', 0.035), ('ciency', 0.035), ('buhlmann', 0.034), ('dimensional', 0.034), ('appendix', 0.034), ('recover', 0.034), ('ces', 0.034), ('noise', 0.034), ('donoho', 0.033), ('variable', 0.033), ('consequently', 0.033), ('subdifferential', 0.032), ('inf', 0.031), ('nity', 0.031), ('retaining', 0.031), ('rn', 0.03), ('zhao', 0.03), ('cormode', 0.03), ('info', 0.03), ('ledoux', 0.03), ('vershynin', 0.03), ('claim', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="45-tfidf-1" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>Author: Dapo Omidiran, Martin J. Wainwright</p><p>Abstract: We consider the problem of high-dimensional variable selection: given n noisy observations of a k-sparse vector β∗ ∈ R p , estimate the subset of non-zero entries of β∗ . A signiﬁcant body of work has studied behavior of ℓ1 -relaxations when applied to random measurement matrices that are dense (e.g., Gaussian, Bernoulli). In this paper, we analyze sparsiﬁed measurement ensembles, and consider the trade-off between measurement sparsity, as measured by the fraction γ of nonzero entries, and the statistical efﬁciency, as measured by the minimal number of observations n required for correct variable selection with probability converging to one. Our main result is to prove that it is possible to let the fraction on non-zero entries γ → 0 at some rate, yielding measurement matrices with a vanishing fraction of non-zeros per row, while retaining the same statistical efﬁciency as dense ensembles. A variety of simulation results conﬁrm the sharpness of our theoretical predictions. Keywords: variable selection, sparse random projections, high-dimensional statistics, Lasso, consistency, ℓ1 -regularization</p><p>2 0.25046951 <a title="45-tfidf-2" href="./jmlr-2010-Message-passing_for_Graph-structured_Linear_Programs%3A_Proximal_Methods_and_Rounding_Schemes.html">76 jmlr-2010-Message-passing for Graph-structured Linear Programs: Proximal Methods and Rounding Schemes</a></p>
<p>Author: Pradeep Ravikumar, Alekh Agarwal, Martin J. Wainwright</p><p>Abstract: The problem of computing a maximum a posteriori (MAP) conﬁguration is a central computational challenge associated with Markov random ﬁelds. There has been some focus on “tree-based” linear programming (LP) relaxations for the MAP problem. This paper develops a family of super-linearly convergent algorithms for solving these LPs, based on proximal minimization schemes using Bregman divergences. As with standard message-passing on graphs, the algorithms are distributed and exploit the underlying graphical structure, and so scale well to large problems. Our algorithms have a double-loop character, with the outer loop corresponding to the proximal sequence, and an inner loop of cyclic Bregman projections used to compute each proximal update. We establish convergence guarantees for our algorithms, and illustrate their performance via some simulations. We also develop two classes of rounding schemes, deterministic and randomized, for obtaining integral conﬁgurations from the LP solutions. Our deterministic rounding schemes use a “re-parameterization” property of our algorithms so that when the LP solution is integral, the MAP solution can be obtained even before the LP-solver converges to the optimum. We also propose graph-structured randomized rounding schemes applicable to iterative LP-solving algorithms in general. We analyze the performance of and report simulations comparing these rounding schemes. Keywords: graphical models, MAP Estimation, LP relaxation, proximal minimization, rounding schemes</p><p>3 0.23471038 <a title="45-tfidf-3" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>Author: Garvesh Raskutti, Martin J. Wainwright, Bin Yu</p><p>Abstract: Methods based on ℓ1 -relaxation, such as basis pursuit and the Lasso, are very popular for sparse regression in high dimensions. The conditions for success of these methods are now well-understood: (1) exact recovery in the noiseless setting is possible if and only if the design matrix X satisﬁes the restricted nullspace property, and (2) the squared ℓ2 -error of a Lasso estimate decays at the minimax optimal rate k log p , where k is the sparsity of the p-dimensional regression problem with additive n Gaussian noise, whenever the design satisﬁes a restricted eigenvalue condition. The key issue is thus to determine when the design matrix X satisﬁes these desirable properties. Thus far, there have been numerous results showing that the restricted isometry property, which implies both the restricted nullspace and eigenvalue conditions, is satisﬁed when all entries of X are independent and identically distributed (i.i.d.), or the rows are unitary. This paper proves directly that the restricted nullspace and eigenvalue conditions hold with high probability for quite general classes of Gaussian matrices for which the predictors may be highly dependent, and hence restricted isometry conditions can be violated with high probability. In this way, our results extend the attractive theoretical guarantees on ℓ1 -relaxations to a much broader class of problems than the case of completely independent or unitary designs. Keywords: Lasso, basis pursuit, random matrix theory, Gaussian comparison inequality, concentration of measure</p><p>4 0.13746908 <a title="45-tfidf-4" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>Author: Ming Yuan</p><p>Abstract: This paper considers the problem of estimating a high dimensional inverse covariance matrix that can be well approximated by “sparse” matrices. Taking advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix, we propose an estimating procedure that can effectively exploit such “sparsity”. The proposed method can be computed using linear programming and therefore has the potential to be used in very high dimensional problems. Oracle inequalities are established for the estimation error in terms of several operator norms, showing that the method is adaptive to different types of sparsity of the problem. Keywords: covariance selection, Dantzig selector, Gaussian graphical model, inverse covariance matrix, Lasso, linear programming, oracle inequality, sparsity</p><p>5 0.1013991 <a title="45-tfidf-5" href="./jmlr-2010-Rate_Minimaxity_of_the_Lasso_and_Dantzig_Selector_for_thelqLoss_inlrBalls.html">96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</a></p>
<p>Author: Fei Ye, Cun-Hui Zhang</p><p>Abstract: We consider the estimation of regression coefﬁcients in a high-dimensional linear model. For regression coefﬁcients in ℓr balls, we provide lower bounds for the minimax ℓq risk and minimax quantiles of the ℓq loss for all design matrices. Under an ℓ0 sparsity condition on a target coefﬁcient vector, we sharpen and unify existing oracle inequalities for the Lasso and Dantzig selector. We derive oracle inequalities for target coefﬁcient vectors with many small elements and smaller threshold levels than the universal threshold. These oracle inequalities provide sufﬁcient conditions on the design matrix for the rate minimaxity of the Lasso and Dantzig selector for the ℓq risk and loss in ℓr balls, 0 ≤ r ≤ 1 ≤ q ≤ ∞. By allowing q = ∞, our risk bounds imply the variable selection consistency of threshold Lasso and Dantzig selectors. Keywords: variable selection, estimation, oracle inequality, minimax, linear regression, penalized least squares, linear programming</p><p>6 0.093927905 <a title="45-tfidf-6" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>7 0.089813232 <a title="45-tfidf-7" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>8 0.086085014 <a title="45-tfidf-8" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>9 0.079631075 <a title="45-tfidf-9" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>10 0.068319529 <a title="45-tfidf-10" href="./jmlr-2010-Learning_Translation_Invariant_Kernels_for_Classification.html">65 jmlr-2010-Learning Translation Invariant Kernels for Classification</a></p>
<p>11 0.067944951 <a title="45-tfidf-11" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>12 0.066979639 <a title="45-tfidf-12" href="./jmlr-2010-A_Fast_Hybrid_Algorithm_for_Large-Scalel1-Regularized_Logistic_Regression.html">3 jmlr-2010-A Fast Hybrid Algorithm for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>13 0.063465126 <a title="45-tfidf-13" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>14 0.06071597 <a title="45-tfidf-14" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>15 0.052698094 <a title="45-tfidf-15" href="./jmlr-2010-Bundle_Methods_for_Regularized_Risk_Minimization.html">18 jmlr-2010-Bundle Methods for Regularized Risk Minimization</a></p>
<p>16 0.0487133 <a title="45-tfidf-16" href="./jmlr-2010-Linear_Algorithms_for_Online_Multitask_Classification.html">66 jmlr-2010-Linear Algorithms for Online Multitask Classification</a></p>
<p>17 0.04862085 <a title="45-tfidf-17" href="./jmlr-2010-Classification_Methods_with_Reject_Option_Based_on_Convex_Risk_Minimization.html">21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</a></p>
<p>18 0.04816144 <a title="45-tfidf-18" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>19 0.047236074 <a title="45-tfidf-19" href="./jmlr-2010-Posterior_Regularization_for_Structured_Latent_Variable_Models.html">91 jmlr-2010-Posterior Regularization for Structured Latent Variable Models</a></p>
<p>20 0.046790771 <a title="45-tfidf-20" href="./jmlr-2010-Gaussian_Processes_for_Machine_Learning_%28GPML%29_Toolbox.html">41 jmlr-2010-Gaussian Processes for Machine Learning (GPML) Toolbox</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.264), (1, -0.177), (2, 0.182), (3, -0.105), (4, -0.173), (5, -0.161), (6, 0.075), (7, -0.179), (8, -0.033), (9, 0.027), (10, 0.257), (11, -0.087), (12, 0.068), (13, 0.22), (14, -0.252), (15, -0.037), (16, 0.016), (17, 0.087), (18, 0.144), (19, 0.153), (20, 0.084), (21, 0.088), (22, -0.03), (23, -0.005), (24, -0.111), (25, 0.022), (26, 0.004), (27, 0.007), (28, -0.101), (29, -0.062), (30, 0.041), (31, 0.09), (32, 0.042), (33, 0.011), (34, 0.107), (35, -0.038), (36, -0.135), (37, 0.048), (38, -0.039), (39, -0.053), (40, -0.04), (41, -0.083), (42, -0.02), (43, 0.056), (44, -0.047), (45, -0.025), (46, 0.107), (47, 0.037), (48, -0.035), (49, 0.113)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94463575 <a title="45-lsi-1" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>Author: Dapo Omidiran, Martin J. Wainwright</p><p>Abstract: We consider the problem of high-dimensional variable selection: given n noisy observations of a k-sparse vector β∗ ∈ R p , estimate the subset of non-zero entries of β∗ . A signiﬁcant body of work has studied behavior of ℓ1 -relaxations when applied to random measurement matrices that are dense (e.g., Gaussian, Bernoulli). In this paper, we analyze sparsiﬁed measurement ensembles, and consider the trade-off between measurement sparsity, as measured by the fraction γ of nonzero entries, and the statistical efﬁciency, as measured by the minimal number of observations n required for correct variable selection with probability converging to one. Our main result is to prove that it is possible to let the fraction on non-zero entries γ → 0 at some rate, yielding measurement matrices with a vanishing fraction of non-zeros per row, while retaining the same statistical efﬁciency as dense ensembles. A variety of simulation results conﬁrm the sharpness of our theoretical predictions. Keywords: variable selection, sparse random projections, high-dimensional statistics, Lasso, consistency, ℓ1 -regularization</p><p>2 0.76558197 <a title="45-lsi-2" href="./jmlr-2010-Message-passing_for_Graph-structured_Linear_Programs%3A_Proximal_Methods_and_Rounding_Schemes.html">76 jmlr-2010-Message-passing for Graph-structured Linear Programs: Proximal Methods and Rounding Schemes</a></p>
<p>Author: Pradeep Ravikumar, Alekh Agarwal, Martin J. Wainwright</p><p>Abstract: The problem of computing a maximum a posteriori (MAP) conﬁguration is a central computational challenge associated with Markov random ﬁelds. There has been some focus on “tree-based” linear programming (LP) relaxations for the MAP problem. This paper develops a family of super-linearly convergent algorithms for solving these LPs, based on proximal minimization schemes using Bregman divergences. As with standard message-passing on graphs, the algorithms are distributed and exploit the underlying graphical structure, and so scale well to large problems. Our algorithms have a double-loop character, with the outer loop corresponding to the proximal sequence, and an inner loop of cyclic Bregman projections used to compute each proximal update. We establish convergence guarantees for our algorithms, and illustrate their performance via some simulations. We also develop two classes of rounding schemes, deterministic and randomized, for obtaining integral conﬁgurations from the LP solutions. Our deterministic rounding schemes use a “re-parameterization” property of our algorithms so that when the LP solution is integral, the MAP solution can be obtained even before the LP-solver converges to the optimum. We also propose graph-structured randomized rounding schemes applicable to iterative LP-solving algorithms in general. We analyze the performance of and report simulations comparing these rounding schemes. Keywords: graphical models, MAP Estimation, LP relaxation, proximal minimization, rounding schemes</p><p>3 0.71366602 <a title="45-lsi-3" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>Author: Garvesh Raskutti, Martin J. Wainwright, Bin Yu</p><p>Abstract: Methods based on ℓ1 -relaxation, such as basis pursuit and the Lasso, are very popular for sparse regression in high dimensions. The conditions for success of these methods are now well-understood: (1) exact recovery in the noiseless setting is possible if and only if the design matrix X satisﬁes the restricted nullspace property, and (2) the squared ℓ2 -error of a Lasso estimate decays at the minimax optimal rate k log p , where k is the sparsity of the p-dimensional regression problem with additive n Gaussian noise, whenever the design satisﬁes a restricted eigenvalue condition. The key issue is thus to determine when the design matrix X satisﬁes these desirable properties. Thus far, there have been numerous results showing that the restricted isometry property, which implies both the restricted nullspace and eigenvalue conditions, is satisﬁed when all entries of X are independent and identically distributed (i.i.d.), or the rows are unitary. This paper proves directly that the restricted nullspace and eigenvalue conditions hold with high probability for quite general classes of Gaussian matrices for which the predictors may be highly dependent, and hence restricted isometry conditions can be violated with high probability. In this way, our results extend the attractive theoretical guarantees on ℓ1 -relaxations to a much broader class of problems than the case of completely independent or unitary designs. Keywords: Lasso, basis pursuit, random matrix theory, Gaussian comparison inequality, concentration of measure</p><p>4 0.50541109 <a title="45-lsi-4" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>Author: Ming Yuan</p><p>Abstract: This paper considers the problem of estimating a high dimensional inverse covariance matrix that can be well approximated by “sparse” matrices. Taking advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix, we propose an estimating procedure that can effectively exploit such “sparsity”. The proposed method can be computed using linear programming and therefore has the potential to be used in very high dimensional problems. Oracle inequalities are established for the estimation error in terms of several operator norms, showing that the method is adaptive to different types of sparsity of the problem. Keywords: covariance selection, Dantzig selector, Gaussian graphical model, inverse covariance matrix, Lasso, linear programming, oracle inequality, sparsity</p><p>5 0.44205132 <a title="45-lsi-5" href="./jmlr-2010-Rate_Minimaxity_of_the_Lasso_and_Dantzig_Selector_for_thelqLoss_inlrBalls.html">96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</a></p>
<p>Author: Fei Ye, Cun-Hui Zhang</p><p>Abstract: We consider the estimation of regression coefﬁcients in a high-dimensional linear model. For regression coefﬁcients in ℓr balls, we provide lower bounds for the minimax ℓq risk and minimax quantiles of the ℓq loss for all design matrices. Under an ℓ0 sparsity condition on a target coefﬁcient vector, we sharpen and unify existing oracle inequalities for the Lasso and Dantzig selector. We derive oracle inequalities for target coefﬁcient vectors with many small elements and smaller threshold levels than the universal threshold. These oracle inequalities provide sufﬁcient conditions on the design matrix for the rate minimaxity of the Lasso and Dantzig selector for the ℓq risk and loss in ℓr balls, 0 ≤ r ≤ 1 ≤ q ≤ ∞. By allowing q = ∞, our risk bounds imply the variable selection consistency of threshold Lasso and Dantzig selectors. Keywords: variable selection, estimation, oracle inequality, minimax, linear regression, penalized least squares, linear programming</p><p>6 0.43466547 <a title="45-lsi-6" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>7 0.40412283 <a title="45-lsi-7" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>8 0.30478701 <a title="45-lsi-8" href="./jmlr-2010-A_Fast_Hybrid_Algorithm_for_Large-Scalel1-Regularized_Logistic_Regression.html">3 jmlr-2010-A Fast Hybrid Algorithm for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>9 0.29067937 <a title="45-lsi-9" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>10 0.28842518 <a title="45-lsi-10" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>11 0.26233038 <a title="45-lsi-11" href="./jmlr-2010-Learning_Translation_Invariant_Kernels_for_Classification.html">65 jmlr-2010-Learning Translation Invariant Kernels for Classification</a></p>
<p>12 0.25815511 <a title="45-lsi-12" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>13 0.23209657 <a title="45-lsi-13" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>14 0.22604498 <a title="45-lsi-14" href="./jmlr-2010-Posterior_Regularization_for_Structured_Latent_Variable_Models.html">91 jmlr-2010-Posterior Regularization for Structured Latent Variable Models</a></p>
<p>15 0.2254286 <a title="45-lsi-15" href="./jmlr-2010-Spectral_Regularization_Algorithms_for_Learning_Large_Incomplete_Matrices.html">105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</a></p>
<p>16 0.21564856 <a title="45-lsi-16" href="./jmlr-2010-Approximate_Riemannian_Conjugate_Gradient_Learning_for_Fixed-Form_Variational_Bayes.html">14 jmlr-2010-Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes</a></p>
<p>17 0.21380918 <a title="45-lsi-17" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>18 0.21280421 <a title="45-lsi-18" href="./jmlr-2010-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">79 jmlr-2010-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>19 0.20992297 <a title="45-lsi-19" href="./jmlr-2010-Bayesian_Learning_in_Sparse_Graphical_Factor_Models_via_Variational_Mean-Field_Annealing.html">17 jmlr-2010-Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing</a></p>
<p>20 0.20626608 <a title="45-lsi-20" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.043), (8, 0.475), (15, 0.012), (21, 0.021), (32, 0.035), (36, 0.032), (37, 0.1), (75, 0.144), (85, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86591691 <a title="45-lda-1" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>Author: Dapo Omidiran, Martin J. Wainwright</p><p>Abstract: We consider the problem of high-dimensional variable selection: given n noisy observations of a k-sparse vector β∗ ∈ R p , estimate the subset of non-zero entries of β∗ . A signiﬁcant body of work has studied behavior of ℓ1 -relaxations when applied to random measurement matrices that are dense (e.g., Gaussian, Bernoulli). In this paper, we analyze sparsiﬁed measurement ensembles, and consider the trade-off between measurement sparsity, as measured by the fraction γ of nonzero entries, and the statistical efﬁciency, as measured by the minimal number of observations n required for correct variable selection with probability converging to one. Our main result is to prove that it is possible to let the fraction on non-zero entries γ → 0 at some rate, yielding measurement matrices with a vanishing fraction of non-zeros per row, while retaining the same statistical efﬁciency as dense ensembles. A variety of simulation results conﬁrm the sharpness of our theoretical predictions. Keywords: variable selection, sparse random projections, high-dimensional statistics, Lasso, consistency, ℓ1 -regularization</p><p>2 0.69895375 <a title="45-lda-2" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>Author: Franz Pernkopf, Jeff A. Bilmes</p><p>Abstract: We introduce a simple order-based greedy heuristic for learning discriminative structure within generative Bayesian network classiﬁers. We propose two methods for establishing an order of N features. They are based on the conditional mutual information and classiﬁcation rate (i.e., risk), respectively. Given an ordering, we can ﬁnd a discriminative structure with O N k+1 score evaluations (where constant k is the tree-width of the sub-graph over the attributes). We present results on 25 data sets from the UCI repository, for phonetic classiﬁcation using the TIMIT database, for a visual surface inspection task, and for two handwritten digit recognition tasks. We provide classiﬁcation performance for both discriminative and generative parameter learning on both discriminatively and generatively structured networks. The discriminative structure found by our new procedures signiﬁcantly outperforms generatively produced structures, and achieves a classiﬁcation accuracy on par with the best discriminative (greedy) Bayesian network learning approach, but does so with a factor of ∼10-40 speedup. We also show that the advantages of generative discriminatively structured Bayesian network classiﬁers still hold in the case of missing features, a case where generative classiﬁers have an advantage over discriminative classiﬁers. Keywords: Bayesian networks, classiﬁcation, discriminative learning, structure learning, graphical model, missing feature</p><p>3 0.5897882 <a title="45-lda-3" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>Author: Jean-Yves Audibert, Sébastien Bubeck</p><p>Abstract: This work deals with four classical prediction settings, namely full information, bandit, label efﬁcient and bandit label efﬁcient as well as four different notions of regret: pseudo-regret, expected regret, high probability regret and tracking the best expert regret. We introduce a new forecaster, INF (Implicitly Normalized Forecaster) based on an arbitrary function ψ for which we propose a uniﬁed γ analysis of its pseudo-regret in the four games we consider. In particular, for ψ(x) = exp(ηx) + K , INF reduces to the classical exponentially weighted average forecaster and our analysis of the pseudo-regret recovers known results while for the expected regret we slightly tighten the bounds. γ η q On the other hand with ψ(x) = −x + K , which deﬁnes a new forecaster, we are able to remove the extraneous logarithmic factor in the pseudo-regret bounds for bandits games, and thus ﬁll in a long open gap in the characterization of the minimax rate for the pseudo-regret in the bandit game. We also provide high probability bounds depending on the cumulative reward of the optimal action. Finally, we consider the stochastic bandit game, and prove that an appropriate modiﬁcation of the upper conﬁdence bound policy UCB1 (Auer et al., 2002a) achieves the distribution-free optimal rate while still having a distribution-dependent rate logarithmic in the number of plays. Keywords: Bandits (adversarial and stochastic), regret bound, minimax rate, label efﬁcient, upper conﬁdence bound (UCB) policy, online learning, prediction with limited feedback.</p><p>4 0.47104409 <a title="45-lda-4" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>Author: Ming Yuan</p><p>Abstract: This paper considers the problem of estimating a high dimensional inverse covariance matrix that can be well approximated by “sparse” matrices. Taking advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix, we propose an estimating procedure that can effectively exploit such “sparsity”. The proposed method can be computed using linear programming and therefore has the potential to be used in very high dimensional problems. Oracle inequalities are established for the estimation error in terms of several operator norms, showing that the method is adaptive to different types of sparsity of the problem. Keywords: covariance selection, Dantzig selector, Gaussian graphical model, inverse covariance matrix, Lasso, linear programming, oracle inequality, sparsity</p><p>5 0.46735722 <a title="45-lda-5" href="./jmlr-2010-Rate_Minimaxity_of_the_Lasso_and_Dantzig_Selector_for_thelqLoss_inlrBalls.html">96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</a></p>
<p>Author: Fei Ye, Cun-Hui Zhang</p><p>Abstract: We consider the estimation of regression coefﬁcients in a high-dimensional linear model. For regression coefﬁcients in ℓr balls, we provide lower bounds for the minimax ℓq risk and minimax quantiles of the ℓq loss for all design matrices. Under an ℓ0 sparsity condition on a target coefﬁcient vector, we sharpen and unify existing oracle inequalities for the Lasso and Dantzig selector. We derive oracle inequalities for target coefﬁcient vectors with many small elements and smaller threshold levels than the universal threshold. These oracle inequalities provide sufﬁcient conditions on the design matrix for the rate minimaxity of the Lasso and Dantzig selector for the ℓq risk and loss in ℓr balls, 0 ≤ r ≤ 1 ≤ q ≤ ∞. By allowing q = ∞, our risk bounds imply the variable selection consistency of threshold Lasso and Dantzig selectors. Keywords: variable selection, estimation, oracle inequality, minimax, linear regression, penalized least squares, linear programming</p><p>6 0.46036586 <a title="45-lda-6" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>7 0.42553312 <a title="45-lda-7" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>8 0.42266673 <a title="45-lda-8" href="./jmlr-2010-Message-passing_for_Graph-structured_Linear_Programs%3A_Proximal_Methods_and_Rounding_Schemes.html">76 jmlr-2010-Message-passing for Graph-structured Linear Programs: Proximal Methods and Rounding Schemes</a></p>
<p>9 0.41487202 <a title="45-lda-9" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>10 0.40766314 <a title="45-lda-10" href="./jmlr-2010-Chromatic_PAC-Bayes_Bounds_for_Non-IID_Data%3A_Applications_to_Ranking_and_Stationary_%CE%B2-Mixing_Processes.html">20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</a></p>
<p>11 0.40731972 <a title="45-lda-11" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>12 0.40530014 <a title="45-lda-12" href="./jmlr-2010-A_Generalized_Path_Integral_Control_Approach_to_Reinforcement_Learning.html">4 jmlr-2010-A Generalized Path Integral Control Approach to Reinforcement Learning</a></p>
<p>13 0.40233028 <a title="45-lda-13" href="./jmlr-2010-Stability_Bounds_for_Stationary_%CF%86-mixing_and_%CE%B2-mixing_Processes.html">106 jmlr-2010-Stability Bounds for Stationary φ-mixing and β-mixing Processes</a></p>
<p>14 0.39713442 <a title="45-lda-14" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>15 0.3968865 <a title="45-lda-15" href="./jmlr-2010-Iterative_Scaling_and_Coordinate_Descent_Methods_for_Maximum_Entropy_Models.html">57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</a></p>
<p>16 0.39565146 <a title="45-lda-16" href="./jmlr-2010-Sparse_Spectrum_Gaussian_Process_Regression.html">104 jmlr-2010-Sparse Spectrum Gaussian Process Regression</a></p>
<p>17 0.3919149 <a title="45-lda-17" href="./jmlr-2010-Spectral_Regularization_Algorithms_for_Learning_Large_Incomplete_Matrices.html">105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</a></p>
<p>18 0.39181668 <a title="45-lda-18" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>19 0.38994589 <a title="45-lda-19" href="./jmlr-2010-Asymptotic_Equivalence_of_Bayes_Cross_Validation_and_Widely_Applicable_Information_Criterion_in_Singular_Learning_Theory.html">16 jmlr-2010-Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory</a></p>
<p>20 0.38957921 <a title="45-lda-20" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
