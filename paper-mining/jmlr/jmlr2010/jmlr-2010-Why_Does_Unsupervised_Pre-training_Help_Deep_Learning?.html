<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>117 jmlr-2010-Why Does Unsupervised Pre-training Help Deep Learning?</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-117" href="#">jmlr2010-117</a> knowledge-graph by maker-knowledge-mining</p><h1>117 jmlr-2010-Why Does Unsupervised Pre-training Help Deep Learning?</h1>
<br/><p>Source: <a title="jmlr-2010-117-pdf" href="http://jmlr.org/papers/volume11/erhan10a/erhan10a.pdf">pdf</a></p><p>Author: Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, Samy Bengio</p><p>Abstract: Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difﬁcult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the inﬂuence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments conﬁrm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pretraining guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training. Keywords: deep architectures, unsupervised pre-training, deep belief networks, stacked denoising auto-encoders, non-convex optimization</p><p>Reference: <a title="jmlr-2010-117-reference" href="../jmlr2010_reference/jmlr-2010-Why_Does_Unsupervised_Pre-training_Help_Deep_Learning%3F_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lay', 0.493), ('unsuperv', 0.349), ('deep', 0.311), ('engio', 0.243), ('eep', 0.158), ('ourvil', 0.158), ('rhan', 0.158), ('elp', 0.149), ('architect', 0.136), ('train', 0.136), ('anzagol', 0.135), ('basin', 0.132), ('nsuperv', 0.128), ('infinitemn', 0.114), ('deno', 0.11), ('hy', 0.106), ('bengio', 0.103), ('oe', 0.099), ('hid', 0.095), ('rbm', 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="117-tfidf-1" href="./jmlr-2010-Why_Does_Unsupervised_Pre-training_Help_Deep_Learning%3F.html">117 jmlr-2010-Why Does Unsupervised Pre-training Help Deep Learning?</a></p>
<p>2 0.36146757 <a title="117-tfidf-2" href="./jmlr-2010-Stacked_Denoising_Autoencoders%3A_Learning_Useful_Representations_in_a_Deep_Network_with_a_Local_Denoising_Criterion.html">107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</a></p>
<p>3 0.093123987 <a title="117-tfidf-3" href="./jmlr-2010-PyBrain.html">93 jmlr-2010-PyBrain</a></p>
<p>4 0.083015032 <a title="117-tfidf-4" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>5 0.072637342 <a title="117-tfidf-5" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>6 0.070834823 <a title="117-tfidf-6" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>7 0.058593523 <a title="117-tfidf-7" href="./jmlr-2010-Importance_Sampling_for_Continuous_Time_Bayesian_Networks.html">51 jmlr-2010-Importance Sampling for Continuous Time Bayesian Networks</a></p>
<p>8 0.054730743 <a title="117-tfidf-8" href="./jmlr-2010-Posterior_Regularization_for_Structured_Latent_Variable_Models.html">91 jmlr-2010-Posterior Regularization for Structured Latent Variable Models</a></p>
<p>9 0.052290697 <a title="117-tfidf-9" href="./jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization.html">54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</a></p>
<p>10 0.052051876 <a title="117-tfidf-10" href="./jmlr-2010-Covariance_in_Unsupervised_Learning_of_Probabilistic_Grammars.html">29 jmlr-2010-Covariance in Unsupervised Learning of Probabilistic Grammars</a></p>
<p>11 0.050089516 <a title="117-tfidf-11" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<p>12 0.047880888 <a title="117-tfidf-12" href="./jmlr-2010-Approximate_Tree_Kernels.html">15 jmlr-2010-Approximate Tree Kernels</a></p>
<p>13 0.047681734 <a title="117-tfidf-13" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>14 0.04730757 <a title="117-tfidf-14" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>15 0.047138069 <a title="117-tfidf-15" href="./jmlr-2010-Evolving_Static_Representations_for_Task_Transfer.html">37 jmlr-2010-Evolving Static Representations for Task Transfer</a></p>
<p>16 0.045986462 <a title="117-tfidf-16" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>17 0.04347606 <a title="117-tfidf-17" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>18 0.043323275 <a title="117-tfidf-18" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>19 0.04292424 <a title="117-tfidf-19" href="./jmlr-2010-Inducing_Tree-Substitution_Grammars.html">53 jmlr-2010-Inducing Tree-Substitution Grammars</a></p>
<p>20 0.042892139 <a title="117-tfidf-20" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.206), (1, -0.109), (2, 0.016), (3, -0.021), (4, 0.071), (5, 0.201), (6, 0.123), (7, 0.155), (8, -0.164), (9, 0.434), (10, -0.184), (11, -0.21), (12, -0.278), (13, 0.009), (14, 0.13), (15, -0.068), (16, 0.089), (17, 0.117), (18, 0.162), (19, -0.049), (20, -0.011), (21, 0.114), (22, 0.069), (23, 0.017), (24, -0.012), (25, -0.004), (26, -0.104), (27, -0.032), (28, -0.048), (29, 0.038), (30, 0.015), (31, 0.058), (32, -0.002), (33, 0.069), (34, -0.018), (35, 0.042), (36, 0.025), (37, 0.035), (38, 0.03), (39, -0.024), (40, -0.024), (41, 0.04), (42, -0.05), (43, 0.001), (44, 0.025), (45, 0.027), (46, 0.053), (47, 0.013), (48, -0.011), (49, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92598522 <a title="117-lsi-1" href="./jmlr-2010-Why_Does_Unsupervised_Pre-training_Help_Deep_Learning%3F.html">117 jmlr-2010-Why Does Unsupervised Pre-training Help Deep Learning?</a></p>
<p>2 0.92323893 <a title="117-lsi-2" href="./jmlr-2010-Stacked_Denoising_Autoencoders%3A_Learning_Useful_Representations_in_a_Deep_Network_with_a_Local_Denoising_Criterion.html">107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</a></p>
<p>3 0.48489472 <a title="117-lsi-3" href="./jmlr-2010-PyBrain.html">93 jmlr-2010-PyBrain</a></p>
<p>4 0.36223307 <a title="117-lsi-4" href="./jmlr-2010-Evolving_Static_Representations_for_Task_Transfer.html">37 jmlr-2010-Evolving Static Representations for Task Transfer</a></p>
<p>5 0.33932543 <a title="117-lsi-5" href="./jmlr-2010-Image_Denoising_with_Kernels_Based_on_Natural_Image_Relations.html">50 jmlr-2010-Image Denoising with Kernels Based on Natural Image Relations</a></p>
<p>6 0.26579332 <a title="117-lsi-6" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>7 0.26516056 <a title="117-lsi-7" href="./jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization.html">54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</a></p>
<p>8 0.25219962 <a title="117-lsi-8" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>9 0.245277 <a title="117-lsi-9" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>10 0.24334975 <a title="117-lsi-10" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>11 0.23689356 <a title="117-lsi-11" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<p>12 0.23631899 <a title="117-lsi-12" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>13 0.23101619 <a title="117-lsi-13" href="./jmlr-2010-Expectation_Truncation_and_the_Benefits_of_Preselection_In_Training_Generative_Models.html">38 jmlr-2010-Expectation Truncation and the Benefits of Preselection In Training Generative Models</a></p>
<p>14 0.22160189 <a title="117-lsi-14" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>15 0.21999279 <a title="117-lsi-15" href="./jmlr-2010-Posterior_Regularization_for_Structured_Latent_Variable_Models.html">91 jmlr-2010-Posterior Regularization for Structured Latent Variable Models</a></p>
<p>16 0.20790274 <a title="117-lsi-16" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>17 0.20103718 <a title="117-lsi-17" href="./jmlr-2010-Consensus-Based_Distributed_Support_Vector_Machines.html">26 jmlr-2010-Consensus-Based Distributed Support Vector Machines</a></p>
<p>18 0.19872272 <a title="117-lsi-18" href="./jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">48 jmlr-2010-How to Explain Individual Classification Decisions</a></p>
<p>19 0.19850697 <a title="117-lsi-19" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>20 0.19748737 <a title="117-lsi-20" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.16), (13, 0.056), (22, 0.039), (28, 0.017), (46, 0.019), (49, 0.013), (57, 0.015), (62, 0.014), (65, 0.077), (68, 0.015), (71, 0.076), (87, 0.03), (94, 0.382)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78073055 <a title="117-lda-1" href="./jmlr-2010-Iterative_Scaling_and_Coordinate_Descent_Methods_for_Maximum_Entropy_Models.html">57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</a></p>
<p>same-paper 2 0.65896118 <a title="117-lda-2" href="./jmlr-2010-Why_Does_Unsupervised_Pre-training_Help_Deep_Learning%3F.html">117 jmlr-2010-Why Does Unsupervised Pre-training Help Deep Learning?</a></p>
<p>3 0.54324907 <a title="117-lda-3" href="./jmlr-2010-Stacked_Denoising_Autoencoders%3A_Learning_Useful_Representations_in_a_Deep_Network_with_a_Local_Denoising_Criterion.html">107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</a></p>
<p>4 0.46383357 <a title="117-lda-4" href="./jmlr-2010-Mean_Field_Variational_Approximation_for_Continuous-Time_Bayesian_Networks.html">75 jmlr-2010-Mean Field Variational Approximation for Continuous-Time Bayesian Networks</a></p>
<p>5 0.46017063 <a title="117-lda-5" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>6 0.45923799 <a title="117-lda-6" href="./jmlr-2010-An_Exponential_Model_for_Infinite_Rankings.html">10 jmlr-2010-An Exponential Model for Infinite Rankings</a></p>
<p>7 0.45921218 <a title="117-lda-7" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>8 0.45858115 <a title="117-lda-8" href="./jmlr-2010-On-Line_Sequential_Bin_Packing.html">80 jmlr-2010-On-Line Sequential Bin Packing</a></p>
<p>9 0.45854533 <a title="117-lda-9" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>10 0.45823425 <a title="117-lda-10" href="./jmlr-2010-Classification_Methods_with_Reject_Option_Based_on_Convex_Risk_Minimization.html">21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</a></p>
<p>11 0.45749265 <a title="117-lda-11" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>12 0.45732448 <a title="117-lda-12" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>13 0.45635197 <a title="117-lda-13" href="./jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization.html">54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</a></p>
<p>14 0.45578763 <a title="117-lda-14" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>15 0.45576453 <a title="117-lda-15" href="./jmlr-2010-On_the_Foundations_of_Noise-free_Selective_Classification.html">85 jmlr-2010-On the Foundations of Noise-free Selective Classification</a></p>
<p>16 0.45570701 <a title="117-lda-16" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>17 0.45524958 <a title="117-lda-17" href="./jmlr-2010-Bundle_Methods_for_Regularized_Risk_Minimization.html">18 jmlr-2010-Bundle Methods for Regularized Risk Minimization</a></p>
<p>18 0.45507997 <a title="117-lda-18" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>19 0.45475459 <a title="117-lda-19" href="./jmlr-2010-PyBrain.html">93 jmlr-2010-PyBrain</a></p>
<p>20 0.45432419 <a title="117-lda-20" href="./jmlr-2010-On_Finding_Predictors_for_Arbitrary_Families_of_Processes.html">81 jmlr-2010-On Finding Predictors for Arbitrary Families of Processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
