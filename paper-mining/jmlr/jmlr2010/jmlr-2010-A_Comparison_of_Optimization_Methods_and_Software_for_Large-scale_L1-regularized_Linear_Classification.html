<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 jmlr-2010-A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-1" href="#">jmlr2010-1</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 jmlr-2010-A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</h1>
<br/><p>Source: <a title="jmlr-2010-1-pdf" href="http://jmlr.org/papers/volume11/yuan10c/yuan10c.pdf">pdf</a></p><p>Author: Guo-Xun Yuan, Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Large-scale linear classiﬁcation is widely used in many areas. The L1-regularized form can be applied for feature selection; however, its non-differentiability causes more difﬁculties in training. Although various optimization methods have been proposed in recent years, these have not yet been compared suitably. In this paper, we ﬁrst broadly review existing methods. Then, we discuss state-of-the-art software packages in detail and propose two efﬁcient implementations. Extensive comparisons indicate that carefully implemented coordinate descent methods are very suitable for training large document data. Keywords: L1 regularization, linear classiﬁcation, optimization methods, logistic regression, support vector machines, document classiﬁcation</p><p>Reference: <a title="jmlr-2010-1-reference" href="../jmlr2010_reference/jmlr-2010-A_Comparison_of_Optimization_Methods_and_Software_for_Large-scale_L1-regularized_Linear_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Extensive comparisons indicate that carefully implemented coordinate descent methods are very suitable for training large document data. [sent-18, score-0.18]
</p><p>2 , l, xi ∈ Rn , yi ∈ {−1, +1}, training an L1-regularized linear classiﬁer involves the following unconstrained optimization problem: l  min w  f (w) ≡ w  1 +C ∑ ξ(w; xi , yi ),  (1)  i=1  where · 1 denotes the 1-norm and ξ(w; xi , yi ) is a non-negative (convex) loss function. [sent-24, score-0.219]
</p><p>3 Note that minimizing logistic loss is equivalent to maximizing the likelihood, whereas minimizing the regularized loss in (1) is equivalent to maximizing the posterior with independent Laplace priors on the parameters. [sent-38, score-0.162]
</p><p>4 We refer to (1) with logistic loss as L1-regularized logistic regression and (1) with L1/L2 loss as L1-regularized L1-/L2-loss support vector machines (SVMs). [sent-42, score-0.252]
</p><p>5 Many papers have proposed optimization methods for large-scale L1-regularized logistic regression (i. [sent-48, score-0.14]
</p><p>6 We exclude L1 loss from the discussion because most methods for logistic regression or L2-loss SVMs assume the differentiability of the loss functions. [sent-59, score-0.159]
</p><p>7 1 C YCLIC C OORDINATE D ESCENT M ETHODS A simple coordinate descent method cyclically chooses one variable at a time and solves the following one-variable sub-problem: min z  g j (z) ≡ f (w + ze j ) − f (w),  (11)  where e j is deﬁned in (10). [sent-136, score-0.19]
</p><p>8 Several works have applied coordinate descent methods to solve (1) with logistic loss. [sent-139, score-0.25]
</p><p>9 (2007) implemented a cyclic coordinate descent method called BBR to solve L1regularized logistic regression. [sent-148, score-0.274]
</p><p>10 Hereafter, we refer to this efﬁcient coordinate descent method as CDN (coordinate descent using one-dimensional Newton steps). [sent-159, score-0.187]
</p><p>11 One of their approaches is a cyclic coordinate descent method. [sent-161, score-0.149]
</p><p>12 If we randomly select the working variable, then the procedure becomes a stochastic coordinate descent method. [sent-164, score-0.173]
</p><p>13 Duchi and Singer (2009) proposed a similar coordinate descent method for the maximum entropy model, which is a generalization of logistic regression. [sent-166, score-0.218]
</p><p>14 Because of the use of gradient information, the number of iterations is fewer than those in cyclic coordinate descent methods. [sent-171, score-0.194]
</p><p>15 In their method, one variable is chosen at a time and one-dimensional Newton steps are applied; therefore, their method differs from the cyclic coordinate descent methods described in Section 2. [sent-174, score-0.149]
</p><p>16 (2008) applied TRON to solve L2-regularized logistic regression and showed that TRON outperforms LBFGS for document data. [sent-216, score-0.178]
</p><p>17 (2007) proposed an interior point method to solve L1-regularized logistic regression. [sent-220, score-0.157]
</p><p>18 If k is the iteration index, EG updates w by the following rule: wk+1 = j  wk exp −ηk ∇ j ∑l ξ(wk ; xi , yi ) i=1 j Zk  ,  where Zk is a normalization term for maintaining wk 1 = K, ∀k and ηk is a user-deﬁned learning rate. [sent-240, score-0.916]
</p><p>19 The update rule is wk+1 = arg min w  wk − ηk ∇  ∑i=1 ξ(wk ; xi , yi ) l  −w  w  1  ≤K . [sent-243, score-0.489]
</p><p>20 2 Kim and Kim (2004) discussed a coordinate descent method to solve (14). [sent-249, score-0.157]
</p><p>21 1 E XPECTATION M AXIMIZATION Many studies have considered Expectation Maximization (EM) frameworks to solve (1) with logistic loss (e. [sent-269, score-0.144]
</p><p>22 A constrained line search on the same sub-space is then conducted and the property wk+1 wk ≥ 0 is maintained. [sent-291, score-0.465]
</p><p>23 (2010) replaced the loss term with a second-order approximation at the beginning of each iteration and then applied a cyclic coordinate descent method to minimize the quadratic function. [sent-304, score-0.207]
</p><p>24 In contrast, methods such as coordinate descent or stochastic gradient descent methods are easy to implement. [sent-339, score-0.232]
</p><p>25 Wu and Lange (2008) considered a coordinate descent method, but used the gradient information for selecting the working variable at each iteration. [sent-376, score-0.218]
</p><p>26 The interior point method for logistic regression by solving (13) has been applied to the least-square regression problem (Kim et al. [sent-410, score-0.199]
</p><p>27 One is a coordinate descent method (CDN) and the other is a trust region Newton method (TRON). [sent-425, score-0.217]
</p><p>28 1 Cyclic Coordinate Descent Methods From the current solution wk , a coordinate descent method updates one variable at a time to generate wk, j ∈ Rn , j = 1, . [sent-435, score-0.551]
</p><p>29 , n + 1, such that wk,1 = wk , wk,n+1 = wk+1 , and wk, j = wk+1 , . [sent-438, score-0.426]
</p><p>30 , from wk to wk+1 ) as an outer iteration and the step of updating one element (i. [sent-467, score-0.446]
</p><p>31 (2007) propose a coordinate descent method BBR for (1) and (5) with logistic loss. [sent-476, score-0.218]
</p><p>32 If logistic loss (2) is used, BBR suggests setting U j as l  U j ≡ C ∑ xi2j F yi (wk, j )T xi , ∆ j |xi j | ,  (22)  i=1  where F(r, δ) =  0. [sent-489, score-0.156]
</p><p>33 For logistic loss, L′j (0) needed for calculating g′j (0) is l  L′j (0) = C ∑ yi xi j τ(yi (wk, j )T xi ) − 1 , i=1  3195  (24)  Y UAN , C HANG , H SIEH AND L IN  Algorithm 2 BBR: Approximately solving the sub-problem by a trust region method 1. [sent-499, score-0.265]
</p><p>34 3 S TOCHASTIC C OORDINATE D ESCENT M ETHOD (SCD) Shalev-Shwartz and Tewari (2009) propose a stochastic coordinate descent method (SCD) to solve the bound-constrained problem in (12). [sent-606, score-0.157]
</p><p>35 The one-variable sub-problem is n n 1 1 min z  g j (z) ≡ z + L j (z; wk,+ − wk,− ) − L j (0; wk,+ − wk,− ), 3198  C OMPARING M ETHODS AND S OFTWARE FOR L1- REGULARIZED L INEAR C LASSIFICATION  Algorithm 4 SCD for L1-regularized logistic regression 1. [sent-614, score-0.14]
</p><p>36 3199  Y UAN , C HANG , H SIEH AND L IN  Following the principle of decomposition methods, if wk is the current solution and J is the set of working variables, one should solve the following sub-problem: min L(wk + d) − L(wk ) + wk + d d  1−  wk  1  subject to d j = 0, ∀ j ∈ J. [sent-641, score-1.393]
</p><p>37 / Because it is still difﬁcult to solve this sub-problem, CGD-GS considers a quadratic approximation of the loss term: 1 min qk (d) ≡ ∇L(wk )T d + dT Hd + wk + d 1 − wk 1 d 2 (38) subject to d j = 0, ∀ j ∈ J, / where H is either ∇2 L(wk ) or its approximation. [sent-642, score-1.056]
</p><p>38 To ensure the convergence, CGD-GS conducts a backtrack line search to ﬁnd λ such that λd satisﬁes f (wk + λd) − f (wk ) ≤ σλ ∇L(wk )T d + γdT Hd + wk + d  1−  wk  1  ,  (39)  where 0 < σ < 1 and 0 ≤ γ < 1. [sent-643, score-0.908]
</p><p>39 The other condition for selecting J is qk (d(J)) ≤ v · qk (d(N)),  (41)  where v ∈ (0, 1) and qk (d) is deﬁned in (38). [sent-649, score-0.363]
</p><p>40 At the kth iteration of the trust region Newton method, we have an iterate wk , a size ∆k of the trust region, and a quadratic model 1 qk (d) ≡ dT ∇2 f¯(wk )d + ∇ f¯(wk )T d 2 to approximate the value f¯(wk + d) − f¯(wk ). [sent-691, score-0.749]
</p><p>41 Next, we ﬁnd a step dk by approximately solving the following trust region problem min qk (d) d  subject to  d ≤ ∆k , wk + d ∈ Ω. [sent-692, score-0.73]
</p><p>42 (44)  We then update wk and ∆k by checking the ratio ρk =  f¯(wk + dk ) − f¯(wk ) qk (dk ) 3201  (45)  Y UAN , C HANG , H SIEH AND L IN  Algorithm 6 A trust region method for L1-regularized logistic regression 1. [sent-693, score-0.814]
</p><p>43 • Update wk to wk+1 according to (46) and update ∆k to ∆k+1 . [sent-701, score-0.426]
</p><p>44 Find the Cauchy step dk,C and the Cauchy point wk,1 = wk + dk,C  and  dk,1 = dk,C . [sent-704, score-0.426]
</p><p>45 / • If Ft = 0, then stop and return dk = wk,t − wk . [sent-710, score-0.486]
</p><p>46 • Approximately solve min qk (dk,t + v) vFt  subject to  dk,t + v ≤ ∆k , vBt = 0,  by Conjugate Gradient (CG) methods. [sent-711, score-0.166]
</p><p>47 • If one of the following situations occurs: ∇qk (dk,t+1 )Ft ≤ ε ∇ f¯(wk )Ft , or CG abnormally stops (explained in text), then stop and return dk = wk,t+1 − wk . [sent-715, score-0.486]
</p><p>48 The direction dk is accepted if ρk is large enough: wk+1 =  wk + dk wk  if ρk > η0 , if ρk ≤ η0 ,  (46)  where η0 > 0 is a pre-speciﬁed value. [sent-717, score-0.993]
</p><p>49 With a proper line search to ensure the reduction of the quadratic model qk (d), not only do we effectively guess the set of bounded components at an optimum, but also the convergence is guaranteed. [sent-733, score-0.173]
</p><p>50 To be more precise, we ﬁnd a step size λ > 0 so that qk (dk,C ) ≤ qk (0) + σ∇qk (0)T dk,C  and  dk,C ≤ ∆k ,  (47)  where dk,C = P[wk − λ∇ f¯(wk )] − wk  (48)  is called the Cauchy step in bound-constrained optimization and σ ∈ (0, 1/2) is a constant. [sent-734, score-0.675]
</p><p>51 The projection operator P[·] maps wk − λ∇ f¯(wk ) back to the feasible region Ω: P[w j ] = min (u j , max(w j , l j )) ,  (49)  so some components become bounded. [sent-735, score-0.479]
</p><p>52 The point wk,C ≡ wk + dk,C is referred to as the Cauchy point. [sent-740, score-0.426]
</p><p>53 We obtain the free/bounded sets at the Cauchy point F ≡ F(wk,C ) = { j | l j < wk,C < u j } and j  B ≡ B(wk,C ) = { j | j ∈ F}, /  (50)  and ﬁnd a Newton direction on the space F by solving min qk (dk,C + v) vF  subject to  dk,C + v ≤ ∆k , vB = 0. [sent-747, score-0.173]
</p><p>54 If m inner iterations are taken, then the direction dk for the kth trust region iteration in Algorithm 6 is dk = wk,m+1 − wk . [sent-757, score-0.698]
</p><p>55 With vBt = 0, 1 qk (dk,t + v) = vTt ∇2 f¯(wk )Ft ,Ft vFt + ∇qk (dk,t )Tt vFt + qk (dk,t ), F 2 F so minimizing qk (dk,t + v) is equivalent to solving the following linear system ∇2 f¯(wk )Ft ,Ft vFt = −∇qk (dk,t )Ft . [sent-763, score-0.363]
</p><p>56 Once a direction vk,t is identiﬁed, we conduct a projected line search to ensure the feasibility and the sufﬁcient decrease of qk (d). [sent-769, score-0.207]
</p><p>57 , by a backtrack line search) such that wk,t+1 = P[wk,t + λvk,t ], dk,t+1 = wk,t+1 − wk , and  (55)  qk (dk,t+1 ) ≤ qk (dk,t ) + σ∇qk (dk,t )Tt (dk,t+1 − dk,t )Ft , F where P[·] is deﬁned in (49) and σ is the same as that in (47). [sent-773, score-0.689]
</p><p>58 For logistic regression, XT D X −X :,F , ∇2 f¯(w)Ft ,Ft = C (56) t −X T F ,: t  where D is an l × l diagonal matrix with Dii = τ yi (w+ − w− )T xi  1 − τ(yi (w+ − w− )T xi ) . [sent-777, score-0.155]
</p><p>59 (2008) for L2-regularized problems, Hessian-vector products are only needed in CG, but here they are also used in the projected line search for calculating qk (dk,t+1 ) − qk (dk,t ); see (52) and (55). [sent-782, score-0.301]
</p><p>60 (2007) proposed an interior point method to solve (13) with logistic loss. [sent-796, score-0.157]
</p><p>61 The following linear system is solved:   ∆b 2 k k k  (59) ∇ φtk (b , w , u ) ∆w = −∇φtk (bk , wk , uk ). [sent-806, score-0.426]
</p><p>62 In addition to the sequence of iterations {wk }, for faster convergence, Nesterov’s method uses another sequence of searching points {sk }, where sk = wk + βk (wk − wk−1 ), for some positive parameter βk . [sent-836, score-0.461]
</p><p>63 From sk , we obtain wk+1 by taking the negative gradient direction: ¯ wk+1 = sk − λk ∇L(sk ), where l  ¯ L(w) ≡ ∑ ξi (w; xi , yi ) i=1  ¯ is the objective function of (14) and λk is the step size. [sent-837, score-0.159]
</p><p>64 , n}, j where if wk = 0, j otherwise,  (64)  if w j > 0 or (w j = 0 and L′j (w) + 1 < 0), if w j < 0 or (w j = 0 and L′j (w) − 1 > 0), otherwise  (65)  sgn(wk ) j ¯ sgn(−∇ j f (wk ))  sk ≡ j and   L′j (w) + 1  ¯ ∇ j f (w) ≡ L′j (w) − 1   0  is deﬁned as the pseudo gradient of f (w). [sent-876, score-0.506]
</p><p>65 In (64), if wk = 0, we consider the space where w j j can be moved by taking the negative gradient direction. [sent-877, score-0.471]
</p><p>66 OWL-QN then approximately minimizes a quadratic approximation of (1) in the search space Ωk : 1 ¯ f (wk ) + ∇ f (wk )T d + dT Hk d d 2 subject to wk + d ∈ Ωk , min  (66)  where Hk approximates the Hessian of f (wk ) by the ﬁrst-order information gathered from previous iterations. [sent-878, score-0.487]
</p><p>67 Note that wk+1 must be in Ωk , so following (64), wk+1 = j  wk + λd¯k j j 0  if sgn(wk + λd¯k ) = sk , j j j otherwise. [sent-881, score-0.461]
</p><p>68 Because of the twice differentiability of the logistic loss function, the gradient of L(w) is shown in (42) and the Hessian is ∇2 L(w) =CX T DX,  (69)  where D ∈ Rl×l is a diagonal matrix with Dii = τ(yi wT xi ) 1 − τ(yi wT xi )  (70)  and τ(·) is deﬁned in (9). [sent-902, score-0.193]
</p><p>69 Given the current solution wk , GLMNET considers a quadratic approximation of L(w). [sent-904, score-0.445]
</p><p>70 By the second-order Taylor expansion, f (wk + d) − f (wk ) =  wk + d  k 1 + L(w + d)  −  wk  k 1 + L(w )  1 ≈∇L(wk )T d + dT ∇2 L(wk )d + wk + d 2  1−  wk 1 . [sent-905, score-1.704]
</p><p>71 Then, GLMNET solves the Newton-like system min d  1 qk (d) ≡ ∇L(wk )T d + dT ∇2 L(wk )d + wk + d 2  1−  wk  1  (71)  by a cyclic coordinate descent method. [sent-906, score-1.135]
</p><p>72 In GLMNET implementation, instead of ﬁnding z, a different but equivalent update is used to get new wk + d j . [sent-910, score-0.426]
</p><p>73 • While dk is not optimal for minimizing qk (d) – For j = 1, . [sent-918, score-0.175]
</p><p>74 , n ∗ Solve the following one-variable problem by (27): z = arg min ¯ z  qk (dk + ze j ) − qk (dk ). [sent-921, score-0.277]
</p><p>75 • w Because calculating the matrix D involves many exponential operations, GLMNET also considers using an approximation of ∇2 L(w) and minimizes 1 qk (d) ≡ ∇L(wk )T d + dT Hd + wk + d 2  wk 1 ,  1−  where H ≡ 0. [sent-924, score-0.967]
</p><p>76 Using a sparse representation of w and maintaining an index set Ω to indicate the non-zero elements of d, GLMNET solves a smaller problem by a coordinate descent method: 1 min ∇L(wk )T d + dT Hd + wk + d 1 − wk 1 . [sent-932, score-0.996]
</p><p>77 For minimizing the quadratic approximation qk (d), GLMNET measures the relative step change in the successive coordinate descent iterations. [sent-937, score-0.278]
</p><p>78 Deciding when to stop minimizing qk (d) is an issue because a strict stopping condition may already cause long running time for q1 (d). [sent-938, score-0.149]
</p><p>79 , the ﬁrst-order Taylor expansion) of L(w) at w = wk : L(w) ≥∇L(wk )T (w − wk ) + L(wk ) =aT w + bk , ∀w, k  (72)  where ak ≡ ∇L(wk ) and  bk ≡ L(wk ) − aT wk . [sent-959, score-1.278]
</p><p>80 Like the situation for logistic regression, the major cost for ﬁnding the Newton direction d and for the line search procedure is to calculate wT xi , ∀i ∈ I. [sent-994, score-0.171]
</p><p>81 The quadratic model qk (d) requires the gradient and the Hessian of f¯(w). [sent-1003, score-0.179]
</p><p>82 • BBR: the cyclic coordinate descent method for logistic regression is described in Section 4. [sent-1106, score-0.27]
</p><p>83 • CDN: the cyclic coordinate descent methods for logistic regression and L2-loss SVMs are respectively described in Sections 4. [sent-1113, score-0.27]
</p><p>84 • SCD: the stochastic coordinate descent method for logistic regression is described in Section 4. [sent-1132, score-0.246]
</p><p>85 3214  C OMPARING M ETHODS AND S OFTWARE FOR L1- REGULARIZED L INEAR C LASSIFICATION  • CGD-GS: the block coordinate gradient descent method for logistic regression is described in Section 4. [sent-1138, score-0.291]
</p><p>86 TRON: the trust region Newton methods for logistic regression and L2-loss SVMs are respectively described in Sections 5. [sent-1154, score-0.213]
</p><p>87 IPM: the interior point method for logistic regression is described in Section 5. [sent-1164, score-0.153]
</p><p>88 The coordinate descent method for minimizing the quadratic approximation qk (d) stops according to a tolerance the same as the overall stopping tolerance. [sent-1203, score-0.275]
</p><p>89 In the rest of this section, we compare the training speed of solvers for logistic regression and L2-loss SVMs by using the best parameter C of each data set. [sent-1244, score-0.17]
</p><p>90 Following the comparison for methods solving (1), we begin with checking the running time to reduce the relative error of the function value and the scaled norm of the minimum-norm sub-gradient; see (82) and (83), respectively. [sent-1307, score-0.151]
</p><p>91 Following the experiment for logistic regression, we plot the relative difference to the optimal function value in Figure 9 and the scaled norm of the minimum-norm sub-gradient in Figure 10. [sent-1517, score-0.226]
</p><p>92 GLMNET iteratively considers quadratic approximations and applies coordinate descent methods at each iteration. [sent-1561, score-0.144]
</p><p>93 Our extensive comparison shows that carefully implemented coordinate descent methods are effective for L1-regularized classiﬁcation with large-scale document data. [sent-1572, score-0.15]
</p><p>94 If S is not bounded, then there is a sequence {wk } ⊂ S such that wk 1 → ∞. [sent-1579, score-0.426]
</p><p>95 Because we assume that ξ(w; xi , yi ) ≥ 0, f (wk ) ≥ wk 1 . [sent-1580, score-0.47]
</p><p>96 Proof of Theorem 1  From the assumption that wk → w∗ and the way that w is cyclically updated, we have wk, j → w∗ as well. [sent-1590, score-0.444]
</p><p>97 If one variable is updated at a time, their condition is reduced to that between wk and wk+1 , one must go through n sub-problems covering all w1 , . [sent-1634, score-0.444]
</p><p>98 That is, there exist µ ∈ (0, 1) and a positive integer K such that ¯ ¯ wk+1 − w ≤ (1 − µ) wk − w 2 , ∀k > K. [sent-1665, score-0.426]
</p><p>99 A coordinate gradient descent method for nonsmooth separable minimization. [sent-1966, score-0.17]
</p><p>100 A coordinate gradient descent method for l1-regularized convex minimization. [sent-1986, score-0.17]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cdn', 0.51), ('wk', 0.426), ('tron', 0.256), ('glmnet', 0.224), ('bbr', 0.218), ('scd', 0.197), ('bmrm', 0.186), ('sec', 0.173), ('lassplore', 0.166), ('ipm', 0.162), ('yun', 0.13), ('owl', 0.116), ('qk', 0.115), ('oftware', 0.112), ('omparing', 0.112), ('cgd', 0.107), ('newton', 0.101), ('scaled', 0.096), ('logistic', 0.093), ('sieh', 0.09), ('uan', 0.083), ('inear', 0.07), ('ethods', 0.067), ('coordinate', 0.063), ('qn', 0.063), ('descent', 0.062), ('gs', 0.061), ('dk', 0.06), ('tseng', 0.058), ('trust', 0.058), ('hang', 0.058), ('ft', 0.056), ('mor', 0.054), ('lassification', 0.054), ('working', 0.048), ('gradient', 0.045), ('toh', 0.041), ('nonzeros', 0.036), ('ns', 0.036), ('sk', 0.035), ('quasi', 0.034), ('region', 0.034), ('interior', 0.032), ('solve', 0.032), ('cauchy', 0.032), ('projected', 0.032), ('regularized', 0.031), ('vft', 0.031), ('training', 0.03), ('koh', 0.029), ('regression', 0.028), ('ze', 0.028), ('hsieh', 0.027), ('wt', 0.026), ('yi', 0.026), ('chang', 0.026), ('document', 0.025), ('cyclic', 0.024), ('lin', 0.024), ('log', 0.024), ('shrinking', 0.024), ('search', 0.023), ('svms', 0.023), ('mangasarian', 0.021), ('bias', 0.021), ('direction', 0.021), ('hessian', 0.02), ('iteration', 0.02), ('loss', 0.019), ('kim', 0.019), ('cg', 0.019), ('kth', 0.019), ('sgn', 0.019), ('solvers', 0.019), ('bundle', 0.019), ('relative', 0.019), ('optimization', 0.019), ('quadratic', 0.019), ('min', 0.019), ('condition', 0.018), ('cyclically', 0.018), ('xi', 0.018), ('aw', 0.018), ('testing', 0.018), ('norm', 0.018), ('solving', 0.018), ('differentiable', 0.018), ('liu', 0.017), ('backtrack', 0.017), ('krishnapuram', 0.017), ('osborne', 0.017), ('wj', 0.017), ('bi', 0.017), ('stopping', 0.016), ('decomposition', 0.016), ('bz', 0.016), ('escent', 0.016), ('line', 0.016), ('mario', 0.015), ('olvi', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="1-tfidf-1" href="./jmlr-2010-A_Comparison_of_Optimization_Methods_and_Software_for_Large-scale_L1-regularized_Linear_Classification.html">1 jmlr-2010-A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</a></p>
<p>Author: Guo-Xun Yuan, Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Large-scale linear classiﬁcation is widely used in many areas. The L1-regularized form can be applied for feature selection; however, its non-differentiability causes more difﬁculties in training. Although various optimization methods have been proposed in recent years, these have not yet been compared suitably. In this paper, we ﬁrst broadly review existing methods. Then, we discuss state-of-the-art software packages in detail and propose two efﬁcient implementations. Extensive comparisons indicate that carefully implemented coordinate descent methods are very suitable for training large document data. Keywords: L1 regularization, linear classiﬁcation, optimization methods, logistic regression, support vector machines, document classiﬁcation</p><p>2 0.1352316 <a title="1-tfidf-2" href="./jmlr-2010-Iterative_Scaling_and_Coordinate_Descent_Methods_for_Maximum_Entropy_Models.html">57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</a></p>
<p>Author: Fang-Lan Huang, Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin</p><p>Abstract: Maximum entropy (Maxent) is useful in natural language processing and many other areas. Iterative scaling (IS) methods are one of the most popular approaches to solve Maxent. With many variants of IS methods, it is difﬁcult to understand them and see the differences. In this paper, we create a general and uniﬁed framework for iterative scaling methods. This framework also connects iterative scaling and coordinate descent methods. We prove general convergence results for IS methods and analyze their computational complexity. Based on the proposed framework, we extend a coordinate descent method for linear SVM to Maxent. Results show that it is faster than existing iterative scaling methods. Keywords: maximum entropy, iterative scaling, coordinate descent, natural language processing, optimization</p><p>3 0.11094449 <a title="1-tfidf-3" href="./jmlr-2010-Bundle_Methods_for_Regularized_Risk_Minimization.html">18 jmlr-2010-Bundle Methods for Regularized Risk Minimization</a></p>
<p>Author: Choon Hui Teo, S.V.N. Vishwanthan, Alex J. Smola, Quoc V. Le</p><p>Abstract: A wide variety of machine learning problems can be described as minimizing a regularized risk functional, with different algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Machines (SVMs), Gaussian Processes, Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper describes the theory and implementation of a scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as L1 and L2 penalties. In addition to the uniﬁed framework we present tight convergence bounds, which show that our algorithm converges in O(1/ε) steps to ε precision for general convex problems and in O(log(1/ε)) steps for continuously differentiable problems. We demonstrate the performance of our general purpose solver on a variety of publicly available data sets. Keywords: optimization, subgradient methods, cutting plane method, bundle methods, regularized risk minimization, parallel optimization ∗. Also at Canberra Research Laboratory, NICTA. c 2010 Choon Hui Teo, S.V. N. Vishwanthan, Alex J. Smola and Quoc V. Le. T EO , V ISHWANATHAN , S MOLA AND L E</p><p>4 0.11011568 <a title="1-tfidf-4" href="./jmlr-2010-On_Finding_Predictors_for_Arbitrary_Families_of_Processes.html">81 jmlr-2010-On Finding Predictors for Arbitrary Families of Processes</a></p>
<p>Author: Daniil Ryabko</p><p>Abstract: The problem is sequence prediction in the following setting. A sequence x1 , . . . , xn , . . . of discretevalued observations is generated according to some unknown probabilistic law (measure) µ. After observing each outcome, it is required to give the conditional probabilities of the next observation. The measure µ belongs to an arbitrary but known class C of stochastic process measures. We are interested in predictors ρ whose conditional probabilities converge (in some sense) to the “true” µ-conditional probabilities, if any µ ∈ C is chosen to generate the sequence. The contribution of this work is in characterizing the families C for which such predictors exist, and in providing a speciﬁc and simple form in which to look for a solution. We show that if any predictor works, then there exists a Bayesian predictor, whose prior is discrete, and which works too. We also ﬁnd several sufﬁcient and necessary conditions for the existence of a predictor, in terms of topological characterizations of the family C , as well as in terms of local behaviour of the measures in C , which in some cases lead to procedures for constructing such predictors. It should be emphasized that the framework is completely general: the stochastic processes considered are not required to be i.i.d., stationary, or to belong to any parametric or countable family. Keywords: sequence prediction, time series, online prediction, Bayesian prediction</p><p>5 0.10975348 <a title="1-tfidf-5" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We consider learning formulations with non-convex objective functions that often occur in practical applications. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to a sub-optimal solution in reality. This paper tries to remedy the above gap between theory and practice. In particular, we present a multi-stage convex relaxation scheme for solving problems with non-convex objective functions. For learning formulations with sparse regularization, we analyze the behavior of a speciﬁc multistage relaxation scheme. Under appropriate conditions, we show that the local solution obtained by this procedure is superior to the global solution of the standard L1 convex relaxation for learning sparse targets. Keywords: sparsity, non-convex optimization, convex relaxation, multi-stage convex relaxation</p><p>6 0.082962736 <a title="1-tfidf-6" href="./jmlr-2010-A_Quasi-Newton_Approach_to_Nonsmooth_Convex_Optimization_Problems_in_Machine_Learning.html">5 jmlr-2010-A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning</a></p>
<p>7 0.081186615 <a title="1-tfidf-7" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>8 0.080559827 <a title="1-tfidf-8" href="./jmlr-2010-A_Fast_Hybrid_Algorithm_for_Large-Scalel1-Regularized_Logistic_Regression.html">3 jmlr-2010-A Fast Hybrid Algorithm for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>9 0.064377226 <a title="1-tfidf-9" href="./jmlr-2010-On_Spectral_Learning.html">84 jmlr-2010-On Spectral Learning</a></p>
<p>10 0.055727702 <a title="1-tfidf-10" href="./jmlr-2010-Dual_Averaging_Methods_for_Regularized_Stochastic_Learning_and_Online_Optimization.html">31 jmlr-2010-Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a></p>
<p>11 0.053149413 <a title="1-tfidf-11" href="./jmlr-2010-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">79 jmlr-2010-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>12 0.046747211 <a title="1-tfidf-12" href="./jmlr-2010-Classification_Methods_with_Reject_Option_Based_on_Convex_Risk_Minimization.html">21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</a></p>
<p>13 0.041938726 <a title="1-tfidf-13" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>14 0.038143076 <a title="1-tfidf-14" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>15 0.03812458 <a title="1-tfidf-15" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>16 0.035889611 <a title="1-tfidf-16" href="./jmlr-2010-Approximate_Riemannian_Conjugate_Gradient_Learning_for_Fixed-Form_Variational_Bayes.html">14 jmlr-2010-Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes</a></p>
<p>17 0.034615327 <a title="1-tfidf-17" href="./jmlr-2010-Learning_Translation_Invariant_Kernels_for_Classification.html">65 jmlr-2010-Learning Translation Invariant Kernels for Classification</a></p>
<p>18 0.030390941 <a title="1-tfidf-18" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>19 0.028694406 <a title="1-tfidf-19" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>20 0.028577296 <a title="1-tfidf-20" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.16), (1, -0.124), (2, 0.136), (3, -0.016), (4, 0.187), (5, -0.023), (6, 0.014), (7, -0.027), (8, 0.025), (9, -0.05), (10, 0.011), (11, -0.009), (12, 0.023), (13, -0.028), (14, 0.182), (15, -0.118), (16, 0.156), (17, -0.328), (18, 0.059), (19, -0.195), (20, 0.086), (21, -0.044), (22, 0.027), (23, 0.116), (24, -0.11), (25, 0.13), (26, 0.154), (27, -0.082), (28, -0.12), (29, 0.118), (30, 0.034), (31, 0.038), (32, 0.136), (33, -0.235), (34, 0.021), (35, 0.017), (36, 0.069), (37, 0.003), (38, 0.019), (39, -0.081), (40, -0.054), (41, -0.066), (42, 0.051), (43, 0.077), (44, 0.081), (45, 0.004), (46, 0.082), (47, -0.011), (48, -0.017), (49, -0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94492227 <a title="1-lsi-1" href="./jmlr-2010-A_Comparison_of_Optimization_Methods_and_Software_for_Large-scale_L1-regularized_Linear_Classification.html">1 jmlr-2010-A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</a></p>
<p>Author: Guo-Xun Yuan, Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Large-scale linear classiﬁcation is widely used in many areas. The L1-regularized form can be applied for feature selection; however, its non-differentiability causes more difﬁculties in training. Although various optimization methods have been proposed in recent years, these have not yet been compared suitably. In this paper, we ﬁrst broadly review existing methods. Then, we discuss state-of-the-art software packages in detail and propose two efﬁcient implementations. Extensive comparisons indicate that carefully implemented coordinate descent methods are very suitable for training large document data. Keywords: L1 regularization, linear classiﬁcation, optimization methods, logistic regression, support vector machines, document classiﬁcation</p><p>2 0.66838515 <a title="1-lsi-2" href="./jmlr-2010-Iterative_Scaling_and_Coordinate_Descent_Methods_for_Maximum_Entropy_Models.html">57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</a></p>
<p>Author: Fang-Lan Huang, Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin</p><p>Abstract: Maximum entropy (Maxent) is useful in natural language processing and many other areas. Iterative scaling (IS) methods are one of the most popular approaches to solve Maxent. With many variants of IS methods, it is difﬁcult to understand them and see the differences. In this paper, we create a general and uniﬁed framework for iterative scaling methods. This framework also connects iterative scaling and coordinate descent methods. We prove general convergence results for IS methods and analyze their computational complexity. Based on the proposed framework, we extend a coordinate descent method for linear SVM to Maxent. Results show that it is faster than existing iterative scaling methods. Keywords: maximum entropy, iterative scaling, coordinate descent, natural language processing, optimization</p><p>3 0.49333966 <a title="1-lsi-3" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We consider learning formulations with non-convex objective functions that often occur in practical applications. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to a sub-optimal solution in reality. This paper tries to remedy the above gap between theory and practice. In particular, we present a multi-stage convex relaxation scheme for solving problems with non-convex objective functions. For learning formulations with sparse regularization, we analyze the behavior of a speciﬁc multistage relaxation scheme. Under appropriate conditions, we show that the local solution obtained by this procedure is superior to the global solution of the standard L1 convex relaxation for learning sparse targets. Keywords: sparsity, non-convex optimization, convex relaxation, multi-stage convex relaxation</p><p>4 0.48803523 <a title="1-lsi-4" href="./jmlr-2010-A_Fast_Hybrid_Algorithm_for_Large-Scalel1-Regularized_Logistic_Regression.html">3 jmlr-2010-A Fast Hybrid Algorithm for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>Author: Jianing Shi, Wotao Yin, Stanley Osher, Paul Sajda</p><p>Abstract: ℓ1 -regularized logistic regression, also known as sparse logistic regression, is widely used in machine learning, computer vision, data mining, bioinformatics and neural signal processing. The use of ℓ1 regularization attributes attractive properties to the classiﬁer, such as feature selection, robustness to noise, and as a result, classiﬁer generality in the context of supervised learning. When a sparse logistic regression problem has large-scale data in high dimensions, it is computationally expensive to minimize the non-differentiable ℓ1 -norm in the objective function. Motivated by recent work (Koh et al., 2007; Hale et al., 2008), we propose a novel hybrid algorithm based on combining two types of optimization iterations: one being very fast and memory friendly while the other being slower but more accurate. Called hybrid iterative shrinkage (HIS), the resulting algorithm is comprised of a ﬁxed point continuation phase and an interior point phase. The ﬁrst phase is based completely on memory efﬁcient operations such as matrix-vector multiplications, while the second phase is based on a truncated Newton’s method. Furthermore, we show that various optimization techniques, including line search and continuation, can signiﬁcantly accelerate convergence. The algorithm has global convergence at a geometric rate (a Q-linear rate in optimization terminology). We present a numerical comparison with several existing algorithms, including an analysis using benchmark data from the UCI machine learning repository, and show our algorithm is the most computationally efﬁcient without loss of accuracy. Keywords: logistic regression, ℓ1 regularization, ﬁxed point continuation, supervised learning, large scale c 2010 Jianing Shi, Wotao Yin, Stanley Osher and Paul Sajda. S HI , Y IN , O SHER AND S AJDA</p><p>5 0.38335657 <a title="1-lsi-5" href="./jmlr-2010-On_Finding_Predictors_for_Arbitrary_Families_of_Processes.html">81 jmlr-2010-On Finding Predictors for Arbitrary Families of Processes</a></p>
<p>Author: Daniil Ryabko</p><p>Abstract: The problem is sequence prediction in the following setting. A sequence x1 , . . . , xn , . . . of discretevalued observations is generated according to some unknown probabilistic law (measure) µ. After observing each outcome, it is required to give the conditional probabilities of the next observation. The measure µ belongs to an arbitrary but known class C of stochastic process measures. We are interested in predictors ρ whose conditional probabilities converge (in some sense) to the “true” µ-conditional probabilities, if any µ ∈ C is chosen to generate the sequence. The contribution of this work is in characterizing the families C for which such predictors exist, and in providing a speciﬁc and simple form in which to look for a solution. We show that if any predictor works, then there exists a Bayesian predictor, whose prior is discrete, and which works too. We also ﬁnd several sufﬁcient and necessary conditions for the existence of a predictor, in terms of topological characterizations of the family C , as well as in terms of local behaviour of the measures in C , which in some cases lead to procedures for constructing such predictors. It should be emphasized that the framework is completely general: the stochastic processes considered are not required to be i.i.d., stationary, or to belong to any parametric or countable family. Keywords: sequence prediction, time series, online prediction, Bayesian prediction</p><p>6 0.29081887 <a title="1-lsi-6" href="./jmlr-2010-On_Spectral_Learning.html">84 jmlr-2010-On Spectral Learning</a></p>
<p>7 0.288434 <a title="1-lsi-7" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>8 0.28737959 <a title="1-lsi-8" href="./jmlr-2010-A_Quasi-Newton_Approach_to_Nonsmooth_Convex_Optimization_Problems_in_Machine_Learning.html">5 jmlr-2010-A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning</a></p>
<p>9 0.26990843 <a title="1-lsi-9" href="./jmlr-2010-Bundle_Methods_for_Regularized_Risk_Minimization.html">18 jmlr-2010-Bundle Methods for Regularized Risk Minimization</a></p>
<p>10 0.2174048 <a title="1-lsi-10" href="./jmlr-2010-Approximate_Riemannian_Conjugate_Gradient_Learning_for_Fixed-Form_Variational_Bayes.html">14 jmlr-2010-Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes</a></p>
<p>11 0.20904905 <a title="1-lsi-11" href="./jmlr-2010-Dual_Averaging_Methods_for_Regularized_Stochastic_Learning_and_Online_Optimization.html">31 jmlr-2010-Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a></p>
<p>12 0.19789712 <a title="1-lsi-12" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>13 0.19330503 <a title="1-lsi-13" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>14 0.16919491 <a title="1-lsi-14" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>15 0.1667954 <a title="1-lsi-15" href="./jmlr-2010-Classification_Methods_with_Reject_Option_Based_on_Convex_Risk_Minimization.html">21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</a></p>
<p>16 0.15813097 <a title="1-lsi-16" href="./jmlr-2010-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">79 jmlr-2010-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>17 0.14412078 <a title="1-lsi-17" href="./jmlr-2010-Regularized_Discriminant_Analysis%2C_Ridge_Regression_and_Beyond.html">98 jmlr-2010-Regularized Discriminant Analysis, Ridge Regression and Beyond</a></p>
<p>18 0.13621563 <a title="1-lsi-18" href="./jmlr-2010-Tree_Decomposition_for_Large-Scale_SVM_Problems.html">113 jmlr-2010-Tree Decomposition for Large-Scale SVM Problems</a></p>
<p>19 0.12902567 <a title="1-lsi-19" href="./jmlr-2010-Learning_Translation_Invariant_Kernels_for_Classification.html">65 jmlr-2010-Learning Translation Invariant Kernels for Classification</a></p>
<p>20 0.12505822 <a title="1-lsi-20" href="./jmlr-2010-Learning_Gradients%3A_Predictive_Models_that_Infer_Geometry_and_Statistical_Dependence.html">62 jmlr-2010-Learning Gradients: Predictive Models that Infer Geometry and Statistical Dependence</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.016), (3, 0.012), (8, 0.017), (21, 0.05), (32, 0.047), (36, 0.021), (37, 0.063), (46, 0.349), (75, 0.199), (85, 0.084), (96, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73935717 <a title="1-lda-1" href="./jmlr-2010-A_Comparison_of_Optimization_Methods_and_Software_for_Large-scale_L1-regularized_Linear_Classification.html">1 jmlr-2010-A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</a></p>
<p>Author: Guo-Xun Yuan, Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Large-scale linear classiﬁcation is widely used in many areas. The L1-regularized form can be applied for feature selection; however, its non-differentiability causes more difﬁculties in training. Although various optimization methods have been proposed in recent years, these have not yet been compared suitably. In this paper, we ﬁrst broadly review existing methods. Then, we discuss state-of-the-art software packages in detail and propose two efﬁcient implementations. Extensive comparisons indicate that carefully implemented coordinate descent methods are very suitable for training large document data. Keywords: L1 regularization, linear classiﬁcation, optimization methods, logistic regression, support vector machines, document classiﬁcation</p><p>2 0.54914194 <a title="1-lda-2" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>Author: Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, Chih-Jen Lin</p><p>Abstract: Kernel techniques have long been used in SVM to handle linearly inseparable problems by transforming data to a high dimensional space, but training and testing large data sets is often time consuming. In contrast, we can efﬁciently train and test much larger data sets using linear SVM without kernels. In this work, we apply fast linear-SVM methods to the explicit form of polynomially mapped data and investigate implementation issues. The approach enjoys fast training and testing, but may sometimes achieve accuracy close to that of using highly nonlinear kernels. Empirical experiments show that the proposed method is useful for certain large-scale data sets. We successfully apply the proposed method to a natural language processing (NLP) application by improving the testing accuracy under some training/testing speed requirements. Keywords: decomposition methods, low-degree polynomial mapping, kernel functions, support vector machines, dependency parsing, natural language processing</p><p>3 0.54783058 <a title="1-lda-3" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>Author: Shiliang Sun, John Shawe-Taylor</p><p>Abstract: In this paper, we propose a general framework for sparse semi-supervised learning, which concerns using a small portion of unlabeled data and a few labeled data to represent target functions and thus has the merit of accelerating function evaluations when predicting the output of a new example. This framework makes use of Fenchel-Legendre conjugates to rewrite a convex insensitive loss involving a regularization with unlabeled data, and is applicable to a family of semi-supervised learning methods such as multi-view co-regularized least squares and single-view Laplacian support vector machines (SVMs). As an instantiation of this framework, we propose sparse multi-view SVMs which use a squared ε-insensitive loss. The resultant optimization is an inf-sup problem and the optimal solutions have arguably saddle-point properties. We present a globally optimal iterative algorithm to optimize the problem. We give the margin bound on the generalization error of the sparse multi-view SVMs, and derive the empirical Rademacher complexity for the induced function class. Experiments on artiﬁcial and real-world data show their effectiveness. We further give a sequential training approach to show their possibility and potential for uses in large-scale problems and provide encouraging experimental results indicating the efﬁcacy of the margin bound and empirical Rademacher complexity on characterizing the roles of unlabeled data for semi-supervised learning. Keywords: semi-supervised learning, Fenchel-Legendre conjugate, representer theorem, multiview regularization, support vector machine, statistical learning theory</p><p>4 0.54373699 <a title="1-lda-4" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>Author: Pannagadatta K. Shivaswamy, Tony Jebara</p><p>Abstract: Leading classiﬁcation methods such as support vector machines (SVMs) and their counterparts achieve strong generalization performance by maximizing the margin of separation between data classes. While the maximum margin approach has achieved promising performance, this article identiﬁes its sensitivity to afﬁne transformations of the data and to directions with large data spread. Maximum margin solutions may be misled by the spread of data and preferentially separate classes along large spread directions. This article corrects these weaknesses by measuring margin not in the absolute sense but rather only relative to the spread of data in any projection direction. Maximum relative margin corresponds to a data-dependent regularization on the classiﬁcation function while maximum absolute margin corresponds to an ℓ2 norm constraint on the classiﬁcation function. Interestingly, the proposed improvements only require simple extensions to existing maximum margin formulations and preserve the computational efﬁciency of SVMs. Through the maximization of relative margin, surprising performance gains are achieved on real-world problems such as digit, text classiﬁcation and on several other benchmark data sets. In addition, risk bounds are derived for the new formulation based on Rademacher averages. Keywords: support vector machines, kernel methods, large margin, Rademacher complexity</p><p>5 0.54240549 <a title="1-lda-5" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>Author: Ming Yuan</p><p>Abstract: This paper considers the problem of estimating a high dimensional inverse covariance matrix that can be well approximated by “sparse” matrices. Taking advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix, we propose an estimating procedure that can effectively exploit such “sparsity”. The proposed method can be computed using linear programming and therefore has the potential to be used in very high dimensional problems. Oracle inequalities are established for the estimation error in terms of several operator norms, showing that the method is adaptive to different types of sparsity of the problem. Keywords: covariance selection, Dantzig selector, Gaussian graphical model, inverse covariance matrix, Lasso, linear programming, oracle inequality, sparsity</p><p>6 0.53891438 <a title="1-lda-6" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>7 0.53694785 <a title="1-lda-7" href="./jmlr-2010-A_Quasi-Newton_Approach_to_Nonsmooth_Convex_Optimization_Problems_in_Machine_Learning.html">5 jmlr-2010-A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning</a></p>
<p>8 0.53567874 <a title="1-lda-8" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<p>9 0.53544056 <a title="1-lda-9" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>10 0.53487849 <a title="1-lda-10" href="./jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</a></p>
<p>11 0.53396702 <a title="1-lda-11" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>12 0.53380996 <a title="1-lda-12" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>13 0.53332615 <a title="1-lda-13" href="./jmlr-2010-Linear_Algorithms_for_Online_Multitask_Classification.html">66 jmlr-2010-Linear Algorithms for Online Multitask Classification</a></p>
<p>14 0.53231621 <a title="1-lda-14" href="./jmlr-2010-Tree_Decomposition_for_Large-Scale_SVM_Problems.html">113 jmlr-2010-Tree Decomposition for Large-Scale SVM Problems</a></p>
<p>15 0.5302763 <a title="1-lda-15" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>16 0.52985662 <a title="1-lda-16" href="./jmlr-2010-Spectral_Regularization_Algorithms_for_Learning_Large_Incomplete_Matrices.html">105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</a></p>
<p>17 0.52953911 <a title="1-lda-17" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>18 0.52950919 <a title="1-lda-18" href="./jmlr-2010-Bayesian_Learning_in_Sparse_Graphical_Factor_Models_via_Variational_Mean-Field_Annealing.html">17 jmlr-2010-Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing</a></p>
<p>19 0.52892363 <a title="1-lda-19" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>20 0.52827585 <a title="1-lda-20" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
