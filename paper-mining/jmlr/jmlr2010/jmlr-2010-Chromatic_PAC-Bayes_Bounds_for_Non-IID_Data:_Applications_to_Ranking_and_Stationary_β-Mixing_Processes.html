<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-20" href="#">jmlr2010-20</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</h1>
<br/><p>Source: <a title="jmlr-2010-20-pdf" href="http://jmlr.org/papers/volume11/ralaivola10a/ralaivola10a.pdf">pdf</a></p><p>Author: Liva Ralaivola, Marie Szafranski, Guillaume Stempfel</p><p>Abstract: PAC-Bayes bounds are among the most accurate generalization bounds for classiﬁers learned from independently and identically distributed (IID) data, and it is particularly so for margin classiﬁers: there have been recent contributions showing how practical these bounds can be either to perform model selection (Ambroladze et al., 2007) or even to directly guide the learning of linear classiﬁers (Germain et al., 2009). However, there are many practical situations where the training data show some dependencies and where the traditional IID assumption does not hold. Stating generalization bounds for such frameworks is therefore of the utmost interest, both from theoretical and practical standpoints. In this work, we propose the ﬁrst—to the best of our knowledge—PAC-Bayes generalization bounds for classiﬁers trained on data exhibiting interdependencies. The approach undertaken to establish our results is based on the decomposition of a so-called dependency graph that encodes the dependencies within the data, in sets of independent data, thanks to graph fractional covers. Our bounds are very general, since being able to ﬁnd an upper bound on the fractional chromatic number of the dependency graph is sufﬁcient to get new PAC-Bayes bounds for speciﬁc settings. We show how our results can be used to derive bounds for ranking statistics (such as AUC) and classiﬁers trained on data distributed according to a stationary β-mixing process. In the way, we show how our approach seamlessly allows us to deal with U-processes. As a side note, we also provide a PAC-Bayes generalization bound for classiﬁers learned on data from stationary ϕ-mixing distributions. Keywords: PAC-Bayes bounds, non IID data, ranking, U-statistics, mixing processes c 2010 Liva Ralaivola, Marie Szafranski and Guillaume Stempfel. R ALAIVOLA , S ZAFRANSKI AND S TEMPFEL</p><p>Reference: <a title="jmlr-2010-20-reference" href="../jmlr2010_reference/jmlr-2010-Chromatic_PAC-Bayes_Bounds_for_Non-IID_Data%3A_Applications_to_Ranking_and_Stationary_%CE%B2-Mixing_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The approach undertaken to establish our results is based on the decomposition of a so-called dependency graph that encodes the dependencies within the data, in sets of independent data, thanks to graph fractional covers. [sent-16, score-0.467]
</p><p>2 Our bounds are very general, since being able to ﬁnd an upper bound on the fractional chromatic number of the dependency graph is sufﬁcient to get new PAC-Bayes bounds for speciﬁc settings. [sent-17, score-0.928]
</p><p>3 We show how our results can be used to derive bounds for ranking statistics (such as AUC) and classiﬁers trained on data distributed according to a stationary β-mixing process. [sent-18, score-0.284]
</p><p>4 As a side note, we also provide a PAC-Bayes generalization bound for classiﬁers learned on data from stationary ϕ-mixing distributions. [sent-20, score-0.153]
</p><p>5 1 Background Recently, there has been much progress in the ﬁeld of generalization bounds for classiﬁers, the most noticeable of which are Rademacher-complexity-based bounds (Bartlett and Mendelson, 2002; Bartlett et al. [sent-27, score-0.154]
</p><p>6 , 2005), stability-based bounds (Bousquet and Elisseeff, 2002) and PAC-Bayes bounds (McAllester, 1999). [sent-28, score-0.126]
</p><p>7 Here, we propose the ﬁrst PAC-Bayes bounds for classiﬁers trained on non-IID data; they constitute a generalization of the IID PAC-Bayes bound and they are generic enough to provide a principled way to establish generalization bounds for a number of non-IID settings. [sent-37, score-0.27]
</p><p>8 To establish these bounds, we make use of simple tools from probability theory, convexity properties of some functions, and we exploit the notion of fractional covers of graphs (Schreinerman and Ullman, 1997). [sent-38, score-0.377]
</p><p>9 Note that we essentially provide bounds for the case of identically and non-independently distributed data; the additional results that we give in the appendix generalizes to non-identically and nonindependently distributed data. [sent-40, score-0.141]
</p><p>10 It allows us to derive generalization bounds on the ranking performance of scoring/ranking functions using two different performance measures, among which the area under the ROC curve (AUC) . [sent-48, score-0.23]
</p><p>11 Agarwal and Niyogi (2009) base their analysis of ranking performances on algorithmic stability, and the qualitative comparison of their bounds and ours is not straightforward because stability arguments are somewhat different from the arguments used for PAC-Bayes bounds (and other uniform bounds). [sent-57, score-0.265]
</p><p>12 As already observed by Janson (2004), coloring provides a way to generalize large deviation results based on U-statistics; this observation carries over when generalization bounds are considered, which allows us to draw a connection between the results we obtain and that of Cl´ mencon et al. [sent-58, score-0.214]
</p><p>13 In particular, we show how our chromatic bounds can be used to easily derive new generalization bounds for βmixing processes. [sent-61, score-0.52]
</p><p>14 The striking feature is that it is done at a very low price: the independent block method proposed by Yu (1994) directly gives a dependency graph whose chromatic number is straightforward to compute. [sent-64, score-0.458]
</p><p>15 As we shall see, this sufﬁces to instantiate our chromatic bounds, which, after simple calculations, leads to appropriate generalization bound. [sent-65, score-0.394]
</p><p>16 For sake of completeness, we also provide a PAC-Bayes bound for stationary ϕ-mixing processes; it is based on a different approach and its presentation is postponed to the appendix together with the tools that allows us to derive it. [sent-66, score-0.125]
</p><p>17 Section 3 introduces the notion of fractional covers and states the new chromatic PAC-Bayes bounds, which rely on the fractional chromatic number of the dependency graph of the data at hand. [sent-70, score-1.457]
</p><p>18 Section 4 provides speciﬁc versions of our bounds for the case of IID data, ranking and stationary β-mixing processes, giving rise to original generalization bounds. [sent-71, score-0.29]
</p><p>19 A PAC-Bayes bound for stationary ϕ-mixing based on arguments different from the chromatic PAC-Bayes bound is provided, in the appendix. [sent-72, score-0.556]
</p><p>20 We mainly consider the problem of binary classiﬁcation over the input space X and we denote the set of possible labels as Y = {−1, +1} (for the case of ranking described in Section 4, we we use Y = R ); Z denotes the product space X × Y . [sent-75, score-0.139]
</p><p>21 Theorem 1 (IID PAC-Bayes Bound) ∀D, ∀H , ∀δ ∈ (0, 1], ∀P, with probability at least 1 − δ over the random draw of Z ∼ Dm = Dm , the following holds: ∀Q, kl(eQ (Z)||eQ ) ≤ ˆ  m+1 1 KL(Q||P) + ln . [sent-87, score-0.122]
</p><p>22 m δ  (1)  This theorem provides a generalization error bound for the Gibbs classiﬁer gQ : given a distribution Q, this stochastic classiﬁer predicts a class for x ∈ X by ﬁrst drawing a hypothesis h according to Q and then outputting sign(h(x)). [sent-88, score-0.146]
</p><p>23 This makes it possible to rewrite bound (1) in a more ‘usual’ form: ∀Q, eQ ≤ eQ (Z) + kl−1 eQ (Z), ˆ ˆ  1 m+1 KL(Q||P) + ln m δ  . [sent-96, score-0.152]
</p><p>24 These results rely on properties of a dependency graph that is built according to the dependencies within Z. [sent-102, score-0.12]
</p><p>25 • C = {(C j , ω j )}n , with C j ⊆ V and ω j ∈ [0, 1], is a proper exact fractional cover of V if each j=1 C j is independent and ∀i ∈ V , ∑n ω j Ii∈C j = 1; ω(C) = ∑n ωi is the chromatic weight of j=1 j=1 C. [sent-112, score-0.73]
</p><p>26 The problem of computing the (fractional) chromatic number of a graph is N P-hard (Schreinerman and Ullman, 1997). [sent-115, score-0.411]
</p><p>27 Lemma 4 If C = {(C j , ω j )}n is an exact fractional cover of Γ = (V, E), with V = [m], then j=1 m  n  i=1  ∀t ∈ R m ,  j=1  ∑ t i = ∑ ω j ∑ tk . [sent-128, score-0.34]
</p><p>28 P EFC(Dm ) is the set of proper exact fractional covers of Γ(Dm ). [sent-135, score-0.378]
</p><p>29 We would like to emphasize that the same type of result, using the same proof techniques, can be obtained if simple (that is, not exact nor proper) fractional covers are considered. [sent-143, score-0.382]
</p><p>30 However, as we shall see, the ‘best’ (in terms of tightness) bound is achieved for covers from the set of proper exact fractional covers, and this is the reason why we have stated Theorem 5 with a restriction to this particular set of covers. [sent-144, score-0.443]
</p><p>31 Given a cover and a (possibly factorized) prior Pn , look for a factorized posterior Qn = ⊗n Q j such that each Q j j=1 independently minimizes the usual IID PAC-Bayes bound given in Theorem 1 on each Z( j) . [sent-156, score-0.126]
</p><p>32 This theorem says that even in the case of non IID data, a PAC-Bayes bound very similar to the IID PAC-Bayes bound (1) can be stated, with a worsening (since χ∗ ≥ 1) proportional to χ∗ , that is, proportional to the amount of dependencies in the data. [sent-167, score-0.211]
</p><p>33 In addition, the new PAC-Bayes bounds is valid with any priors and posteriors, without the need for these distributions to depend on the chosen cover (as is the case with the more general Theorem 5). [sent-168, score-0.124]
</p><p>34 Among all elements of P EFC(Dm ), χ∗ is the best constant achievable in terms of the tightness of the bound (6) on eQ : getting an optimal coloring gives rise to an ‘optimal’ bound. [sent-170, score-0.132]
</p><p>35 As χ∗ is the smallest chromatic weight, it gives the tightest bound. [sent-172, score-0.366]
</p><p>36 ∀Dm , ∀H , ∀k ∈ [m], ∀δ ∈ (0, 1], ∀P, with probability at least 1 − δ over the random draw of Z ∼ Dm : ∀Q, eQ ≤ min  s∈{m}#k  ˆ eQ (Zs ) + kl−1 eQ (Zs ), ˆ  χ∗ |s| + χ∗ 1 m s s KL(Q||P) + ln + ln + ln ∗ |s| χs δ k  . [sent-183, score-0.296]
</p><p>37 ˆ where χ∗ is the fractional chromatic number of Γ(Ds ), and where eQ (Zs ) is the empirical error of s ˆ Zs ). [sent-184, score-0.645]
</p><p>38 the Gibbs classiﬁer gQ on Zs , that is: eQ (Zs ) = Eh∼Q R(h, ˆ m Proof Simply apply the union bound to Equation (6) of Theorem 8: for ﬁxed k, there are m−k = m m m k subgraphs and using δ/ k makes the bound hold with probability 1 − δ for all possible k subgraphs (simultaneously). [sent-185, score-0.184]
</p><p>39 3 On the Relevance of Fractional Covers One may wonder whether using the fractional cover framework is the only way to establish a result similar to the one provided by Theorem 5. [sent-190, score-0.363]
</p><p>40 (For instance, one may manipulate subsets of independent variables, assign weights to these subsets without referring to fractional covers, and arrive at results that are comparable to ours. [sent-192, score-0.279]
</p><p>41 ) However, if we assume that singling out independent sets of variables is the cornerstone of dealing with interdependent random variables, we ﬁnd it enlightening to cast our approach within the rich and well-studied fractional cover/coloring framework. [sent-193, score-0.301]
</p><p>42 On the one hand, our objective of deriving tight bounds amounts to ﬁnding a decomposition of the set of random variables at hand into few and large independent subsets and taking the graph theory point of view, this obviously corresponds to a problem of graph coloring. [sent-194, score-0.153]
</p><p>43 Explicitly using the fractional cover/coloring argument allows us to directly beneﬁt from the wealth of related results, such as Property 1 or, for instance, approaches as to how compute a cover or approximate the fractional chromatic number (for instance, linear programming). [sent-195, score-0.985]
</p><p>44 On the other hand, from a technical point of view, making use of the fractional cover argument allows us to preserve the simple structure of the proof of the classical IID PACBayes bound to derive Theorem 5. [sent-196, score-0.433]
</p><p>45 Lemma 11 ∀Dm , ∀C = {(C j , ω j )}n , ∀Pn , ∀Qn , with probability at least 1 − δ over the random j=1 draw of Z ∼ Dm , the following holds m n m+ω n ˆ ∑ j=1 π j Eh∼Qnj kl(R(h, Z( j) )||R(h)) ≤ ∑ j=1 α j KL(Qnj ||Pnj ) + ln δω . [sent-208, score-0.122]
</p><p>46 1937  R ALAIVOLA , S ZAFRANSKI AND S TEMPFEL  (a) IID data  (b) Bipartite ranking data  Figure 2: Dependency graphs for different settings described in Section 4. [sent-215, score-0.139]
</p><p>47 (a) When the data are IID, the dependency graph is disconnected and the fractional number is χ∗ = 1; (b) a dependency graph obtained for bipartite ranking from a sample of 4 positive and 2 negative instances: χ∗ = 4. [sent-217, score-0.643]
</p><p>48 Applications In this section, we provide instances of Theorem 8 for various settings; amazingly, they alllow us to easily derive PAC-Bayes generalization bounds for problems such as ranking and learning from stationary β-mixing processes. [sent-219, score-0.29]
</p><p>49 In this case, the training sample Z = {(Xi ,Yi )}m i=1 is distributed according to Dm = Dm and the fractional chromatic number of Γ(Dm ) is χ∗ = 1, since the dependency graph, depicted in Figure 2a is totally disconnected (see Property 1). [sent-223, score-0.714]
</p><p>50 Plugging in this value of χ∗ in the bound of Theorem 8 gives the IID PAC-Bayes bound of Theorem 1. [sent-224, score-0.13]
</p><p>51 D is a distribution over X × Y with Y = R and one looks for a ranking rule h ∈ R X ×X that minimizes the ranking risk Rrank (h) deﬁned as: Rrank (h) := P  (X,Y )∼D (X ′ ,Y ′ )∼D  ((Y −Y ′ )h(X, X ′ ) < 0). [sent-228, score-0.31]
</p><p>52 The ranking rule h predicts X to be better than X ′ if sign(h(X, X ′ )) = 1 and conversely. [sent-230, score-0.139]
</p><p>53 A natural question is to bound the ranking risk for any learning rule h given S, where the difﬁculty is that (9) is a sum of identically but not independently random variables, namely the variables IYi j h(Xi ,X j ) . [sent-233, score-0.27]
</p><p>54 Henceforth, we are clearly in the framework for the application of the chromatic PAC-Bayes bounds deﬁned in the previous section. [sent-236, score-0.429]
</p><p>55 In particular, to instantiate Theorem 8 to the present ranking problem, we simply need to have at hand the value χ∗ , or an upper rank bound thereof, of the fractional chromatic number of Γrank . [sent-237, score-0.886]
</p><p>56 To do so, we consider a rank ˆ fractional cover of Γrank motivated by the theory of U-statistics (Hoeffding, 1948, 1963). [sent-241, score-0.377]
</p><p>57 A proper exact fractional cover Crank can be derived from this decomposition as1 Crank :=  Cσ := Zσ(i)σ(⌊ℓ/2⌋+i)  ⌊ℓ/2⌋ , ωσ i=1  :=  1 (ℓ − 2)! [sent-245, score-0.364]
</p><p>58 1939  R ALAIVOLA , S ZAFRANSKI AND S TEMPFEL  which proves that Crank is a proper exact fractional cover. [sent-256, score-0.303]
</p><p>59 ⌊ℓ/2⌋  The theorem follows by an instantiation of Theorem 8 with m := ℓ(ℓ − 1) and the bound on χ∗ rank we have just proven. [sent-260, score-0.155]
</p><p>60 To our knowledge, this is the ﬁrst PAC-Bayes bound on the ranking risk, while a Rademachercomplexity based analysis was given by Cl´ mencon et al. [sent-261, score-0.248]
</p><p>61 In the proof, we have used e ¸ arguments from the analysis of U-processes, which allow us to easily derive a convenient fractional cover of the dependency graph of Z. [sent-263, score-0.432]
</p><p>62 (2008) to establish fast rates of convergence for empirical ranking risk minimizers could be used to draw possibly tighter PAC-Bayes bounds. [sent-267, score-0.229]
</p><p>63 Of course, the ranking rule may be based on a scoring function f ∈ R X such that h(X, X ′ ) = f (X) − f (X ′ ), in which case all the results that we state in terms of h can be stated similarly in terms of f . [sent-271, score-0.139]
</p><p>64 If ℓ is even, then our bound on χ∗ is equal to 2(ℓ − 1) and so is χ∗ ; if ℓ is rank rank rank odd, then our bound is 2ℓ. [sent-275, score-0.241]
</p><p>65 3 Bipartite Ranking and a Bound on the AUC A particular ranking setting is that of bipartite ranking, where Y = {−1, +1}. [sent-277, score-0.18]
</p><p>66 , 2005), one may be interested in controlling what we call the bipartite misranking risk RAUC (h) (the reason for the AUC superscript will become clear in the sequel), of a ranking rule h ∈ R X ×X by RAUC (h) := P X∼D+1 (h(X, X ′ ) < 0). [sent-280, score-0.212]
</p><p>67 As a consequence, providing a PAC-Bayes bound on RAUC (h) (or RAUC ( f )) amounts to providing a generalization (lower) bound on the AUC, which is a widely used measure in practice to evaluate the performance of a scoring function. [sent-288, score-0.158]
</p><p>68 Theorem 8 can thus be directly applied to classiﬁers trained on Z, the structure of Γ(Dy,m ) and its corresponding fractional chromatic number χ∗ y being completely determined by y. [sent-310, score-0.645]
</p><p>69 Hence, ∀δ ∈ (0, 1], ∀P, with probability at least 1 − δ over the random draw of S ∼ Dℓ , A ∀Q, kl(eQUC ||eQUC ) ≤ ˆA  mS + χ∗ χ∗ S S KL(Q||P) + ln . [sent-319, score-0.122]
</p><p>70 mS δχ∗ S  (13)  where χ∗ is the fractional chromatic number of the graph Γ(Z), with Z deﬁned from S as in the ﬁrst S part of the proof, where the observed (random) labels are now taken into account; here mS = ℓ+ ℓ− , where ℓ+ (ℓ− ) is the number of positive (negative) data in S. [sent-320, score-0.69]
</p><p>71 3 C OMPUTING  THE  F RACTIONAL C HROMATIC N UMBER  In order to ﬁnish the proof, it sufﬁces to observe that, for Z = {Zi j }i j , if ℓmax = max(ℓ+ , ℓ− ), then the fractional chromatic number of Γ(Z) is χ∗ = ℓmax . [sent-323, score-0.645]
</p><p>72 This cover is of size ℓ+ = ℓmax , which means that it achieves the minimal possible weight over proper exact (fractional) covers since χ∗ ≥ ℓmax . [sent-344, score-0.16]
</p><p>73 f (x The ranking rule h is thus a linear classiﬁer acting on the difference of its arguments (the next result we present therefore carries over to kernel classiﬁers). [sent-351, score-0.139]
</p><p>74 We arbitrarily choose to provide it for this AUC based bound as learning linear ranking rule by AUC minimization is a common approach (Ataman et al. [sent-355, score-0.204]
</p><p>75 The bounds given in Theorem 14 and Theorem 15 are very similar to what we would get if applying IID PAC-Bayes bound to one (independent) element C j of a minimal cover (i. [sent-357, score-0.189]
</p><p>76 , its weight equals the fractional chromatic number) C = {C j }n such as the one we used in the proof of j=1 Theorem 14. [sent-359, score-0.673]
</p><p>77 It turns out that, for proper exact fractional covers C = {(C j , ω)}n with elements C j having the same size, it is better, in terms of j=1 1943  R ALAIVOLA , S ZAFRANSKI AND S TEMPFEL  absolute moments of the empirical error, to assess it on the whole data set, rather than on only one C j . [sent-361, score-0.378]
</p><p>78 The bound that we propose is in the same vein as the one proposed by Mohri and Rostamizadeh (2009), with the difference that our bound is a PAC-Bayes bound and theirs a Rademacher-complexitybased bounds. [sent-389, score-0.195]
</p><p>79 Let us state our generalization bound for classiﬁers trained on samples Z drawn from stationary β-mixing distributions. [sent-391, score-0.153]
</p><p>80 ˆ m  Proof The proof makes use of the independent block decomposition proposed by Yu (1994), our chromatic PAC-Bayes bound of Theorem 8, and Corollary 24 (Appendix). [sent-395, score-0.459]
</p><p>81 2 A B OUND  FOR  Z0  To establish the bound for Z0 , it sufﬁces to use Corollary 24 (Appendix) with c(z) being deﬁned as: c(z) := IΦ(P,z,δ) , which is a bounded measurable function on the blocks Zs (and thus on the blocks Zs ). [sent-429, score-0.152]
</p><p>82 The derivation of these results rely on the use of fractional covers of graphs, convexity and standard tools from probability theory. [sent-440, score-0.354]
</p><p>83 The results that we provide are very general and can easily be instantiated for speciﬁc learning settings such as ranking and learning from from mixing distributions: amazingly, we obtain at a very low cost original PAC-Bayes bounds for these settings. [sent-441, score-0.226]
</p><p>84 Using a generalized PAC-Bayes bound, we provide in the appendix a chromatic PAC-Bayes bound that holds for non-independently and non-identically distributed data: it allows us to derive a PACBayes bound for classiﬁers trained on data from a stationary ϕ-mixing distribution. [sent-442, score-0.578]
</p><p>85 It might be interesting to make this connection clearer to see if, for instance, tighter and still general bounds can be obtained with more appropriate variational relaxations than the one incurred by the use of fractional covers. [sent-445, score-0.342]
</p><p>86 (2009) is also another contribution that tends to support that a practical use of our bounds should provide competitive results (note that Theorem 25 gives a sufﬁcient condition for the general PAC-Bayes bound of Germain et al. [sent-450, score-0.128]
</p><p>87 Likewise, it would be interesting to see how the possibly more accurate PAC-Bayes bound for large margin classiﬁers proposed by Langford and Shawe-taylor (2002), which should translate to the case of bipartite ranking as well, performs empirically. [sent-452, score-0.245]
</p><p>88 The connection between our ranking bounds and the theory of U-statistics makes it possible to envision the use of higher order moments in establishing PAC-Bayes bounds, thanks to Hoeffding’s decomposition. [sent-456, score-0.202]
</p><p>89 We plan to investigate further in this direction, for both the ranking measures we have studied (noting that the AUC is a two-sample U-statistics Hoeffding, 1963). [sent-457, score-0.139]
</p><p>90 Finally, we have been working on a more general way to establish chromatic bounds from IID bounds (covering VC, Rademacher, PAC-Bayes and—possibly—binomial tail test set bounds), without the need to perform ‘low-level’ calculations such as the ones proposed in Section 3. [sent-458, score-0.515]
</p><p>91 The meta-bound that we have been developing is in the spirit of that proposed by Blanchard and Fleuret (2007), except that the randomization we propose is on the subsets constituting the fractional cover (and not the hypothesis set). [sent-460, score-0.34]
</p><p>92 In other terms, given a cover C = {(C j , ω j )} j , the fact that an IID bound holds on one subset C j of a cover is considered as a random event, the probability of a subset to be chosen being ω j /ω(C). [sent-461, score-0.187]
</p><p>93 A simple union bound gives our generic result, which translates into cover-independent (but fractional-chromatic-number-dependent) chromatic bounds such as (6) (Theorem 8) under very mild conditions on the shape of the base IID bound. [sent-462, score-0.494]
</p><p>94 Using the entropy extremal inequality ln EX∼P f (X) ≥ − KL(Q||P)) + EX∼Q ln f (X), ∀P, Q, X (see the proof of Lemma 11), and the fact that x → ln x is nondecreasing, the previous step leads to PZ ∃Q : − KL(Q||P) + (β − 1)Eh∼Q ∆(Eψ(h), Eψ(h) − ψ(h, Z)) ≥ ln  1950  αβ ≤ δ. [sent-495, score-0.401]
</p><p>95 1 Generalized Chromatic PAC-Bayes Bound To get a chromatic PAC-Bayes theorem for non-identically non-independently distributed data, we simply make use of the following concentration inequality of Janson (2004). [sent-523, score-0.506]
</p><p>96 If SZ = ∑m Zi , then, i=1 ∀ε > 0, PSZ [ESZ − SZ ≥ ε] ≤ exp −  2ε2 , χ∗ (Dm ) ∑m (bi − ai )2 i=1  where χ∗ (Dm ) is the fractional chromatic number of the dependency graph of Dm . [sent-526, score-0.737]
</p><p>97 This concentration inequality gives rise to the following generalized chromatic PAC-Bayes bound that applies to non independently, possibly non identically distributed data and allows us to use any bounded loss functions r. [sent-528, score-0.552]
</p><p>98 • In the case of using identically distributed random variables and the 0-1 loss, there is no concentration inequality that allows us to retrieve the tighter PAC-Bayes bound given in Theorem 8. [sent-533, score-0.186]
</p><p>99 In order to establish our new PAC-Bayes bounds for stationary ϕ-mixing distributions, it sufﬁces to make use of the following concentration inequality by Kontorovich and Ramanan (2008). [sent-542, score-0.211]
</p><p>100 Theorem 31 (PAC-Bayes bound for stationary ϕ-mixing processes) Let Dϕ be a stationary ϕϕ mixing distribution over Z and Dm be the distribution of m-samples according to Dϕ . [sent-549, score-0.209]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('chromatic', 0.366), ('dm', 0.362), ('eq', 0.342), ('eh', 0.283), ('fractional', 0.279), ('kl', 0.232), ('ez', 0.175), ('iid', 0.149), ('hromatic', 0.139), ('ranking', 0.139), ('alaivola', 0.131), ('tempfel', 0.131), ('zafranski', 0.131), ('pz', 0.119), ('zs', 0.119), ('eqn', 0.113), ('qnj', 0.113), ('rauc', 0.113), ('equc', 0.105), ('qn', 0.089), ('rrank', 0.087), ('ln', 0.087), ('zi', 0.086), ('ounds', 0.076), ('covers', 0.075), ('auc', 0.073), ('pac', 0.069), ('bound', 0.065), ('bounds', 0.063), ('efc', 0.061), ('cover', 0.061), ('stationary', 0.06), ('agarwal', 0.06), ('theorem', 0.053), ('pn', 0.05), ('dependency', 0.047), ('graph', 0.045), ('coloring', 0.044), ('crank', 0.044), ('erank', 0.044), ('janson', 0.044), ('mencon', 0.044), ('pnj', 0.044), ('schreinerman', 0.044), ('usunier', 0.044), ('langford', 0.042), ('bipartite', 0.041), ('germain', 0.04), ('concentration', 0.04), ('mcallester', 0.039), ('dy', 0.039), ('mohri', 0.038), ('rostamizadeh', 0.037), ('rank', 0.037), ('ex', 0.035), ('iyi', 0.035), ('draw', 0.035), ('identically', 0.034), ('bq', 0.034), ('za', 0.034), ('ers', 0.033), ('subgraph', 0.033), ('blocks', 0.032), ('risk', 0.032), ('ces', 0.031), ('ambroladze', 0.03), ('marseille', 0.03), ('ullman', 0.03), ('seeger', 0.028), ('rademacher', 0.028), ('dependencies', 0.028), ('proof', 0.028), ('generalization', 0.028), ('zt', 0.027), ('gq', 0.027), ('subgraphs', 0.027), ('guillaume', 0.026), ('ound', 0.026), ('szafranski', 0.026), ('classi', 0.026), ('clique', 0.025), ('roc', 0.025), ('pq', 0.025), ('inequality', 0.025), ('mixing', 0.024), ('proper', 0.024), ('ps', 0.024), ('hoeffding', 0.024), ('lemma', 0.023), ('tightness', 0.023), ('gibbs', 0.023), ('establish', 0.023), ('ds', 0.023), ('cnrs', 0.022), ('kontorovich', 0.022), ('liva', 0.022), ('ralaivola', 0.022), ('interdependent', 0.022), ('stationarity', 0.022), ('distributed', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="20-tfidf-1" href="./jmlr-2010-Chromatic_PAC-Bayes_Bounds_for_Non-IID_Data%3A_Applications_to_Ranking_and_Stationary_%CE%B2-Mixing_Processes.html">20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</a></p>
<p>Author: Liva Ralaivola, Marie Szafranski, Guillaume Stempfel</p><p>Abstract: PAC-Bayes bounds are among the most accurate generalization bounds for classiﬁers learned from independently and identically distributed (IID) data, and it is particularly so for margin classiﬁers: there have been recent contributions showing how practical these bounds can be either to perform model selection (Ambroladze et al., 2007) or even to directly guide the learning of linear classiﬁers (Germain et al., 2009). However, there are many practical situations where the training data show some dependencies and where the traditional IID assumption does not hold. Stating generalization bounds for such frameworks is therefore of the utmost interest, both from theoretical and practical standpoints. In this work, we propose the ﬁrst—to the best of our knowledge—PAC-Bayes generalization bounds for classiﬁers trained on data exhibiting interdependencies. The approach undertaken to establish our results is based on the decomposition of a so-called dependency graph that encodes the dependencies within the data, in sets of independent data, thanks to graph fractional covers. Our bounds are very general, since being able to ﬁnd an upper bound on the fractional chromatic number of the dependency graph is sufﬁcient to get new PAC-Bayes bounds for speciﬁc settings. We show how our results can be used to derive bounds for ranking statistics (such as AUC) and classiﬁers trained on data distributed according to a stationary β-mixing process. In the way, we show how our approach seamlessly allows us to deal with U-processes. As a side note, we also provide a PAC-Bayes generalization bound for classiﬁers learned on data from stationary ϕ-mixing distributions. Keywords: PAC-Bayes bounds, non IID data, ranking, U-statistics, mixing processes c 2010 Liva Ralaivola, Marie Szafranski and Guillaume Stempfel. R ALAIVOLA , S ZAFRANSKI AND S TEMPFEL</p><p>2 0.15054144 <a title="20-tfidf-2" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>Author: Yevgeny Seldin, Naftali Tishby</p><p>Abstract: We derive PAC-Bayesian generalization bounds for supervised and unsupervised learning models based on clustering, such as co-clustering, matrix tri-factorization, graphical models, graph clustering, and pairwise clustering.1 We begin with the analysis of co-clustering, which is a widely used approach to the analysis of data matrices. We distinguish among two tasks in matrix data analysis: discriminative prediction of the missing entries in data matrices and estimation of the joint probability distribution of row and column variables in co-occurrence matrices. We derive PAC-Bayesian generalization bounds for the expected out-of-sample performance of co-clustering-based solutions for these two tasks. The analysis yields regularization terms that were absent in the previous formulations of co-clustering. The bounds suggest that the expected performance of co-clustering is governed by a trade-off between its empirical performance and the mutual information preserved by the cluster variables on row and column IDs. We derive an iterative projection algorithm for ﬁnding a local optimum of this trade-off for discriminative prediction tasks. This algorithm achieved stateof-the-art performance in the MovieLens collaborative ﬁltering task. Our co-clustering model can also be seen as matrix tri-factorization and the results provide generalization bounds, regularization terms, and new algorithms for this form of matrix factorization. The analysis of co-clustering is extended to tree-shaped graphical models, which can be used to analyze high dimensional tensors. According to the bounds, the generalization abilities of treeshaped graphical models depend on a trade-off between their empirical data ﬁt and the mutual information that is propagated up the tree levels. We also formulate weighted graph clustering as a prediction problem: given a subset of edge weights we analyze the ability of graph clustering to predict the remaining edge weights. The analysis of co-clustering easily</p><p>3 0.083748259 <a title="20-tfidf-3" href="./jmlr-2010-Stability_Bounds_for_Stationary_%CF%86-mixing_and_%CE%B2-mixing_Processes.html">106 jmlr-2010-Stability Bounds for Stationary φ-mixing and β-mixing Processes</a></p>
<p>Author: Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, independently of any algorithm. In contrast, the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to speciﬁc learning algorithms by exploiting their particular properties. However, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed. In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence. This paper studies the scenario where the observations are drawn from a stationary ϕ-mixing or β-mixing sequence, a widely adopted assumption in the study of non-i.i.d. processes that implies a dependence between observations weakening over time. We prove novel and distinct stability-based generalization bounds for stationary ϕ-mixing and β-mixing sequences. These bounds strictly generalize the bounds given in the i.i.d. case and apply to all stable learning algorithms, thereby extending the use of stability-bounds to non-i.i.d. scenarios. We also illustrate the application of our ϕ-mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression, Kernel Ridge Regression, and Support Vector Machines, and many other kernel regularization-based and relative entropy-based regularization algorithms. These novel bounds can thus be viewed as the ﬁrst theoretical basis for the use of these algorithms in non-i.i.d. scenarios. Keywords: learning in non-i.i.d. scenarios, weakly dependent observations, mixing distributions, algorithmic stability, generalization bounds, learning theory</p><p>4 0.082234398 <a title="20-tfidf-4" href="./jmlr-2010-Posterior_Regularization_for_Structured_Latent_Variable_Models.html">91 jmlr-2010-Posterior Regularization for Structured Latent Variable Models</a></p>
<p>Author: Kuzman Ganchev, João Graça, Jennifer Gillenwater, Ben Taskar</p><p>Abstract: We present posterior regularization, a probabilistic framework for structured, weakly supervised learning. Our framework efﬁciently incorporates indirect supervision via constraints on posterior distributions of probabilistic models with latent variables. Posterior regularization separates model complexity from the complexity of structural constraints it is desired to satisfy. By directly imposing decomposable regularization on the posterior moments of latent variables during learning, we retain the computational efﬁciency of the unconstrained model while ensuring desired constraints hold in expectation. We present an efﬁcient algorithm for learning with posterior regularization and illustrate its versatility on a diverse set of structural constraints such as bijectivity, symmetry and group sparsity in several large scale experiments, including multi-view learning, cross-lingual dependency grammar induction, unsupervised part-of-speech induction, and bitext word alignment.1 Keywords: posterior regularization framework, unsupervised learning, latent variables models, prior knowledge, natural language processing</p><p>5 0.066847801 <a title="20-tfidf-5" href="./jmlr-2010-On_Finding_Predictors_for_Arbitrary_Families_of_Processes.html">81 jmlr-2010-On Finding Predictors for Arbitrary Families of Processes</a></p>
<p>Author: Daniil Ryabko</p><p>Abstract: The problem is sequence prediction in the following setting. A sequence x1 , . . . , xn , . . . of discretevalued observations is generated according to some unknown probabilistic law (measure) µ. After observing each outcome, it is required to give the conditional probabilities of the next observation. The measure µ belongs to an arbitrary but known class C of stochastic process measures. We are interested in predictors ρ whose conditional probabilities converge (in some sense) to the “true” µ-conditional probabilities, if any µ ∈ C is chosen to generate the sequence. The contribution of this work is in characterizing the families C for which such predictors exist, and in providing a speciﬁc and simple form in which to look for a solution. We show that if any predictor works, then there exists a Bayesian predictor, whose prior is discrete, and which works too. We also ﬁnd several sufﬁcient and necessary conditions for the existence of a predictor, in terms of topological characterizations of the family C , as well as in terms of local behaviour of the measures in C , which in some cases lead to procedures for constructing such predictors. It should be emphasized that the framework is completely general: the stochastic processes considered are not required to be i.i.d., stationary, or to belong to any parametric or countable family. Keywords: sequence prediction, time series, online prediction, Bayesian prediction</p><p>6 0.049951874 <a title="20-tfidf-6" href="./jmlr-2010-Learning_Gradients%3A_Predictive_Models_that_Infer_Geometry_and_Statistical_Dependence.html">62 jmlr-2010-Learning Gradients: Predictive Models that Infer Geometry and Statistical Dependence</a></p>
<p>7 0.048084714 <a title="20-tfidf-7" href="./jmlr-2010-Approximate_Riemannian_Conjugate_Gradient_Learning_for_Fixed-Form_Variational_Bayes.html">14 jmlr-2010-Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes</a></p>
<p>8 0.047623172 <a title="20-tfidf-8" href="./jmlr-2010-Consistent_Nonparametric_Tests_of_Independence.html">27 jmlr-2010-Consistent Nonparametric Tests of Independence</a></p>
<p>9 0.047209922 <a title="20-tfidf-9" href="./jmlr-2010-Maximum_Likelihood_in_Cost-Sensitive_Learning%3A_Model_Specification%2C_Approximations%2C_and_Upper_Bounds.html">73 jmlr-2010-Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds</a></p>
<p>10 0.044761553 <a title="20-tfidf-10" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>11 0.044371773 <a title="20-tfidf-11" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>12 0.040392444 <a title="20-tfidf-12" href="./jmlr-2010-Learnability%2C_Stability_and_Uniform_Convergence.html">60 jmlr-2010-Learnability, Stability and Uniform Convergence</a></p>
<p>13 0.039718285 <a title="20-tfidf-13" href="./jmlr-2010-Classification_Methods_with_Reject_Option_Based_on_Convex_Risk_Minimization.html">21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</a></p>
<p>14 0.039569587 <a title="20-tfidf-14" href="./jmlr-2010-Mean_Field_Variational_Approximation_for_Continuous-Time_Bayesian_Networks.html">75 jmlr-2010-Mean Field Variational Approximation for Continuous-Time Bayesian Networks</a></p>
<p>15 0.038403723 <a title="20-tfidf-15" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>16 0.037983157 <a title="20-tfidf-16" href="./jmlr-2010-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">79 jmlr-2010-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>17 0.037820246 <a title="20-tfidf-17" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>18 0.034328829 <a title="20-tfidf-18" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>19 0.034327436 <a title="20-tfidf-19" href="./jmlr-2010-An_Exponential_Model_for_Infinite_Rankings.html">10 jmlr-2010-An Exponential Model for Infinite Rankings</a></p>
<p>20 0.034110334 <a title="20-tfidf-20" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.172), (1, -0.048), (2, 0.0), (3, -0.016), (4, -0.07), (5, -0.043), (6, -0.028), (7, 0.215), (8, 0.049), (9, 0.164), (10, -0.109), (11, 0.023), (12, 0.01), (13, 0.069), (14, 0.06), (15, -0.058), (16, -0.057), (17, -0.128), (18, 0.172), (19, -0.003), (20, -0.042), (21, -0.026), (22, -0.134), (23, 0.025), (24, -0.037), (25, -0.078), (26, 0.093), (27, 0.143), (28, 0.05), (29, -0.02), (30, 0.037), (31, 0.156), (32, -0.091), (33, 0.261), (34, -0.131), (35, -0.003), (36, -0.043), (37, -0.05), (38, -0.157), (39, -0.045), (40, 0.005), (41, -0.048), (42, -0.104), (43, 0.17), (44, -0.067), (45, 0.008), (46, -0.079), (47, 0.168), (48, 0.033), (49, 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94467306 <a title="20-lsi-1" href="./jmlr-2010-Chromatic_PAC-Bayes_Bounds_for_Non-IID_Data%3A_Applications_to_Ranking_and_Stationary_%CE%B2-Mixing_Processes.html">20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</a></p>
<p>Author: Liva Ralaivola, Marie Szafranski, Guillaume Stempfel</p><p>Abstract: PAC-Bayes bounds are among the most accurate generalization bounds for classiﬁers learned from independently and identically distributed (IID) data, and it is particularly so for margin classiﬁers: there have been recent contributions showing how practical these bounds can be either to perform model selection (Ambroladze et al., 2007) or even to directly guide the learning of linear classiﬁers (Germain et al., 2009). However, there are many practical situations where the training data show some dependencies and where the traditional IID assumption does not hold. Stating generalization bounds for such frameworks is therefore of the utmost interest, both from theoretical and practical standpoints. In this work, we propose the ﬁrst—to the best of our knowledge—PAC-Bayes generalization bounds for classiﬁers trained on data exhibiting interdependencies. The approach undertaken to establish our results is based on the decomposition of a so-called dependency graph that encodes the dependencies within the data, in sets of independent data, thanks to graph fractional covers. Our bounds are very general, since being able to ﬁnd an upper bound on the fractional chromatic number of the dependency graph is sufﬁcient to get new PAC-Bayes bounds for speciﬁc settings. We show how our results can be used to derive bounds for ranking statistics (such as AUC) and classiﬁers trained on data distributed according to a stationary β-mixing process. In the way, we show how our approach seamlessly allows us to deal with U-processes. As a side note, we also provide a PAC-Bayes generalization bound for classiﬁers learned on data from stationary ϕ-mixing distributions. Keywords: PAC-Bayes bounds, non IID data, ranking, U-statistics, mixing processes c 2010 Liva Ralaivola, Marie Szafranski and Guillaume Stempfel. R ALAIVOLA , S ZAFRANSKI AND S TEMPFEL</p><p>2 0.64768475 <a title="20-lsi-2" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>Author: Yevgeny Seldin, Naftali Tishby</p><p>Abstract: We derive PAC-Bayesian generalization bounds for supervised and unsupervised learning models based on clustering, such as co-clustering, matrix tri-factorization, graphical models, graph clustering, and pairwise clustering.1 We begin with the analysis of co-clustering, which is a widely used approach to the analysis of data matrices. We distinguish among two tasks in matrix data analysis: discriminative prediction of the missing entries in data matrices and estimation of the joint probability distribution of row and column variables in co-occurrence matrices. We derive PAC-Bayesian generalization bounds for the expected out-of-sample performance of co-clustering-based solutions for these two tasks. The analysis yields regularization terms that were absent in the previous formulations of co-clustering. The bounds suggest that the expected performance of co-clustering is governed by a trade-off between its empirical performance and the mutual information preserved by the cluster variables on row and column IDs. We derive an iterative projection algorithm for ﬁnding a local optimum of this trade-off for discriminative prediction tasks. This algorithm achieved stateof-the-art performance in the MovieLens collaborative ﬁltering task. Our co-clustering model can also be seen as matrix tri-factorization and the results provide generalization bounds, regularization terms, and new algorithms for this form of matrix factorization. The analysis of co-clustering is extended to tree-shaped graphical models, which can be used to analyze high dimensional tensors. According to the bounds, the generalization abilities of treeshaped graphical models depend on a trade-off between their empirical data ﬁt and the mutual information that is propagated up the tree levels. We also formulate weighted graph clustering as a prediction problem: given a subset of edge weights we analyze the ability of graph clustering to predict the remaining edge weights. The analysis of co-clustering easily</p><p>3 0.36857453 <a title="20-lsi-3" href="./jmlr-2010-Stability_Bounds_for_Stationary_%CF%86-mixing_and_%CE%B2-mixing_Processes.html">106 jmlr-2010-Stability Bounds for Stationary φ-mixing and β-mixing Processes</a></p>
<p>Author: Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, independently of any algorithm. In contrast, the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to speciﬁc learning algorithms by exploiting their particular properties. However, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed. In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence. This paper studies the scenario where the observations are drawn from a stationary ϕ-mixing or β-mixing sequence, a widely adopted assumption in the study of non-i.i.d. processes that implies a dependence between observations weakening over time. We prove novel and distinct stability-based generalization bounds for stationary ϕ-mixing and β-mixing sequences. These bounds strictly generalize the bounds given in the i.i.d. case and apply to all stable learning algorithms, thereby extending the use of stability-bounds to non-i.i.d. scenarios. We also illustrate the application of our ϕ-mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression, Kernel Ridge Regression, and Support Vector Machines, and many other kernel regularization-based and relative entropy-based regularization algorithms. These novel bounds can thus be viewed as the ﬁrst theoretical basis for the use of these algorithms in non-i.i.d. scenarios. Keywords: learning in non-i.i.d. scenarios, weakly dependent observations, mixing distributions, algorithmic stability, generalization bounds, learning theory</p><p>4 0.30178639 <a title="20-lsi-4" href="./jmlr-2010-Approximate_Riemannian_Conjugate_Gradient_Learning_for_Fixed-Form_Variational_Bayes.html">14 jmlr-2010-Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes</a></p>
<p>Author: Antti Honkela, Tapani Raiko, Mikael Kuusela, Matti Tornio, Juha Karhunen</p><p>Abstract: Variational Bayesian (VB) methods are typically only applied to models in the conjugate-exponential family using the variational Bayesian expectation maximisation (VB EM) algorithm or one of its variants. In this paper we present an efﬁcient algorithm for applying VB to more general models. The method is based on specifying the functional form of the approximation, such as multivariate Gaussian. The parameters of the approximation are optimised using a conjugate gradient algorithm that utilises the Riemannian geometry of the space of the approximations. This leads to a very efﬁcient algorithm for suitably structured approximations. It is shown empirically that the proposed method is comparable or superior in efﬁciency to the VB EM in a case where both are applicable. We also apply the algorithm to learning a nonlinear state-space model and a nonlinear factor analysis model for which the VB EM is not applicable. For these models, the proposed algorithm outperforms alternative gradient-based methods by a signiﬁcant margin. Keywords: variational inference, approximate Riemannian conjugate gradient, ﬁxed-form approximation, Gaussian approximation</p><p>5 0.29977575 <a title="20-lsi-5" href="./jmlr-2010-Posterior_Regularization_for_Structured_Latent_Variable_Models.html">91 jmlr-2010-Posterior Regularization for Structured Latent Variable Models</a></p>
<p>Author: Kuzman Ganchev, João Graça, Jennifer Gillenwater, Ben Taskar</p><p>Abstract: We present posterior regularization, a probabilistic framework for structured, weakly supervised learning. Our framework efﬁciently incorporates indirect supervision via constraints on posterior distributions of probabilistic models with latent variables. Posterior regularization separates model complexity from the complexity of structural constraints it is desired to satisfy. By directly imposing decomposable regularization on the posterior moments of latent variables during learning, we retain the computational efﬁciency of the unconstrained model while ensuring desired constraints hold in expectation. We present an efﬁcient algorithm for learning with posterior regularization and illustrate its versatility on a diverse set of structural constraints such as bijectivity, symmetry and group sparsity in several large scale experiments, including multi-view learning, cross-lingual dependency grammar induction, unsupervised part-of-speech induction, and bitext word alignment.1 Keywords: posterior regularization framework, unsupervised learning, latent variables models, prior knowledge, natural language processing</p><p>6 0.2876063 <a title="20-lsi-6" href="./jmlr-2010-On_Finding_Predictors_for_Arbitrary_Families_of_Processes.html">81 jmlr-2010-On Finding Predictors for Arbitrary Families of Processes</a></p>
<p>7 0.28231406 <a title="20-lsi-7" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>8 0.27779374 <a title="20-lsi-8" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>9 0.24992074 <a title="20-lsi-9" href="./jmlr-2010-Maximum_Likelihood_in_Cost-Sensitive_Learning%3A_Model_Specification%2C_Approximations%2C_and_Upper_Bounds.html">73 jmlr-2010-Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds</a></p>
<p>10 0.2428712 <a title="20-lsi-10" href="./jmlr-2010-Mean_Field_Variational_Approximation_for_Continuous-Time_Bayesian_Networks.html">75 jmlr-2010-Mean Field Variational Approximation for Continuous-Time Bayesian Networks</a></p>
<p>11 0.23917589 <a title="20-lsi-11" href="./jmlr-2010-Learning_Gradients%3A_Predictive_Models_that_Infer_Geometry_and_Statistical_Dependence.html">62 jmlr-2010-Learning Gradients: Predictive Models that Infer Geometry and Statistical Dependence</a></p>
<p>12 0.23242027 <a title="20-lsi-12" href="./jmlr-2010-Classification_Methods_with_Reject_Option_Based_on_Convex_Risk_Minimization.html">21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</a></p>
<p>13 0.22136751 <a title="20-lsi-13" href="./jmlr-2010-Learnability%2C_Stability_and_Uniform_Convergence.html">60 jmlr-2010-Learnability, Stability and Uniform Convergence</a></p>
<p>14 0.21969159 <a title="20-lsi-14" href="./jmlr-2010-Efficient_Algorithms_for_Conditional_Independence_Inference.html">32 jmlr-2010-Efficient Algorithms for Conditional Independence Inference</a></p>
<p>15 0.21939304 <a title="20-lsi-15" href="./jmlr-2010-Information_Theoretic_Measures_for_Clusterings_Comparison%3A_Variants%2C_Properties%2C_Normalization_and_Correction_for_Chance.html">55 jmlr-2010-Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance</a></p>
<p>16 0.19785105 <a title="20-lsi-16" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>17 0.19641249 <a title="20-lsi-17" href="./jmlr-2010-Consistent_Nonparametric_Tests_of_Independence.html">27 jmlr-2010-Consistent Nonparametric Tests of Independence</a></p>
<p>18 0.19178939 <a title="20-lsi-18" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>19 0.18469143 <a title="20-lsi-19" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>20 0.18415523 <a title="20-lsi-20" href="./jmlr-2010-Asymptotic_Equivalence_of_Bayes_Cross_Validation_and_Widely_Applicable_Information_Criterion_in_Singular_Learning_Theory.html">16 jmlr-2010-Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.012), (3, 0.052), (4, 0.018), (8, 0.025), (15, 0.011), (21, 0.015), (32, 0.055), (36, 0.038), (37, 0.048), (49, 0.465), (73, 0.013), (75, 0.091), (85, 0.041), (96, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.65755832 <a title="20-lda-1" href="./jmlr-2010-Chromatic_PAC-Bayes_Bounds_for_Non-IID_Data%3A_Applications_to_Ranking_and_Stationary_%CE%B2-Mixing_Processes.html">20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</a></p>
<p>Author: Liva Ralaivola, Marie Szafranski, Guillaume Stempfel</p><p>Abstract: PAC-Bayes bounds are among the most accurate generalization bounds for classiﬁers learned from independently and identically distributed (IID) data, and it is particularly so for margin classiﬁers: there have been recent contributions showing how practical these bounds can be either to perform model selection (Ambroladze et al., 2007) or even to directly guide the learning of linear classiﬁers (Germain et al., 2009). However, there are many practical situations where the training data show some dependencies and where the traditional IID assumption does not hold. Stating generalization bounds for such frameworks is therefore of the utmost interest, both from theoretical and practical standpoints. In this work, we propose the ﬁrst—to the best of our knowledge—PAC-Bayes generalization bounds for classiﬁers trained on data exhibiting interdependencies. The approach undertaken to establish our results is based on the decomposition of a so-called dependency graph that encodes the dependencies within the data, in sets of independent data, thanks to graph fractional covers. Our bounds are very general, since being able to ﬁnd an upper bound on the fractional chromatic number of the dependency graph is sufﬁcient to get new PAC-Bayes bounds for speciﬁc settings. We show how our results can be used to derive bounds for ranking statistics (such as AUC) and classiﬁers trained on data distributed according to a stationary β-mixing process. In the way, we show how our approach seamlessly allows us to deal with U-processes. As a side note, we also provide a PAC-Bayes generalization bound for classiﬁers learned on data from stationary ϕ-mixing distributions. Keywords: PAC-Bayes bounds, non IID data, ranking, U-statistics, mixing processes c 2010 Liva Ralaivola, Marie Szafranski and Guillaume Stempfel. R ALAIVOLA , S ZAFRANSKI AND S TEMPFEL</p><p>2 0.27433687 <a title="20-lda-2" href="./jmlr-2010-Stability_Bounds_for_Stationary_%CF%86-mixing_and_%CE%B2-mixing_Processes.html">106 jmlr-2010-Stability Bounds for Stationary φ-mixing and β-mixing Processes</a></p>
<p>Author: Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, independently of any algorithm. In contrast, the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to speciﬁc learning algorithms by exploiting their particular properties. However, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed. In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence. This paper studies the scenario where the observations are drawn from a stationary ϕ-mixing or β-mixing sequence, a widely adopted assumption in the study of non-i.i.d. processes that implies a dependence between observations weakening over time. We prove novel and distinct stability-based generalization bounds for stationary ϕ-mixing and β-mixing sequences. These bounds strictly generalize the bounds given in the i.i.d. case and apply to all stable learning algorithms, thereby extending the use of stability-bounds to non-i.i.d. scenarios. We also illustrate the application of our ϕ-mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression, Kernel Ridge Regression, and Support Vector Machines, and many other kernel regularization-based and relative entropy-based regularization algorithms. These novel bounds can thus be viewed as the ﬁrst theoretical basis for the use of these algorithms in non-i.i.d. scenarios. Keywords: learning in non-i.i.d. scenarios, weakly dependent observations, mixing distributions, algorithmic stability, generalization bounds, learning theory</p><p>3 0.27411646 <a title="20-lda-3" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>Author: Ming Yuan</p><p>Abstract: This paper considers the problem of estimating a high dimensional inverse covariance matrix that can be well approximated by “sparse” matrices. Taking advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix, we propose an estimating procedure that can effectively exploit such “sparsity”. The proposed method can be computed using linear programming and therefore has the potential to be used in very high dimensional problems. Oracle inequalities are established for the estimation error in terms of several operator norms, showing that the method is adaptive to different types of sparsity of the problem. Keywords: covariance selection, Dantzig selector, Gaussian graphical model, inverse covariance matrix, Lasso, linear programming, oracle inequality, sparsity</p><p>4 0.2690399 <a title="20-lda-4" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>Author: Yevgeny Seldin, Naftali Tishby</p><p>Abstract: We derive PAC-Bayesian generalization bounds for supervised and unsupervised learning models based on clustering, such as co-clustering, matrix tri-factorization, graphical models, graph clustering, and pairwise clustering.1 We begin with the analysis of co-clustering, which is a widely used approach to the analysis of data matrices. We distinguish among two tasks in matrix data analysis: discriminative prediction of the missing entries in data matrices and estimation of the joint probability distribution of row and column variables in co-occurrence matrices. We derive PAC-Bayesian generalization bounds for the expected out-of-sample performance of co-clustering-based solutions for these two tasks. The analysis yields regularization terms that were absent in the previous formulations of co-clustering. The bounds suggest that the expected performance of co-clustering is governed by a trade-off between its empirical performance and the mutual information preserved by the cluster variables on row and column IDs. We derive an iterative projection algorithm for ﬁnding a local optimum of this trade-off for discriminative prediction tasks. This algorithm achieved stateof-the-art performance in the MovieLens collaborative ﬁltering task. Our co-clustering model can also be seen as matrix tri-factorization and the results provide generalization bounds, regularization terms, and new algorithms for this form of matrix factorization. The analysis of co-clustering is extended to tree-shaped graphical models, which can be used to analyze high dimensional tensors. According to the bounds, the generalization abilities of treeshaped graphical models depend on a trade-off between their empirical data ﬁt and the mutual information that is propagated up the tree levels. We also formulate weighted graph clustering as a prediction problem: given a subset of edge weights we analyze the ability of graph clustering to predict the remaining edge weights. The analysis of co-clustering easily</p><p>5 0.26508591 <a title="20-lda-5" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>Author: Shiliang Sun, John Shawe-Taylor</p><p>Abstract: In this paper, we propose a general framework for sparse semi-supervised learning, which concerns using a small portion of unlabeled data and a few labeled data to represent target functions and thus has the merit of accelerating function evaluations when predicting the output of a new example. This framework makes use of Fenchel-Legendre conjugates to rewrite a convex insensitive loss involving a regularization with unlabeled data, and is applicable to a family of semi-supervised learning methods such as multi-view co-regularized least squares and single-view Laplacian support vector machines (SVMs). As an instantiation of this framework, we propose sparse multi-view SVMs which use a squared ε-insensitive loss. The resultant optimization is an inf-sup problem and the optimal solutions have arguably saddle-point properties. We present a globally optimal iterative algorithm to optimize the problem. We give the margin bound on the generalization error of the sparse multi-view SVMs, and derive the empirical Rademacher complexity for the induced function class. Experiments on artiﬁcial and real-world data show their effectiveness. We further give a sequential training approach to show their possibility and potential for uses in large-scale problems and provide encouraging experimental results indicating the efﬁcacy of the margin bound and empirical Rademacher complexity on characterizing the roles of unlabeled data for semi-supervised learning. Keywords: semi-supervised learning, Fenchel-Legendre conjugate, representer theorem, multiview regularization, support vector machine, statistical learning theory</p><p>6 0.26324591 <a title="20-lda-6" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>7 0.26314265 <a title="20-lda-7" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>8 0.26277903 <a title="20-lda-8" href="./jmlr-2010-Spectral_Regularization_Algorithms_for_Learning_Large_Incomplete_Matrices.html">105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</a></p>
<p>9 0.26132965 <a title="20-lda-9" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>10 0.26085582 <a title="20-lda-10" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>11 0.26045206 <a title="20-lda-11" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>12 0.26030469 <a title="20-lda-12" href="./jmlr-2010-Learnability%2C_Stability_and_Uniform_Convergence.html">60 jmlr-2010-Learnability, Stability and Uniform Convergence</a></p>
<p>13 0.25984803 <a title="20-lda-13" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>14 0.25858 <a title="20-lda-14" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>15 0.25790137 <a title="20-lda-15" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>16 0.2572844 <a title="20-lda-16" href="./jmlr-2010-On_Learning_with_Integral_Operators.html">82 jmlr-2010-On Learning with Integral Operators</a></p>
<p>17 0.25700888 <a title="20-lda-17" href="./jmlr-2010-Message-passing_for_Graph-structured_Linear_Programs%3A_Proximal_Methods_and_Rounding_Schemes.html">76 jmlr-2010-Message-passing for Graph-structured Linear Programs: Proximal Methods and Rounding Schemes</a></p>
<p>18 0.25689778 <a title="20-lda-18" href="./jmlr-2010-A_Convergent_Online_Single_Time_Scale_Actor_Critic_Algorithm.html">2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</a></p>
<p>19 0.25573391 <a title="20-lda-19" href="./jmlr-2010-Asymptotic_Equivalence_of_Bayes_Cross_Validation_and_Widely_Applicable_Information_Criterion_in_Singular_Learning_Theory.html">16 jmlr-2010-Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory</a></p>
<p>20 0.2550846 <a title="20-lda-20" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
