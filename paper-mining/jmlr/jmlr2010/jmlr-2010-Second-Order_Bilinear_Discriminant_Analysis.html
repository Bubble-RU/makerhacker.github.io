<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>101 jmlr-2010-Second-Order Bilinear Discriminant Analysis</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-101" href="#">jmlr2010-101</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>101 jmlr-2010-Second-Order Bilinear Discriminant Analysis</h1>
<br/><p>Source: <a title="jmlr-2010-101-pdf" href="http://jmlr.org/papers/volume11/christoforou10a/christoforou10a.pdf">pdf</a></p><p>Author: Christoforos Christoforou, Robert Haralick, Paul Sajda, Lucas C. Parra</p><p>Abstract: Traditional analysis methods for single-trial classiﬁcation of electro-encephalography (EEG) focus on two types of paradigms: phase-locked methods, in which the amplitude of the signal is used as the feature for classiﬁcation, that is, event related potentials; and second-order methods, in which the feature of interest is the power of the signal, that is, event related (de)synchronization. The process of deciding which paradigm to use is ad hoc and is driven by assumptions regarding the underlying neural generators. Here we propose a method that provides an uniﬁed framework for the analysis of EEG, combining ﬁrst and second-order spatial and temporal features based on a bilinear model. Evaluation of the proposed method on simulated data shows that the technique outperforms state-of-the art techniques for single-trial classiﬁcation for a broad range of signal-to-noise ratios. Evaluations on human EEG—including one benchmark data set from the Brain Computer Interface (BCI) competition—show statistically signiﬁcant gains in classiﬁcation accuracy, with a reduction in overall classiﬁcation error from 26%-28% to 19%. Keywords: regularization, classiﬁcation, bilinear decomposition, neural signals, brain computer interface</p><p>Reference: <a title="jmlr-2010-101-reference" href="../jmlr2010_reference/jmlr-2010-Second-Order_Bilinear_Discriminant_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Here we propose a method that provides an uniﬁed framework for the analysis of EEG, combining ﬁrst and second-order spatial and temporal features based on a bilinear model. [sent-13, score-0.615]
</p><p>2 Keywords: regularization, classiﬁcation, bilinear decomposition, neural signals, brain computer interface  1. [sent-16, score-0.246]
</p><p>3 Introduction The work presented in this paper is motivated by the analysis of functional brain imaging signals recorded via electroencephalography (EEG). [sent-17, score-0.114]
</p><p>4 Many of these classiﬁcation techniques were originally developed in the context of Brain Computer Interfaces (BCI) but are now more widely used to interpret activity associated with neural processing. [sent-24, score-0.141]
</p><p>5 , 2002, 2003) the aim is to decode brain activity on a single-trial basis in order to provide a direct control pathway between a user’s intentions and a computer. [sent-28, score-0.212]
</p><p>6 By extracting activity that differs maximally between two experimental conditions, the typically low signal-to-noise ratio of EEG can be overcome. [sent-35, score-0.141]
</p><p>7 Typical in this case is to average the measured potentials following stimulus presentation, thereby canceling uncorrelated noise that is not reproducible from one trial to the next. [sent-44, score-0.134]
</p><p>8 This averaged activity, called an event related potential (ERP), captures activity that is time-locked to the stimulus presentation but cancels induced oscillatory activity that is not locked in phase to the timing of the stimulus. [sent-45, score-0.483]
</p><p>9 Alternatively, many studies compute the oscillatory activity in speciﬁc frequency bands by ﬁltering and squaring the signal prior to averaging. [sent-46, score-0.413]
</p><p>10 Induced changes in oscillatory activity are termed event related synchronization or desynchronization (ERS/ERD) Pfurtscheller and da Silva (1999). [sent-47, score-0.254]
</p><p>11 First order methods include temporal ﬁltering and thresholding (Birbaumer et al. [sent-49, score-0.157]
</p><p>12 , 2007) and the well known common spatial patterns method (CSP) (Ramoser et al. [sent-56, score-0.283]
</p><p>13 , 2005), and common sparse spectral spatial patterns (CSSSP) (Dornhege et al. [sent-58, score-0.283]
</p><p>14 Instead, it would be desirable for the analysis method to extract the relevant neurophysiological activity de novo with minimal prior expectations. [sent-62, score-0.141]
</p><p>15 Through a bilinear formulation, the method can simultaneously identify spatial linear components as well as temporal modulation of activity. [sent-64, score-0.648]
</p><p>16 Further, through the bilinear formulation, the method exploits the spatio-temporal nature of the EEG signals and provides a reduced parametrization of the high dimensional data space. [sent-66, score-0.218]
</p><p>17 We show that a broad set of state-of-the-art EEG analysis methods can be characterized as special cases under this bilinear framework. [sent-67, score-0.175]
</p><p>18 We then present the bilinear model, discuss interpretation in the context of EEG, and establish a link to current analysis methods. [sent-73, score-0.175]
</p><p>19 The goal will be to use the N examples to optimize these parameters such that the discriminant function takes on positive values for examples with yn = +1 and negative values for yn = −1. [sent-85, score-0.27]
</p><p>20 3 Interpretation and Rationale of the Model The discriminant criterion deﬁned in (1) is the sum of a linear and a quadratic term, each combining bilinear components of the EEG signal. [sent-90, score-0.298]
</p><p>21 By re-writing the term as: Trace(U⊤ XV) = Trace(VU⊤ X) = Trace(W⊤ X) , where we deﬁned, W = UV⊤ , it is easy to see that the bilinear projection is a linear combination of elements of X with a rank-R constraint on W. [sent-94, score-0.175]
</p><p>22 In particular, the polarity of the signal (positive evoked response versus negative evoked response) will contribute to discrimination if it is consistent across trials. [sent-96, score-0.141]
</p><p>23 This bilinear projection reduces the number of model parameters of W from D × T dimensions to R × (D + T ) which is a signiﬁcant dimensionality reduction that alleviates the problem of over-ﬁtting in parameters estimation given the small training set size. [sent-98, score-0.175]
</p><p>24 The second term of Equation (1) is the power of spatially and temporally weighted signals and thus captures the second-order statistics of the signal. [sent-101, score-0.113]
</p><p>25 7), then B deﬁnes a temporal ﬁlter on the signal and the model captures powers of the ﬁltered signal. [sent-106, score-0.244]
</p><p>26 Further, by allowing B to be learned from the data, we may be able to identify new frequency bands that have so far not been identiﬁed in novel experimental paradigms. [sent-107, score-0.108]
</p><p>27 The spatial weights A together with the Trace operation ensure that the power is measured, not in individual electrodes, but in some component space that may reﬂect activity distributed across several electrodes. [sent-108, score-0.49]
</p><p>28 The diagonal matrix Λ partitions the K spatial components (i. [sent-109, score-0.316]
</p><p>29 , K columns of A) into those that contribute power positively and those that contribute power negatively to the total sum. [sent-111, score-0.102]
</p><p>30 Since each column of A measures the power from different sources, then by multiplying the expression with Λ we capture the difference in power between different spatial components. [sent-112, score-0.351]
</p><p>31 It is known that imagining a movement of the left hand reduces oscillatory activity over the motor cortex of the right hemisphere, while an imagined right-hand movement reduces oscillations over the left motor cortex. [sent-114, score-0.527]
</p><p>32 Each of these cortical areas will be captured by a different spatial distribution in the EEG. [sent-115, score-0.283]
</p><p>33 If we limit the columns of A to two, then these columns may capture the power of oscillatory activity over the right and left motor cortex respectively. [sent-116, score-0.464]
</p><p>34 C = 1 indicates that the discriminant activity is dominated by the ﬁrst-order features; C = 0 indicates that the activity is dominated by second-order features, and any value in between denotes the importance of one component versus the other. [sent-120, score-0.404]
</p><p>35 , ur and ar map the sensor signal to a current-source space, vr are temporal weight on a source signal and br can be arranged to represent a temporal ﬁlter) it becomes intuitive for the experimenter to incorporate prior knowledge of an experimental setup in the model. [sent-124, score-0.698]
</p><p>36 If the signal of interest is known to be in a speciﬁc frequency band, one can ﬁx matrix B to capture only the desired frequency band. [sent-125, score-0.267]
</p><p>37 For example, B can be ﬁxed to a Toeplitz matrix with coefﬁcients corresponding to an 8Hz-12Hz bandpass ﬁlter, then the second-order term is able to extract power in the alpha-band which is known to be modulated during motor related tasks. [sent-126, score-0.147]
</p><p>38 In such a scenario the experimenter can ﬁx the temporal proﬁle parameter V to emphasize time samples around the expected location of the peak activity and optimize over the rest of the parameters. [sent-128, score-0.33]
</p><p>39 fMRI has high spatial resolution and can provide locations within the brain that may be known to participate in the processing during a particular experimental paradigm. [sent-130, score-0.354]
</p><p>40 This location information can be incorporated into the present model by ﬁxing the spatial parameters ur and a to reﬂect a localized source (often approximated as a current dipole). [sent-131, score-0.348]
</p><p>41 The remaining temporal parameters of the model can then be optimized. [sent-132, score-0.157]
</p><p>42 The following list identiﬁes some of the algorithms and how they relate to the model used in the SOBDA framework: • Set C = 1, R = 1 and choose temporal component v to select a time window of interest (i. [sent-135, score-0.216]
</p><p>43 (2002, 2005) • Set C = 1 and select some R > 1 and choose the component vectors vr to select multiple time windows of interest as in 1. [sent-140, score-0.122]
</p><p>44 Learn for each temporal window the corresponding spatial vector ur from examples separately and then combine these components by learning a linear combination of the elements. [sent-141, score-0.565]
</p><p>45 (2008) • Set C = 1, R = D while constraining U to be a diagonal matrix and select, separately for each channel, the time window vr which is most discriminative. [sent-144, score-0.117]
</p><p>46 Then train the diagonal terms of 669  C HRISTOFOROU , H ARALICK , S AJDA AND PARRA  U resulting in a latency dependent spatial ﬁlter (Luo and Sajda, 2006a). [sent-145, score-0.283]
</p><p>47 Alternatively, in the ﬁrst step, use feature selection to ﬁnd the right set of time windows vr simultaneously for all channels (Luo and Sajda, 2006b). [sent-146, score-0.125]
</p><p>48 • Set C = 1,R = 1 and learn the spatial and temporal components u, v simultaneously. [sent-147, score-0.473]
</p><p>49 This reduces to the rank-one bilinear discriminant as in Dyrholm and Parra (2006) • Select C = 1 and some R > 1 and learn all columns of the spatial and temporal projection matrix U and V simultaneously. [sent-148, score-0.739]
</p><p>50 • Set C = 0, K = 2 and ﬁx B to a Toeplitz structure encoding a speciﬁc frequency band and set the diagonal of Λ to be [1 − 1]. [sent-151, score-0.197]
</p><p>51 With this deﬁnition, the discriminant criterion given by the log-odds ratio of the posterior class probability P(y = +1|X) = f (X; θ) , log P(y = −1|X) is simply the discriminant function which we chose to deﬁne in (1) as a sum of linear and quadratic terms. [sent-164, score-0.18]
</p><p>52 In such a case we can re-write Equations (3) and (4) as ∂L(θ) ∂ar  = 2(1 −C) ∑ yn πn Xn FH DDH FX⊤ ar . [sent-170, score-0.144]
</p><p>53 Spatial and temporal smoothness is typically a valid assumption in EEG (Penny et al. [sent-183, score-0.187]
</p><p>54 This is done through choice of covariance function: Let r be a spatial or temporal measure in context of X. [sent-188, score-0.44]
</p><p>55 For instance r is a measure of spatial distance between data acquisition sensors, or a measure of time difference between two samples in the data. [sent-189, score-0.283]
</p><p>56 Similarly, the Gaussian prior can be used on the columns of the temporal matrix V (i. [sent-195, score-0.191]
</p><p>57 Following Rasmussen and Williams (2005) the shape parameter was chosen to be ν = 100 for the spatial components and ν = 2. [sent-204, score-0.316]
</p><p>58 However, for the spectral parameter we found it important to initialize to a frequency band that was expected to carry useful information, for example, 8Hz-30Hz. [sent-220, score-0.197]
</p><p>59 The simulation aims to quantify the algorithm’s performance on a broad spectrum of conditions and various noise levels, as well as to compare the extracted spatial, temporal and frequency components with ground truth. [sent-225, score-0.359]
</p><p>60 We used two spatial patterns (SP) and employ a logistic regression classiﬁer on the resulting SP. [sent-239, score-0.314]
</p><p>61 Since CSP,MLR and sMLR require the data to be band-pass ﬁltered to the frequency of interest, data sets where ﬁltered in the range of 8Hz-30Hz for these two methods. [sent-243, score-0.108]
</p><p>62 For the spatial parameter A we set K = 2 allowing for two spatial patterns, while we enforce a Toeplitz structure on B. [sent-245, score-0.566]
</p><p>63 We used 3 dipoles at three different locations, with one dipole used to generate evoked response activity, one dipole to generate induced oscillatory activity, and one dipole to generate unrelated noise/interference. [sent-257, score-0.566]
</p><p>64 We used a half-sinusoid lasting 125ms with the peak positioned at 300ms after trial-onset and a trial-to-trial Gaussian temporal jitter with standard deviation of 10ms. [sent-259, score-0.189]
</p><p>65 The second dipole’s component simulates ERS/ERD in the frequency band of 8Hz to 30Hz. [sent-260, score-0.229]
</p><p>66 A variable signal in this frequency band was generated by bandpass ﬁltering an uncorrelated Gaussian process. [sent-261, score-0.293]
</p><p>67 The third dipole was used to generate noise in the source space representing brain activity that is not related to the evoked/induced activity. [sent-262, score-0.348]
</p><p>68 The SNR of the ERP component is in the range of -33dB to -13dB, and in the range of -22dB to -10dB for the oscillatory component. [sent-271, score-0.145]
</p><p>69 As a decomposition method, SOBDA extracts spatial, temporal and frequency components. [sent-317, score-0.265]
</p><p>70 The ﬁrst row shows the extracted temporal component U and the frequency component d. [sent-320, score-0.39]
</p><p>71 2 We can see that the method extracted a temporal component with a peak at 300ms which is exactly the signal used in the simulation data design. [sent-321, score-0.333]
</p><p>72 Similarly, the frequency band extracted shows a higher amplitude in the range of 8Hz-30Hz which is the band used to generate the oscillatory component. [sent-322, score-0.508]
</p><p>73 The spatial components extracted and the corresponding dipole used in the model generation are shown in rows two and three in the ﬁgure. [sent-323, score-0.513]
</p><p>74 The last column of the ﬁgure captures the second-order oscillatory component and the dipole of the rank one noise. [sent-325, score-0.317]
</p><p>75 Top row: Extracted temporal weight of linear term (left) and frequency weights of quadratic term (right). [sent-335, score-0.265]
</p><p>76 Speciﬁcally, we evaluated the SOBDA algorithm on a simulated data set using the process described above, but this time we initialize matrix B to a high-pass ﬁlter with cut of frequency at 1 Hz. [sent-343, score-0.149]
</p><p>77 Figure 3 shows the temporal and frequency component obtained from SOBDA. [sent-345, score-0.297]
</p><p>78 As it is evident from the ﬁgure, the resulting frequency component has higher weights for frequencies in the band 8Hz-30Hz, which is the band used to generate the power component in the simulated data. [sent-346, score-0.425]
</p><p>79 Thus the proposed method is able optimize the frequency band even in cases where we use a generic initialization of the matrix B. [sent-347, score-0.197]
</p><p>80 676  S ECOND -O RDER B ILINEAR D ISCRIMINANT A NALYSIS  First Order Temporal coefficient v  Magnitude of FFT coefficients 1. [sent-348, score-0.125]
</p><p>81 The Fourier coefﬁcients were initialized to a high-pass ﬁlter with cut off frequency at 1 Hz Left ﬁgure: Extracted temporal weight of linear term. [sent-361, score-0.265]
</p><p>82 Speciﬁcally, we are looking for one ERP components associated with the readiness potential that is, the slow increase in amplitude before an actual hand movement. [sent-376, score-0.115]
</p><p>83 In the case of the second-order term involving the parameter A we set K = 2 because we are interested in ﬁnding the modulation of oscillatory activity associated with the different movements of the movements of the hands. [sent-377, score-0.254]
</p><p>84 Hands and ﬁngers are represented in somato-sensory cortex covering different areas and will hence modulate activity in distinct spatial proﬁles. [sent-378, score-0.464]
</p><p>85 The temporal ﬁlter was selected based on prior knowledge of the relevant frequency band. [sent-381, score-0.265]
</p><p>86 We note that in all three cases the extracted components follow the general shape of the pre-motor or readiness potential (a. [sent-450, score-0.128]
</p><p>87 In addition, for two of the data sets, the frequency weightings suggest that alpha band activity also provides discriminant information for this task. [sent-454, score-0.428]
</p><p>88 This ﬁnding is consistent with the changes in the µ rhythm— that is, alpha-band activity localized over the motor cortex and associated with motor planning and execution. [sent-455, score-0.317]
</p><p>89 Speciﬁcally, in the current experimental paradigm, we are looking for one ERP components associated with the readiness potential, that is, the slow increase in amplitude before an actual hand movement. [sent-533, score-0.115]
</p><p>90 In the case of the second-order term involving the parameter A we set the K = 2 because we are interested in ﬁnding two components corresponding to the two different spatial proﬁles of the two classes. [sent-535, score-0.316]
</p><p>91 The parametrization 680  S ECOND -O RDER B ILINEAR D ISCRIMINANT A NALYSIS  Temporal coefficient V  Magnitute of FFT coefficients  Amplitute  0. [sent-551, score-0.125]
</p><p>92 A  2  1 0 −400  −200 Time (ms)  0 0  0  Temporal coefficient V  Magnitute of FFT coefficients  0. [sent-564, score-0.125]
</p><p>93 Left: Temporal weights of linear component (ﬁrst column) and and frequency weights of quadratic component (second column). [sent-569, score-0.172]
</p><p>94 Right: Spatial weights of linear component (third column) and two spatial weights for second-order spatial components (fourth and ﬁfth column). [sent-570, score-0.631]
</p><p>95 = C  The gradient with respect to vr , the rth column of V is: ∂{ f (Xn ; θ) + w0} ∂vr  ∂{Trace U⊤ Xn V} ∂vr R ∂{∑r′ =1 u⊤ Xn vr′ } r′ = C ∂vr ⊤ = Cur Xn . [sent-589, score-0.129]
</p><p>96 Classifying single trial EEG: Towards brain computer interu facing. [sent-608, score-0.122]
</p><p>97 The bci competition 2003: progress and perspectives in detection and discrimination of EEG single trials. [sent-648, score-0.106]
</p><p>98 Combined optimization u of spatial and temporal ﬁlters for improving brain-computer interfacing. [sent-657, score-0.44]
</p><p>99 Linear spatial integration for single-trial detection in encephalogra phy. [sent-745, score-0.283]
</p><p>100 Optimal spatial ﬁltering of single trial EEG u during imagined hand movement. [sent-804, score-0.379]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('eeg', 0.428), ('sobda', 0.306), ('spatial', 0.283), ('snr', 0.211), ('erp', 0.204), ('parra', 0.192), ('bilinear', 0.175), ('temporal', 0.157), ('activity', 0.141), ('csp', 0.136), ('dipole', 0.136), ('aralick', 0.113), ('blankertz', 0.113), ('hristoforou', 0.113), ('ilinear', 0.113), ('oscillatory', 0.113), ('rder', 0.113), ('frequency', 0.108), ('db', 0.105), ('bdca', 0.102), ('iscriminant', 0.097), ('mlr', 0.091), ('yn', 0.09), ('discriminant', 0.09), ('vr', 0.09), ('band', 0.089), ('ajda', 0.087), ('econd', 0.087), ('toeplitz', 0.087), ('tomioka', 0.079), ('dyrholm', 0.079), ('nalysis', 0.071), ('brain', 0.071), ('erd', 0.07), ('hz', 0.068), ('motor', 0.068), ('coefficient', 0.068), ('bci', 0.068), ('xn', 0.067), ('ur', 0.065), ('gerson', 0.061), ('extracted', 0.061), ('fh', 0.058), ('birbaumer', 0.057), ('coefficients', 0.057), ('fft', 0.057), ('ramoser', 0.057), ('ar', 0.054), ('lter', 0.052), ('stimulus', 0.052), ('signal', 0.051), ('fourier', 0.051), ('trial', 0.051), ('smlr', 0.048), ('amplitude', 0.048), ('trials', 0.047), ('br', 0.047), ('aihara', 0.045), ('bandpass', 0.045), ('curio', 0.045), ('evoked', 0.045), ('imagined', 0.045), ('magnitute', 0.045), ('pfurtscheller', 0.045), ('wolpaw', 0.045), ('philiastides', 0.044), ('sajda', 0.044), ('trace', 0.043), ('signals', 0.043), ('simulated', 0.041), ('cortex', 0.04), ('christoforou', 0.039), ('rth', 0.039), ('competition', 0.038), ('captures', 0.036), ('luo', 0.035), ('channels', 0.035), ('guration', 0.035), ('power', 0.034), ('columns', 0.034), ('amplitute', 0.034), ('componet', 0.034), ('dornhege', 0.034), ('haralick', 0.034), ('lemm', 0.034), ('readiness', 0.034), ('components', 0.033), ('component', 0.032), ('peak', 0.032), ('logistic', 0.031), ('potentials', 0.031), ('smoothness', 0.03), ('df', 0.029), ('window', 0.027), ('ms', 0.026), ('movement', 0.026), ('crossvalidation', 0.026), ('sensor', 0.026), ('neuroimage', 0.024), ('fmri', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="101-tfidf-1" href="./jmlr-2010-Second-Order_Bilinear_Discriminant_Analysis.html">101 jmlr-2010-Second-Order Bilinear Discriminant Analysis</a></p>
<p>Author: Christoforos Christoforou, Robert Haralick, Paul Sajda, Lucas C. Parra</p><p>Abstract: Traditional analysis methods for single-trial classiﬁcation of electro-encephalography (EEG) focus on two types of paradigms: phase-locked methods, in which the amplitude of the signal is used as the feature for classiﬁcation, that is, event related potentials; and second-order methods, in which the feature of interest is the power of the signal, that is, event related (de)synchronization. The process of deciding which paradigm to use is ad hoc and is driven by assumptions regarding the underlying neural generators. Here we propose a method that provides an uniﬁed framework for the analysis of EEG, combining ﬁrst and second-order spatial and temporal features based on a bilinear model. Evaluation of the proposed method on simulated data shows that the technique outperforms state-of-the art techniques for single-trial classiﬁcation for a broad range of signal-to-noise ratios. Evaluations on human EEG—including one benchmark data set from the Brain Computer Interface (BCI) competition—show statistically signiﬁcant gains in classiﬁcation accuracy, with a reduction in overall classiﬁcation error from 26%-28% to 19%. Keywords: regularization, classiﬁcation, bilinear decomposition, neural signals, brain computer interface</p><p>2 0.086133383 <a title="101-tfidf-2" href="./jmlr-2010-Maximum_Likelihood_in_Cost-Sensitive_Learning%3A_Model_Specification%2C_Approximations%2C_and_Upper_Bounds.html">73 jmlr-2010-Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds</a></p>
<p>Author: Jacek P. Dmochowski, Paul Sajda, Lucas C. Parra</p><p>Abstract: The presence of asymmetry in the misclassiﬁcation costs or class prevalences is a common occurrence in the pattern classiﬁcation domain. While much interest has been devoted to the study of cost-sensitive learning techniques, the relationship between cost-sensitive learning and the speciﬁcation of the model set in a parametric estimation framework remains somewhat unclear. To that end, we differentiate between the case of the model including the true posterior, and that in which the model is misspeciﬁed. In the former case, it is shown that thresholding the maximum likelihood (ML) estimate is an asymptotically optimal solution to the risk minimization problem. On the other hand, under model misspeciﬁcation, it is demonstrated that thresholded ML is suboptimal and that the risk-minimizing solution varies with the misclassiﬁcation cost ratio. Moreover, we analytically show that the negative weighted log likelihood (Elkan, 2001) is a tight, convex upper bound of the empirical loss. Coupled with empirical results on several real-world data sets, we argue that weighted ML is the preferred cost-sensitive technique. Keywords: empirical risk minimization, loss function, cost-sensitive learning, imbalanced data sets</p><p>3 0.056947071 <a title="101-tfidf-3" href="./jmlr-2010-Image_Denoising_with_Kernels_Based_on_Natural_Image_Relations.html">50 jmlr-2010-Image Denoising with Kernels Based on Natural Image Relations</a></p>
<p>Author: Valero Laparra, Juan Gutiérrez, Gustavo Camps-Valls, Jesús Malo</p><p>Abstract: A successful class of image denoising methods is based on Bayesian approaches working in wavelet representations. The performance of these methods improves when relations among the local frequency coefﬁcients are explicitly included. However, in these techniques, analytical estimates can be obtained only for particular combinations of analytical models of signal and noise, thus precluding its straightforward extension to deal with other arbitrary noise sources. In this paper, we propose an alternative non-explicit way to take into account the relations among natural image wavelet coefﬁcients for denoising: we use support vector regression (SVR) in the wavelet domain to enforce these relations in the estimated signal. Since relations among the coefﬁcients are speciﬁc to the signal, the regularization property of SVR is exploited to remove the noise, which does not share this feature. The speciﬁc signal relations are encoded in an anisotropic kernel obtained from mutual information measures computed on a representative image database. In the proposed scheme, training considers minimizing the Kullback-Leibler divergence (KLD) between the estimated and actual probability functions (or histograms) of signal and noise in order to enforce similarity up to the higher (computationally estimable) order. Due to its non-parametric nature, the method can eventually cope with different noise sources without the need of an explicit re-formulation, as it is strictly necessary under parametric Bayesian formalisms. Results under several noise levels and noise sources show that: (1) the proposed method outperforms conventional wavelet methods that assume coefﬁcient independence, (2) it is similar to state-of-the-art methods that do explicitly include these relations when the noise source is Gaussian, and (3) it gives better numerical and visual performance when more complex, realistic noise sources are considered. Therefore, the proposed machine learning approach can be seen as a mor</p><p>4 0.056109536 <a title="101-tfidf-4" href="./jmlr-2010-A_Fast_Hybrid_Algorithm_for_Large-Scalel1-Regularized_Logistic_Regression.html">3 jmlr-2010-A Fast Hybrid Algorithm for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>Author: Jianing Shi, Wotao Yin, Stanley Osher, Paul Sajda</p><p>Abstract: ℓ1 -regularized logistic regression, also known as sparse logistic regression, is widely used in machine learning, computer vision, data mining, bioinformatics and neural signal processing. The use of ℓ1 regularization attributes attractive properties to the classiﬁer, such as feature selection, robustness to noise, and as a result, classiﬁer generality in the context of supervised learning. When a sparse logistic regression problem has large-scale data in high dimensions, it is computationally expensive to minimize the non-differentiable ℓ1 -norm in the objective function. Motivated by recent work (Koh et al., 2007; Hale et al., 2008), we propose a novel hybrid algorithm based on combining two types of optimization iterations: one being very fast and memory friendly while the other being slower but more accurate. Called hybrid iterative shrinkage (HIS), the resulting algorithm is comprised of a ﬁxed point continuation phase and an interior point phase. The ﬁrst phase is based completely on memory efﬁcient operations such as matrix-vector multiplications, while the second phase is based on a truncated Newton’s method. Furthermore, we show that various optimization techniques, including line search and continuation, can signiﬁcantly accelerate convergence. The algorithm has global convergence at a geometric rate (a Q-linear rate in optimization terminology). We present a numerical comparison with several existing algorithms, including an analysis using benchmark data from the UCI machine learning repository, and show our algorithm is the most computationally efﬁcient without loss of accuracy. Keywords: logistic regression, ℓ1 regularization, ﬁxed point continuation, supervised learning, large scale c 2010 Jianing Shi, Wotao Yin, Stanley Osher and Paul Sajda. S HI , Y IN , O SHER AND S AJDA</p><p>5 0.040524159 <a title="101-tfidf-5" href="./jmlr-2010-A_Convergent_Online_Single_Time_Scale_Actor_Critic_Algorithm.html">2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</a></p>
<p>Author: Dotan Di Castro, Ron Meir</p><p>Abstract: Actor-Critic based approaches were among the ﬁrst to address reinforcement learning in a general setting. Recently, these algorithms have gained renewed interest due to their generality, good convergence properties, and possible biological relevance. In this paper, we introduce an online temporal difference based actor-critic algorithm which is proved to converge to a neighborhood of a local maximum of the average reward. Linear function approximation is used by the critic in order estimate the value function, and the temporal difference signal, which is passed from the critic to the actor. The main distinguishing feature of the present convergence proof is that both the actor and the critic operate on a similar time scale, while in most current convergence proofs they are required to have very different time scales in order to converge. Moreover, the same temporal difference signal is used to update the parameters of both the actor and the critic. A limitation of the proposed approach, compared to results available for two time scale convergence, is that convergence is guaranteed only to a neighborhood of an optimal value, rather to an optimal value itself. The single time scale and identical temporal difference signal used by the actor and the critic, may provide a step towards constructing more biologically realistic models of reinforcement learning in the brain. Keywords: actor critic, single time scale convergence, temporal difference</p><p>6 0.040407561 <a title="101-tfidf-6" href="./jmlr-2010-Regularized_Discriminant_Analysis%2C_Ridge_Regression_and_Beyond.html">98 jmlr-2010-Regularized Discriminant Analysis, Ridge Regression and Beyond</a></p>
<p>7 0.038267463 <a title="101-tfidf-7" href="./jmlr-2010-Matched_Gene_Selection_and_Committee_Classifier_for_Molecular_Classification_of_Heterogeneous_Diseases.html">71 jmlr-2010-Matched Gene Selection and Committee Classifier for Molecular Classification of Heterogeneous Diseases</a></p>
<p>8 0.038112298 <a title="101-tfidf-8" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>9 0.034450993 <a title="101-tfidf-9" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>10 0.031048547 <a title="101-tfidf-10" href="./jmlr-2010-Gaussian_Processes_for_Machine_Learning_%28GPML%29_Toolbox.html">41 jmlr-2010-Gaussian Processes for Machine Learning (GPML) Toolbox</a></p>
<p>11 0.030330135 <a title="101-tfidf-11" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>12 0.030267948 <a title="101-tfidf-12" href="./jmlr-2010-Estimation_of_a_Structural_Vector_Autoregression_Model_Using_Non-Gaussianity.html">36 jmlr-2010-Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity</a></p>
<p>13 0.029753778 <a title="101-tfidf-13" href="./jmlr-2010-Spectral_Regularization_Algorithms_for_Learning_Large_Incomplete_Matrices.html">105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</a></p>
<p>14 0.029463395 <a title="101-tfidf-14" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>15 0.028535502 <a title="101-tfidf-15" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>16 0.027630256 <a title="101-tfidf-16" href="./jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">48 jmlr-2010-How to Explain Individual Classification Decisions</a></p>
<p>17 0.026956528 <a title="101-tfidf-17" href="./jmlr-2010-Sparse_Spectrum_Gaussian_Process_Regression.html">104 jmlr-2010-Sparse Spectrum Gaussian Process Regression</a></p>
<p>18 0.025212457 <a title="101-tfidf-18" href="./jmlr-2010-On_Over-fitting_in_Model_Selection_and_Subsequent_Selection_Bias_in_Performance_Evaluation.html">83 jmlr-2010-On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</a></p>
<p>19 0.024014326 <a title="101-tfidf-19" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>20 0.023538701 <a title="101-tfidf-20" href="./jmlr-2010-Learning_From_Crowds.html">61 jmlr-2010-Learning From Crowds</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.127), (1, 0.0), (2, 0.011), (3, 0.015), (4, -0.044), (5, 0.059), (6, 0.081), (7, -0.053), (8, -0.019), (9, -0.014), (10, 0.01), (11, 0.035), (12, -0.046), (13, -0.065), (14, 0.001), (15, -0.047), (16, 0.131), (17, 0.109), (18, -0.081), (19, -0.112), (20, -0.049), (21, 0.021), (22, 0.016), (23, 0.026), (24, 0.074), (25, -0.002), (26, -0.063), (27, -0.007), (28, -0.035), (29, 0.133), (30, 0.106), (31, 0.359), (32, -0.34), (33, -0.066), (34, 0.152), (35, 0.155), (36, 0.053), (37, -0.073), (38, -0.177), (39, 0.027), (40, 0.122), (41, 0.034), (42, -0.155), (43, -0.035), (44, 0.084), (45, 0.104), (46, -0.134), (47, -0.099), (48, 0.038), (49, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93330568 <a title="101-lsi-1" href="./jmlr-2010-Second-Order_Bilinear_Discriminant_Analysis.html">101 jmlr-2010-Second-Order Bilinear Discriminant Analysis</a></p>
<p>Author: Christoforos Christoforou, Robert Haralick, Paul Sajda, Lucas C. Parra</p><p>Abstract: Traditional analysis methods for single-trial classiﬁcation of electro-encephalography (EEG) focus on two types of paradigms: phase-locked methods, in which the amplitude of the signal is used as the feature for classiﬁcation, that is, event related potentials; and second-order methods, in which the feature of interest is the power of the signal, that is, event related (de)synchronization. The process of deciding which paradigm to use is ad hoc and is driven by assumptions regarding the underlying neural generators. Here we propose a method that provides an uniﬁed framework for the analysis of EEG, combining ﬁrst and second-order spatial and temporal features based on a bilinear model. Evaluation of the proposed method on simulated data shows that the technique outperforms state-of-the art techniques for single-trial classiﬁcation for a broad range of signal-to-noise ratios. Evaluations on human EEG—including one benchmark data set from the Brain Computer Interface (BCI) competition—show statistically signiﬁcant gains in classiﬁcation accuracy, with a reduction in overall classiﬁcation error from 26%-28% to 19%. Keywords: regularization, classiﬁcation, bilinear decomposition, neural signals, brain computer interface</p><p>2 0.42495647 <a title="101-lsi-2" href="./jmlr-2010-Image_Denoising_with_Kernels_Based_on_Natural_Image_Relations.html">50 jmlr-2010-Image Denoising with Kernels Based on Natural Image Relations</a></p>
<p>Author: Valero Laparra, Juan Gutiérrez, Gustavo Camps-Valls, Jesús Malo</p><p>Abstract: A successful class of image denoising methods is based on Bayesian approaches working in wavelet representations. The performance of these methods improves when relations among the local frequency coefﬁcients are explicitly included. However, in these techniques, analytical estimates can be obtained only for particular combinations of analytical models of signal and noise, thus precluding its straightforward extension to deal with other arbitrary noise sources. In this paper, we propose an alternative non-explicit way to take into account the relations among natural image wavelet coefﬁcients for denoising: we use support vector regression (SVR) in the wavelet domain to enforce these relations in the estimated signal. Since relations among the coefﬁcients are speciﬁc to the signal, the regularization property of SVR is exploited to remove the noise, which does not share this feature. The speciﬁc signal relations are encoded in an anisotropic kernel obtained from mutual information measures computed on a representative image database. In the proposed scheme, training considers minimizing the Kullback-Leibler divergence (KLD) between the estimated and actual probability functions (or histograms) of signal and noise in order to enforce similarity up to the higher (computationally estimable) order. Due to its non-parametric nature, the method can eventually cope with different noise sources without the need of an explicit re-formulation, as it is strictly necessary under parametric Bayesian formalisms. Results under several noise levels and noise sources show that: (1) the proposed method outperforms conventional wavelet methods that assume coefﬁcient independence, (2) it is similar to state-of-the-art methods that do explicitly include these relations when the noise source is Gaussian, and (3) it gives better numerical and visual performance when more complex, realistic noise sources are considered. Therefore, the proposed machine learning approach can be seen as a mor</p><p>3 0.40322793 <a title="101-lsi-3" href="./jmlr-2010-A_Fast_Hybrid_Algorithm_for_Large-Scalel1-Regularized_Logistic_Regression.html">3 jmlr-2010-A Fast Hybrid Algorithm for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>Author: Jianing Shi, Wotao Yin, Stanley Osher, Paul Sajda</p><p>Abstract: ℓ1 -regularized logistic regression, also known as sparse logistic regression, is widely used in machine learning, computer vision, data mining, bioinformatics and neural signal processing. The use of ℓ1 regularization attributes attractive properties to the classiﬁer, such as feature selection, robustness to noise, and as a result, classiﬁer generality in the context of supervised learning. When a sparse logistic regression problem has large-scale data in high dimensions, it is computationally expensive to minimize the non-differentiable ℓ1 -norm in the objective function. Motivated by recent work (Koh et al., 2007; Hale et al., 2008), we propose a novel hybrid algorithm based on combining two types of optimization iterations: one being very fast and memory friendly while the other being slower but more accurate. Called hybrid iterative shrinkage (HIS), the resulting algorithm is comprised of a ﬁxed point continuation phase and an interior point phase. The ﬁrst phase is based completely on memory efﬁcient operations such as matrix-vector multiplications, while the second phase is based on a truncated Newton’s method. Furthermore, we show that various optimization techniques, including line search and continuation, can signiﬁcantly accelerate convergence. The algorithm has global convergence at a geometric rate (a Q-linear rate in optimization terminology). We present a numerical comparison with several existing algorithms, including an analysis using benchmark data from the UCI machine learning repository, and show our algorithm is the most computationally efﬁcient without loss of accuracy. Keywords: logistic regression, ℓ1 regularization, ﬁxed point continuation, supervised learning, large scale c 2010 Jianing Shi, Wotao Yin, Stanley Osher and Paul Sajda. S HI , Y IN , O SHER AND S AJDA</p><p>4 0.39617491 <a title="101-lsi-4" href="./jmlr-2010-Regularized_Discriminant_Analysis%2C_Ridge_Regression_and_Beyond.html">98 jmlr-2010-Regularized Discriminant Analysis, Ridge Regression and Beyond</a></p>
<p>Author: Zhihua Zhang, Guang Dai, Congfu Xu, Michael I. Jordan</p><p>Abstract: Fisher linear discriminant analysis (FDA) and its kernel extension—kernel discriminant analysis (KDA)—are well known methods that consider dimensionality reduction and classiﬁcation jointly. While widely deployed in practical problems, there are still unresolved issues surrounding their efﬁcient implementation and their relationship with least mean squares procedures. In this paper we address these issues within the framework of regularized estimation. Our approach leads to a ﬂexible and efﬁcient implementation of FDA as well as KDA. We also uncover a general relationship between regularized discriminant analysis and ridge regression. This relationship yields variations on conventional FDA based on the pseudoinverse and a direct equivalence to an ordinary least squares estimator. Keywords: Fisher discriminant analysis, reproducing kernel, generalized eigenproblems, ridge regression, singular value decomposition, eigenvalue decomposition</p><p>5 0.33936444 <a title="101-lsi-5" href="./jmlr-2010-Maximum_Likelihood_in_Cost-Sensitive_Learning%3A_Model_Specification%2C_Approximations%2C_and_Upper_Bounds.html">73 jmlr-2010-Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds</a></p>
<p>Author: Jacek P. Dmochowski, Paul Sajda, Lucas C. Parra</p><p>Abstract: The presence of asymmetry in the misclassiﬁcation costs or class prevalences is a common occurrence in the pattern classiﬁcation domain. While much interest has been devoted to the study of cost-sensitive learning techniques, the relationship between cost-sensitive learning and the speciﬁcation of the model set in a parametric estimation framework remains somewhat unclear. To that end, we differentiate between the case of the model including the true posterior, and that in which the model is misspeciﬁed. In the former case, it is shown that thresholding the maximum likelihood (ML) estimate is an asymptotically optimal solution to the risk minimization problem. On the other hand, under model misspeciﬁcation, it is demonstrated that thresholded ML is suboptimal and that the risk-minimizing solution varies with the misclassiﬁcation cost ratio. Moreover, we analytically show that the negative weighted log likelihood (Elkan, 2001) is a tight, convex upper bound of the empirical loss. Coupled with empirical results on several real-world data sets, we argue that weighted ML is the preferred cost-sensitive technique. Keywords: empirical risk minimization, loss function, cost-sensitive learning, imbalanced data sets</p><p>6 0.31026012 <a title="101-lsi-6" href="./jmlr-2010-A_Convergent_Online_Single_Time_Scale_Actor_Critic_Algorithm.html">2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</a></p>
<p>7 0.26162842 <a title="101-lsi-7" href="./jmlr-2010-Model-based_Boosting_2.0.html">77 jmlr-2010-Model-based Boosting 2.0</a></p>
<p>8 0.241064 <a title="101-lsi-8" href="./jmlr-2010-Matched_Gene_Selection_and_Committee_Classifier_for_Molecular_Classification_of_Heterogeneous_Diseases.html">71 jmlr-2010-Matched Gene Selection and Committee Classifier for Molecular Classification of Heterogeneous Diseases</a></p>
<p>9 0.21469106 <a title="101-lsi-9" href="./jmlr-2010-Evolving_Static_Representations_for_Task_Transfer.html">37 jmlr-2010-Evolving Static Representations for Task Transfer</a></p>
<p>10 0.17824623 <a title="101-lsi-10" href="./jmlr-2010-Consensus-Based_Distributed_Support_Vector_Machines.html">26 jmlr-2010-Consensus-Based Distributed Support Vector Machines</a></p>
<p>11 0.1712807 <a title="101-lsi-11" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>12 0.16622598 <a title="101-lsi-12" href="./jmlr-2010-Collective_Inference_for__Extraction_MRFs_Coupled_with_Symmetric_Clique_Potentials.html">24 jmlr-2010-Collective Inference for  Extraction MRFs Coupled with Symmetric Clique Potentials</a></p>
<p>13 0.165012 <a title="101-lsi-13" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>14 0.16324452 <a title="101-lsi-14" href="./jmlr-2010-Using_Contextual_Representations_to_Efficiently_Learn_Context-Free_Languages.html">115 jmlr-2010-Using Contextual Representations to Efficiently Learn Context-Free Languages</a></p>
<p>15 0.16226491 <a title="101-lsi-15" href="./jmlr-2010-Classification_with_Incomplete_Data_Using_Dirichlet_Process_Priors.html">23 jmlr-2010-Classification with Incomplete Data Using Dirichlet Process Priors</a></p>
<p>16 0.16193834 <a title="101-lsi-16" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>17 0.15772083 <a title="101-lsi-17" href="./jmlr-2010-Spectral_Regularization_Algorithms_for_Learning_Large_Incomplete_Matrices.html">105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</a></p>
<p>18 0.15177554 <a title="101-lsi-18" href="./jmlr-2010-Sparse_Spectrum_Gaussian_Process_Regression.html">104 jmlr-2010-Sparse Spectrum Gaussian Process Regression</a></p>
<p>19 0.15078768 <a title="101-lsi-19" href="./jmlr-2010-Efficient_Algorithms_for_Conditional_Independence_Inference.html">32 jmlr-2010-Efficient Algorithms for Conditional Independence Inference</a></p>
<p>20 0.13636981 <a title="101-lsi-20" href="./jmlr-2010-Learning_From_Crowds.html">61 jmlr-2010-Learning From Crowds</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.015), (4, 0.011), (5, 0.448), (8, 0.014), (21, 0.014), (32, 0.096), (33, 0.014), (36, 0.038), (37, 0.048), (75, 0.123), (81, 0.015), (85, 0.047), (96, 0.018), (97, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69262064 <a title="101-lda-1" href="./jmlr-2010-Second-Order_Bilinear_Discriminant_Analysis.html">101 jmlr-2010-Second-Order Bilinear Discriminant Analysis</a></p>
<p>Author: Christoforos Christoforou, Robert Haralick, Paul Sajda, Lucas C. Parra</p><p>Abstract: Traditional analysis methods for single-trial classiﬁcation of electro-encephalography (EEG) focus on two types of paradigms: phase-locked methods, in which the amplitude of the signal is used as the feature for classiﬁcation, that is, event related potentials; and second-order methods, in which the feature of interest is the power of the signal, that is, event related (de)synchronization. The process of deciding which paradigm to use is ad hoc and is driven by assumptions regarding the underlying neural generators. Here we propose a method that provides an uniﬁed framework for the analysis of EEG, combining ﬁrst and second-order spatial and temporal features based on a bilinear model. Evaluation of the proposed method on simulated data shows that the technique outperforms state-of-the art techniques for single-trial classiﬁcation for a broad range of signal-to-noise ratios. Evaluations on human EEG—including one benchmark data set from the Brain Computer Interface (BCI) competition—show statistically signiﬁcant gains in classiﬁcation accuracy, with a reduction in overall classiﬁcation error from 26%-28% to 19%. Keywords: regularization, classiﬁcation, bilinear decomposition, neural signals, brain computer interface</p><p>2 0.64815706 <a title="101-lda-2" href="./jmlr-2010-A_Convergent_Online_Single_Time_Scale_Actor_Critic_Algorithm.html">2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</a></p>
<p>Author: Dotan Di Castro, Ron Meir</p><p>Abstract: Actor-Critic based approaches were among the ﬁrst to address reinforcement learning in a general setting. Recently, these algorithms have gained renewed interest due to their generality, good convergence properties, and possible biological relevance. In this paper, we introduce an online temporal difference based actor-critic algorithm which is proved to converge to a neighborhood of a local maximum of the average reward. Linear function approximation is used by the critic in order estimate the value function, and the temporal difference signal, which is passed from the critic to the actor. The main distinguishing feature of the present convergence proof is that both the actor and the critic operate on a similar time scale, while in most current convergence proofs they are required to have very different time scales in order to converge. Moreover, the same temporal difference signal is used to update the parameters of both the actor and the critic. A limitation of the proposed approach, compared to results available for two time scale convergence, is that convergence is guaranteed only to a neighborhood of an optimal value, rather to an optimal value itself. The single time scale and identical temporal difference signal used by the actor and the critic, may provide a step towards constructing more biologically realistic models of reinforcement learning in the brain. Keywords: actor critic, single time scale convergence, temporal difference</p><p>3 0.34597325 <a title="101-lda-3" href="./jmlr-2010-Expectation_Truncation_and_the_Benefits_of_Preselection_In_Training_Generative_Models.html">38 jmlr-2010-Expectation Truncation and the Benefits of Preselection In Training Generative Models</a></p>
<p>Author: Jörg Lücke, Julian Eggert</p><p>Abstract: We show how a preselection of hidden variables can be used to efﬁciently train generative models with binary hidden variables. The approach is based on Expectation Maximization (EM) and uses an efﬁciently computable approximation to the sufﬁcient statistics of a given model. The computational cost to compute the sufﬁcient statistics is strongly reduced by selecting, for each data point, the relevant hidden causes. The approximation is applicable to a wide range of generative models and provides an interpretation of the beneﬁts of preselection in terms of a variational EM approximation. To empirically show that the method maximizes the data likelihood, it is applied to different types of generative models including: a version of non-negative matrix factorization (NMF), a model for non-linear component extraction (MCA), and a linear generative model similar to sparse coding. The derived algorithms are applied to both artiﬁcial and realistic data, and are compared to other models in the literature. We ﬁnd that the training scheme can reduce computational costs by orders of magnitude and allows for a reliable extraction of hidden causes. Keywords: maximum likelihood, deterministic approximations, variational EM, generative models, component extraction, multiple-cause models</p><p>4 0.34443688 <a title="101-lda-4" href="./jmlr-2010-Regularized_Discriminant_Analysis%2C_Ridge_Regression_and_Beyond.html">98 jmlr-2010-Regularized Discriminant Analysis, Ridge Regression and Beyond</a></p>
<p>Author: Zhihua Zhang, Guang Dai, Congfu Xu, Michael I. Jordan</p><p>Abstract: Fisher linear discriminant analysis (FDA) and its kernel extension—kernel discriminant analysis (KDA)—are well known methods that consider dimensionality reduction and classiﬁcation jointly. While widely deployed in practical problems, there are still unresolved issues surrounding their efﬁcient implementation and their relationship with least mean squares procedures. In this paper we address these issues within the framework of regularized estimation. Our approach leads to a ﬂexible and efﬁcient implementation of FDA as well as KDA. We also uncover a general relationship between regularized discriminant analysis and ridge regression. This relationship yields variations on conventional FDA based on the pseudoinverse and a direct equivalence to an ordinary least squares estimator. Keywords: Fisher discriminant analysis, reproducing kernel, generalized eigenproblems, ridge regression, singular value decomposition, eigenvalue decomposition</p><p>5 0.3420215 <a title="101-lda-5" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>Author: Yevgeny Seldin, Naftali Tishby</p><p>Abstract: We derive PAC-Bayesian generalization bounds for supervised and unsupervised learning models based on clustering, such as co-clustering, matrix tri-factorization, graphical models, graph clustering, and pairwise clustering.1 We begin with the analysis of co-clustering, which is a widely used approach to the analysis of data matrices. We distinguish among two tasks in matrix data analysis: discriminative prediction of the missing entries in data matrices and estimation of the joint probability distribution of row and column variables in co-occurrence matrices. We derive PAC-Bayesian generalization bounds for the expected out-of-sample performance of co-clustering-based solutions for these two tasks. The analysis yields regularization terms that were absent in the previous formulations of co-clustering. The bounds suggest that the expected performance of co-clustering is governed by a trade-off between its empirical performance and the mutual information preserved by the cluster variables on row and column IDs. We derive an iterative projection algorithm for ﬁnding a local optimum of this trade-off for discriminative prediction tasks. This algorithm achieved stateof-the-art performance in the MovieLens collaborative ﬁltering task. Our co-clustering model can also be seen as matrix tri-factorization and the results provide generalization bounds, regularization terms, and new algorithms for this form of matrix factorization. The analysis of co-clustering is extended to tree-shaped graphical models, which can be used to analyze high dimensional tensors. According to the bounds, the generalization abilities of treeshaped graphical models depend on a trade-off between their empirical data ﬁt and the mutual information that is propagated up the tree levels. We also formulate weighted graph clustering as a prediction problem: given a subset of edge weights we analyze the ability of graph clustering to predict the remaining edge weights. The analysis of co-clustering easily</p><p>6 0.33953986 <a title="101-lda-6" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>7 0.33778149 <a title="101-lda-7" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>8 0.33760905 <a title="101-lda-8" href="./jmlr-2010-On_the_Foundations_of_Noise-free_Selective_Classification.html">85 jmlr-2010-On the Foundations of Noise-free Selective Classification</a></p>
<p>9 0.33563417 <a title="101-lda-9" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>10 0.33415416 <a title="101-lda-10" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>11 0.33353952 <a title="101-lda-11" href="./jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</a></p>
<p>12 0.3334623 <a title="101-lda-12" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>13 0.33235762 <a title="101-lda-13" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>14 0.33180463 <a title="101-lda-14" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>15 0.33167559 <a title="101-lda-15" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>16 0.33129326 <a title="101-lda-16" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>17 0.33020949 <a title="101-lda-17" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>18 0.32990861 <a title="101-lda-18" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>19 0.32981488 <a title="101-lda-19" href="./jmlr-2010-Maximum_Likelihood_in_Cost-Sensitive_Learning%3A_Model_Specification%2C_Approximations%2C_and_Upper_Bounds.html">73 jmlr-2010-Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds</a></p>
<p>20 0.32922542 <a title="101-lda-20" href="./jmlr-2010-Spectral_Regularization_Algorithms_for_Learning_Large_Incomplete_Matrices.html">105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
