<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-67" href="#">jmlr2010-67</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</h1>
<br/><p>Source: <a title="jmlr-2010-67-pdf" href="http://jmlr.org/papers/volume11/aliferis10a/aliferis10a.pdf">pdf</a></p><p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: We present an algorithmic framework for learning local causal structure around target variables of interest in the form of direct causes/effects and Markov blankets applicable to very large data sets with relatively small samples. The selected feature sets can be used for causal discovery and classiÄ?Ĺš cation. The framework (Generalized Local Learning, or GLL) can be instantiated in numerous ways, giving rise to both existing state-of-the-art as well as novel algorithms. The resulting algorithms are sound under well-deÄ?Ĺš ned sufÄ?Ĺš cient conditions. In a Ä?Ĺš rst set of experiments we evaluate several algorithms derived from this framework in terms of predictivity and feature set parsimony and compare to other local causal discovery methods and to state-of-the-art non-causal feature selection methods using real data. A second set of experimental evaluations compares the algorithms in terms of ability to induce local causal neighborhoods using simulated and resimulated data and examines the relation of predictivity with causal induction performance. Our experiments demonstrate, consistently with causal feature selection theory, that local causal feature selection methods (under broad assumptions encompassing appropriate family of distribuc 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS tions, types of classiÄ?Ĺš ers, and loss functions) exhibit strong feature set parsimony, high predictivity and local causal interpretability. Although non-causal feature selection methods are often used in practice to shed light on causal relationships, we Ä?Ĺš nd that they cannot be interpreted causally even when they achieve excellent predictivity. Therefore we conclude that only local causal techniques should be used when insight into causal structure is sought. In a companion paper we examine in depth the behavior of GLL algorithms, provide extensions, and show</p><p>Reference: <a title="jmlr-2010-67-reference" href="../jmlr2010_reference/jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_I%3A_Algorithms_and_Empirical_Evaluation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The selected feature sets can be used for causal discovery and classiÄ? [sent-16, score-0.609]
</p><p>2 Ĺš rst set of experiments we evaluate several algorithms derived from this framework in terms of predictivity and feature set parsimony and compare to other local causal discovery methods and to state-of-the-art non-causal feature selection methods using real data. [sent-23, score-0.917]
</p><p>3 A second set of experimental evaluations compares the algorithms in terms of ability to induce local causal neighborhoods using simulated and resimulated data and examines the relation of predictivity with causal induction performance. [sent-24, score-1.106]
</p><p>4 Our experiments demonstrate, consistently with causal feature selection theory, that local causal feature selection methods (under broad assumptions encompassing appropriate family of distribuc 2010 Constantin F. [sent-25, score-1.187]
</p><p>5 Ĺš ers, and loss functions) exhibit strong feature set parsimony, high predictivity and local causal interpretability. [sent-29, score-0.662]
</p><p>6 Although non-causal feature selection methods are often used in practice to shed light on causal relationships, we Ä? [sent-30, score-0.573]
</p><p>7 Therefore we conclude that only local causal techniques should be used when insight into causal structure is sought. [sent-32, score-0.925]
</p><p>8 In a companion paper we examine in depth the behavior of GLL algorithms, provide extensions, and show how local techniques can be used for scalable and accurate global causal graph learning. [sent-33, score-0.549]
</p><p>9 Keywords: local causal discovery, Markov blanket induction, feature selection, classiÄ? [sent-34, score-0.721]
</p><p>10 Introduction This paper addresses the problem of how to learn local causal structure around a target variable of interest using observational data. [sent-36, score-0.519]
</p><p>11 Ĺš cant because it plays a central role in causal discovery and classiÄ? [sent-42, score-0.53]
</p><p>12 Ĺš cally, solving the local causal induction problem helps understanding how natural and artiÄ? [sent-49, score-0.517]
</p><p>13 Ĺš nally local causal discovery can form the basis of efÄ? [sent-52, score-0.571]
</p><p>14 Section 4 provides a general algorithmic framework, Generalized Local Learning (GLL), which can be instantiated in many different ways yielding sound algorithms for local causal discovery and feature selection. [sent-59, score-0.65]
</p><p>15 Section 5 evaluates a multitude of algorithmic instantiations and parameterizations from GLL and compares them to state-of-the-art local causal discovery and feature selection methods in terms of classiÄ? [sent-60, score-0.741]
</p><p>16 A companion paper (part II of the present work) studies the GLL algorithm properties 172  L OCAL C AUSAL AND M ARKOV B LANKET I NDUCTION PART I  empirically and theoretically, introduces algorithmic extensions, and connects local to global causal graph learning (Aliferis et al. [sent-70, score-0.549]
</p><p>17 For principled causal discovery with known theoretical properties a causal theory is needed and classiÄ? [sent-153, score-0.972]
</p><p>18 Since then, algorithms that infer such causal relations have been developed that can greatly reduce the number of experiments required to discover the causal structure. [sent-169, score-0.884]
</p><p>19 One of the most common methods to model and induce causal relations is by learning causal Bayesian networks (Neapolitan, 2004; Spirtes et al. [sent-174, score-0.884]
</p><p>20 Heckerman and his colleagues studied theoretically the properties of the various scoring metrics as they pertain to causal discovery (Glymour and Cooper, 1999; Heckerman, 1995; Heckerman et al. [sent-183, score-0.53]
</p><p>21 Another prototypical method for learning causal relationships by inducing causal Bayesian networks is the constraint-based approach as exempliÄ? [sent-186, score-0.884]
</p><p>22 The PC induces causal relations by assuming faithfulness and by performing tests of independence. [sent-189, score-0.503]
</p><p>23 These networks belong to the same Markov equivalence class of causal graphs and contain the same causal edges but may disagree on the direction of some of them, that is, whether A causes B or vice-versa (Chickering, 2002; Spirtes et al. [sent-196, score-0.932]
</p><p>24 An essential graph is a graph where the directed edges represent the causal relations on which all equivalent networks agree upon their directionality and all the remaining edges are undirected. [sent-198, score-0.502]
</p><p>25 Causal discovery by employing causal Bayesian networks is based on the following principles. [sent-199, score-0.53]
</p><p>26 Algorithms, such as the FCI, that in some cases can discover causal relationships in the 175  A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS  presence of hidden confounding variables and selection bias, have also been designed (see Spirtes et al. [sent-209, score-0.494]
</p><p>27 Ĺš nite size), one can infer only a Markov equivalence class of causal graphs, which may be inadequate for causal discovery. [sent-212, score-0.884]
</p><p>28 Fundamental theoretical results connecting Markov blanket induction for feature selection and local causal discovery to standard notions of relevance were given in Tsamardinos and Aliferis (2003). [sent-242, score-0.895]
</p><p>29 In faithful distributions and under the conditions of #5, the strongly/weakly/irrelevant taxonomy of variables (Kohavi and John, 1997) can be mapped naturally to causal graph properties. [sent-275, score-0.566]
</p><p>30 We will refer to algorithms that perform feature selection by formal causal induction as causal feature selection and algorithms that do not as non-causal. [sent-283, score-1.18]
</p><p>31 As highly complementary to the above results we would add the arguments in favor of causal feature selection presented in Guyon et al. [sent-284, score-0.573]
</p><p>32 Ĺš cient conditions that make Markov blanket the optimal solution to the feature selection and local causal discovery problem, state-of-the-art methods such as ranking features by SVM weights (RFE being a prototypical algorithm Guyon et al. [sent-288, score-0.861]
</p><p>33 &  /  ' 7  ( * )  +  ,  5HODWLRQVKLS EHWZHHQ FDXVDO VWUXFWXUH DQG SUHGLFWLYLW\ LQ IDLWKIXO GLVWULEXWLRQV  Figure 1: Relationship between causal structure and predictivity in faithful distributions. [sent-290, score-0.636]
</p><p>34 correct causal neighborhood and are not minimal, that is, do not solve the feature selection problem) even in the large sample limit. [sent-303, score-0.573]
</p><p>35 3 Methods to Speed-up Discovery: Local Discovery as a Critical Tool for Scalability As appealing as causal discovery may be for understanding a domain, predicting effects of intervention, and pursuing principled feature selection for classiÄ? [sent-312, score-0.694]
</p><p>36 In early 2000Ă˘&euro;&trade;s predictions about the feasibility of causal discovery in high-dimensional data were bleak (Silverstein et al. [sent-325, score-0.53]
</p><p>37 A variety of methods to scale up causal discovery have been devised to address the problem: 1. [sent-327, score-0.53]
</p><p>38 Abandon the effort to learn the full causal graph and instead develop methods that learn the local neighborhood of a speciÄ? [sent-333, score-0.513]
</p><p>39 Techniques #4 Ă˘&circ;&rsquo; 6 pertain to local learning: Technique #4 seeks to learn the complete causal neighbourhood around a target variable provided by the user (Aliferis et al. [sent-346, score-0.519]
</p><p>40 In the present paper we explore methods to learn local causal neighborhoods and test them in high-dimensional data sets. [sent-358, score-0.483]
</p><p>41 Figure 2 provides a visual reference guide to the kinds of causal discovery problems the methods in the present work are able to address by starting from local causal discovery. [sent-362, score-1.013]
</p><p>42 7 6  ;  2  3  -  4  / <  8  3UREOHP  'LVFRYHU XQGLUHFWHG JUDSK  Figure 2: Five types of causal discovery from local (types 1, 2), to global (4, 5) and intermediate (3). [sent-368, score-0.571]
</p><p>43 Ĺš ciently solving all other types of causal discovery as well (see text for details). [sent-370, score-0.53]
</p><p>44 Furthermore, all local learning methods exploit either the constraint-based framework for causal discovery developed by Spirtes, Glymour, Schienes, Pearl, and Verma and their co-authors (Spirtes et al. [sent-379, score-0.571]
</p><p>45 Ĺš rst introduced in 1996 (Koller and Sahami, 1996), incomplete causal methods in 1997 (Cooper, 1997), and local causal discovery methods (for targeted complete induction of direct causes and effects) were Ä? [sent-385, score-1.095]
</p><p>46 In 1997 Cooper introduced an incomplete method for causal discovery (Cooper et al. [sent-392, score-0.53]
</p><p>47 To avoid notational confusion we point out that the algorithm was termed LCD (local causal discovery) despite being an incomplete rather than local algorithm as local algorithms are deÄ? [sent-395, score-0.524]
</p><p>48 Ĺš ned by Kohavi and John (1997) in terms of Markov blanket and causal connectivity (Tsamardi181  A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS  nos and Aliferis, 2003). [sent-438, score-0.601]
</p><p>49 We furthermore study comparative performance in terms of predictivity and parsimony of state-of-the-art local causal algorithms; we compare them to non-causal algorithms in real and simulated data sets using the same criteria; and show how novel algorithms can be obtained. [sent-529, score-0.619]
</p><p>50 A second major hypothesis (and set of experiments in the present paper) is that non-causal feature selection methods may yield predictively optimal feature sets while from a causal perspective their output is unreliable. [sent-530, score-0.652]
</p><p>51 Thus, together faithfulness and the causal Markov Condition establish a close relationship between a causal graph G and some empirical or theoretical probability distribution J. [sent-601, score-0.914]
</p><p>52 A General Framework for Local Learning In this section we present a formal general framework for learning local causal structure. [sent-661, score-0.483]
</p><p>53 Ĺš ciency holds for the variables V , and the distribution P is faithful to a causal Bayesian network, then the direct causes, direct effects, and direct causes of the direct effects of T , correspond to the parents, children, and spouses of T respectively in that network. [sent-676, score-0.617]
</p><p>54 Therefore, under the assumptions of the existence of a causal Bayesian network that faithfully captures P and causal sufÄ? [sent-686, score-0.921]
</p><p>55 Problem 1 is geared toward local causal discovery, while Problem 2 is oriented toward causal feature selection for classiÄ? [sent-689, score-1.056]
</p><p>56 There is a causal Bayesian network faithful to the data distribution P; b. [sent-809, score-0.573]
</p><p>57 Ĺš cient for the determination of the conditional distribution of T : P(T |MB(T )) = P(T |V \ {T }) and further, it coincides with the parents, children and spouses of T in any network faithful to the distribution (if any) under causal sufÄ? [sent-918, score-0.613]
</p><p>58 There is a causal Bayesian network faithful to the data distribution P; b. [sent-948, score-0.573]
</p><p>59 For example, the PC algorithm for global causal discovery (Spirtes et al. [sent-1062, score-0.53]
</p><p>60 Ĺš cation performance as possible for each data set and compare them with other local causal structure discovery methods as well as non-causal feature selection methods. [sent-1106, score-0.702]
</p><p>61 Ĺš guration, both GLL and other local causal methods (i. [sent-1397, score-0.483]
</p><p>62 These 6 cases correspond strictly to non-GLL causal feature selection algorithms and at the expense of severe predictive suboptimality (0. [sent-1408, score-0.573]
</p><p>63 As can be seen, GLL algorithms that induce PC sets dominate both other causal and non-causal feature 207  A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS  selection algorithms. [sent-1469, score-0.573]
</p><p>64 In Table S10 of the online supplement we provide statistical comparisons of non-GLL causal feature selection methods in terms of predictivity and parsimony. [sent-1484, score-0.72]
</p><p>65 Ĺš elds researchers routinely apply feature selection in order to gain insights about the causal structure of the domain. [sent-1504, score-0.573]
</p><p>66 It is thus necessary to test the appropriateness of various feature selection techniques for causal discovery, not just classiÄ? [sent-1510, score-0.573]
</p><p>67 In order to compare the performance of the tested techniques for causal discovery, we simulate data from known Bayesian networks and also use resimulation, whereby real data is used to elicit a causal network and then data is simulated from the obtained network (see Table 11 in the Appendix). [sent-1512, score-0.958]
</p><p>68 We note that SCA has greatly different inductive bias from the GLL variants and thus the comparison (provided that the causal generative model is a Bayesian network) is not unduly biased toward them, while still allowing induction of a credible causal graphical model. [sent-1520, score-0.918]
</p><p>69 GLL algorithms induce a local causal neighborhood, under the distributional assumption of faithfulness and causal sufÄ? [sent-1523, score-0.925]
</p><p>70 In other words successful causal structure discovery performance in simulated and resimulated networks represents at a minimum Ă˘&euro;&oelig;gate-keeperĂ˘&euro;? [sent-1543, score-0.577]
</p><p>71 We note that the causal discovery evaluations emphasize local discovery of direct causes and direct effects and this choice is supported by several reasons. [sent-1579, score-0.74]
</p><p>72 Second, for non-causal feature selection methods, a natural causal interpretation of their output is being among the direct causes and direct effects (or the Markov blanket) of the target. [sent-1583, score-0.654]
</p><p>73 The logic of this argument is that algorithms either return global or local causal knowledge. [sent-1589, score-0.483]
</p><p>74 If algorithm B outputs a correct local causal set (e. [sent-1592, score-0.483]
</p><p>75 Finally, if an algorithm outputs an incorrect non-empty local causal set, this implies that B returns non-causes as direct causes or remote causes as direct causes (and the same for effects). [sent-1596, score-0.627]
</p><p>76 As a result, local causal consistency is necessary for global consistency as well. [sent-1598, score-0.483]
</p><p>77 A second reason for focusing on local causal discovery is that it is much harder in practice than indirect causal discovery in highly interconnected causal networks. [sent-1599, score-1.543]
</p><p>78 Conversely, when one has a locally correct causal discovery algorithm as elucidated in Section 2, global causal learners can be relatively easily constructed. [sent-1607, score-0.972]
</p><p>79 Third, oriented local causal discovery is harder than unoriented one (Ramsey et al. [sent-1610, score-0.571]
</p><p>80 , 2006), and it makes sense to examine the ability of the feature selection algorithms for causal discovery in tasks of incremental difÄ? [sent-1611, score-0.661]
</p><p>81 1 Superiority of Causal Over Non-Causal Feature Selection Methods for Causal Discovery Causal methods achieve, consistently under a variety of conditions and across all metrics employed, superior causal discovery performance than non-causal feature selection methods in our experiments. [sent-1615, score-0.661]
</p><p>82 The closer are points to the origin, the better is ability for local causal discovery. [sent-1684, score-0.483]
</p><p>83 Ĺš cation performance is thus greatly misleading as a criterion for quality of causal hypotheses generated by non-causal feature selection algorithms. [sent-1715, score-0.573]
</p><p>84 In conclusion, the results in the present section strongly undermine the hope that non-causal feature selection methods can be used as good heuristics for causal discovery. [sent-1716, score-0.573]
</p><p>85 The idea that noncausal feature selection can be used for causal discovery should be viewed with caution (Guyon et al. [sent-1717, score-0.661]
</p><p>86 Whole research programs are, in many domains, built on experiments motivated by causal hypotheses that were generated by non-causal feature selection results (Zhou et al. [sent-1719, score-0.573]
</p><p>87 On the other hand, generalized local learning algorithms in simulated and resimulated experiments show great potential for local causal discovery. [sent-1724, score-0.571]
</p><p>88 We used resimulated and simulated data and showed that causally-motivated feature selection methods connect local causal discovery with feature selection for classiÄ? [sent-1735, score-0.88]
</p><p>89 Feature selection algorithms that are not causal have a tendency to return highly predictive feature sets that are scattered all over the network, or that are in the periphery of the network, and cannot be otherwise interpreted in a way that makes useful and consistent causal sense. [sent-1737, score-1.015]
</p><p>90 We strongly caution practitioners to use principled causal discovery algorithms whenever available and to not substitute causal discovery methods with predictive/non-causal feature selection ones for reasons of convenience or due to non familiarity with such methods. [sent-1738, score-1.191]
</p><p>91 , special types of violations of faithfulness), many Markov blankets may exist and the algorithms will return a predictively optimal feature set but both causal localization and optimal parsimony may be lost (Statnikov, 2008). [sent-1795, score-0.557]
</p><p>92 Ĺš&sbquo;uence the quality and number of statistical decisions, explain the inductive bias of the algorithms, show how non-causal feature selection methods can be understood in light of Markov blanket induction theory, and address divide-and-conquer local to global causal graph learning strategies. [sent-1834, score-0.837]
</p><p>93 First notice, that as we mentioned above, when conditions (a) and (c) hold the direct causes and direct effects of T will coincide with the parents and children of T in the causal Bayesian network G that faithfully captures the distribution (Spirtes et al. [sent-1839, score-0.634]
</p><p>94 Algorithms for large-scale local causal discovery and feature selection in the presence of small sample or large causal neighborhoods. [sent-2017, score-1.144]
</p><p>95 Using local causal induction to improve global causal discovery: Enhancing the sparse candidate set. [sent-2023, score-0.959]
</p><p>96 Local causal and Markov blanket induction for causal discovery and feature selection for classiÄ? [sent-2076, score-1.296]
</p><p>97 UXVNDO:DOOLV690   8$)6LJQDO1RLVH690   8$)6LJQDO1RLVH690   / /$56(1 IRU PXOWLFODVV UHVSRQVH /$56(1 RQHYHUVXVUHVW  Table 12: Algorithms used in local causal discovery experiments with simulated and resimulated data. [sent-2138, score-0.618]
</p><p>98 A study in causal discovery from population-based infant birth and death records. [sent-2661, score-0.53]
</p><p>99 Causal discovery using a Bayesian local causal discovery algorithm. [sent-2667, score-0.659]
</p><p>100 Ĺš cient discovery of Markov blankets and direct causal relations. [sent-2965, score-0.53]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('causal', 0.442), ('tpc', 0.291), ('pc', 0.277), ('gll', 0.197), ('aliferis', 0.164), ('blanket', 0.159), ('tsamardinos', 0.157), ('dqg', 0.143), ('koutsoukos', 0.122), ('wkh', 0.118), ('ani', 0.115), ('liferis', 0.115), ('samardinos', 0.115), ('tatnikov', 0.115), ('zlwk', 0.115), ('pd', 0.112), ('arkov', 0.111), ('lanket', 0.111), ('nduction', 0.111), ('parentsg', 0.109), ('lq', 0.104), ('iurp', 0.1), ('predictivity', 0.1), ('ausal', 0.1), ('faithful', 0.094), ('ocal', 0.092), ('yduldeoh', 0.09), ('mb', 0.089), ('uhpryh', 0.088), ('discovery', 0.088), ('spirtes', 0.083), ('cooper', 0.081), ('feature', 0.079), ('epc', 0.075), ('markov', 0.073), ('ihdwxuhv', 0.072), ('vwudwhj', 0.072), ('admissibility', 0.071), ('returned', 0.067), ('ri', 0.062), ('wr', 0.057), ('yduldeohv', 0.057), ('statnikov', 0.056), ('mmpc', 0.055), ('sso', 0.055), ('iru', 0.054), ('univariate', 0.052), ('iamb', 0.052), ('selection', 0.052), ('sulrulw', 0.05), ('tmb', 0.05), ('members', 0.05), ('pearl', 0.05), ('causes', 0.048), ('guyon', 0.048), ('supplement', 0.047), ('holplqdwlrq', 0.047), ('interleaved', 0.047), ('resimulated', 0.047), ('vhohfwhg', 0.047), ('kljkhvw', 0.046), ('obeyed', 0.046), ('glymour', 0.045), ('symmetry', 0.043), ('shuirupdqfh', 0.043), ('whvw', 0.043), ('bayesian', 0.042), ('local', 0.041), ('knn', 0.041), ('auc', 0.041), ('children', 0.04), ('cancer', 0.039), ('lw', 0.039), ('instantiations', 0.039), ('network', 0.037), ('target', 0.036), ('vhw', 0.036), ('parsimony', 0.036), ('companion', 0.036), ('classification', 0.035), ('classi', 0.035), ('correction', 0.034), ('induction', 0.034), ('parents', 0.034), ('hiihfw', 0.034), ('mani', 0.033), ('effects', 0.033), ('proportion', 0.033), ('lqfoxvlrq', 0.032), ('qrqv', 0.032), ('tests', 0.032), ('statistically', 0.031), ('gene', 0.031), ('correctness', 0.031), ('elimination', 0.03), ('graph', 0.03), ('termination', 0.029), ('cpn', 0.029), ('magnified', 0.029), ('performing', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000014 <a title="67-tfidf-1" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_I%3A_Algorithms_and_Empirical_Evaluation.html">67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</a></p>
<p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: We present an algorithmic framework for learning local causal structure around target variables of interest in the form of direct causes/effects and Markov blankets applicable to very large data sets with relatively small samples. The selected feature sets can be used for causal discovery and classiÄ?Ĺš cation. The framework (Generalized Local Learning, or GLL) can be instantiated in numerous ways, giving rise to both existing state-of-the-art as well as novel algorithms. The resulting algorithms are sound under well-deÄ?Ĺš ned sufÄ?Ĺš cient conditions. In a Ä?Ĺš rst set of experiments we evaluate several algorithms derived from this framework in terms of predictivity and feature set parsimony and compare to other local causal discovery methods and to state-of-the-art non-causal feature selection methods using real data. A second set of experimental evaluations compares the algorithms in terms of ability to induce local causal neighborhoods using simulated and resimulated data and examines the relation of predictivity with causal induction performance. Our experiments demonstrate, consistently with causal feature selection theory, that local causal feature selection methods (under broad assumptions encompassing appropriate family of distribuc 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS tions, types of classiÄ?Ĺš ers, and loss functions) exhibit strong feature set parsimony, high predictivity and local causal interpretability. Although non-causal feature selection methods are often used in practice to shed light on causal relationships, we Ä?Ĺš nd that they cannot be interpreted causally even when they achieve excellent predictivity. Therefore we conclude that only local causal techniques should be used when insight into causal structure is sought. In a companion paper we examine in depth the behavior of GLL algorithms, provide extensions, and show</p><p>2 0.66050166 <a title="67-tfidf-2" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_II%3A_Analysis_and_Extensions.html">68 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions</a></p>
<p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: In part I of this work we introduced and evaluated the Generalized Local Learning (GLL) framework for producing local causal and Markov blanket induction algorithms. In the present second part we analyze the behavior of GLL algorithms and provide extensions to the core methods. SpeciÄ?Ĺš cally, we investigate the empirical convergence of GLL to the true local neighborhood as a function of sample size. Moreover, we study how predictivity improves with increasing sample size. Then we investigate how sensitive are the algorithms to multiple statistical testing, especially in the presence of many irrelevant features. Next we discuss the role of the algorithm parameters and also show that Markov blanket and causal graph concepts can be used to understand deviations from optimality of state-of-the-art non-causal algorithms. The present paper also introduces the following extensions to the core GLL framework: parallel and distributed versions of GLL algorithms, versions with false discovery rate control, strategies for constructing novel heuristics for speciÄ?Ĺš c domains, and divide-and-conquer local-to-global learning (LGL) strategies. We test the generality of the LGL approach by deriving a novel LGL-based algorithm that compares favorably c 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS to the state-of-the-art global learning algorithms. In addition, we investigate the use of non-causal feature selection methods to facilitate global learning. Open problems and future research paths related to local and local-to-global causal learning are discussed. Keywords: local causal discovery, Markov blanket induction, feature selection, classiÄ?Ĺš cation, causal structure learning, learning of Bayesian networks</p><p>3 0.38284513 <a title="67-tfidf-3" href="./jmlr-2010-Introduction_to_Causal_Inference.html">56 jmlr-2010-Introduction to Causal Inference</a></p>
<p>Author: Peter Spirtes</p><p>Abstract: The goal of many sciences is to understand the mechanisms by which variables came to take on the values they have (that is, to ﬁnd a generative model), and to predict what the values of those variables would be if the naturally occurring mechanisms were subject to outside manipulations. The past 30 years has seen a number of conceptual developments that are partial solutions to the problem of causal inference from observational sample data or a mixture of observational sample and experimental data, particularly in the area of graphical causal modeling. However, in many domains, problems such as the large numbers of variables, small samples sizes, and possible presence of unmeasured causes, remain serious impediments to practical applications of these developments. The articles in the Special Topic on Causality address these and other problems in applying graphical causal modeling algorithms. This introduction to the Special Topic on Causality provides a brief introduction to graphical causal modeling, places the articles in a broader context, and describes the differences between causal inference and ordinary machine learning classiﬁcation and prediction problems. Keywords: Bayesian networks, causation, causal inference</p><p>4 0.17096773 <a title="67-tfidf-4" href="./jmlr-2010-Estimation_of_a_Structural_Vector_Autoregression_Model_Using_Non-Gaussianity.html">36 jmlr-2010-Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity</a></p>
<p>Author: Aapo Hyvärinen, Kun Zhang, Shohei Shimizu, Patrik O. Hoyer</p><p>Abstract: Analysis of causal effects between continuous-valued variables typically uses either autoregressive models or structural equation models with instantaneous effects. Estimation of Gaussian, linear structural equation models poses serious identiﬁability problems, which is why it was recently proposed to use non-Gaussian models. Here, we show how to combine the non-Gaussian instantaneous model with autoregressive models. This is effectively what is called a structural vector autoregression (SVAR) model, and thus our work contributes to the long-standing problem of how to estimate SVAR’s. We show that such a non-Gaussian model is identiﬁable without prior knowledge of network structure. We propose computationally efﬁcient methods for estimating the model, as well as methods to assess the signiﬁcance of the causal inﬂuences. The model is successfully applied on ﬁnancial and brain imaging data. Keywords: structural vector autoregression, structural equation models, independent component analysis, non-Gaussianity, causality</p><p>5 0.13723412 <a title="67-tfidf-5" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>Author: Shyam Visweswaran, Gregory F. Cooper</p><p>Abstract: This paper introduces a Bayesian algorithm for constructing predictive models from data that are optimized to predict a target variable well for a particular instance. This algorithm learns Markov blanket models, carries out Bayesian model averaging over a set of models to predict a target variable of the instance at hand, and employs an instance-speciﬁc heuristic to locate a set of suitable models to average over. We call this method the instance-speciﬁc Markov blanket (ISMB) algorithm. The ISMB algorithm was evaluated on 21 UCI data sets using ﬁve different performance measures and its performance was compared to that of several commonly used predictive algorithms, including nave Bayes, C4.5 decision tree, logistic regression, neural networks, k-Nearest Neighbor, Lazy Bayesian Rules, and AdaBoost. Over all the data sets, the ISMB algorithm performed better on average on all performance measures against all the comparison algorithms. Keywords: instance-speciﬁc, Bayesian network, Markov blanket, Bayesian model averaging</p><p>6 0.084309772 <a title="67-tfidf-6" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>7 0.056825276 <a title="67-tfidf-7" href="./jmlr-2010-Matched_Gene_Selection_and_Committee_Classifier_for_Molecular_Classification_of_Heterogeneous_Diseases.html">71 jmlr-2010-Matched Gene Selection and Committee Classifier for Molecular Classification of Heterogeneous Diseases</a></p>
<p>8 0.053145923 <a title="67-tfidf-8" href="./jmlr-2010-Optimal_Search_on_Clustered_Structural_Constraint_for_Learning_Bayesian_Network_Structure.html">88 jmlr-2010-Optimal Search on Clustered Structural Constraint for Learning Bayesian Network Structure</a></p>
<p>9 0.049821034 <a title="67-tfidf-9" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>10 0.047036897 <a title="67-tfidf-10" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>11 0.046710785 <a title="67-tfidf-11" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>12 0.043895498 <a title="67-tfidf-12" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>13 0.037873887 <a title="67-tfidf-13" href="./jmlr-2010-Continuous_Time_Bayesian_Network_Reasoning_and_Learning_Engine.html">28 jmlr-2010-Continuous Time Bayesian Network Reasoning and Learning Engine</a></p>
<p>14 0.036447499 <a title="67-tfidf-14" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>15 0.035866044 <a title="67-tfidf-15" href="./jmlr-2010-On_Over-fitting_in_Model_Selection_and_Subsequent_Selection_Bias_in_Performance_Evaluation.html">83 jmlr-2010-On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</a></p>
<p>16 0.034171324 <a title="67-tfidf-16" href="./jmlr-2010-Importance_Sampling_for_Continuous_Time_Bayesian_Networks.html">51 jmlr-2010-Importance Sampling for Continuous Time Bayesian Networks</a></p>
<p>17 0.032228164 <a title="67-tfidf-17" href="./jmlr-2010-Inducing_Tree-Substitution_Grammars.html">53 jmlr-2010-Inducing Tree-Substitution Grammars</a></p>
<p>18 0.030048074 <a title="67-tfidf-18" href="./jmlr-2010-Mean_Field_Variational_Approximation_for_Continuous-Time_Bayesian_Networks.html">75 jmlr-2010-Mean Field Variational Approximation for Continuous-Time Bayesian Networks</a></p>
<p>19 0.030038033 <a title="67-tfidf-19" href="./jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">48 jmlr-2010-How to Explain Individual Classification Decisions</a></p>
<p>20 0.029640673 <a title="67-tfidf-20" href="./jmlr-2010-Graph_Kernels.html">44 jmlr-2010-Graph Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.23), (1, 0.721), (2, 0.385), (3, -0.007), (4, 0.039), (5, -0.112), (6, -0.042), (7, -0.0), (8, 0.053), (9, -0.003), (10, 0.0), (11, -0.042), (12, 0.017), (13, 0.023), (14, 0.01), (15, 0.041), (16, -0.008), (17, -0.051), (18, 0.016), (19, 0.012), (20, -0.035), (21, -0.0), (22, 0.029), (23, 0.025), (24, -0.006), (25, -0.039), (26, -0.01), (27, -0.113), (28, 0.019), (29, -0.015), (30, -0.001), (31, 0.023), (32, -0.04), (33, 0.068), (34, -0.046), (35, 0.025), (36, -0.005), (37, 0.022), (38, -0.019), (39, -0.047), (40, 0.044), (41, 0.006), (42, 0.015), (43, 0.055), (44, 0.011), (45, 0.013), (46, 0.044), (47, 0.006), (48, 0.001), (49, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93874443 <a title="67-lsi-1" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_I%3A_Algorithms_and_Empirical_Evaluation.html">67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</a></p>
<p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: We present an algorithmic framework for learning local causal structure around target variables of interest in the form of direct causes/effects and Markov blankets applicable to very large data sets with relatively small samples. The selected feature sets can be used for causal discovery and classiÄ?Ĺš cation. The framework (Generalized Local Learning, or GLL) can be instantiated in numerous ways, giving rise to both existing state-of-the-art as well as novel algorithms. The resulting algorithms are sound under well-deÄ?Ĺš ned sufÄ?Ĺš cient conditions. In a Ä?Ĺš rst set of experiments we evaluate several algorithms derived from this framework in terms of predictivity and feature set parsimony and compare to other local causal discovery methods and to state-of-the-art non-causal feature selection methods using real data. A second set of experimental evaluations compares the algorithms in terms of ability to induce local causal neighborhoods using simulated and resimulated data and examines the relation of predictivity with causal induction performance. Our experiments demonstrate, consistently with causal feature selection theory, that local causal feature selection methods (under broad assumptions encompassing appropriate family of distribuc 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS tions, types of classiÄ?Ĺš ers, and loss functions) exhibit strong feature set parsimony, high predictivity and local causal interpretability. Although non-causal feature selection methods are often used in practice to shed light on causal relationships, we Ä?Ĺš nd that they cannot be interpreted causally even when they achieve excellent predictivity. Therefore we conclude that only local causal techniques should be used when insight into causal structure is sought. In a companion paper we examine in depth the behavior of GLL algorithms, provide extensions, and show</p><p>2 0.92900497 <a title="67-lsi-2" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_II%3A_Analysis_and_Extensions.html">68 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions</a></p>
<p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: In part I of this work we introduced and evaluated the Generalized Local Learning (GLL) framework for producing local causal and Markov blanket induction algorithms. In the present second part we analyze the behavior of GLL algorithms and provide extensions to the core methods. SpeciÄ?Ĺš cally, we investigate the empirical convergence of GLL to the true local neighborhood as a function of sample size. Moreover, we study how predictivity improves with increasing sample size. Then we investigate how sensitive are the algorithms to multiple statistical testing, especially in the presence of many irrelevant features. Next we discuss the role of the algorithm parameters and also show that Markov blanket and causal graph concepts can be used to understand deviations from optimality of state-of-the-art non-causal algorithms. The present paper also introduces the following extensions to the core GLL framework: parallel and distributed versions of GLL algorithms, versions with false discovery rate control, strategies for constructing novel heuristics for speciÄ?Ĺš c domains, and divide-and-conquer local-to-global learning (LGL) strategies. We test the generality of the LGL approach by deriving a novel LGL-based algorithm that compares favorably c 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS to the state-of-the-art global learning algorithms. In addition, we investigate the use of non-causal feature selection methods to facilitate global learning. Open problems and future research paths related to local and local-to-global causal learning are discussed. Keywords: local causal discovery, Markov blanket induction, feature selection, classiÄ?Ĺš cation, causal structure learning, learning of Bayesian networks</p><p>3 0.77022743 <a title="67-lsi-3" href="./jmlr-2010-Introduction_to_Causal_Inference.html">56 jmlr-2010-Introduction to Causal Inference</a></p>
<p>Author: Peter Spirtes</p><p>Abstract: The goal of many sciences is to understand the mechanisms by which variables came to take on the values they have (that is, to ﬁnd a generative model), and to predict what the values of those variables would be if the naturally occurring mechanisms were subject to outside manipulations. The past 30 years has seen a number of conceptual developments that are partial solutions to the problem of causal inference from observational sample data or a mixture of observational sample and experimental data, particularly in the area of graphical causal modeling. However, in many domains, problems such as the large numbers of variables, small samples sizes, and possible presence of unmeasured causes, remain serious impediments to practical applications of these developments. The articles in the Special Topic on Causality address these and other problems in applying graphical causal modeling algorithms. This introduction to the Special Topic on Causality provides a brief introduction to graphical causal modeling, places the articles in a broader context, and describes the differences between causal inference and ordinary machine learning classiﬁcation and prediction problems. Keywords: Bayesian networks, causation, causal inference</p><p>4 0.45630285 <a title="67-lsi-4" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>Author: Shyam Visweswaran, Gregory F. Cooper</p><p>Abstract: This paper introduces a Bayesian algorithm for constructing predictive models from data that are optimized to predict a target variable well for a particular instance. This algorithm learns Markov blanket models, carries out Bayesian model averaging over a set of models to predict a target variable of the instance at hand, and employs an instance-speciﬁc heuristic to locate a set of suitable models to average over. We call this method the instance-speciﬁc Markov blanket (ISMB) algorithm. The ISMB algorithm was evaluated on 21 UCI data sets using ﬁve different performance measures and its performance was compared to that of several commonly used predictive algorithms, including nave Bayes, C4.5 decision tree, logistic regression, neural networks, k-Nearest Neighbor, Lazy Bayesian Rules, and AdaBoost. Over all the data sets, the ISMB algorithm performed better on average on all performance measures against all the comparison algorithms. Keywords: instance-speciﬁc, Bayesian network, Markov blanket, Bayesian model averaging</p><p>5 0.37500867 <a title="67-lsi-5" href="./jmlr-2010-Estimation_of_a_Structural_Vector_Autoregression_Model_Using_Non-Gaussianity.html">36 jmlr-2010-Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity</a></p>
<p>Author: Aapo Hyvärinen, Kun Zhang, Shohei Shimizu, Patrik O. Hoyer</p><p>Abstract: Analysis of causal effects between continuous-valued variables typically uses either autoregressive models or structural equation models with instantaneous effects. Estimation of Gaussian, linear structural equation models poses serious identiﬁability problems, which is why it was recently proposed to use non-Gaussian models. Here, we show how to combine the non-Gaussian instantaneous model with autoregressive models. This is effectively what is called a structural vector autoregression (SVAR) model, and thus our work contributes to the long-standing problem of how to estimate SVAR’s. We show that such a non-Gaussian model is identiﬁable without prior knowledge of network structure. We propose computationally efﬁcient methods for estimating the model, as well as methods to assess the signiﬁcance of the causal inﬂuences. The model is successfully applied on ﬁnancial and brain imaging data. Keywords: structural vector autoregression, structural equation models, independent component analysis, non-Gaussianity, causality</p><p>6 0.21899341 <a title="67-lsi-6" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>7 0.20856529 <a title="67-lsi-7" href="./jmlr-2010-Optimal_Search_on_Clustered_Structural_Constraint_for_Learning_Bayesian_Network_Structure.html">88 jmlr-2010-Optimal Search on Clustered Structural Constraint for Learning Bayesian Network Structure</a></p>
<p>8 0.18698806 <a title="67-lsi-8" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>9 0.18615411 <a title="67-lsi-9" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>10 0.18277359 <a title="67-lsi-10" href="./jmlr-2010-Matched_Gene_Selection_and_Committee_Classifier_for_Molecular_Classification_of_Heterogeneous_Diseases.html">71 jmlr-2010-Matched Gene Selection and Committee Classifier for Molecular Classification of Heterogeneous Diseases</a></p>
<p>11 0.1587408 <a title="67-lsi-11" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>12 0.15111956 <a title="67-lsi-12" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>13 0.14894943 <a title="67-lsi-13" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>14 0.14782725 <a title="67-lsi-14" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>15 0.13958544 <a title="67-lsi-15" href="./jmlr-2010-On_Over-fitting_in_Model_Selection_and_Subsequent_Selection_Bias_in_Performance_Evaluation.html">83 jmlr-2010-On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</a></p>
<p>16 0.13289274 <a title="67-lsi-16" href="./jmlr-2010-Quadratic_Programming_Feature_Selection.html">94 jmlr-2010-Quadratic Programming Feature Selection</a></p>
<p>17 0.13029312 <a title="67-lsi-17" href="./jmlr-2010-Continuous_Time_Bayesian_Network_Reasoning_and_Learning_Engine.html">28 jmlr-2010-Continuous Time Bayesian Network Reasoning and Learning Engine</a></p>
<p>18 0.12746669 <a title="67-lsi-18" href="./jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">48 jmlr-2010-How to Explain Individual Classification Decisions</a></p>
<p>19 0.12436397 <a title="67-lsi-19" href="./jmlr-2010-Learning_Gradients%3A_Predictive_Models_that_Infer_Geometry_and_Statistical_Dependence.html">62 jmlr-2010-Learning Gradients: Predictive Models that Infer Geometry and Statistical Dependence</a></p>
<p>20 0.12153532 <a title="67-lsi-20" href="./jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization.html">54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.012), (4, 0.018), (8, 0.019), (18, 0.016), (21, 0.017), (24, 0.013), (30, 0.014), (32, 0.041), (33, 0.276), (36, 0.039), (37, 0.032), (38, 0.213), (75, 0.103), (81, 0.016), (85, 0.052), (96, 0.02), (97, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.74630529 <a title="67-lda-1" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_II%3A_Analysis_and_Extensions.html">68 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions</a></p>
<p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: In part I of this work we introduced and evaluated the Generalized Local Learning (GLL) framework for producing local causal and Markov blanket induction algorithms. In the present second part we analyze the behavior of GLL algorithms and provide extensions to the core methods. SpeciÄ?Ĺš cally, we investigate the empirical convergence of GLL to the true local neighborhood as a function of sample size. Moreover, we study how predictivity improves with increasing sample size. Then we investigate how sensitive are the algorithms to multiple statistical testing, especially in the presence of many irrelevant features. Next we discuss the role of the algorithm parameters and also show that Markov blanket and causal graph concepts can be used to understand deviations from optimality of state-of-the-art non-causal algorithms. The present paper also introduces the following extensions to the core GLL framework: parallel and distributed versions of GLL algorithms, versions with false discovery rate control, strategies for constructing novel heuristics for speciÄ?Ĺš c domains, and divide-and-conquer local-to-global learning (LGL) strategies. We test the generality of the LGL approach by deriving a novel LGL-based algorithm that compares favorably c 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS to the state-of-the-art global learning algorithms. In addition, we investigate the use of non-causal feature selection methods to facilitate global learning. Open problems and future research paths related to local and local-to-global causal learning are discussed. Keywords: local causal discovery, Markov blanket induction, feature selection, classiÄ?Ĺš cation, causal structure learning, learning of Bayesian networks</p><p>same-paper 2 0.70197731 <a title="67-lda-2" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_I%3A_Algorithms_and_Empirical_Evaluation.html">67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</a></p>
<p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: We present an algorithmic framework for learning local causal structure around target variables of interest in the form of direct causes/effects and Markov blankets applicable to very large data sets with relatively small samples. The selected feature sets can be used for causal discovery and classiÄ?Ĺš cation. The framework (Generalized Local Learning, or GLL) can be instantiated in numerous ways, giving rise to both existing state-of-the-art as well as novel algorithms. The resulting algorithms are sound under well-deÄ?Ĺš ned sufÄ?Ĺš cient conditions. In a Ä?Ĺš rst set of experiments we evaluate several algorithms derived from this framework in terms of predictivity and feature set parsimony and compare to other local causal discovery methods and to state-of-the-art non-causal feature selection methods using real data. A second set of experimental evaluations compares the algorithms in terms of ability to induce local causal neighborhoods using simulated and resimulated data and examines the relation of predictivity with causal induction performance. Our experiments demonstrate, consistently with causal feature selection theory, that local causal feature selection methods (under broad assumptions encompassing appropriate family of distribuc 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS tions, types of classiÄ?Ĺš ers, and loss functions) exhibit strong feature set parsimony, high predictivity and local causal interpretability. Although non-causal feature selection methods are often used in practice to shed light on causal relationships, we Ä?Ĺš nd that they cannot be interpreted causally even when they achieve excellent predictivity. Therefore we conclude that only local causal techniques should be used when insight into causal structure is sought. In a companion paper we examine in depth the behavior of GLL algorithms, provide extensions, and show</p><p>3 0.66718602 <a title="67-lda-3" href="./jmlr-2010-Estimation_of_a_Structural_Vector_Autoregression_Model_Using_Non-Gaussianity.html">36 jmlr-2010-Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity</a></p>
<p>Author: Aapo Hyvärinen, Kun Zhang, Shohei Shimizu, Patrik O. Hoyer</p><p>Abstract: Analysis of causal effects between continuous-valued variables typically uses either autoregressive models or structural equation models with instantaneous effects. Estimation of Gaussian, linear structural equation models poses serious identiﬁability problems, which is why it was recently proposed to use non-Gaussian models. Here, we show how to combine the non-Gaussian instantaneous model with autoregressive models. This is effectively what is called a structural vector autoregression (SVAR) model, and thus our work contributes to the long-standing problem of how to estimate SVAR’s. We show that such a non-Gaussian model is identiﬁable without prior knowledge of network structure. We propose computationally efﬁcient methods for estimating the model, as well as methods to assess the signiﬁcance of the causal inﬂuences. The model is successfully applied on ﬁnancial and brain imaging data. Keywords: structural vector autoregression, structural equation models, independent component analysis, non-Gaussianity, causality</p><p>4 0.6229127 <a title="67-lda-4" href="./jmlr-2010-WEKA%E2%88%92Experiences_with_a_Java_Open-Source_Project.html">116 jmlr-2010-WEKA−Experiences with a Java Open-Source Project</a></p>
<p>Author: Remco R. Bouckaert, Eibe Frank, Mark A. Hall, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, Ian H. Witten</p><p>Abstract: WEKA is a popular machine learning workbench with a development life of nearly two decades. This article provides an overview of the factors that we believe to be important to its success. Rather than focussing on the software’s functionality, we review aspects of project management and historical development decisions that likely had an impact on the uptake of the project. Keywords: machine learning software, open source software</p><p>5 0.60073102 <a title="67-lda-5" href="./jmlr-2010-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">79 jmlr-2010-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>Author: Thomas Jaksch, Ronald Ortner, Peter Auer</p><p>Abstract: For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s, s′ there is a policy which moves from s to s′ in at most D steps (on average). √ ˜ We present a reinforcement learning algorithm with total regret O(DS AT ) after T steps for any unknown MDP with S states, A actions per state, and diameter D. A corresponding lower bound of √ Ω( DSAT ) on the total regret of any learning algorithm is given as well. These results are complemented by a sample complexity bound on the number of suboptimal steps taken by our algorithm. This bound can be used to achieve a (gap-dependent) regret bound that is logarithmic in T . Finally, we also consider a setting where the MDP is allowed to change a ﬁxed number of ℓ times. We present a modiﬁcation of our algorithm that is able to deal with this setting and show a √ ˜ regret bound of O(ℓ1/3 T 2/3 DS A). Keywords: undiscounted reinforcement learning, Markov decision process, regret, online learning, sample complexity</p><p>6 0.46088704 <a title="67-lda-6" href="./jmlr-2010-Introduction_to_Causal_Inference.html">56 jmlr-2010-Introduction to Causal Inference</a></p>
<p>7 0.38016844 <a title="67-lda-7" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>8 0.37902018 <a title="67-lda-8" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>9 0.3732501 <a title="67-lda-9" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>10 0.37244755 <a title="67-lda-10" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>11 0.37127087 <a title="67-lda-11" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>12 0.36614871 <a title="67-lda-12" href="./jmlr-2010-Bayesian_Learning_in_Sparse_Graphical_Factor_Models_via_Variational_Mean-Field_Annealing.html">17 jmlr-2010-Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing</a></p>
<p>13 0.36530423 <a title="67-lda-13" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>14 0.36314902 <a title="67-lda-14" href="./jmlr-2010-Evolving_Static_Representations_for_Task_Transfer.html">37 jmlr-2010-Evolving Static Representations for Task Transfer</a></p>
<p>15 0.36104247 <a title="67-lda-15" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>16 0.35812303 <a title="67-lda-16" href="./jmlr-2010-An_Investigation_of_Missing_Data_Methods_for_Classification_Trees_Applied_to_Binary_Response_Data.html">11 jmlr-2010-An Investigation of Missing Data Methods for Classification Trees Applied to Binary Response Data</a></p>
<p>17 0.35352927 <a title="67-lda-17" href="./jmlr-2010-Matched_Gene_Selection_and_Committee_Classifier_for_Molecular_Classification_of_Heterogeneous_Diseases.html">71 jmlr-2010-Matched Gene Selection and Committee Classifier for Molecular Classification of Heterogeneous Diseases</a></p>
<p>18 0.35304677 <a title="67-lda-18" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>19 0.35278302 <a title="67-lda-19" href="./jmlr-2010-Importance_Sampling_for_Continuous_Time_Bayesian_Networks.html">51 jmlr-2010-Importance Sampling for Continuous Time Bayesian Networks</a></p>
<p>20 0.35165626 <a title="67-lda-20" href="./jmlr-2010-A_Convergent_Online_Single_Time_Scale_Actor_Critic_Algorithm.html">2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
