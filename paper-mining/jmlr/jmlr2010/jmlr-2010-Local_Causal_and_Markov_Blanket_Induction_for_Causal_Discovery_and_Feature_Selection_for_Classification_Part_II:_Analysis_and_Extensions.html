<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-68" href="#">jmlr2010-68</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>68 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions</h1>
<br/><p>Source: <a title="jmlr-2010-68-pdf" href="http://jmlr.org/papers/volume11/aliferis10b/aliferis10b.pdf">pdf</a></p><p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: In part I of this work we introduced and evaluated the Generalized Local Learning (GLL) framework for producing local causal and Markov blanket induction algorithms. In the present second part we analyze the behavior of GLL algorithms and provide extensions to the core methods. SpeciÄ?Ĺš cally, we investigate the empirical convergence of GLL to the true local neighborhood as a function of sample size. Moreover, we study how predictivity improves with increasing sample size. Then we investigate how sensitive are the algorithms to multiple statistical testing, especially in the presence of many irrelevant features. Next we discuss the role of the algorithm parameters and also show that Markov blanket and causal graph concepts can be used to understand deviations from optimality of state-of-the-art non-causal algorithms. The present paper also introduces the following extensions to the core GLL framework: parallel and distributed versions of GLL algorithms, versions with false discovery rate control, strategies for constructing novel heuristics for speciÄ?Ĺš c domains, and divide-and-conquer local-to-global learning (LGL) strategies. We test the generality of the LGL approach by deriving a novel LGL-based algorithm that compares favorably c 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS to the state-of-the-art global learning algorithms. In addition, we investigate the use of non-causal feature selection methods to facilitate global learning. Open problems and future research paths related to local and local-to-global causal learning are discussed. Keywords: local causal discovery, Markov blanket induction, feature selection, classiÄ?Ĺš cation, causal structure learning, learning of Bayesian networks</p><p>Reference: <a title="jmlr-2010-68-reference" href="../jmlr2010_reference/jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_II%3A_Analysis_and_Extensions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Keywords: local causal discovery, Markov blanket induction, feature selection, classiÄ? [sent-30, score-0.427]
</p><p>2 Ĺš cation of highly predictive and parsimonious feature sets (feature selection problem), and for scaling up causal discovery. [sent-38, score-0.34]
</p><p>3 Moreover, state-of-the-art non-causal feature selection methods often achieve excellent predictivity but are misleading in terms of causal discovery. [sent-41, score-0.402]
</p><p>4 Section 7 uses causal feature selection theory to shed light on limitations of established and newer feature selection methods and the inappropriateness of causally interpreting their output. [sent-58, score-0.471]
</p><p>5 We will show that GLL algorithms have inherent control to false positives due to multiple comparisons while the same is not true for other non-causal feature selection methods tested. [sent-185, score-0.43]
</p><p>6 Ĺš cance level, that is probability that a truly null hypothesis is rejected, thus falsely concluding that a statistical difference or association or dependence exists when in reality it does not) it is expected that Ă&#x17D;Ä&hellip;Ă&sbquo;Ë&Dagger;n false positives will occur on average. [sent-190, score-0.375]
</p><p>7 Ĺš cant features (namely the features that are truly differentially expressed and detectable at Ă&#x17D;Ä&hellip; but nondetectable at Ă&#x17D;Ä&hellip;/n), hence creates false negatives that were not present before the correction. [sent-202, score-0.369]
</p><p>8 This is highly useful in exploratory analysis of high-dimensional data where subsequent experimentation can sort out false positives easily but where false negatives have high cost. [sent-213, score-0.632]
</p><p>9 Constraint-based causal methods employ, in large data sets and depending on connectivity and inclusion heuristic efÄ? [sent-214, score-0.388]
</p><p>10 Ĺš rst, testing under the null hypothesis does not only occur when irrelevant features exist but also whenever we test weakly relevant features conditioned on a set of variables that blocks all paths connecting it with the target. [sent-217, score-0.346]
</p><p>11 Other feature selection methods do not explicitly conduct statistical tests of independence but may also be sensitive to many irrelevant features as we will show. [sent-218, score-0.387]
</p><p>12 Ĺš rst systematically explore empirically and then examine theoretically the degree of sensitivity of GLL algorithms to irrelevant features, how they address the multiple testing problem, and how other feature selection and causal discovery algorithms compare along these dimensions. [sent-220, score-0.486]
</p><p>13 In the Lung Cancer network we focused our attention on the natural target variable; this target has 26 members of the parents and children set and 18 spouses, 14 irrelevant variables, and 741 weakly relevant ones. [sent-229, score-0.476]
</p><p>14 The color of each table cell denotes number of false positives with yellow (light) corresponding to smaller values and red (dark) to larger ones. [sent-242, score-0.343]
</p><p>15 false positives that are irrelevant and total false positives. [sent-243, score-0.639]
</p><p>16 Ĺš cation performance is mildly or not affected by false positives and false negatives (Table 1). [sent-246, score-0.632]
</p><p>17 When many false negatives are present, predictivity is compensated by the few remaining strong relevant features plus strongly predictive weakly relevant ones. [sent-247, score-0.652]
</p><p>18 (b) As expected, false negatives are reduced as sample size grows (because power increases), however they also increase as max-k grows, because the number of tests increases as max-k grows and thus overall power decreases (Table 2). [sent-250, score-0.444]
</p><p>19 (c) When no irrelevant features are present, as sample size grows the number of false positives that are weakly relevant increases if max-k is not sufÄ? [sent-251, score-0.682]
</p><p>20 As max-k increases the false positives decrease to the point that they vanish (Table 3). [sent-253, score-0.343]
</p><p>21 The color of each table cell denotes number of false positives with yellow (light) corresponding to smaller values and red (dark) to larger ones. [sent-255, score-0.343]
</p><p>22 (d) When irrelevant features are present, as sample size grows the number of false positives that are weakly relevant increases if max-k is not sufÄ? [sent-260, score-0.682]
</p><p>23 As max-k increases, the false positives decrease to the point that they vanish (Table 3). [sent-262, score-0.343]
</p><p>24 Thus, overall, with enough sample size and right value of max-k, both false negatives and false positives vanish (Tables 2 and 4). [sent-265, score-0.665]
</p><p>25 (e) When the predictive signal is weaker, both false negatives are increased and false positives within weakly relevant variables are decreased for a given sample size (because power is smaller) (Tables 2 and 3). [sent-266, score-0.826]
</p><p>26 This is due to the fact that fewer features enter the TPC(T ) set thus leading to fewer tests that can be performed hence smaller capacity to remove irrelevant false positives. [sent-268, score-0.458]
</p><p>27 As previously with enough sample and right max-k, false positives and negatives are fully eliminated (Tables 2 and 4). [sent-269, score-0.474]
</p><p>28 (f) When the data consists only of irrelevant features, false positives (irrelevant) are reduced as max-k increases for all sample sizes (Table 5). [sent-270, score-0.481]
</p><p>29 There is a very small persistent residual number of false positives regardless of how small the sample is or how big the max-k. [sent-271, score-0.376]
</p><p>30 The color of each table cell denotes number of false positives with yellow (light) corresponding to smaller values and red (dark) to larger ones. [sent-273, score-0.343]
</p><p>31 , tentative parents and children of T ) in order to execute conditional independence tests and remove the false positive irrelevant features. [sent-277, score-0.555]
</p><p>32 (i) When no irrelevant features are present and in the stronger signal setting, simple and FDRcorrected UAF (but not wrapped UAF) has the least false negatives in very small samples (Table 7). [sent-285, score-0.434]
</p><p>33 , 1-2 false negatives and zero false positives) at sample size 1,000 and higher (Table 8). [sent-307, score-0.513]
</p><p>34 No other method simultaneously minimizes false positives and false negatives as GLL. [sent-308, score-0.632]
</p><p>35 (j) In the setting of strong signal with irrelevant features, simple UAF has the least false negatives in very small samples (Table 7) and the largest number of false positives (Table 8). [sent-309, score-0.737]
</p><p>36 (k) When the predictive signal is weaker, false negatives are increased and weakly relevant false positives are decreased for a given sample size compared to the stronger signal case (Tables 7 and 8). [sent-310, score-0.826]
</p><p>37 Simple UAF is again most sensitive in terms of detecting strongly relevant features in smaller samples until sample size 1,000-2,000 where UAF-Bonferroni and UAF-FDR and GLL match the false negative rates (Table 7). [sent-311, score-0.364]
</p><p>38 The color of each table cell denotes number of false positives with yellow (light) corresponding to smaller values and red (dark) to larger ones. [sent-319, score-0.343]
</p><p>39 MMPC performing similarly) achieves excellent false positive rates better than those by FDR not only for weakly relevant but also for irrelevant features. [sent-320, score-0.457]
</p><p>40 Ĺš ltering behaves almost identically as regular HITONPC except for the case with only irrelevant features in the data where HITON-PC without FDR admits a few false positives (Table 9). [sent-322, score-0.488]
</p><p>41 The color of each table cell denotes number of false positives with yellow (light) corresponding to smaller values and red (dark) to larger ones. [sent-331, score-0.343]
</p><p>42 Parameter max-k controls the false positives due to both weakly relevant and irrelevant features. [sent-334, score-0.609]
</p><p>43 Both established feature selectors such as variants of UAF and newer ones are very sensitive to irrelevant features and produce large numbers of false positives. [sent-337, score-0.393]
</p><p>44 Although these tests are highly correlated and combined power is larger than the product of powers of the same set of tests performed on independent samples, still the more tests are executed the smaller the combined power and the larger the possibility of falsely eliminating S becomes. [sent-377, score-0.366]
</p><p>45 Fourth, the order of executing the tests and constructing conditioning sets is important for reducing the number of tests performed on strongly relevant variables. [sent-401, score-0.392]
</p><p>46 , 5% or smaller) and that as the number of tests under the null increases, the combined Ă&#x17D;Ä&hellip; drops up to exponentially fast, and eliminating weakly relevant features occurs with high probability as the number of applied tests increases. [sent-414, score-0.445]
</p><p>47 In terms of rejecting a weakly relevant feature in TPC(T ), the larger max-k and the smaller h-ps become, the easier it is to eliminate a weakly relevant feature. [sent-416, score-0.379]
</p><p>48 Ĺš lter (Benjamini and Yekutieli, 2001; Benjamini and Hochberg, 1995), we not only gain the security that if the data consists exclusively of irrelevant variables fewer or no false positives will be returned, but also we can use max-k to control sensitivity and speciÄ? [sent-424, score-0.448]
</p><p>49 Ĺš city trading weakly relevant false positives for strongly relevant true positives and vice versa (i. [sent-425, score-0.756]
</p><p>50 When h-ps is very small, tests are allowed with very large conditioning tests and as long as max-k does not disallow them, the total number of tests grow very large. [sent-434, score-0.414]
</p><p>51 , strongly relevant features, or unblocked weakly relevant ones) requires measuring all relevant associations, whereas HITON requires just the univariate ones for inclusion purposes. [sent-445, score-0.438]
</p><p>52 For example, univariate tests have cost = 1, tests with conditioning on two variables have cost = 3. [sent-462, score-0.364]
</p><p>53 4 Inductive Bias of GLL Informally the inductive bias of GLL is that it seeks a balance of false negatives for strongly relevant variables with false positives for weakly relevant and irrelevant variables. [sent-502, score-0.998]
</p><p>54 Smaller max-k empirically decreases false negatives and increases false positives overall. [sent-506, score-0.632]
</p><p>55 Larger max-k increases the false negatives and decreases the false positives. [sent-507, score-0.48]
</p><p>56 GLL in moderate to large samples achieves small numbers of false negatives and small numbers of false positives. [sent-508, score-0.48]
</p><p>57 In very small samples GLL prefers false positive errors than false negative ones when max-k is small. [sent-509, score-0.382]
</p><p>58 Notice that as max-k grows many more tests can be executed provided that a liberal h-ps is chosen, and these tests can be used to eliminate both weakly relevant as well as strongly relevant features in TPC(T ). [sent-513, score-0.545]
</p><p>59 The choice of a more liberal h-ps default value in GLL (compared to the more stringent value in the published implementation of PC algorithm) allows a more effective control of the tradeoff between false positives and false negatives in small samples by changing values of max-k. [sent-514, score-0.632]
</p><p>60 Bayesian scoring methods in small samples are dominated by their priors and typically they prefer sparse networks which lead to fewer false positives and more false negatives. [sent-519, score-0.534]
</p><p>61 Ĺš&sbquo;ection of the inductive bias of GLL which prefers to admit potential false positives if they cannot be shown for sample size reasons to be independent of the target. [sent-558, score-0.376]
</p><p>62 The numbers of strongly relevant, weakly relevant and irrelevant features is not critical to the existence of the problem, neither is the type of wrapper (forward, backward, forward-backward, GA, etc. [sent-621, score-0.343]
</p><p>63 Ĺš ltering relies little on error estimation3 and uses robust mechanisms to control false negatives and false positives separately for strongly relevant, weakly relevant and irrelevant features respectively. [sent-624, score-0.975]
</p><p>64 Ĺš ed example with just one strongly irrelevant feature inside TPC(T ), each irrelevant feature has probability of entering and staying in TPC(T ) of at most Ă&#x17D;Ä&hellip;2 = 0. [sent-631, score-0.361]
</p><p>65 2010) and some false positives which cannot be eliminated without conditioning on PC(T ) members that belong to another chunk. [sent-680, score-0.429]
</p><p>66 is the best causal feature selection algorithm among techniques that do not incorporate FDR. [sent-742, score-0.34]
</p><p>67 improve the number of false positives in HITON-PC and MMPC. [sent-747, score-0.343]
</p><p>68 Since however the algorithm exhibits small sensitivity to false positives due to multiple comparisons when many irrelevant features are expected and few relevant features are present, we recommend pre-Ä? [sent-760, score-0.591]
</p><p>69 We remind that a major motivation for pursuing local causal learning methods is scaling up causal discovery and causal feature selection as explained in Aliferis et al. [sent-783, score-0.924]
</p><p>70 Ĺš ciency An important parameter of local-to-global learning previously unnoticed in algorithms such as SCA and MMHC is the ordering of variables when executing the local causal discovery variable-byvariable (i. [sent-830, score-0.362]
</p><p>71 4 Using non-Causal Feature Selection for Global Learning In recent years several researchers have proposed that because modern feature selection methods can deal with large dimensionality/small sample data sets, they could also be used to speed up or approximate large scale causal discovery (e. [sent-919, score-0.414]
</p><p>72 Ĺš nd encouraging evidence that non-causal feature selection can be used as an adjunct to global causal discovery. [sent-946, score-0.34]
</p><p>73 , does the algorithm exhibit false positives and false negatives relative to minimal feature set that yields optimal predictivity? [sent-990, score-0.689]
</p><p>74 , whether the feature selection output is locally causally correct), and when this is not obtained, we examine whether some other useful causal inference can be made. [sent-996, score-0.384]
</p><p>75 However, because univariate associations of non-MB(T ) members can be higher than those of members, false positives are incurred when selecting features using univariate association-based Ä? [sent-1009, score-0.565]
</p><p>76 The embedded table shows the false positives and false negatives (relative to the gold standard set MB(T )) at each possible threshold for variable inclusion. [sent-1013, score-0.632]
</p><p>77 From the causal discovery perspective, the example makes evident that non-causally relevant features such as A and B can be selected with higher ranking than causally relevant ones such as D and E. [sent-1015, score-0.504]
</p><p>78 Ĺš nding the original features that are important and non-redundant the method leads to false positives (since the coefÄ? [sent-1025, score-0.383]
</p><p>79 Application of causal learning via the usual assumptions and procedures reveals that X is a direct cause or effect of T and that Y is not directly causally linked with T (the requisite conditional independence tests are depicted). [sent-1030, score-0.452]
</p><p>80 (2006) it was found that weights of irrelevant features occasionally exceed those of weakly relevant features and furthermore that SVM weights are also susceptible to assigning larger weights to synthesis features rather than direct causes and effects. [sent-1063, score-0.42]
</p><p>81 A causal discovery procedure such as HITON-PC given enough sample and a suitable statistical test of independence will discover {A, B} as the correct set of direct causes and direct effects. [sent-1071, score-0.36]
</p><p>82 Ĺš erĂ˘&euro;&trade;s inductive bias (as wrapping does) can be an obstacle to sound causal discovery. [sent-1085, score-0.345]
</p><p>83 It is shown that these algorithms can admit false positives and false negatives both predictively and causally with respect to the target variable neighborhood. [sent-1109, score-0.71]
</p><p>84 Ĺš nding was that GLL algorithms exhibit strong intrinsic control of false positives due not only to weakly relevant but also due to irrelevant features. [sent-1119, score-0.609]
</p><p>85 The same cannot be said for other feature selection methods that were found to be particularly prone to false positives due to both irrelevant and weakly relevant features. [sent-1123, score-0.696]
</p><p>86 On the other hand, it needs to be noted that classical FDR methods do not control at all weakly relevant false positives (as GLL does). [sent-1124, score-0.504]
</p><p>87 Ĺš ltering of GLL algorithms with an FDR control method eliminates false positives in all cases tested and yields the best algorithm for 275  A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS  local causal learning among tested algorithms. [sent-1126, score-0.633]
</p><p>88 Within the GLL framework both the max-k and h-ps parameters control the false positives and false negatives tradeoff, through control of combined power and combined signiÄ? [sent-1130, score-0.632]
</p><p>89 We also used a causal graph point of view and Markov blanket concepts to understand a variety of non-causal feature selection algorithms. [sent-1139, score-0.42]
</p><p>90 We made this point by showing that the theory readily reveals why prominent feature selection methods exhibit many false positives and why they cannot be used for sound causal discovery. [sent-1141, score-0.715]
</p><p>91 (2010) that demonstrate empirical feature selection and causal discovery suboptimality for many state-of-the-art non-causal feature selection methods. [sent-1144, score-0.468]
</p><p>92 The exploration of parallel and distributed techniques in the present paper showed that GLL is amenable to parallelized and distributed local causal discovery and feature selection. [sent-1205, score-0.388]
</p><p>93 The presented parallel algorithm can also be used for distributed feature selection and causal discovery in a principled manner. [sent-1207, score-0.381]
</p><p>94 Ĺš ned by formal causal global methods are very slow and typically produce lower-quality graphs than LGL instantiations relying on sound local causal methods. [sent-1221, score-0.575]
</p><p>95 (2010) and in the present paper merely scratch the surface of causal feature selection algorithms, local causal learning, and local-to-global learning. [sent-1224, score-0.63]
</p><p>96 , cellular) aggregation, feedback loops, and limited local causality on feasibility of local causal discovery will be helpful in determining the space of practical usefulness of the GLL framework. [sent-1237, score-0.368]
</p><p>97 A previously underemphasized important parameter for false negatives control is the order of conditional independence tests used for elimination (i. [sent-1238, score-0.444]
</p><p>98 , 2010) support the notion that local causal learning in the form of Markov blanket and local neighborhood induction is a theoretically well-motivated and empirically robust learning methodology as embodied in the Generalized Local Learning framework. [sent-1299, score-0.407]
</p><p>99 UXVNDO:DOOLV690   8$)6LJQDO1RLVH690   8$)6LJQDO1RLVH690   / /$56(1 IRU PXOWLFODVV UHVSRQVH /$56(1 RQHYHUVXVUHVW  Table 16: Algorithms used in local causal discovery experiments with simulated and resimulated data. [sent-1318, score-0.371]
</p><p>100 Local causal and markov blanket induction for causal discovery and feature selection for classiÄ? [sent-1350, score-0.748]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gll', 0.328), ('causal', 0.253), ('luuhohydqw', 0.244), ('yduldeohv', 0.226), ('false', 0.191), ('tpc', 0.186), ('aliferis', 0.172), ('qhwzrun', 0.169), ('ruljlqdo', 0.16), ('positives', 0.152), ('pc', 0.149), ('mb', 0.141), ('tests', 0.122), ('koutsoukos', 0.12), ('lgl', 0.119), ('ani', 0.111), ('iru', 0.111), ('liferis', 0.111), ('samardinos', 0.111), ('tatnikov', 0.111), ('hhc', 0.109), ('arkov', 0.106), ('lanket', 0.106), ('nduction', 0.106), ('irrelevant', 0.105), ('fdr', 0.099), ('idovh', 0.099), ('weakly', 0.098), ('negatives', 0.098), ('ausal', 0.096), ('vljqdo', 0.093), ('zhdnhqhg', 0.093), ('connectivity', 0.093), ('ocal', 0.088), ('pd', 0.085), ('vl', 0.082), ('lq', 0.081), ('blanket', 0.08), ('uhgxfwlrq', 0.08), ('mmhc', 0.076), ('univariate', 0.072), ('qxpehu', 0.071), ('cits', 0.067), ('ri', 0.066), ('tsamardinos', 0.065), ('relevant', 0.063), ('srvlwlyhv', 0.062), ('dqg', 0.062), ('predictivity', 0.062), ('parents', 0.061), ('wrapping', 0.06), ('odup', 0.058), ('vnhohwrq', 0.057), ('feature', 0.057), ('sdudphwhu', 0.052), ('uaf', 0.049), ('wkh', 0.049), ('zlwk', 0.049), ('conditioning', 0.048), ('mmpc', 0.048), ('correction', 0.047), ('rqo', 0.044), ('wr', 0.044), ('causally', 0.044), ('children', 0.043), ('inclusion', 0.042), ('phwkrg', 0.042), ('rqihuurql', 0.042), ('vdpsoh', 0.042), ('discovery', 0.041), ('features', 0.04), ('resimulated', 0.04), ('members', 0.038), ('strongly', 0.037), ('local', 0.037), ('chunked', 0.036), ('zloo', 0.036), ('benjamini', 0.036), ('statnikov', 0.034), ('markov', 0.034), ('target', 0.034), ('synthesis', 0.034), ('independence', 0.033), ('sample', 0.033), ('sound', 0.032), ('association', 0.032), ('lexicographical', 0.031), ('plqxwhv', 0.031), ('qhjdwlyhv', 0.031), ('suredelolw', 0.031), ('ordering', 0.031), ('dvvrfldwlrq', 0.031), ('hgjhv', 0.031), ('lqfoxvlrq', 0.031), ('sca', 0.031), ('zlwkrxw', 0.031), ('chunk', 0.031), ('selection', 0.03), ('xor', 0.029), ('skeleton', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000024 <a title="68-tfidf-1" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_II%3A_Analysis_and_Extensions.html">68 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions</a></p>
<p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: In part I of this work we introduced and evaluated the Generalized Local Learning (GLL) framework for producing local causal and Markov blanket induction algorithms. In the present second part we analyze the behavior of GLL algorithms and provide extensions to the core methods. SpeciÄ?Ĺš cally, we investigate the empirical convergence of GLL to the true local neighborhood as a function of sample size. Moreover, we study how predictivity improves with increasing sample size. Then we investigate how sensitive are the algorithms to multiple statistical testing, especially in the presence of many irrelevant features. Next we discuss the role of the algorithm parameters and also show that Markov blanket and causal graph concepts can be used to understand deviations from optimality of state-of-the-art non-causal algorithms. The present paper also introduces the following extensions to the core GLL framework: parallel and distributed versions of GLL algorithms, versions with false discovery rate control, strategies for constructing novel heuristics for speciÄ?Ĺš c domains, and divide-and-conquer local-to-global learning (LGL) strategies. We test the generality of the LGL approach by deriving a novel LGL-based algorithm that compares favorably c 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS to the state-of-the-art global learning algorithms. In addition, we investigate the use of non-causal feature selection methods to facilitate global learning. Open problems and future research paths related to local and local-to-global causal learning are discussed. Keywords: local causal discovery, Markov blanket induction, feature selection, classiÄ?Ĺš cation, causal structure learning, learning of Bayesian networks</p><p>2 0.66050166 <a title="68-tfidf-2" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_I%3A_Algorithms_and_Empirical_Evaluation.html">67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</a></p>
<p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: We present an algorithmic framework for learning local causal structure around target variables of interest in the form of direct causes/effects and Markov blankets applicable to very large data sets with relatively small samples. The selected feature sets can be used for causal discovery and classiÄ?Ĺš cation. The framework (Generalized Local Learning, or GLL) can be instantiated in numerous ways, giving rise to both existing state-of-the-art as well as novel algorithms. The resulting algorithms are sound under well-deÄ?Ĺš ned sufÄ?Ĺš cient conditions. In a Ä?Ĺš rst set of experiments we evaluate several algorithms derived from this framework in terms of predictivity and feature set parsimony and compare to other local causal discovery methods and to state-of-the-art non-causal feature selection methods using real data. A second set of experimental evaluations compares the algorithms in terms of ability to induce local causal neighborhoods using simulated and resimulated data and examines the relation of predictivity with causal induction performance. Our experiments demonstrate, consistently with causal feature selection theory, that local causal feature selection methods (under broad assumptions encompassing appropriate family of distribuc 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS tions, types of classiÄ?Ĺš ers, and loss functions) exhibit strong feature set parsimony, high predictivity and local causal interpretability. Although non-causal feature selection methods are often used in practice to shed light on causal relationships, we Ä?Ĺš nd that they cannot be interpreted causally even when they achieve excellent predictivity. Therefore we conclude that only local causal techniques should be used when insight into causal structure is sought. In a companion paper we examine in depth the behavior of GLL algorithms, provide extensions, and show</p><p>3 0.22810929 <a title="68-tfidf-3" href="./jmlr-2010-Introduction_to_Causal_Inference.html">56 jmlr-2010-Introduction to Causal Inference</a></p>
<p>Author: Peter Spirtes</p><p>Abstract: The goal of many sciences is to understand the mechanisms by which variables came to take on the values they have (that is, to ﬁnd a generative model), and to predict what the values of those variables would be if the naturally occurring mechanisms were subject to outside manipulations. The past 30 years has seen a number of conceptual developments that are partial solutions to the problem of causal inference from observational sample data or a mixture of observational sample and experimental data, particularly in the area of graphical causal modeling. However, in many domains, problems such as the large numbers of variables, small samples sizes, and possible presence of unmeasured causes, remain serious impediments to practical applications of these developments. The articles in the Special Topic on Causality address these and other problems in applying graphical causal modeling algorithms. This introduction to the Special Topic on Causality provides a brief introduction to graphical causal modeling, places the articles in a broader context, and describes the differences between causal inference and ordinary machine learning classiﬁcation and prediction problems. Keywords: Bayesian networks, causation, causal inference</p><p>4 0.12325779 <a title="68-tfidf-4" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>Author: Shyam Visweswaran, Gregory F. Cooper</p><p>Abstract: This paper introduces a Bayesian algorithm for constructing predictive models from data that are optimized to predict a target variable well for a particular instance. This algorithm learns Markov blanket models, carries out Bayesian model averaging over a set of models to predict a target variable of the instance at hand, and employs an instance-speciﬁc heuristic to locate a set of suitable models to average over. We call this method the instance-speciﬁc Markov blanket (ISMB) algorithm. The ISMB algorithm was evaluated on 21 UCI data sets using ﬁve different performance measures and its performance was compared to that of several commonly used predictive algorithms, including nave Bayes, C4.5 decision tree, logistic regression, neural networks, k-Nearest Neighbor, Lazy Bayesian Rules, and AdaBoost. Over all the data sets, the ISMB algorithm performed better on average on all performance measures against all the comparison algorithms. Keywords: instance-speciﬁc, Bayesian network, Markov blanket, Bayesian model averaging</p><p>5 0.10413922 <a title="68-tfidf-5" href="./jmlr-2010-Estimation_of_a_Structural_Vector_Autoregression_Model_Using_Non-Gaussianity.html">36 jmlr-2010-Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity</a></p>
<p>Author: Aapo Hyvärinen, Kun Zhang, Shohei Shimizu, Patrik O. Hoyer</p><p>Abstract: Analysis of causal effects between continuous-valued variables typically uses either autoregressive models or structural equation models with instantaneous effects. Estimation of Gaussian, linear structural equation models poses serious identiﬁability problems, which is why it was recently proposed to use non-Gaussian models. Here, we show how to combine the non-Gaussian instantaneous model with autoregressive models. This is effectively what is called a structural vector autoregression (SVAR) model, and thus our work contributes to the long-standing problem of how to estimate SVAR’s. We show that such a non-Gaussian model is identiﬁable without prior knowledge of network structure. We propose computationally efﬁcient methods for estimating the model, as well as methods to assess the signiﬁcance of the causal inﬂuences. The model is successfully applied on ﬁnancial and brain imaging data. Keywords: structural vector autoregression, structural equation models, independent component analysis, non-Gaussianity, causality</p><p>6 0.072888747 <a title="68-tfidf-6" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>7 0.054147866 <a title="68-tfidf-7" href="./jmlr-2010-Optimal_Search_on_Clustered_Structural_Constraint_for_Learning_Bayesian_Network_Structure.html">88 jmlr-2010-Optimal Search on Clustered Structural Constraint for Learning Bayesian Network Structure</a></p>
<p>8 0.051974192 <a title="68-tfidf-8" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>9 0.0470329 <a title="68-tfidf-9" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>10 0.043119926 <a title="68-tfidf-10" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>11 0.040348463 <a title="68-tfidf-11" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>12 0.035691388 <a title="68-tfidf-12" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>13 0.035385415 <a title="68-tfidf-13" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>14 0.035078727 <a title="68-tfidf-14" href="./jmlr-2010-Matched_Gene_Selection_and_Committee_Classifier_for_Molecular_Classification_of_Heterogeneous_Diseases.html">71 jmlr-2010-Matched Gene Selection and Committee Classifier for Molecular Classification of Heterogeneous Diseases</a></p>
<p>15 0.034833293 <a title="68-tfidf-15" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>16 0.031669479 <a title="68-tfidf-16" href="./jmlr-2010-Consistent_Nonparametric_Tests_of_Independence.html">27 jmlr-2010-Consistent Nonparametric Tests of Independence</a></p>
<p>17 0.029570542 <a title="68-tfidf-17" href="./jmlr-2010-A_Streaming_Parallel_Decision_Tree_Algorithm.html">7 jmlr-2010-A Streaming Parallel Decision Tree Algorithm</a></p>
<p>18 0.02918428 <a title="68-tfidf-18" href="./jmlr-2010-Bayesian_Learning_in_Sparse_Graphical_Factor_Models_via_Variational_Mean-Field_Annealing.html">17 jmlr-2010-Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing</a></p>
<p>19 0.028990725 <a title="68-tfidf-19" href="./jmlr-2010-Learning_Gradients%3A_Predictive_Models_that_Infer_Geometry_and_Statistical_Dependence.html">62 jmlr-2010-Learning Gradients: Predictive Models that Infer Geometry and Statistical Dependence</a></p>
<p>20 0.028499585 <a title="68-tfidf-20" href="./jmlr-2010-On_Over-fitting_in_Model_Selection_and_Subsequent_Selection_Bias_in_Performance_Evaluation.html">83 jmlr-2010-On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.208), (1, 0.657), (2, 0.355), (3, 0.017), (4, 0.038), (5, -0.1), (6, -0.036), (7, 0.018), (8, 0.051), (9, -0.004), (10, 0.007), (11, -0.025), (12, 0.016), (13, 0.025), (14, 0.017), (15, 0.035), (16, -0.018), (17, -0.043), (18, 0.005), (19, 0.014), (20, -0.048), (21, 0.031), (22, 0.037), (23, 0.034), (24, -0.018), (25, -0.032), (26, 0.018), (27, -0.187), (28, 0.036), (29, -0.043), (30, -0.009), (31, 0.016), (32, -0.068), (33, 0.082), (34, -0.07), (35, 0.054), (36, -0.021), (37, 0.028), (38, -0.024), (39, -0.059), (40, 0.083), (41, 0.006), (42, 0.026), (43, 0.079), (44, 0.013), (45, 0.022), (46, 0.052), (47, 0.049), (48, -0.001), (49, 0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9394151 <a title="68-lsi-1" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_II%3A_Analysis_and_Extensions.html">68 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions</a></p>
<p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: In part I of this work we introduced and evaluated the Generalized Local Learning (GLL) framework for producing local causal and Markov blanket induction algorithms. In the present second part we analyze the behavior of GLL algorithms and provide extensions to the core methods. SpeciÄ?Ĺš cally, we investigate the empirical convergence of GLL to the true local neighborhood as a function of sample size. Moreover, we study how predictivity improves with increasing sample size. Then we investigate how sensitive are the algorithms to multiple statistical testing, especially in the presence of many irrelevant features. Next we discuss the role of the algorithm parameters and also show that Markov blanket and causal graph concepts can be used to understand deviations from optimality of state-of-the-art non-causal algorithms. The present paper also introduces the following extensions to the core GLL framework: parallel and distributed versions of GLL algorithms, versions with false discovery rate control, strategies for constructing novel heuristics for speciÄ?Ĺš c domains, and divide-and-conquer local-to-global learning (LGL) strategies. We test the generality of the LGL approach by deriving a novel LGL-based algorithm that compares favorably c 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS to the state-of-the-art global learning algorithms. In addition, we investigate the use of non-causal feature selection methods to facilitate global learning. Open problems and future research paths related to local and local-to-global causal learning are discussed. Keywords: local causal discovery, Markov blanket induction, feature selection, classiÄ?Ĺš cation, causal structure learning, learning of Bayesian networks</p><p>2 0.919644 <a title="68-lsi-2" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_I%3A_Algorithms_and_Empirical_Evaluation.html">67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</a></p>
<p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: We present an algorithmic framework for learning local causal structure around target variables of interest in the form of direct causes/effects and Markov blankets applicable to very large data sets with relatively small samples. The selected feature sets can be used for causal discovery and classiÄ?Ĺš cation. The framework (Generalized Local Learning, or GLL) can be instantiated in numerous ways, giving rise to both existing state-of-the-art as well as novel algorithms. The resulting algorithms are sound under well-deÄ?Ĺš ned sufÄ?Ĺš cient conditions. In a Ä?Ĺš rst set of experiments we evaluate several algorithms derived from this framework in terms of predictivity and feature set parsimony and compare to other local causal discovery methods and to state-of-the-art non-causal feature selection methods using real data. A second set of experimental evaluations compares the algorithms in terms of ability to induce local causal neighborhoods using simulated and resimulated data and examines the relation of predictivity with causal induction performance. Our experiments demonstrate, consistently with causal feature selection theory, that local causal feature selection methods (under broad assumptions encompassing appropriate family of distribuc 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS tions, types of classiÄ?Ĺš ers, and loss functions) exhibit strong feature set parsimony, high predictivity and local causal interpretability. Although non-causal feature selection methods are often used in practice to shed light on causal relationships, we Ä?Ĺš nd that they cannot be interpreted causally even when they achieve excellent predictivity. Therefore we conclude that only local causal techniques should be used when insight into causal structure is sought. In a companion paper we examine in depth the behavior of GLL algorithms, provide extensions, and show</p><p>3 0.66098195 <a title="68-lsi-3" href="./jmlr-2010-Introduction_to_Causal_Inference.html">56 jmlr-2010-Introduction to Causal Inference</a></p>
<p>Author: Peter Spirtes</p><p>Abstract: The goal of many sciences is to understand the mechanisms by which variables came to take on the values they have (that is, to ﬁnd a generative model), and to predict what the values of those variables would be if the naturally occurring mechanisms were subject to outside manipulations. The past 30 years has seen a number of conceptual developments that are partial solutions to the problem of causal inference from observational sample data or a mixture of observational sample and experimental data, particularly in the area of graphical causal modeling. However, in many domains, problems such as the large numbers of variables, small samples sizes, and possible presence of unmeasured causes, remain serious impediments to practical applications of these developments. The articles in the Special Topic on Causality address these and other problems in applying graphical causal modeling algorithms. This introduction to the Special Topic on Causality provides a brief introduction to graphical causal modeling, places the articles in a broader context, and describes the differences between causal inference and ordinary machine learning classiﬁcation and prediction problems. Keywords: Bayesian networks, causation, causal inference</p><p>4 0.4971877 <a title="68-lsi-4" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>Author: Shyam Visweswaran, Gregory F. Cooper</p><p>Abstract: This paper introduces a Bayesian algorithm for constructing predictive models from data that are optimized to predict a target variable well for a particular instance. This algorithm learns Markov blanket models, carries out Bayesian model averaging over a set of models to predict a target variable of the instance at hand, and employs an instance-speciﬁc heuristic to locate a set of suitable models to average over. We call this method the instance-speciﬁc Markov blanket (ISMB) algorithm. The ISMB algorithm was evaluated on 21 UCI data sets using ﬁve different performance measures and its performance was compared to that of several commonly used predictive algorithms, including nave Bayes, C4.5 decision tree, logistic regression, neural networks, k-Nearest Neighbor, Lazy Bayesian Rules, and AdaBoost. Over all the data sets, the ISMB algorithm performed better on average on all performance measures against all the comparison algorithms. Keywords: instance-speciﬁc, Bayesian network, Markov blanket, Bayesian model averaging</p><p>5 0.2433636 <a title="68-lsi-5" href="./jmlr-2010-Estimation_of_a_Structural_Vector_Autoregression_Model_Using_Non-Gaussianity.html">36 jmlr-2010-Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity</a></p>
<p>Author: Aapo Hyvärinen, Kun Zhang, Shohei Shimizu, Patrik O. Hoyer</p><p>Abstract: Analysis of causal effects between continuous-valued variables typically uses either autoregressive models or structural equation models with instantaneous effects. Estimation of Gaussian, linear structural equation models poses serious identiﬁability problems, which is why it was recently proposed to use non-Gaussian models. Here, we show how to combine the non-Gaussian instantaneous model with autoregressive models. This is effectively what is called a structural vector autoregression (SVAR) model, and thus our work contributes to the long-standing problem of how to estimate SVAR’s. We show that such a non-Gaussian model is identiﬁable without prior knowledge of network structure. We propose computationally efﬁcient methods for estimating the model, as well as methods to assess the signiﬁcance of the causal inﬂuences. The model is successfully applied on ﬁnancial and brain imaging data. Keywords: structural vector autoregression, structural equation models, independent component analysis, non-Gaussianity, causality</p><p>6 0.21185426 <a title="68-lsi-6" href="./jmlr-2010-Optimal_Search_on_Clustered_Structural_Constraint_for_Learning_Bayesian_Network_Structure.html">88 jmlr-2010-Optimal Search on Clustered Structural Constraint for Learning Bayesian Network Structure</a></p>
<p>7 0.20526491 <a title="68-lsi-7" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>8 0.19458422 <a title="68-lsi-8" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>9 0.1855633 <a title="68-lsi-9" href="./jmlr-2010-Matched_Gene_Selection_and_Committee_Classifier_for_Molecular_Classification_of_Heterogeneous_Diseases.html">71 jmlr-2010-Matched Gene Selection and Committee Classifier for Molecular Classification of Heterogeneous Diseases</a></p>
<p>10 0.17694226 <a title="68-lsi-10" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>11 0.17524679 <a title="68-lsi-11" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>12 0.1678627 <a title="68-lsi-12" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>13 0.15725681 <a title="68-lsi-13" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>14 0.15041554 <a title="68-lsi-14" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>15 0.1493482 <a title="68-lsi-15" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>16 0.14779297 <a title="68-lsi-16" href="./jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization.html">54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</a></p>
<p>17 0.13408406 <a title="68-lsi-17" href="./jmlr-2010-Chromatic_PAC-Bayes_Bounds_for_Non-IID_Data%3A_Applications_to_Ranking_and_Stationary_%CE%B2-Mixing_Processes.html">20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</a></p>
<p>18 0.13225105 <a title="68-lsi-18" href="./jmlr-2010-Quadratic_Programming_Feature_Selection.html">94 jmlr-2010-Quadratic Programming Feature Selection</a></p>
<p>19 0.12613583 <a title="68-lsi-19" href="./jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">48 jmlr-2010-How to Explain Individual Classification Decisions</a></p>
<p>20 0.12401894 <a title="68-lsi-20" href="./jmlr-2010-Consensus-Based_Distributed_Support_Vector_Machines.html">26 jmlr-2010-Consensus-Based Distributed Support Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.01), (4, 0.016), (8, 0.016), (21, 0.014), (32, 0.037), (33, 0.529), (36, 0.033), (37, 0.03), (38, 0.035), (75, 0.086), (81, 0.016), (85, 0.063), (96, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77094406 <a title="68-lda-1" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_II%3A_Analysis_and_Extensions.html">68 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions</a></p>
<p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: In part I of this work we introduced and evaluated the Generalized Local Learning (GLL) framework for producing local causal and Markov blanket induction algorithms. In the present second part we analyze the behavior of GLL algorithms and provide extensions to the core methods. SpeciÄ?Ĺš cally, we investigate the empirical convergence of GLL to the true local neighborhood as a function of sample size. Moreover, we study how predictivity improves with increasing sample size. Then we investigate how sensitive are the algorithms to multiple statistical testing, especially in the presence of many irrelevant features. Next we discuss the role of the algorithm parameters and also show that Markov blanket and causal graph concepts can be used to understand deviations from optimality of state-of-the-art non-causal algorithms. The present paper also introduces the following extensions to the core GLL framework: parallel and distributed versions of GLL algorithms, versions with false discovery rate control, strategies for constructing novel heuristics for speciÄ?Ĺš c domains, and divide-and-conquer local-to-global learning (LGL) strategies. We test the generality of the LGL approach by deriving a novel LGL-based algorithm that compares favorably c 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS to the state-of-the-art global learning algorithms. In addition, we investigate the use of non-causal feature selection methods to facilitate global learning. Open problems and future research paths related to local and local-to-global causal learning are discussed. Keywords: local causal discovery, Markov blanket induction, feature selection, classiÄ?Ĺš cation, causal structure learning, learning of Bayesian networks</p><p>2 0.6689797 <a title="68-lda-2" href="./jmlr-2010-Estimation_of_a_Structural_Vector_Autoregression_Model_Using_Non-Gaussianity.html">36 jmlr-2010-Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity</a></p>
<p>Author: Aapo Hyvärinen, Kun Zhang, Shohei Shimizu, Patrik O. Hoyer</p><p>Abstract: Analysis of causal effects between continuous-valued variables typically uses either autoregressive models or structural equation models with instantaneous effects. Estimation of Gaussian, linear structural equation models poses serious identiﬁability problems, which is why it was recently proposed to use non-Gaussian models. Here, we show how to combine the non-Gaussian instantaneous model with autoregressive models. This is effectively what is called a structural vector autoregression (SVAR) model, and thus our work contributes to the long-standing problem of how to estimate SVAR’s. We show that such a non-Gaussian model is identiﬁable without prior knowledge of network structure. We propose computationally efﬁcient methods for estimating the model, as well as methods to assess the signiﬁcance of the causal inﬂuences. The model is successfully applied on ﬁnancial and brain imaging data. Keywords: structural vector autoregression, structural equation models, independent component analysis, non-Gaussianity, causality</p><p>3 0.60494989 <a title="68-lda-3" href="./jmlr-2010-WEKA%E2%88%92Experiences_with_a_Java_Open-Source_Project.html">116 jmlr-2010-WEKA−Experiences with a Java Open-Source Project</a></p>
<p>Author: Remco R. Bouckaert, Eibe Frank, Mark A. Hall, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, Ian H. Witten</p><p>Abstract: WEKA is a popular machine learning workbench with a development life of nearly two decades. This article provides an overview of the factors that we believe to be important to its success. Rather than focussing on the software’s functionality, we review aspects of project management and historical development decisions that likely had an impact on the uptake of the project. Keywords: machine learning software, open source software</p><p>4 0.56316531 <a title="68-lda-4" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_I%3A_Algorithms_and_Empirical_Evaluation.html">67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</a></p>
<p>Author: Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, Xenofon D. Koutsoukos</p><p>Abstract: We present an algorithmic framework for learning local causal structure around target variables of interest in the form of direct causes/effects and Markov blankets applicable to very large data sets with relatively small samples. The selected feature sets can be used for causal discovery and classiÄ?Ĺš cation. The framework (Generalized Local Learning, or GLL) can be instantiated in numerous ways, giving rise to both existing state-of-the-art as well as novel algorithms. The resulting algorithms are sound under well-deÄ?Ĺš ned sufÄ?Ĺš cient conditions. In a Ä?Ĺš rst set of experiments we evaluate several algorithms derived from this framework in terms of predictivity and feature set parsimony and compare to other local causal discovery methods and to state-of-the-art non-causal feature selection methods using real data. A second set of experimental evaluations compares the algorithms in terms of ability to induce local causal neighborhoods using simulated and resimulated data and examines the relation of predictivity with causal induction performance. Our experiments demonstrate, consistently with causal feature selection theory, that local causal feature selection methods (under broad assumptions encompassing appropriate family of distribuc 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. A LIFERIS , S TATNIKOV, T SAMARDINOS , M ANI AND KOUTSOUKOS tions, types of classiÄ?Ĺš ers, and loss functions) exhibit strong feature set parsimony, high predictivity and local causal interpretability. Although non-causal feature selection methods are often used in practice to shed light on causal relationships, we Ä?Ĺš nd that they cannot be interpreted causally even when they achieve excellent predictivity. Therefore we conclude that only local causal techniques should be used when insight into causal structure is sought. In a companion paper we examine in depth the behavior of GLL algorithms, provide extensions, and show</p><p>5 0.35846382 <a title="68-lda-5" href="./jmlr-2010-Introduction_to_Causal_Inference.html">56 jmlr-2010-Introduction to Causal Inference</a></p>
<p>Author: Peter Spirtes</p><p>Abstract: The goal of many sciences is to understand the mechanisms by which variables came to take on the values they have (that is, to ﬁnd a generative model), and to predict what the values of those variables would be if the naturally occurring mechanisms were subject to outside manipulations. The past 30 years has seen a number of conceptual developments that are partial solutions to the problem of causal inference from observational sample data or a mixture of observational sample and experimental data, particularly in the area of graphical causal modeling. However, in many domains, problems such as the large numbers of variables, small samples sizes, and possible presence of unmeasured causes, remain serious impediments to practical applications of these developments. The articles in the Special Topic on Causality address these and other problems in applying graphical causal modeling algorithms. This introduction to the Special Topic on Causality provides a brief introduction to graphical causal modeling, places the articles in a broader context, and describes the differences between causal inference and ordinary machine learning classiﬁcation and prediction problems. Keywords: Bayesian networks, causation, causal inference</p><p>6 0.26264393 <a title="68-lda-6" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>7 0.25806251 <a title="68-lda-7" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>8 0.25227234 <a title="68-lda-8" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>9 0.24979216 <a title="68-lda-9" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>10 0.24723995 <a title="68-lda-10" href="./jmlr-2010-An_Investigation_of_Missing_Data_Methods_for_Classification_Trees_Applied_to_Binary_Response_Data.html">11 jmlr-2010-An Investigation of Missing Data Methods for Classification Trees Applied to Binary Response Data</a></p>
<p>11 0.24407364 <a title="68-lda-11" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>12 0.24315225 <a title="68-lda-12" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>13 0.24166474 <a title="68-lda-13" href="./jmlr-2010-Bayesian_Learning_in_Sparse_Graphical_Factor_Models_via_Variational_Mean-Field_Annealing.html">17 jmlr-2010-Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing</a></p>
<p>14 0.2364293 <a title="68-lda-14" href="./jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization.html">54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</a></p>
<p>15 0.2356561 <a title="68-lda-15" href="./jmlr-2010-Matched_Gene_Selection_and_Committee_Classifier_for_Molecular_Classification_of_Heterogeneous_Diseases.html">71 jmlr-2010-Matched Gene Selection and Committee Classifier for Molecular Classification of Heterogeneous Diseases</a></p>
<p>16 0.23018774 <a title="68-lda-16" href="./jmlr-2010-Approximate_Tree_Kernels.html">15 jmlr-2010-Approximate Tree Kernels</a></p>
<p>17 0.22981746 <a title="68-lda-17" href="./jmlr-2010-Image_Denoising_with_Kernels_Based_on_Natural_Image_Relations.html">50 jmlr-2010-Image Denoising with Kernels Based on Natural Image Relations</a></p>
<p>18 0.22666323 <a title="68-lda-18" href="./jmlr-2010-Evolving_Static_Representations_for_Task_Transfer.html">37 jmlr-2010-Evolving Static Representations for Task Transfer</a></p>
<p>19 0.2263891 <a title="68-lda-19" href="./jmlr-2010-Second-Order_Bilinear_Discriminant_Analysis.html">101 jmlr-2010-Second-Order Bilinear Discriminant Analysis</a></p>
<p>20 0.22464244 <a title="68-lda-20" href="./jmlr-2010-Importance_Sampling_for_Continuous_Time_Bayesian_Networks.html">51 jmlr-2010-Importance Sampling for Continuous Time Bayesian Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
