<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-54" href="#">jmlr2010-54</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</h1>
<br/><p>Source: <a title="jmlr-2010-54-pdf" href="http://jmlr.org/papers/volume11/venna10a/venna10a.pdf">pdf</a></p><p>Author: Jarkko Venna, Jaakko Peltonen, Kristian Nybo, Helena Aidos, Samuel Kaski</p><p>Abstract: Nonlinear dimensionality reduction methods are often used to visualize high-dimensional data, although the existing methods have been designed for other related tasks such as manifold learning. It has been difﬁcult to assess the quality of visualizations since the task has not been well-deﬁned. We give a rigorous deﬁnition for a speciﬁc visualization task, resulting in quantiﬁable goodness measures and new visualization methods. The task is information retrieval given the visualization: to ﬁnd similar data based on the similarities shown on the display. The fundamental tradeoff between precision and recall of information retrieval can then be quantiﬁed in visualizations as well. The user needs to give the relative cost of missing similar points vs. retrieving dissimilar points, after which the total cost can be measured. We then introduce a new method NeRV (neighbor retrieval visualizer) which produces an optimal visualization by minimizing the cost. We further derive a variant for supervised visualization; class information is taken rigorously into account when computing the similarity relationships. We show empirically that the unsupervised version outperforms existing unsupervised dimensionality reduction methods in the visualization task, and the supervised version outperforms existing supervised methods. Keywords: information retrieval, manifold learning, multidimensional scaling, nonlinear dimensionality reduction, visualization</p><p>Reference: <a title="jmlr-2010-54-reference" href="../jmlr2010_reference/jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We give a rigorous deﬁnition for a speciﬁc visualization task, resulting in quantiﬁable goodness measures and new visualization methods. [sent-15, score-0.434]
</p><p>2 The fundamental tradeoff between precision and recall of information retrieval can then be quantiﬁed in visualizations as well. [sent-17, score-0.362]
</p><p>3 We then introduce a new method NeRV (neighbor retrieval visualizer) which produces an optimal visualization by minimizing the cost. [sent-20, score-0.297]
</p><p>4 We show empirically that the unsupervised version outperforms existing unsupervised dimensionality reduction methods in the visualization task, and the supervised version outperforms existing supervised methods. [sent-22, score-0.423]
</p><p>5 Keywords: information retrieval, manifold learning, multidimensional scaling, nonlinear dimensionality reduction, visualization  1. [sent-23, score-0.286]
</p><p>6 Several methods had severe difﬁculties when the output dimensionality was ﬁxed to two for visualization purposes (Venna and Kaski, 2007a). [sent-32, score-0.241]
</p><p>7 In this paper we discuss the speciﬁc visualization task of projecting the data to points on a twodimensional display. [sent-34, score-0.237]
</p><p>8 The problem with using the above approaches to measure visualization performance is that their connection to visualization is unclear and indirect at best. [sent-43, score-0.404]
</p><p>9 The conceptualization as information retrieval explicitly reveals the necessary tradeoff between precision and recall, of making true similarities visible and avoiding false similarities. [sent-49, score-0.265]
</p><p>10 We then show that the resulting method, called NeRV for neighbor retrieval visualizer, can be further extended to supervised visualization, and that both the unsupervised and supervised methods empirically outperform their alternatives. [sent-51, score-0.287]
</p><p>11 NeRV includes the previous method called stochastic neighbor embedding (SNE; Hinton and Roweis, 2002) as a special case where the tradeoff is set so that only recall is maximized; thus we give a new information retrieval interpretation to SNE. [sent-52, score-0.332]
</p><p>12 Visualization as Information Retrieval In this section we deﬁne formally the speciﬁc visualization task; this is a novel formalization of visualization as an information retrieval task. [sent-56, score-0.499]
</p><p>13 1 Similarity Visualization with Binary Neighborhood Relationships In the following we ﬁrst deﬁne the speciﬁc visualization task and a cost function for it; we then show that the cost function is related to the traditional information retrieval measures precision and recall. [sent-61, score-0.457]
</p><p>14 2 R ELATIONSHIP  TO  (1)  P RECISION AND R ECALL  The cost function of similarity visualization (1) bears a close relationship to the traditional measures of information retrieval, precision and recall. [sent-79, score-0.362]
</p><p>15 ki  E(ki , ri ) =  The traditional deﬁnition of precision for a single query is precision(i) =  NFP,i NTP,i = 1− , ki ki  and recall is  NTP,i NMISS,i = 1− . [sent-82, score-0.378]
</p><p>16 Finally, to assess performance of the full visualization the cost needs to be averaged over all samples (queries) which yields mean precision and recall of the visualization. [sent-84, score-0.411]
</p><p>17 Visualization B has lower precision at the left end of the curve, but precision does not drop as much even when high recall is reached. [sent-93, score-0.308]
</p><p>18 This requires a rich enough retrieval model, in the sense that the number of retrieved points can be different from the number of relevant points, so that precision and recall get different values. [sent-95, score-0.356]
</p><p>19 It is well-known in information retrieval that if the numbers of relevant and retrieved items (here points) are equal, precision and recall become equal. [sent-96, score-0.321]
</p><p>20 The simple visualization setup presented in this section is a novel formulation of visualization and useful as a clearly deﬁned starting point. [sent-101, score-0.404]
</p><p>21 2 Similarity Visualization with Continuous Neighborhood Relationships We generalize the simple binary neighborhood case by deﬁning probabilistic neighborhoods both in the (i) input and (ii) output spaces, and (iii) replacing the binary precision and recall measures with probabilistic ones. [sent-106, score-0.293]
</p><p>22 It will ﬁnally be shown that for binary neighborhoods, interpreted as a constant high probability of being a neighbor within the neighborhood set and a constant low probability elsewhere, the measures reduce to the standard precision and recall. [sent-107, score-0.247]
</p><p>23 Such a distribution is interpretable as a model about how the user does the retrieval given the visualization display. [sent-111, score-0.297]
</p><p>24 The embedding A has better precision (yielding higher values at the left end of the curve) whereas the embedding B has better recall (yielding higher values at the right end of the curve). [sent-134, score-0.362]
</p><p>25 We call D(qi , pi ) smoothed precision and D(pi , qi ) smoothed recall. [sent-163, score-0.611]
</p><p>26 Mean smoothed precision and recall are analogous to mean precision and recall in that we cannot in general reach the optimum of both simultaneously. [sent-165, score-0.567]
</p><p>27 The subﬁgure B was created by maximizing mean smoothed recall; the sphere is squashed ﬂat, which minimizes the number of misses, as all the points that were close to each other in the original data are close to each other in the visualization. [sent-168, score-0.246]
</p><p>28 However, there are then a large number of false positives because opposite sides of the sphere have been mapped on top of each other, so that many points that appear close to each other in the visualization are actually originally far away from each other. [sent-169, score-0.27]
</p><p>29 Neighborhood Retrieval Visualizer (NeRV) In Section 2 we deﬁned similarity visualization as an information retrieval task. [sent-179, score-0.297]
</p><p>30 The quality of a visualization can be measured by the two loss functions, mean smoothed precision and recall. [sent-180, score-0.543]
</p><p>31 These measures generalize the straightforward precision and recall measures to non-binary neighborhoods. [sent-181, score-0.238]
</p><p>32 It is then easy to use the measures as optimization criteria for a visualization method. [sent-183, score-0.232]
</p><p>33 We now introduce a visualization algorithm that optimizes visual information retrieval performance. [sent-184, score-0.297]
</p><p>34 As demonstrated in Figure 2, precision and recall cannot in general be minimized simultaneously, and the user has to choose which loss function (average smoothed precision or recall) is more important, by assigning a cost for misses and a cost for false positives. [sent-186, score-0.533]
</p><p>35 In general, NeRV optimizes a user-deﬁned cost which forms a tradeoff between mean smoothed precision and mean smoothed recall. [sent-197, score-0.592]
</p><p>36 Once the analyst has speciﬁed the relative importance of precision and recall by choosing a value for λ, the NeRV algorithm computes the embedding based on the distances it is given. [sent-214, score-0.343]
</p><p>37 In this section we will make extensive experiments comparing the performance of NeRV with other dimensionality reduction methods on unsupervised visualization of several data sets, including both benchmark data sets and real-life bioinformatics data sets. [sent-215, score-0.288]
</p><p>38 The difference is that Isomap does not compute pairwise input-space distances as simple Euclidean distances but as geodesic distances along the manifold of the data (technically, along a graph formed by connecting all k-nearest neighbors). [sent-233, score-0.264]
</p><p>39 The idea is to replace the Euclidean distances in the original space with geodesic distances in the same manner as in the isomap algorithm. [sent-245, score-0.301]
</p><p>40 The ﬁrst pair is mean smoothed precision-mean smoothed recall, that is, our new measures of visualization quality. [sent-323, score-0.623]
</p><p>41 Although we feel, as explained in Section 2, that smoothed precision and smoothed recall are more sophisticated measures of visualization performance than precision and recall, we have also plotted standard mean precision-mean recall curves. [sent-326, score-0.979]
</p><p>42 The curves were plotted by ﬁxing the 20 nearest neighbors of a point in the original data as the set of relevant items, and then varying the number of neighbors retrieved from the visualization between 1 and 100, plotting mean precision and recall for each number. [sent-327, score-0.555]
</p><p>43 Our third pair of measures are the rank-based variants of our new measures, mean rank-based smoothed precision-mean rank-based smoothed recall. [sent-328, score-0.421]
</p><p>44 Recall that we introduced the rank-based 463  V ENNA , P ELTONEN , N YBO , A IDOS AND K ASKI  variants as easier-to-interpret, but less discriminating, alternatives to mean smoothed precision and mean smoothed recall. [sent-329, score-0.552]
</p><p>45 The intuitive motivation behind these measures was the same trade-off between precision and recall as in this paper, but the measures were deﬁned in a more ad hoc way. [sent-333, score-0.238]
</p><p>46 That is, we chose the parameter yielding the largest value of 2(P · R)/(P + R) where P and R are the mean rank-based smoothed precision and recall. [sent-341, score-0.341]
</p><p>47 We ﬁrst show the curves of mean smoothed precision-mean smoothed recall, that is, the loss functions associated with our formalization of visualization as information retrieval. [sent-353, score-0.593]
</p><p>48 LMDS has a relatively good mean smoothed precision, but does not perform as well in terms of mean smoothed recall. [sent-359, score-0.422]
</p><p>49 Because we formulated visualization as an information retrieval task, it is natural to also try existing measures of information retrieval performance, that is, mean precision and mean recall, even though they do not take into account grades of relevance as discussed in Section 2. [sent-361, score-0.614]
</p><p>50 The curves of mean rank-based smoothed precision-mean rank-based smoothed recall are shown in Figure 6. [sent-371, score-0.439]
</p><p>51 For any face 465  V ENNA , P ELTONEN , N YBO , A IDOS AND K ASKI  Mean smoothed precision (vertical axes) − Mean smoothed recall (horizontal axes) Seawater temperature time series Plain s−curve 500 NeRV 0  LMVU MVU  CDA −500  LMDS  λ=0  −1000  NeRV CCA  LMDS  λ=0. [sent-388, score-0.538]
</p><p>52 9  −2 MVU  NeRV  LE PCA Isomap LMVU  LLE  −2000 −3000  MDS  LMDS  −4 Isomap CDA  LLE  −6  LMVU PCA  HLLE CCA CDA  −8  −5000  −15  −12000 −10000 −8000 −6000 −4000 −2000  −10  −5 4  x 10  Figure 4: Mean smoothed precision-mean smoothed recall plotted for all six data sets. [sent-394, score-0.408]
</p><p>53 We have actually plotted −1·(mean smoothed precision) and −1·(mean smoothed recall) to maintain visual consistency with the plots for other measures: in each plot, the best performing methods appear in the top right corner. [sent-396, score-0.36]
</p><p>54 In spite of the very high dimensionality of the input space and the reduction of 467  V ENNA , P ELTONEN , N YBO , A IDOS AND K ASKI  Mean rank−based smoothed precision (vertical axes) − Mean rank−based smoothed recall (horizontal axes) Plain s−curve Seawater temperature time series 0. [sent-466, score-0.577]
</p><p>55 9  Figure 6: Mean rank-based smoothed precision-mean rank-based smoothed recall plotted for all six data sets. [sent-533, score-0.408]
</p><p>56 We have actually plotted 1−(mean rank-based smoothed precision) and 1−(mean rank-based smoothed recall) to maintain visual consistency with the plots for other measures: in each plot, the best performance is in the top right corner. [sent-535, score-0.36]
</p><p>57 NeRV is the best according to both of our proposed measures of visualization performance, mean smoothed precision and recall; MDS and local MDS also perform well. [sent-537, score-0.573]
</p><p>58 In terms of the simple mean precision and mean recall NeRV is the second best with CDA being slightly better. [sent-538, score-0.24]
</p><p>59 Overall, NeRV was the best in these unsupervised visualization tasks, although it was not the best in all, and in some tasks it had tough competition. [sent-619, score-0.249]
</p><p>60 2 0  −5000  Mean smoothed precision (vertical axis) − Mean smoothed recall (horizontal axis) λ=0  CDA  0. [sent-623, score-0.538]
</p><p>61 05  Mean rank−based smoothed precision (vertical axis) − Mean rank−based smoothed recall (horizontal axis)  0. [sent-646, score-0.538]
</p><p>62 Methods are evaluated by k-nearest neighbor classiﬁcation accuracy (with k = 5), that is, each sample in the visualization is classiﬁed by majority vote of its k nearest neighbors in the visualization, and the classiﬁcation is compared to the ground truth label. [sent-663, score-0.307]
</p><p>63 637  Table 2: Error rates of k-nearest neighbor classiﬁcation based on the visualization, for unsupervised visualization methods. [sent-723, score-0.306]
</p><p>64 2 that this cost function has an information retrieval interpretation: it corresponds to mean smoothed recall of retrieving neighbors of query points. [sent-728, score-0.448]
</p><p>65 Both choices are done based on the visualization, and the choices are compared by smoothed precision and smoothed recall to the relevant pairs of queries and neighbors that are deﬁned based on the input space. [sent-748, score-0.586]
</p><p>66 As a special case, setting λ = 1 in the above cost function, that is, optimizing only smoothed recall of the two-step retrieval task, yields the cost function of t-SNE. [sent-752, score-0.323]
</p><p>67 The main conceptual difference between NeRV and t-NeRV is that in t-NeRV the probability of picking a query point in the visualization and in the input space depends on the densities in the visualization and input space respectively; in NeRV all potential query points are treated equally. [sent-754, score-0.531]
</p><p>68 According to the mean smoothed precision and mean smoothed recall measures, t-NeRV does worse in terms of recall. [sent-764, score-0.6]
</p><p>69 The rank-based measures indicate a similar result; however, there t-NeRV does fairly well in terms of mean rank-based smoothed precision. [sent-765, score-0.241]
</p><p>70 The curves of mean precision versus mean recall show that t-NeRV does achieve better precision for small values of recall (i. [sent-767, score-0.418]
</p><p>71 All that is required for supervised visualization is to compute the input-space distances in a supervised manner. [sent-788, score-0.363]
</p><p>72 The distances are then plugged into the NeRV algorithm and the visualization proceeds as usual. [sent-789, score-0.275]
</p><p>73 Under certain parameter set474  D IMENSIONALITY R EDUCTION FOR V ISUALIZATION  Mean smoothed precision (vertical axis) − Mean smoothed recall (horizontal axis)  −400  −500  −600  t−NeRV  NeRV  B  λ=0  −650  −4  −2  0. [sent-801, score-0.538]
</p><p>74 2  Mean rank−based smoothed precision (vertical axis) − Mean rank−based smoothed recall (horizontal axis) 0. [sent-808, score-0.538]
</p><p>75 93  Mean smoothed precision in the t−NeRV sense (vertical axis) − Mean smoothed recall in the t−NeRV sense (horizontal axis)  λ=0  −0. [sent-832, score-0.538]
</p><p>76 3 (A-D), and for mean smoothed precision/recall corresponding to the information retrieval interpretation of t-NeRV (E; ﬁrst and second terms of Eqn. [sent-837, score-0.306]
</p><p>77 tings SNeRV can be seen as a new, supervised version of stochastic neighbor embedding, but more generally it manages a ﬂexible tradeoff between precision and recall of the information retrieval just like the unsupervised NeRV does. [sent-839, score-0.461]
</p><p>78 The class distribution is estimated through 476  D IMENSIONALITY R EDUCTION FOR V ISUALIZATION  Figure 11: Example visualization of the Faces data set with NeRV, here maximizing precision (tradeoff parameter λ = 0). [sent-856, score-0.332]
</p><p>79 477  (9)  V ENNA , P ELTONEN , N YBO , A IDOS AND K ASKI  Figure 12: Example visualization of the Faces data set with t-NeRV, here maximizing precision (tradeoff parameter λ = 0). [sent-865, score-0.332]
</p><p>80 Because the learning metric distances are deﬁned as minimal path integrals they preserve the topology of the input space; roughly speaking, if the distance between two points is small, then there must be a path between them where distances are small along the entire path. [sent-873, score-0.245]
</p><p>81 unsupervised visualization is up to the analyst; in general, supervised embedding will preserve differences between classes better but at the expense of within-class details. [sent-893, score-0.385]
</p><p>82 Multiple relational embedding (MRE; Memisevic and Hinton, 2005) was proposed as an extension of stochastic neighbor embedding (Hinton and Roweis, 2002). [sent-895, score-0.241]
</p><p>83 The only difference to unsupervised isomap is a new deﬁnition of the input-space distances: roughly speaking, distances between points in different classes will grow faster than distances between same-class points. [sent-905, score-0.383]
</p><p>84 The actual embedding is done in the same way as in unsupervised isomap (described in Section 4. [sent-906, score-0.294]
</p><p>85 5) to compare supervised NeRV and the ﬁve supervised visualization methods described in Section 5. [sent-915, score-0.29]
</p><p>86 2, namely multiple relational embedding (MRE), colored maximum variance unfolding (MUHSIC), supervised isomap (S-Isomap), parametric embedding (PE), and neighbourhood component analysis (NCA). [sent-916, score-0.42]
</p><p>87 3 for the unsupervised experiments, that is, by mean smoothed precision and recall; the only difference would be to use the supervised learning metric for the evaluation. [sent-920, score-0.464]
</p><p>88 Conclusions and Discussion By formulating the task of nonlinear projection for information visualization as an information retrieval task, we have derived a rigorously motivated pair of measures for visualization performance, mean smoothed precision and mean smoothed recall. [sent-981, score-1.081]
</p><p>89 Analogously, mean smoothed recall is an extension of mean recall, the proportion of misses incurred by the retrieved neighborhood. [sent-986, score-0.383]
</p><p>90 We introduced an algorithm called neighbor retrieval visualizer (NeRV) that optimizes the total cost, interpretable as a tradeoff between mean smoothed precision and mean smoothed recall. [sent-987, score-0.744]
</p><p>91 The earlier method stochastic neighbor embedding is obtained as a special case when λ = 1, optimizing mean smoothed recall. [sent-989, score-0.36]
</p><p>92 For unsupervised visualization, we simply use ﬁxed input distances; for supervised visualization we learn a supervised distance metric for the input space and plug the resulting input distances to the NeRV algorithm. [sent-991, score-0.474]
</p><p>93 NeRV is designed to ﬁnd a mapping that is, in a well-deﬁned sense, optimal for a certain type of visualization regardless of the intrinsic dimensionality of the data. [sent-997, score-0.241]
</p><p>94 Our notion of plug-in supervised metrics could in principle be used with other methods too; other unsupervised embedding algorithms that work based on a distance matrix can also be turned into supervised versions, by plugging in learning metric distances into the distance matrix. [sent-1005, score-0.396]
</p><p>95 An implementation of the NeRV and local MDS algorithms as well as the mean smoothed precision-mean smoothed recall measures is available at http://www. [sent-1010, score-0.469]
</p><p>96 2 we introduced Kullback-Leibler divergences as cost functions for visual neighbor retrieval, based on probability distributions qi and pi which generalize the relevance model implicit in precision and recall. [sent-1019, score-0.308]
</p><p>97 We deﬁne the probability of choosing a neighbor from the visualization as q∗ = j|i  ci ≡ 1−δ , if point j is in Qi ki δ di ≡ N−ki −1 , otherwise. [sent-1034, score-0.297]
</p><p>98 The number of misses, that is, the number of points that have a low probability in the visualization although the probability in the data is high, is NMISS,i . [sent-1040, score-0.237]
</p><p>99 This yields bi ci 1−δ (N − ki − 1) (1 − δ) = NMISS,i log + log ri δ ri δ δ (1 − δ) + NFP,i log − log N − ri − 1 N − ri − 1 ki (N − ki − 1) (1 − δ) 1−δ = NMISS,i log + log ri ri δ δ ki (1 − δ) + NFP,i log − log N − ri − 1 N − ri − 1 δ  D(p∗ , q∗ ) ≈ NMISS,i ai log i i  ai di  + NFP,i bi log  . [sent-1044, score-0.472]
</p><p>100 Because D(q∗ , p∗ ) and D(p∗ , q∗ ) are equivalent to precision and recall, and pi and qi can be i i i i seen as more sophisticated generalizations of p∗ and q∗ , we interpret D(qi , pi ) and D(pi , qi ) as more i i sophisticated generalizations of precision and recall. [sent-1048, score-0.502]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nerv', 0.589), ('visualization', 0.202), ('mvu', 0.19), ('mds', 0.187), ('cda', 0.185), ('smoothed', 0.18), ('isomap', 0.155), ('lmds', 0.151), ('lmvu', 0.151), ('snerv', 0.136), ('precision', 0.13), ('cca', 0.129), ('lle', 0.127), ('aski', 0.097), ('eltonen', 0.097), ('enna', 0.097), ('idos', 0.097), ('isualization', 0.097), ('ybo', 0.097), ('vv', 0.095), ('retrieval', 0.095), ('eduction', 0.092), ('embedding', 0.092), ('kaski', 0.083), ('gg', 0.078), ('aa', 0.078), ('distances', 0.073), ('imensionality', 0.071), ('hlle', 0.071), ('esc', 0.068), ('ww', 0.068), ('mm', 0.064), ('venna', 0.063), ('mg', 0.062), ('qi', 0.062), ('ff', 0.061), ('kh', 0.06), ('pi', 0.059), ('mre', 0.058), ('peltonen', 0.058), ('phoneme', 0.058), ('neighbor', 0.057), ('zz', 0.056), ('neighborhoods', 0.055), ('uu', 0.055), ('pp', 0.052), ('pca', 0.051), ('cc', 0.049), ('visualizations', 0.049), ('qq', 0.049), ('neighbors', 0.048), ('retrieved', 0.048), ('recall', 0.048), ('unsupervised', 0.047), ('le', 0.046), ('query', 0.046), ('misses', 0.045), ('samuel', 0.045), ('oo', 0.045), ('yy', 0.045), ('manifold', 0.045), ('supervised', 0.044), ('jarkko', 0.044), ('localmds', 0.044), ('muhsic', 0.044), ('tt', 0.043), ('ya', 0.042), ('timit', 0.041), ('jj', 0.041), ('faces', 0.04), ('nn', 0.04), ('ri', 0.04), ('tradeoff', 0.04), ('dimensionality', 0.039), ('landsat', 0.039), ('ki', 0.038), ('unfolding', 0.037), ('curvilinear', 0.037), ('ee', 0.036), ('weinberger', 0.035), ('points', 0.035), ('eigenmap', 0.034), ('nca', 0.034), ('trustworthiness', 0.034), ('mouse', 0.034), ('hh', 0.034), ('axis', 0.034), ('letter', 0.033), ('positives', 0.033), ('distance', 0.032), ('metric', 0.032), ('ss', 0.031), ('mean', 0.031), ('bb', 0.03), ('neighborhood', 0.03), ('mf', 0.03), ('measures', 0.03), ('laplacian', 0.029), ('aaa', 0.029), ('cmiss', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="54-tfidf-1" href="./jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization.html">54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</a></p>
<p>Author: Jarkko Venna, Jaakko Peltonen, Kristian Nybo, Helena Aidos, Samuel Kaski</p><p>Abstract: Nonlinear dimensionality reduction methods are often used to visualize high-dimensional data, although the existing methods have been designed for other related tasks such as manifold learning. It has been difﬁcult to assess the quality of visualizations since the task has not been well-deﬁned. We give a rigorous deﬁnition for a speciﬁc visualization task, resulting in quantiﬁable goodness measures and new visualization methods. The task is information retrieval given the visualization: to ﬁnd similar data based on the similarities shown on the display. The fundamental tradeoff between precision and recall of information retrieval can then be quantiﬁed in visualizations as well. The user needs to give the relative cost of missing similar points vs. retrieving dissimilar points, after which the total cost can be measured. We then introduce a new method NeRV (neighbor retrieval visualizer) which produces an optimal visualization by minimizing the cost. We further derive a variant for supervised visualization; class information is taken rigorously into account when computing the similarity relationships. We show empirically that the unsupervised version outperforms existing unsupervised dimensionality reduction methods in the visualization task, and the supervised version outperforms existing supervised methods. Keywords: information retrieval, manifold learning, multidimensional scaling, nonlinear dimensionality reduction, visualization</p><p>2 0.10080943 <a title="54-tfidf-2" href="./jmlr-2010-Dimensionality_Estimation%2C_Manifold_Learning_and_Function_Approximation_using_Tensor_Voting.html">30 jmlr-2010-Dimensionality Estimation, Manifold Learning and Function Approximation using Tensor Voting</a></p>
<p>Author: Philippos Mordohai, Gérard Medioni</p><p>Abstract: We address instance-based learning from a perceptual organization standpoint and present methods for dimensionality estimation, manifold learning and function approximation. Under our approach, manifolds in high-dimensional spaces are inferred by estimating geometric relationships among the input instances. Unlike conventional manifold learning, we do not perform dimensionality reduction, but instead perform all operations in the original input space. For this purpose we employ a novel formulation of tensor voting, which allows an N-D implementation. Tensor voting is a perceptual organization framework that has mostly been applied to computer vision problems. Analyzing the estimated local structure at the inputs, we are able to obtain reliable dimensionality estimates at each instance, instead of a global estimate for the entire data set. Moreover, these local dimensionality and structure estimates enable us to measure geodesic distances and perform nonlinear interpolation for data sets with varying density, outliers, perturbation and intersections, that cannot be handled by state-of-the-art methods. Quantitative results on the estimation of local manifold structure using ground truth data are presented. In addition, we compare our approach with several leading methods for manifold learning at the task of measuring geodesic distances. Finally, we show competitive function approximation results on real data. Keywords: dimensionality estimation, manifold learning, geodesic distance, function approximation, high-dimensional processing, tensor voting</p><p>3 0.063597456 <a title="54-tfidf-3" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>Author: Gal Chechik, Varun Sharma, Uri Shalit, Samy Bengio</p><p>Abstract: Learning a measure of similarity between pairs of objects is an important generic problem in machine learning. It is particularly useful in large scale applications like searching for an image that is similar to a given image or ﬁnding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, the approaches that exist today for learning such semantic similarity do not scale to large data sets. This is both because typically their CPU and storage requirements grow quadratically with the sample size, and because many methods impose complex positivity constraints on the space of learned similarity functions. The current paper presents OASIS, an Online Algorithm for Scalable Image Similarity learning that learns a bilinear similarity measure over sparse representations. OASIS is an online dual approach using the passive-aggressive family of learning algorithms with a large margin criterion and an efﬁcient hinge loss cost. Our experiments show that OASIS is both fast and accurate at a wide range of scales: for a data set with thousands of images, it achieves better results than existing state-of-the-art methods, while being an order of magnitude faster. For large, web scale, data sets, OASIS can be trained on more than two million images from 150K text queries within 3 days on a single CPU. On this large scale data set, human evaluations showed that 35% of the ten nearest neighbors of a given test image, as found by OASIS, were semantically relevant to that image. This suggests that query independent similarity could be accurately learned even for large scale data sets that could not be handled before. Keywords: large scale, metric learning, image similarity, online learning ∗. Varun Sharma and Uri Shalit contributed equally to this work. †. Also at ICNC, The Hebrew University of Jerusalem, 91904, Israel. c 2010 Gal Chechik, Varun Sharma, Uri Shalit</p><p>4 0.053940024 <a title="54-tfidf-4" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>Author: Miloš Radovanović, Alexandros Nanopoulos, Mirjana Ivanović</p><p>Abstract: Different aspects of the curse of dimensionality are known to present serious challenges to various machine-learning methods and tasks. This paper explores a new aspect of the dimensionality curse, referred to as hubness, that affects the distribution of k-occurrences: the number of times a point appears among the k nearest neighbors of other points in a data set. Through theoretical and empirical analysis involving synthetic and real data sets we show that under commonly used assumptions this distribution becomes considerably skewed as dimensionality increases, causing the emergence of hubs, that is, points with very high k-occurrences which effectively represent “popular” nearest neighbors. We examine the origins of this phenomenon, showing that it is an inherent property of data distributions in high-dimensional vector space, discuss its interaction with dimensionality reduction, and explore its inﬂuence on a wide range of machine-learning tasks directly or indirectly based on measuring distances, belonging to supervised, semi-supervised, and unsupervised learning families. Keywords: nearest neighbors, curse of dimensionality, classiﬁcation, semi-supervised learning, clustering</p><p>5 0.05026044 <a title="54-tfidf-5" href="./jmlr-2010-Why_Does_Unsupervised_Pre-training_Help_Deep_Learning%3F.html">117 jmlr-2010-Why Does Unsupervised Pre-training Help Deep Learning?</a></p>
<p>Author: Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, Samy Bengio</p><p>Abstract: Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difﬁcult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the inﬂuence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments conﬁrm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pretraining guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training. Keywords: deep architectures, unsupervised pre-training, deep belief networks, stacked denoising auto-encoders, non-convex optimization</p><p>6 0.049059384 <a title="54-tfidf-6" href="./jmlr-2010-Learning_Gradients%3A_Predictive_Models_that_Infer_Geometry_and_Statistical_Dependence.html">62 jmlr-2010-Learning Gradients: Predictive Models that Infer Geometry and Statistical Dependence</a></p>
<p>7 0.041223846 <a title="54-tfidf-7" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>8 0.03730879 <a title="54-tfidf-8" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>9 0.037017848 <a title="54-tfidf-9" href="./jmlr-2010-Rate_Minimaxity_of_the_Lasso_and_Dantzig_Selector_for_thelqLoss_inlrBalls.html">96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</a></p>
<p>10 0.036473308 <a title="54-tfidf-10" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>11 0.03427637 <a title="54-tfidf-11" href="./jmlr-2010-Consistent_Nonparametric_Tests_of_Independence.html">27 jmlr-2010-Consistent Nonparametric Tests of Independence</a></p>
<p>12 0.033919372 <a title="54-tfidf-12" href="./jmlr-2010-A_Streaming_Parallel_Decision_Tree_Algorithm.html">7 jmlr-2010-A Streaming Parallel Decision Tree Algorithm</a></p>
<p>13 0.033440083 <a title="54-tfidf-13" href="./jmlr-2010-Hilbert_Space_Embeddings_and_Metrics_on_Probability_Measures.html">47 jmlr-2010-Hilbert Space Embeddings and Metrics on Probability Measures</a></p>
<p>14 0.032952521 <a title="54-tfidf-14" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>15 0.031762473 <a title="54-tfidf-15" href="./jmlr-2010-Bayesian_Learning_in_Sparse_Graphical_Factor_Models_via_Variational_Mean-Field_Annealing.html">17 jmlr-2010-Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing</a></p>
<p>16 0.030819286 <a title="54-tfidf-16" href="./jmlr-2010-On_Over-fitting_in_Model_Selection_and_Subsequent_Selection_Bias_in_Performance_Evaluation.html">83 jmlr-2010-On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</a></p>
<p>17 0.0307131 <a title="54-tfidf-17" href="./jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</a></p>
<p>18 0.030617436 <a title="54-tfidf-18" href="./jmlr-2010-Continuous_Time_Bayesian_Network_Reasoning_and_Learning_Engine.html">28 jmlr-2010-Continuous Time Bayesian Network Reasoning and Learning Engine</a></p>
<p>19 0.029195299 <a title="54-tfidf-19" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>20 0.029013019 <a title="54-tfidf-20" href="./jmlr-2010-Learning_From_Crowds.html">61 jmlr-2010-Learning From Crowds</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.147), (1, 0.006), (2, -0.019), (3, 0.036), (4, -0.047), (5, 0.046), (6, -0.007), (7, -0.057), (8, 0.066), (9, -0.041), (10, -0.024), (11, 0.095), (12, 0.053), (13, 0.029), (14, -0.0), (15, 0.006), (16, -0.23), (17, 0.084), (18, -0.067), (19, -0.134), (20, 0.095), (21, 0.032), (22, 0.196), (23, 0.165), (24, -0.07), (25, 0.239), (26, -0.173), (27, -0.037), (28, -0.006), (29, 0.023), (30, 0.036), (31, -0.056), (32, 0.137), (33, 0.08), (34, -0.112), (35, 0.043), (36, 0.008), (37, -0.028), (38, -0.034), (39, -0.069), (40, 0.035), (41, 0.086), (42, 0.076), (43, 0.102), (44, -0.032), (45, -0.003), (46, -0.2), (47, 0.065), (48, 0.011), (49, 0.139)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93741685 <a title="54-lsi-1" href="./jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization.html">54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</a></p>
<p>Author: Jarkko Venna, Jaakko Peltonen, Kristian Nybo, Helena Aidos, Samuel Kaski</p><p>Abstract: Nonlinear dimensionality reduction methods are often used to visualize high-dimensional data, although the existing methods have been designed for other related tasks such as manifold learning. It has been difﬁcult to assess the quality of visualizations since the task has not been well-deﬁned. We give a rigorous deﬁnition for a speciﬁc visualization task, resulting in quantiﬁable goodness measures and new visualization methods. The task is information retrieval given the visualization: to ﬁnd similar data based on the similarities shown on the display. The fundamental tradeoff between precision and recall of information retrieval can then be quantiﬁed in visualizations as well. The user needs to give the relative cost of missing similar points vs. retrieving dissimilar points, after which the total cost can be measured. We then introduce a new method NeRV (neighbor retrieval visualizer) which produces an optimal visualization by minimizing the cost. We further derive a variant for supervised visualization; class information is taken rigorously into account when computing the similarity relationships. We show empirically that the unsupervised version outperforms existing unsupervised dimensionality reduction methods in the visualization task, and the supervised version outperforms existing supervised methods. Keywords: information retrieval, manifold learning, multidimensional scaling, nonlinear dimensionality reduction, visualization</p><p>2 0.64753187 <a title="54-lsi-2" href="./jmlr-2010-Dimensionality_Estimation%2C_Manifold_Learning_and_Function_Approximation_using_Tensor_Voting.html">30 jmlr-2010-Dimensionality Estimation, Manifold Learning and Function Approximation using Tensor Voting</a></p>
<p>Author: Philippos Mordohai, Gérard Medioni</p><p>Abstract: We address instance-based learning from a perceptual organization standpoint and present methods for dimensionality estimation, manifold learning and function approximation. Under our approach, manifolds in high-dimensional spaces are inferred by estimating geometric relationships among the input instances. Unlike conventional manifold learning, we do not perform dimensionality reduction, but instead perform all operations in the original input space. For this purpose we employ a novel formulation of tensor voting, which allows an N-D implementation. Tensor voting is a perceptual organization framework that has mostly been applied to computer vision problems. Analyzing the estimated local structure at the inputs, we are able to obtain reliable dimensionality estimates at each instance, instead of a global estimate for the entire data set. Moreover, these local dimensionality and structure estimates enable us to measure geodesic distances and perform nonlinear interpolation for data sets with varying density, outliers, perturbation and intersections, that cannot be handled by state-of-the-art methods. Quantitative results on the estimation of local manifold structure using ground truth data are presented. In addition, we compare our approach with several leading methods for manifold learning at the task of measuring geodesic distances. Finally, we show competitive function approximation results on real data. Keywords: dimensionality estimation, manifold learning, geodesic distance, function approximation, high-dimensional processing, tensor voting</p><p>3 0.55692315 <a title="54-lsi-3" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>Author: Gal Chechik, Varun Sharma, Uri Shalit, Samy Bengio</p><p>Abstract: Learning a measure of similarity between pairs of objects is an important generic problem in machine learning. It is particularly useful in large scale applications like searching for an image that is similar to a given image or ﬁnding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, the approaches that exist today for learning such semantic similarity do not scale to large data sets. This is both because typically their CPU and storage requirements grow quadratically with the sample size, and because many methods impose complex positivity constraints on the space of learned similarity functions. The current paper presents OASIS, an Online Algorithm for Scalable Image Similarity learning that learns a bilinear similarity measure over sparse representations. OASIS is an online dual approach using the passive-aggressive family of learning algorithms with a large margin criterion and an efﬁcient hinge loss cost. Our experiments show that OASIS is both fast and accurate at a wide range of scales: for a data set with thousands of images, it achieves better results than existing state-of-the-art methods, while being an order of magnitude faster. For large, web scale, data sets, OASIS can be trained on more than two million images from 150K text queries within 3 days on a single CPU. On this large scale data set, human evaluations showed that 35% of the ten nearest neighbors of a given test image, as found by OASIS, were semantically relevant to that image. This suggests that query independent similarity could be accurately learned even for large scale data sets that could not be handled before. Keywords: large scale, metric learning, image similarity, online learning ∗. Varun Sharma and Uri Shalit contributed equally to this work. †. Also at ICNC, The Hebrew University of Jerusalem, 91904, Israel. c 2010 Gal Chechik, Varun Sharma, Uri Shalit</p><p>4 0.51769114 <a title="54-lsi-4" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>Author: Miloš Radovanović, Alexandros Nanopoulos, Mirjana Ivanović</p><p>Abstract: Different aspects of the curse of dimensionality are known to present serious challenges to various machine-learning methods and tasks. This paper explores a new aspect of the dimensionality curse, referred to as hubness, that affects the distribution of k-occurrences: the number of times a point appears among the k nearest neighbors of other points in a data set. Through theoretical and empirical analysis involving synthetic and real data sets we show that under commonly used assumptions this distribution becomes considerably skewed as dimensionality increases, causing the emergence of hubs, that is, points with very high k-occurrences which effectively represent “popular” nearest neighbors. We examine the origins of this phenomenon, showing that it is an inherent property of data distributions in high-dimensional vector space, discuss its interaction with dimensionality reduction, and explore its inﬂuence on a wide range of machine-learning tasks directly or indirectly based on measuring distances, belonging to supervised, semi-supervised, and unsupervised learning families. Keywords: nearest neighbors, curse of dimensionality, classiﬁcation, semi-supervised learning, clustering</p><p>5 0.28305259 <a title="54-lsi-5" href="./jmlr-2010-Regularized_Discriminant_Analysis%2C_Ridge_Regression_and_Beyond.html">98 jmlr-2010-Regularized Discriminant Analysis, Ridge Regression and Beyond</a></p>
<p>Author: Zhihua Zhang, Guang Dai, Congfu Xu, Michael I. Jordan</p><p>Abstract: Fisher linear discriminant analysis (FDA) and its kernel extension—kernel discriminant analysis (KDA)—are well known methods that consider dimensionality reduction and classiﬁcation jointly. While widely deployed in practical problems, there are still unresolved issues surrounding their efﬁcient implementation and their relationship with least mean squares procedures. In this paper we address these issues within the framework of regularized estimation. Our approach leads to a ﬂexible and efﬁcient implementation of FDA as well as KDA. We also uncover a general relationship between regularized discriminant analysis and ridge regression. This relationship yields variations on conventional FDA based on the pseudoinverse and a direct equivalence to an ordinary least squares estimator. Keywords: Fisher discriminant analysis, reproducing kernel, generalized eigenproblems, ridge regression, singular value decomposition, eigenvalue decomposition</p><p>6 0.27105224 <a title="54-lsi-6" href="./jmlr-2010-Learning_Gradients%3A_Predictive_Models_that_Infer_Geometry_and_Statistical_Dependence.html">62 jmlr-2010-Learning Gradients: Predictive Models that Infer Geometry and Statistical Dependence</a></p>
<p>7 0.24240394 <a title="54-lsi-7" href="./jmlr-2010-Characterization%2C_Stability_and_Convergence_of_Hierarchical_Clustering_Methods.html">19 jmlr-2010-Characterization, Stability and Convergence of Hierarchical Clustering Methods</a></p>
<p>8 0.22529465 <a title="54-lsi-8" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>9 0.20785692 <a title="54-lsi-9" href="./jmlr-2010-Evolving_Static_Representations_for_Task_Transfer.html">37 jmlr-2010-Evolving Static Representations for Task Transfer</a></p>
<p>10 0.19836789 <a title="54-lsi-10" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<p>11 0.19407001 <a title="54-lsi-11" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>12 0.19055402 <a title="54-lsi-12" href="./jmlr-2010-A_Streaming_Parallel_Decision_Tree_Algorithm.html">7 jmlr-2010-A Streaming Parallel Decision Tree Algorithm</a></p>
<p>13 0.1836406 <a title="54-lsi-13" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>14 0.18269503 <a title="54-lsi-14" href="./jmlr-2010-Rate_Minimaxity_of_the_Lasso_and_Dantzig_Selector_for_thelqLoss_inlrBalls.html">96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</a></p>
<p>15 0.18250507 <a title="54-lsi-15" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>16 0.178381 <a title="54-lsi-16" href="./jmlr-2010-Why_Does_Unsupervised_Pre-training_Help_Deep_Learning%3F.html">117 jmlr-2010-Why Does Unsupervised Pre-training Help Deep Learning?</a></p>
<p>17 0.17540623 <a title="54-lsi-17" href="./jmlr-2010-Bayesian_Learning_in_Sparse_Graphical_Factor_Models_via_Variational_Mean-Field_Annealing.html">17 jmlr-2010-Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing</a></p>
<p>18 0.17094412 <a title="54-lsi-18" href="./jmlr-2010-Expectation_Truncation_and_the_Benefits_of_Preselection_In_Training_Generative_Models.html">38 jmlr-2010-Expectation Truncation and the Benefits of Preselection In Training Generative Models</a></p>
<p>19 0.16986848 <a title="54-lsi-19" href="./jmlr-2010-Approximate_Riemannian_Conjugate_Gradient_Learning_for_Fixed-Form_Variational_Bayes.html">14 jmlr-2010-Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes</a></p>
<p>20 0.16680324 <a title="54-lsi-20" href="./jmlr-2010-On_Learning_with_Integral_Operators.html">82 jmlr-2010-On Learning with Integral Operators</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.025), (4, 0.026), (8, 0.013), (17, 0.389), (21, 0.024), (24, 0.011), (32, 0.055), (33, 0.013), (36, 0.031), (37, 0.043), (52, 0.01), (75, 0.107), (81, 0.056), (83, 0.014), (85, 0.06), (96, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69978052 <a title="54-lda-1" href="./jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization.html">54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</a></p>
<p>Author: Jarkko Venna, Jaakko Peltonen, Kristian Nybo, Helena Aidos, Samuel Kaski</p><p>Abstract: Nonlinear dimensionality reduction methods are often used to visualize high-dimensional data, although the existing methods have been designed for other related tasks such as manifold learning. It has been difﬁcult to assess the quality of visualizations since the task has not been well-deﬁned. We give a rigorous deﬁnition for a speciﬁc visualization task, resulting in quantiﬁable goodness measures and new visualization methods. The task is information retrieval given the visualization: to ﬁnd similar data based on the similarities shown on the display. The fundamental tradeoff between precision and recall of information retrieval can then be quantiﬁed in visualizations as well. The user needs to give the relative cost of missing similar points vs. retrieving dissimilar points, after which the total cost can be measured. We then introduce a new method NeRV (neighbor retrieval visualizer) which produces an optimal visualization by minimizing the cost. We further derive a variant for supervised visualization; class information is taken rigorously into account when computing the similarity relationships. We show empirically that the unsupervised version outperforms existing unsupervised dimensionality reduction methods in the visualization task, and the supervised version outperforms existing supervised methods. Keywords: information retrieval, manifold learning, multidimensional scaling, nonlinear dimensionality reduction, visualization</p><p>2 0.3610833 <a title="54-lda-2" href="./jmlr-2010-An_Exponential_Model_for_Infinite_Rankings.html">10 jmlr-2010-An Exponential Model for Infinite Rankings</a></p>
<p>Author: Marina Meilă, Le Bao</p><p>Abstract: This paper presents a statistical model for expressing preferences through rankings, when the number of alternatives (items to rank) is large. A human ranker will then typically rank only the most preferred items, and may not even examine the whole set of items, or know how many they are. Similarly, a user presented with the ranked output of a search engine, will only consider the highest ranked items. We model such situations by introducing a stagewise ranking model that operates with ﬁnite ordered lists called top-t orderings over an inﬁnite space of items. We give algorithms to estimate this model from data, and demonstrate that it has sufﬁcient statistics, being thus an exponential family model with continuous and discrete parameters. We describe its conjugate prior and other statistical properties. Then, we extend the estimation problem to multimodal data by introducing an Exponential-Blurring-Mean-Shift nonparametric clustering algorithm. The experiments highlight the properties of our model and demonstrate that inﬁnite models over permutations can be simple, elegant and practical. Keywords: permutations, partial orderings, Mallows model, distance based ranking model, exponential family, non-parametric clustering, branch-and-bound</p><p>3 0.35901964 <a title="54-lda-3" href="./jmlr-2010-Dimensionality_Estimation%2C_Manifold_Learning_and_Function_Approximation_using_Tensor_Voting.html">30 jmlr-2010-Dimensionality Estimation, Manifold Learning and Function Approximation using Tensor Voting</a></p>
<p>Author: Philippos Mordohai, Gérard Medioni</p><p>Abstract: We address instance-based learning from a perceptual organization standpoint and present methods for dimensionality estimation, manifold learning and function approximation. Under our approach, manifolds in high-dimensional spaces are inferred by estimating geometric relationships among the input instances. Unlike conventional manifold learning, we do not perform dimensionality reduction, but instead perform all operations in the original input space. For this purpose we employ a novel formulation of tensor voting, which allows an N-D implementation. Tensor voting is a perceptual organization framework that has mostly been applied to computer vision problems. Analyzing the estimated local structure at the inputs, we are able to obtain reliable dimensionality estimates at each instance, instead of a global estimate for the entire data set. Moreover, these local dimensionality and structure estimates enable us to measure geodesic distances and perform nonlinear interpolation for data sets with varying density, outliers, perturbation and intersections, that cannot be handled by state-of-the-art methods. Quantitative results on the estimation of local manifold structure using ground truth data are presented. In addition, we compare our approach with several leading methods for manifold learning at the task of measuring geodesic distances. Finally, we show competitive function approximation results on real data. Keywords: dimensionality estimation, manifold learning, geodesic distance, function approximation, high-dimensional processing, tensor voting</p><p>4 0.35564303 <a title="54-lda-4" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>Author: Miloš Radovanović, Alexandros Nanopoulos, Mirjana Ivanović</p><p>Abstract: Different aspects of the curse of dimensionality are known to present serious challenges to various machine-learning methods and tasks. This paper explores a new aspect of the dimensionality curse, referred to as hubness, that affects the distribution of k-occurrences: the number of times a point appears among the k nearest neighbors of other points in a data set. Through theoretical and empirical analysis involving synthetic and real data sets we show that under commonly used assumptions this distribution becomes considerably skewed as dimensionality increases, causing the emergence of hubs, that is, points with very high k-occurrences which effectively represent “popular” nearest neighbors. We examine the origins of this phenomenon, showing that it is an inherent property of data distributions in high-dimensional vector space, discuss its interaction with dimensionality reduction, and explore its inﬂuence on a wide range of machine-learning tasks directly or indirectly based on measuring distances, belonging to supervised, semi-supervised, and unsupervised learning families. Keywords: nearest neighbors, curse of dimensionality, classiﬁcation, semi-supervised learning, clustering</p><p>5 0.35196409 <a title="54-lda-5" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>Author: Yevgeny Seldin, Naftali Tishby</p><p>Abstract: We derive PAC-Bayesian generalization bounds for supervised and unsupervised learning models based on clustering, such as co-clustering, matrix tri-factorization, graphical models, graph clustering, and pairwise clustering.1 We begin with the analysis of co-clustering, which is a widely used approach to the analysis of data matrices. We distinguish among two tasks in matrix data analysis: discriminative prediction of the missing entries in data matrices and estimation of the joint probability distribution of row and column variables in co-occurrence matrices. We derive PAC-Bayesian generalization bounds for the expected out-of-sample performance of co-clustering-based solutions for these two tasks. The analysis yields regularization terms that were absent in the previous formulations of co-clustering. The bounds suggest that the expected performance of co-clustering is governed by a trade-off between its empirical performance and the mutual information preserved by the cluster variables on row and column IDs. We derive an iterative projection algorithm for ﬁnding a local optimum of this trade-off for discriminative prediction tasks. This algorithm achieved stateof-the-art performance in the MovieLens collaborative ﬁltering task. Our co-clustering model can also be seen as matrix tri-factorization and the results provide generalization bounds, regularization terms, and new algorithms for this form of matrix factorization. The analysis of co-clustering is extended to tree-shaped graphical models, which can be used to analyze high dimensional tensors. According to the bounds, the generalization abilities of treeshaped graphical models depend on a trade-off between their empirical data ﬁt and the mutual information that is propagated up the tree levels. We also formulate weighted graph clustering as a prediction problem: given a subset of edge weights we analyze the ability of graph clustering to predict the remaining edge weights. The analysis of co-clustering easily</p><p>6 0.34989974 <a title="54-lda-6" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>7 0.34868741 <a title="54-lda-7" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>8 0.34734017 <a title="54-lda-8" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>9 0.34641257 <a title="54-lda-9" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>10 0.34460685 <a title="54-lda-10" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>11 0.34227598 <a title="54-lda-11" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>12 0.34081119 <a title="54-lda-12" href="./jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</a></p>
<p>13 0.33995557 <a title="54-lda-13" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>14 0.33943462 <a title="54-lda-14" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>15 0.33772737 <a title="54-lda-15" href="./jmlr-2010-Spectral_Regularization_Algorithms_for_Learning_Large_Incomplete_Matrices.html">105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</a></p>
<p>16 0.33692208 <a title="54-lda-16" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>17 0.33668301 <a title="54-lda-17" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>18 0.33566153 <a title="54-lda-18" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>19 0.335352 <a title="54-lda-19" href="./jmlr-2010-Bundle_Methods_for_Regularized_Risk_Minimization.html">18 jmlr-2010-Bundle Methods for Regularized Risk Minimization</a></p>
<p>20 0.33526796 <a title="54-lda-20" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
