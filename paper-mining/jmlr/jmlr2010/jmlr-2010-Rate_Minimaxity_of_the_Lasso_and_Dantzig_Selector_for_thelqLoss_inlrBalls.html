<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-96" href="#">jmlr2010-96</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</h1>
<br/><p>Source: <a title="jmlr-2010-96-pdf" href="http://jmlr.org/papers/volume11/ye10a/ye10a.pdf">pdf</a></p><p>Author: Fei Ye, Cun-Hui Zhang</p><p>Abstract: We consider the estimation of regression coefﬁcients in a high-dimensional linear model. For regression coefﬁcients in ℓr balls, we provide lower bounds for the minimax ℓq risk and minimax quantiles of the ℓq loss for all design matrices. Under an ℓ0 sparsity condition on a target coefﬁcient vector, we sharpen and unify existing oracle inequalities for the Lasso and Dantzig selector. We derive oracle inequalities for target coefﬁcient vectors with many small elements and smaller threshold levels than the universal threshold. These oracle inequalities provide sufﬁcient conditions on the design matrix for the rate minimaxity of the Lasso and Dantzig selector for the ℓq risk and loss in ℓr balls, 0 ≤ r ≤ 1 ≤ q ≤ ∞. By allowing q = ∞, our risk bounds imply the variable selection consistency of threshold Lasso and Dantzig selectors. Keywords: variable selection, estimation, oracle inequality, minimax, linear regression, penalized least squares, linear programming</p><p>Reference: <a title="jmlr-2010-96-reference" href="../jmlr2010_reference/jmlr-2010-Rate_Minimaxity_of_the_Lasso_and_Dantzig_Selector_for_thelqLoss_inlrBalls_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 For regression coefﬁcients in ℓr balls, we provide lower bounds for the minimax ℓq risk and minimax quantiles of the ℓq loss for all design matrices. [sent-5, score-0.364]
</p><p>2 Under an ℓ0 sparsity condition on a target coefﬁcient vector, we sharpen and unify existing oracle inequalities for the Lasso and Dantzig selector. [sent-6, score-0.319]
</p><p>3 We derive oracle inequalities for target coefﬁcient vectors with many small elements and smaller threshold levels than the universal threshold. [sent-7, score-0.251]
</p><p>4 These oracle inequalities provide sufﬁcient conditions on the design matrix for the rate minimaxity of the Lasso and Dantzig selector for the ℓq risk and loss in ℓr balls, 0 ≤ r ≤ 1 ≤ q ≤ ∞. [sent-8, score-0.639]
</p><p>5 Keywords: variable selection, estimation, oracle inequality, minimax, linear regression, penalized least squares, linear programming  1. [sent-10, score-0.174]
</p><p>6 Speciﬁcally, we are interested in the rate minimaxity of the Lasso and Dantzig selector under the ℓq loss for the estimation of regression coefﬁcients in ℓr balls. [sent-16, score-0.371]
</p><p>7 This requires lower bounds of the minimax ℓq risk and minimax quantiles of the ℓq loss over all estimators as well as matching upper bounds for the Lasso and Dantzig selector. [sent-17, score-0.392]
</p><p>8 (2)  A focus of recent studies of high-dimensional linear regression has been on the performance of the Lasso and Dantzig selector for the estimation of the regression coefﬁcients. [sent-27, score-0.249]
</p><p>9 Candes and Tao (2007) derived an elegant probabilistic upper bound for the ℓ2 loss of the Dantzig selector under a condition on the number of nonzero coefﬁcients and a uniform uncertainty principle on the Gram matrix. [sent-28, score-0.224]
</p><p>10 For N(0, σ2 ) errors and standardized designs with x j = n, these studies require a universal penalty level λuniv = σ (2/n) log p or greater for the Dantzig selector and a penalty level λ greater by a constant factor than λuniv for the Lasso. [sent-32, score-0.319]
</p><p>11 We provide lower bounds for the minimax ℓq risk in ℓr balls and the minimax quantiles of the ℓq loss for all designs X. [sent-35, score-0.403]
</p><p>12 We derive sufﬁcient conditions on X for the Lasso and Dantzig selector to attain the rate of the minimax ℓq risk and the minimax quantiles of the ℓq loss. [sent-36, score-0.491]
</p><p>13 We provide oracle inequalities for the ℓq loss of the Lasso and Dantzig selector which sharpen, unify and extend the existing results and allow the penalty level λ to be of smaller order than the universal penalty level. [sent-37, score-0.572]
</p><p>14 In Section 2, we describe lower bounds for the minimax risk and loss in ℓr balls. [sent-39, score-0.2]
</p><p>15 In Section 3, we provide oracle inequalities for the Lasso and Dantzig selector under the ℓ0 sparsity of regression coefﬁcients. [sent-40, score-0.501]
</p><p>16 We compare these oracle inequalities with existing ones and describe their implications in variable selection and rate minimaxity in ℓ0 balls. [sent-41, score-0.375]
</p><p>17 In Section 4, we provide more general oracle inequalities to allow many small regression coefﬁcients and penalty levels of smaller order than σ (2/n) log p. [sent-42, score-0.351]
</p><p>18 These oracle inequalities are used to establish the rate minimaxity for the ℓq loss in ℓr balls. [sent-43, score-0.396]
</p><p>19 Let σn = σ/ n and r  0  ≤ k},  (4)  σr p 1/2 n , λmm = σn 2 log(p/k), (5) Rr be respectively the universal (univ) penalty level (Donoho and Johnstone, 1994) and the minimax (mm) penalty levels for the ℓr and ℓ0 balls. [sent-68, score-0.185]
</p><p>20 Here the inﬁmum is taken over all Borel mappings mm δ of proper dimensions. [sent-71, score-0.472]
</p><p>21 For any class of vectors Θ ⊂ R p , the minimax ℓq risk is Rq (Θ; X) = q infδ supβ∈Θ Eβ,X δ(X, y) − β q . [sent-73, score-0.147]
</p><p>22 Then, mm Rq (Θr,R ; X) q−r Rr λmm  ≥ (1 + o(1)),  Rq (Θ0,k ; X) ≥ (1 + o(1)). [sent-76, score-0.472]
</p><p>23 q kλmm  (6)  Moreover, for either Θ = Θr,R with k = Rr /λr or Θ = Θ0,k , mm inf inf sup Pβ,X X  δ β∈Θ  δ(X, y) − β  q q  ≥ (1 − ε)kλq mm ≥ 3521  ε + o(1) , 0 < ε < 1. [sent-77, score-1.11]
</p><p>24 3q  (7)  Y E AND Z HANG  Remark 2 The value k in (7) can be viewed as an intrinsic lower bound for the number of parameters to be estimated for the minimaxity under the ℓq loss. [sent-78, score-0.124]
</p><p>25 2 provide conditions on X for the Lasso and Dantzig selector to match the rate of q the minimax lower bound kλmm in (7). [sent-82, score-0.31]
</p><p>26 For example, the minimax risk in a parameter class Θ ∋ 0 is no smaller than the radius of the null set Θ ∩ {b : Xb = 0}. [sent-87, score-0.147]
</p><p>27 Oracle Inequalities under ℓ0 Sparsity and Variable Selection We discuss in three subsections oracle inequalities for the ℓq loss, related work, and variable selection. [sent-91, score-0.251]
</p><p>28 We ﬁrst present oracle inequalities in terms of the (sign-restricted) CIF. [sent-102, score-0.251]
</p><p>29 (D)  (L)  Theorem 3 Let β and β be the Lasso and Dantzig selector in (1) and (2), 1 ≤ q ≤ ∞, β∗ ∈ R p , J = { j : β∗ = 0} and z∗ = X ′ (y − Xβ∗ )/n ∞ . [sent-103, score-0.203]
</p><p>30 Theorem 3 immediately provides a sufﬁcient condition on the design X for the rate minimaxity of the Lasso and Dantzig selector in the quantiles of the ℓq loss in ℓ0 balls, in the sense of (7) of Theorem 1. [sent-107, score-0.382]
</p><p>31 Then, the Dantzig selector β = (D)  β  with λ = λuniv in (5) is rate minimax in the sense of sup Pβ,X  β∈Θ0,k  β−β  q  ≥ M∗ k1/q λmm → 0. [sent-112, score-0.35]
</p><p>32 We show that the (sign-restricted) CIF-based oracle inequalities in Theorem 3 and Corollary 7 sharpen, unify and extend existing performance bounds and offer a clear explanation of the differences among existing analyses. [sent-138, score-0.302]
</p><p>33 The CIF and SCIF in (8) and (9) are related to the restricted eigenvalues (RE) REq,ℓ (ξ, J) = inf  |J|1/q Xu |nJ|1/2 uA  q  : u ∈ C (ξ, J), |A \ J| ≤ ℓ ,  (25)  since they all conveniently provide factors required in proofs of the same type of oracle inequalities. [sent-139, score-0.237]
</p><p>34 The use of the gradient bound in (8) and (9) provides sharper oracle inequalities by allowing the ﬂexibility with the choice of v in (18) and (19). [sent-146, score-0.299]
</p><p>35 For the ℓ2 loss of the Dantzig selector, the oracle inequality of Bickel, Ritov, and Tsybakov (2009) and Theorem 3 can be compared as follows: for z∗ ≤ λ h(D) ≤  4(1 + |J|/ℓ)|J|1/2 λ 2|J|1/2 λ 4|J|1/2 λ ≤ ≤ . [sent-147, score-0.222]
</p><p>36 The third inequality in (26) follows from RE1,0 (ξ, J) ≥ RE2,ℓ (ξ, J) as in van de Geer and B¨ hlmann (2009) and (1 + ξ|J|/ℓ)1/2 RE2,p (ξ, J) ≥ RE2,ℓ (ξ, J) by u the shifting inequality in Candes and Tao (2007). [sent-154, score-0.15]
</p><p>37 A signiﬁcant difference between the (sign-restricted) CIF- and RE- based oracle inequalities is their relationships to the oracle inequalities of Candes and Tao (2007) and Zhang (2009b). [sent-160, score-0.502]
</p><p>38 While 3525  Y E AND Z HANG  Theorem 3 is sharper than these inequalities, the RE-based oracle inequalities seem not to have this property. [sent-161, score-0.299]
</p><p>39 The 1/2 1/2 square of the potentially small ( ΣA uA − ΣAc uAc )+ has been used to bound u′ Σu = Xu 2 /n from below in the proofs of RE-based oracle inequalities. [sent-163, score-0.174]
</p><p>40 6) in Bickel, Ritov, and Tsybakov (2009), even after an application of sharper shifting inequalities to their Lemma 4. [sent-167, score-0.161]
</p><p>41 Corollary 7 sharpens the oracle inequality of Candes and Tao (2007). [sent-171, score-0.22]
</p><p>42 It turns A out that the combination of (11) and (19) with the same choice of w and (r, ar ) = (∞, 1) is at least by a factor of 5/12 sharper than (31), even with the suboptimal aq = 1. [sent-185, score-0.203]
</p><p>43 Moreover, the error bound in (23) for the Dantzig selector is also of the order k1/q λ for q ≤ 2 with much smaller constant factors, and the difference between k1/q and (k + ℓ)1/q diminishes for large q as in Theorem 8. [sent-188, score-0.203]
</p><p>44 We have proved that Theorem 3 sharpens and uniﬁes a number of existing oracle inequalities for the Lasso and Dantzig estimators, so that they can all be viewed as (possibly more explicit) upper bounds for the right-hand sides of (10) and (11). [sent-190, score-0.302]
</p><p>45 The choice r = ∞ in (16), and consequently in (18), (19), (20) and (21), typically gives oracle inequalities of the sharpest form involving the dimensionnormalized · ∞,q norm as in (30), compared with the typical · q,q norm in the literature. [sent-191, score-0.276]
</p><p>46 Although (10) and (11) are of the same format, the J Dantzig selector requires smaller λ and smaller ξ = 1. [sent-193, score-0.203]
</p><p>47 This theoretical advantage of the Dantzig selector for z∗ ≤ λ reverses if β ∞ λ. [sent-194, score-0.203]
</p><p>48 We focus on the Dantzig selector here although parallel results can be obtained for the Lasso in the same way. [sent-201, score-0.203]
</p><p>49 Lounici (2008) considered thresholding either the Lasso or the Dantzig selector under the stronger condition max j=k |(Σ) jk | ≤ 1/{α(1 + 2ξ)|J|} with α > 1, (Σ) j j = 1, ξ = 3 for the Lasso, and ξ = 1 for the Dantzig selector. [sent-203, score-0.232]
</p><p>50 Candes and Tao (2007) considered the Gauss-Dantzig selector (GD)  β  = arg min b  (D)  y − Xb : |β j | ≤ λ′ ⇒ b j = 0, ∀ j, β = β  (λ) . [sent-204, score-0.256]
</p><p>51 (32)  For threshold functions t(x; λ) satisfying {x : t(x; λ) = 0} = {x : |x| ≤ λ} and xt(x; λ) ≥ 0, deﬁne the threshold Dantzig selector as β  (T D)  (D)  =t β  (λ); λ′ . [sent-205, score-0.203]
</p><p>52 The ﬁrst subsection provides non-probabilistic oracle inequalities: conditions on the data (X, y) and a target coefﬁcient vector β∗ for upper bounds of β − β∗ q for the Lasso and Dantzig selector. [sent-216, score-0.206]
</p><p>53 The second subsection provides sufﬁcient condition on the design X for the rate minimaxity for the ℓq loss in ℓr balls under Pβ,X . [sent-217, score-0.207]
</p><p>54 1 Oracle Inequalities The oracle inequalities here differ from Theorem 3 (i) by allowing target vectors with many small entries and smaller penalty levels. [sent-219, score-0.29]
</p><p>55 Let ∞ q,d q,d Crelax (ξ, J, d) = u : uJ c  1  ≤ ξd 1/2 max  |A|=d,A⊇J  uA  (39)  as a relaxed cone, and deﬁne the corresponding relaxed CIF as CIFq, relax (ξ, J, d) =  d 1/q ΣA,∗ u : A = arg max uB d 1/2 u q |B|=d,B⊇J  inf  u∈Crelax (ξ,J,d)  . [sent-236, score-0.242]
</p><p>56 (40)  The following quantity plays the role of (35) for relaxed cones: Mq, relax (λ, ρ, J, d) = sup  u  u 1 ≤ρ  q  : ΣA,∗ u ≤ d 1/2 λ, A = arg max uB  . [sent-237, score-0.169]
</p><p>57 Then, for z∗ ≤ λ(ξ0 − 1)/(ξ0 + 1), J 2,d 2,d (L)  β  − β∗  q  ≤ max  ξ1 d 1/q λ , Mq, relax ξ1 λ, ξ2 ρJ , J, d CIFq, relax (ξ, J, d)  ,  (42)  where ξ1 = 2ξ0 /(ξ0 + 1) and ξ2 = (ξ0 + 1)(ξ + 1)/(ξ − ξ0 ). [sent-244, score-0.135]
</p><p>58 Zhang (2010) derived similar oracle inequalities for the Lasso, MC+, and other concave penalized least squares estimators at the same λ under the sparse Riesz condition |J| ≤ d/{(1 + δ+ )/(1 − δ− ) + 1/2}. [sent-247, score-0.27]
</p><p>59 We use the following quantities to bound the constant factors in Theorems 10 and 12: ηq,d = max Σ−1 A |A|=d  ∞,q /d  1/q  , η∗ = max Σ−1 q,d A A  q,q ,  κq,d,ℓ = max min max ℓ1−1/q (ar /ℓ)1−1/r Σ−1 ΣA,B A |A|=d r≥1 |B|=ℓ  3529  r,q . [sent-250, score-0.164]
</p><p>60 Let {ηq,d , η∗ , κq,d,ℓ } be as in (43) and aq = (1 − 1/q)/q1/(q−1) . [sent-252, score-0.122]
</p><p>61 2 Rate ℓq Minimax Estimation in ℓr Balls We present sufﬁcient conditions for the rate minimaxity of the Lasso and Dantzig selector in ℓr balls in (4) in the sense of (6) and (7). [sent-258, score-0.389]
</p><p>62 Suppose (log p)/n = O(1) and Rr /λr ≍ d ≤ n ∧ p for some mm integer d → ∞ satisfying (log d)/ log p ≤ c0 < 1. [sent-262, score-0.51]
</p><p>63 Let 0 < α0 < 1 and β be either the Lasso or 2 2 Dantzig selector with λ = λuniv /α0 . [sent-263, score-0.203]
</p><p>64 Then, supβ∈Θr,R Eβ,X β − β  q q  Rr λmm  q−r  max  C1 I{r < 1} 1 ξ∗C2 ,C1 Mq 1/q , 1/q−1 CIFq∗ (ξ, d) d d  q  ,  (47)  √ √ where C1 = 2(dλr /Rr )1/q /(α0 1 − c0 ), C2 = α0 1 − c0 R/(d 1/r λmm ), CIFq∗ (ξ, d) is as in (12) mm and Mq (λ, ρ) is as in (35). [sent-267, score-0.501]
</p><p>65 Consequently, if either ηq,d + κq,d,d = O(1) with the {ηq,d , κq,d,d } in (43) or Mq (d −1/q , d 1−1/q ) + I{r < 1}/CIFq∗ (ξ, d) = O(1), then sup Eβ,X β − β  β r ≤R  q q  inf sup Eβ,X δ(X, y) − β q . [sent-268, score-0.143]
</p><p>66 Theorem 17 differs from existing results by directly comparing the ℓq risk of estimators with the minimax risk, instead of ﬁnding upper bounds for the ℓq loss. [sent-271, score-0.198]
</p><p>67 It is based on the oracle inequality for λ > λuniv in Theorem 10. [sent-272, score-0.201]
</p><p>68 For λmm /λuniv = o(1), 3530  R ATE M INIMAXITY OF ℓ1 M ETHODS  oracle inequalities requiring penalty levels λ ≥ λuniv do not match the order of the minimax lower bounds in Theorem 1. [sent-275, score-0.429]
</p><p>69 For example, when p = n log n and d ≍ Rr /λr ≍ n/ log log n, λmm /λuniv → 0 mm as n → ∞ and the regularity conditions on X may still hold. [sent-276, score-0.605]
</p><p>70 Theorem 19 below closes this gap by providing the rate minimaxity of the Lasso in the quantiles of the ℓq loss with λ ≍ λmm = o(λuniv ). [sent-277, score-0.179]
</p><p>71 Deﬁne ∗ CIFq,relax (ξ, k, d) =  inf  |A|=d  ∗ Mq,relax (λ, ρ, k, d) =  d 1/q ΣA,∗ u : min uJ c d 1/2 u q |A\J|=d−k  sup  u  q  |A|=d  : ΣA,∗ u ≤ d 1/2 λ,  1  < ξd 1/2 uA  min  |A\J|=d−k  uJ c  1  ,  (49)  ≤ρ . [sent-278, score-0.157]
</p><p>72 Suppose n ∧ p ≥ d ≍ Rr /λr → ∞, δ+ = O(1) and mm d λmm n1/2 /σ → ∞. [sent-281, score-0.472]
</p><p>73 Then, the Lasso is rate minimax in ℓr balls in the sense that for all ε > 0, (L)  β  inf t : sup Pβ,X β r ≤R  inf t : inf sup Pβ,X δ  β r ≤R  −β  q q  q−r ≥ t q Rr λmm ≤ ε  δ(X, y) − β  q q  (51)  q−r ≥ t q Rr λmm ≤ ε < ∞. [sent-283, score-0.438]
</p><p>74 In such comparisons, the H¨ lder inequality and (22) give o ηq,d ≤ η∗ , κq,d,ℓ ≤ κ∗ , η∗ = 1/(1 − δ− ), κ∗ ≤ θd,ℓ η∗ , q,d q,d,ℓ 2,d 2,d,ℓ 2,d d 1−1/q  where κ∗ = aq q,d,ℓ  maxA,B Σ−1 ΣA,B A  q,q  / with |A| = d, |B| = ℓ and A ∩ B = 0. [sent-287, score-0.174]
</p><p>75 2, the use of Σu ∞ in the numerator of (8) and (9) seems necessary to ensure the dominance of Theorem 3 over the oracle inequalities of the type (30). [sent-294, score-0.251]
</p><p>76 However, if we make the numerator quadratic in u, Corollary 7 still holds up to a constant factor with the following weak CIF: w CIFq,ℓ (ξ, J) =  |J|1/q u′ ΣA,∗ u |J|1/q u′ ΣA,∗ u w A A , SCIFq,ℓ (ξ, J) = inf , uJ 1 uA q uJ 1 uA q u∈C (ξ,J) u∈C− (ξ,J) inf  (53)  where A = arg max|A\J|≤ℓ uA . [sent-295, score-0.152]
</p><p>77 Since the oracle inequalities in this paper apply directly to data points (X, y) and target vectors β∗ , the normality assumption on the error in (3) is not crucial for the upper bounds for the estimation risk and loss (not even the condition Eβ,X y = Xβ). [sent-297, score-0.344]
</p><p>78 q,d For λ < λuniv , the proof of Theorem 12 can be extended to the Dantzig selector with the feasi(L)  bility of β∗ replaced by the feasibility of β . [sent-302, score-0.224]
</p><p>79 However, if we modify the Dantzig selector as β = arg min b  b  1  : max X ′ (y − Xb) ≤ A |A|=d  √  dλ ,  (54)  the feasibility of β∗ would be guaranteed in the event z∗ ≤ λ even for λ = o(λuniv ) as in Theorems 2,d 12 and 19. [sent-304, score-0.331]
</p><p>80 By (5), mm λmm /σn = 2 log(p/k) − r log(λmm /σn ) for r > 0. [sent-313, score-0.472]
</p><p>81 Under Pµ,w , β r = Nµr and β q = Nµq , so that r β ∈ Θ ⇔ N ≤ k/(1 − ε)r ⇒ β  q q  ≤ kλq , ∀ r ≥ 0, mm  (57)  p due to Rr /µr = k/(1 − ε)r and {k/(1 − ε)r }µ p /λmm = k(1 − ε)q−r ≤ k for r > 0. [sent-319, score-0.472]
</p><p>82 Since the conditional Bayes risk of δ∗ is no greater than the minimax risk in Θ, (1 + o(1))kλq mm ≤ Eµ,w Eβ,X δ∗ − β q β ∈ Θ + Eµ,w Eβ,X δ∗ − β q I{β ∈ Θ} q q ≤ R(Θ; X) + 2(q−1)+ Eµ,w Eβ,X δ∗ q + β q I{β ∈ Θ}. [sent-321, score-0.659]
</p><p>83 Thus, since N ∼ Binomial(p, w) with pw = (1 − ε)k → ∞, q  q  q  (58) q (q−1)+ kλq q≤2 mm  Eµ,w Eβ,X δ∗ q + β q I{β ∈ Θ} q q ≤ 2(q−1)+ kλq Pµ,w {N > wp/(1 − ε)} + µq Eµ,w NI{N > wp/(1 − ε)} mm = o(1)kλq mm by (57). [sent-324, score-1.416]
</p><p>84 3533  (59)  Y E AND Z HANG  By (57), β ∈ Θ implies β β−β  q q  q q  q  ≤ kλmm and β  + Since β  q q  δ−β  ≤ cq kλq I mm  ≤ ck1/q λmm  q  q + (1 + c)k  1/q  λmm  q  I  δ−β  q  > ck1/q λmm . [sent-328, score-0.501]
</p><p>85 q  = Nµq ≤ Nλmm , it follows that Eµ,w Eβ,X β − β  ≤ cq kλq + (2 + c)q kλq max Eβ,X L(δ(X, y), β) mm mm  q q  β∈Θ  +2q−1 λq Eµ,w mm  N + (1 + c)q k I{β ∈ Θ}. [sent-329, score-1.474]
</p><p>86 (2 + c)q  Since the o(1) is uniform in the choice of δ(X, y), we ﬁnd δ(X, y) − β  inf sup Pβ,X δ β∈Θ  q q  > (1 − ε)kλq mm ≥  ε + o(1) , ∀0 < ε < 1. [sent-331, score-0.575]
</p><p>87 The following lemma combines and extends the sharp shifting inequalities of Cai, Wang, and Xu (2010) for q = 2 and Ye and Zhang (2009) for w′ h with q = ∞. [sent-341, score-0.16]
</p><p>88 Moreover, for any vector w ∈ R p ,  ∑ w jh j ≤  hJ c  j∈A  1  a aq ∨ ℓ ℓ  1−1/q  max  wB  q/(q−1)  / : B ∩ A = 0, |B| ≤ ⌈ℓ/a⌉ . [sent-348, score-0.151]
</p><p>89 0  (62)  R ATE M INIMAXITY OF ℓ1 M ETHODS  With x = at/ℓ − m and possibly different h ↓ 0, the above inequality is a consequence of 1+a  max  1  hq (x)dx : 0  a  h(x)dx = 1 ≤ max 1, (aq /a)q−1  (63)  It sufﬁces to consider 0 < a ≤ aq . [sent-351, score-0.248]
</p><p>90 As in (14) and (15), Σh  ∞  ≤ ξ1 λ, hJ c  1  ≤ ξ0 hJ  1 + (ξ0 + 1)ρJ ,  (65)  in the given events, with {ξ0 , ξ1 } = (1, 2) for the Dantzig selector and the {ξ0 , ξ1 } in (37) for the Lasso. [sent-372, score-0.203]
</p><p>91 Since Rr /λr ≍ d, (5) and (55) give mm 2σ2 log(p/d) ≈ λ2 . [sent-417, score-0.472]
</p><p>92 (69)  Since (log d)/ log p ≤ c0 , (1 − c0 )λ2 = (1 − c0 )2σ2 log p ≤ 2σ2 log(p/d) ≈ λ2 and mm n n univ ∗ ∗ C1 ≈ C1 = 2(λ/λmm )(dλr /Rr )1/q , C2 ≈ C2 = R/(λd 1/r ). [sent-419, score-0.801]
</p><p>93 Thus, due to λ2 /σ2 ≍ log p and n ≥ d ≍ Rr /λr ≍ mm n Rr /λr → ∞, we have 2  Pβ,X (Ω0 )(λr /Rr ){(σ/λ)2q + (R/λ)q } = o(1)p1−(α1 /α0 ) (nq /d + d q/r−1 ) = o(1). [sent-422, score-0.51]
</p><p>94 Since ξ > (1 + α)/(1 − α), Theorem 12 asserts that for J = 2,d arg max|A|=k βA 1 and certain constants {ξ1 , ξ2 }, h  q  ≤ max  ξ1 d 1/q λ , Mq, relax ξ1 λ, ξ2 ρJ , J, d CIFq, relax (ξ, J, d)  . [sent-448, score-0.161]
</p><p>95 The dantzig selector: statistical estimation when p is much larger than n (with discussion). [sent-476, score-0.255]
</p><p>96 Discussion: The dantzig selector: statistical estimation when p is much larger than n. [sent-501, score-0.255]
</p><p>97 Variable selection via nonconcave penalized likelihood and its oracle properties. [sent-506, score-0.174]
</p><p>98 Sup–norm convergence rate and sign concentration property of lasso and dantzig estimators. [sent-539, score-0.451]
</p><p>99 On the conditions used to prove oracle results for the lasso. [sent-590, score-0.174]
</p><p>100 The sparsity and bias of the lasso selection in high–dimensional linear regression. [sent-617, score-0.22]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ua', 0.482), ('mm', 0.472), ('dantzig', 0.255), ('univ', 0.253), ('selector', 0.203), ('lasso', 0.196), ('oracle', 0.174), ('cifq', 0.172), ('hj', 0.169), ('rr', 0.147), ('mq', 0.142), ('minimaxity', 0.124), ('aq', 0.122), ('uj', 0.12), ('minimax', 0.107), ('inimaxity', 0.096), ('wa', 0.078), ('candes', 0.078), ('inequalities', 0.077), ('cif', 0.077), ('ate', 0.074), ('geer', 0.065), ('inf', 0.063), ('balls', 0.062), ('ha', 0.059), ('scifq', 0.057), ('sgn', 0.055), ('relax', 0.053), ('hang', 0.052), ('theorem', 0.052), ('uac', 0.048), ('ritov', 0.048), ('sharper', 0.048), ('ethods', 0.048), ('tao', 0.048), ('tsybakov', 0.043), ('bickel', 0.041), ('hq', 0.041), ('vj', 0.041), ('risk', 0.04), ('sup', 0.04), ('penalty', 0.039), ('log', 0.038), ('hlmann', 0.038), ('shifting', 0.036), ('quantiles', 0.034), ('ar', 0.033), ('zhang', 0.033), ('bounds', 0.032), ('ye', 0.032), ('xb', 0.031), ('remark', 0.031), ('max', 0.029), ('cq', 0.029), ('min', 0.027), ('johnstone', 0.027), ('inequality', 0.027), ('arg', 0.026), ('bn', 0.025), ('sharp', 0.025), ('cone', 0.025), ('event', 0.025), ('irrepresentable', 0.025), ('sharpen', 0.025), ('lder', 0.025), ('sharpest', 0.025), ('coef', 0.024), ('meinshausen', 0.024), ('sparsity', 0.024), ('regression', 0.023), ('donoho', 0.023), ('rq', 0.023), ('va', 0.023), ('dx', 0.022), ('van', 0.022), ('osborne', 0.022), ('presnell', 0.022), ('gd', 0.022), ('cones', 0.022), ('lemma', 0.022), ('annals', 0.022), ('cai', 0.022), ('quantities', 0.021), ('cients', 0.021), ('loss', 0.021), ('feasibility', 0.021), ('relaxed', 0.021), ('rutgers', 0.02), ('raskutti', 0.02), ('hastie', 0.02), ('estimators', 0.019), ('crelax', 0.019), ('greenshtein', 0.019), ('minb', 0.019), ('scif', 0.019), ('sharpens', 0.019), ('zac', 0.019), ('regularity', 0.019), ('unify', 0.019), ('huang', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="96-tfidf-1" href="./jmlr-2010-Rate_Minimaxity_of_the_Lasso_and_Dantzig_Selector_for_thelqLoss_inlrBalls.html">96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</a></p>
<p>Author: Fei Ye, Cun-Hui Zhang</p><p>Abstract: We consider the estimation of regression coefﬁcients in a high-dimensional linear model. For regression coefﬁcients in ℓr balls, we provide lower bounds for the minimax ℓq risk and minimax quantiles of the ℓq loss for all design matrices. Under an ℓ0 sparsity condition on a target coefﬁcient vector, we sharpen and unify existing oracle inequalities for the Lasso and Dantzig selector. We derive oracle inequalities for target coefﬁcient vectors with many small elements and smaller threshold levels than the universal threshold. These oracle inequalities provide sufﬁcient conditions on the design matrix for the rate minimaxity of the Lasso and Dantzig selector for the ℓq risk and loss in ℓr balls, 0 ≤ r ≤ 1 ≤ q ≤ ∞. By allowing q = ∞, our risk bounds imply the variable selection consistency of threshold Lasso and Dantzig selectors. Keywords: variable selection, estimation, oracle inequality, minimax, linear regression, penalized least squares, linear programming</p><p>2 0.14897354 <a title="96-tfidf-2" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>Author: Ming Yuan</p><p>Abstract: This paper considers the problem of estimating a high dimensional inverse covariance matrix that can be well approximated by “sparse” matrices. Taking advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix, we propose an estimating procedure that can effectively exploit such “sparsity”. The proposed method can be computed using linear programming and therefore has the potential to be used in very high dimensional problems. Oracle inequalities are established for the estimation error in terms of several operator norms, showing that the method is adaptive to different types of sparsity of the problem. Keywords: covariance selection, Dantzig selector, Gaussian graphical model, inverse covariance matrix, Lasso, linear programming, oracle inequality, sparsity</p><p>3 0.13994309 <a title="96-tfidf-3" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>Author: Garvesh Raskutti, Martin J. Wainwright, Bin Yu</p><p>Abstract: Methods based on ℓ1 -relaxation, such as basis pursuit and the Lasso, are very popular for sparse regression in high dimensions. The conditions for success of these methods are now well-understood: (1) exact recovery in the noiseless setting is possible if and only if the design matrix X satisﬁes the restricted nullspace property, and (2) the squared ℓ2 -error of a Lasso estimate decays at the minimax optimal rate k log p , where k is the sparsity of the p-dimensional regression problem with additive n Gaussian noise, whenever the design satisﬁes a restricted eigenvalue condition. The key issue is thus to determine when the design matrix X satisﬁes these desirable properties. Thus far, there have been numerous results showing that the restricted isometry property, which implies both the restricted nullspace and eigenvalue conditions, is satisﬁed when all entries of X are independent and identically distributed (i.i.d.), or the rows are unitary. This paper proves directly that the restricted nullspace and eigenvalue conditions hold with high probability for quite general classes of Gaussian matrices for which the predictors may be highly dependent, and hence restricted isometry conditions can be violated with high probability. In this way, our results extend the attractive theoretical guarantees on ℓ1 -relaxations to a much broader class of problems than the case of completely independent or unitary designs. Keywords: Lasso, basis pursuit, random matrix theory, Gaussian comparison inequality, concentration of measure</p><p>4 0.1013991 <a title="96-tfidf-4" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>Author: Dapo Omidiran, Martin J. Wainwright</p><p>Abstract: We consider the problem of high-dimensional variable selection: given n noisy observations of a k-sparse vector β∗ ∈ R p , estimate the subset of non-zero entries of β∗ . A signiﬁcant body of work has studied behavior of ℓ1 -relaxations when applied to random measurement matrices that are dense (e.g., Gaussian, Bernoulli). In this paper, we analyze sparsiﬁed measurement ensembles, and consider the trade-off between measurement sparsity, as measured by the fraction γ of nonzero entries, and the statistical efﬁciency, as measured by the minimal number of observations n required for correct variable selection with probability converging to one. Our main result is to prove that it is possible to let the fraction on non-zero entries γ → 0 at some rate, yielding measurement matrices with a vanishing fraction of non-zeros per row, while retaining the same statistical efﬁciency as dense ensembles. A variety of simulation results conﬁrm the sharpness of our theoretical predictions. Keywords: variable selection, sparse random projections, high-dimensional statistics, Lasso, consistency, ℓ1 -regularization</p><p>5 0.091483846 <a title="96-tfidf-5" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We consider learning formulations with non-convex objective functions that often occur in practical applications. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to a sub-optimal solution in reality. This paper tries to remedy the above gap between theory and practice. In particular, we present a multi-stage convex relaxation scheme for solving problems with non-convex objective functions. For learning formulations with sparse regularization, we analyze the behavior of a speciﬁc multistage relaxation scheme. Under appropriate conditions, we show that the local solution obtained by this procedure is superior to the global solution of the standard L1 convex relaxation for learning sparse targets. Keywords: sparsity, non-convex optimization, convex relaxation, multi-stage convex relaxation</p><p>6 0.066649981 <a title="96-tfidf-6" href="./jmlr-2010-On_Spectral_Learning.html">84 jmlr-2010-On Spectral Learning</a></p>
<p>7 0.048262417 <a title="96-tfidf-7" href="./jmlr-2010-Classification_Methods_with_Reject_Option_Based_on_Convex_Risk_Minimization.html">21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</a></p>
<p>8 0.04745381 <a title="96-tfidf-8" href="./jmlr-2010-Characterization%2C_Stability_and_Convergence_of_Hierarchical_Clustering_Methods.html">19 jmlr-2010-Characterization, Stability and Convergence of Hierarchical Clustering Methods</a></p>
<p>9 0.043085631 <a title="96-tfidf-9" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>10 0.03916885 <a title="96-tfidf-10" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>11 0.037749283 <a title="96-tfidf-11" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>12 0.037017848 <a title="96-tfidf-12" href="./jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization.html">54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</a></p>
<p>13 0.035208147 <a title="96-tfidf-13" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>14 0.031917162 <a title="96-tfidf-14" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>15 0.027059566 <a title="96-tfidf-15" href="./jmlr-2010-Composite_Binary_Losses.html">25 jmlr-2010-Composite Binary Losses</a></p>
<p>16 0.026286639 <a title="96-tfidf-16" href="./jmlr-2010-A_Fast_Hybrid_Algorithm_for_Large-Scalel1-Regularized_Logistic_Regression.html">3 jmlr-2010-A Fast Hybrid Algorithm for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>17 0.025260445 <a title="96-tfidf-17" href="./jmlr-2010-Hilbert_Space_Embeddings_and_Metrics_on_Probability_Measures.html">47 jmlr-2010-Hilbert Space Embeddings and Metrics on Probability Measures</a></p>
<p>18 0.023941971 <a title="96-tfidf-18" href="./jmlr-2010-Regularized_Discriminant_Analysis%2C_Ridge_Regression_and_Beyond.html">98 jmlr-2010-Regularized Discriminant Analysis, Ridge Regression and Beyond</a></p>
<p>19 0.023745436 <a title="96-tfidf-19" href="./jmlr-2010-Maximum_Likelihood_in_Cost-Sensitive_Learning%3A_Model_Specification%2C_Approximations%2C_and_Upper_Bounds.html">73 jmlr-2010-Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds</a></p>
<p>20 0.023731507 <a title="96-tfidf-20" href="./jmlr-2010-Learning_From_Crowds.html">61 jmlr-2010-Learning From Crowds</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.137), (1, -0.12), (2, 0.163), (3, -0.037), (4, -0.171), (5, -0.124), (6, 0.077), (7, -0.092), (8, -0.029), (9, -0.089), (10, 0.105), (11, -0.113), (12, 0.02), (13, 0.1), (14, -0.11), (15, -0.128), (16, -0.131), (17, -0.079), (18, -0.051), (19, 0.049), (20, 0.048), (21, -0.163), (22, -0.01), (23, -0.074), (24, -0.106), (25, 0.144), (26, -0.05), (27, 0.016), (28, 0.105), (29, -0.028), (30, -0.067), (31, -0.034), (32, -0.119), (33, -0.08), (34, -0.214), (35, 0.042), (36, 0.09), (37, -0.055), (38, 0.141), (39, 0.035), (40, 0.177), (41, 0.085), (42, 0.022), (43, 0.051), (44, -0.004), (45, 0.048), (46, -0.069), (47, -0.004), (48, 0.05), (49, -0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9529624 <a title="96-lsi-1" href="./jmlr-2010-Rate_Minimaxity_of_the_Lasso_and_Dantzig_Selector_for_thelqLoss_inlrBalls.html">96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</a></p>
<p>Author: Fei Ye, Cun-Hui Zhang</p><p>Abstract: We consider the estimation of regression coefﬁcients in a high-dimensional linear model. For regression coefﬁcients in ℓr balls, we provide lower bounds for the minimax ℓq risk and minimax quantiles of the ℓq loss for all design matrices. Under an ℓ0 sparsity condition on a target coefﬁcient vector, we sharpen and unify existing oracle inequalities for the Lasso and Dantzig selector. We derive oracle inequalities for target coefﬁcient vectors with many small elements and smaller threshold levels than the universal threshold. These oracle inequalities provide sufﬁcient conditions on the design matrix for the rate minimaxity of the Lasso and Dantzig selector for the ℓq risk and loss in ℓr balls, 0 ≤ r ≤ 1 ≤ q ≤ ∞. By allowing q = ∞, our risk bounds imply the variable selection consistency of threshold Lasso and Dantzig selectors. Keywords: variable selection, estimation, oracle inequality, minimax, linear regression, penalized least squares, linear programming</p><p>2 0.68427783 <a title="96-lsi-2" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>Author: Garvesh Raskutti, Martin J. Wainwright, Bin Yu</p><p>Abstract: Methods based on ℓ1 -relaxation, such as basis pursuit and the Lasso, are very popular for sparse regression in high dimensions. The conditions for success of these methods are now well-understood: (1) exact recovery in the noiseless setting is possible if and only if the design matrix X satisﬁes the restricted nullspace property, and (2) the squared ℓ2 -error of a Lasso estimate decays at the minimax optimal rate k log p , where k is the sparsity of the p-dimensional regression problem with additive n Gaussian noise, whenever the design satisﬁes a restricted eigenvalue condition. The key issue is thus to determine when the design matrix X satisﬁes these desirable properties. Thus far, there have been numerous results showing that the restricted isometry property, which implies both the restricted nullspace and eigenvalue conditions, is satisﬁed when all entries of X are independent and identically distributed (i.i.d.), or the rows are unitary. This paper proves directly that the restricted nullspace and eigenvalue conditions hold with high probability for quite general classes of Gaussian matrices for which the predictors may be highly dependent, and hence restricted isometry conditions can be violated with high probability. In this way, our results extend the attractive theoretical guarantees on ℓ1 -relaxations to a much broader class of problems than the case of completely independent or unitary designs. Keywords: Lasso, basis pursuit, random matrix theory, Gaussian comparison inequality, concentration of measure</p><p>3 0.65210456 <a title="96-lsi-3" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>Author: Ming Yuan</p><p>Abstract: This paper considers the problem of estimating a high dimensional inverse covariance matrix that can be well approximated by “sparse” matrices. Taking advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix, we propose an estimating procedure that can effectively exploit such “sparsity”. The proposed method can be computed using linear programming and therefore has the potential to be used in very high dimensional problems. Oracle inequalities are established for the estimation error in terms of several operator norms, showing that the method is adaptive to different types of sparsity of the problem. Keywords: covariance selection, Dantzig selector, Gaussian graphical model, inverse covariance matrix, Lasso, linear programming, oracle inequality, sparsity</p><p>4 0.40768409 <a title="96-lsi-4" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>Author: Dapo Omidiran, Martin J. Wainwright</p><p>Abstract: We consider the problem of high-dimensional variable selection: given n noisy observations of a k-sparse vector β∗ ∈ R p , estimate the subset of non-zero entries of β∗ . A signiﬁcant body of work has studied behavior of ℓ1 -relaxations when applied to random measurement matrices that are dense (e.g., Gaussian, Bernoulli). In this paper, we analyze sparsiﬁed measurement ensembles, and consider the trade-off between measurement sparsity, as measured by the fraction γ of nonzero entries, and the statistical efﬁciency, as measured by the minimal number of observations n required for correct variable selection with probability converging to one. Our main result is to prove that it is possible to let the fraction on non-zero entries γ → 0 at some rate, yielding measurement matrices with a vanishing fraction of non-zeros per row, while retaining the same statistical efﬁciency as dense ensembles. A variety of simulation results conﬁrm the sharpness of our theoretical predictions. Keywords: variable selection, sparse random projections, high-dimensional statistics, Lasso, consistency, ℓ1 -regularization</p><p>5 0.40360779 <a title="96-lsi-5" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We consider learning formulations with non-convex objective functions that often occur in practical applications. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to a sub-optimal solution in reality. This paper tries to remedy the above gap between theory and practice. In particular, we present a multi-stage convex relaxation scheme for solving problems with non-convex objective functions. For learning formulations with sparse regularization, we analyze the behavior of a speciﬁc multistage relaxation scheme. Under appropriate conditions, we show that the local solution obtained by this procedure is superior to the global solution of the standard L1 convex relaxation for learning sparse targets. Keywords: sparsity, non-convex optimization, convex relaxation, multi-stage convex relaxation</p><p>6 0.35640955 <a title="96-lsi-6" href="./jmlr-2010-Characterization%2C_Stability_and_Convergence_of_Hierarchical_Clustering_Methods.html">19 jmlr-2010-Characterization, Stability and Convergence of Hierarchical Clustering Methods</a></p>
<p>7 0.31821817 <a title="96-lsi-7" href="./jmlr-2010-Classification_Methods_with_Reject_Option_Based_on_Convex_Risk_Minimization.html">21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</a></p>
<p>8 0.28389439 <a title="96-lsi-8" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>9 0.27380908 <a title="96-lsi-9" href="./jmlr-2010-On_Spectral_Learning.html">84 jmlr-2010-On Spectral Learning</a></p>
<p>10 0.27257189 <a title="96-lsi-10" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>11 0.23118229 <a title="96-lsi-11" href="./jmlr-2010-Model-based_Boosting_2.0.html">77 jmlr-2010-Model-based Boosting 2.0</a></p>
<p>12 0.22796518 <a title="96-lsi-12" href="./jmlr-2010-Regularized_Discriminant_Analysis%2C_Ridge_Regression_and_Beyond.html">98 jmlr-2010-Regularized Discriminant Analysis, Ridge Regression and Beyond</a></p>
<p>13 0.20924769 <a title="96-lsi-13" href="./jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization.html">54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</a></p>
<p>14 0.18772291 <a title="96-lsi-14" href="./jmlr-2010-Composite_Binary_Losses.html">25 jmlr-2010-Composite Binary Losses</a></p>
<p>15 0.18463452 <a title="96-lsi-15" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>16 0.17822616 <a title="96-lsi-16" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>17 0.16815358 <a title="96-lsi-17" href="./jmlr-2010-A_Convergent_Online_Single_Time_Scale_Actor_Critic_Algorithm.html">2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</a></p>
<p>18 0.16180012 <a title="96-lsi-18" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>19 0.15974253 <a title="96-lsi-19" href="./jmlr-2010-Hilbert_Space_Embeddings_and_Metrics_on_Probability_Measures.html">47 jmlr-2010-Hilbert Space Embeddings and Metrics on Probability Measures</a></p>
<p>20 0.14360547 <a title="96-lsi-20" href="./jmlr-2010-Stochastic_Complexity_and_Generalization_Error_of_a_Restricted_Boltzmann_Machine_in_Bayesian_Estimation.html">108 jmlr-2010-Stochastic Complexity and Generalization Error of a Restricted Boltzmann Machine in Bayesian Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.013), (3, 0.04), (8, 0.052), (21, 0.037), (24, 0.016), (32, 0.048), (36, 0.021), (37, 0.088), (52, 0.408), (75, 0.102), (81, 0.011), (85, 0.036), (96, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76936859 <a title="96-lda-1" href="./jmlr-2010-Rate_Minimaxity_of_the_Lasso_and_Dantzig_Selector_for_thelqLoss_inlrBalls.html">96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</a></p>
<p>Author: Fei Ye, Cun-Hui Zhang</p><p>Abstract: We consider the estimation of regression coefﬁcients in a high-dimensional linear model. For regression coefﬁcients in ℓr balls, we provide lower bounds for the minimax ℓq risk and minimax quantiles of the ℓq loss for all design matrices. Under an ℓ0 sparsity condition on a target coefﬁcient vector, we sharpen and unify existing oracle inequalities for the Lasso and Dantzig selector. We derive oracle inequalities for target coefﬁcient vectors with many small elements and smaller threshold levels than the universal threshold. These oracle inequalities provide sufﬁcient conditions on the design matrix for the rate minimaxity of the Lasso and Dantzig selector for the ℓq risk and loss in ℓr balls, 0 ≤ r ≤ 1 ≤ q ≤ ∞. By allowing q = ∞, our risk bounds imply the variable selection consistency of threshold Lasso and Dantzig selectors. Keywords: variable selection, estimation, oracle inequality, minimax, linear regression, penalized least squares, linear programming</p><p>2 0.70854628 <a title="96-lda-2" href="./jmlr-2010-Composite_Binary_Losses.html">25 jmlr-2010-Composite Binary Losses</a></p>
<p>Author: Mark D. Reid, Robert C. Williamson</p><p>Abstract: We study losses for binary classiﬁcation and class probability estimation and extend the understanding of them from margin losses to general composite losses which are the composition of a proper loss with a link function. We characterise when margin losses can be proper composite losses, explicitly show how to determine a symmetric loss in full from half of one of its partial losses, introduce an intrinsic parametrisation of composite binary losses and give a complete characterisation of the relationship between proper losses and “classiﬁcation calibrated” losses. We also consider the question of the “best” surrogate binary loss. We introduce a precise notion of “best” and show there exist situations where two convex surrogate losses are incommensurable. We provide a complete explicit characterisation of the convexity of composite binary losses in terms of the link function and the weight function associated with the proper loss which make up the composite loss. This characterisation suggests new ways of “surrogate tuning” as well as providing an explicit characterisation of when Bregman divergences on the unit interval are convex in their second argument. Finally, in an appendix we present some new algorithm-independent results on the relationship between properness, convexity and robustness to misclassiﬁcation noise for binary losses and show that all convex proper losses are non-robust to misclassiﬁcation noise. Keywords: surrogate loss, convexity, probability estimation, classiﬁcation, Fisher consistency, classiﬁcation-calibrated, regret bound, proper scoring rule, Bregman divergence, robustness, misclassiﬁcation noise</p><p>3 0.34632728 <a title="96-lda-3" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>Author: Ming Yuan</p><p>Abstract: This paper considers the problem of estimating a high dimensional inverse covariance matrix that can be well approximated by “sparse” matrices. Taking advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix, we propose an estimating procedure that can effectively exploit such “sparsity”. The proposed method can be computed using linear programming and therefore has the potential to be used in very high dimensional problems. Oracle inequalities are established for the estimation error in terms of several operator norms, showing that the method is adaptive to different types of sparsity of the problem. Keywords: covariance selection, Dantzig selector, Gaussian graphical model, inverse covariance matrix, Lasso, linear programming, oracle inequality, sparsity</p><p>4 0.34424108 <a title="96-lda-4" href="./jmlr-2010-Iterative_Scaling_and_Coordinate_Descent_Methods_for_Maximum_Entropy_Models.html">57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</a></p>
<p>Author: Fang-Lan Huang, Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin</p><p>Abstract: Maximum entropy (Maxent) is useful in natural language processing and many other areas. Iterative scaling (IS) methods are one of the most popular approaches to solve Maxent. With many variants of IS methods, it is difﬁcult to understand them and see the differences. In this paper, we create a general and uniﬁed framework for iterative scaling methods. This framework also connects iterative scaling and coordinate descent methods. We prove general convergence results for IS methods and analyze their computational complexity. Based on the proposed framework, we extend a coordinate descent method for linear SVM to Maxent. Results show that it is faster than existing iterative scaling methods. Keywords: maximum entropy, iterative scaling, coordinate descent, natural language processing, optimization</p><p>5 0.33983287 <a title="96-lda-5" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>Author: Garvesh Raskutti, Martin J. Wainwright, Bin Yu</p><p>Abstract: Methods based on ℓ1 -relaxation, such as basis pursuit and the Lasso, are very popular for sparse regression in high dimensions. The conditions for success of these methods are now well-understood: (1) exact recovery in the noiseless setting is possible if and only if the design matrix X satisﬁes the restricted nullspace property, and (2) the squared ℓ2 -error of a Lasso estimate decays at the minimax optimal rate k log p , where k is the sparsity of the p-dimensional regression problem with additive n Gaussian noise, whenever the design satisﬁes a restricted eigenvalue condition. The key issue is thus to determine when the design matrix X satisﬁes these desirable properties. Thus far, there have been numerous results showing that the restricted isometry property, which implies both the restricted nullspace and eigenvalue conditions, is satisﬁed when all entries of X are independent and identically distributed (i.i.d.), or the rows are unitary. This paper proves directly that the restricted nullspace and eigenvalue conditions hold with high probability for quite general classes of Gaussian matrices for which the predictors may be highly dependent, and hence restricted isometry conditions can be violated with high probability. In this way, our results extend the attractive theoretical guarantees on ℓ1 -relaxations to a much broader class of problems than the case of completely independent or unitary designs. Keywords: Lasso, basis pursuit, random matrix theory, Gaussian comparison inequality, concentration of measure</p><p>6 0.33935022 <a title="96-lda-6" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>7 0.33782107 <a title="96-lda-7" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>8 0.33528695 <a title="96-lda-8" href="./jmlr-2010-Sparse_Spectrum_Gaussian_Process_Regression.html">104 jmlr-2010-Sparse Spectrum Gaussian Process Regression</a></p>
<p>9 0.33410949 <a title="96-lda-9" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>10 0.33144599 <a title="96-lda-10" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>11 0.32537252 <a title="96-lda-11" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>12 0.32408971 <a title="96-lda-12" href="./jmlr-2010-The_SHOGUN_Machine_Learning_Toolbox.html">110 jmlr-2010-The SHOGUN Machine Learning Toolbox</a></p>
<p>13 0.32393211 <a title="96-lda-13" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>14 0.32111746 <a title="96-lda-14" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>15 0.31984556 <a title="96-lda-15" href="./jmlr-2010-Spectral_Regularization_Algorithms_for_Learning_Large_Incomplete_Matrices.html">105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</a></p>
<p>16 0.31925589 <a title="96-lda-16" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>17 0.31867573 <a title="96-lda-17" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>18 0.31787488 <a title="96-lda-18" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<p>19 0.31747314 <a title="96-lda-19" href="./jmlr-2010-Stability_Bounds_for_Stationary_%CF%86-mixing_and_%CE%B2-mixing_Processes.html">106 jmlr-2010-Stability Bounds for Stationary φ-mixing and β-mixing Processes</a></p>
<p>20 0.31671423 <a title="96-lda-20" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
