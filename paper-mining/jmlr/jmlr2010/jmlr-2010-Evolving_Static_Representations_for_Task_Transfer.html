<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 jmlr-2010-Evolving Static Representations for Task Transfer</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-37" href="#">jmlr2010-37</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>37 jmlr-2010-Evolving Static Representations for Task Transfer</h1>
<br/><p>Source: <a title="jmlr-2010-37-pdf" href="http://jmlr.org/papers/volume11/verbancsics10a/verbancsics10a.pdf">pdf</a></p><p>Author: Phillip Verbancsics, Kenneth O. Stanley</p><p>Abstract: An important goal for machine learning is to transfer knowledge between tasks. For example, learning to play RoboCup Keepaway should contribute to learning the full game of RoboCup soccer. Previous approaches to transfer in Keepaway have focused on transforming the original representation to ﬁt the new task. In contrast, this paper explores the idea that transfer is most effective if the representation is designed to be the same even across different tasks. To demonstrate this point, a bird’s eye view (BEV) representation is introduced that can represent different tasks on the same two-dimensional map. For example, both the 3 vs. 2 and 4 vs. 3 Keepaway tasks can be represented on the same BEV. Yet the problem is that a raw two-dimensional map is high-dimensional and unstructured. This paper shows how this problem is addressed naturally by an idea from evolutionary computation called indirect encoding, which compresses the representation by exploiting its geometry. The result is that the BEV learns a Keepaway policy that transfers without further learning or manipulation. It also facilitates transferring knowledge learned in a different domain, Knight Joust, into Keepaway. Finally, the indirect encoding of the BEV means that its geometry can be changed without altering the solution. Thus static representations facilitate several kinds of transfer.</p><p>Reference: <a title="jmlr-2010-37-reference" href="../jmlr2010_reference/jmlr-2010-Evolving_Static_Representations_for_Task_Transfer_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Previous approaches to transfer in Keepaway have focused on transforming the original representation to ﬁt the new task. [sent-8, score-0.312]
</p><p>2 In contrast, this paper explores the idea that transfer is most effective if the representation is designed to be the same even across different tasks. [sent-9, score-0.312]
</p><p>3 , state) representation might remain static during transfer is plausible because the raw inputs to biological organisms, for example, vision, remain the same even when new tasks are confronted. [sent-41, score-0.5]
</p><p>4 The main idea in this paper is that such static representation, when possible, facilitates transfer by ensuring that the semantics of the representation are preserved even when the task changes. [sent-43, score-0.499]
</p><p>5 To demonstrate the critical role of static representation in transfer, a novel state representation is introduced called a bird’s eye view (BEV), which is a two-dimensional depiction of objects on the ground from above. [sent-44, score-0.361]
</p><p>6 However, more importantly, unlike any method so far, HyperNEAT can transfer from 3 vs. [sent-70, score-0.263]
</p><p>7 Additional types of transfer within Keepaway are investigated wherein the representation of the policy (i. [sent-74, score-0.443]
</p><p>8 Finally, cross-domain transfer is demonstrated by training on a distinctly different domain, Knight Joust (Taylor et al. [sent-77, score-0.263]
</p><p>9 The main result is that transfer through a static representation is consistently more robust and often provides immediate beneﬁts even without any further training. [sent-80, score-0.448]
</p><p>10 Thus, while machine 1738  E VOLVING S TATIC R EPRESENTATIONS FOR TASK T RANSFER  learning often focuses on the learning algorithm, the hope is that this paper provokes a fruitful conversation on the role of representation in transfer and learning in general. [sent-82, score-0.312]
</p><p>11 The next section describes the importance of representation in learning, prior research in transfer learning, and the methods that underlie the BEV representation. [sent-85, score-0.312]
</p><p>12 In Section 4, the experiments that investigate the performance of the BEV in learning and transfer are described. [sent-87, score-0.263]
</p><p>13 Background This section examines the critical role of representation in RL and then explains the geometry-based methods that underlie the static representation investigated in this paper, and their relation to task transfer. [sent-90, score-0.285]
</p><p>14 For example, in the Keepaway soccer domain, the state space S for the keeper with the ball can be deﬁned as the set of distances and angles to each other player. [sent-97, score-0.317]
</p><p>15 A simple policy π would be to pass to the most open teammate when takers are close and hold the ball otherwise. [sent-102, score-0.296]
</p><p>16 One popular approach to state representation, for example, in the RoboCup Keepaway soccer domain, is to express the state as distances and angles to the other players relative to the agent with the ball (Metzen et al. [sent-104, score-0.426]
</p><p>17 These additional parameters mean that the same representation cannot be applied to both tasks, thereby complicating the transfer of knowledge between tasks. [sent-115, score-0.312]
</p><p>18 This transfer could be across objects in the domain or across different tasks. [sent-127, score-0.328]
</p><p>19 2 Task Transfer Task transfer means applying knowledge learned in one task to a new, related task (Caruana, 1997; Talvitie and Singh, 2007; Taylor et al. [sent-146, score-0.393]
</p><p>20 Thus the capability to transfer is becoming increasingly important as the tasks studied in RL increase in complexity. [sent-150, score-0.291]
</p><p>21 However, transfer learning faces several challenges: First, transfer is only effective among compatible tasks and the particular knowledge that can transfer from one task to another must be identiﬁed. [sent-151, score-0.868]
</p><p>22 Second, a method must be derived to actually implement the transfer of knowledge. [sent-152, score-0.263]
</p><p>23 Finally, cases in which transfer hinders performance, or negative transfer, must be avoided (Pan and Yang, 2008). [sent-153, score-0.263]
</p><p>24 There are several types of transfer learning problems and a variety of methods that exploit their characteristics. [sent-154, score-0.263]
</p><p>25 , 2007a), choosing the best policy for the current task from a set of previously learned policies (Talvitie and Singh, 2007), extracting advice from previously learned tasks (Torrey et al. [sent-157, score-0.359]
</p><p>26 An intuitive approach to transfer learning is to transform the representation of knowledge learned in one task to a suitable form for a new task and then continue learning from that point. [sent-160, score-0.442]
</p><p>27 A successful method that takes this approach is transfer via inter-task mapping for policy search methods (TVITM-PS; Taylor et al. [sent-161, score-0.394]
</p><p>28 TVITM-PS is such a leading method for transforming the policy learned in the source task into a policy usable in the target task. [sent-163, score-0.341]
</p><p>29 In TVITM-PS, a transfer functional ρ is deﬁned to transform the policy π for a source task into the policy for a target task, such that ρ(πsource ) = πtarget . [sent-164, score-0.576]
</p><p>30 TVITM-PS is a milestone in task transfer because it introduces a formal approach to moving from one domain to another that deﬁnes how ambiguous variables in the target domain should be treated. [sent-173, score-0.372]
</p><p>31 Alternating trusting Exploration and suspicious exploitation (AtEase; Talvitie and Singh 2007) is such a transfer method; it aims to recognize when tasks are related and when to exploit knowledge gained from previous tasks. [sent-178, score-0.291]
</p><p>32 An important consideration in transfer is whether a human can understand the knowledge being transferred among tasks. [sent-191, score-0.306]
</p><p>33 Through observation, it may be apparent that a learned policy always passes the ball if opponents approach within one meter, which may then be transformed into a rule to transfer to another task. [sent-203, score-0.486]
</p><p>34 Instead, knowledge may transfer among several tasks that are simultaneously being learned. [sent-206, score-0.291]
</p><p>35 This paper adds to our understanding of task transfer by focusing on the role of representation. [sent-213, score-0.314]
</p><p>36 The main idea in HyperNEAT is that it is possible to learn geometric relationships in the domain through an indirect encoding that describes how the connectivity of the ANN can be generated as a function of the domain geometry. [sent-266, score-0.269]
</p><p>37 That is, HyperNEAT discovers the regularities in the domain geometry and learns a policy based on them. [sent-272, score-0.276]
</p><p>38 (1) Every connection between layers in the substrate is queried by the CPPN to determine its weight; the line connecting layers in the substrate represents a sample such connection. [sent-291, score-0.44]
</p><p>39 As a rule of thumb, nodes are placed on the substrate to reﬂect the geometry of the domain (i. [sent-306, score-0.298]
</p><p>40 This way, the connectivity of the substrate becomes a direct function of the domain geometry, which means that knowledge about the problem can be injected into the search and HyperNEAT can exploit the regularities (e. [sent-311, score-0.296]
</p><p>41 Approach: Bird’s Eye View A major challenge for the state representation in RL tasks is that speciﬁc state variables are often tied to agents or individual objects, which makes it difﬁcult to add more such objects without expanding the state space (Taylor et al. [sent-328, score-0.295]
</p><p>42 , in soccer), the geometry of the input layer of the substrate is made two-dimensional, as in Figure 5. [sent-374, score-0.303]
</p><p>43 Each dimension ranges between [−1, 1] and the input and output planes of the substrate are equivalently constructed to take advantage of geometric regularities between states and actions. [sent-388, score-0.265]
</p><p>44 That way, task transfer to different numbers of players is made simple through the static representation. [sent-395, score-0.55]
</p><p>45 For example, if the substrate resolution is 20 × 20 then the number of possible connections in the substrate is 400 × 400 = 160, 000. [sent-410, score-0.439]
</p><p>46 Of course, some representations are better suited to transfer in a given domain than others. [sent-419, score-0.319]
</p><p>47 Further, the ability to transfer between tasks is dependent on the similarity of the tasks. [sent-420, score-0.291]
</p><p>48 However, this paper focuses on the idea that a particularly effective representation for transfer is one that does not need to change from one task to the next. [sent-421, score-0.363]
</p><p>49 Takers follow static policies, wherein the ﬁrst two takers go towards the ball and additional takers attempt to block open keepers. [sent-453, score-0.352]
</p><p>50 These include each player’s distance to the center of the ﬁeld, the distance from the keeper with the ball to each other player, the distance from each other keeper to the closest taker, and the minimum angle between the other keepers and the takers (Figure 7). [sent-459, score-0.368]
</p><p>51 To investigate the ability of a static representation, that is, the HyperNEAT BEV, to learn this task, it is compared to both static policies (Stone et al. [sent-467, score-0.338]
</p><p>52 The static benchmarks are Always-Hold, Random, and a Hand-Coded policy, which holds the ball if no takers are within 10m (Stone and Sutton, 2001). [sent-473, score-0.276]
</p><p>53 These static benchmarks provide a baseline to validate that the BEV learns a non-trivial policy in the initial task. [sent-474, score-0.267]
</p><p>54 The keeper with the ball is the small square, other keepers are circles, and the takers are triangles. [sent-508, score-0.297]
</p><p>55 Unlike the HyperNEAT BEV, TVITM-PS requires further training after transfer because ρ expands the ANN by adding new state variables. [sent-559, score-0.303]
</p><p>56 The ﬁrst is transfer to increasing ﬁeld sizes, which is evaluated by ﬁrst training individuals on a small (15m×15m) ﬁeld size and then testing trained individuals on the trained and larger ﬁeld sizes (each of 15m×15m, 20m×20m, and 25m×25m). [sent-561, score-0.469]
</p><p>57 , if the ﬁeld size is 15m×15m, the substrate is 15 × 15; if it is 25m×25m, the substrate is 25 × 25). [sent-564, score-0.386]
</p><p>58 Second, transfer to substrates of different resolutions is evaluated by training individuals on a single ﬁeld size, then doubling the resolution in each dimension of the substrate (i. [sent-569, score-0.594]
</p><p>59 , an individual trained on a 20m×20m ﬁeld with a 20 × 20 substrate is reevaluated on a substrate changed to 40 × 40). [sent-571, score-0.424]
</p><p>60 The higher resolution 20×20 4 BEV is then tested on the same ﬁeld size to evaluate the ability to transfer knowledge between substrate resolutions. [sent-575, score-0.509]
</p><p>61 The name Knight Joust reﬂects that the player is allowed three potential moves: move forward, knight jump left, and knight jump right, where a knight jump is two steps in the direction left or right and then forward (as in chess). [sent-582, score-0.46]
</p><p>62 This state information can similarly be drawn on the substrate of the BEV by marking the position of the player, opponent, the path between them, and the paths to the corners. [sent-588, score-0.278]
</p><p>63 1755  V ERBANCSICS AND S TANLEY  The evaluation of cross-domain transfer is completed by ﬁrst training for 20 generations in the Knight Joust domain. [sent-603, score-0.332]
</p><p>64 This experiment is interesting because it can help to show that static transfer is beneﬁcial with the BEV even in cases where the input semantics of the two tasks have slightly different meaning. [sent-609, score-0.427]
</p><p>65 Results This section describes the results of training the BEV on the Keepaway benchmark, the transfer performance among variations of the Keepaway task, and ﬁnally the performance of the BEV in cross-domain transfer from Knight Joust to Keepaway. [sent-611, score-0.526]
</p><p>66 2 Keepaway on the 20m×20m ﬁeld, the best keepers from each of the ﬁve runs controlled by a BEV substrate trained by HyperNEAT maintain possession of the ball on average for 15. [sent-629, score-0.401]
</p><p>67 2 Keepaway Transfer Results In transfer learning, the main focus of this work, the BEV is evaluated by testing individuals trained for 20 generations only on the 3 vs. [sent-642, score-0.435]
</p><p>68 (2007b), in which teams trained on the smaller task are further trained on the larger task after the transfer because new parameters are added. [sent-648, score-0.441]
</p><p>69 2 task improves, the transfer performance on the 4 vs. [sent-671, score-0.314]
</p><p>70 In contrast, the previous best approach to transfer learning in this domain required executing a transfer function and additional 1757  V ERBANCSICS AND S TANLEY  training for between 50 and 200 hours (depending on the chosen transfer function) beyond the initial bootstrap training in 3 vs. [sent-684, score-0.818]
</p><p>71 Thus, because the BEV is static, transfer is instantaneous and requires no special adjustments to the representation to achieve the same result as many hours of further training with the TVITM-PS transfer method. [sent-688, score-0.575]
</p><p>72 Thus transfer learning is also evaluated by testing the best policy trained in 3 vs. [sent-737, score-0.432]
</p><p>73 The substrate resolution of the champion individuals from ﬁve runs from training on three ﬁeld sizes (15m×15m, 20m×20m, and 25m×25m) are doubled in each dimension and then tested again on the same ﬁeld size. [sent-776, score-0.336]
</p><p>74 The regularities learned by the indirect encoding are not dependent on the particular substrate resolution and may be extrapolated to higher resolutions. [sent-798, score-0.429]
</p><p>75 3 Knight Joust Transfer Results Cross-domain transfer is evaluated from the non-Keepaway task of Knight Joust on a 20 × 20 grid to 3 vs. [sent-801, score-0.314]
</p><p>76 After one generation of evolution, the best individuals from transfer exceed the raw performance by 0. [sent-808, score-0.328]
</p><p>77 Finally, after ten further generations, the best individuals with transfer hold the ball for 1. [sent-810, score-0.392]
</p><p>78 Discussion and Future Work Methods that alter representation remain important tools in task transfer for domains in which the representation must change with the task. [sent-819, score-0.412]
</p><p>79 The deeper lesson is the critical role of representation in transfer and the consequent need for algorithms that can learn from relatively high-dimensional static representations of task geometry. [sent-821, score-0.526]
</p><p>80 Thus performance on Keepaway, both instantaneous and with further training, beneﬁts from transfer from the Knight Joust domain with signiﬁcance p < 0. [sent-836, score-0.292]
</p><p>81 HyperNEAT substrates with hidden layers have been shown to work in the past in domains without transfer (D’Ambrosio and Stanley, 2008; Clune et al. [sent-839, score-0.31]
</p><p>82 1762  E VOLVING S TATIC R EPRESENTATIONS FOR TASK T RANSFER  The role of representation in transfer is relevant to all approaches to learning because transfer is always an option for extending the scope of learning. [sent-854, score-0.575]
</p><p>83 Additionally, the static nature of the representation allows the same policy to train on multiple tasks simultaneously. [sent-859, score-0.344]
</p><p>84 For example, a soccer player does not practice by playing only soccer games. [sent-860, score-0.366]
</p><p>85 For example, in this paper the policy is encoded by a CPPN that is expressed as a function of the task geometry, which enables the solution to exploit regularities in the geometry and extrapolate to previously unseen areas of the geometry. [sent-863, score-0.298]
</p><p>86 1 Prospects for Full RoboCup Soccer An exciting implication of this work is that the power of static transfer and indirect encoding can potentially bootstrap learning the complete game of soccer. [sent-870, score-0.514]
</p><p>87 The static BEV state representation enables the learned policy to transfer to variations of the task in which the number of players is changed (e. [sent-873, score-0.798]
</p><p>88 Furthermore, indirectly encoding the policy enables the same policy to be applied to variations of the task in which the geometry has been changed (e. [sent-878, score-0.457]
</p><p>89 Interestingly, the Keepaway domain was designed as a stepping stone to scaling machine learning methods to the full RoboCup soccer domain (Stone and Sutton, 2001). [sent-883, score-0.308]
</p><p>90 The same principles that enable the BEV to transfer among variations of the Keepaway domain also can potentially enable the BEV to scale to full Keepaway soccer. [sent-884, score-0.292]
</p><p>91 For example, because the representation remains static no matter how many players are on the ﬁeld, training can begin with a small number of players, such as 3 vs. [sent-885, score-0.285]
</p><p>92 Furthermore, varying the substrate conﬁguration while the solution encoding remains static makes it possible to train skills relevant to RoboCup on subsets of the full ﬁeld, for example, half-ﬁeld offense/defense. [sent-888, score-0.397]
</p><p>93 In this way, varying the number of players and varying the ﬁeld size are both required to transfer from the RoboCup Keepaway domain to full RoboCup soccer. [sent-889, score-0.392]
</p><p>94 Thus an interesting property of the BEV is that the state space can transfer, by accommodating new players or ﬁeld sizes, and the action space can also transfer in the same way. [sent-900, score-0.431]
</p><p>95 Ultimately, the promise of such transfer is tied to the idea of static representation, whose potential was highlighted in this paper. [sent-901, score-0.399]
</p><p>96 Conclusion This paper introduced the BEV representation, which simpliﬁes task transfer by making the state representation static. [sent-903, score-0.403]
</p><p>97 In addition to results competitive with leading methods on the Keepaway benchmark, the BEV, which is enabled by an indirect encoding, achieved transfer learning from 3 vs. [sent-908, score-0.31]
</p><p>98 The hope is that advanced representations in conjunction with indirect encoding can later contribute to scaling learning techniques to more challenging tasks, such as the complete RoboCup soccer domain. [sent-918, score-0.284]
</p><p>99 Performance evaluation of EANT in the robocup keepaway benchmark. [sent-1051, score-0.476]
</p><p>100 Advice taking and transfer learning: Naturally inspired extensions to reinforcement learning. [sent-1198, score-0.314]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bev', 0.492), ('keepaway', 0.324), ('transfer', 0.263), ('cppn', 0.258), ('hyperneat', 0.218), ('neat', 0.193), ('substrate', 0.193), ('robocup', 0.152), ('soccer', 0.142), ('static', 0.136), ('policy', 0.131), ('stanley', 0.126), ('knight', 0.126), ('joust', 0.122), ('stone', 0.108), ('players', 0.1), ('gauci', 0.096), ('cppns', 0.086), ('keepers', 0.086), ('eld', 0.084), ('player', 0.082), ('erbancsics', 0.081), ('ransfer', 0.081), ('tanley', 0.081), ('tatic', 0.081), ('volving', 0.081), ('sarsa', 0.078), ('takers', 0.076), ('geometry', 0.076), ('keeper', 0.071), ('generations', 0.069), ('epresentations', 0.069), ('encoding', 0.068), ('policies', 0.066), ('individuals', 0.065), ('ball', 0.064), ('agents', 0.062), ('evolutionary', 0.06), ('ann', 0.058), ('resolution', 0.053), ('taylor', 0.052), ('actions', 0.051), ('reinforcement', 0.051), ('task', 0.051), ('eye', 0.051), ('neuroevolution', 0.051), ('representation', 0.049), ('indirect', 0.047), ('opponent', 0.047), ('sutton', 0.045), ('transferred', 0.043), ('metzen', 0.041), ('whiteson', 0.041), ('state', 0.04), ('kenneth', 0.04), ('agent', 0.04), ('seconds', 0.04), ('regularities', 0.04), ('trained', 0.038), ('objects', 0.036), ('miikkulainen', 0.035), ('topologies', 0.035), ('layer', 0.034), ('connectivity', 0.034), ('geometric', 0.032), ('evolving', 0.031), ('eant', 0.03), ('multiagent', 0.03), ('anns', 0.03), ('rl', 0.03), ('relationships', 0.03), ('domain', 0.029), ('relational', 0.029), ('tasks', 0.028), ('action', 0.028), ('learned', 0.028), ('bird', 0.027), ('advice', 0.027), ('layers', 0.027), ('representations', 0.027), ('population', 0.026), ('ambrosio', 0.025), ('champion', 0.025), ('clune', 0.025), ('risto', 0.025), ('tadepalli', 0.025), ('taker', 0.025), ('talvitie', 0.025), ('teammate', 0.025), ('teammates', 0.025), ('torrey', 0.025), ('paths', 0.025), ('inputs', 0.024), ('reward', 0.024), ('peter', 0.023), ('champions', 0.02), ('marking', 0.02), ('possession', 0.02), ('shimon', 0.02), ('substrates', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="37-tfidf-1" href="./jmlr-2010-Evolving_Static_Representations_for_Task_Transfer.html">37 jmlr-2010-Evolving Static Representations for Task Transfer</a></p>
<p>Author: Phillip Verbancsics, Kenneth O. Stanley</p><p>Abstract: An important goal for machine learning is to transfer knowledge between tasks. For example, learning to play RoboCup Keepaway should contribute to learning the full game of RoboCup soccer. Previous approaches to transfer in Keepaway have focused on transforming the original representation to ﬁt the new task. In contrast, this paper explores the idea that transfer is most effective if the representation is designed to be the same even across different tasks. To demonstrate this point, a bird’s eye view (BEV) representation is introduced that can represent different tasks on the same two-dimensional map. For example, both the 3 vs. 2 and 4 vs. 3 Keepaway tasks can be represented on the same BEV. Yet the problem is that a raw two-dimensional map is high-dimensional and unstructured. This paper shows how this problem is addressed naturally by an idea from evolutionary computation called indirect encoding, which compresses the representation by exploiting its geometry. The result is that the BEV learns a Keepaway policy that transfers without further learning or manipulation. It also facilitates transferring knowledge learned in a different domain, Knight Joust, into Keepaway. Finally, the indirect encoding of the BEV means that its geometry can be changed without altering the solution. Thus static representations facilitate several kinds of transfer.</p><p>2 0.059505332 <a title="37-tfidf-2" href="./jmlr-2010-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">79 jmlr-2010-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>Author: Thomas Jaksch, Ronald Ortner, Peter Auer</p><p>Abstract: For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s, s′ there is a policy which moves from s to s′ in at most D steps (on average). √ ˜ We present a reinforcement learning algorithm with total regret O(DS AT ) after T steps for any unknown MDP with S states, A actions per state, and diameter D. A corresponding lower bound of √ Ω( DSAT ) on the total regret of any learning algorithm is given as well. These results are complemented by a sample complexity bound on the number of suboptimal steps taken by our algorithm. This bound can be used to achieve a (gap-dependent) regret bound that is logarithmic in T . Finally, we also consider a setting where the MDP is allowed to change a ﬁxed number of ℓ times. We present a modiﬁcation of our algorithm that is able to deal with this setting and show a √ ˜ regret bound of O(ℓ1/3 T 2/3 DS A). Keywords: undiscounted reinforcement learning, Markov decision process, regret, online learning, sample complexity</p><p>3 0.04491172 <a title="37-tfidf-3" href="./jmlr-2010-PyBrain.html">93 jmlr-2010-PyBrain</a></p>
<p>Author: Tom Schaul, Justin Bayer, Daan Wierstra, Yi Sun, Martin Felder, Frank Sehnke, Thomas Rückstieß, Jürgen Schmidhuber</p><p>Abstract: PyBrain is a versatile machine learning library for Python. Its goal is to provide ﬂexible, easyto-use yet still powerful algorithms for machine learning tasks, including a variety of predeﬁned environments and benchmarks to test and compare algorithms. Implemented algorithms include Long Short-Term Memory (LSTM), policy gradient methods, (multidimensional) recurrent neural networks and deep belief networks. Keywords: Python, neural networks, reinforcement learning, optimization</p><p>4 0.041060634 <a title="37-tfidf-4" href="./jmlr-2010-Why_Does_Unsupervised_Pre-training_Help_Deep_Learning%3F.html">117 jmlr-2010-Why Does Unsupervised Pre-training Help Deep Learning?</a></p>
<p>Author: Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, Samy Bengio</p><p>Abstract: Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difﬁcult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the inﬂuence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments conﬁrm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pretraining guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training. Keywords: deep architectures, unsupervised pre-training, deep belief networks, stacked denoising auto-encoders, non-convex optimization</p><p>5 0.035392825 <a title="37-tfidf-5" href="./jmlr-2010-A_Generalized_Path_Integral_Control_Approach_to_Reinforcement_Learning.html">4 jmlr-2010-A Generalized Path Integral Control Approach to Reinforcement Learning</a></p>
<p>Author: Evangelos Theodorou, Jonas Buchli, Stefan Schaal</p><p>Abstract: With the goal to generate more scalable algorithms with higher efﬁciency and fewer open parameters, reinforcement learning (RL) has recently moved towards combining classical techniques from optimal control and dynamic programming with modern learning techniques from statistical estimation theory. In this vein, this paper suggests to use the framework of stochastic optimal control with path integrals to derive a novel approach to RL with parameterized policies. While solidly grounded in value function estimation and optimal control based on the stochastic Hamilton-JacobiBellman (HJB) equations, policy improvements can be transformed into an approximation problem of a path integral which has no open algorithmic parameters other than the exploration noise. The resulting algorithm can be conceived of as model-based, semi-model-based, or even model free, depending on how the learning problem is structured. The update equations have no danger of numerical instabilities as neither matrix inversions nor gradient learning rates are required. Our new algorithm demonstrates interesting similarities with previous RL research in the framework of probability matching and provides intuition why the slightly heuristically motivated probability matching approach can actually perform well. Empirical evaluations demonstrate signiﬁcant performance improvements over gradient-based policy learning and scalability to high-dimensional control problems. Finally, a learning experiment on a simulated 12 degree-of-freedom robot dog illustrates the functionality of our algorithm in a complex robot learning scenario. We believe that Policy Improvement with Path Integrals (PI2 ) offers currently one of the most efﬁcient, numerically robust, and easy to implement algorithms for RL based on trajectory roll-outs. Keywords: stochastic optimal control, reinforcement learning, parameterized policies</p><p>6 0.035283834 <a title="37-tfidf-6" href="./jmlr-2010-A_Convergent_Online_Single_Time_Scale_Actor_Critic_Algorithm.html">2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</a></p>
<p>7 0.032255072 <a title="37-tfidf-7" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>8 0.030604335 <a title="37-tfidf-8" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>9 0.028850719 <a title="37-tfidf-9" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<p>10 0.02878494 <a title="37-tfidf-10" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>11 0.028555153 <a title="37-tfidf-11" href="./jmlr-2010-Stacked_Denoising_Autoencoders%3A_Learning_Useful_Representations_in_a_Deep_Network_with_a_Local_Denoising_Criterion.html">107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</a></p>
<p>12 0.027326886 <a title="37-tfidf-12" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>13 0.027169712 <a title="37-tfidf-13" href="./jmlr-2010-Posterior_Regularization_for_Structured_Latent_Variable_Models.html">91 jmlr-2010-Posterior Regularization for Structured Latent Variable Models</a></p>
<p>14 0.025578266 <a title="37-tfidf-14" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>15 0.022709766 <a title="37-tfidf-15" href="./jmlr-2010-Mean_Field_Variational_Approximation_for_Continuous-Time_Bayesian_Networks.html">75 jmlr-2010-Mean Field Variational Approximation for Continuous-Time Bayesian Networks</a></p>
<p>16 0.022696631 <a title="37-tfidf-16" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>17 0.020722667 <a title="37-tfidf-17" href="./jmlr-2010-Dimensionality_Estimation%2C_Manifold_Learning_and_Function_Approximation_using_Tensor_Voting.html">30 jmlr-2010-Dimensionality Estimation, Manifold Learning and Function Approximation using Tensor Voting</a></p>
<p>18 0.020470774 <a title="37-tfidf-18" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>19 0.019846395 <a title="37-tfidf-19" href="./jmlr-2010-A_Fast_Hybrid_Algorithm_for_Large-Scalel1-Regularized_Logistic_Regression.html">3 jmlr-2010-A Fast Hybrid Algorithm for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>20 0.019250698 <a title="37-tfidf-20" href="./jmlr-2010-FastInf%3A_An_Efficient_Approximate_Inference_Library.html">39 jmlr-2010-FastInf: An Efficient Approximate Inference Library</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.095), (1, 0.018), (2, -0.039), (3, -0.028), (4, 0.022), (5, 0.031), (6, 0.033), (7, -0.027), (8, 0.1), (9, 0.086), (10, 0.102), (11, -0.003), (12, 0.017), (13, -0.094), (14, 0.075), (15, 0.076), (16, 0.055), (17, 0.036), (18, -0.064), (19, -0.017), (20, -0.033), (21, -0.013), (22, 0.028), (23, -0.009), (24, -0.071), (25, -0.032), (26, -0.195), (27, 0.017), (28, 0.137), (29, -0.004), (30, -0.122), (31, 0.025), (32, -0.079), (33, -0.166), (34, -0.007), (35, -0.167), (36, -0.05), (37, -0.078), (38, -0.05), (39, -0.078), (40, -0.083), (41, -0.028), (42, -0.152), (43, -0.093), (44, -0.039), (45, -0.053), (46, -0.146), (47, 0.032), (48, -0.101), (49, 0.314)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9570806 <a title="37-lsi-1" href="./jmlr-2010-Evolving_Static_Representations_for_Task_Transfer.html">37 jmlr-2010-Evolving Static Representations for Task Transfer</a></p>
<p>Author: Phillip Verbancsics, Kenneth O. Stanley</p><p>Abstract: An important goal for machine learning is to transfer knowledge between tasks. For example, learning to play RoboCup Keepaway should contribute to learning the full game of RoboCup soccer. Previous approaches to transfer in Keepaway have focused on transforming the original representation to ﬁt the new task. In contrast, this paper explores the idea that transfer is most effective if the representation is designed to be the same even across different tasks. To demonstrate this point, a bird’s eye view (BEV) representation is introduced that can represent different tasks on the same two-dimensional map. For example, both the 3 vs. 2 and 4 vs. 3 Keepaway tasks can be represented on the same BEV. Yet the problem is that a raw two-dimensional map is high-dimensional and unstructured. This paper shows how this problem is addressed naturally by an idea from evolutionary computation called indirect encoding, which compresses the representation by exploiting its geometry. The result is that the BEV learns a Keepaway policy that transfers without further learning or manipulation. It also facilitates transferring knowledge learned in a different domain, Knight Joust, into Keepaway. Finally, the indirect encoding of the BEV means that its geometry can be changed without altering the solution. Thus static representations facilitate several kinds of transfer.</p><p>2 0.56314111 <a title="37-lsi-2" href="./jmlr-2010-PyBrain.html">93 jmlr-2010-PyBrain</a></p>
<p>Author: Tom Schaul, Justin Bayer, Daan Wierstra, Yi Sun, Martin Felder, Frank Sehnke, Thomas Rückstieß, Jürgen Schmidhuber</p><p>Abstract: PyBrain is a versatile machine learning library for Python. Its goal is to provide ﬂexible, easyto-use yet still powerful algorithms for machine learning tasks, including a variety of predeﬁned environments and benchmarks to test and compare algorithms. Implemented algorithms include Long Short-Term Memory (LSTM), policy gradient methods, (multidimensional) recurrent neural networks and deep belief networks. Keywords: Python, neural networks, reinforcement learning, optimization</p><p>3 0.30560213 <a title="37-lsi-3" href="./jmlr-2010-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">79 jmlr-2010-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>Author: Thomas Jaksch, Ronald Ortner, Peter Auer</p><p>Abstract: For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s, s′ there is a policy which moves from s to s′ in at most D steps (on average). √ ˜ We present a reinforcement learning algorithm with total regret O(DS AT ) after T steps for any unknown MDP with S states, A actions per state, and diameter D. A corresponding lower bound of √ Ω( DSAT ) on the total regret of any learning algorithm is given as well. These results are complemented by a sample complexity bound on the number of suboptimal steps taken by our algorithm. This bound can be used to achieve a (gap-dependent) regret bound that is logarithmic in T . Finally, we also consider a setting where the MDP is allowed to change a ﬁxed number of ℓ times. We present a modiﬁcation of our algorithm that is able to deal with this setting and show a √ ˜ regret bound of O(ℓ1/3 T 2/3 DS A). Keywords: undiscounted reinforcement learning, Markov decision process, regret, online learning, sample complexity</p><p>4 0.29160666 <a title="37-lsi-4" href="./jmlr-2010-A_Convergent_Online_Single_Time_Scale_Actor_Critic_Algorithm.html">2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</a></p>
<p>Author: Dotan Di Castro, Ron Meir</p><p>Abstract: Actor-Critic based approaches were among the ﬁrst to address reinforcement learning in a general setting. Recently, these algorithms have gained renewed interest due to their generality, good convergence properties, and possible biological relevance. In this paper, we introduce an online temporal difference based actor-critic algorithm which is proved to converge to a neighborhood of a local maximum of the average reward. Linear function approximation is used by the critic in order estimate the value function, and the temporal difference signal, which is passed from the critic to the actor. The main distinguishing feature of the present convergence proof is that both the actor and the critic operate on a similar time scale, while in most current convergence proofs they are required to have very different time scales in order to converge. Moreover, the same temporal difference signal is used to update the parameters of both the actor and the critic. A limitation of the proposed approach, compared to results available for two time scale convergence, is that convergence is guaranteed only to a neighborhood of an optimal value, rather to an optimal value itself. The single time scale and identical temporal difference signal used by the actor and the critic, may provide a step towards constructing more biologically realistic models of reinforcement learning in the brain. Keywords: actor critic, single time scale convergence, temporal difference</p><p>5 0.2833668 <a title="37-lsi-5" href="./jmlr-2010-Linear_Algorithms_for_Online_Multitask_Classification.html">66 jmlr-2010-Linear Algorithms for Online Multitask Classification</a></p>
<p>Author: Giovanni Cavallanti, Nicolò Cesa-Bianchi, Claudio Gentile</p><p>Abstract: We introduce new Perceptron-based algorithms for the online multitask binary classiﬁcation problem. Under suitable regularity conditions, our algorithms are shown to improve on their baselines by a factor proportional to the number of tasks. We achieve these improvements using various types of regularization that bias our algorithms towards speciﬁc notions of task relatedness. More specifically, similarity among tasks is either measured in terms of the geometric closeness of the task reference vectors or as a function of the dimension of their spanned subspace. In addition to adapting to the online setting a mix of known techniques, such as the multitask kernels of Evgeniou et al., our analysis also introduces a matrix-based multitask extension of the p-norm Perceptron, which is used to implement spectral co-regularization. Experiments on real-world data sets complement and support our theoretical ﬁndings. Keywords: mistake bounds, perceptron algorithm, multitask learning, spectral regularization</p><p>6 0.25711542 <a title="37-lsi-6" href="./jmlr-2010-A_Generalized_Path_Integral_Control_Approach_to_Reinforcement_Learning.html">4 jmlr-2010-A Generalized Path Integral Control Approach to Reinforcement Learning</a></p>
<p>7 0.25360066 <a title="37-lsi-7" href="./jmlr-2010-A_Fast_Hybrid_Algorithm_for_Large-Scalel1-Regularized_Logistic_Regression.html">3 jmlr-2010-A Fast Hybrid Algorithm for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>8 0.21149622 <a title="37-lsi-8" href="./jmlr-2010-Second-Order_Bilinear_Discriminant_Analysis.html">101 jmlr-2010-Second-Order Bilinear Discriminant Analysis</a></p>
<p>9 0.1966365 <a title="37-lsi-9" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>10 0.18995099 <a title="37-lsi-10" href="./jmlr-2010-Dimensionality_Estimation%2C_Manifold_Learning_and_Function_Approximation_using_Tensor_Voting.html">30 jmlr-2010-Dimensionality Estimation, Manifold Learning and Function Approximation using Tensor Voting</a></p>
<p>11 0.1883326 <a title="37-lsi-11" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>12 0.1784313 <a title="37-lsi-12" href="./jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization.html">54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</a></p>
<p>13 0.17782918 <a title="37-lsi-13" href="./jmlr-2010-The_SHOGUN_Machine_Learning_Toolbox.html">110 jmlr-2010-The SHOGUN Machine Learning Toolbox</a></p>
<p>14 0.1750841 <a title="37-lsi-14" href="./jmlr-2010-Why_Does_Unsupervised_Pre-training_Help_Deep_Learning%3F.html">117 jmlr-2010-Why Does Unsupervised Pre-training Help Deep Learning?</a></p>
<p>15 0.16346473 <a title="37-lsi-15" href="./jmlr-2010-Classification_with_Incomplete_Data_Using_Dirichlet_Process_Priors.html">23 jmlr-2010-Classification with Incomplete Data Using Dirichlet Process Priors</a></p>
<p>16 0.15883557 <a title="37-lsi-16" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>17 0.15844315 <a title="37-lsi-17" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<p>18 0.13679224 <a title="37-lsi-18" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>19 0.13399281 <a title="37-lsi-19" href="./jmlr-2010-Collective_Inference_for__Extraction_MRFs_Coupled_with_Symmetric_Clique_Potentials.html">24 jmlr-2010-Collective Inference for  Extraction MRFs Coupled with Symmetric Clique Potentials</a></p>
<p>20 0.13101654 <a title="37-lsi-20" href="./jmlr-2010-Consensus-Based_Distributed_Support_Vector_Machines.html">26 jmlr-2010-Consensus-Based Distributed Support Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.01), (4, 0.015), (8, 0.01), (21, 0.017), (24, 0.016), (25, 0.01), (32, 0.039), (33, 0.018), (34, 0.474), (36, 0.065), (37, 0.043), (38, 0.013), (75, 0.088), (81, 0.018), (85, 0.043), (96, 0.023), (97, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70681453 <a title="37-lda-1" href="./jmlr-2010-Evolving_Static_Representations_for_Task_Transfer.html">37 jmlr-2010-Evolving Static Representations for Task Transfer</a></p>
<p>Author: Phillip Verbancsics, Kenneth O. Stanley</p><p>Abstract: An important goal for machine learning is to transfer knowledge between tasks. For example, learning to play RoboCup Keepaway should contribute to learning the full game of RoboCup soccer. Previous approaches to transfer in Keepaway have focused on transforming the original representation to ﬁt the new task. In contrast, this paper explores the idea that transfer is most effective if the representation is designed to be the same even across different tasks. To demonstrate this point, a bird’s eye view (BEV) representation is introduced that can represent different tasks on the same two-dimensional map. For example, both the 3 vs. 2 and 4 vs. 3 Keepaway tasks can be represented on the same BEV. Yet the problem is that a raw two-dimensional map is high-dimensional and unstructured. This paper shows how this problem is addressed naturally by an idea from evolutionary computation called indirect encoding, which compresses the representation by exploiting its geometry. The result is that the BEV learns a Keepaway policy that transfers without further learning or manipulation. It also facilitates transferring knowledge learned in a different domain, Knight Joust, into Keepaway. Finally, the indirect encoding of the BEV means that its geometry can be changed without altering the solution. Thus static representations facilitate several kinds of transfer.</p><p>2 0.63795871 <a title="37-lda-2" href="./jmlr-2010-Image_Denoising_with_Kernels_Based_on_Natural_Image_Relations.html">50 jmlr-2010-Image Denoising with Kernels Based on Natural Image Relations</a></p>
<p>Author: Valero Laparra, Juan Gutiérrez, Gustavo Camps-Valls, Jesús Malo</p><p>Abstract: A successful class of image denoising methods is based on Bayesian approaches working in wavelet representations. The performance of these methods improves when relations among the local frequency coefﬁcients are explicitly included. However, in these techniques, analytical estimates can be obtained only for particular combinations of analytical models of signal and noise, thus precluding its straightforward extension to deal with other arbitrary noise sources. In this paper, we propose an alternative non-explicit way to take into account the relations among natural image wavelet coefﬁcients for denoising: we use support vector regression (SVR) in the wavelet domain to enforce these relations in the estimated signal. Since relations among the coefﬁcients are speciﬁc to the signal, the regularization property of SVR is exploited to remove the noise, which does not share this feature. The speciﬁc signal relations are encoded in an anisotropic kernel obtained from mutual information measures computed on a representative image database. In the proposed scheme, training considers minimizing the Kullback-Leibler divergence (KLD) between the estimated and actual probability functions (or histograms) of signal and noise in order to enforce similarity up to the higher (computationally estimable) order. Due to its non-parametric nature, the method can eventually cope with different noise sources without the need of an explicit re-formulation, as it is strictly necessary under parametric Bayesian formalisms. Results under several noise levels and noise sources show that: (1) the proposed method outperforms conventional wavelet methods that assume coefﬁcient independence, (2) it is similar to state-of-the-art methods that do explicitly include these relations when the noise source is Gaussian, and (3) it gives better numerical and visual performance when more complex, realistic noise sources are considered. Therefore, the proposed machine learning approach can be seen as a mor</p><p>3 0.25353831 <a title="37-lda-3" href="./jmlr-2010-Mean_Field_Variational_Approximation_for_Continuous-Time_Bayesian_Networks.html">75 jmlr-2010-Mean Field Variational Approximation for Continuous-Time Bayesian Networks</a></p>
<p>Author: Ido Cohn, Tal El-Hay, Nir Friedman, Raz Kupferman</p><p>Abstract: Continuous-time Bayesian networks is a natural structured representation language for multicomponent stochastic processes that evolve continuously over time. Despite the compact representation provided by this language, inference in such models is intractable even in relatively simple structured networks. We introduce a mean ﬁeld variational approximation in which we use a product of inhomogeneous Markov processes to approximate a joint distribution over trajectories. This variational approach leads to a globally consistent distribution, which can be efﬁciently queried. Additionally, it provides a lower bound on the probability of observations, thus making it attractive for learning tasks. Here we describe the theoretical foundations for the approximation, an efﬁcient implementation that exploits the wide range of highly optimized ordinary differential equations (ODE) solvers, experimentally explore characterizations of processes for which this approximation is suitable, and show applications to a large-scale real-world inference problem. Keywords: continuous time Markov processes, continuous time Bayesian networks, variational approximations, mean ﬁeld approximation</p><p>4 0.2511062 <a title="37-lda-4" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>Author: Yevgeny Seldin, Naftali Tishby</p><p>Abstract: We derive PAC-Bayesian generalization bounds for supervised and unsupervised learning models based on clustering, such as co-clustering, matrix tri-factorization, graphical models, graph clustering, and pairwise clustering.1 We begin with the analysis of co-clustering, which is a widely used approach to the analysis of data matrices. We distinguish among two tasks in matrix data analysis: discriminative prediction of the missing entries in data matrices and estimation of the joint probability distribution of row and column variables in co-occurrence matrices. We derive PAC-Bayesian generalization bounds for the expected out-of-sample performance of co-clustering-based solutions for these two tasks. The analysis yields regularization terms that were absent in the previous formulations of co-clustering. The bounds suggest that the expected performance of co-clustering is governed by a trade-off between its empirical performance and the mutual information preserved by the cluster variables on row and column IDs. We derive an iterative projection algorithm for ﬁnding a local optimum of this trade-off for discriminative prediction tasks. This algorithm achieved stateof-the-art performance in the MovieLens collaborative ﬁltering task. Our co-clustering model can also be seen as matrix tri-factorization and the results provide generalization bounds, regularization terms, and new algorithms for this form of matrix factorization. The analysis of co-clustering is extended to tree-shaped graphical models, which can be used to analyze high dimensional tensors. According to the bounds, the generalization abilities of treeshaped graphical models depend on a trade-off between their empirical data ﬁt and the mutual information that is propagated up the tree levels. We also formulate weighted graph clustering as a prediction problem: given a subset of edge weights we analyze the ability of graph clustering to predict the remaining edge weights. The analysis of co-clustering easily</p><p>5 0.24584168 <a title="37-lda-5" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>Author: Joshua W. Robinson, Alexander J. Hartemink</p><p>Abstract: Learning dynamic Bayesian network structures provides a principled mechanism for identifying conditional dependencies in time-series data. An important assumption of traditional DBN structure learning is that the data are generated by a stationary process, an assumption that is not true in many important settings. In this paper, we introduce a new class of graphical model called a nonstationary dynamic Bayesian network, in which the conditional dependence structure of the underlying data-generation process is permitted to change over time. Non-stationary dynamic Bayesian networks represent a new framework for studying problems in which the structure of a network is evolving over time. Some examples of evolving networks are transcriptional regulatory networks during an organism’s development, neural pathways during learning, and trafﬁc patterns during the day. We deﬁne the non-stationary DBN model, present an MCMC sampling algorithm for learning the structure of the model from time-series data under different assumptions, and demonstrate the effectiveness of the algorithm on both simulated and biological data. Keywords: Bayesian networks, graphical models, model selection, structure learning, Monte Carlo methods</p><p>6 0.24427983 <a title="37-lda-6" href="./jmlr-2010-Importance_Sampling_for_Continuous_Time_Bayesian_Networks.html">51 jmlr-2010-Importance Sampling for Continuous Time Bayesian Networks</a></p>
<p>7 0.24415255 <a title="37-lda-7" href="./jmlr-2010-Introduction_to_Causal_Inference.html">56 jmlr-2010-Introduction to Causal Inference</a></p>
<p>8 0.24348204 <a title="37-lda-8" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>9 0.24331588 <a title="37-lda-9" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>10 0.24294548 <a title="37-lda-10" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_I%3A_Algorithms_and_Empirical_Evaluation.html">67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</a></p>
<p>11 0.2426876 <a title="37-lda-11" href="./jmlr-2010-Bayesian_Learning_in_Sparse_Graphical_Factor_Models_via_Variational_Mean-Field_Annealing.html">17 jmlr-2010-Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing</a></p>
<p>12 0.242006 <a title="37-lda-12" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>13 0.24199706 <a title="37-lda-13" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>14 0.24147262 <a title="37-lda-14" href="./jmlr-2010-Stacked_Denoising_Autoencoders%3A_Learning_Useful_Representations_in_a_Deep_Network_with_a_Local_Denoising_Criterion.html">107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</a></p>
<p>15 0.24139036 <a title="37-lda-15" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>16 0.24135718 <a title="37-lda-16" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>17 0.24131562 <a title="37-lda-17" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>18 0.23928952 <a title="37-lda-18" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>19 0.23908298 <a title="37-lda-19" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>20 0.23893137 <a title="37-lda-20" href="./jmlr-2010-WEKA%E2%88%92Experiences_with_a_Java_Open-Source_Project.html">116 jmlr-2010-WEKA−Experiences with a Java Open-Source Project</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
