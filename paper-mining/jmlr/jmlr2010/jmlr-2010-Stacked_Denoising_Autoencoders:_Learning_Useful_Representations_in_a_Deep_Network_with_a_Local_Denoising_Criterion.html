<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-107" href="#">jmlr2010-107</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</h1>
<br/><p>Source: <a title="jmlr-2010-107-pdf" href="http://jmlr.org/papers/volume11/vincent10a/vincent10a.pdf">pdf</a></p><p>Author: Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol</p><p>Abstract: We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classiﬁcation problems to yield signiﬁcantly lower classiﬁcation error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classiﬁers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations. Keywords: deep learning, unsupervised feature learning, deep belief networks, autoencoders, denoising</p><p>Reference: <a title="jmlr-2010-107-reference" href="../jmlr2010_reference/jmlr-2010-Stacked_Denoising_Autoencoders%3A_Learning_Useful_Representations_in_a_Deep_Network_with_a_Local_Denoising_Criterion_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. [sent-16, score-1.218]
</p><p>2 This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations. [sent-17, score-0.557]
</p><p>3 Keywords: deep learning, unsupervised feature learning, deep belief networks, autoencoders, denoising  1. [sent-18, score-0.941]
</p><p>4 It is thus not surprising that initializing a deep network by stacking autoencoders yields almost as good a classiﬁcation performance as when stacking RBMs (Bengio et al. [sent-60, score-0.831]
</p><p>5 This leads us in Section 3 to motivate an alternative denoising criterion, and derive the denoising autoencoder model, for which we also give a possible intuitive geometric interpretation. [sent-69, score-1.236]
</p><p>6 Section 5 presents experiments that qualitatively study the feature detectors learnt by a single-layer denoising autoencoder under various conditions. [sent-72, score-1.012]
</p><p>7 Section 6 describes experiments with multi-layer architectures obtained by stacking denoising autoencoders and compares their classiﬁcation performance with other state-of-the-art models. [sent-73, score-0.98]
</p><p>8 Section 7 is an attempt at turning stacked (denoising) autoencoders into practical generative models, to allow for a qualitative comparison of generated samples with DBNs. [sent-74, score-0.473]
</p><p>9 It can thus be said that training an autoencoder to minimize reconstruction error amounts to maximizing a lower bound on the mutual information between input X and learnt representation Y . [sent-182, score-0.669]
</p><p>10 The traditional approach to autoencoders uses a bottleneck to produce an under-complete representation where d ′ < d. [sent-191, score-0.437]
</p><p>11 When using afﬁne encoder and decoder without any nonlinearity and a squared error loss, the autoencoder essentially performs principal component analysis (PCA) as showed by 3. [sent-193, score-0.465]
</p><p>12 • Second, it is expected that performing the denoising task well requires extracting features that capture useful structure in the input distribution. [sent-221, score-0.508]
</p><p>13 3378  S TACKED D ENOISING AUTOENCODERS  We emphasize here that our goal is not the task of denoising per se. [sent-224, score-0.461]
</p><p>14 Rather denoising is advocated and investigated as a training criterion for learning to extract useful features that will constitute better higher level representation. [sent-225, score-0.556]
</p><p>15 A denoising autoencoder (DAE) is trained to reconstruct a clean “repaired” input from a corrupted version of it (the speciﬁc types of corruptions we consider will be discussed below). [sent-229, score-1.033]
</p><p>16 x Note that denoising autoencoders are still minimizing the same reconstruction loss between a clean X and its reconstruction from Y . [sent-238, score-1.019]
</p><p>17 y gθ′  fθ  LH (x, z)  qD ˜ x  x  z  Figure 1: The denoising autoencoder architecture. [sent-242, score-0.775]
</p><p>18 The autoencoder then maps it to y (via encoder fθ ) and attempts to reconstruct x via decoder gθ′ , producing reconstruction z. [sent-244, score-0.499]
</p><p>19 During denoising training, we learn a stochastic operator p(X|X) that maps a corrupted X back to its uncorrupted X, for example, in the case of binary data, X|X ∼ B (gθ′ ( fθ (X))). [sent-250, score-0.649]
</p><p>20 Successful denoising implies that the operator maps even far away points to a small region close to the manifold. [sent-254, score-0.461]
</p><p>21 The denoising autoencoder can thus be seen as a way to deﬁne and learn a manifold. [sent-255, score-0.775]
</p><p>22 3380  S TACKED D ENOISING AUTOENCODERS  particular we want it to be usable for learning ever higher level representations by stacking denoising autoencoders. [sent-268, score-0.64]
</p><p>23 The salt-and-pepper noise will also be considered, as it is a natural choice for input domains which are interpretable as binary or near binary such as black and white images or the representations produced at the hidden layer after a sigmoid squashing function. [sent-272, score-0.442]
</p><p>24 All information about these masked components is thus removed from that particular input pattern, and we can view the denoising autoencoder as trained to ﬁll-in these artiﬁcially introduced “blanks”. [sent-274, score-0.866]
</p><p>25 4 Extension: Putting an Emphasis on Corrupted Dimensions Noise types such as masking noise and salt-and-pepper that erase only a changing subset of the input’s components while leaving the others untouched suggest a straightforward extension of the denoising autoencoder criterion. [sent-281, score-0.93]
</p><p>26 j∈J (˜ ) / x  We call this extension emphasized denoising autoencoder. [sent-287, score-0.491]
</p><p>27 5 Stacking Denoising Autoencoders to Build Deep Architectures Stacking denoising autoencoders to initialize a deep network works in much the same way as stacking RBMs in deep belief networks (Hinton et al. [sent-290, score-1.456]
</p><p>28 The complete procedure for learning and stacking several layers of denoising autoencoders is shown in Figure 3. [sent-298, score-1.046]
</p><p>29 1 Previous Work on Training Neural Networks for Denoising The idea of training a multi-layer perceptron using error backpropagation on a denoising task is not new. [sent-305, score-0.499]
</p><p>30 Both the model and training procedure in this precursory work are very similar to the denoising autoencoder we describe. [sent-309, score-0.813]
</p><p>31 Also their denoising procedure considers doing several recurrent passes through the autoencoder network, as in a recurrent net. [sent-313, score-0.831]
</p><p>32 3382  S TACKED D ENOISING AUTOENCODERS  LH  (2)  gθ′  (2)  fθ  (2)  fθ qD  fθ  fθ  fθ  x  x  x  Figure 3: Stacking denoising autoencoders. [sent-314, score-0.461]
</p><p>33 After training a ﬁrst level denoising autoencoder (see Figure 1) its learnt encoding function fθ is used on clean input (left). [sent-315, score-1.047]
</p><p>34 The resulting representation is used to train a second level denoising autoencoder (middle) to learn a (2) second level encoding function fθ . [sent-316, score-0.874]
</p><p>35 By contrast, our work is motivated by the search and understanding of unsupervised pretraining criteria to initialize deep networks. [sent-325, score-0.437]
</p><p>36 Our primary interest is thus in investigating the ability of the denoising criterion to learn good feature extractors, with which to initialize a deep network by stacking and composing these feature extractors. [sent-326, score-0.826]
</p><p>37 Again, it differs form our study mainly by its focus a) on recurrent networks6 and b) on the image denoising task per se. [sent-332, score-0.525]
</p><p>38 But apparently, neither of us was initially aware of the other group’s relevant work on denoising (Vincent et al. [sent-340, score-0.461]
</p><p>39 Again the focus of Seung (1998) on image denoising per se differs from our own focus on studying deep network pretraining for classiﬁcation tasks and results in marked differences in the actual algorithms. [sent-342, score-0.929]
</p><p>40 This thread of research is less directly related to autoencoders and denoising than the studies discussed in the previous section. [sent-351, score-0.857]
</p><p>41 After all, denoising amounts to using noisy patterns as input with the clean pattern as a supervised target, albeit a rather high dimensional one. [sent-353, score-0.624]
</p><p>42 1 clearly show qualitatively very different results when using denoising autoencoders (i. [sent-366, score-0.857]
</p><p>43 , noisy inputs) than when using regular autoencoders with a L2 weight decay. [sent-368, score-0.459]
</p><p>44 3 Pseudo-Likelihood and Dependency Networks The view of denoising training as “ﬁlling in the blanks” that motivated the masking noise and the extension that puts an emphasis on corrupted dimensions presented in Section 3. [sent-380, score-0.823]
</p><p>45 This is in effect what an emphasized 1 denoising autoencoder with a masking noise that masks but one input component (ν = d ), and a full emphasis (α = 1, β = 0), is trained to do. [sent-390, score-1.083]
</p><p>46 Note that with denoising autoencoders, all d conditional distributions are constrained to share common parameters, which deﬁne the mapping to and from hidden representation Y . [sent-392, score-0.568]
</p><p>47 By contrast, denoising autoencoders can and will typically be trained with a larger fraction ν of corrupted components, so that reliable prediction of a component cannot rely exclusively on a single other component. [sent-398, score-1.038]
</p><p>48 Experiments on Single Denoising Autoencoders: Qualitative Evaluation of Learned Feature Detectors A ﬁrst series of experiments was carried out using single denoising autoencoders, that is, without any stacking nor supervised ﬁne tuning. [sent-400, score-0.583]
</p><p>49 The goal was to examine qualitatively the kind of feature detectors learnt by a denoising criterion, for various noise types, and compare these to what ordinary autoencoders yield. [sent-401, score-1.214]
</p><p>50 1 Feature Detectors Learnt from Natural Image Patches We trained both regular autoencoders and denoising autoencoders on 12×12 patches from whitened natural scene images, made available by Olshausen (Olshausen and Field, 1996). [sent-406, score-1.369]
</p><p>51 Figure 5 displays ﬁlters learnt by a regular under-complete autoencoder that used a bottleneck of 50 hidden units, as well as those learnt by an over-complete autoencoder using 200 hidden units. [sent-418, score-1.046]
</p><p>52 Middle: ﬁlters learnt by a regular under-complete autoencoder (50 hidden units) using tied weights and L2 reconstruction error. [sent-423, score-0.604]
</p><p>53 Right: ﬁlters learnt by a regular over-complete autoencoder (200 hidden units). [sent-424, score-0.538]
</p><p>54 We then trained 200 hidden units over-complete noiseless autoencoders regularized with L2 weight decay, as well as 200 hidden units denoising autoencoders with isotropic Gaussian noise (but no weight decay). [sent-427, score-1.591]
</p><p>55 Note that a denoising autoencoder with a noise level of 0 is identical to a regular autoencoder. [sent-429, score-0.928]
</p><p>56 So, naturally, ﬁlters learnt by a denoising autoencoder at small noise levels (not shown) look like those obtained with a regular autoencoder previously shown in Figure 5. [sent-430, score-1.363]
</p><p>57 5), the denoising autoencoder learns Gabor-like local oriented edge detectors (see Figure 6). [sent-432, score-0.884]
</p><p>58 The L2 regularized autoencoder on the other hand learnt nothing interesting beyond restoring some of the local blob detectors found in the under-complete case. [sent-434, score-0.59]
</p><p>59 Right: a denoising autoencoder with additive Gaussian noise (σ = 0. [sent-459, score-0.869]
</p><p>60 2 Feature Detectors Learnt from Handwritten Digits We also trained denoising autoencoders on the 28 × 28 gray-scale images of handwritten digits from the MNIST data set. [sent-466, score-0.901]
</p><p>61 For this experiment, we used denoising autoencoders with tied weights, cross-entropy reconstruction error, and zero-masking noise. [sent-467, score-0.923]
</p><p>62 So we trained several denoising autoencoders, all starting from the same initial random point in weight space, but with different noise levels. [sent-469, score-0.599]
</p><p>63 It was to be expected that denoising a more corrupted input requires detecting bigger, less local structures: the denoising auto-encoder must rely on longer range statistical dependencies and pool evidence from a larger subset of pixels. [sent-474, score-1.106]
</p><p>64 3388  S TACKED D ENOISING AUTOENCODERS  Figure 7: Filters obtained on natural image patches by denoising autoencoders using other noise types. [sent-477, score-1.029]
</p><p>65 For the three considered noise types, denoising training appears to learn ﬁlters that capture meaningful natural image statistics structure. [sent-481, score-0.629]
</p><p>66 Experiments on Stacked Denoising Autoencoders In this section, we evaluate denoising autoencoders as a pretraining strategy for building deep networks, using the stacking procedure that we described in Section 3. [sent-483, score-1.353]
</p><p>67 We shall mainly compare the classiﬁcation performance of networks pretrained by stacking denoising autoencoders (SDAE), versus stacking regular autoencoders (SAE), versus stacking restricted Boltzmann machines (DBN), on a benchmark of classiﬁcation problems. [sent-485, score-1.636]
</p><p>68 (a-c) show some of the ﬁlters learnt by denoising autoencoders trained with various corruption levels ν. [sent-501, score-1.197]
</p><p>69 As we increase the noise level, denoising training forces the ﬁlters to differentiate more, and capture more distinctive features. [sent-505, score-0.593]
</p><p>70 However for training the ﬁrst layer on the tzanetakis problem, that is, for reconstructing MPC coefﬁcients, a linear decoder and a squared reconstruction cost were deemed more appropriate (subsequent layers used sigmoid cross entropy as before). [sent-546, score-0.518]
</p><p>71 This shows clearly that denoising pretraining with a non-zero noise level is a better strategy than pretraining with regular autoencoders. [sent-579, score-1.0]
</p><p>72 In most cases, stacking 3 layers of denoising autoencoder seems to be on par or better than stacking 3 layers of RBMs in DBN-3. [sent-583, score-1.153]
</p><p>73 In the following subsections, we will be conducting further detailed experiments to shed light on particular aspects of the denoising autoencoder pretraining strategy. [sent-584, score-0.968]
</p><p>74 05)  Table 3: Comparison of stacked denoising autoencoders (SDAE-3) with other models. [sent-688, score-0.934]
</p><p>75 Figure 10 shows the evolution of the performance as we increase the number of hidden layers from 1 to 3, for three different network training strategies: without any pretraining (standard MLP), with ordinary autoencoder pretraining (SAE) and with denoising autoencoder pretraining (SDAE). [sent-696, score-1.923]
</p><p>76 We clearly see a strict ordering: denoising pretraining being better than autoencoder pretraining being better than no pretraining. [sent-697, score-1.161]
</p><p>77 The advantage appears to increase with the number of layers (note that without pretraining it seems impossible to successfully train a 3 hidden layer network) and with the number of hidden units. [sent-698, score-0.554]
</p><p>78 This general behavior is a typical illustration of what is gained by pretraining deep networks with a good unsupervised criterion, and appears to be common to several pretraining strategies. [sent-699, score-0.656]
</p><p>79 2 the important distinction between denoising pretraining as it is done in SDAE and simply training with noisy inputs. [sent-714, score-0.725]
</p><p>80 SDAE uses a denoising criterion to learn good initial feature extractors at each layer that will be used as initialization for a noiseless supervised training. [sent-715, score-0.651]
</p><p>81 Note that in the case of the SAE, since there are two phases (pretraining and ﬁne-tuning), it is possible to use noisy inputs for only the pretraining or for both the pretraining 13. [sent-718, score-0.443]
</p><p>82 We see that denoising pretraining with SDAE, for a large range of noise levels, yields signiﬁcantly improved performance, whereas training with noisy inputs sometimes degrades the performance, and sometimes improves it slightly but is clearly less beneﬁcial than SDAE. [sent-727, score-0.843]
</p><p>83 3 as well as the emphasized denoising autoencoder variant described in Section 3. [sent-730, score-0.805]
</p><p>84 Besides zero-masking noise (MN) we trained 3 hidden layer SDAE using salt-and-pepper noise (SP) and additive Gaussian noise (GS). [sent-733, score-0.53]
</p><p>85 As already mentioned previously, since Gaussian noise corrupts every dimension, emphasized denoising does not make sense for this type of corruption. [sent-738, score-0.585]
</p><p>86 1)  Table 4: Variations on 3-hidden-layer stacked denoising autoencoders (SDAE-3): alternative noise types and effect of emphasis. [sent-798, score-1.028]
</p><p>87 These included the number of units per layer (same for all layers), the corruption level ν (fraction of corrupted dimensions for MN and SP, or standard deviation for GS), with the usual considered values (listed previously in Table 2). [sent-807, score-0.484]
</p><p>88 3399  V INCENT, L AROCHELLE , L AJOIE , B ENGIO AND M ANZAGOL  Data Set MNIST basic rot bg-rand bg-img bg-img-rot rect rect-img convex tzanetakis  SVM kernel linear rbf linear rbf linear rbf linear rbf linear rbf linear rbf linear rbf linear rbf linear rbf linear rbf  SVM0  SVM1  SVM2  SVM3  5. [sent-839, score-0.592]
</p><p>89 This is further evidence of the qualitative difference resulting from optimizing a denoising criterion instead of mere reconstruction criterion. [sent-1045, score-0.555]
</p><p>90 At the same time we were motivated by a desire to bridge a remaining performance gap between deep belief networks and the stacking of ordinary autoencoders (Bengio et al. [sent-1052, score-0.782]
</p><p>91 As a deep network pretraining strategy, stacking of denoising autoencoders yielded in most cases a signiﬁcant improvement in performance over the stacking of ordinary autoencoders. [sent-1067, score-1.511]
</p><p>92 The representations thus extracted layer by layer, using a purely unsupervised local denoising criterion, appear to make subsequent classiﬁcation tasks much easier. [sent-1068, score-0.69]
</p><p>93 Close examination of the feature extractors learnt by denoising autoencoders showed that they were able to zero in on useful structure in the data (such as Gabor-like edge detectors on natural image patches) that regular autoencoders seemed unable to learn. [sent-1070, score-1.556]
</p><p>94 In addition, we were able to show that, contrary to what it may seem on the surface based on popular myths, the denoising training we advocate is not equivalent to using a mere weight decay regularization, nor is it the same as direct supervised training with corrupted (jittered) examples. [sent-1075, score-0.698]
</p><p>95 Beyond the speciﬁcities and practical usefulness of the simple algorithm we developed, our results clearly establish the value of using a denoising criterion as an unsupervised objective to guide the learning of useful higher level representations. [sent-1076, score-0.557]
</p><p>96 Indeed, denoising performance can easily be measured and directly optimized. [sent-1078, score-0.461]
</p><p>97 The use of a denoising criterion is very different from the contrastive divergence training of RBMs or the direct enforcing of sparsity in autoencoders. [sent-1079, score-0.527]
</p><p>98 We hope that our very encouraging results will inspire further research in this direction, both theoretical (to better understand the relationship between denoising and representation learning), and practical (to develop better learning algorithms based on this understanding). [sent-1080, score-0.502]
</p><p>99 In particular, while stacking denoising autoencoders allows us to build a deep network, the denoising autoencoders we used here were shallow. [sent-1082, score-2.017]
</p><p>100 It would thus be interesting to investigate deep denoising autoencoders with several hidden layers, and their ability to form useful representations. [sent-1083, score-1.128]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('denoising', 0.461), ('autoencoders', 0.396), ('autoencoder', 0.314), ('sdae', 0.231), ('deep', 0.205), ('pretraining', 0.193), ('sae', 0.148), ('corruption', 0.146), ('layer', 0.138), ('corrupted', 0.137), ('learnt', 0.128), ('ajoie', 0.122), ('arochelle', 0.122), ('detectors', 0.109), ('anzagol', 0.105), ('incent', 0.105), ('tacked', 0.099), ('stacking', 0.098), ('engio', 0.094), ('noise', 0.094), ('layers', 0.091), ('enoising', 0.089), ('lters', 0.083), ('larochelle', 0.083), ('stacked', 0.077), ('dbn', 0.076), ('tzanetakis', 0.071), ('decoder', 0.069), ('ih', 0.068), ('reconstruction', 0.066), ('hidden', 0.066), ('masking', 0.061), ('rbms', 0.061), ('representations', 0.052), ('uncorrupted', 0.051), ('encoder', 0.05), ('input', 0.047), ('bengio', 0.047), ('rot', 0.045), ('sigmoid', 0.045), ('rbf', 0.045), ('hinton', 0.044), ('trained', 0.044), ('patches', 0.042), ('mnist', 0.042), ('representation', 0.041), ('rbm', 0.04), ('unsupervised', 0.039), ('stroke', 0.039), ('blob', 0.039), ('erhan', 0.039), ('training', 0.038), ('image', 0.036), ('mutual', 0.035), ('network', 0.034), ('seung', 0.034), ('units', 0.034), ('noisy', 0.033), ('pretrained', 0.033), ('emphasis', 0.032), ('regenerated', 0.032), ('nonlinearity', 0.032), ('qd', 0.032), ('belief', 0.031), ('clean', 0.03), ('emphasized', 0.03), ('olshausen', 0.03), ('regular', 0.03), ('ranzato', 0.03), ('patterns', 0.029), ('hyperparameters', 0.029), ('level', 0.029), ('criterion', 0.028), ('recurrent', 0.028), ('vincent', 0.027), ('lecun', 0.027), ('ordinary', 0.026), ('networks', 0.026), ('encoders', 0.026), ('idkl', 0.026), ('infomax', 0.026), ('mpc', 0.026), ('rect', 0.026), ('architectures', 0.025), ('tikhonov', 0.025), ('svm', 0.025), ('hop', 0.025), ('filters', 0.025), ('inputs', 0.024), ('dbns', 0.024), ('classi', 0.024), ('dence', 0.024), ('supervised', 0.024), ('deterministic', 0.023), ('levels', 0.022), ('af', 0.022), ('performer', 0.022), ('corrupting', 0.022), ('neurons', 0.022), ('bergstra', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="107-tfidf-1" href="./jmlr-2010-Stacked_Denoising_Autoencoders%3A_Learning_Useful_Representations_in_a_Deep_Network_with_a_Local_Denoising_Criterion.html">107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</a></p>
<p>Author: Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol</p><p>Abstract: We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classiﬁcation problems to yield signiﬁcantly lower classiﬁcation error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classiﬁers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations. Keywords: deep learning, unsupervised feature learning, deep belief networks, autoencoders, denoising</p><p>2 0.40339333 <a title="107-tfidf-2" href="./jmlr-2010-Why_Does_Unsupervised_Pre-training_Help_Deep_Learning%3F.html">117 jmlr-2010-Why Does Unsupervised Pre-training Help Deep Learning?</a></p>
<p>Author: Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, Samy Bengio</p><p>Abstract: Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difﬁcult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the inﬂuence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments conﬁrm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pretraining guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training. Keywords: deep architectures, unsupervised pre-training, deep belief networks, stacked denoising auto-encoders, non-convex optimization</p><p>3 0.1533424 <a title="107-tfidf-3" href="./jmlr-2010-Image_Denoising_with_Kernels_Based_on_Natural_Image_Relations.html">50 jmlr-2010-Image Denoising with Kernels Based on Natural Image Relations</a></p>
<p>Author: Valero Laparra, Juan Gutiérrez, Gustavo Camps-Valls, Jesús Malo</p><p>Abstract: A successful class of image denoising methods is based on Bayesian approaches working in wavelet representations. The performance of these methods improves when relations among the local frequency coefﬁcients are explicitly included. However, in these techniques, analytical estimates can be obtained only for particular combinations of analytical models of signal and noise, thus precluding its straightforward extension to deal with other arbitrary noise sources. In this paper, we propose an alternative non-explicit way to take into account the relations among natural image wavelet coefﬁcients for denoising: we use support vector regression (SVR) in the wavelet domain to enforce these relations in the estimated signal. Since relations among the coefﬁcients are speciﬁc to the signal, the regularization property of SVR is exploited to remove the noise, which does not share this feature. The speciﬁc signal relations are encoded in an anisotropic kernel obtained from mutual information measures computed on a representative image database. In the proposed scheme, training considers minimizing the Kullback-Leibler divergence (KLD) between the estimated and actual probability functions (or histograms) of signal and noise in order to enforce similarity up to the higher (computationally estimable) order. Due to its non-parametric nature, the method can eventually cope with different noise sources without the need of an explicit re-formulation, as it is strictly necessary under parametric Bayesian formalisms. Results under several noise levels and noise sources show that: (1) the proposed method outperforms conventional wavelet methods that assume coefﬁcient independence, (2) it is similar to state-of-the-art methods that do explicitly include these relations when the noise source is Gaussian, and (3) it gives better numerical and visual performance when more complex, realistic noise sources are considered. Therefore, the proposed machine learning approach can be seen as a mor</p><p>4 0.053681426 <a title="107-tfidf-4" href="./jmlr-2010-PyBrain.html">93 jmlr-2010-PyBrain</a></p>
<p>Author: Tom Schaul, Justin Bayer, Daan Wierstra, Yi Sun, Martin Felder, Frank Sehnke, Thomas Rückstieß, Jürgen Schmidhuber</p><p>Abstract: PyBrain is a versatile machine learning library for Python. Its goal is to provide ﬂexible, easyto-use yet still powerful algorithms for machine learning tasks, including a variety of predeﬁned environments and benchmarks to test and compare algorithms. Implemented algorithms include Long Short-Term Memory (LSTM), policy gradient methods, (multidimensional) recurrent neural networks and deep belief networks. Keywords: Python, neural networks, reinforcement learning, optimization</p><p>5 0.045337655 <a title="107-tfidf-5" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>Author: Isabelle Guyon, Amir Saffari, Gideon Dror, Gavin Cawley</p><p>Abstract: The principle of parsimony also known as “Ockham’s razor” has inspired many theories of model selection. Yet such theories, all making arguments in favor of parsimony, are based on very different premises and have developed distinct methodologies to derive algorithms. We have organized challenges and edited a special issue of JMLR and several conference proceedings around the theme of model selection. In this editorial, we revisit the problem of avoiding overﬁtting in light of the latest results. We note the remarkable convergence of theories as different as Bayesian theory, Minimum Description Length, bias/variance tradeoff, Structural Risk Minimization, and regularization, in some approaches. We also present new and interesting examples of the complementarity of theories leading to hybrid algorithms, neither frequentist, nor Bayesian, or perhaps both frequentist and Bayesian! Keywords: model selection, ensemble methods, multilevel inference, multilevel optimization, performance prediction, bias-variance tradeoff, Bayesian priors, structural risk minimization, guaranteed risk minimization, over-ﬁtting, regularization, minimum description length</p><p>6 0.044696175 <a title="107-tfidf-6" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>7 0.042072624 <a title="107-tfidf-7" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>8 0.041227557 <a title="107-tfidf-8" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>9 0.040162586 <a title="107-tfidf-9" href="./jmlr-2010-Expectation_Truncation_and_the_Benefits_of_Preselection_In_Training_Generative_Models.html">38 jmlr-2010-Expectation Truncation and the Benefits of Preselection In Training Generative Models</a></p>
<p>10 0.039503671 <a title="107-tfidf-10" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>11 0.03612164 <a title="107-tfidf-11" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>12 0.035768196 <a title="107-tfidf-12" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<p>13 0.033431053 <a title="107-tfidf-13" href="./jmlr-2010-On_Over-fitting_in_Model_Selection_and_Subsequent_Selection_Bias_in_Performance_Evaluation.html">83 jmlr-2010-On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</a></p>
<p>14 0.032094177 <a title="107-tfidf-14" href="./jmlr-2010-Posterior_Regularization_for_Structured_Latent_Variable_Models.html">91 jmlr-2010-Posterior Regularization for Structured Latent Variable Models</a></p>
<p>15 0.030711595 <a title="107-tfidf-15" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>16 0.030100061 <a title="107-tfidf-16" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>17 0.02965451 <a title="107-tfidf-17" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>18 0.028555153 <a title="107-tfidf-18" href="./jmlr-2010-Evolving_Static_Representations_for_Task_Transfer.html">37 jmlr-2010-Evolving Static Representations for Task Transfer</a></p>
<p>19 0.028544066 <a title="107-tfidf-19" href="./jmlr-2010-Learning_From_Crowds.html">61 jmlr-2010-Learning From Crowds</a></p>
<p>20 0.028287999 <a title="107-tfidf-20" href="./jmlr-2010-Dimensionality_Estimation%2C_Manifold_Learning_and_Function_Approximation_using_Tensor_Voting.html">30 jmlr-2010-Dimensionality Estimation, Manifold Learning and Function Approximation using Tensor Voting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.166), (1, 0.031), (2, -0.133), (3, 0.094), (4, 0.041), (5, 0.209), (6, 0.197), (7, -0.309), (8, 0.562), (9, 0.125), (10, -0.075), (11, -0.128), (12, -0.104), (13, -0.05), (14, -0.126), (15, -0.037), (16, -0.062), (17, -0.05), (18, 0.059), (19, 0.1), (20, -0.05), (21, -0.037), (22, -0.039), (23, 0.034), (24, 0.021), (25, 0.009), (26, 0.153), (27, -0.006), (28, 0.012), (29, -0.013), (30, -0.008), (31, -0.012), (32, -0.01), (33, 0.043), (34, 0.044), (35, 0.013), (36, 0.055), (37, -0.038), (38, 0.021), (39, 0.007), (40, 0.01), (41, 0.013), (42, 0.01), (43, -0.039), (44, 0.005), (45, -0.025), (46, 0.052), (47, 0.006), (48, 0.013), (49, -0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94202161 <a title="107-lsi-1" href="./jmlr-2010-Stacked_Denoising_Autoencoders%3A_Learning_Useful_Representations_in_a_Deep_Network_with_a_Local_Denoising_Criterion.html">107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</a></p>
<p>Author: Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol</p><p>Abstract: We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classiﬁcation problems to yield signiﬁcantly lower classiﬁcation error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classiﬁers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations. Keywords: deep learning, unsupervised feature learning, deep belief networks, autoencoders, denoising</p><p>2 0.87792426 <a title="107-lsi-2" href="./jmlr-2010-Why_Does_Unsupervised_Pre-training_Help_Deep_Learning%3F.html">117 jmlr-2010-Why Does Unsupervised Pre-training Help Deep Learning?</a></p>
<p>Author: Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, Samy Bengio</p><p>Abstract: Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difﬁcult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the inﬂuence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments conﬁrm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pretraining guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training. Keywords: deep architectures, unsupervised pre-training, deep belief networks, stacked denoising auto-encoders, non-convex optimization</p><p>3 0.48259744 <a title="107-lsi-3" href="./jmlr-2010-Image_Denoising_with_Kernels_Based_on_Natural_Image_Relations.html">50 jmlr-2010-Image Denoising with Kernels Based on Natural Image Relations</a></p>
<p>Author: Valero Laparra, Juan Gutiérrez, Gustavo Camps-Valls, Jesús Malo</p><p>Abstract: A successful class of image denoising methods is based on Bayesian approaches working in wavelet representations. The performance of these methods improves when relations among the local frequency coefﬁcients are explicitly included. However, in these techniques, analytical estimates can be obtained only for particular combinations of analytical models of signal and noise, thus precluding its straightforward extension to deal with other arbitrary noise sources. In this paper, we propose an alternative non-explicit way to take into account the relations among natural image wavelet coefﬁcients for denoising: we use support vector regression (SVR) in the wavelet domain to enforce these relations in the estimated signal. Since relations among the coefﬁcients are speciﬁc to the signal, the regularization property of SVR is exploited to remove the noise, which does not share this feature. The speciﬁc signal relations are encoded in an anisotropic kernel obtained from mutual information measures computed on a representative image database. In the proposed scheme, training considers minimizing the Kullback-Leibler divergence (KLD) between the estimated and actual probability functions (or histograms) of signal and noise in order to enforce similarity up to the higher (computationally estimable) order. Due to its non-parametric nature, the method can eventually cope with different noise sources without the need of an explicit re-formulation, as it is strictly necessary under parametric Bayesian formalisms. Results under several noise levels and noise sources show that: (1) the proposed method outperforms conventional wavelet methods that assume coefﬁcient independence, (2) it is similar to state-of-the-art methods that do explicitly include these relations when the noise source is Gaussian, and (3) it gives better numerical and visual performance when more complex, realistic noise sources are considered. Therefore, the proposed machine learning approach can be seen as a mor</p><p>4 0.3369796 <a title="107-lsi-4" href="./jmlr-2010-PyBrain.html">93 jmlr-2010-PyBrain</a></p>
<p>Author: Tom Schaul, Justin Bayer, Daan Wierstra, Yi Sun, Martin Felder, Frank Sehnke, Thomas Rückstieß, Jürgen Schmidhuber</p><p>Abstract: PyBrain is a versatile machine learning library for Python. Its goal is to provide ﬂexible, easyto-use yet still powerful algorithms for machine learning tasks, including a variety of predeﬁned environments and benchmarks to test and compare algorithms. Implemented algorithms include Long Short-Term Memory (LSTM), policy gradient methods, (multidimensional) recurrent neural networks and deep belief networks. Keywords: Python, neural networks, reinforcement learning, optimization</p><p>5 0.20209941 <a title="107-lsi-5" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>Author: Gal Chechik, Varun Sharma, Uri Shalit, Samy Bengio</p><p>Abstract: Learning a measure of similarity between pairs of objects is an important generic problem in machine learning. It is particularly useful in large scale applications like searching for an image that is similar to a given image or ﬁnding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, the approaches that exist today for learning such semantic similarity do not scale to large data sets. This is both because typically their CPU and storage requirements grow quadratically with the sample size, and because many methods impose complex positivity constraints on the space of learned similarity functions. The current paper presents OASIS, an Online Algorithm for Scalable Image Similarity learning that learns a bilinear similarity measure over sparse representations. OASIS is an online dual approach using the passive-aggressive family of learning algorithms with a large margin criterion and an efﬁcient hinge loss cost. Our experiments show that OASIS is both fast and accurate at a wide range of scales: for a data set with thousands of images, it achieves better results than existing state-of-the-art methods, while being an order of magnitude faster. For large, web scale, data sets, OASIS can be trained on more than two million images from 150K text queries within 3 days on a single CPU. On this large scale data set, human evaluations showed that 35% of the ten nearest neighbors of a given test image, as found by OASIS, were semantically relevant to that image. This suggests that query independent similarity could be accurately learned even for large scale data sets that could not be handled before. Keywords: large scale, metric learning, image similarity, online learning ∗. Varun Sharma and Uri Shalit contributed equally to this work. †. Also at ICNC, The Hebrew University of Jerusalem, 91904, Israel. c 2010 Gal Chechik, Varun Sharma, Uri Shalit</p><p>6 0.19225241 <a title="107-lsi-6" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>7 0.17809704 <a title="107-lsi-7" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>8 0.16567241 <a title="107-lsi-8" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>9 0.15796664 <a title="107-lsi-9" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>10 0.15786688 <a title="107-lsi-10" href="./jmlr-2010-Expectation_Truncation_and_the_Benefits_of_Preselection_In_Training_Generative_Models.html">38 jmlr-2010-Expectation Truncation and the Benefits of Preselection In Training Generative Models</a></p>
<p>11 0.15632327 <a title="107-lsi-11" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>12 0.15288661 <a title="107-lsi-12" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>13 0.14953165 <a title="107-lsi-13" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>14 0.14863239 <a title="107-lsi-14" href="./jmlr-2010-Evolving_Static_Representations_for_Task_Transfer.html">37 jmlr-2010-Evolving Static Representations for Task Transfer</a></p>
<p>15 0.14744356 <a title="107-lsi-15" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>16 0.1473072 <a title="107-lsi-16" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>17 0.14653233 <a title="107-lsi-17" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<p>18 0.14137539 <a title="107-lsi-18" href="./jmlr-2010-Consensus-Based_Distributed_Support_Vector_Machines.html">26 jmlr-2010-Consensus-Based Distributed Support Vector Machines</a></p>
<p>19 0.13957778 <a title="107-lsi-19" href="./jmlr-2010-Tree_Decomposition_for_Large-Scale_SVM_Problems.html">113 jmlr-2010-Tree Decomposition for Large-Scale SVM Problems</a></p>
<p>20 0.1297861 <a title="107-lsi-20" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.093), (3, 0.013), (8, 0.026), (15, 0.01), (21, 0.024), (32, 0.065), (36, 0.046), (37, 0.053), (39, 0.325), (75, 0.114), (81, 0.018), (85, 0.063), (96, 0.02), (97, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69862992 <a title="107-lda-1" href="./jmlr-2010-Stacked_Denoising_Autoencoders%3A_Learning_Useful_Representations_in_a_Deep_Network_with_a_Local_Denoising_Criterion.html">107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</a></p>
<p>Author: Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol</p><p>Abstract: We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classiﬁcation problems to yield signiﬁcantly lower classiﬁcation error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classiﬁers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations. Keywords: deep learning, unsupervised feature learning, deep belief networks, autoencoders, denoising</p><p>2 0.50451362 <a title="107-lda-2" href="./jmlr-2010-Why_Does_Unsupervised_Pre-training_Help_Deep_Learning%3F.html">117 jmlr-2010-Why Does Unsupervised Pre-training Help Deep Learning?</a></p>
<p>Author: Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, Samy Bengio</p><p>Abstract: Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difﬁcult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the inﬂuence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments conﬁrm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pretraining guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training. Keywords: deep architectures, unsupervised pre-training, deep belief networks, stacked denoising auto-encoders, non-convex optimization</p><p>3 0.43058288 <a title="107-lda-3" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>Author: Erik Ĺ trumbelj, Igor Kononenko</p><p>Abstract: We present a general method for explaining individual predictions of classiﬁcation models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method’s initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efﬁcient and that the explanations are intuitive and useful. Keywords: data postprocessing, classiﬁcation, explanation, visualization</p><p>4 0.41860142 <a title="107-lda-4" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>Author: Yevgeny Seldin, Naftali Tishby</p><p>Abstract: We derive PAC-Bayesian generalization bounds for supervised and unsupervised learning models based on clustering, such as co-clustering, matrix tri-factorization, graphical models, graph clustering, and pairwise clustering.1 We begin with the analysis of co-clustering, which is a widely used approach to the analysis of data matrices. We distinguish among two tasks in matrix data analysis: discriminative prediction of the missing entries in data matrices and estimation of the joint probability distribution of row and column variables in co-occurrence matrices. We derive PAC-Bayesian generalization bounds for the expected out-of-sample performance of co-clustering-based solutions for these two tasks. The analysis yields regularization terms that were absent in the previous formulations of co-clustering. The bounds suggest that the expected performance of co-clustering is governed by a trade-off between its empirical performance and the mutual information preserved by the cluster variables on row and column IDs. We derive an iterative projection algorithm for ﬁnding a local optimum of this trade-off for discriminative prediction tasks. This algorithm achieved stateof-the-art performance in the MovieLens collaborative ﬁltering task. Our co-clustering model can also be seen as matrix tri-factorization and the results provide generalization bounds, regularization terms, and new algorithms for this form of matrix factorization. The analysis of co-clustering is extended to tree-shaped graphical models, which can be used to analyze high dimensional tensors. According to the bounds, the generalization abilities of treeshaped graphical models depend on a trade-off between their empirical data ﬁt and the mutual information that is propagated up the tree levels. We also formulate weighted graph clustering as a prediction problem: given a subset of edge weights we analyze the ability of graph clustering to predict the remaining edge weights. The analysis of co-clustering easily</p><p>5 0.41820064 <a title="107-lda-5" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>Author: Pinar Donmez, Guy Lebanon, Krishnakumar Balasubramanian</p><p>Abstract: Estimating the error rates of classiﬁers or regression models is a fundamental task in machine learning which has thus far been studied exclusively using supervised learning techniques. We propose a novel unsupervised framework for estimating these error rates using only unlabeled data and mild assumptions. We prove consistency results for the framework and demonstrate its practical applicability on both synthetic and real world data. Keywords: classiﬁcation and regression, maximum likelihood, latent variable models</p><p>6 0.4152554 <a title="107-lda-6" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>7 0.41418791 <a title="107-lda-7" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>8 0.41222069 <a title="107-lda-8" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>9 0.41033137 <a title="107-lda-9" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>10 0.41029403 <a title="107-lda-10" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>11 0.40741363 <a title="107-lda-11" href="./jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</a></p>
<p>12 0.40731028 <a title="107-lda-12" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>13 0.4071607 <a title="107-lda-13" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>14 0.4066987 <a title="107-lda-14" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>15 0.40640685 <a title="107-lda-15" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>16 0.40438563 <a title="107-lda-16" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>17 0.40244052 <a title="107-lda-17" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>18 0.40224987 <a title="107-lda-18" href="./jmlr-2010-Bayesian_Learning_in_Sparse_Graphical_Factor_Models_via_Variational_Mean-Field_Annealing.html">17 jmlr-2010-Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing</a></p>
<p>19 0.40191194 <a title="107-lda-19" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>20 0.40176418 <a title="107-lda-20" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
