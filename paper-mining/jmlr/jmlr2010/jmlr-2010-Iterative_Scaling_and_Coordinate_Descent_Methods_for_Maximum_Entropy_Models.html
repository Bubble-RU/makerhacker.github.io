<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-57" href="#">jmlr2010-57</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</h1>
<br/><p>Source: <a title="jmlr-2010-57-pdf" href="http://jmlr.org/papers/volume11/huang10a/huang10a.pdf">pdf</a></p><p>Author: Fang-Lan Huang, Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin</p><p>Abstract: Maximum entropy (Maxent) is useful in natural language processing and many other areas. Iterative scaling (IS) methods are one of the most popular approaches to solve Maxent. With many variants of IS methods, it is difﬁcult to understand them and see the differences. In this paper, we create a general and uniﬁed framework for iterative scaling methods. This framework also connects iterative scaling and coordinate descent methods. We prove general convergence results for IS methods and analyze their computational complexity. Based on the proposed framework, we extend a coordinate descent method for linear SVM to Maxent. Results show that it is faster than existing iterative scaling methods. Keywords: maximum entropy, iterative scaling, coordinate descent, natural language processing, optimization</p><p>Reference: <a title="jmlr-2010-57-reference" href="../jmlr2010_reference/jmlr-2010-Iterative_Scaling_and_Coordinate_Descent_Methods_for_Maximum_Entropy_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This framework also connects iterative scaling and coordinate descent methods. [sent-17, score-0.087]
</p><p>2 Keywords: maximum entropy, iterative scaling, coordinate descent, natural language processing, optimization  1. [sent-21, score-0.058]
</p><p>3 Maxent models the conditional probability as: Pw (y|x) ≡  Sw (x, y) , Tw (x)  Sw (x, y) ≡ e∑t wt ft (x,y) , Tw (x) ≡ ∑ Sw (x, y),  (1)  y  where x indicates a context, y is the label of the context, and w ∈ Rn is the weight vector. [sent-27, score-0.359]
</p><p>4 A realvalued function ft (x, y) denotes the t-th feature extracted from the context x and the label y. [sent-28, score-0.314]
</p><p>5 In some cases, ft (x, y) is 0/1 to indicate a particular property. [sent-30, score-0.314]
</p><p>6 P(x) = ∑y P(x, y) is the marginal probability of x, and P( ft ) = ˜ ∑x,y P(x, y) ft (x, y) is the expected value of ft (x, y). [sent-34, score-0.942]
</p><p>7 To avoid overﬁtting the training samples, some add a regularization term to (2) and solve: min L(w) ≡ min w  w  1  ˜ ˜ ∑ P(x) log Tw (x) − ∑ wt P( ft ) + 2σ2 ∑ wt2 , t  x  (3)  t  where σ is a regularization parameter. [sent-35, score-0.413]
</p><p>8 Existing IS methods include generalized iterative scaling (GIS) by Darroch and Ratcliff (1972), improved iterative scaling (IIS) by Della Pietra et al. [sent-44, score-0.088]
</p><p>9 (1997), and sequential conditional generalized iterative scaling (SCGIS) by Goodman (2002). [sent-45, score-0.057]
</p><p>10 (4) n x,y t: f (x,y)=0 t  In this paper, we assume non-negative feature values: ft (x, y) ≥ 0, ∀t, x, y. [sent-91, score-0.314]
</p><p>11 Conceptually, the one-variable subproblem is related to the function reduction L(w + zt et ) − L(w), where et ≡ [0, . [sent-96, score-0.601]
</p><p>12 1 The Framework To introduce the framework, we separately discuss coordinate descent methods according to whether w is sequentially or parallely updated. [sent-113, score-0.062]
</p><p>13 If the t-th component is selected for update, a sequential IS method solves the following one-variable sub-problem: min At (zt ), zt  where At (zt ) is twice differentiable and bounds the function difference: At (zt ) ≥ L(w + zt et ) − L(w), ∀zt . [sent-119, score-1.231]
</p><p>14 (6)  We hope that by minimizing At (zt ), the resulting L(w + zt et ) can be smaller than L(w). [sent-120, score-0.601]
</p><p>15 If At′ (0) = 0 and assume zt ≡ arg minzt At (zt ) exists, with the condition At (0) = 0, ¯ we have At (¯t ) < 0. [sent-123, score-0.624]
</p><p>16 This property and (6) then imply L(w + zt et ) < L(w). [sent-124, score-0.601]
</p><p>17 In this situation, the convexity of L(w) and prove that ∇t L(w) = 0, t t ∇t L(w) = 0 imply that we cannot decrease the function value by modifying wt , so we should move on to modify other components of w. [sent-126, score-0.056]
</p><p>18 Its approximate function At (zt ) is simply the function difference: AtCD (zt ) = L(w + zt et ) − L(w). [sent-128, score-0.612]
</p><p>19 Deﬁne a function D(zt ) ≡ A(zt ) − (L(w + zt et ) − L(w)). [sent-141, score-0.601]
</p><p>20 Since D(0) = 0, we can ﬁnd a zt such that A(zt )−(L(w+zt et )−L(w)) < 0, a contradiction to (6). [sent-144, score-0.601]
</p><p>21 , n ¯ Approximately solve minzt At (zt ) to get zt . [sent-160, score-0.624]
</p><p>22 z  t=1  zt  That is, we can minimize At (zt ), ∀zt simultaneously, and then update wt ∀t together. [sent-169, score-0.657]
</p><p>23 If A(z) satisﬁes (9), taking z = zt et implies that (6) and (7) hold for At (zt ), ∀t = 1, . [sent-172, score-0.601]
</p><p>24 Their approximate functions aim to bound the change of the function values ˜ L(w + z) − L(w) = ∑ P(x) log x  Tw+z (x) + ∑ Qt (zt ), Tw (x) t  (10)  where Tw (x) is deﬁned in (1) and 2wt zt + zt2 ˜ − zt P( ft ). [sent-181, score-1.536]
</p><p>25 t  x,y  GIS deﬁnes  f # (x, y) ≡ ∑ ft (x, y),  f # ≡ max f # (x, y), x,y  f#  t  f # (x, y)  and adds a feature fn+1 (x, y) ≡ − with zn+1 = 0. [sent-184, score-0.314]
</p><p>26 Using Jensen’s inequality and the assumption of non-negative feature values (5), n+1 ∑t=1  e∑t=1 zt ft (x,y) = e n  n+1  ≤∑  t=1  ft (x,y) zt f # f#  (13) #  n n ft (x, y) zt f # f # − f # (x, y) ft (x, y) zt f # e =∑ e + =∑ f# f# f# t=1 t=1  ezt f − 1 ft (x, y) + 1. [sent-185, score-4.12]
</p><p>27 f#  Substituting (13) into (12), the approximate function of GIS is A  GIS  ˜ (z) = ∑ Qt (zt ) + ∑ P(x)Pw (y|x) ∑ t  t  x,y  #  ezt f − 1 ft (x, y) . [sent-186, score-0.471]
</p><p>28 f#  Then we obtain n independent one-variable functions: #  GIS  At  ezt f − 1 ˜ P(x)Pw (y|x) ft (x, y). [sent-187, score-0.46]
</p><p>29 It replaces f # in GIS with  ft# ≡ max ft (x, y). [sent-190, score-0.314]
</p><p>30 x,y  (14)  Using zt et as z in (10), a derivation similar to (13) gives ezt ft (x,y) ≤  ft (x, y) zt ft# ft# − ft (x, y) e + . [sent-191, score-2.29]
</p><p>31 ft# ft# 820  (15)  I TERATIVE S CALING AND C OORDINATE D ESCENT M ETHODS FOR M AXIMUM E NTROPY M ODELS  The approximate function of SCGIS is #  SCGIS  At  ezt ft − 1 ˜ (zt ) = Qt (zt ) + ∑ P(x)Pw (y|x) ft (x, y). [sent-192, score-0.785]
</p><p>32 We denote wk as the point after each iteration of the while loop in Algorithm 1 or 2. [sent-210, score-0.133]
</p><p>33 Hence from wk to wk+1 , n sub-problems are solved. [sent-211, score-0.12]
</p><p>34 If the algorithm satisﬁes wk+1 − wk  k+1  L(w  k  ≥ η ∇L(wk ) , k+1  ) − L(w ) ≤ −ν w  (23) k 2  −w  ,  (24)  for some positive constants η and ν, then the sequence {wk } generated by the algorithm linearly converges. [sent-216, score-0.12]
</p><p>35 We do not discuss some rare situations where this property does not hold (for example, minzt AtGIS (zt ) has an ˜ optimal solution zt = −∞ if P( ft ) = 0 and the regularization term is not considered). [sent-229, score-0.938]
</p><p>36 Without the regularization term, by At′ (zt ) = 0, GIS and SCGIS both have a simple closed-form solution of the sub-problem: zt =  1 log fs  ˜ P( ft ) ˜ ∑x,y P(x)Pw (y|x) ft (x, y)  ,  822  where f s ≡  f # if s is GIS, ft# if s is SCGIS. [sent-230, score-1.238]
</p><p>37 (25)  I TERATIVE S CALING AND C OORDINATE D ESCENT M ETHODS FOR M AXIMUM E NTROPY M ODELS  #  For IIS, the term ezt f (x,y) in AtIIS (zt ) depends on x and y, so it does not have a closed-form solution. [sent-231, score-0.146]
</p><p>38 The Newton method minimizes Ats (zt ) by iteratively updating zt : (26) zt ← zt − Ats ′ (zt )/Ats ′′ (zt ), where s indicates an IS or a CD method. [sent-235, score-1.803]
</p><p>39 The Newton directions of GIS and SCGIS are similar: s ˜ Qt′ (zt ) + ezt f ∑x,y P(x)Pw (y|x) ft (x, y) Ats ′ (zt ) − s ′′ , = − ′′ s ˜ At (zt ) Qt (zt ) + f s ezt f ∑x,y P(x)Pw (y|x) ft (x, y)  (27)  where f s is deﬁned in (25). [sent-239, score-0.92]
</p><p>40 For IIS, the Newton direction is: −  ′  AtIIS (zt ) ′′  AtIIS (zt )  #  =−  ˜ Qt′ (zt ) + ∑x,y P(x)Pw (y|x) ft (x, y)ezt f (x,y) . [sent-240, score-0.325]
</p><p>41 To get Sw (x, y) ∀x, y, the operation ∑ wt ft (x, y) ∀x, y t  needs O(#nz) time. [sent-249, score-0.359]
</p><p>42 If XY > #nz, one can calculate ewt ft (x,y) , ∀ ft (x, y) = 0 and then the product ∏t: ft (x,y)=0 ewt ft (x,y) . [sent-253, score-1.288]
</p><p>43 Since Sw+zt et (x, y) = Sw (x, y), if ft (x, y) = 0, this procedure reduces the number of operations ¯ from the O(#nz) operations to O(l). [sent-258, score-0.348]
</p><p>44 From (27) and (28), all remaining operations of GIS, IIS, and SCGIS involve the calculation of ˜ ∑ P(x)Pw (y|x) ft (x, y)(a function of zt ),  (32)  x,y  ¯ which needs O(l) under a ﬁxed t. [sent-261, score-0.932]
</p><p>45 For GIS and SCGIS, since the function of zt in (32) is independent ˜ of x, y, we can calculate and store ∑x,y P(x)Pw (y|x) ft (x, y) in the ﬁrst Newton iteration. [sent-262, score-0.942]
</p><p>46 Therefore, ¯ the overall cost (including calculating Pw (y|x)) is O(l) for the ﬁrst Newton iteration and O(1) for zt f # (x,y) in (28) depends on x and y, we need O(l) for ¯ each subsequent iteration. [sent-263, score-0.601]
</p><p>47 For CD, it calculates Pw+zt et (y|x) for every zt , so the cost per Newton ¯ direction is O(l). [sent-265, score-0.612]
</p><p>48 Approximate L(w + zt et ) − L(w) or L(w + z) − L(w) to obtain functions At (zt ). [sent-269, score-0.601]
</p><p>49 To check how IS and CD methods ﬁt into this explanation, we obtain the following relationship of their approximate functions: AtCD (zt ) ≤ AtSCGIS (zt ) ≤ AtGIS (zt ),  AtCD (zt ) ≤ AtIIS (zt ) ≤ AtGIS (zt ) ∀ zt . [sent-310, score-0.612]
</p><p>50 The Newton direction at zt = 0 is d=−  ′  AtCD (0) ′′  AtCD (0)  . [sent-324, score-0.612]
</p><p>51 Hence we need a line search procedure to ﬁnd λ ≥ 0 such that zt = λd satisﬁes the following sufﬁcient 825  H UANG , H SIEH , C HANG AND L IN  Algorithm 3 A fast coordinate descent method for Maxent • Choose β ∈ (0, 1) and γ ∈ (0, 1/2). [sent-327, score-0.653]
</p><p>52 Calculate the Newton direction ′  ′′  d = −AtCD (0)/AtCD (0) =  wt ˜ − ∑x,y P(x)Pw (y|x) ft (x, y) + σ2 ˜ ˜ ∑x,y P(x)Pw (y|x) ft (x, y)2 − ∑x P(x) ∑y Pw (y|x) ft (x, y)  where Pw (y|x) =  2  1 + σ2  ,  Sw (x, y) . [sent-333, score-0.998]
</p><p>53 (a) Let zt = λd (b) Calculate ˜ AtCD (zt ) = Qt (zt ) + ∑ P(x) log 1 + ∑ y  x  Sw (x, y) zt ft (x,y) (e − 1) Tw (x)  ′  (c) If AtCD (zt ) ≤ γzt AtCD (0), then break. [sent-338, score-1.525]
</p><p>54 Note that zt AtCD (0) is negative under the deﬁnition of d in (34). [sent-342, score-0.601]
</p><p>55 There is λ > 0 such that zt = λd satisﬁes (35) ¯ for all 0 ≤ λ < λ. [sent-356, score-0.601]
</p><p>56 For any w satisfying w − w∗ ≤ ε, if we select an index t and generate directions ′  ′′  d = −AtCD (0)/AtCD (0) and  d s = arg min Ats (zt ), zt  s = GIS, IIS or SCGIS,  (37)  then δt (d) < min δt (d GIS ), δt (d IIS ), δt (d SCGIS ) , where δt (zt ) ≡ L(w + zt et ) − L(w). [sent-375, score-1.234]
</p><p>57 (a) Let zt = λd ¯ (b) Calculate AtCD (zt ) ′ ¯ (c) If AtCD (zt ) ≤ γzt AtCD (0), then break. [sent-389, score-0.601]
</p><p>58 827  (38)  H UANG , H SIEH , C HANG AND L IN  We assume non-negative feature values and obtain zt ft#  e ¯ ˜ AtCD (zt ) ≡ Qt (zt ) + Pt log 1 +  −1  ˜ ft# Pt  ˜ ∑ P(x)Pw (y|x) ft (x, y)  ,  (39)  x,y  where ft# is deﬁned in (14), ˜ ˜ Pt ≡ ∑ P(x),  and  Ωt  Ωt ≡ {x : ∃y such that ft (x, y) = 0}. [sent-391, score-1.238]
</p><p>59 1 Row Versus Column Format In many Maxent applications, data are sparse with few nonzero ft (x, y). [sent-403, score-0.314]
</p><p>60 ” For the row format, each (x, y) corresponds to a list of nonzero ft (x, y), while for the column format, each feature t is associated with a list of (x, y). [sent-406, score-0.324]
</p><p>61 The loop to access data in the row format is (x, y) → t, while for the column format it is t → (x, y). [sent-407, score-0.077]
</p><p>62 For IIS, an implementation by the row format is # more complicated due to the ezt f (x,y) term in AtIIS (zt ). [sent-414, score-0.183]
</p><p>63 methods, they also need O(XY ) spaces if using the column format: To calculate e∑t wt ft (x,y) ∀x, y via a loop of t → (x, y), we need O(XY ) positions to store ∑t wt ft (x, y) ∀x, y. [sent-426, score-0.758]
</p><p>64 IIS and CD need O(l) exp operations for every Newton zt f # (x,y) in (28) and ezt ft (x,y) in (17), respectively. [sent-435, score-1.078]
</p><p>65 If zt ft (x,y) in (17) becomes ezt , a value independent of x, y. [sent-437, score-1.061]
</p><p>66 We set the feature ft (xi , y) as x ¯ i=1 ¯ ft (xi , y) =  xit ¯ 0  if y = 1, if y = −1,  ¯ where xi denotes the index of the i-th training instance xi . [sent-575, score-0.683]
</p><p>67 Then Sw (xi , y) =  e∑t wt ft (xi ,y)  and Pw (y|xi ) =  Tx ¯  =  ew 1  i  if y = 1, if y = −1,  Sw (xi , y) 1 . [sent-576, score-0.359]
</p><p>68 In summary, we create a general framework for iterative scaling and coordinate descent methods for maximum entropy. [sent-672, score-0.087]
</p><p>69 To prove (23), we use ¯ wk+1 − wk = z − 0 1 1 1 z ∇As (0) = ∇L(wk ) , ≥ ∇As (¯ ) − ∇As (0) = K K K  (57)  where the inequality is from the same derivation for (49) in Theorem 1. [sent-723, score-0.131]
</p><p>70 2 2σ 2σ  (58)  From (9) and (58), ¯ L(wk ) − L(wk+1 ) = L(wk ) − L(wk + z) ≥ As (0) − As (¯ ) ≥ z  1 1 T ¯ ¯ z z = 2 wk+1 − wk 2 . [sent-727, score-0.12]
</p><p>71 At each iterk=0 ation, wk+1 is constructed by sequentially updating each component of wk . [sent-732, score-0.13]
</p><p>72 , n, such that wk,1 = wk , wk,n+1 = wk+1 , and k+1 k+1 wk,t = [w1 , . [sent-736, score-0.12]
</p><p>73 To prove (23), taking the summation of (59) from t = 1 to n, wk+1 − wk  n  n  k,1 k,t k,1 k,t ¯ ¯ 1 ≥ η ∑ |∇L(w )t | ≥ η ∑ |∇L(w )t | − |∇L(w )t − ∇L(w )t | t=1  t=1  ¯ =η  ∇L(wk,1 )  n  1−  ∑ |∇L(wk,t )t − ∇L(wk,1 )t |  . [sent-757, score-0.131]
</p><p>74 (61)  t=1  Since L(w) satisﬁes (21), using (44), n  n  ∑ |∇L(wk,t )t − ∇L(wk,1 )t | ≤ ∑  t=1 n √  ≤∑  t=1  t=1  nτmax wk,t − wk,1  1  ∇L(wk,t ) − ∇L(wk,1 )  1  √ ≤ n nτmax wk+1 − wk 1 . [sent-758, score-0.12]
</p><p>75 From (61) and (62), we have wk+1 − wk  1  ≥  ¯ η √ ∇L(wk,1 ) 1 . [sent-759, score-0.12]
</p><p>76 ¯ 1 + ηn nτmax  This inequality and (44) imply wk+1 − wk ≥  1 √ n  wk+1 − wk  1  ≥  √  ¯ η ¯ n+ηn2 τmax  ∇L(wk ) . [sent-760, score-0.24]
</p><p>77 Taking the summation of (60) from t = 1 to n, we get (24): L(wk ) − L(wk+1 ) ≥  1 wk+1 − wk 2 . [sent-763, score-0.12]
</p><p>78 4 Derivation of (30)-(31) Using (18)-(19), we have dSw+zt et (x, y) = Sw (x, y)ezt ft (x,y) ft (x, y) = Sw+zt et (x, y) ft (x, y) dzt and  dTw+zt et (x) = ∑ Sw (x, y)ezt ft (x,y) ft (x, y) = ∑ Sw+zt et (x, y) ft (x, y). [sent-765, score-1.911]
</p><p>79 dzt y y 838  (62)  I TERATIVE S CALING AND C OORDINATE D ESCENT M ETHODS FOR M AXIMUM E NTROPY M ODELS  Then (30) can be obtained from (16), the deﬁnition of Tw+zt et (x) in (1), and the following calculation: d log Tw+zt et (x) ∑y Sw+zt et (x, y) ft (x, y) = ∑ Pw+zt et (y|x) ft (x, y). [sent-766, score-0.664]
</p><p>80 Then #  #  D′ (zt ) = ezt f − ezt ft  ˜ ∑ P(x)Pw (y|x) ft (x, y). [sent-772, score-0.92]
</p><p>81 x,y  Since f # ≥ ft# ≥ 0, D′ (zt ) ≥ 0 if zt > 0,  (63)  D′ (zt ) ≤ 0 if zt < 0. [sent-773, score-1.202]
</p><p>82 From Taylor expansion, there exists h between 0 and zt such that D(zt ) = D(0) + zt D′ (h). [sent-774, score-1.202]
</p><p>83 By (63), zt D′ (h) ≥ 0, so AtGIS (zt ) − AtSCGIS (zt ) = D(zt ) ≥ D(0) = 0. [sent-776, score-0.601]
</p><p>84 6 Proof of Theorem 4 From (31), we can deﬁne H = max t  1 ˜ + P(x) ft (x, y)2 σ2 ∑ x,y 839  ′′  ≥ AtCD (zt ), ∀zt . [sent-779, score-0.314]
</p><p>85 This and (34) suggest that the step size zt = λd in Algorithm 3 satisﬁes |zt | = λ  ′  −AtCD (0) ′′ AtCD (0)  ≥  2β(1 − γ) CD ′ At (0) . [sent-791, score-0.601]
</p><p>86 H  (67)  ′′  From (34), (35), zt = λd, AtCD (0) ≥ 1/σ2 and λ ≤ 1, we have ′  ′′  AtCD (zt ) − AtCD (0) ≤ γzt AtCD (0) = −γzt dAtCD (0) ≤ −  γ γ 2 z ≤ − 2 zt2 . [sent-792, score-0.601]
</p><p>87 2 t λσ σ  (68)  Note that zt is the step taken for updating wtk,t to wtk,t+1 . [sent-793, score-0.601]
</p><p>88 8 Proof of Theorem 6 ′′′  A direct calculation of AtCD (zt ) shows that it is bounded for all zt and wtk . [sent-799, score-0.601]
</p><p>89 Using Taylor expansion, if zt f s (x, y) < 0, then ezt f  s (x,y)  1 ≤ 1 + zt f s (x, y) + zt2 ( f s (x, y))2 , 2 841  (72)  H UANG , H SIEH , C HANG AND L IN  where    f # if s is GIS,  f s (x, y) ≡ ft# if s is SCGIS,   # f (x, y) if s is IIS. [sent-815, score-1.348]
</p><p>90 δt t  (74)  From (31), 2  δt′′ (0)  ˜ ˜ = ∑ P(x)Pw (y|x) ft (x, y)2 − ∑ P(x) x,y  x  ∑ Pw (y|x) ft (x, y) y  +  1 σ2  (75)  1 ≤ R1 (w) − R3 (w) + 2 , σ where  2  ˜ R3 (w) ≡ ∑ P(x) x  ∑ Pw (y|x) ft (x, y)  . [sent-818, score-0.942]
</p><p>91 ≤− 1 1+λ R1 (w) + σ2 − 1 |d s |R2 (w) 2  (78)  From (73) with zt = d/(1 + λ) and (78), Ats ′  d 1+λ  ≤ Ats ′ (0) − Ats ′ (0) = 0. [sent-828, score-0.601]
</p><p>92 Using Taylor expansion, if zt f s (x, y) > 0, we have ezt f  s (x,y)  ≥ 1 + zt f s (x, y). [sent-831, score-1.348]
</p><p>93 Then (73) becomes Ats ′ (zt ) ≥ =  wt  ˜ ˜ ∑ P(x)Pw (y|x) ft (x, y) + σ2 − P( ft )  + R1 (w) +  x,y  Ats ′ (0) +  1 R1 (w) + 2 σ  1 σ2  zt (79)  zt . [sent-832, score-1.875]
</p><p>94 ≤ log  ˜ ˜ ∑Ω P(x) ∑Ω P(x) t  t  With (16), (19) and (40), we have AtCD (zt )  ∑Ωt ˜ ≤ Qt (zt ) + Pt log 1 +  ˜ P(x) ∑y Pw (y|x)(ezt ft (x,y) − 1) ˜ Pt  . [sent-845, score-0.332]
</p><p>95 By the inequality (15),    ˜ AtCD (zt ) ≤ Qt (zt ) + Pt log 1 +   ˜ ∑Ωt P(x) ∑y Pw (y|x)  #  ft (x,y)ezt ft ft#  +  ft# − ft (x,y) ft#  ˜ Pt   # ˜ ezt ft − 1 ∑Ωt P(x) ∑y Pw (y|x) ft (x, y)  ˜ = Qt (zt ) + Pt log 1 + ˜ ft# Pt     −1     ¯ = AtCD (zt ). [sent-846, score-1.734]
</p><p>96 Note that ˜ P(xi , y) =  1 l  0  if y = yi , ¯ 1 ˜ ¯ and P( ft ) = ∑ xit . [sent-851, score-0.356]
</p><p>97 and  j  i:yi =1 ¯  Similarly, IIS and SCGIS respectively solve #  ezt f (i) − 1 xit ¯ ∑ 1 + e−wT xi f # (i) i  1 At (zt ) = Qt (zt ) + l IIS  SCGIS  At  ,  #  xit ¯ ezt ft − 1 ∑ 1 + e−wT xi # ft i  1 (zt ) = Qt (zt ) + l  ,  ¯ ¯ where ft# = maxi xit and f # (i) = ∑t xit . [sent-853, score-1.088]
</p><p>98 Finally, from (17), (30), and (31), AtCD (zt ) = Qt (zt ) + ′  AtCD (0) = ′′  AtCD (0) =  ¯ 1 ezt xit − 1 log 1 + ¯ l∑ 1 + e−wT xi i  wt 1 + σ2 l  ∑ 1 + e−w x ¯  1 1 + σ2 l  ¯ 2 ¯ e−w xi xit ∑ (1 + e−wT xi )2 ¯ i  xit ¯  T  i  i  −  ∑  ,  xit , ¯  i:yi =1 ¯  T  . [sent-854, score-0.368]
</p><p>99 Iterative scaling and coordinate descent methods for maximum entropy. [sent-942, score-0.064]
</p><p>100 On the convergence of coordinate descent method for convex differentiable minimization. [sent-980, score-0.054]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('zt', 0.601), ('atcd', 0.433), ('ft', 0.314), ('gis', 0.219), ('pw', 0.214), ('scgis', 0.21), ('cd', 0.177), ('ezt', 0.146), ('ats', 0.132), ('wk', 0.12), ('tw', 0.109), ('sw', 0.102), ('tron', 0.101), ('iis', 0.094), ('lbfgs', 0.088), ('uang', 0.077), ('qt', 0.076), ('newton', 0.074), ('terative', 0.062), ('ntropy', 0.062), ('maxent', 0.062), ('sieh', 0.06), ('oordinate', 0.056), ('aximum', 0.056), ('escent', 0.052), ('caling', 0.052), ('wt', 0.045), ('xit', 0.042), ('atgis', 0.041), ('atscgis', 0.041), ('odels', 0.039), ('hang', 0.039), ('atiis', 0.036), ('ethods', 0.036), ('nz', 0.032), ('pos', 0.031), ('datcd', 0.027), ('dzt', 0.027), ('format', 0.027), ('nlp', 0.027), ('coordinate', 0.024), ('iterative', 0.023), ('minzt', 0.023), ('opennlp', 0.023), ('scaling', 0.021), ('chang', 0.02), ('logistic', 0.02), ('descent', 0.019), ('cdd', 0.019), ('pt', 0.019), ('operations', 0.017), ('min', 0.016), ('tagging', 0.016), ('della', 0.016), ('malouf', 0.016), ('dud', 0.016), ('xy', 0.015), ('lin', 0.015), ('chunking', 0.014), ('csie', 0.014), ('ntu', 0.014), ('baldridge', 0.014), ('calculate', 0.014), ('training', 0.013), ('lange', 0.013), ('pietra', 0.013), ('store', 0.013), ('loop', 0.013), ('sequential', 0.013), ('theorem', 0.013), ('linguistics', 0.012), ('document', 0.012), ('brown', 0.012), ('prove', 0.011), ('approximate', 0.011), ('collins', 0.011), ('convergence', 0.011), ('checking', 0.011), ('hsieh', 0.011), ('language', 0.011), ('direction', 0.011), ('update', 0.011), ('daum', 0.011), ('satis', 0.01), ('row', 0.01), ('relative', 0.01), ('sequentially', 0.01), ('testing', 0.01), ('goodman', 0.01), ('entropy', 0.01), ('search', 0.009), ('log', 0.009), ('taylor', 0.009), ('darroch', 0.009), ('dsw', 0.009), ('ewt', 0.009), ('grippo', 0.009), ('parallely', 0.009), ('pdate', 0.009), ('rong', 0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="57-tfidf-1" href="./jmlr-2010-Iterative_Scaling_and_Coordinate_Descent_Methods_for_Maximum_Entropy_Models.html">57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</a></p>
<p>Author: Fang-Lan Huang, Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin</p><p>Abstract: Maximum entropy (Maxent) is useful in natural language processing and many other areas. Iterative scaling (IS) methods are one of the most popular approaches to solve Maxent. With many variants of IS methods, it is difﬁcult to understand them and see the differences. In this paper, we create a general and uniﬁed framework for iterative scaling methods. This framework also connects iterative scaling and coordinate descent methods. We prove general convergence results for IS methods and analyze their computational complexity. Based on the proposed framework, we extend a coordinate descent method for linear SVM to Maxent. Results show that it is faster than existing iterative scaling methods. Keywords: maximum entropy, iterative scaling, coordinate descent, natural language processing, optimization</p><p>2 0.1352316 <a title="57-tfidf-2" href="./jmlr-2010-A_Comparison_of_Optimization_Methods_and_Software_for_Large-scale_L1-regularized_Linear_Classification.html">1 jmlr-2010-A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</a></p>
<p>Author: Guo-Xun Yuan, Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Large-scale linear classiﬁcation is widely used in many areas. The L1-regularized form can be applied for feature selection; however, its non-differentiability causes more difﬁculties in training. Although various optimization methods have been proposed in recent years, these have not yet been compared suitably. In this paper, we ﬁrst broadly review existing methods. Then, we discuss state-of-the-art software packages in detail and propose two efﬁcient implementations. Extensive comparisons indicate that carefully implemented coordinate descent methods are very suitable for training large document data. Keywords: L1 regularization, linear classiﬁcation, optimization methods, logistic regression, support vector machines, document classiﬁcation</p><p>3 0.079838082 <a title="57-tfidf-3" href="./jmlr-2010-Dual_Averaging_Methods_for_Regularized_Stochastic_Learning_and_Online_Optimization.html">31 jmlr-2010-Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a></p>
<p>Author: Lin Xiao</p><p>Abstract: We consider regularized stochastic learning and online optimization problems, where the objective function is the sum of two convex terms: one is the loss function of the learning task, and the other is a simple regularization term such as ℓ1 -norm for promoting sparsity. We develop extensions of Nesterov’s dual averaging method, that can exploit the regularization structure in an online setting. At each iteration of these methods, the learning variables are adjusted by solving a simple minimization problem that involves the running average of all past subgradients of the loss function and the whole regularization term, not just its subgradient. In the case of ℓ1 -regularization, our method is particularly effective in obtaining sparse solutions. We show that these methods achieve the optimal convergence rates or regret bounds that are standard in the literature on stochastic and online convex optimization. For stochastic learning problems in which the loss functions have Lipschitz continuous gradients, we also present an accelerated version of the dual averaging method. Keywords: stochastic learning, online optimization, ℓ1 -regularization, structural convex optimization, dual averaging methods, accelerated gradient methods</p><p>4 0.072722539 <a title="57-tfidf-4" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>Author: Jean-Yves Audibert, Sébastien Bubeck</p><p>Abstract: This work deals with four classical prediction settings, namely full information, bandit, label efﬁcient and bandit label efﬁcient as well as four different notions of regret: pseudo-regret, expected regret, high probability regret and tracking the best expert regret. We introduce a new forecaster, INF (Implicitly Normalized Forecaster) based on an arbitrary function ψ for which we propose a uniﬁed γ analysis of its pseudo-regret in the four games we consider. In particular, for ψ(x) = exp(ηx) + K , INF reduces to the classical exponentially weighted average forecaster and our analysis of the pseudo-regret recovers known results while for the expected regret we slightly tighten the bounds. γ η q On the other hand with ψ(x) = −x + K , which deﬁnes a new forecaster, we are able to remove the extraneous logarithmic factor in the pseudo-regret bounds for bandits games, and thus ﬁll in a long open gap in the characterization of the minimax rate for the pseudo-regret in the bandit game. We also provide high probability bounds depending on the cumulative reward of the optimal action. Finally, we consider the stochastic bandit game, and prove that an appropriate modiﬁcation of the upper conﬁdence bound policy UCB1 (Auer et al., 2002a) achieves the distribution-free optimal rate while still having a distribution-dependent rate logarithmic in the number of plays. Keywords: Bandits (adversarial and stochastic), regret bound, minimax rate, label efﬁcient, upper conﬁdence bound (UCB) policy, online learning, prediction with limited feedback.</p><p>5 0.052216895 <a title="57-tfidf-5" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>Author: Julien Mairal, Francis Bach, Jean Ponce, Guillermo Sapiro</p><p>Abstract: Sparse coding—that is, modelling data vectors as sparse linear combinations of basis elements—is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to speciﬁc data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets. Keywords: basis pursuit, dictionary learning, matrix factorization, online learning, sparse coding, sparse principal component analysis, stochastic approximations, stochastic optimization, nonnegative matrix factorization</p><p>6 0.050631057 <a title="57-tfidf-6" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>7 0.04997728 <a title="57-tfidf-7" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>8 0.040251039 <a title="57-tfidf-8" href="./jmlr-2010-A_Generalized_Path_Integral_Control_Approach_to_Reinforcement_Learning.html">4 jmlr-2010-A Generalized Path Integral Control Approach to Reinforcement Learning</a></p>
<p>9 0.038037159 <a title="57-tfidf-9" href="./jmlr-2010-Bundle_Methods_for_Regularized_Risk_Minimization.html">18 jmlr-2010-Bundle Methods for Regularized Risk Minimization</a></p>
<p>10 0.036655784 <a title="57-tfidf-10" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>11 0.035979897 <a title="57-tfidf-11" href="./jmlr-2010-A_Fast_Hybrid_Algorithm_for_Large-Scalel1-Regularized_Logistic_Regression.html">3 jmlr-2010-A Fast Hybrid Algorithm for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>12 0.033283353 <a title="57-tfidf-12" href="./jmlr-2010-On_Finding_Predictors_for_Arbitrary_Families_of_Processes.html">81 jmlr-2010-On Finding Predictors for Arbitrary Families of Processes</a></p>
<p>13 0.033268388 <a title="57-tfidf-13" href="./jmlr-2010-A_Quasi-Newton_Approach_to_Nonsmooth_Convex_Optimization_Problems_in_Machine_Learning.html">5 jmlr-2010-A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning</a></p>
<p>14 0.031493228 <a title="57-tfidf-14" href="./jmlr-2010-Stability_Bounds_for_Stationary_%CF%86-mixing_and_%CE%B2-mixing_Processes.html">106 jmlr-2010-Stability Bounds for Stationary φ-mixing and β-mixing Processes</a></p>
<p>15 0.031326383 <a title="57-tfidf-15" href="./jmlr-2010-Posterior_Regularization_for_Structured_Latent_Variable_Models.html">91 jmlr-2010-Posterior Regularization for Structured Latent Variable Models</a></p>
<p>16 0.025143243 <a title="57-tfidf-16" href="./jmlr-2010-Erratum%3A_SGDQN_is_Less_Careful_than_Expected.html">34 jmlr-2010-Erratum: SGDQN is Less Careful than Expected</a></p>
<p>17 0.021577135 <a title="57-tfidf-17" href="./jmlr-2010-A_Rotation_Test_to_Verify_Latent_Structure.html">6 jmlr-2010-A Rotation Test to Verify Latent Structure</a></p>
<p>18 0.020967839 <a title="57-tfidf-18" href="./jmlr-2010-On_Spectral_Learning.html">84 jmlr-2010-On Spectral Learning</a></p>
<p>19 0.020372927 <a title="57-tfidf-19" href="./jmlr-2010-Chromatic_PAC-Bayes_Bounds_for_Non-IID_Data%3A_Applications_to_Ranking_and_Stationary_%CE%B2-Mixing_Processes.html">20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</a></p>
<p>20 0.019167187 <a title="57-tfidf-20" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.099), (1, -0.087), (2, 0.083), (3, -0.024), (4, 0.162), (5, -0.055), (6, 0.003), (7, 0.018), (8, 0.059), (9, 0.019), (10, 0.034), (11, 0.054), (12, 0.024), (13, 0.046), (14, 0.184), (15, -0.069), (16, 0.103), (17, -0.227), (18, 0.077), (19, -0.193), (20, 0.033), (21, -0.02), (22, -0.111), (23, -0.015), (24, -0.085), (25, 0.087), (26, 0.11), (27, -0.003), (28, -0.116), (29, 0.075), (30, -0.026), (31, 0.02), (32, 0.147), (33, -0.064), (34, -0.048), (35, 0.228), (36, -0.175), (37, -0.18), (38, 0.064), (39, -0.016), (40, -0.026), (41, -0.215), (42, 0.025), (43, -0.115), (44, 0.202), (45, 0.187), (46, 0.059), (47, 0.095), (48, 0.118), (49, -0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98312116 <a title="57-lsi-1" href="./jmlr-2010-Iterative_Scaling_and_Coordinate_Descent_Methods_for_Maximum_Entropy_Models.html">57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</a></p>
<p>Author: Fang-Lan Huang, Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin</p><p>Abstract: Maximum entropy (Maxent) is useful in natural language processing and many other areas. Iterative scaling (IS) methods are one of the most popular approaches to solve Maxent. With many variants of IS methods, it is difﬁcult to understand them and see the differences. In this paper, we create a general and uniﬁed framework for iterative scaling methods. This framework also connects iterative scaling and coordinate descent methods. We prove general convergence results for IS methods and analyze their computational complexity. Based on the proposed framework, we extend a coordinate descent method for linear SVM to Maxent. Results show that it is faster than existing iterative scaling methods. Keywords: maximum entropy, iterative scaling, coordinate descent, natural language processing, optimization</p><p>2 0.58885705 <a title="57-lsi-2" href="./jmlr-2010-A_Comparison_of_Optimization_Methods_and_Software_for_Large-scale_L1-regularized_Linear_Classification.html">1 jmlr-2010-A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</a></p>
<p>Author: Guo-Xun Yuan, Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Large-scale linear classiﬁcation is widely used in many areas. The L1-regularized form can be applied for feature selection; however, its non-differentiability causes more difﬁculties in training. Although various optimization methods have been proposed in recent years, these have not yet been compared suitably. In this paper, we ﬁrst broadly review existing methods. Then, we discuss state-of-the-art software packages in detail and propose two efﬁcient implementations. Extensive comparisons indicate that carefully implemented coordinate descent methods are very suitable for training large document data. Keywords: L1 regularization, linear classiﬁcation, optimization methods, logistic regression, support vector machines, document classiﬁcation</p><p>3 0.26715958 <a title="57-lsi-3" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>Author: Jean-Yves Audibert, Sébastien Bubeck</p><p>Abstract: This work deals with four classical prediction settings, namely full information, bandit, label efﬁcient and bandit label efﬁcient as well as four different notions of regret: pseudo-regret, expected regret, high probability regret and tracking the best expert regret. We introduce a new forecaster, INF (Implicitly Normalized Forecaster) based on an arbitrary function ψ for which we propose a uniﬁed γ analysis of its pseudo-regret in the four games we consider. In particular, for ψ(x) = exp(ηx) + K , INF reduces to the classical exponentially weighted average forecaster and our analysis of the pseudo-regret recovers known results while for the expected regret we slightly tighten the bounds. γ η q On the other hand with ψ(x) = −x + K , which deﬁnes a new forecaster, we are able to remove the extraneous logarithmic factor in the pseudo-regret bounds for bandits games, and thus ﬁll in a long open gap in the characterization of the minimax rate for the pseudo-regret in the bandit game. We also provide high probability bounds depending on the cumulative reward of the optimal action. Finally, we consider the stochastic bandit game, and prove that an appropriate modiﬁcation of the upper conﬁdence bound policy UCB1 (Auer et al., 2002a) achieves the distribution-free optimal rate while still having a distribution-dependent rate logarithmic in the number of plays. Keywords: Bandits (adversarial and stochastic), regret bound, minimax rate, label efﬁcient, upper conﬁdence bound (UCB) policy, online learning, prediction with limited feedback.</p><p>4 0.26331621 <a title="57-lsi-4" href="./jmlr-2010-A_Fast_Hybrid_Algorithm_for_Large-Scalel1-Regularized_Logistic_Regression.html">3 jmlr-2010-A Fast Hybrid Algorithm for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>Author: Jianing Shi, Wotao Yin, Stanley Osher, Paul Sajda</p><p>Abstract: ℓ1 -regularized logistic regression, also known as sparse logistic regression, is widely used in machine learning, computer vision, data mining, bioinformatics and neural signal processing. The use of ℓ1 regularization attributes attractive properties to the classiﬁer, such as feature selection, robustness to noise, and as a result, classiﬁer generality in the context of supervised learning. When a sparse logistic regression problem has large-scale data in high dimensions, it is computationally expensive to minimize the non-differentiable ℓ1 -norm in the objective function. Motivated by recent work (Koh et al., 2007; Hale et al., 2008), we propose a novel hybrid algorithm based on combining two types of optimization iterations: one being very fast and memory friendly while the other being slower but more accurate. Called hybrid iterative shrinkage (HIS), the resulting algorithm is comprised of a ﬁxed point continuation phase and an interior point phase. The ﬁrst phase is based completely on memory efﬁcient operations such as matrix-vector multiplications, while the second phase is based on a truncated Newton’s method. Furthermore, we show that various optimization techniques, including line search and continuation, can signiﬁcantly accelerate convergence. The algorithm has global convergence at a geometric rate (a Q-linear rate in optimization terminology). We present a numerical comparison with several existing algorithms, including an analysis using benchmark data from the UCI machine learning repository, and show our algorithm is the most computationally efﬁcient without loss of accuracy. Keywords: logistic regression, ℓ1 regularization, ﬁxed point continuation, supervised learning, large scale c 2010 Jianing Shi, Wotao Yin, Stanley Osher and Paul Sajda. S HI , Y IN , O SHER AND S AJDA</p><p>5 0.23832126 <a title="57-lsi-5" href="./jmlr-2010-Erratum%3A_SGDQN_is_Less_Careful_than_Expected.html">34 jmlr-2010-Erratum: SGDQN is Less Careful than Expected</a></p>
<p>Author: Antoine Bordes, Léon Bottou, Patrick Gallinari, Jonathan Chang, S. Alex Smith</p><p>Abstract: The SGD-QN algorithm described in Bordes et al. (2009) contains a subtle ﬂaw that prevents it from reaching its design goals. Yet the ﬂawed SGD-QN algorithm has worked well enough to be a winner of the ﬁrst Pascal Large Scale Learning Challenge (Sonnenburg et al., 2008). This document clariﬁes the situation, proposes a corrected algorithm, and evaluates its performance. Keywords: stochastic gradient descent, support vector machine, conditional random ﬁelds</p><p>6 0.23689295 <a title="57-lsi-6" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>7 0.23159239 <a title="57-lsi-7" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>8 0.20856047 <a title="57-lsi-8" href="./jmlr-2010-A_Generalized_Path_Integral_Control_Approach_to_Reinforcement_Learning.html">4 jmlr-2010-A Generalized Path Integral Control Approach to Reinforcement Learning</a></p>
<p>9 0.19032347 <a title="57-lsi-9" href="./jmlr-2010-Dual_Averaging_Methods_for_Regularized_Stochastic_Learning_and_Online_Optimization.html">31 jmlr-2010-Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a></p>
<p>10 0.16156346 <a title="57-lsi-10" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>11 0.1556524 <a title="57-lsi-11" href="./jmlr-2010-Image_Denoising_with_Kernels_Based_on_Natural_Image_Relations.html">50 jmlr-2010-Image Denoising with Kernels Based on Natural Image Relations</a></p>
<p>12 0.15145402 <a title="57-lsi-12" href="./jmlr-2010-The_SHOGUN_Machine_Learning_Toolbox.html">110 jmlr-2010-The SHOGUN Machine Learning Toolbox</a></p>
<p>13 0.14877404 <a title="57-lsi-13" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>14 0.13108359 <a title="57-lsi-14" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>15 0.12096357 <a title="57-lsi-15" href="./jmlr-2010-Chromatic_PAC-Bayes_Bounds_for_Non-IID_Data%3A_Applications_to_Ranking_and_Stationary_%CE%B2-Mixing_Processes.html">20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</a></p>
<p>16 0.11686262 <a title="57-lsi-16" href="./jmlr-2010-Optimal_Search_on_Clustered_Structural_Constraint_for_Learning_Bayesian_Network_Structure.html">88 jmlr-2010-Optimal Search on Clustered Structural Constraint for Learning Bayesian Network Structure</a></p>
<p>17 0.11066931 <a title="57-lsi-17" href="./jmlr-2010-Stability_Bounds_for_Stationary_%CF%86-mixing_and_%CE%B2-mixing_Processes.html">106 jmlr-2010-Stability Bounds for Stationary φ-mixing and β-mixing Processes</a></p>
<p>18 0.10949098 <a title="57-lsi-18" href="./jmlr-2010-Tree_Decomposition_for_Large-Scale_SVM_Problems.html">113 jmlr-2010-Tree Decomposition for Large-Scale SVM Problems</a></p>
<p>19 0.10576486 <a title="57-lsi-19" href="./jmlr-2010-Efficient_Algorithms_for_Conditional_Independence_Inference.html">32 jmlr-2010-Efficient Algorithms for Conditional Independence Inference</a></p>
<p>20 0.10092471 <a title="57-lsi-20" href="./jmlr-2010-A_Rotation_Test_to_Verify_Latent_Structure.html">6 jmlr-2010-A Rotation Test to Verify Latent Structure</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.013), (8, 0.011), (21, 0.057), (32, 0.03), (36, 0.012), (37, 0.516), (46, 0.025), (75, 0.124), (85, 0.039), (96, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98315609 <a title="57-lda-1" href="./jmlr-2010-Gaussian_Processes_for_Machine_Learning_%28GPML%29_Toolbox.html">41 jmlr-2010-Gaussian Processes for Machine Learning (GPML) Toolbox</a></p>
<p>Author: Carl Edward Rasmussen, Hannes Nickisch</p><p>Abstract: The GPML toolbox provides a wide range of functionality for Gaussian process (GP) inference and prediction. GPs are speciﬁed by mean and covariance functions; we offer a library of simple mean and covariance functions and mechanisms to compose more complex ones. Several likelihood functions are supported including Gaussian and heavy-tailed for regression as well as others suitable for classiﬁcation. Finally, a range of inference methods is provided, including exact and variational inference, Expectation Propagation, and Laplace’s method dealing with non-Gaussian likelihoods and FITC for dealing with large regression tasks. Keywords: Gaussian processes, nonparametric Bayes, probabilistic regression and classiﬁcation Gaussian processes (GPs) (Rasmussen and Williams, 2006) have convenient properties for many modelling tasks in machine learning and statistics. They can be used to specify distributions over functions without having to commit to a speciﬁc functional form. Applications range from regression over classiﬁcation to reinforcement learning, spatial models, survival and other time series1 models. Predictions of GP models come with a natural conﬁdence measure: predictive error-bars. Although the implementation of the basic principles in the simplest case is straight forward, various complicating features are often desired in practice. For example, a GP is determined by a mean function and a covariance function, but these functions are mostly difﬁcult to specify fully a priori, and typically they are given in terms of hyperparameters, that is, parameters which have to be inferred. Another source of difﬁculty is the likelihood function. For Gaussian likelihoods, inference is analytically tractable; however, in many tasks, Gaussian likelihoods are not appropriate, and approximate inference methods such as Expectation Propagation (EP) (Minka, 2001), Laplace’s approximation (LA) (Williams and Barber, 1998) and variational bounds (VB) (Gibbs and MacKay, 2000) become necessary (Nickisch and Rasmussen, 2008). In case of large training data, approximations (Candela and Rasmussen, 2005) like FITC (Snelson and Ghahramani, 2006) are needed. The GPML toolbox is designed to overcome these hurdles with its variety of mean, covariance and likelihood functions as well as inference methods, while being simple to use and easy to extend. ∗. Also at Max Planck Institute for Biological Cybernetics, Spemannstraße 38, 72076 T¨ bingen, Germany. u 1. Note, that here we typically think of GPs with a more general index set than time. ©2010 Carl Edward Rasmussen and Hannes Nickisch. R ASMUSSEN AND N ICKISCH 1. Implementation The GPML toolbox can be obtained from http://gaussianprocess.org/gpml/code/matlab/ and also http://mloss.org/software/view/263/ under the FreeBSD license. Based on simple interfaces for covariance, mean, likelihood functions as well as inference methods, we offer full compatibility to both Matlab 7.x2 and GNU Octave 3.2.x.3 Special attention has been given to properly disentangle covariance, likelihood and mean hyperparameters. Also, care has been taken to avoid numerical inaccuracies, for example, safe likelihood evaluations for extreme inputs and stable matrix operations. For example, the covariance matrix K can become numerically close to singular making its naive inversion numerically unsafe. We handle these situations in a principled way4 such that Cholesky decompositions are computed of well-conditioned matrices only. As a result, our code shows a high level of robustness along the full spectrum of possible hyperparameters. The focus of the toolbox is on approximate inference using dense matrix algebra. We currently do not support covariance matrix approximation techniques to deal with large numbers of training examples n. Looking at the (growing) body of literature on sparse approximations, this knowledge is still somewhat in ﬂux, and consensus on the best approaches has not yet been reached. We provide stable and modular code checked by an exhaustive suite of test cases. A single function gp.m serves as main interface to the user—it can make inference and predictions and allows the mean, covariance and likelihood function as well as the inference methods to be speciﬁed freely. Furthermore, gp.m enables convenient learning of the hyperparameters by maximising the log marginal likelihood ln Z. One of the particularly appealing properties of GP models is that principled and practical approaches exist for learning the parameters of mean, covariance and likelihood functions. Good adaptation of such parameters can be essential to obtain both high quality predictions and insights into the properties of the data. The GPML toolbox is particularly ﬂexible, including a large library of different covariance and mean functions, and ﬂexible ways to combine these into more expressive, specialised functions. The user can choose between two gradient-based optimisers: one uses conjugate gradients (CG)5 and the other one relies on a quasi-Newton scheme.6 ∂ Computing the derivatives w.r.t. hyperparameters ∂θi ln Z with gp.m does not need any extra programming effort; every inference method automatically collects the respective derivatives from the mean, covariance and likelihood functions and passes them to gp.m. Our documentation comes in two pieces: a hypertext user documentation7 doc/index.html with examples and code browsing and a technical documentation8 doc/manual.pdf focusing on the interfaces and more technical issues. A casual user will use the hypertext document to quickly get his data analysed, however a power user will consult the pdf document once he wants to include his own mean, covariance, likelihood and inference routines or learn about implementation details. 2. 3. 4. 5. 6. 7. 8. Matlab is available from MathWorks, http://www.mathworks.com/. Octave is available from the Free Software Foundation, http://www.gnu.org/software/octave/. We do not consider the “blind” addition of a “small ridge” to K a principled way. Carl Rasmussen’s code is available at http://www.kyb.tuebingen.mpg.de/bs/people/carl/code/minimize/. Peter Carbonetto’s wrapper can be found at http://www.cs.ubc.ca/˜pcarbo/lbfgsb-for-matlab.html. Documentation can be found at http://www.gaussianprocess.org/gpml/code/matlab/doc/index.html. Technical docs are available at http://www.gaussianprocess.org/gpml/code/matlab/doc/manual.pdf. 3012 G AUSSIAN P ROCESSES FOR M ACHINE L EARNING T OOLBOX 2. The GPML Toolbox We illustrate the modular structure of the GPML toolbox by means of a simple code example. GPs are used to formalise and update knowledge about distributions over functions. A GP prior distribution on an unknown latent function f ∼ GP (mφ (x), kψ (x, x′ )), consists of a mean function m(x) = E[ f (x)], and a covariance function k(x, x) = E[( f (x) − m(x))( f (x′ ) − m(x′ ))], both of which typically contain hyperparameters φ and ψ, which we want to ﬁt in the light of data. We generally assume independent observations, that is, input/output pairs (xi , yi ) of f with joint likelihood Pρ (y|f) = ∏n Pρ (yi | f (xi )) factorising over cases. Finally, after speciﬁcation of the prior and i=1 ﬁtting of the hyperparameters θ = {φ, ψ, ρ}, we wish to compute predictive distributions for test cases. 1 2 3 4 5 6 7 % 1) SET UP THE GP : COVARIANCE ; MEAN , LIKELIHOOD , INFERENCE METHOD mf = { ’ meanSum ’ ,{ ’ meanLinear ’, @meanConst }}; a = 2; b = 1; % m(x) = a*x+b cf = { ’ covSEiso ’}; sf = 1; ell = 0.7; % squared exponential covariance funct lf = ’ likLaplace ’; sn = 0.2; % assume Laplace noise with variance sn ˆ2 hyp0 . mean = [a;b ]; hyp0 . cov = log([ ell ; sf ]); hyp0 . lik = log( sn ); % hypers inf = ’ infEP ’; % specify expectation propagation as inference method % 2) MINIMISE NEGATIVE LOG MARGINAL LIKELIHOOD nlZ wrt . hyp ; do 50 CG steps Ncg = 50; [hyp , nlZ ] = minimize ( hyp0 , ’gp ’, -Ncg , inf , mf , cf , lf , X , y ); % 3) PREDICT AT UNKNOWN TEST INPUTS [ymu , ys2 ] = gp (hyp , inf , mf , cf , lf , X , y , Xs ); % test input Xs In line 1, we specify the mean mφ (x) = a⊤ x + b of the GP with hyperparameters φ = {a, b}. First, the functional form of the mean function is given and its parameters are initialised. The desired mean function, happens not to exist in the library of mean functions; instead we have to make a composite mean function from simple constituents. This is done using a nested cell array containing the algebraic expression for m(x): As the sum of a linear (mean/meanLinear.m) and a constant mean function (mean/meanConst.m) it is an afﬁne function. In addition to linear and constant mean functions, the toolbox offers m(x) = 0 and m(x) = 1. These simple mean functions can be combined by composite mean functions to obtain sums (mean/meanSum.m) m(x) = ∑ j m j (x), products m(x) = ∏ j m j (x), scaled versions m(x) = αm0 (x) and powers m(x) = m0 (x)d . This ﬂexible mechanism is used for convenient speciﬁcation of an extensible algebra of mean functions. Note that functions are referred to either as name strings ’meanConst’ or alternatively function handles @meanConst. The order of components of the hyperparameters φ is the same as in the speciﬁcation of the cell array. Every mean function implements its evaluation m = mφ (X) and ﬁrst derivative ∂ computation mi = ∂φi mφ (X) on a data set X. In the same spirit, the squared exponential covariance kψ (x, x′ ) = σ f ² exp(− x − x′ 2 /2ℓ2 ) (cov/covSEiso.m) with hyperparameters ψ = {ln ℓ, ln σ f } is set up in line 2. Note, that the hyperparameters are represented by the logarithms, as these parameters are naturally positive. Many other simple covariance functions are contained in the toolbox. Among others, we offer linear, constant, Mat´ rn, rational quadratic, polynomial, periodic, neural network and ﬁnite support coe variance functions. Composite covariance functions allow for sums k(x, x′ ) = ∑ j k j (x, x′ ), products k(x, x′ ) = ∏ j k j (x, x′ ), positive scaling k(x, x′ ) = σ2 k0 (x, x′ ) and masking of components f k(x, x′ ) = k0 (xI , x′ ) with I ⊆ [1, 2, .., D], x ∈ RD . Again, the interface is simple since only the I ∂ evaluation of the covariance matrix K = kψ (X) and its derivatives ∂i K = ∂ψi kψ (X) on a data set X are required. Furthermore, we need cross terms k∗ = kψ (X, x∗ ) and k∗∗ = kψ (x∗ , x∗ ) for prediction. There are no restrictions on the composition of both mean and covariance functions—any combination is allowed including nested composition. 3013 R ASMUSSEN AND N ICKISCH √ √ The Laplace (lik/likLaplace.m) likelihood Pρ (y| f ) = exp(− 2/σn |y − f |)/ 2σn with hyperparameters ρ = {ln σn } is speciﬁed in line 3. There are only simple likelihood functions: Gaussian, Sech-squared, Laplacian and Student’s t for ordinary and sparse regression as well as the error and the logistic function for classiﬁcation. Again, the same inference code is used for any likelihood function. Although the speciﬁcation of likelihood functions is simple for the user, writing new likelihood functions is slightly more involved as different inference methods require access to different properties; for example, LA requires second derivatives and EP requires derivatives of moments. All hyperparameters θ = {φ, ψ, ρ} are stored in a struct hyp.{mean,cov,lik}, which is initialised in line 4; we select the approximate inference algorithm EP (inf/infEP.m) in line 5. We optimise the hyperparameters θ ≡ hyp by calling the CG optimiser (util/minimize.m) with initial value θ0 ≡ hyp0 in line 6 allowing at most N = 50 evaluations of the EP approximation to the marginal likelihood ZEP (θ) as done by gp.m. Here, D = (X, y) ≡ (X,y) is the training data where X = {x1 , .., xn } and y ∈ Rn . Under the hood, gp.m computes in every step a Gaussian ∂ posterior approximation and the derivatives ∂θ ln ZEP (θ) of the marginal likelihood by calling EP. Predictions with optimised hyperparameters are done in line 7, where we call gp.m with the unseen test inputs X∗ ≡ Xs as additional argument. As a result, we obtain the approximate marginal predictive mean E[P(y∗ |D , X∗ )] ≡ ymu and the predictive variance V[P(y∗ |D , X∗ )] ≡ ys2. Likelihood \ Inference Gaussian Sech-squared Laplacian Student’s t Error function Logistic function Exact FITC EP Laplace VB Type, Output Domain regression, R regression, R regression, R regression, R classiﬁcation, {±1} classiﬁcation, {±1} Alternate Name logistic distribution double exponential probit regression logit regression Table 1: Likelihood ↔ inference compatibility in the GPML toolbox Table 1 gives the legal likelihood/inference combinations. Exact inference and the FITC approximation support the Gaussian likelihood only. Variational Bayesian (VB) inference is applicable to all likelihoods. Expectation propagation (EP) for the Student’s t likelihood is inherently unstable due to its non-log-concavity. The Laplace approximation (LA) for Laplace likelihoods is not sensible due to the non-differentiable peak of the Laplace likelihood. Special care has been taken for the non-convex optimisation problem imposed by the combination Student’s t likelihood and LA. If the number of training examples is larger than a few thousand, dense matrix computations become too slow. We provide the FITC approximation for regression with Gaussian likelihood where ˜ instead of the exact covariance matrix K, a low-rank plus diagonal matrix K = Q + diag(K − Q) ⊤ K−1 K is used. The matrices K and K contain covariances and cross-covariances where Q = Ku uu u uu u of and between inducing inputs ui and data points x j . Using inf/infFITC.m together with any covariance function wrapped into cov/covFITC.m makes the computations feasible for large n. Acknowledgments Thanks to Ed Snelson for assisting with the FITC approximation. 3014 G AUSSIAN P ROCESSES FOR M ACHINE L EARNING T OOLBOX References Joaquin Qui˜ onero Candela and Carl E. Rasmussen. A unifying view of sparse approximate Gausn sian process regression. Journal of Machine Learning Research, 6(6):1935–1959, 2005. Mark N. Gibbs and David J. C. MacKay. Variational Gaussian process classiﬁers. IEEE Transactions on Neural Networks, 11(6):1458–1464, 2000. Thomas P. Minka. Expectation propagation for approximate Bayesian inference. In UAI, pages 362–369. Morgan Kaufmann, 2001. Hannes Nickisch and Carl E. Rasmussen. Approximations for binary Gaussian process classiﬁcation. Journal of Machine Learning Research, 9:2035–2078, 10 2008. Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, Cambridge, MA, 2006. Ed Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems 18, 2006. Christopher K. I. Williams and D. Barber. Bayesian classiﬁcation with Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(20):1342–1351, 1998. 3015</p><p>2 0.92956913 <a title="57-lda-2" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>Author: Garvesh Raskutti, Martin J. Wainwright, Bin Yu</p><p>Abstract: Methods based on ℓ1 -relaxation, such as basis pursuit and the Lasso, are very popular for sparse regression in high dimensions. The conditions for success of these methods are now well-understood: (1) exact recovery in the noiseless setting is possible if and only if the design matrix X satisﬁes the restricted nullspace property, and (2) the squared ℓ2 -error of a Lasso estimate decays at the minimax optimal rate k log p , where k is the sparsity of the p-dimensional regression problem with additive n Gaussian noise, whenever the design satisﬁes a restricted eigenvalue condition. The key issue is thus to determine when the design matrix X satisﬁes these desirable properties. Thus far, there have been numerous results showing that the restricted isometry property, which implies both the restricted nullspace and eigenvalue conditions, is satisﬁed when all entries of X are independent and identically distributed (i.i.d.), or the rows are unitary. This paper proves directly that the restricted nullspace and eigenvalue conditions hold with high probability for quite general classes of Gaussian matrices for which the predictors may be highly dependent, and hence restricted isometry conditions can be violated with high probability. In this way, our results extend the attractive theoretical guarantees on ℓ1 -relaxations to a much broader class of problems than the case of completely independent or unitary designs. Keywords: Lasso, basis pursuit, random matrix theory, Gaussian comparison inequality, concentration of measure</p><p>3 0.87447125 <a title="57-lda-3" href="./jmlr-2010-The_SHOGUN_Machine_Learning_Toolbox.html">110 jmlr-2010-The SHOGUN Machine Learning Toolbox</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Sebastian Henschel, Christian Widmer, Jonas Behr, Alexander Zien, Fabio de Bona, Alexander Binder, Christian Gehl, Vojtěch Franc</p><p>Abstract: We have developed a machine learning toolbox, called SHOGUN, which is designed for uniﬁed large-scale learning for a broad range of feature types and learning settings. It offers a considerable number of machine learning models such as support vector machines, hidden Markov models, multiple kernel learning, linear discriminant analysis, and more. Most of the speciﬁc algorithms are able to deal with several different data classes. We have used this toolbox in several applications from computational biology, some of them coming with no less than 50 million training examples and others with 7 billion test examples. With more than a thousand installations worldwide, SHOGUN is already widely adopted in the machine learning community and beyond. SHOGUN is , implemented in C++ and interfaces to MATLABTM R, Octave, Python, and has a stand-alone command line interface. The source code is freely available under the GNU General Public License, Version 3 at http://www.shogun-toolbox.org. Keywords: support vector machines, kernels, large-scale learning, Python, Octave, R</p><p>same-paper 4 0.84380203 <a title="57-lda-4" href="./jmlr-2010-Iterative_Scaling_and_Coordinate_Descent_Methods_for_Maximum_Entropy_Models.html">57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</a></p>
<p>Author: Fang-Lan Huang, Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin</p><p>Abstract: Maximum entropy (Maxent) is useful in natural language processing and many other areas. Iterative scaling (IS) methods are one of the most popular approaches to solve Maxent. With many variants of IS methods, it is difﬁcult to understand them and see the differences. In this paper, we create a general and uniﬁed framework for iterative scaling methods. This framework also connects iterative scaling and coordinate descent methods. We prove general convergence results for IS methods and analyze their computational complexity. Based on the proposed framework, we extend a coordinate descent method for linear SVM to Maxent. Results show that it is faster than existing iterative scaling methods. Keywords: maximum entropy, iterative scaling, coordinate descent, natural language processing, optimization</p><p>5 0.84284848 <a title="57-lda-5" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>Author: Gideon S. Mann, Andrew McCallum</p><p>Abstract: In this paper, we present an overview of generalized expectation criteria (GE), a simple, robust, scalable method for semi-supervised training using weakly-labeled data. GE ﬁts model parameters by favoring models that match certain expectation constraints, such as marginal label distributions, on the unlabeled data. This paper shows how to apply generalized expectation criteria to two classes of parametric models: maximum entropy models and conditional random ﬁelds. Experimental results demonstrate accuracy improvements over supervised training and a number of other stateof-the-art semi-supervised learning methods for these models. Keywords: generalized expectation criteria, semi-supervised learning, logistic regression, conditional random ﬁelds</p><p>6 0.66344196 <a title="57-lda-6" href="./jmlr-2010-Sparse_Spectrum_Gaussian_Process_Regression.html">104 jmlr-2010-Sparse Spectrum Gaussian Process Regression</a></p>
<p>7 0.63902324 <a title="57-lda-7" href="./jmlr-2010-libDAI%3A_A_Free_and_Open_Source_C%2B%2B_Library_for_Discrete_Approximate_Inference_in_Graphical_Models.html">118 jmlr-2010-libDAI: A Free and Open Source C++ Library for Discrete Approximate Inference in Graphical Models</a></p>
<p>8 0.5584296 <a title="57-lda-8" href="./jmlr-2010-WEKA%E2%88%92Experiences_with_a_Java_Open-Source_Project.html">116 jmlr-2010-WEKA−Experiences with a Java Open-Source Project</a></p>
<p>9 0.53339124 <a title="57-lda-9" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>10 0.53182864 <a title="57-lda-10" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>11 0.51747847 <a title="57-lda-11" href="./jmlr-2010-Rate_Minimaxity_of_the_Lasso_and_Dantzig_Selector_for_thelqLoss_inlrBalls.html">96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</a></p>
<p>12 0.51490897 <a title="57-lda-12" href="./jmlr-2010-Collective_Inference_for__Extraction_MRFs_Coupled_with_Symmetric_Clique_Potentials.html">24 jmlr-2010-Collective Inference for  Extraction MRFs Coupled with Symmetric Clique Potentials</a></p>
<p>13 0.48925033 <a title="57-lda-13" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>14 0.48416856 <a title="57-lda-14" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>15 0.48141378 <a title="57-lda-15" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>16 0.46869519 <a title="57-lda-16" href="./jmlr-2010-Approximate_Tree_Kernels.html">15 jmlr-2010-Approximate Tree Kernels</a></p>
<p>17 0.46773922 <a title="57-lda-17" href="./jmlr-2010-SFO%3A_A_Toolbox_for_Submodular_Function_Optimization.html">100 jmlr-2010-SFO: A Toolbox for Submodular Function Optimization</a></p>
<p>18 0.45827055 <a title="57-lda-18" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>19 0.45667729 <a title="57-lda-19" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>20 0.4565942 <a title="57-lda-20" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
