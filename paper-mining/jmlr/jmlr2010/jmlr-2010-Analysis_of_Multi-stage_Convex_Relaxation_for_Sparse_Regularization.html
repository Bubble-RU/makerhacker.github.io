<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-12" href="#">jmlr2010-12</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</h1>
<br/><p>Source: <a title="jmlr-2010-12-pdf" href="http://jmlr.org/papers/volume11/zhang10a/zhang10a.pdf">pdf</a></p><p>Author: Tong Zhang</p><p>Abstract: We consider learning formulations with non-convex objective functions that often occur in practical applications. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to a sub-optimal solution in reality. This paper tries to remedy the above gap between theory and practice. In particular, we present a multi-stage convex relaxation scheme for solving problems with non-convex objective functions. For learning formulations with sparse regularization, we analyze the behavior of a speciﬁc multistage relaxation scheme. Under appropriate conditions, we show that the local solution obtained by this procedure is superior to the global solution of the standard L1 convex relaxation for learning sparse targets. Keywords: sparsity, non-convex optimization, convex relaxation, multi-stage convex relaxation</p><p>Reference: <a title="jmlr-2010-12-reference" href="../jmlr2010_reference/jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 • Convex relaxation such as L1 -regularization that solves the problem under some conditions. [sent-6, score-0.498]
</p><p>2 In particular, we present a multi-stage convex relaxation scheme for solving problems with non-convex objective functions. [sent-9, score-0.684]
</p><p>3 For learning formulations with sparse regularization, we analyze the behavior of a speciﬁc multistage relaxation scheme. [sent-10, score-0.67]
</p><p>4 Under appropriate conditions, we show that the local solution obtained by this procedure is superior to the global solution of the standard L1 convex relaxation for learning sparse targets. [sent-11, score-0.991]
</p><p>5 Keywords: sparsity, non-convex optimization, convex relaxation, multi-stage convex relaxation  1. [sent-12, score-0.895]
</p><p>6 Introduction We consider the general regularization framework for machine learning, where a loss function is minimized, subject to a regularization condition on the model parameter. [sent-13, score-0.538]
</p><p>7 A major difﬁculty with nonconvex formulations is that the global optimal solution cannot be efﬁciently computed, and the behavior of a local solution is hard to analyze. [sent-16, score-0.457]
</p><p>8 In practice, convex relaxation (such as support vector machine for classiﬁcation or L1 regularization for sparse learning) has been adopted to remedy the problem. [sent-17, score-1.044]
</p><p>9 However, for many practical problems, such simple convex relaxation schemes can be sub-optimal. [sent-21, score-0.684]
</p><p>10 Our goal is to design a numerical procedure that leads to a reproducible solution which is better than the standard convex relaxation solution. [sent-26, score-0.778]
</p><p>11 In order to achieve this, we present a general framework of multi-stage convex relaxation, which iteratively reﬁne the convex relaxation formulation to give better solutions. [sent-27, score-0.973]
</p><p>12 The method is derived from concave duality, and involves solving a sequence of convex relaxation problems, leading to better and better approximations to the original nonconvex formulation. [sent-28, score-1.003]
</p><p>13 Since each stage of multi-stage convex relaxation is a convex optimization problem, the approach is also computationally efﬁcient. [sent-31, score-0.947]
</p><p>14 Although the method only leads to a local optimal solution for the original nonconvex problem, this local solution is a reﬁnement of the global solution for the initial convex relaxation. [sent-32, score-0.676]
</p><p>15 In order to prove this observation more rigorously, we consider least squares regression with nonconvex sparse regularization terms, for which we can analyze the effectiveness of the multi-stage convex relaxation. [sent-34, score-0.8]
</p><p>16 It is shown that under appropriate assumptions, the (local) solution computed by the multi-stage convex relaxation method using nonconvex regularization achieves better parameter estimation performance than the standard convex relaxation with L1 regularization. [sent-35, score-1.904]
</p><p>17 This demonstrates the effectiveness of multi-stage convex relaxation for a speciﬁc but important problem. [sent-37, score-0.718]
</p><p>18 Although without theoretical analysis, we shall also present the general idea of multi-stage convex relaxation in Section 2, because it can be applied to other potential application examples as illustrated in Appendix C. [sent-38, score-0.718]
</p><p>19 Multi-stage Convex Relaxation This section presents the general idea of multi-stage convex relaxation which can be applied to various optimization problems. [sent-45, score-0.684]
</p><p>20 1 Regularized Learning Formulation The multi-stage convex relaxation approach considered in the paper can be applied to the following optimization problem, which can be motivated from supervised learning formulation. [sent-48, score-0.684]
</p><p>21 Under this assumption, we can rewrite the regularization function Rk (w) as: v⊤ hk (w) − R∗ (vk ) k k  Rk (w) = inf  vk ∈Rdk  (3)  ¯ using concave duality (Rockafellar, 1970). [sent-64, score-0.801]
</p><p>22 Given an appropriate vector vk ∈ Rdk , a simple convex relaxation of (1) becomes K  ˆ w = arg min R0 (w) + ∑ hk (w)⊤ vk . [sent-73, score-1.545]
</p><p>23 w∈Rd  (5)  k=1  This simple relaxation yields a solution that is different from the solution of (1). [sent-74, score-0.567]
</p><p>24 If we can ﬁnd a good approximation of v = {ˆ k } that v ˆ improves upon the initial value of vk = 1, then the above formulation can lead to a reﬁned convex problem in w that is a better convex relaxation than (5). [sent-79, score-1.256]
</p><p>25 Our numerical procedure exploits the above fact, which tries to improve the estimation of vk over the initial choice of vk = 1 in (5) using an iterative algorithm. [sent-80, score-0.591]
</p><p>26 ˆ Initialize v = 1 Repeat the following two steps until convergence: • Let  K  ˆ ˆ w = arg min R0 (w) + ∑ hk (w)⊤ vk . [sent-83, score-0.578]
</p><p>27 However, in order to apply MM, for each particular choice of h, one has to demonstrate that the convex relaxation is indeed an upper bound, which is 1084  M ULTISTAGE C ONVEX R ELAXATION  necessary to show convergence. [sent-92, score-0.684]
</p><p>28 In the concave relaxation formulation adopted in this work, the justiﬁcation of convergence is automatically embedded in (6), which becomes a joint optimization problem. [sent-93, score-0.634]
</p><p>29 Note that by repeatedly reﬁning the parameter v, we can potentially obtain better and better convex relaxation in Figure 1, leading to a solution superior to that of the initial convex relaxation. [sent-99, score-0.976]
</p><p>30 In order to demonstrate the effectiveness of multi-stage convex relaxation, we shall include a more careful analysis for the special case of sparse regularization in Section 3. [sent-103, score-0.598]
</p><p>31 Our theory shows that the local solution of multi-stage relaxation with a nonconvex sparse regularizer is superior to the convex L1 regularization solution (under appropriate conditions). [sent-105, score-1.437]
</p><p>32 4 Constrained Formulation The multi-stage convex relaxation idea can also be used to solve the constrained formulation (2). [sent-107, score-0.762]
</p><p>33 The one-stage convex relaxation of (2), given ﬁxed relaxation parameter vk , becomes K  w∈Rd  K  k=1  ˆ w = arg min R0 (w) subject to  k=1  ∑ hk (w)⊤ vk ≤ A + ∑ R∗ (vk ). [sent-108, score-2.018]
</p><p>34 This means that by optimizing v in addition to w, we obtain the following algorithm: ˆ • Initialize v = 1 • Repeat the following two steps until convergence: – Let K  w  K  k=1  ˆ w = arg min R0 (w) subject to  k=1  ˆ ∑ hk (w)⊤ vk ≤ A + ∑ R∗ (ˆ k ). [sent-110, score-0.578]
</p><p>35 , K) ˆ If an optimization problem includes both nonconvex penalization and nonconvex constrains, then one may use the above algorithm with Figure 1. [sent-114, score-0.472]
</p><p>36 Multi-stage Convex Relaxation for Sparse Regularization The multi-stage convex relaxation method described in the previous section tries to obtain better approximations of the original nonconvex problem by reﬁning the convex relaxation formulation. [sent-116, score-1.604]
</p><p>37 Since the local solution found by the algorithm is the global solution of a reﬁned convex relaxation formulation, it should be closer to the desired solution than that of the standard one-stage convex relaxation method. [sent-117, score-1.6]
</p><p>38 This is because this problem has been well-studied in recent years, and the behavior of convex relaxation (L1 regularization) is well-understood. [sent-121, score-0.684]
</p><p>39 The situation is very different for a convex relaxation formulation such as L1 -regularization (Lasso). [sent-128, score-0.762]
</p><p>40 For example, it is known from the compressed sensing literature that under certain conditions, the solution of L1 relaxation may be equivalent to L0 regularization asymptotically (e. [sent-132, score-0.751]
</p><p>41 In the following, we formally prove a result for multi-stage convex relaxation with non-convex sparse regularization that is superior to the Lasso result. [sent-142, score-1.037]
</p><p>42 In essence, we establish a performance guarantee for non-convex formulations when they are solved by using the multistage convex relaxation approach which is more sophisticated than the standard one-stage convex relaxation. [sent-143, score-1.026]
</p><p>43 It follows that (9) can be solved using the multi-stage convex relaxation algorithm in Figure 2, which we will analyze. [sent-168, score-0.684]
</p><p>44 This is the ﬁrst general analysis of multi-stage convex relaxation for high dimensional sparse learning, although some simpler asymptotic results for low dimensional two-stage procedures were obtained in Zou (2006) and Zou and Li (2008), they are not comparable to ours. [sent-210, score-0.772]
</p><p>45 Observe that the multi-stage convex relaxation method only computes a local minimum, (ℓ−1) (ℓ−1) (ℓ−1) ˆ and the regularization update rule is given by λ j = g′ (w j ). [sent-244, score-0.959]
</p><p>46 Since the correct feature #2 shows up in stage 2, we are able to identify it and further improve the convex relaxation in stage 3. [sent-374, score-0.788]
</p><p>47 2 Empirical Study Although this paper focuses on the development of the general multi-stage convex relaxation framework as well as its theoretical understanding (in particular the major result given in Theorem 2), we include two simple numerical examples to verify our theory. [sent-379, score-0.684]
</p><p>48 Figure 4: Performance of multi-stage convex relaxation on simulation data. [sent-399, score-0.684]
</p><p>49 As expected from Theorem 2 and the discussion thereafter, since d becomes large, the multi-stage convex relaxation approach with capped-L1 regularization and L0. [sent-415, score-0.915]
</p><p>50 Figure 5: Performance of multi-stage convex relaxation on the original Boston Housing data. [sent-417, score-0.684]
</p><p>51 1092  M ULTISTAGE C ONVEX R ELAXATION  Figure 6: Performance of multi-stage convex relaxation on the modiﬁed Boston Housing data. [sent-423, score-0.684]
</p><p>52 • Convex relaxation such as L1 -regularization that solves the problem under some conditions. [sent-425, score-0.498]
</p><p>53 In particular, we investigated a multi-stage convex relaxation scheme for solving problems with non-convex objective functions. [sent-428, score-0.684]
</p><p>54 The intuition is to reﬁne convex relaxation iteratively by using solutions obtained from earlier stages. [sent-431, score-0.684]
</p><p>55 This leads to better and better convex relaxation formulations, and thus better and better solutions. [sent-432, score-0.684]
</p><p>56 Although the scheme only ﬁnds a local minimum, the above argument indicates that the local minimum it ﬁnds should be closer to the original nonconvex problem than the standard convex relaxation solution. [sent-433, score-1.03]
</p><p>57 In order to prove the effectiveness of this approach theoretically, we considered the sparse learning problem where the behavior of convex relaxation (Lasso) has been well studied in recent years. [sent-434, score-0.806]
</p><p>58 We showed that under appropriate conditions, the local solution from the multi-stage convex relaxation algorithm is superior to the global solution of the standard L1 convex relaxation for learning sparse targets. [sent-435, score-1.65]
</p><p>59 We shall mention that our theory only shows that nonconvex regularization behaves better than Lasso under appropriate sparse eigenvalue conditions. [sent-437, score-0.646]
</p><p>60 When such conditions hold, multi-stage convex relaxation is superior. [sent-438, score-0.707]
</p><p>61 On the other hand, when such conditions fail, neither Lasso nor (the local solution of) multi-stage convex relaxation can be shown to work well. [sent-439, score-0.798]
</p><p>62 Our empirical experience suggests that when features are highly correlated, convex formulations may perform better than (non-bagged) nonconvex formulations due to the added sta1093  Z HANG  bility. [sent-442, score-0.613]
</p><p>63 However, since our analysis doesn’t yield any insights in this scenario, further investigation is necessary to theoretically compare convex formulations to bagged nonconvex formulations. [sent-443, score-0.552]
</p><p>64 Finally, multi-stage convex relaxation is not the only numerical method that can solve nonconvex formulations with strong theoretical guarantee. [sent-444, score-1.003]
</p><p>65 In practice, for computational reasons, a convex relaxation such as the SVM loss φ(w⊤ x, y) = max(0, 1 − w⊤ xy) 1100  M ULTISTAGE C ONVEX R ELAXATION  is often used to substitute the classiﬁcation error loss. [sent-611, score-0.713]
</p><p>66 Such a convex loss is often referred to as a surrogate loss function, and the resulting method becomes a convex relaxation method for solving binary classiﬁcation. [sent-612, score-0.953]
</p><p>67 2 Regularization Condition Some examples of regularization conditions in (15) include squared regularization g(w) = w⊤ w, and 1-norm regularization g(w) = w 1 . [sent-620, score-0.739]
</p><p>68 Speciﬁcally, L0 regularization is equivalent to (15) by choosing the regularization function as g(w) = w 0 . [sent-634, score-0.462]
</p><p>69 In particular, by choosing the closest approximation with p = 1, one obtain Lasso, which is the standard convex relaxation formulation for sparse learning. [sent-638, score-0.872]
</p><p>70 The traditional approach is to use convex relaxation to approximate it, leading to a single stage convex formulation. [sent-642, score-0.947]
</p><p>71 In this paper, we try to extend the idea by looking at a more general multi-stage convex relaxation method, which leads to more accurate approximations. [sent-643, score-0.684]
</p><p>72 • Smoothed classiﬁcation error loss: formulation (15) with convex regularization g(w) and nonconvex loss function (with α ≥ 1) φ(w⊤ x, y) = min(α, max(0, 1 − w⊤ xy)). [sent-645, score-0.785]
</p><p>73 • L p regularization (0 ≤ p ≤ 1): formulation (15) with nonconvex regularization g(w) = w p p and a loss function φ(·, ·) that is convex in w. [sent-650, score-1.016]
</p><p>74 • Smoothed L p regularization (with parameters α > 0 and 0 ≤ p ≤ 1): formulation (15) with nonconvex regularization g(w) = ∑k [(α + |wk |) p − α p ]/(pα p−1 ), and a loss function φ(·, ·) that is convex in w. [sent-655, score-1.016]
</p><p>75 The main difference between standard L p and smoothed L p is at |wk | = 0, where the smoothed L p regularization is differentiable, with derivative 1. [sent-660, score-0.491]
</p><p>76 • Smoothed log regularization (with parameter α > 0): formulation (15) with nonconvex regularization g(w) = ∑k α ln(α + |wk |), and a loss function φ(·, ·) that is convex in w. [sent-663, score-1.016]
</p><p>77 1102  M ULTISTAGE C ONVEX R ELAXATION  • Capped-L1 regularization (with parameter α > 0): formulation (15) with nonconvex regularization g(w) = ∑d min(|w j |, α), and a loss function φ(·, ·) that is convex in w. [sent-669, score-1.016]
</p><p>78 Therefore when α → 0, this regularization condition is equivalent to the sparse L0 regularization up to a rescaling of λ. [sent-675, score-0.597]
</p><p>79 Capped-L1 regularization is a simpler but less smooth version of the SCAD regularization (Jianqing Fan, 2001). [sent-676, score-0.462]
</p><p>80 Some Examples of Multi-stage Convex Relaxation Methods The multi-stage convex relaxation method can be used with examples in Section 2. [sent-679, score-0.684]
</p><p>81 The optimization problem is n  ∑ min(α, max(0, 1 − w⊤xi yi )) + λg(w)  ˆ w = arg min w  ,  i=1  where we assume that g(w) is a convex regularization condition such as g(w) = λ w 2 . [sent-687, score-0.658]
</p><p>82 2 implies that the multi-stage convex relaxation solves the weighted SVM formulation n  ˆ w = arg min w  ˆ ∑ vi max(0, 1 − w⊤xi yi ) + λg(w)  ,  i=1  where the relaxation parameter v is updated as ˆ ˆ vi = I(w⊤ xi yi ≥ 1 − α) (i = 1, . [sent-697, score-1.49]
</p><p>83 An alternative is to relax smoothed L p regularization (p ∈ (0, 1)) directly to Lq regularization for q ≥ 1 (one usually takes either q = 1 or q = 2). [sent-717, score-0.609]
</p><p>84 In summary, for L p and smoothed L p , we consider the following optimization formulation for some α ≥ 0 and p ∈ (0, 1]: d  ˆ w = arg min R0 (w) + λ ∑ (α + |w j |) p , w  j=1  where we assume that R0 (w) is a convex function of w. [sent-721, score-0.539]
</p><p>85 From previous discussion, the multi-stage convex relaxation method in Section 2. [sent-722, score-0.684]
</p><p>86 2 becomes a weighted Lq regularization formulation for q ≥ 1: d  ˆ ˆ w = arg min R0 (w) + ∑ v j |w j |q , w  j=1  where the relaxation parameter v is updated as ˆ ˆ ˆ v j = λ(p/q)(α + |w j |) p−1 |w j |1−q  ( j = 1, . [sent-723, score-0.936]
</p><p>87 3 Smoothed log Regularization This is a different sparse regularization condition, where we consider a regularization term Rk (w) = λα ln(α+|wk |) for some α > 0. [sent-731, score-0.55]
</p><p>88 The dual is R∗ (vk ) = λ(α/q)[ln vk + 1 − k ˆ ln(λα/q)], deﬁned on the domain vk ≥ 0. [sent-733, score-0.566]
</p><p>89 Similar to smoothed log regularization, the multi-stage convex relaxation method in Section 2. [sent-737, score-0.803]
</p><p>90 2 becomes a weighted Lq regularization formulation for q ≥ 1: d  ˆ ˆ w = arg min R0 (w) + ∑ v j |w j |q , w  j=1  where the relaxation parameter v is updated as ˆ v j = λ(α/q)(α + |w j |)−1 |w j |1−q  ( j = 1, . [sent-738, score-0.936]
</p><p>91 2, the multi-stage convex relaxation becomes a weighted L1 regularization formulation: d  ˆ ˆ w = arg min R0 (w) + ∑ v j |w j | , w  j=1  where the relaxation parameter v is updated as ˆ ˆ v j = λI(|w j | ≤ α) ( j = 1, . [sent-755, score-1.542]
</p><p>92 The capped-L1 formulation removes the bias by adaptively adjusting the relaxation parameter ˆ ˆ v j so that if |w j | is large, then we do not penalize the corresponding variable j. [sent-761, score-0.551]
</p><p>93 2 S PARSE E IGENVALUE P ROBLEM We use a simple example to illustrate that the multi-stage convex relaxation idea does not only apply to formulations with convex risks. [sent-764, score-0.978]
</p><p>94 Although the standard eigenvalue problem is not convex in w, it has a convex relaxation to a semi-deﬁnite programming problem, and thus can be efﬁciently solved. [sent-768, score-0.974]
</p><p>95 The multi-stage convex relaxation becomes: d  ˆ w = arg max  w 2 ≤1  w⊤ Aw − ∑ v j w2 , j j=1  which is a standard eigenvalue problem. [sent-770, score-0.833]
</p><p>96 The relaxation parameter is updated as ˆ ˆ ˆ v j = λ(p/2)(α + |w j |) p−1 |w j |−1 1105  ( j = 1, . [sent-771, score-0.496]
</p><p>97 The matrix regularization used here is the counterpart of L p regularization for vectors. [sent-782, score-0.462]
</p><p>98 Again, this problem can be solved with multi-stage convex relaxation method. [sent-788, score-0.684]
</p><p>99 In this case, the relaxation parameter v is a positive semi-deﬁnite matrix, and we relax the regularization term to h(w) = (αI + ww⊤ ) as a matrix. [sent-789, score-0.732]
</p><p>100 Similar to the vector case, we have the following update formula for the relaxation parameter: ˆ ˆˆ v = λ(p/2)(αI + ww⊤ )(p−2)/2 . [sent-796, score-0.473]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('relaxation', 0.473), ('vk', 0.283), ('gc', 0.236), ('nonconvex', 0.236), ('regularization', 0.231), ('convex', 0.211), ('rk', 0.201), ('ultistage', 0.196), ('wk', 0.171), ('elaxation', 0.168), ('hk', 0.164), ('onvex', 0.151), ('wg', 0.142), ('smoothed', 0.119), ('uk', 0.116), ('ey', 0.106), ('lasso', 0.101), ('hang', 0.098), ('ln', 0.088), ('sparse', 0.088), ('wi', 0.087), ('concave', 0.083), ('formulations', 0.083), ('zou', 0.078), ('formulation', 0.078), ('housing', 0.077), ('pqi', 0.075), ('arg', 0.07), ('regularizers', 0.068), ('lemma', 0.066), ('sgn', 0.065), ('lla', 0.064), ('pz', 0.064), ('min', 0.061), ('jianqing', 0.06), ('eigenvalue', 0.057), ('candes', 0.057), ('rd', 0.055), ('lq', 0.054), ('stage', 0.052), ('rdk', 0.052), ('zhang', 0.049), ('solution', 0.047), ('condition', 0.047), ('lqa', 0.045), ('tong', 0.045), ('local', 0.044), ('remedy', 0.041), ('boston', 0.04), ('duality', 0.04), ('ww', 0.04), ('yi', 0.038), ('mm', 0.038), ('sparsity', 0.037), ('xw', 0.036), ('census', 0.035), ('eyi', 0.035), ('yuille', 0.035), ('effectiveness', 0.034), ('superior', 0.034), ('xy', 0.034), ('shall', 0.034), ('proposition', 0.033), ('cccp', 0.03), ('runze', 0.03), ('scad', 0.03), ('yk', 0.03), ('aw', 0.03), ('pr', 0.03), ('target', 0.029), ('li', 0.029), ('loss', 0.029), ('inequality', 0.028), ('relax', 0.028), ('risk', 0.028), ('wd', 0.027), ('regularizer', 0.026), ('multistage', 0.026), ('alexandre', 0.026), ('emmanuel', 0.026), ('rangarajan', 0.026), ('xf', 0.026), ('procedure', 0.025), ('desired', 0.025), ('solves', 0.025), ('mc', 0.024), ('theorem', 0.024), ('let', 0.023), ('squared', 0.023), ('updated', 0.023), ('bunea', 0.023), ('nonconcave', 0.023), ('conditions', 0.023), ('qi', 0.023), ('theoretically', 0.022), ('standard', 0.022), ('yn', 0.022), ('convenience', 0.022), ('rutgers', 0.021), ('thereafter', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="12-tfidf-1" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We consider learning formulations with non-convex objective functions that often occur in practical applications. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to a sub-optimal solution in reality. This paper tries to remedy the above gap between theory and practice. In particular, we present a multi-stage convex relaxation scheme for solving problems with non-convex objective functions. For learning formulations with sparse regularization, we analyze the behavior of a speciﬁc multistage relaxation scheme. Under appropriate conditions, we show that the local solution obtained by this procedure is superior to the global solution of the standard L1 convex relaxation for learning sparse targets. Keywords: sparsity, non-convex optimization, convex relaxation, multi-stage convex relaxation</p><p>2 0.11669865 <a title="12-tfidf-2" href="./jmlr-2010-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">79 jmlr-2010-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>Author: Thomas Jaksch, Ronald Ortner, Peter Auer</p><p>Abstract: For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s, s′ there is a policy which moves from s to s′ in at most D steps (on average). √ ˜ We present a reinforcement learning algorithm with total regret O(DS AT ) after T steps for any unknown MDP with S states, A actions per state, and diameter D. A corresponding lower bound of √ Ω( DSAT ) on the total regret of any learning algorithm is given as well. These results are complemented by a sample complexity bound on the number of suboptimal steps taken by our algorithm. This bound can be used to achieve a (gap-dependent) regret bound that is logarithmic in T . Finally, we also consider a setting where the MDP is allowed to change a ﬁxed number of ℓ times. We present a modiﬁcation of our algorithm that is able to deal with this setting and show a √ ˜ regret bound of O(ℓ1/3 T 2/3 DS A). Keywords: undiscounted reinforcement learning, Markov decision process, regret, online learning, sample complexity</p><p>3 0.10975348 <a title="12-tfidf-3" href="./jmlr-2010-A_Comparison_of_Optimization_Methods_and_Software_for_Large-scale_L1-regularized_Linear_Classification.html">1 jmlr-2010-A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</a></p>
<p>Author: Guo-Xun Yuan, Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Large-scale linear classiﬁcation is widely used in many areas. The L1-regularized form can be applied for feature selection; however, its non-differentiability causes more difﬁculties in training. Although various optimization methods have been proposed in recent years, these have not yet been compared suitably. In this paper, we ﬁrst broadly review existing methods. Then, we discuss state-of-the-art software packages in detail and propose two efﬁcient implementations. Extensive comparisons indicate that carefully implemented coordinate descent methods are very suitable for training large document data. Keywords: L1 regularization, linear classiﬁcation, optimization methods, logistic regression, support vector machines, document classiﬁcation</p><p>4 0.10616499 <a title="12-tfidf-4" href="./jmlr-2010-On_Spectral_Learning.html">84 jmlr-2010-On Spectral Learning</a></p>
<p>Author: Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: In this paper, we study the problem of learning a matrix W from a set of linear measurements. Our formulation consists in solving an optimization problem which involves regularization with a spectral penalty term. That is, the penalty term is a function of the spectrum of the covariance of W . Instances of this problem in machine learning include multi-task learning, collaborative ﬁltering and multi-view learning, among others. Our goal is to elucidate the form of the optimal solution of spectral learning. The theory of spectral learning relies on the von Neumann characterization of orthogonally invariant norms and their association with symmetric gauge functions. Using this tool we formulate a representer theorem for spectral regularization and specify it to several useful example, such as Schatten p−norms, trace norm and spectral norm, which should proved useful in applications. Keywords: kernel methods, matrix learning, minimal norm interpolation, multi-task learning, orthogonally invariant norms, regularization</p><p>5 0.10321908 <a title="12-tfidf-5" href="./jmlr-2010-A_Fast_Hybrid_Algorithm_for_Large-Scalel1-Regularized_Logistic_Regression.html">3 jmlr-2010-A Fast Hybrid Algorithm for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>Author: Jianing Shi, Wotao Yin, Stanley Osher, Paul Sajda</p><p>Abstract: ℓ1 -regularized logistic regression, also known as sparse logistic regression, is widely used in machine learning, computer vision, data mining, bioinformatics and neural signal processing. The use of ℓ1 regularization attributes attractive properties to the classiﬁer, such as feature selection, robustness to noise, and as a result, classiﬁer generality in the context of supervised learning. When a sparse logistic regression problem has large-scale data in high dimensions, it is computationally expensive to minimize the non-differentiable ℓ1 -norm in the objective function. Motivated by recent work (Koh et al., 2007; Hale et al., 2008), we propose a novel hybrid algorithm based on combining two types of optimization iterations: one being very fast and memory friendly while the other being slower but more accurate. Called hybrid iterative shrinkage (HIS), the resulting algorithm is comprised of a ﬁxed point continuation phase and an interior point phase. The ﬁrst phase is based completely on memory efﬁcient operations such as matrix-vector multiplications, while the second phase is based on a truncated Newton’s method. Furthermore, we show that various optimization techniques, including line search and continuation, can signiﬁcantly accelerate convergence. The algorithm has global convergence at a geometric rate (a Q-linear rate in optimization terminology). We present a numerical comparison with several existing algorithms, including an analysis using benchmark data from the UCI machine learning repository, and show our algorithm is the most computationally efﬁcient without loss of accuracy. Keywords: logistic regression, ℓ1 regularization, ﬁxed point continuation, supervised learning, large scale c 2010 Jianing Shi, Wotao Yin, Stanley Osher and Paul Sajda. S HI , Y IN , O SHER AND S AJDA</p><p>6 0.092130415 <a title="12-tfidf-6" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>7 0.091483846 <a title="12-tfidf-7" href="./jmlr-2010-Rate_Minimaxity_of_the_Lasso_and_Dantzig_Selector_for_thelqLoss_inlrBalls.html">96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</a></p>
<p>8 0.089813232 <a title="12-tfidf-8" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>9 0.082772233 <a title="12-tfidf-9" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>10 0.078194603 <a title="12-tfidf-10" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>11 0.076270141 <a title="12-tfidf-11" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>12 0.068437025 <a title="12-tfidf-12" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>13 0.06815058 <a title="12-tfidf-13" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>14 0.068039872 <a title="12-tfidf-14" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>15 0.061472077 <a title="12-tfidf-15" href="./jmlr-2010-On_Finding_Predictors_for_Arbitrary_Families_of_Processes.html">81 jmlr-2010-On Finding Predictors for Arbitrary Families of Processes</a></p>
<p>16 0.058934279 <a title="12-tfidf-16" href="./jmlr-2010-Classification_Methods_with_Reject_Option_Based_on_Convex_Risk_Minimization.html">21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</a></p>
<p>17 0.058219448 <a title="12-tfidf-17" href="./jmlr-2010-Bundle_Methods_for_Regularized_Risk_Minimization.html">18 jmlr-2010-Bundle Methods for Regularized Risk Minimization</a></p>
<p>18 0.056552567 <a title="12-tfidf-18" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>19 0.053803597 <a title="12-tfidf-19" href="./jmlr-2010-Dual_Averaging_Methods_for_Regularized_Stochastic_Learning_and_Online_Optimization.html">31 jmlr-2010-Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a></p>
<p>20 0.053299814 <a title="12-tfidf-20" href="./jmlr-2010-A_Quasi-Newton_Approach_to_Nonsmooth_Convex_Optimization_Problems_in_Machine_Learning.html">5 jmlr-2010-A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.249), (1, -0.161), (2, 0.169), (3, -0.025), (4, -0.027), (5, -0.072), (6, 0.074), (7, -0.031), (8, 0.074), (9, -0.073), (10, 0.051), (11, -0.079), (12, 0.138), (13, 0.041), (14, 0.058), (15, -0.106), (16, 0.125), (17, -0.189), (18, -0.038), (19, -0.09), (20, -0.044), (21, -0.063), (22, 0.068), (23, 0.098), (24, 0.009), (25, 0.089), (26, -0.024), (27, -0.066), (28, -0.044), (29, 0.044), (30, 0.053), (31, -0.123), (32, -0.003), (33, -0.135), (34, -0.084), (35, -0.005), (36, 0.128), (37, -0.007), (38, -0.058), (39, -0.039), (40, -0.041), (41, 0.218), (42, -0.077), (43, 0.105), (44, -0.127), (45, -0.072), (46, -0.018), (47, -0.003), (48, -0.026), (49, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96148705 <a title="12-lsi-1" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We consider learning formulations with non-convex objective functions that often occur in practical applications. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to a sub-optimal solution in reality. This paper tries to remedy the above gap between theory and practice. In particular, we present a multi-stage convex relaxation scheme for solving problems with non-convex objective functions. For learning formulations with sparse regularization, we analyze the behavior of a speciﬁc multistage relaxation scheme. Under appropriate conditions, we show that the local solution obtained by this procedure is superior to the global solution of the standard L1 convex relaxation for learning sparse targets. Keywords: sparsity, non-convex optimization, convex relaxation, multi-stage convex relaxation</p><p>2 0.63066602 <a title="12-lsi-2" href="./jmlr-2010-A_Comparison_of_Optimization_Methods_and_Software_for_Large-scale_L1-regularized_Linear_Classification.html">1 jmlr-2010-A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</a></p>
<p>Author: Guo-Xun Yuan, Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Large-scale linear classiﬁcation is widely used in many areas. The L1-regularized form can be applied for feature selection; however, its non-differentiability causes more difﬁculties in training. Although various optimization methods have been proposed in recent years, these have not yet been compared suitably. In this paper, we ﬁrst broadly review existing methods. Then, we discuss state-of-the-art software packages in detail and propose two efﬁcient implementations. Extensive comparisons indicate that carefully implemented coordinate descent methods are very suitable for training large document data. Keywords: L1 regularization, linear classiﬁcation, optimization methods, logistic regression, support vector machines, document classiﬁcation</p><p>3 0.629996 <a title="12-lsi-3" href="./jmlr-2010-A_Fast_Hybrid_Algorithm_for_Large-Scalel1-Regularized_Logistic_Regression.html">3 jmlr-2010-A Fast Hybrid Algorithm for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>Author: Jianing Shi, Wotao Yin, Stanley Osher, Paul Sajda</p><p>Abstract: ℓ1 -regularized logistic regression, also known as sparse logistic regression, is widely used in machine learning, computer vision, data mining, bioinformatics and neural signal processing. The use of ℓ1 regularization attributes attractive properties to the classiﬁer, such as feature selection, robustness to noise, and as a result, classiﬁer generality in the context of supervised learning. When a sparse logistic regression problem has large-scale data in high dimensions, it is computationally expensive to minimize the non-differentiable ℓ1 -norm in the objective function. Motivated by recent work (Koh et al., 2007; Hale et al., 2008), we propose a novel hybrid algorithm based on combining two types of optimization iterations: one being very fast and memory friendly while the other being slower but more accurate. Called hybrid iterative shrinkage (HIS), the resulting algorithm is comprised of a ﬁxed point continuation phase and an interior point phase. The ﬁrst phase is based completely on memory efﬁcient operations such as matrix-vector multiplications, while the second phase is based on a truncated Newton’s method. Furthermore, we show that various optimization techniques, including line search and continuation, can signiﬁcantly accelerate convergence. The algorithm has global convergence at a geometric rate (a Q-linear rate in optimization terminology). We present a numerical comparison with several existing algorithms, including an analysis using benchmark data from the UCI machine learning repository, and show our algorithm is the most computationally efﬁcient without loss of accuracy. Keywords: logistic regression, ℓ1 regularization, ﬁxed point continuation, supervised learning, large scale c 2010 Jianing Shi, Wotao Yin, Stanley Osher and Paul Sajda. S HI , Y IN , O SHER AND S AJDA</p><p>4 0.5409711 <a title="12-lsi-4" href="./jmlr-2010-On_Spectral_Learning.html">84 jmlr-2010-On Spectral Learning</a></p>
<p>Author: Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: In this paper, we study the problem of learning a matrix W from a set of linear measurements. Our formulation consists in solving an optimization problem which involves regularization with a spectral penalty term. That is, the penalty term is a function of the spectrum of the covariance of W . Instances of this problem in machine learning include multi-task learning, collaborative ﬁltering and multi-view learning, among others. Our goal is to elucidate the form of the optimal solution of spectral learning. The theory of spectral learning relies on the von Neumann characterization of orthogonally invariant norms and their association with symmetric gauge functions. Using this tool we formulate a representer theorem for spectral regularization and specify it to several useful example, such as Schatten p−norms, trace norm and spectral norm, which should proved useful in applications. Keywords: kernel methods, matrix learning, minimal norm interpolation, multi-task learning, orthogonally invariant norms, regularization</p><p>5 0.47300726 <a title="12-lsi-5" href="./jmlr-2010-Rate_Minimaxity_of_the_Lasso_and_Dantzig_Selector_for_thelqLoss_inlrBalls.html">96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</a></p>
<p>Author: Fei Ye, Cun-Hui Zhang</p><p>Abstract: We consider the estimation of regression coefﬁcients in a high-dimensional linear model. For regression coefﬁcients in ℓr balls, we provide lower bounds for the minimax ℓq risk and minimax quantiles of the ℓq loss for all design matrices. Under an ℓ0 sparsity condition on a target coefﬁcient vector, we sharpen and unify existing oracle inequalities for the Lasso and Dantzig selector. We derive oracle inequalities for target coefﬁcient vectors with many small elements and smaller threshold levels than the universal threshold. These oracle inequalities provide sufﬁcient conditions on the design matrix for the rate minimaxity of the Lasso and Dantzig selector for the ℓq risk and loss in ℓr balls, 0 ≤ r ≤ 1 ≤ q ≤ ∞. By allowing q = ∞, our risk bounds imply the variable selection consistency of threshold Lasso and Dantzig selectors. Keywords: variable selection, estimation, oracle inequality, minimax, linear regression, penalized least squares, linear programming</p><p>6 0.44131309 <a title="12-lsi-6" href="./jmlr-2010-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">79 jmlr-2010-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>7 0.37988204 <a title="12-lsi-7" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<p>8 0.35768121 <a title="12-lsi-8" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>9 0.35504305 <a title="12-lsi-9" href="./jmlr-2010-Regularized_Discriminant_Analysis%2C_Ridge_Regression_and_Beyond.html">98 jmlr-2010-Regularized Discriminant Analysis, Ridge Regression and Beyond</a></p>
<p>10 0.35364488 <a title="12-lsi-10" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>11 0.34680951 <a title="12-lsi-11" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>12 0.3313489 <a title="12-lsi-12" href="./jmlr-2010-On_Finding_Predictors_for_Arbitrary_Families_of_Processes.html">81 jmlr-2010-On Finding Predictors for Arbitrary Families of Processes</a></p>
<p>13 0.33097467 <a title="12-lsi-13" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>14 0.32085446 <a title="12-lsi-14" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>15 0.32041314 <a title="12-lsi-15" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>16 0.31936467 <a title="12-lsi-16" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>17 0.31743047 <a title="12-lsi-17" href="./jmlr-2010-A_Convergent_Online_Single_Time_Scale_Actor_Critic_Algorithm.html">2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</a></p>
<p>18 0.31672943 <a title="12-lsi-18" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>19 0.30868769 <a title="12-lsi-19" href="./jmlr-2010-Composite_Binary_Losses.html">25 jmlr-2010-Composite Binary Losses</a></p>
<p>20 0.3082096 <a title="12-lsi-20" href="./jmlr-2010-Spectral_Regularization_Algorithms_for_Learning_Large_Incomplete_Matrices.html">105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.011), (3, 0.025), (8, 0.027), (21, 0.468), (32, 0.057), (36, 0.021), (37, 0.058), (75, 0.171), (85, 0.045), (96, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89063871 <a title="12-lda-1" href="./jmlr-2010-On_the_Rate_of_Convergence_of_the_Bagged_Nearest_Neighbor_Estimate.html">86 jmlr-2010-On the Rate of Convergence of the Bagged Nearest Neighbor Estimate</a></p>
<p>Author: Gérard Biau, Frédéric Cérou, Arnaud Guyader</p><p>Abstract: Bagging is a simple way to combine estimates in order to improve their performance. This method, suggested by Breiman in 1996, proceeds by resampling from the original data set, constructing a predictor from each subsample, and decide by combining. By bagging an n-sample, the crude nearest neighbor regression estimate is turned into a consistent weighted nearest neighbor regression estimate, which is amenable to statistical analysis. Letting the resampling size kn grows appropriately with n, it is shown that this estimate may achieve optimal rate of convergence, independently from the fact that resampling is done with or without replacement. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, adaptation results by data-splitting are presented. Keywords: bagging, resampling, nearest neighbor estimate, rates of convergence</p><p>same-paper 2 0.84063137 <a title="12-lda-2" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We consider learning formulations with non-convex objective functions that often occur in practical applications. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to a sub-optimal solution in reality. This paper tries to remedy the above gap between theory and practice. In particular, we present a multi-stage convex relaxation scheme for solving problems with non-convex objective functions. For learning formulations with sparse regularization, we analyze the behavior of a speciﬁc multistage relaxation scheme. Under appropriate conditions, we show that the local solution obtained by this procedure is superior to the global solution of the standard L1 convex relaxation for learning sparse targets. Keywords: sparsity, non-convex optimization, convex relaxation, multi-stage convex relaxation</p><p>3 0.78738922 <a title="12-lda-3" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<p>Author: James Henderson, Ivan Titov</p><p>Abstract: We propose a class of Bayesian networks appropriate for structured prediction problems where the Bayesian network’s model structure is a function of the predicted output structure. These incremental sigmoid belief networks (ISBNs) make decoding possible because inference with partial output structures does not require summing over the unboundedly many compatible model structures, due to their directed edges and incrementally speciﬁed model structure. ISBNs are speciﬁcally targeted at challenging structured prediction problems such as natural language parsing, where learning the domain’s complex statistical dependencies beneﬁts from large numbers of latent variables. While exact inference in ISBNs with large numbers of latent variables is not tractable, we propose two efﬁcient approximations. First, we demonstrate that a previous neural network parsing model can be viewed as a coarse mean-ﬁeld approximation to inference with ISBNs. We then derive a more accurate but still tractable variational approximation, which proves effective in artiﬁcial experiments. We compare the effectiveness of these models on a benchmark natural language parsing task, where they achieve accuracy competitive with the state-of-the-art. The model which is a closer approximation to an ISBN has better parsing accuracy, suggesting that ISBNs are an appropriate abstract model of natural language grammar learning. Keywords: Bayesian networks, dynamic Bayesian networks, grammar learning, natural language parsing, neural networks</p><p>4 0.68608129 <a title="12-lda-4" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>Author: Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, Chih-Jen Lin</p><p>Abstract: Kernel techniques have long been used in SVM to handle linearly inseparable problems by transforming data to a high dimensional space, but training and testing large data sets is often time consuming. In contrast, we can efﬁciently train and test much larger data sets using linear SVM without kernels. In this work, we apply fast linear-SVM methods to the explicit form of polynomially mapped data and investigate implementation issues. The approach enjoys fast training and testing, but may sometimes achieve accuracy close to that of using highly nonlinear kernels. Empirical experiments show that the proposed method is useful for certain large-scale data sets. We successfully apply the proposed method to a natural language processing (NLP) application by improving the testing accuracy under some training/testing speed requirements. Keywords: decomposition methods, low-degree polynomial mapping, kernel functions, support vector machines, dependency parsing, natural language processing</p><p>5 0.46395674 <a title="12-lda-5" href="./jmlr-2010-A_Comparison_of_Optimization_Methods_and_Software_for_Large-scale_L1-regularized_Linear_Classification.html">1 jmlr-2010-A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</a></p>
<p>Author: Guo-Xun Yuan, Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Large-scale linear classiﬁcation is widely used in many areas. The L1-regularized form can be applied for feature selection; however, its non-differentiability causes more difﬁculties in training. Although various optimization methods have been proposed in recent years, these have not yet been compared suitably. In this paper, we ﬁrst broadly review existing methods. Then, we discuss state-of-the-art software packages in detail and propose two efﬁcient implementations. Extensive comparisons indicate that carefully implemented coordinate descent methods are very suitable for training large document data. Keywords: L1 regularization, linear classiﬁcation, optimization methods, logistic regression, support vector machines, document classiﬁcation</p><p>6 0.46339309 <a title="12-lda-6" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>7 0.45520794 <a title="12-lda-7" href="./jmlr-2010-A_Convergent_Online_Single_Time_Scale_Actor_Critic_Algorithm.html">2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</a></p>
<p>8 0.45371804 <a title="12-lda-8" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<p>9 0.45317075 <a title="12-lda-9" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>10 0.45012832 <a title="12-lda-10" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>11 0.44979432 <a title="12-lda-11" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>12 0.44755465 <a title="12-lda-12" href="./jmlr-2010-Iterative_Scaling_and_Coordinate_Descent_Methods_for_Maximum_Entropy_Models.html">57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</a></p>
<p>13 0.44519046 <a title="12-lda-13" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>14 0.44453731 <a title="12-lda-14" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>15 0.44386107 <a title="12-lda-15" href="./jmlr-2010-Using_Contextual_Representations_to_Efficiently_Learn_Context-Free_Languages.html">115 jmlr-2010-Using Contextual Representations to Efficiently Learn Context-Free Languages</a></p>
<p>16 0.44259897 <a title="12-lda-16" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>17 0.44202629 <a title="12-lda-17" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>18 0.44055811 <a title="12-lda-18" href="./jmlr-2010-On_Spectral_Learning.html">84 jmlr-2010-On Spectral Learning</a></p>
<p>19 0.43983072 <a title="12-lda-19" href="./jmlr-2010-A_Quasi-Newton_Approach_to_Nonsmooth_Convex_Optimization_Problems_in_Machine_Learning.html">5 jmlr-2010-A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning</a></p>
<p>20 0.43960077 <a title="12-lda-20" href="./jmlr-2010-Spectral_Regularization_Algorithms_for_Learning_Large_Incomplete_Matrices.html">105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
