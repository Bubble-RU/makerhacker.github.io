<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>108 jmlr-2010-Stochastic Complexity and Generalization Error of a Restricted Boltzmann Machine in Bayesian Estimation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-108" href="#">jmlr2010-108</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>108 jmlr-2010-Stochastic Complexity and Generalization Error of a Restricted Boltzmann Machine in Bayesian Estimation</h1>
<br/><p>Source: <a title="jmlr-2010-108-pdf" href="http://jmlr.org/papers/volume11/aoyagi10a/aoyagi10a.pdf">pdf</a></p><p>Author: Miki Aoyagi</p><p>Abstract: In this paper, we consider the asymptotic form of the generalization error for the restricted Boltzmann machine in Bayesian estimation. It has been shown that obtaining the maximum pole of zeta functions is related to the asymptotic form of the generalization error for hierarchical learning models (Watanabe, 2001a,b). The zeta function is deﬁned by using a Kullback function. We use two methods to obtain the maximum pole: a new eigenvalue analysis method and a recursive blowing up process. We show that these methods are effective for obtaining the asymptotic form of the generalization error of hierarchical learning models. Keywords: Boltzmann machine, non-regular learning machine, resolution of singularities, zeta function</p><p>Reference: <a title="jmlr-2010-108-reference" href="../jmlr2010_reference/jmlr-2010-Stochastic_Complexity_and_Generalization_Error_of_a_Restricted_Boltzmann_Machine_in_Bayesian_Estimation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It has been shown that obtaining the maximum pole of zeta functions is related to the asymptotic form of the generalization error for hierarchical learning models (Watanabe, 2001a,b). [sent-5, score-0.4]
</p><p>2 We use two methods to obtain the maximum pole: a new eigenvalue analysis method and a recursive blowing up process. [sent-7, score-0.183]
</p><p>3 We show that these methods are effective for obtaining the asymptotic form of the generalization error of hierarchical learning models. [sent-8, score-0.114]
</p><p>4 Keywords: Boltzmann machine, non-regular learning machine, resolution of singularities, zeta function  1. [sent-9, score-0.109]
</p><p>5 For example, consider a simple restricted Boltzmann machine that has two observable units and one hidden unit with binary variables (Fig. [sent-16, score-0.108]
</p><p>6 Z(a) xi =±1,y=±1,  AOYAGI  Figure 1: Simple restricted Boltzmann machine model: Two observable units and one hidden unit. [sent-20, score-0.108]
</p><p>7 Consequently, the many theoretical problems, such as clarifying generalization errors in learning theory, have remained unsolved. [sent-28, score-0.09]
</p><p>8 The generalization error measures the difference between the true density function q(x) and the predictive density function p(x|xn ) obtained using n distributed training samples xn = (x1 , . [sent-29, score-0.171]
</p><p>9 The asymptotic form of the generalization error is important for model selection methods. [sent-38, score-0.088]
</p><p>10 Therefore, it is important to construct a mathematical foundation for clarifying the generalization error of non-regular models. [sent-41, score-0.115]
</p><p>11 Several papers (Yamazaki and Watanabe, 2005; Nishiyama and Watanabe, 2006) have reported upper bounds for the asymptotic form of the generalization error for the Boltzmann machine model, but not the exact main terms. [sent-45, score-0.088]
</p><p>12 The direct problem involves solving the generalization error with a known true density function. [sent-47, score-0.094]
</p><p>13 The inverse problem is ﬁnding proper learning models and learning algorithms to minimize the generalization error under the condition of an unknown true density function. [sent-48, score-0.094]
</p><p>14 We have already obtained the exact asymptotic forms of the generalization errors for the three layered neural network (Aoyagi and Watanabe, 2005a; Aoyagi, 2006), and for the reduced rank regression (Aoyagi and Watanabe, 2005b). [sent-51, score-0.148]
</p><p>15 It has recently been proved that the maximum pole of a zeta function gives the generalization error of hierarchical learning models asymptotically, assuming that the function approximation error is negligible compared to the statistical estimation error (Watanabe, 2001a,b). [sent-73, score-0.369]
</p><p>16 If the parameter’s dimension of the true distribution is larger than that of the learning model, clarifying the behavior of the generalization error is rather easy. [sent-76, score-0.09]
</p><p>17 p(x|w)  Then, for the maximum pole −λ of J(z) and its order θ, we have F(n) = λ log n − (θ − 1) log log n + O(1),  (1)  where O(1) is a bounded function of n, and if G(n) has an asymptotic expansion, G(n) ∼ λ/n − (θ − 1)/(n log n) as n → ∞. [sent-79, score-0.232]
</p><p>18 = 1246  (2)  S TOCHASTIC C OMPLEXITY OF R ESTRICTED B OLTZMANN M ACHINE  Figure 2: A restricted Boltzmann machine: M is the number of binary observable units x and N is the number of binary hidden units y. [sent-80, score-0.148]
</p><p>19 The learning model is p(x, y|a) ∝ exp(∑M ∑N ai j xi y j ), where ai j is a parameter between xi and y j . [sent-81, score-0.088]
</p><p>20 To assist in achieving this aim, we use the desingularization in algebraic geometry (Watanabe, 2009). [sent-83, score-0.155]
</p><p>21 It is, however, a new problem, even in mathematics, to obtain the desingularization of Kullback functions, since the singularities of these functions are very complicated and as such most of them have not yet been investigated (Appendix A). [sent-84, score-0.172]
</p><p>22 We, therefore, need a new method of eigenvalue analysis and a recursive blowing up process. [sent-85, score-0.183]
</p><p>23 Set exp(∑M ∑N ai j xi y j ) i=1 j=1 p(x, y|a) = , Z(a) where Z(a) =  ∑  M  xi =±1,y j =±1,  N  exp( ∑ ∑ ai j xi y j ), i=1 j=1  x = (xi ) ∈ {1, −1}M and y = (y j ) ∈ {1, −1}N (Fig. [sent-89, score-0.088]
</p><p>24 0≤p≤M/2 i1 <···  N using a recursive blowing up. [sent-92, score-0.139]
</p><p>25 (2) are given by using the maximum pole −λ = − MN of 4 1, if M > N + 1, J(z) and its order θ = M, if M = N + 1. [sent-96, score-0.201]
</p><p>26 Conclusion In this paper, we obtain the generalization error of restricted Boltzmann machines asymptotically (Fig. [sent-109, score-0.093]
</p><p>27 We use a new method of eigenvalue analysis and a recursive blowing up in algebraic geometry and show that these are effective for solving the problem in learning theory. [sent-111, score-0.253]
</p><p>28 Eigenvalue analysis seems to be necessary for solving the behavior of the restricted Boltzmann machine model’s generalization error for M ≤ N. [sent-113, score-0.093]
</p><p>29 In this paper, we clarify the generalization error for (i) M = 3 (Theorem 2) and (ii) M > N, ∗ = 0 (Theorem 3) explicitly and give new bounds for the generalization error of the other types a (Theorem 4). [sent-114, score-0.114]
</p><p>30 n 1 2 Applying Hironaka’s theorem to the Kullback function K(w), for each w ∈ K −1 (0) ∩ W , we have a proper analytic map µw from an analytic manifold Uw to a neighborhood Vw of w satisfying Hironaka’s Theorem (1) and (2). [sent-133, score-0.171]
</p><p>31 For example, the function Z  U0  (u2s1 u2s2 · · · u2sd )z ut11 ut12 · · · ut1d du 1 2 d  has the poles −(t1 + 1)/(2s1 ), · · · , −(td + 1)/(2sd ), where U0 is a small neighborhood of 0. [sent-136, score-0.105]
</p><p>32 It is known that µ of an arbitrary polynomial in Hironaka’s Theorem can be obtained by using a blowing up process. [sent-141, score-0.113]
</p><p>33 R  Lemma 6 (Aoyagi and Watanabe, 2005b) Let U be a neighborhood of w0 ∈ Rd , C(w) be an analytic H × H ′ matrix function from U, ψ(w) be a C∞ function from U with compact support, and P and Q be any regular H × H and H ′ × H ′ R matrices, respectively. [sent-155, score-0.109]
</p><p>34 Then the maximum pole of R C(w) 2z ψ(w)dw and its order are those of U PC(w)Q 2z ψ(w)dw. [sent-156, score-0.201]
</p><p>35 Then the maximum pole of  and its order are those of  N  {  U {∑x∈X (p(x|a)−  R  ∑ (∑(logW j (x, a) − logW j (x, a∗ ) − logW j (x′ , a) + logW j (x′ , a∗ ))}2 ψ(w)dw. [sent-158, score-0.201]
</p><p>36 Eigenvalue Analysis The purpose of eigenvalue analysis is to simplify the blowing up process. [sent-165, score-0.157]
</p><p>37 Set ℓI =  Then ℓ = (ℓI ) is  an eigenvector of CN and its eigenvalue is  ∑ ℓI bIN =  I∈I  ∏M (1 + xi bi ) + ∏M (1 − xi bi ) i=1 i=1 , where xi = −1 if i ∈ K1 , and xi = 1 if i ∈ K1 . [sent-210, score-0.333]
</p><p>38 2  Note that ∑I∈I ℓI bI > 0 since bi = tanh(ai ). [sent-211, score-0.131]
</p><p>39 1 ··· 1 N H 0 0 0 · · · ∏ j=2 s j We have ˜ ˜ (det S)D′−1 S−1 D′−1 2M−1 (BN B∗ 0 − B∗ N B0 ) N N 1255  AOYAGI   ∏N s∗ 0 ∏i=0 ∏N sij j=2 j=1 j   . [sent-291, score-0.481]
</p><p>40   ∏N s∗ H ∏i=H ∏N sij j=2 j=1 j   ∏i=0 ∏N sij j=2   . [sent-295, score-0.962]
</p><p>41 N i ∏i=H ∏ j=2 s j J  cJ ∏M ∏N bi ijj , where cJ ∈ R, 0 ≤ Ji j ∈ Z and {{∑N Ji j }} = Ii . [sent-298, score-0.131]
</p><p>42 ∗ 0 )H−1 H and det S = (B N ∑ ∏ s j∏ ∏ s  N ∗ 0 i2 =0 N j=1i  i=i2 j=2 j N ∗ 1   ∏ j=1 s j ∏i=0 ∏ j=2 s j ∏ j=1 s j ∏i=1 ∏N sij j=2     . [sent-376, score-0.653]
</p><p>43 ∏N s∗ H ∏i=H ∏N sij ∏N sij ∏N s∗ H ∏ j=1 j  Nj=2 ∗ 1  j=1 j i=H 1 j=2 N 0  ∏N s j − ∏ j=2 s j ∏ j=1 s j − ∏N s∗ 0 j=2 j=1 j   . [sent-383, score-0.962]
</p><p>44 N H − N s0 N ∗ H − N s∗ 0 ∏ j=2 s j ∏ j=2 j ∏ j=1 s j ∏ j=1 j  N ∗1  ∏ j=1 s j ∏i=1 ∏N sij j=2   . [sent-389, score-0.481]
</p><p>45 = (B∗ 0 )H−1 {∑H=0 ∏N s∗ ij2 ∏i=i2 ∏N sij 1 − (H + 1) }, N i2 . [sent-391, score-0.481]
</p><p>46 The fact that   ∏i=0 ∏N sij j=2  ∏i=1 ∏N si  j=2 j   D  . [sent-407, score-0.505]
</p><p>47 N i ∏i=H ∏ j=2 s j  0 0 ··· 0 ∏i=0 ∏N sij j=2 N i 0 ···  0 0 ∏i=1 ∏ j=2 s j  = −D . [sent-410, score-0.481]
</p><p>48 0  = −         N    D ∏ ∏   j=2 0=I ′ ∈I      {{J+I ′ }}  and ∑J∈I DI,J s j  0 ···  0    ′  D{{J+I }},K  {{I0  sj  +I ′ }}  sj  0 ′  0 ···  0 {{I1  0 . [sent-425, score-0.094]
</p><p>49 Proof of Theorem 2 By Theorem 9 (4) and Lemma 6, we only need to consider the maximum pole of J(z) =  N ∗0  ∏ j=1 s j ∏i=0 ∏N sij j=2 R   . [sent-450, score-0.682]
</p><p>50 ∏N s∗ H ∏i=H ∏N sij j=2 j=1 j (Case 1): The fact that B11 = ∑N b1k b2k + · · · provides Case 1. [sent-454, score-0.481]
</p><p>51 Then b11 21 31 21 31 11   N  ′  ∏ j=1 s∗ 0 / ∏N s0 j j=2 j b21  N s∗ 1 / N s1  3 N  ∏ j=1 j ∏ j=2 j  Ψ′ = (det S) b′  − ∏ ∏ sij (B∗ 0 )2 (1, D′ ) N ∗ 2 N 2  N 31  ∏ j=1 s j / ∏ j=2 s j  i=0 j=2 b′ 11 ∏N s∗ 3 / ∏N s3 j=2 j j=1 j   and its maximum pole is 3/2 and its order is 1. [sent-461, score-0.682]
</p><p>52 i=1 i 11     (1, D′ )   ∏N s∗ 0 / ∏N s0 j=2 j j=1 j ∏N s∗ 1 / ∏N s1 j=2 j j=1 j ∏N s∗ 2 / ∏N s2 j=2 j j=1 j ∏N s∗ 3 / ∏N s3 j=2 j j=1 j  − ∏3 ∏N sij (B∗ 0 )2 N i=0 j=2      . [sent-463, score-0.481]
</p><p>53 If = −(1, D′ ) ∗ + b∗ ∗ 2 /s∗ 2  −b  −b∗ j − b∗ j b∗ j − b∗ j  0 0 0 s 1 j 2 1 3 1 3j 2j −b∗ j − b∗ j −b∗ j + b∗ j −b∗ j + b∗ j 0 0 0 s∗ 3 /s∗ 3 2 1 3 1 3 2 j 1 ′′ is not singular, its maximum pole is 3/2 and its order is 1. [sent-466, score-0.201]
</p><p>54 Construct the 2 3 11 21 1 2 3 11 21 blow-up of Ψ′′ along the submanifold {b3 j = 0, 2 ≤ j ≤ N}. [sent-468, score-0.109]
</p><p>55 So 32 3 1 3 (  (∏3 ∏N sij (B∗0 )2 )2 ψ1 ψ2 N i=0 j=2 b2 det S 11  − (B∗ 0 )2 ∏3 ∏N sij ψ3 )/u is smooth. [sent-473, score-1.134]
</p><p>56 j=2 j=2 j=2 3 11 21 j=2 3 (∏3 ∏N sij (B∗0 )2 )2 ψ1 ψ2 N i=0 j=2 b2 det S 11  − (B∗ 0 )2 ∏3 ∏N sij ψ3 )/u is smooth. [sent-475, score-1.134]
</p><p>57 Construct the blow-up of Ψ′ along the submanifold {bi j = 0, 1 ≤ i ≤ M, 1 ≤ j ≤ N}. [sent-478, score-0.109]
</p><p>58 Let b11 = u and bi j = ub′ j for (i, j) = (1, 1). [sent-479, score-0.131]
</p><p>59 i b′′ b′ ∑N b′ b′ + u2 f1 k=2 1k 2k 21 21 By setting = +4 /(det S), we have ′′ ′ b31 b31 ∑N b′ b′ + u2 f2 k=2 1k 3k (  u2 Ψ′′ =  det S   (det S)2 b′′ 21  (det S)2 b′′ × 31 ′′ det S − 4 N b′ b′ − 4u2 f )(b′′ det S − 4 N b′ b′ − 4u2 f ) (b21 ∑k=2 1k 3k ∑k=2 1k 2k 2 1 31   0 2 . [sent-481, score-0.516]
</p><p>60 0 +u N ′ b′ + 4u2 f 4 ∑k=2 b2k 3k 3  By using Lemma 6 again, the maximum pole of  ′′  b21 where Ψ′′′ = u2  b′′ , and 31 g1 N  N  k=2  R  k=2  Ψ′′  2z u3N db is that of J(z) =  g1 = ( ∑ b′ b′ + u2 f1 )( ∑ b′ b′ + u2 f2 ) + 1k 2k 1k 3k  R  Ψ′′′  2z u3N db,  det S N ′ ′ ( ∑ b b + u2 f3 ). [sent-482, score-0.448]
</p><p>61 4 k=2 2k 3k  Construct the blow-up of Ψ′′′ along the submanifold {b′′ = 0, b′′ = 0, b′ = 0, 2 ≤ k ≤ N}. [sent-483, score-0.109]
</p><p>62 Then Ψ′′′ = u2 v b′′′ , 31 32 21 21 31 21 3k 3k g′ 1 where g′ = (∑N b′ b′ + u2 f1 )(b′ + ∑N b′ b′′ + u2 f2 /v) + det S (b′ + ∑N b′ b′′ + u2 f3 /v). [sent-486, score-0.172]
</p><p>63 We have N N N det S ′ ( ∑ b′ b′ )(b′ + ∑ b′ b′′ ) + (b22 + ∑ b′ b′′ ) 12 1k 3k 1k 2k 2k 3k 4 k=3 k=2 k=3      = (b′ , b′ , · · · , b′ )  2,2 2,3 2,N      Since    b′ 1,2 b′ 1,3 . [sent-488, score-0.172]
</p><p>64 b′ 1,N       det S    ′  E   (b1,2 , b′ , · · · , b′ ) + 1,3 1,N 4      ′  (b1,2 , b′ , · · · , b′ ) + 1,3 1,N   det S 4 E  1 b′′ 3,3 . [sent-494, score-0.344]
</p><p>65 Let B′ Ii j = BIi j − ∑N bik b jk , which is a polynomial of at least degree four. [sent-539, score-0.411]
</p><p>66 By using a blowing up process together with an inductive method of s, we have functions (5) and (6) below. [sent-542, score-0.113]
</p><p>67 1 Step 1 Construct the blow-up of function (3) along the submanifold {bi j = 0, 1 ≤ i ≤ M, 1 ≤ j ≤ N}. [sent-546, score-0.109]
</p><p>68 i Then we have BI1 j = u2 (b′j1 + ∑N b′ 1k b′ jk + B′I1 j /u2 ) for j ≥ 2 and BIi j = u2 (∑N b′ ik b′ jk + k=1 k=2 1 1 1 B′Ii j /u2 ) for 2 ≤ i < j. [sent-548, score-0.584]
</p><p>69 ′ b jN        (1)  (1)  where f = b′′ i1 (b′′ j1 − ∑N b′ 1k b′ jk ) − (∑N b′ 1k b′ ik )b′′ j1 is an Ii j -type function of k=2  k=2  ′′ i j ′ b 21 b 22 · · · b′ 2N  b′′ 31 b′ 32 · · · b′ 3N    (1)  . [sent-568, score-0.357]
</p><p>70 b′′ M1 b′ M2 · · · b′ MN Next, construct the blow-up along the submanifold {b′ 12 = b′ 13 = · · · = b′ 1N = 0}. [sent-581, score-0.134]
</p><p>71 k=2  ′I  (1)  ij Let bik = b′′ ik for 1 ≤ i ≤ M, 1 ≤ k ≤ N and (i, k) = (1, 1), B(1) = B′Ii j /u4 for 1 ≤ i < j ≤ M 1  ∑M I  and BI = BI /u1 k=1 i for I ∈ I ′ . [sent-750, score-0.384]
</p><p>72 Construct the blow-up of function (5) along the submanifold {bi j = 0, s + 1 ≤ i ≤ M, 1 ≤ j ≤ N}. [sent-755, score-0.109]
</p><p>73 (s)  If b′ ℓ,ℓ′ = 1 for some s + 1 ≤ ℓ ≤ M and 1 ≤ ℓ′ ≤ s, then we have function (6) by using (s)  fi j |b(s) =···=b(s)  (s) (s) i,min{i−1,s} =b j1 =···=b j,min{i−1,s} =0  i1  and Lemma 6. [sent-759, score-0.116]
</p><p>74 (s)  (s)  (s)  (s)  ′ ′ Set b′′ j,s+1 = b′ j,s+1 + ∑N k=s+2 b s+1,k b jk for j ≥ s + 2. [sent-761, score-0.227]
</p><p>75 k=s+2  k=s+2  k=s+2 (s+1)  N  ∑  k=s+2  k=s+2 N  N  (s)  b′ s+1,k b′ jk ) +  k=s+2  k=s+2  =  N  (s)  b′ s+1,k b′ ik )(b′′ j,s+1 −  (s)  (s)  (s)  (s)  (s)  (s)  N ′ ′′ ′ ′ ′ = fi j /u2 +b′′ j,s+1 (b′′ i,s+1 − ∑N k=s+2 b s+1,k b ik )−b i,s+1 (∑k=s+2 b s+1,k b jk ). [sent-763, score-0.83]
</p><p>76    Let fi j  (s)  (s)  b′ M2  b′′ M1  (s+1)  fi j  ···  |b′′ (s) =···=b′′ (s)  (s)  b′ MN  ′′ (s) ′′ (s) i,s+1 =b j1 =···=b j,s+1 =0  i1  = 0. [sent-776, score-0.232]
</p><p>77 (s)  (s)  (s)  Next, construct the blow-up along the submanifold {b′ s+1,s+2 = b′ s+1,s+3 = · · · = b′ s+1,N = 0}. [sent-777, score-0.134]
</p><p>78    We have N  (  ∑  =  ∑  ∑  (s)  (s)  N  (s)  b′ s+1,k b′ jk ) +  ∑  (s) (s)  bik b jk  k=s+2  k=s+2  k=s+2 N  N  (s)  (s)  b′ s+1,k b′ ik )( (s)  b′′ ik b′′ jk . [sent-820, score-1.125]
</p><p>79 k=s+2 (s+1)  Let b ji (s+1)  fi j  I  (s)  (s+1)  = b ji for i, j ≤ s and b ji  (s)  (s+1)  = fi j for i < j ≤ s, fi j  (s)  (s+1)  (s)  = fi j /us+1 for i ≤ s < j, fs+1, j = fs+1, j /u2 for j > s + 1, s+1 ′I  ′I  ′I  ′I  I  (s)  = b′′ ji for i, j > s and (i, j) = (s + 1, s + 1). [sent-821, score-0.956]
</p><p>80 Also let  i ij i ij ij ij B′ (s+1) = B′ (s) for i < j ≤ s, B(s+1) = B(s)j /us+1 for i ≤ s < j, B(s+1) = B(s)j /u2 for s < i < j and s+1  ∑N′  Ii  k =s+1 ′I for I ∈ I ′ . [sent-822, score-0.308]
</p><p>81 Note that we have the same inductive results for Z  {  N  ( ∑ bik b jk )2 }z db,  ∑  (7)  1≤i< j≤M k=1  instead of the function in Eq. [sent-828, score-0.411]
</p><p>82 Now we again consider the maximum pole of the function in Eq. [sent-833, score-0.201]
</p><p>83 Z  ∑  {u4 u4 · · · u4 1 2 s  b2 + u4 u4 · · · u4 ji 1 2 s  1≤i< j≤M,i≤s  ∑  N  (  ∑  bik b jk )2 }z  s+1≤i< j≤M k=s+1 s  ∏ uk  (M−k+1)(N−k+1)+(2M−k)(k−1)−1  dudb. [sent-837, score-0.562]
</p><p>84 ( fi j + ∑ bik b jk )2 }z u1  1267  k=2  (8)  AOYAGI  By using Lemma 6 again, we need to consider Z  {u4 1  ∑  ∑  b2 + u4 j1 1  2≤ j≤M  N  MN−1 dudb. [sent-840, score-0.527]
</p><p>85 Construct the blow-up of function (8) along the submanifold {b ji = 0, 1 ≤ i < j ≤ M, i ≤ s, bkl = 0, s + 1 ≤ k ≤ M, s + 1 ≤ l ≤ N}. [sent-843, score-0.317]
</p><p>86 Then we have Z  {u4 u4 · · · u4 u2 s s+1 1 2  ∑  b2 + u4 u4 · · · u4 u4 s s+1 ji 1 2  1≤i< j≤M,i≤s  N  ∑  (  ∑  bik b jk )2 }z  s+1≤i< j≤M k=s+1 s  ∏ uk  (M−s)(N−s)+(2M−1−s)s/2−1  us+1  (M−k+1)(N−k+1)+(2M−k)(k−1)−1  dudb,  k=1  where we can set b21 = 1 or bs+1,s+1 = 1. [sent-844, score-0.562]
</p><p>87 (9)  k=1  Construct the blow-up of function (9) along the submanifold {b ji = 0, 1 ≤ i < j ≤ M, i ≤ s, us+1 = 0}. [sent-850, score-0.232]
</p><p>88 (8) with s + 1, that is, Z  {u4 u4 · · · u4 u4 1 2 s s+1  ∑  b2 ji  1≤i< j≤M,i≤s  +u4 u4 · · · u4 u4 ( 1 2 s s+1  ∑  b2 j,s+1 +  s+1< j≤M (M−s)(N−s)+(2M−1−s)s−1  us+1  ∑  N  (  ∑  bik b jk )2 )}z  s+2≤i< j≤M k=s+2 s  ∏ uk  (M−k+1)(N−k+1)+(2M−k)(k−1)−1  k=1  1268  dudb. [sent-852, score-0.562]
</p><p>89 2 Finally, we have Z  {u4 u4 · · · u4 1 2 N  ∑  1≤i< j≤M,i≤N  N  b2 }z ∏ uk ji  (M−k+1)(N−k+1)+(2M−k)(k−1)−1  dudb,  k=1  and obtain the poles (M − k + 1)(N − k + 1) + (2M − k)(k − 1) , k = 1, . [sent-857, score-0.227]
</p><p>90 4 Therefore, since we assume that M > N, we have the maximum pole −λ = − MN and its order 4 1, if M > N + 1, θ= Q. [sent-861, score-0.201]
</p><p>91 Proof of Theorem 4 Assume that a∗ = 0 R By the proof of Theorem 3, the maximum pole of {∑1≤i< j≤M (BIi j )2 }z db is that of R { ( N b b )2 }z db even for M ≤ N. [sent-865, score-0.351]
</p><p>92 If M ≤ N then the maximum pole of R ∑1≤i< j≤M ∑k=1 ik jk 2 z Therefore the maximum pole −λ of { ( N b b ) } db is −M(M − 1)/4. [sent-866, score-0.834]
</p><p>93 R ∑1≤i< j≤MI ∑k=1 ik jk 2 }z db satisﬁes λ ≥ M(M − 1)/4, since {∑I=0∈I (B ) ∑1≤i< j≤M (BIi j )2 ≤ ∑I=0∈I (BI )2 . [sent-867, score-0.432]
</p><p>94 The zeta function of learning theory and generalization error of three layered neural perceptron. [sent-902, score-0.202]
</p><p>95 Resolution of singularities and the generalization error with bayesian estimation for layered neural network. [sent-912, score-0.244]
</p><p>96 Resolution of singularities of an algebraic variety over a ﬁeld of characteristic zero. [sent-950, score-0.129]
</p><p>97 A statistical approaches to learning and generalization in layered neural networks. [sent-957, score-0.117]
</p><p>98 Asymptotic behavior of free energy of general boltzmann machines in mean ﬁeld approximation. [sent-986, score-0.16]
</p><p>99 Algebraic geometry of learning machines with singularities and their prior distributions. [sent-1039, score-0.115]
</p><p>100 Singularities in complete bipartite graph-type boltzmann machines and upper bounds of stochastic complexities. [sent-1063, score-0.16]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sij', 0.481), ('aoyagi', 0.302), ('jk', 0.227), ('pole', 0.201), ('bik', 0.184), ('qn', 0.183), ('det', 0.172), ('boltzmann', 0.16), ('estricted', 0.157), ('oltzmann', 0.157), ('watanabe', 0.152), ('omplexity', 0.141), ('bi', 0.131), ('achine', 0.13), ('kullback', 0.13), ('ik', 0.13), ('ji', 0.123), ('tochastic', 0.122), ('tanh', 0.12), ('fi', 0.116), ('blowing', 0.113), ('submanifold', 0.109), ('hironaka', 0.099), ('ii', 0.099), ('qi', 0.094), ('singularities', 0.087), ('desingularization', 0.085), ('zeta', 0.085), ('bkl', 0.085), ('bn', 0.079), ('poles', 0.076), ('db', 0.075), ('cn', 0.072), ('dudb', 0.071), ('ij', 0.07), ('sh', 0.067), ('bii', 0.06), ('layered', 0.06), ('generalization', 0.057), ('dcn', 0.057), ('logw', 0.057), ('vw', 0.056), ('analytic', 0.054), ('jn', 0.053), ('bs', 0.053), ('hagiwara', 0.048), ('nagata', 0.048), ('cosh', 0.048), ('mn', 0.047), ('sj', 0.047), ('dw', 0.046), ('ai', 0.044), ('di', 0.044), ('eigenvalue', 0.044), ('miki', 0.042), ('algebraic', 0.042), ('xn', 0.04), ('units', 0.04), ('amari', 0.04), ('bayesian', 0.04), ('en', 0.038), ('density', 0.037), ('vb', 0.037), ('jw', 0.036), ('murata', 0.036), ('restricted', 0.036), ('theorem', 0.034), ('clarifying', 0.033), ('observable', 0.032), ('odd', 0.032), ('akaike', 0.032), ('asymptotic', 0.031), ('exchange', 0.029), ('neighborhood', 0.029), ('eigenvectors', 0.029), ('nihon', 0.028), ('nishiyama', 0.028), ('okada', 0.028), ('takamatsu', 0.028), ('toda', 0.028), ('toric', 0.028), ('yoshizawa', 0.028), ('uk', 0.028), ('fukumizu', 0.028), ('dd', 0.028), ('geometry', 0.028), ('let', 0.028), ('eigenvector', 0.027), ('regular', 0.026), ('hierarchical', 0.026), ('recursive', 0.026), ('construct', 0.025), ('resolution', 0.024), ('si', 0.024), ('levin', 0.024), ('rusakov', 0.024), ('yamazaki', 0.024), ('arranging', 0.024), ('qs', 0.024), ('nakajima', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="108-tfidf-1" href="./jmlr-2010-Stochastic_Complexity_and_Generalization_Error_of_a_Restricted_Boltzmann_Machine_in_Bayesian_Estimation.html">108 jmlr-2010-Stochastic Complexity and Generalization Error of a Restricted Boltzmann Machine in Bayesian Estimation</a></p>
<p>Author: Miki Aoyagi</p><p>Abstract: In this paper, we consider the asymptotic form of the generalization error for the restricted Boltzmann machine in Bayesian estimation. It has been shown that obtaining the maximum pole of zeta functions is related to the asymptotic form of the generalization error for hierarchical learning models (Watanabe, 2001a,b). The zeta function is deﬁned by using a Kullback function. We use two methods to obtain the maximum pole: a new eigenvalue analysis method and a recursive blowing up process. We show that these methods are effective for obtaining the asymptotic form of the generalization error of hierarchical learning models. Keywords: Boltzmann machine, non-regular learning machine, resolution of singularities, zeta function</p><p>2 0.13544841 <a title="108-tfidf-2" href="./jmlr-2010-Asymptotic_Equivalence_of_Bayes_Cross_Validation_and_Widely_Applicable_Information_Criterion_in_Singular_Learning_Theory.html">16 jmlr-2010-Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory</a></p>
<p>Author: Sumio Watanabe</p><p>Abstract: In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2λ/n, where λ is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion. Keywords: cross-validation, information criterion, singular learning machine, birational invariant</p><p>3 0.074043483 <a title="108-tfidf-3" href="./jmlr-2010-Consistent_Nonparametric_Tests_of_Independence.html">27 jmlr-2010-Consistent Nonparametric Tests of Independence</a></p>
<p>Author: Arthur Gretton, László Györfi</p><p>Abstract: Three simple and explicit procedures for testing the independence of two multi-dimensional random variables are described. Two of the associated test statistics (L1 , log-likelihood) are deﬁned when the empirical distribution of the variables is restricted to ﬁnite partitions. A third test statistic is deﬁned as a kernel-based independence measure. Two kinds of tests are provided. Distributionfree strong consistent tests are derived on the basis of large deviation bounds on the test statistics: these tests make almost surely no Type I or Type II error after a random sample size. Asymptotically α-level tests are obtained from the limiting distribution of the test statistics. For the latter tests, the Type I error converges to a ﬁxed non-zero value α, and the Type II error drops to zero, for increasing sample size. All tests reject the null hypothesis of independence if the test statistics become large. The performance of the tests is evaluated experimentally on benchmark data. Keywords: hypothesis test, independence, L1, log-likelihood, kernel methods, distribution-free consistent test</p><p>4 0.072349146 <a title="108-tfidf-4" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>Author: Joshua V. Dillon, Guy Lebanon</p><p>Abstract: Maximum likelihood estimators are often of limited practical use due to the intensive computation they require. We propose a family of alternative estimators that maximize a stochastic variation of the composite likelihood function. Each of the estimators resolve the computation-accuracy tradeoff differently, and taken together they span a continuous spectrum of computation-accuracy tradeoff resolutions. We prove the consistency of the estimators, provide formulas for their asymptotic variance, statistical robustness, and computational complexity. We discuss experimental results in the context of Boltzmann machines and conditional random ﬁelds. The theoretical and experimental studies demonstrate the effectiveness of the estimators when the computational resources are insufﬁcient. They also demonstrate that in some cases reduced computational complexity is associated with robustness thereby increasing statistical accuracy. Keywords: Markov random ﬁelds, composite likelihood, maximum likelihood estimation</p><p>5 0.068052635 <a title="108-tfidf-5" href="./jmlr-2010-Consensus-Based_Distributed_Support_Vector_Machines.html">26 jmlr-2010-Consensus-Based Distributed Support Vector Machines</a></p>
<p>Author: Pedro A. Forero, Alfonso Cano, Georgios B. Giannakis</p><p>Abstract: This paper develops algorithms to train support vector machines when training data are distributed across different nodes, and their communication to a centralized processing unit is prohibited due to, for example, communication complexity, scalability, or privacy reasons. To accomplish this goal, the centralized linear SVM problem is cast as a set of decentralized convex optimization subproblems (one per node) with consensus constraints on the wanted classiﬁer parameters. Using the alternating direction method of multipliers, fully distributed training algorithms are obtained without exchanging training data among nodes. Different from existing incremental approaches, the overhead associated with inter-node communications is ﬁxed and solely dependent on the network topology rather than the size of the training sets available per node. Important generalizations to train nonlinear SVMs in a distributed fashion are also developed along with sequential variants capable of online processing. Simulated tests illustrate the performance of the novel algorithms.1 Keywords: support vector machine, distributed optimization, distributed data mining, distributed learning, sensor networks</p><p>6 0.058798324 <a title="108-tfidf-6" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>7 0.057349816 <a title="108-tfidf-7" href="./jmlr-2010-On_Learning_with_Integral_Operators.html">82 jmlr-2010-On Learning with Integral Operators</a></p>
<p>8 0.056426413 <a title="108-tfidf-8" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>9 0.054977566 <a title="108-tfidf-9" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>10 0.046004139 <a title="108-tfidf-10" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>11 0.044501629 <a title="108-tfidf-11" href="./jmlr-2010-Expectation_Truncation_and_the_Benefits_of_Preselection_In_Training_Generative_Models.html">38 jmlr-2010-Expectation Truncation and the Benefits of Preselection In Training Generative Models</a></p>
<p>12 0.040399287 <a title="108-tfidf-12" href="./jmlr-2010-Classification_with_Incomplete_Data_Using_Dirichlet_Process_Priors.html">23 jmlr-2010-Classification with Incomplete Data Using Dirichlet Process Priors</a></p>
<p>13 0.037670109 <a title="108-tfidf-13" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>14 0.036821254 <a title="108-tfidf-14" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>15 0.034205008 <a title="108-tfidf-15" href="./jmlr-2010-On_Finding_Predictors_for_Arbitrary_Families_of_Processes.html">81 jmlr-2010-On Finding Predictors for Arbitrary Families of Processes</a></p>
<p>16 0.031685844 <a title="108-tfidf-16" href="./jmlr-2010-Approximate_Riemannian_Conjugate_Gradient_Learning_for_Fixed-Form_Variational_Bayes.html">14 jmlr-2010-Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes</a></p>
<p>17 0.030326799 <a title="108-tfidf-17" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>18 0.029803516 <a title="108-tfidf-18" href="./jmlr-2010-A_Convergent_Online_Single_Time_Scale_Actor_Critic_Algorithm.html">2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</a></p>
<p>19 0.028004112 <a title="108-tfidf-19" href="./jmlr-2010-Chromatic_PAC-Bayes_Bounds_for_Non-IID_Data%3A_Applications_to_Ranking_and_Stationary_%CE%B2-Mixing_Processes.html">20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</a></p>
<p>20 0.027678104 <a title="108-tfidf-20" href="./jmlr-2010-Mean_Field_Variational_Approximation_for_Continuous-Time_Bayesian_Networks.html">75 jmlr-2010-Mean Field Variational Approximation for Continuous-Time Bayesian Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.143), (1, -0.038), (2, 0.025), (3, -0.039), (4, -0.123), (5, -0.022), (6, -0.016), (7, -0.024), (8, 0.017), (9, 0.022), (10, -0.08), (11, 0.105), (12, -0.104), (13, -0.203), (14, 0.015), (15, 0.031), (16, -0.042), (17, -0.093), (18, -0.076), (19, 0.146), (20, 0.421), (21, 0.22), (22, -0.053), (23, -0.117), (24, -0.129), (25, -0.158), (26, -0.029), (27, -0.067), (28, -0.022), (29, -0.026), (30, 0.112), (31, -0.062), (32, -0.046), (33, -0.07), (34, -0.082), (35, 0.076), (36, 0.159), (37, -0.005), (38, -0.135), (39, -0.007), (40, -0.104), (41, -0.076), (42, -0.017), (43, 0.002), (44, 0.058), (45, 0.071), (46, -0.073), (47, -0.032), (48, 0.006), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95595503 <a title="108-lsi-1" href="./jmlr-2010-Stochastic_Complexity_and_Generalization_Error_of_a_Restricted_Boltzmann_Machine_in_Bayesian_Estimation.html">108 jmlr-2010-Stochastic Complexity and Generalization Error of a Restricted Boltzmann Machine in Bayesian Estimation</a></p>
<p>Author: Miki Aoyagi</p><p>Abstract: In this paper, we consider the asymptotic form of the generalization error for the restricted Boltzmann machine in Bayesian estimation. It has been shown that obtaining the maximum pole of zeta functions is related to the asymptotic form of the generalization error for hierarchical learning models (Watanabe, 2001a,b). The zeta function is deﬁned by using a Kullback function. We use two methods to obtain the maximum pole: a new eigenvalue analysis method and a recursive blowing up process. We show that these methods are effective for obtaining the asymptotic form of the generalization error of hierarchical learning models. Keywords: Boltzmann machine, non-regular learning machine, resolution of singularities, zeta function</p><p>2 0.74414343 <a title="108-lsi-2" href="./jmlr-2010-Asymptotic_Equivalence_of_Bayes_Cross_Validation_and_Widely_Applicable_Information_Criterion_in_Singular_Learning_Theory.html">16 jmlr-2010-Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory</a></p>
<p>Author: Sumio Watanabe</p><p>Abstract: In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2λ/n, where λ is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion. Keywords: cross-validation, information criterion, singular learning machine, birational invariant</p><p>3 0.3159112 <a title="108-lsi-3" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>Author: Joshua V. Dillon, Guy Lebanon</p><p>Abstract: Maximum likelihood estimators are often of limited practical use due to the intensive computation they require. We propose a family of alternative estimators that maximize a stochastic variation of the composite likelihood function. Each of the estimators resolve the computation-accuracy tradeoff differently, and taken together they span a continuous spectrum of computation-accuracy tradeoff resolutions. We prove the consistency of the estimators, provide formulas for their asymptotic variance, statistical robustness, and computational complexity. We discuss experimental results in the context of Boltzmann machines and conditional random ﬁelds. The theoretical and experimental studies demonstrate the effectiveness of the estimators when the computational resources are insufﬁcient. They also demonstrate that in some cases reduced computational complexity is associated with robustness thereby increasing statistical accuracy. Keywords: Markov random ﬁelds, composite likelihood, maximum likelihood estimation</p><p>4 0.29278359 <a title="108-lsi-4" href="./jmlr-2010-Consistent_Nonparametric_Tests_of_Independence.html">27 jmlr-2010-Consistent Nonparametric Tests of Independence</a></p>
<p>Author: Arthur Gretton, László Györfi</p><p>Abstract: Three simple and explicit procedures for testing the independence of two multi-dimensional random variables are described. Two of the associated test statistics (L1 , log-likelihood) are deﬁned when the empirical distribution of the variables is restricted to ﬁnite partitions. A third test statistic is deﬁned as a kernel-based independence measure. Two kinds of tests are provided. Distributionfree strong consistent tests are derived on the basis of large deviation bounds on the test statistics: these tests make almost surely no Type I or Type II error after a random sample size. Asymptotically α-level tests are obtained from the limiting distribution of the test statistics. For the latter tests, the Type I error converges to a ﬁxed non-zero value α, and the Type II error drops to zero, for increasing sample size. All tests reject the null hypothesis of independence if the test statistics become large. The performance of the tests is evaluated experimentally on benchmark data. Keywords: hypothesis test, independence, L1, log-likelihood, kernel methods, distribution-free consistent test</p><p>5 0.27433023 <a title="108-lsi-5" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>Author: Fabian Sinz, Matthias Bethge</p><p>Abstract: In this paper, we introduce a new family of probability densities called L p -nested symmetric distributions. The common property, shared by all members of the new class, is the same functional form ˜ x x ρ(x ) = ρ( f (x )), where f is a nested cascade of L p -norms x p = (∑ |xi | p )1/p . L p -nested symmetric distributions thereby are a special case of ν-spherical distributions for which f is only required to be positively homogeneous of degree one. While both, ν-spherical and L p -nested symmetric distributions, contain many widely used families of probability models such as the Gaussian, spherically and elliptically symmetric distributions, L p -spherically symmetric distributions, and certain types of independent component analysis (ICA) and independent subspace analysis (ISA) models, ν-spherical distributions are usually computationally intractable. Here we demonstrate that L p nested symmetric distributions are still computationally feasible by deriving an analytic expression for its normalization constant, gradients for maximum likelihood estimation, analytic expressions for certain types of marginals, as well as an exact and efﬁcient sampling algorithm. We discuss the tight links of L p -nested symmetric distributions to well known machine learning methods such as ICA, ISA and mixed norm regularizers, and introduce the nested radial factorization algorithm (NRF), which is a form of non-linear ICA that transforms any linearly mixed, non-factorial L p nested symmetric source into statistically independent signals. As a corollary, we also introduce the uniform distribution on the L p -nested unit sphere. Keywords: parametric density model, symmetric distribution, ν-spherical distributions, non-linear independent component analysis, independent subspace analysis, robust Bayesian inference, mixed norm density model, uniform distributions on mixed norm spheres, nested radial factorization</p><p>6 0.26381907 <a title="108-lsi-6" href="./jmlr-2010-On_Learning_with_Integral_Operators.html">82 jmlr-2010-On Learning with Integral Operators</a></p>
<p>7 0.26220521 <a title="108-lsi-7" href="./jmlr-2010-Consensus-Based_Distributed_Support_Vector_Machines.html">26 jmlr-2010-Consensus-Based Distributed Support Vector Machines</a></p>
<p>8 0.2333346 <a title="108-lsi-8" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>9 0.23130532 <a title="108-lsi-9" href="./jmlr-2010-Expectation_Truncation_and_the_Benefits_of_Preselection_In_Training_Generative_Models.html">38 jmlr-2010-Expectation Truncation and the Benefits of Preselection In Training Generative Models</a></p>
<p>10 0.20451802 <a title="108-lsi-10" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>11 0.20390521 <a title="108-lsi-11" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>12 0.20133707 <a title="108-lsi-12" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>13 0.1993386 <a title="108-lsi-13" href="./jmlr-2010-Approximate_Riemannian_Conjugate_Gradient_Learning_for_Fixed-Form_Variational_Bayes.html">14 jmlr-2010-Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes</a></p>
<p>14 0.16774382 <a title="108-lsi-14" href="./jmlr-2010-A_Convergent_Online_Single_Time_Scale_Actor_Critic_Algorithm.html">2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</a></p>
<p>15 0.16292156 <a title="108-lsi-15" href="./jmlr-2010-On_Finding_Predictors_for_Arbitrary_Families_of_Processes.html">81 jmlr-2010-On Finding Predictors for Arbitrary Families of Processes</a></p>
<p>16 0.15830857 <a title="108-lsi-16" href="./jmlr-2010-Classification_with_Incomplete_Data_Using_Dirichlet_Process_Priors.html">23 jmlr-2010-Classification with Incomplete Data Using Dirichlet Process Priors</a></p>
<p>17 0.14447023 <a title="108-lsi-17" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>18 0.14366284 <a title="108-lsi-18" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>19 0.14018664 <a title="108-lsi-19" href="./jmlr-2010-Optimal_Search_on_Clustered_Structural_Constraint_for_Learning_Bayesian_Network_Structure.html">88 jmlr-2010-Optimal Search on Clustered Structural Constraint for Learning Bayesian Network Structure</a></p>
<p>20 0.13854396 <a title="108-lsi-20" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.045), (3, 0.042), (4, 0.01), (8, 0.02), (12, 0.015), (21, 0.012), (22, 0.011), (32, 0.043), (35, 0.029), (36, 0.036), (37, 0.046), (57, 0.413), (75, 0.123), (81, 0.015), (85, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.68005139 <a title="108-lda-1" href="./jmlr-2010-Stochastic_Complexity_and_Generalization_Error_of_a_Restricted_Boltzmann_Machine_in_Bayesian_Estimation.html">108 jmlr-2010-Stochastic Complexity and Generalization Error of a Restricted Boltzmann Machine in Bayesian Estimation</a></p>
<p>Author: Miki Aoyagi</p><p>Abstract: In this paper, we consider the asymptotic form of the generalization error for the restricted Boltzmann machine in Bayesian estimation. It has been shown that obtaining the maximum pole of zeta functions is related to the asymptotic form of the generalization error for hierarchical learning models (Watanabe, 2001a,b). The zeta function is deﬁned by using a Kullback function. We use two methods to obtain the maximum pole: a new eigenvalue analysis method and a recursive blowing up process. We show that these methods are effective for obtaining the asymptotic form of the generalization error of hierarchical learning models. Keywords: Boltzmann machine, non-regular learning machine, resolution of singularities, zeta function</p><p>2 0.35332093 <a title="108-lda-2" href="./jmlr-2010-Asymptotic_Equivalence_of_Bayes_Cross_Validation_and_Widely_Applicable_Information_Criterion_in_Singular_Learning_Theory.html">16 jmlr-2010-Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory</a></p>
<p>Author: Sumio Watanabe</p><p>Abstract: In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2λ/n, where λ is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion. Keywords: cross-validation, information criterion, singular learning machine, birational invariant</p><p>3 0.3500315 <a title="108-lda-3" href="./jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</a></p>
<p>Author: Alexander Ilin, Tapani Raiko</p><p>Abstract: Principal component analysis (PCA) is a classical data analysis technique that Ä?Ĺš nds linear transformations of data that retain the maximal amount of variance. We study a case where some of the data values are missing, and show that this problem has many features which are usually associated with nonlinear models, such as overÄ?Ĺš tting and bad locally optimal solutions. A probabilistic formulation of PCA provides a good foundation for handling missing values, and we provide formulas for doing that. In case of high dimensional and very sparse data, overÄ?Ĺš tting becomes a severe problem and traditional algorithms for PCA are very slow. We introduce a novel fast algorithm and extend it to variational Bayesian learning. Different versions of PCA are compared in artiÄ?Ĺš cial experiments, demonstrating the effects of regularization and modeling of posterior variance. The scalability of the proposed algorithm is demonstrated by applying it to the NetÄ?Ĺš&sbquo;ix problem. Keywords: principal component analysis, missing values, overÄ?Ĺš tting, regularization, variational Bayes</p><p>4 0.34628034 <a title="108-lda-4" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>Author: Ming Yuan</p><p>Abstract: This paper considers the problem of estimating a high dimensional inverse covariance matrix that can be well approximated by “sparse” matrices. Taking advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix, we propose an estimating procedure that can effectively exploit such “sparsity”. The proposed method can be computed using linear programming and therefore has the potential to be used in very high dimensional problems. Oracle inequalities are established for the estimation error in terms of several operator norms, showing that the method is adaptive to different types of sparsity of the problem. Keywords: covariance selection, Dantzig selector, Gaussian graphical model, inverse covariance matrix, Lasso, linear programming, oracle inequality, sparsity</p><p>5 0.34536663 <a title="108-lda-5" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>Author: Joshua V. Dillon, Guy Lebanon</p><p>Abstract: Maximum likelihood estimators are often of limited practical use due to the intensive computation they require. We propose a family of alternative estimators that maximize a stochastic variation of the composite likelihood function. Each of the estimators resolve the computation-accuracy tradeoff differently, and taken together they span a continuous spectrum of computation-accuracy tradeoff resolutions. We prove the consistency of the estimators, provide formulas for their asymptotic variance, statistical robustness, and computational complexity. We discuss experimental results in the context of Boltzmann machines and conditional random ﬁelds. The theoretical and experimental studies demonstrate the effectiveness of the estimators when the computational resources are insufﬁcient. They also demonstrate that in some cases reduced computational complexity is associated with robustness thereby increasing statistical accuracy. Keywords: Markov random ﬁelds, composite likelihood, maximum likelihood estimation</p><p>6 0.33692864 <a title="108-lda-6" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>7 0.33668903 <a title="108-lda-7" href="./jmlr-2010-Spectral_Regularization_Algorithms_for_Learning_Large_Incomplete_Matrices.html">105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</a></p>
<p>8 0.33395213 <a title="108-lda-8" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>9 0.33334094 <a title="108-lda-9" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>10 0.32997566 <a title="108-lda-10" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>11 0.32890809 <a title="108-lda-11" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>12 0.3285805 <a title="108-lda-12" href="./jmlr-2010-Linear_Algorithms_for_Online_Multitask_Classification.html">66 jmlr-2010-Linear Algorithms for Online Multitask Classification</a></p>
<p>13 0.32820827 <a title="108-lda-13" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>14 0.32811522 <a title="108-lda-14" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>15 0.32787791 <a title="108-lda-15" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>16 0.32766593 <a title="108-lda-16" href="./jmlr-2010-On_Spectral_Learning.html">84 jmlr-2010-On Spectral Learning</a></p>
<p>17 0.32643977 <a title="108-lda-17" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>18 0.32630703 <a title="108-lda-18" href="./jmlr-2010-A_Quasi-Newton_Approach_to_Nonsmooth_Convex_Optimization_Problems_in_Machine_Learning.html">5 jmlr-2010-A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning</a></p>
<p>19 0.32608783 <a title="108-lda-19" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>20 0.32599229 <a title="108-lda-20" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
