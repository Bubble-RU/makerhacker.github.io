<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-9" href="#">jmlr2010-9</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</h1>
<br/><p>Source: <a title="jmlr-2010-9-pdf" href="http://jmlr.org/papers/volume11/strumbelj10a/strumbelj10a.pdf">pdf</a></p><p>Author: Erik Ĺ trumbelj, Igor Kononenko</p><p>Abstract: We present a general method for explaining individual predictions of classiﬁcation models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method’s initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efﬁcient and that the explanations are intuitive and useful. Keywords: data postprocessing, classiﬁcation, explanation, visualization</p><p>Reference: <a title="jmlr-2010-9-reference" href="../jmlr2010_reference/jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. [sent-8, score-0.866]
</p><p>2 Data postprocessing includes the integration, ﬁltering, evaluation, and explanation of acquired knowledge. [sent-16, score-0.539]
</p><p>3 To introduce the reader with some of the concepts used in this paper, we start with a simple illustrative example of an explanation for a model’s prediction (see Fig. [sent-18, score-0.685]
</p><p>4 In our example, the contributions of the three feature values can be interpreted as follows. [sent-23, score-0.266]
</p><p>5 This is a trivial example, but providing the end-user with such an explanation on top of a prediction, makes the prediction easier to understand and to trust. [sent-28, score-0.585]
</p><p>6 1 is speciﬁc to Naive Bayes, but can we design an explanation method which works for any type of classiﬁer? [sent-34, score-0.497]
</p><p>7 Figure 1: An instance from the well-known Titanic data set with the Naive Bayes model’s prediction and an explanation in the form of contributions of individual feature values. [sent-36, score-0.933]
</p><p>8 1 Related Work Before addressing general explanation methods, we list a few model-speciﬁc methods to emphasize two things. [sent-42, score-0.497]
</p><p>9 And second, providing an explanation in the form of contributions of feature values is a common approach. [sent-44, score-0.763]
</p><p>10 Note that many more model-speciﬁc explanation methods exist and this is far from being a complete reference. [sent-45, score-0.497]
</p><p>11 Nomograms are a way of visualizing contributions of feature values and were applied to Naive Bayes (Moˇ ina et al. [sent-49, score-0.312]
</p><p>12 The explanation and interpretation of artiﬁcial neural networks, which are arguably one of the least transparent models, has also received a lot of attention, especially in the form of rule extraction (Towell and Shavlik, 1993; Andrews et al. [sent-57, score-0.528]
</p><p>13 So, why do we even need a general explanation method? [sent-59, score-0.497]
</p><p>14 It is not difﬁcult to think of a reasonable scenario where a general explanation method would be useful. [sent-60, score-0.497]
</p><p>15 For example, imagine a user using a classiﬁer and a corresponding explanation method. [sent-61, score-0.497]
</p><p>16 The user then has to invest time and effort into adapting to the new explanation method. [sent-63, score-0.497]
</p><p>17 This can be avoided by using a general explanation method. [sent-64, score-0.497]
</p><p>18 Overall, a good general explanation method reduces the dependence between the user-end and the underlying machine learning methods, which makes work with machine learning models more user-friendly. [sent-65, score-0.497]
</p><p>19 An effective and efﬁcient general explanation method would also be a useful tool for comparing how a model predicts different instances and how different models predict the same instance. [sent-67, score-0.599]
</p><p>20 As far as the authors are aware, there exist two other general explanation methods for explaining ˇ a model’s prediction: the work by Robnik-Sikonja and Kononenko (2008) and the work by Lemaire et al. [sent-68, score-0.603]
</p><p>21 While there are several differences between the two methods, both explain a prediction with contributions of feature values and both use the same basic approach. [sent-70, score-0.354]
</p><p>22 A feature value’s contribution is deﬁned as the difference between the model’s initial prediction and its average prediction across perturbations of the corresponding feature. [sent-71, score-0.373]
</p><p>23 This is, of course, an incorrect explanation of how these two values contribute to the persons decision. [sent-85, score-0.588]
</p><p>24 To summarize, we have existing general explanation methods, which sacriﬁce a part of their effectiveness for efﬁciency, and we know that generating effective contributions requires observing the power set of all features, which is far from efﬁcient. [sent-89, score-0.639]
</p><p>25 First, we provide a rigorous theoretical analysis of our explanation method and link it with known concepts from game theory, thus formalizing some of its desirable properties. [sent-91, score-0.801]
</p><p>26 Section 2 introduces some basic concepts from classiﬁcation and coalitional game theory. [sent-94, score-0.521]
</p><p>27 Preliminaries First, we introduce some basic concepts from classiﬁcation and coalitional game theory, which are used in the formal description of our explanation method. [sent-99, score-1.018]
</p><p>28 1 Classiﬁcation In machine learning classiﬁcation is a form of supervised learning where the objective is to predict the class label for unlabelled input instances, each described by feature values from a feature space. [sent-101, score-0.277]
</p><p>29 × An , where each feature Ai is a ﬁnite set of feature values. [sent-109, score-0.248]
</p><p>30 2 Coalitional Game Theory The following concepts from coalitional game theory are used in the formalization of our method, starting with the deﬁnition of a coalitional game. [sent-123, score-0.738]
</p><p>31 Deﬁnition 3 A coalitional form game is a tuple N, v , where N = {1, 2, . [sent-124, score-0.421]
</p><p>32 We start with a description of the intuition behind the method and then link it with coalitional game theory. [sent-156, score-0.421]
</p><p>33 Our goal is to explain how the given feature values contribute to the prediction difference between the classiﬁers prediction for this instance and the expected prediction if no feature values are given (that is, if all feature values are ”ignored”). [sent-169, score-0.726]
</p><p>34 , 2009) we used a different deﬁnition: ∆(S) = ∗ (S) − f ∗ (∅), where f ∗ (W ) is obtained by retraining the classiﬁer only on features in W and fc c re-classifying the instance (similar to the wrappers approach in feature selection Kohavi and John 1997). [sent-176, score-0.322]
</p><p>35 The expression ∆(S) is the difference between the expected prediction when we know only those values of x, whose features are in S, and the expected prediction when no feature values are known. [sent-179, score-0.358]
</p><p>36 The main shortcoming of existing general explanation methods is that they do not take into account all the potential dependencies and interactions between feature values. [sent-183, score-0.689]
</p><p>37 To avoid this issue, we implicitly deﬁne interactions by deﬁning that each prediction difference ∆(S) is composed of 2N contributions of interactions (that is, each subset of feature values might contribute something): ∆(S) =  ∑ I (W ),  S ⊆ N. [sent-184, score-0.535]
</p><p>38 (3)  W ⊂S  Now we distribute the interaction contributions among the n feature values. [sent-187, score-0.311]
</p><p>39 For each interaction the involved feature values can be treated as equally responsible for the interaction as the interaction would otherwise not exist. [sent-188, score-0.259]
</p><p>40 , n}, ∆ is a coalitional form game and ϕ(∆) = (ϕ1 , ϕ2 , . [sent-201, score-0.421]
</p><p>41 Proof Following the deﬁnition of ∆ we get that ∆(∅) = 0, so the explanation of the classiﬁer’s prediction can be treated as a coalitional form game N, ∆ . [sent-205, score-1.006]
</p><p>42 Now we provide an elementary proof that the contributions of individual feature values correspond to the Shapley value for the game 6  E XPLAINING I NDIVIDUAL C LASSIFICATIONS  N, ∆ . [sent-206, score-0.507]
</p><p>43 S⊆N\{i}  ∑  So, the explanation method can be interpreted as follows. [sent-239, score-0.497]
</p><p>44 We divide this change amongst the feature values in a way that is fair to their contributions across all possible sub-coalitions. [sent-241, score-0.295]
</p><p>45 Axioms 1 to 3 and their interpretation in the context of our explanation method are of particular interest. [sent-243, score-0.497]
</p><p>46 (2) - the 7  ˇ S TRUMBELJ AND KONONENKO  sum of all n contributions in an instance’s explanation is equal to the difference in prediction ∆(N). [sent-245, score-0.727]
</p><p>47 According to the 2nd axiom, if two features values have an identical inﬂuence on the prediction they are assigned contributions of equal size. [sent-247, score-0.288]
</p><p>48 The 3rd axiom says that if a feature has no inﬂuence on the prediction it is assigned a contribution of 0. [sent-248, score-0.426]
</p><p>49 When viewed together, these properties ensure that any effect the features might have on the classiﬁers output will be reﬂected in the generated contributions, which effectively deals with the issues of previous general explanation methods. [sent-249, score-0.555]
</p><p>50 1 A N I LLUSTRATIVE E XAMPLE In the introduction we used a simple boolean logic example to illustrate the shortcomings of existing general explanation methods. [sent-252, score-0.497]
</p><p>51 With the same example we illustrate how our explanation method works. [sent-255, score-0.497]
</p><p>52 Intuitively, ∆(S) is the difference between the classiﬁers expected prediction if only values of features in S are known and the expected prediction if no values are known. [sent-260, score-0.234]
</p><p>53 When observed together, the two features contribute less than their individual contributions would suggest, which 1 results in a negative interaction: I (1, 2) = ∆(1, 2) − (I (1) + I (2)) = − 4 . [sent-266, score-0.282]
</p><p>54 The 2 8 2 8 generated contributions reveal that both features contribute the same amount towards the prediction being 1 and the contributions sum up to the initial difference between the prediction for this instance and the prior belief. [sent-268, score-0.658]
</p><p>55 To avoid this and still achieve an efﬁcient explanation method, we extend the sampling algorithm in the following way. [sent-289, score-0.497]
</p><p>56 The optimal Z2  ·σ2  (minimal) number of samples we need for the entire explanation is: mmin ( 1 − α, e ) = n · 1−α , e2 1 where σ2 = n ∑n σ2 . [sent-328, score-0.531]
</p><p>57 When explaining an instance, the sampling process has to be repeated for each of the n feature values. [sent-331, score-0.23]
</p><p>58 Therefore, for a given error and conﬁdence level, the time complexity of the explanation is O(n · T (A )), where function T (A ) describes the instance classiﬁcation time of the model on A . [sent-332, score-0.571]
</p><p>59 We use a variety of different classiﬁers both to illustrate that it is indeed a general explanation method and to investigate how the method behaves with different types of classiﬁcation models. [sent-336, score-0.497]
</p><p>60 All experiments were done on an off-the-shelf laptop computer (2GHz dual core CPU, 2GB RAM), the explanation method is a straightforward Java implementation of the equations presented in this paper, and the classiﬁers were imported from the Weka machine learning software (http://www. [sent-338, score-0.497]
</p><p>61 The ﬁrst 8 data sets are ˇ synthetic data sets, designed speciﬁcally for testing explanation methods (see Robnik-Sikonja and ˇ Kononenko, 2008; Strumbelj et al. [sent-415, score-0.497]
</p><p>62 Those interested in a more detailed description of this data set and how our previous explanation method is successfully applied in practice can refer to ˇ our previous work (Strumbelj et al. [sent-420, score-0.497]
</p><p>63 For each data set we use half of the instances for training and half for testing the explanation method. [sent-427, score-0.538]
</p><p>64 For each data set/classiﬁer pair, we train the classiﬁer on the training set and use both the explanation method and its approximation on each test instance. [sent-428, score-0.497]
</p><p>65 When over 10000 samples are drawn, all the contributions across all features and all test instances are very close to the actual contributions. [sent-441, score-0.304]
</p><p>66 And although the explanations would not tell us much about the concepts behind the data set (we conclude from the model’s performance, that it’s knowledge is useless), they would reveal what the model has learned, which is the purpose of an explanation method. [sent-479, score-0.802]
</p><p>67 Explaining the prediction of the ANN model for an instance Monks2 is the most complex explanation (that is, requires the most samples - see Fig. [sent-487, score-0.693]
</p><p>68 3, which shows the time needed to provide an explanation with the desired error 99%, 0. [sent-491, score-0.497]
</p><p>69 For smaller data sets (smaller in the number of features) the explanation is generated almost instantly. [sent-493, score-0.497]
</p><p>70 For larger data sets, generating an explanation takes less than a minute, with the exception of bstNB on a few data sets and the ANN model on the Mushroom data set. [sent-494, score-0.526]
</p><p>71 The Arrhythmia data set, with its 279 features, is an example of a data set, where the explanation can not be generated in some sensible time. [sent-496, score-0.497]
</p><p>72 For example, it takes more than an hour to generate an explanation for a prediction of the bstNB model. [sent-497, score-0.585]
</p><p>73 The explanation method is therefore less appropriate for explaining models which are built on several hundred features or more. [sent-498, score-0.661]
</p><p>74 Arguably, providing a comprehensible explanation involving a hundred or more features is a problem in its own right and even inherently transparend models become less comprehensible with such a large number of features. [sent-499, score-0.663]
</p><p>75 However, the focus of this paper is on providing an effective and general explanation method, which is computationally feasible on the majority of data sets we encounter. [sent-500, score-0.497]
</p><p>76 Therefore, when considering the number of features the explanation method can still handle, we need not count irrelevant features, which are not included in the ﬁnal model. [sent-502, score-0.586]
</p><p>77 An additional advantage of the generated contributions is that they sum up to the difference between the model’s output prediction and the model’s expected output, given no information about the values of the features. [sent-514, score-0.23]
</p><p>78 They were generated for various classiﬁcation models and data sets, to show the advantage of having a general explanation method. [sent-517, score-0.497]
</p><p>79 However, both NB and bstDT learn the importance of the ﬁfth feature and explanations reveal that value 2 for the ﬁfth feature speaks against class 1. [sent-523, score-0.424]
</p><p>80 The explanations reveal that DT predicts this animal is a bird, because it has feathers. [sent-528, score-0.239]
</p><p>81 These ﬁrst two pairs of examples illustrate how the explanations reﬂect what the model has learnt and how we can compare explanations from different classiﬁers. [sent-532, score-0.281]
</p><p>82 Some examples of underﬁtting and overﬁtting are actually desirable as they allow us to inspect if the explanation method reveals what the classiﬁer has (or has not) learned. [sent-534, score-0.497]
</p><p>83 6(a) shows the explanation of the logREG model’s prediction for an instance from the Xor data set. [sent-536, score-0.63]
</p><p>84 or concept of this data set (for this data set the class label is the odd parity bit for the ﬁrst three feature values) and the explanation is appropriate. [sent-541, score-0.65]
</p><p>85 7(a) is an explanation for ANN’s prediction for the introductory instance from the Titanic data set (see Fig. [sent-545, score-0.63]
</p><p>86 7(b)) is very similar to the inherent explanation (taking into account that a logarithm is applied in the inherent explanation). [sent-548, score-0.553]
</p><p>87 8 is an explanation for RT’s prediction regarding whether breast cancer will (class = 1) or will not (class = 2) recur for this patient. [sent-552, score-0.616]
</p><p>88 According to RF it is more likely that cancer will not recur and the explanation indicates that this is mostly due to a low number of positive lymph nodes (nLymph). [sent-553, score-0.559]
</p><p>89 Conclusion In the introductive section, we asked if an efﬁcient and effective general explanation method for classiﬁers’ predictions can be made. [sent-558, score-0.539]
</p><p>90 Using only the input and output of a classiﬁer we decompose the changes in its prediction into contributions of individual feature values. [sent-560, score-0.391]
</p><p>91 These contributions correspond to known concepts from coalitional game theory. [sent-561, score-0.663]
</p><p>92 Unlike with existing methods, the resulting theoretical properties of the proposed method guarantee that no matter which concepts the classiﬁer learns, the generated contributions will reveal the inﬂuence of feature values. [sent-562, score-0.416]
</p><p>93 As we show 15  ˇ S TRUMBELJ AND KONONENKO  (a) logREG model  (b) SVM model  Figure 6: The left hand side explanation indicates that the feature values have no signiﬁcant inﬂuence on the logREG model on the Xor data set. [sent-564, score-0.708]
</p><p>94 The right hand side explanation shows how SVM overﬁts the Random data set. [sent-565, score-0.497]
</p><p>95 The left hand side explanation is for the ANN model. [sent-567, score-0.497]
</p><p>96 The right hand side explanation is for the NB model. [sent-568, score-0.497]
</p><p>97 Figure 8: An explanation or the RF model’s prediction for a patient from the Oncology data set. [sent-569, score-0.585]
</p><p>98 It would also be interesting to explore the possibility of applying the same principles to the explanation of regression models. [sent-574, score-0.497]
</p><p>99 Polynomial calculation of the shapley value based on o sampling. [sent-593, score-0.253]
</p><p>100 Explaining instance classiﬁcations with interactions of subsets of feature values. [sent-663, score-0.237]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('explanation', 0.497), ('shapley', 0.253), ('kononenko', 0.231), ('coalitional', 0.217), ('game', 0.204), ('axiom', 0.17), ('prei', 0.163), ('trumbelj', 0.163), ('lassifications', 0.145), ('xplaining', 0.145), ('contributions', 0.142), ('explanations', 0.126), ('feature', 0.124), ('ndividual', 0.124), ('strumbelj', 0.124), ('nb', 0.113), ('logreg', 0.108), ('igor', 0.108), ('explaining', 0.106), ('concepts', 0.1), ('ann', 0.099), ('bstdt', 0.09), ('condind', 0.09), ('prediction', 0.088), ('oncology', 0.083), ('naive', 0.081), ('rf', 0.077), ('xor', 0.077), ('bstnb', 0.072), ('passenger', 0.072), ('interactions', 0.068), ('classi', 0.067), ('er', 0.065), ('players', 0.062), ('titanic', 0.06), ('features', 0.058), ('retraining', 0.056), ('coalition', 0.054), ('comprehensible', 0.054), ('disjunct', 0.054), ('nomograms', 0.054), ('zoo', 0.051), ('reveal', 0.05), ('sphere', 0.05), ('erik', 0.048), ('ina', 0.046), ('mushroom', 0.046), ('persons', 0.046), ('bayes', 0.046), ('instance', 0.045), ('interaction', 0.045), ('contribute', 0.045), ('contribution', 0.044), ('survival', 0.043), ('predictions', 0.042), ('population', 0.042), ('mo', 0.042), ('postprocessing', 0.042), ('dt', 0.041), ('instances', 0.041), ('contributes', 0.039), ('fc', 0.039), ('chess', 0.038), ('visualization', 0.038), ('svm', 0.038), ('individual', 0.037), ('ignored', 0.037), ('bla', 0.036), ('eytan', 0.036), ('fri', 0.036), ('invasion', 0.036), ('jakulin', 0.036), ('keinan', 0.036), ('ljubljana', 0.036), ('marko', 0.036), ('martens', 0.036), ('moretti', 0.036), ('szafron', 0.036), ('towell', 0.036), ('samples', 0.034), ('decision', 0.033), ('predicts', 0.032), ('ers', 0.032), ('irrelevant', 0.031), ('janez', 0.031), ('nursery', 0.031), ('andrews', 0.031), ('animal', 0.031), ('lymph', 0.031), ('recur', 0.031), ('soybean', 0.031), ('transparent', 0.031), ('kohavi', 0.031), ('uence', 0.029), ('across', 0.029), ('label', 0.029), ('model', 0.029), ('inherent', 0.028), ('castro', 0.028), ('bird', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="9-tfidf-1" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>Author: Erik Ĺ trumbelj, Igor Kononenko</p><p>Abstract: We present a general method for explaining individual predictions of classiﬁcation models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method’s initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efﬁcient and that the explanations are intuitive and useful. Keywords: data postprocessing, classiﬁcation, explanation, visualization</p><p>2 0.29846248 <a title="9-tfidf-2" href="./jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">48 jmlr-2010-How to Explain Individual Classification Decisions</a></p>
<p>Author: David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, Klaus-Robert Müller</p><p>Abstract: After building a classiﬁer with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most inﬂuential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classiﬁcation method. Keywords: explaining, nonlinear, black box model, kernel methods, Ames mutagenicity</p><p>3 0.092948891 <a title="9-tfidf-3" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>Author: Markus Ojala, Gemma C. Garriga</p><p>Abstract: We explore the framework of permutation-based p-values for assessing the performance of classiﬁers. In this paper we study two simple permutation tests. The ﬁrst test assess whether the classiﬁer has found a real class structure in the data; the corresponding null distribution is estimated by permuting the labels in the data. This test has been used extensively in classiﬁcation problems in computational biology. The second test studies whether the classiﬁer is exploiting the dependency between the features in classiﬁcation; the corresponding null distribution is estimated by permuting the features within classes, inspired by restricted randomization techniques traditionally used in statistics. This new test can serve to identify descriptive features which can be valuable information in improving the classiﬁer performance. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classiﬁer performance via permutation tests is effective. In particular, the restricted permutation test clearly reveals whether the classiﬁer exploits the interdependency between the features in the data. Keywords: classiﬁcation, labeled data, permutation tests, restricted randomization, signiﬁcance testing</p><p>4 0.080508441 <a title="9-tfidf-4" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>Author: Jean-Yves Audibert, Sébastien Bubeck</p><p>Abstract: This work deals with four classical prediction settings, namely full information, bandit, label efﬁcient and bandit label efﬁcient as well as four different notions of regret: pseudo-regret, expected regret, high probability regret and tracking the best expert regret. We introduce a new forecaster, INF (Implicitly Normalized Forecaster) based on an arbitrary function ψ for which we propose a uniﬁed γ analysis of its pseudo-regret in the four games we consider. In particular, for ψ(x) = exp(ηx) + K , INF reduces to the classical exponentially weighted average forecaster and our analysis of the pseudo-regret recovers known results while for the expected regret we slightly tighten the bounds. γ η q On the other hand with ψ(x) = −x + K , which deﬁnes a new forecaster, we are able to remove the extraneous logarithmic factor in the pseudo-regret bounds for bandits games, and thus ﬁll in a long open gap in the characterization of the minimax rate for the pseudo-regret in the bandit game. We also provide high probability bounds depending on the cumulative reward of the optimal action. Finally, we consider the stochastic bandit game, and prove that an appropriate modiﬁcation of the upper conﬁdence bound policy UCB1 (Auer et al., 2002a) achieves the distribution-free optimal rate while still having a distribution-dependent rate logarithmic in the number of plays. Keywords: Bandits (adversarial and stochastic), regret bound, minimax rate, label efﬁcient, upper conﬁdence bound (UCB) policy, online learning, prediction with limited feedback.</p><p>5 0.0549003 <a title="9-tfidf-5" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>Author: Franz Pernkopf, Jeff A. Bilmes</p><p>Abstract: We introduce a simple order-based greedy heuristic for learning discriminative structure within generative Bayesian network classiﬁers. We propose two methods for establishing an order of N features. They are based on the conditional mutual information and classiﬁcation rate (i.e., risk), respectively. Given an ordering, we can ﬁnd a discriminative structure with O N k+1 score evaluations (where constant k is the tree-width of the sub-graph over the attributes). We present results on 25 data sets from the UCI repository, for phonetic classiﬁcation using the TIMIT database, for a visual surface inspection task, and for two handwritten digit recognition tasks. We provide classiﬁcation performance for both discriminative and generative parameter learning on both discriminatively and generatively structured networks. The discriminative structure found by our new procedures signiﬁcantly outperforms generatively produced structures, and achieves a classiﬁcation accuracy on par with the best discriminative (greedy) Bayesian network learning approach, but does so with a factor of ∼10-40 speedup. We also show that the advantages of generative discriminatively structured Bayesian network classiﬁers still hold in the case of missing features, a case where generative classiﬁers have an advantage over discriminative classiﬁers. Keywords: Bayesian networks, classiﬁcation, discriminative learning, structure learning, graphical model, missing feature</p><p>6 0.054179519 <a title="9-tfidf-6" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>7 0.052261017 <a title="9-tfidf-7" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>8 0.045748249 <a title="9-tfidf-8" href="./jmlr-2010-On_Over-fitting_in_Model_Selection_and_Subsequent_Selection_Bias_in_Performance_Evaluation.html">83 jmlr-2010-On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</a></p>
<p>9 0.045135744 <a title="9-tfidf-9" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>10 0.04392533 <a title="9-tfidf-10" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>11 0.042186461 <a title="9-tfidf-11" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>12 0.039534505 <a title="9-tfidf-12" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>13 0.036447499 <a title="9-tfidf-13" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_I%3A_Algorithms_and_Empirical_Evaluation.html">67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</a></p>
<p>14 0.034833293 <a title="9-tfidf-14" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_II%3A_Analysis_and_Extensions.html">68 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions</a></p>
<p>15 0.034556858 <a title="9-tfidf-15" href="./jmlr-2010-Why_Does_Unsupervised_Pre-training_Help_Deep_Learning%3F.html">117 jmlr-2010-Why Does Unsupervised Pre-training Help Deep Learning?</a></p>
<p>16 0.032934658 <a title="9-tfidf-16" href="./jmlr-2010-WEKA%E2%88%92Experiences_with_a_Java_Open-Source_Project.html">116 jmlr-2010-WEKA−Experiences with a Java Open-Source Project</a></p>
<p>17 0.032231066 <a title="9-tfidf-17" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>18 0.031082543 <a title="9-tfidf-18" href="./jmlr-2010-Matched_Gene_Selection_and_Committee_Classifier_for_Molecular_Classification_of_Heterogeneous_Diseases.html">71 jmlr-2010-Matched Gene Selection and Committee Classifier for Molecular Classification of Heterogeneous Diseases</a></p>
<p>19 0.030780617 <a title="9-tfidf-19" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<p>20 0.030604335 <a title="9-tfidf-20" href="./jmlr-2010-Evolving_Static_Representations_for_Task_Transfer.html">37 jmlr-2010-Evolving Static Representations for Task Transfer</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.176), (1, 0.07), (2, -0.043), (3, 0.125), (4, 0.011), (5, 0.199), (6, 0.027), (7, 0.059), (8, -0.037), (9, 0.017), (10, 0.382), (11, 0.085), (12, -0.16), (13, 0.083), (14, 0.384), (15, -0.153), (16, -0.023), (17, 0.079), (18, 0.042), (19, 0.25), (20, 0.081), (21, -0.183), (22, -0.034), (23, -0.144), (24, 0.178), (25, 0.075), (26, -0.039), (27, -0.024), (28, -0.015), (29, -0.026), (30, 0.04), (31, 0.012), (32, 0.066), (33, 0.024), (34, -0.005), (35, -0.017), (36, 0.043), (37, 0.02), (38, -0.048), (39, 0.026), (40, 0.027), (41, 0.05), (42, -0.018), (43, 0.026), (44, 0.017), (45, -0.006), (46, 0.044), (47, 0.012), (48, 0.005), (49, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93877023 <a title="9-lsi-1" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>Author: Erik Ĺ trumbelj, Igor Kononenko</p><p>Abstract: We present a general method for explaining individual predictions of classiﬁcation models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method’s initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efﬁcient and that the explanations are intuitive and useful. Keywords: data postprocessing, classiﬁcation, explanation, visualization</p><p>2 0.89552051 <a title="9-lsi-2" href="./jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">48 jmlr-2010-How to Explain Individual Classification Decisions</a></p>
<p>Author: David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, Klaus-Robert Müller</p><p>Abstract: After building a classiﬁer with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most inﬂuential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classiﬁcation method. Keywords: explaining, nonlinear, black box model, kernel methods, Ames mutagenicity</p><p>3 0.33170691 <a title="9-lsi-3" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>Author: Markus Ojala, Gemma C. Garriga</p><p>Abstract: We explore the framework of permutation-based p-values for assessing the performance of classiﬁers. In this paper we study two simple permutation tests. The ﬁrst test assess whether the classiﬁer has found a real class structure in the data; the corresponding null distribution is estimated by permuting the labels in the data. This test has been used extensively in classiﬁcation problems in computational biology. The second test studies whether the classiﬁer is exploiting the dependency between the features in classiﬁcation; the corresponding null distribution is estimated by permuting the features within classes, inspired by restricted randomization techniques traditionally used in statistics. This new test can serve to identify descriptive features which can be valuable information in improving the classiﬁer performance. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classiﬁer performance via permutation tests is effective. In particular, the restricted permutation test clearly reveals whether the classiﬁer exploits the interdependency between the features in the data. Keywords: classiﬁcation, labeled data, permutation tests, restricted randomization, signiﬁcance testing</p><p>4 0.24637002 <a title="9-lsi-4" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>Author: Franz Pernkopf, Jeff A. Bilmes</p><p>Abstract: We introduce a simple order-based greedy heuristic for learning discriminative structure within generative Bayesian network classiﬁers. We propose two methods for establishing an order of N features. They are based on the conditional mutual information and classiﬁcation rate (i.e., risk), respectively. Given an ordering, we can ﬁnd a discriminative structure with O N k+1 score evaluations (where constant k is the tree-width of the sub-graph over the attributes). We present results on 25 data sets from the UCI repository, for phonetic classiﬁcation using the TIMIT database, for a visual surface inspection task, and for two handwritten digit recognition tasks. We provide classiﬁcation performance for both discriminative and generative parameter learning on both discriminatively and generatively structured networks. The discriminative structure found by our new procedures signiﬁcantly outperforms generatively produced structures, and achieves a classiﬁcation accuracy on par with the best discriminative (greedy) Bayesian network learning approach, but does so with a factor of ∼10-40 speedup. We also show that the advantages of generative discriminatively structured Bayesian network classiﬁers still hold in the case of missing features, a case where generative classiﬁers have an advantage over discriminative classiﬁers. Keywords: Bayesian networks, classiﬁcation, discriminative learning, structure learning, graphical model, missing feature</p><p>5 0.24129307 <a title="9-lsi-5" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>Author: Shyam Visweswaran, Gregory F. Cooper</p><p>Abstract: This paper introduces a Bayesian algorithm for constructing predictive models from data that are optimized to predict a target variable well for a particular instance. This algorithm learns Markov blanket models, carries out Bayesian model averaging over a set of models to predict a target variable of the instance at hand, and employs an instance-speciﬁc heuristic to locate a set of suitable models to average over. We call this method the instance-speciﬁc Markov blanket (ISMB) algorithm. The ISMB algorithm was evaluated on 21 UCI data sets using ﬁve different performance measures and its performance was compared to that of several commonly used predictive algorithms, including nave Bayes, C4.5 decision tree, logistic regression, neural networks, k-Nearest Neighbor, Lazy Bayesian Rules, and AdaBoost. Over all the data sets, the ISMB algorithm performed better on average on all performance measures against all the comparison algorithms. Keywords: instance-speciﬁc, Bayesian network, Markov blanket, Bayesian model averaging</p><p>6 0.23651934 <a title="9-lsi-6" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>7 0.21954158 <a title="9-lsi-7" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>8 0.21842802 <a title="9-lsi-8" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>9 0.20897084 <a title="9-lsi-9" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>10 0.19832005 <a title="9-lsi-10" href="./jmlr-2010-On_Over-fitting_in_Model_Selection_and_Subsequent_Selection_Bias_in_Performance_Evaluation.html">83 jmlr-2010-On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</a></p>
<p>11 0.19195838 <a title="9-lsi-11" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>12 0.18936701 <a title="9-lsi-12" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_II%3A_Analysis_and_Extensions.html">68 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions</a></p>
<p>13 0.18845725 <a title="9-lsi-13" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>14 0.1878382 <a title="9-lsi-14" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_I%3A_Algorithms_and_Empirical_Evaluation.html">67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</a></p>
<p>15 0.18469489 <a title="9-lsi-15" href="./jmlr-2010-Consensus-Based_Distributed_Support_Vector_Machines.html">26 jmlr-2010-Consensus-Based Distributed Support Vector Machines</a></p>
<p>16 0.18043327 <a title="9-lsi-16" href="./jmlr-2010-Classification_with_Incomplete_Data_Using_Dirichlet_Process_Priors.html">23 jmlr-2010-Classification with Incomplete Data Using Dirichlet Process Priors</a></p>
<p>17 0.17819057 <a title="9-lsi-17" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>18 0.16804537 <a title="9-lsi-18" href="./jmlr-2010-Tree_Decomposition_for_Large-Scale_SVM_Problems.html">113 jmlr-2010-Tree Decomposition for Large-Scale SVM Problems</a></p>
<p>19 0.16735049 <a title="9-lsi-19" href="./jmlr-2010-Quadratic_Programming_Feature_Selection.html">94 jmlr-2010-Quadratic Programming Feature Selection</a></p>
<p>20 0.16625433 <a title="9-lsi-20" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.011), (4, 0.016), (8, 0.024), (15, 0.014), (21, 0.019), (32, 0.047), (33, 0.011), (34, 0.01), (36, 0.037), (37, 0.06), (39, 0.046), (40, 0.308), (71, 0.02), (75, 0.138), (81, 0.018), (85, 0.108), (96, 0.012), (97, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80534667 <a title="9-lda-1" href="./jmlr-2010-Efficient_Algorithms_for_Conditional_Independence_Inference.html">32 jmlr-2010-Efficient Algorithms for Conditional Independence Inference</a></p>
<p>Author: Remco Bouckaert, Raymond Hemmecke, Silvia Lindner, Milan Studený</p><p>Abstract: The topic of the paper is computer testing of (probabilistic) conditional independence (CI) implications by an algebraic method of structural imsets. The basic idea is to transform (sets of) CI statements into certain integral vectors and to verify by a computer the corresponding algebraic relation between the vectors, called the independence implication. We interpret the previous methods for computer testing of this implication from the point of view of polyhedral geometry. However, the main contribution of the paper is a new method, based on linear programming (LP). The new method overcomes the limitation of former methods to the number of involved variables. We recall/describe the theoretical basis for all four methods involved in our computational experiments, whose aim was to compare the efﬁciency of the algorithms. The experiments show that the LP method is clearly the fastest one. As an example of possible application of such algorithms we show that testing inclusion of Bayesian network structures or whether a CI statement is encoded in an acyclic directed graph can be done by the algebraic method. Keywords: conditional independence inference, linear programming approach</p><p>same-paper 2 0.71931678 <a title="9-lda-2" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>Author: Erik Ĺ trumbelj, Igor Kononenko</p><p>Abstract: We present a general method for explaining individual predictions of classiﬁcation models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method’s initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efﬁcient and that the explanations are intuitive and useful. Keywords: data postprocessing, classiﬁcation, explanation, visualization</p><p>3 0.53605765 <a title="9-lda-3" href="./jmlr-2010-Stacked_Denoising_Autoencoders%3A_Learning_Useful_Representations_in_a_Deep_Network_with_a_Local_Denoising_Criterion.html">107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</a></p>
<p>Author: Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol</p><p>Abstract: We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classiﬁcation problems to yield signiﬁcantly lower classiﬁcation error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classiﬁers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations. Keywords: deep learning, unsupervised feature learning, deep belief networks, autoencoders, denoising</p><p>4 0.52238089 <a title="9-lda-4" href="./jmlr-2010-Bundle_Methods_for_Regularized_Risk_Minimization.html">18 jmlr-2010-Bundle Methods for Regularized Risk Minimization</a></p>
<p>Author: Choon Hui Teo, S.V.N. Vishwanthan, Alex J. Smola, Quoc V. Le</p><p>Abstract: A wide variety of machine learning problems can be described as minimizing a regularized risk functional, with different algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Machines (SVMs), Gaussian Processes, Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper describes the theory and implementation of a scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as L1 and L2 penalties. In addition to the uniﬁed framework we present tight convergence bounds, which show that our algorithm converges in O(1/ε) steps to ε precision for general convex problems and in O(log(1/ε)) steps for continuously differentiable problems. We demonstrate the performance of our general purpose solver on a variety of publicly available data sets. Keywords: optimization, subgradient methods, cutting plane method, bundle methods, regularized risk minimization, parallel optimization ∗. Also at Canberra Research Laboratory, NICTA. c 2010 Choon Hui Teo, S.V. N. Vishwanthan, Alex J. Smola and Quoc V. Le. T EO , V ISHWANATHAN , S MOLA AND L E</p><p>5 0.52130318 <a title="9-lda-5" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>Author: Pannagadatta K. Shivaswamy, Tony Jebara</p><p>Abstract: Leading classiﬁcation methods such as support vector machines (SVMs) and their counterparts achieve strong generalization performance by maximizing the margin of separation between data classes. While the maximum margin approach has achieved promising performance, this article identiﬁes its sensitivity to afﬁne transformations of the data and to directions with large data spread. Maximum margin solutions may be misled by the spread of data and preferentially separate classes along large spread directions. This article corrects these weaknesses by measuring margin not in the absolute sense but rather only relative to the spread of data in any projection direction. Maximum relative margin corresponds to a data-dependent regularization on the classiﬁcation function while maximum absolute margin corresponds to an ℓ2 norm constraint on the classiﬁcation function. Interestingly, the proposed improvements only require simple extensions to existing maximum margin formulations and preserve the computational efﬁciency of SVMs. Through the maximization of relative margin, surprising performance gains are achieved on real-world problems such as digit, text classiﬁcation and on several other benchmark data sets. In addition, risk bounds are derived for the new formulation based on Rademacher averages. Keywords: support vector machines, kernel methods, large margin, Rademacher complexity</p><p>6 0.51245725 <a title="9-lda-6" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>7 0.50909692 <a title="9-lda-7" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>8 0.5074954 <a title="9-lda-8" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>9 0.50684071 <a title="9-lda-9" href="./jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">48 jmlr-2010-How to Explain Individual Classification Decisions</a></p>
<p>10 0.5053103 <a title="9-lda-10" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>11 0.5049184 <a title="9-lda-11" href="./jmlr-2010-Approximate_Riemannian_Conjugate_Gradient_Learning_for_Fixed-Form_Variational_Bayes.html">14 jmlr-2010-Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes</a></p>
<p>12 0.50426692 <a title="9-lda-12" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>13 0.50404119 <a title="9-lda-13" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>14 0.50245702 <a title="9-lda-14" href="./jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</a></p>
<p>15 0.50203258 <a title="9-lda-15" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>16 0.5016349 <a title="9-lda-16" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>17 0.49984345 <a title="9-lda-17" href="./jmlr-2010-On_Over-fitting_in_Model_Selection_and_Subsequent_Selection_Bias_in_Performance_Evaluation.html">83 jmlr-2010-On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</a></p>
<p>18 0.49721986 <a title="9-lda-18" href="./jmlr-2010-Asymptotic_Equivalence_of_Bayes_Cross_Validation_and_Widely_Applicable_Information_Criterion_in_Singular_Learning_Theory.html">16 jmlr-2010-Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory</a></p>
<p>19 0.49685609 <a title="9-lda-19" href="./jmlr-2010-Classification_with_Incomplete_Data_Using_Dirichlet_Process_Priors.html">23 jmlr-2010-Classification with Incomplete Data Using Dirichlet Process Priors</a></p>
<p>20 0.49684879 <a title="9-lda-20" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
