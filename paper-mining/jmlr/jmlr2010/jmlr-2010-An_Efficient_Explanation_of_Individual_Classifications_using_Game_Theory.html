<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-9" href="#">jmlr2010-9</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</h1>
<br/><p>Source: <a title="jmlr-2010-9-pdf" href="http://jmlr.org/papers/volume11/strumbelj10a/strumbelj10a.pdf">pdf</a></p><p>Author: Erik Ĺ trumbelj, Igor Kononenko</p><p>Abstract: We present a general method for explaining individual predictions of classiﬁcation models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method’s initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efﬁcient and that the explanations are intuitive and useful. Keywords: data postprocessing, classiﬁcation, explanation, visualization</p><p>Reference: <a title="jmlr-2010-9-reference" href="../jmlr2010_reference/jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coalit', 0.366), ('shapley', 0.32), ('kononenko', 0.292), ('gam', 0.234), ('expl', 0.219), ('axiom', 0.214), ('trumbels', 0.206), ('xplaining', 0.183), ('ndivid', 0.156), ('strumbelj', 0.156), ('nb', 0.143), ('feat', 0.139), ('logreg', 0.137), ('oncolog', 0.123), ('bstdt', 0.114), ('condind', 0.114), ('pre', 0.106), ('contribut', 0.098), ('disjunct', 0.098), ('rf', 0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="9-tfidf-1" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>Author: Erik Ĺ trumbelj, Igor Kononenko</p><p>Abstract: We present a general method for explaining individual predictions of classiﬁcation models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method’s initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efﬁcient and that the explanations are intuitive and useful. Keywords: data postprocessing, classiﬁcation, explanation, visualization</p><p>2 0.13757153 <a title="9-tfidf-2" href="./jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">48 jmlr-2010-How to Explain Individual Classification Decisions</a></p>
<p>Author: David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, Klaus-Robert Müller</p><p>Abstract: After building a classiﬁer with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most inﬂuential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classiﬁcation method. Keywords: explaining, nonlinear, black box model, kernel methods, Ames mutagenicity</p><p>3 0.096293233 <a title="9-tfidf-3" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>Author: Markus Ojala, Gemma C. Garriga</p><p>Abstract: We explore the framework of permutation-based p-values for assessing the performance of classiﬁers. In this paper we study two simple permutation tests. The ﬁrst test assess whether the classiﬁer has found a real class structure in the data; the corresponding null distribution is estimated by permuting the labels in the data. This test has been used extensively in classiﬁcation problems in computational biology. The second test studies whether the classiﬁer is exploiting the dependency between the features in classiﬁcation; the corresponding null distribution is estimated by permuting the features within classes, inspired by restricted randomization techniques traditionally used in statistics. This new test can serve to identify descriptive features which can be valuable information in improving the classiﬁer performance. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classiﬁer performance via permutation tests is effective. In particular, the restricted permutation test clearly reveals whether the classiﬁer exploits the interdependency between the features in the data. Keywords: classiﬁcation, labeled data, permutation tests, restricted randomization, signiﬁcance testing</p><p>4 0.09588711 <a title="9-tfidf-4" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>Author: Jean-Yves Audibert, Sébastien Bubeck</p><p>Abstract: This work deals with four classical prediction settings, namely full information, bandit, label efﬁcient and bandit label efﬁcient as well as four different notions of regret: pseudo-regret, expected regret, high probability regret and tracking the best expert regret. We introduce a new forecaster, INF (Implicitly Normalized Forecaster) based on an arbitrary function ψ for which we propose a uniﬁed γ analysis of its pseudo-regret in the four games we consider. In particular, for ψ(x) = exp(ηx) + K , INF reduces to the classical exponentially weighted average forecaster and our analysis of the pseudo-regret recovers known results while for the expected regret we slightly tighten the bounds. γ η q On the other hand with ψ(x) = −x + K , which deﬁnes a new forecaster, we are able to remove the extraneous logarithmic factor in the pseudo-regret bounds for bandits games, and thus ﬁll in a long open gap in the characterization of the minimax rate for the pseudo-regret in the bandit game. We also provide high probability bounds depending on the cumulative reward of the optimal action. Finally, we consider the stochastic bandit game, and prove that an appropriate modiﬁcation of the upper conﬁdence bound policy UCB1 (Auer et al., 2002a) achieves the distribution-free optimal rate while still having a distribution-dependent rate logarithmic in the number of plays. Keywords: Bandits (adversarial and stochastic), regret bound, minimax rate, label efﬁcient, upper conﬁdence bound (UCB) policy, online learning, prediction with limited feedback.</p><p>5 0.066455968 <a title="9-tfidf-5" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>Author: Franz Pernkopf, Jeff A. Bilmes</p><p>Abstract: We introduce a simple order-based greedy heuristic for learning discriminative structure within generative Bayesian network classiﬁers. We propose two methods for establishing an order of N features. They are based on the conditional mutual information and classiﬁcation rate (i.e., risk), respectively. Given an ordering, we can ﬁnd a discriminative structure with O N k+1 score evaluations (where constant k is the tree-width of the sub-graph over the attributes). We present results on 25 data sets from the UCI repository, for phonetic classiﬁcation using the TIMIT database, for a visual surface inspection task, and for two handwritten digit recognition tasks. We provide classiﬁcation performance for both discriminative and generative parameter learning on both discriminatively and generatively structured networks. The discriminative structure found by our new procedures signiﬁcantly outperforms generatively produced structures, and achieves a classiﬁcation accuracy on par with the best discriminative (greedy) Bayesian network learning approach, but does so with a factor of ∼10-40 speedup. We also show that the advantages of generative discriminatively structured Bayesian network classiﬁers still hold in the case of missing features, a case where generative classiﬁers have an advantage over discriminative classiﬁers. Keywords: Bayesian networks, classiﬁcation, discriminative learning, structure learning, graphical model, missing feature</p><p>6 0.057471745 <a title="9-tfidf-6" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>7 0.048589852 <a title="9-tfidf-7" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>8 0.046432607 <a title="9-tfidf-8" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>9 0.045299035 <a title="9-tfidf-9" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>10 0.042180508 <a title="9-tfidf-10" href="./jmlr-2010-On_Over-fitting_in_Model_Selection_and_Subsequent_Selection_Bias_in_Performance_Evaluation.html">83 jmlr-2010-On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</a></p>
<p>11 0.039611377 <a title="9-tfidf-11" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_I%3A_Algorithms_and_Empirical_Evaluation.html">67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</a></p>
<p>12 0.039130632 <a title="9-tfidf-12" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>13 0.039093439 <a title="9-tfidf-13" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_II%3A_Analysis_and_Extensions.html">68 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions</a></p>
<p>14 0.038577527 <a title="9-tfidf-14" href="./jmlr-2010-WEKA%E2%88%92Experiences_with_a_Java_Open-Source_Project.html">116 jmlr-2010-WEKA−Experiences with a Java Open-Source Project</a></p>
<p>15 0.037516575 <a title="9-tfidf-15" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>16 0.036758803 <a title="9-tfidf-16" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>17 0.036566891 <a title="9-tfidf-17" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<p>18 0.036480259 <a title="9-tfidf-18" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>19 0.03357387 <a title="9-tfidf-19" href="./jmlr-2010-Why_Does_Unsupervised_Pre-training_Help_Deep_Learning%3F.html">117 jmlr-2010-Why Does Unsupervised Pre-training Help Deep Learning?</a></p>
<p>20 0.031654548 <a title="9-tfidf-20" href="./jmlr-2010-Classification_with_Incomplete_Data_Using_Dirichlet_Process_Priors.html">23 jmlr-2010-Classification with Incomplete Data Using Dirichlet Process Priors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.168), (1, -0.045), (2, 0.078), (3, 0.064), (4, 0.022), (5, 0.093), (6, 0.047), (7, -0.019), (8, 0.073), (9, 0.086), (10, -0.065), (11, 0.074), (12, 0.089), (13, -0.074), (14, 0.037), (15, 0.008), (16, -0.127), (17, -0.094), (18, -0.191), (19, -0.017), (20, -0.165), (21, -0.024), (22, 0.265), (23, 0.064), (24, 0.092), (25, -0.108), (26, 0.045), (27, -0.24), (28, -0.163), (29, -0.017), (30, -0.081), (31, -0.054), (32, -0.306), (33, 0.125), (34, 0.107), (35, 0.03), (36, -0.147), (37, 0.144), (38, 0.03), (39, -0.039), (40, -0.047), (41, -0.035), (42, 0.037), (43, -0.005), (44, 0.025), (45, 0.027), (46, -0.048), (47, 0.036), (48, -0.103), (49, -0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9250052 <a title="9-lsi-1" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>Author: Erik Ĺ trumbelj, Igor Kononenko</p><p>Abstract: We present a general method for explaining individual predictions of classiﬁcation models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method’s initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efﬁcient and that the explanations are intuitive and useful. Keywords: data postprocessing, classiﬁcation, explanation, visualization</p><p>2 0.78684342 <a title="9-lsi-2" href="./jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">48 jmlr-2010-How to Explain Individual Classification Decisions</a></p>
<p>Author: David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, Klaus-Robert Müller</p><p>Abstract: After building a classiﬁer with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most inﬂuential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classiﬁcation method. Keywords: explaining, nonlinear, black box model, kernel methods, Ames mutagenicity</p><p>3 0.44694379 <a title="9-lsi-3" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>Author: Markus Ojala, Gemma C. Garriga</p><p>Abstract: We explore the framework of permutation-based p-values for assessing the performance of classiﬁers. In this paper we study two simple permutation tests. The ﬁrst test assess whether the classiﬁer has found a real class structure in the data; the corresponding null distribution is estimated by permuting the labels in the data. This test has been used extensively in classiﬁcation problems in computational biology. The second test studies whether the classiﬁer is exploiting the dependency between the features in classiﬁcation; the corresponding null distribution is estimated by permuting the features within classes, inspired by restricted randomization techniques traditionally used in statistics. This new test can serve to identify descriptive features which can be valuable information in improving the classiﬁer performance. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classiﬁer performance via permutation tests is effective. In particular, the restricted permutation test clearly reveals whether the classiﬁer exploits the interdependency between the features in the data. Keywords: classiﬁcation, labeled data, permutation tests, restricted randomization, signiﬁcance testing</p><p>4 0.40875226 <a title="9-lsi-4" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>Author: Franz Pernkopf, Jeff A. Bilmes</p><p>Abstract: We introduce a simple order-based greedy heuristic for learning discriminative structure within generative Bayesian network classiﬁers. We propose two methods for establishing an order of N features. They are based on the conditional mutual information and classiﬁcation rate (i.e., risk), respectively. Given an ordering, we can ﬁnd a discriminative structure with O N k+1 score evaluations (where constant k is the tree-width of the sub-graph over the attributes). We present results on 25 data sets from the UCI repository, for phonetic classiﬁcation using the TIMIT database, for a visual surface inspection task, and for two handwritten digit recognition tasks. We provide classiﬁcation performance for both discriminative and generative parameter learning on both discriminatively and generatively structured networks. The discriminative structure found by our new procedures signiﬁcantly outperforms generatively produced structures, and achieves a classiﬁcation accuracy on par with the best discriminative (greedy) Bayesian network learning approach, but does so with a factor of ∼10-40 speedup. We also show that the advantages of generative discriminatively structured Bayesian network classiﬁers still hold in the case of missing features, a case where generative classiﬁers have an advantage over discriminative classiﬁers. Keywords: Bayesian networks, classiﬁcation, discriminative learning, structure learning, graphical model, missing feature</p><p>5 0.36151576 <a title="9-lsi-5" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>Author: Shyam Visweswaran, Gregory F. Cooper</p><p>Abstract: This paper introduces a Bayesian algorithm for constructing predictive models from data that are optimized to predict a target variable well for a particular instance. This algorithm learns Markov blanket models, carries out Bayesian model averaging over a set of models to predict a target variable of the instance at hand, and employs an instance-speciﬁc heuristic to locate a set of suitable models to average over. We call this method the instance-speciﬁc Markov blanket (ISMB) algorithm. The ISMB algorithm was evaluated on 21 UCI data sets using ﬁve different performance measures and its performance was compared to that of several commonly used predictive algorithms, including nave Bayes, C4.5 decision tree, logistic regression, neural networks, k-Nearest Neighbor, Lazy Bayesian Rules, and AdaBoost. Over all the data sets, the ISMB algorithm performed better on average on all performance measures against all the comparison algorithms. Keywords: instance-speciﬁc, Bayesian network, Markov blanket, Bayesian model averaging</p><p>6 0.35858151 <a title="9-lsi-6" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>7 0.29754603 <a title="9-lsi-7" href="./jmlr-2010-Expectation_Truncation_and_the_Benefits_of_Preselection_In_Training_Generative_Models.html">38 jmlr-2010-Expectation Truncation and the Benefits of Preselection In Training Generative Models</a></p>
<p>8 0.28995222 <a title="9-lsi-8" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>9 0.28742236 <a title="9-lsi-9" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>10 0.27352643 <a title="9-lsi-10" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_II%3A_Analysis_and_Extensions.html">68 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions</a></p>
<p>11 0.26896319 <a title="9-lsi-11" href="./jmlr-2010-On_Over-fitting_in_Model_Selection_and_Subsequent_Selection_Bias_in_Performance_Evaluation.html">83 jmlr-2010-On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</a></p>
<p>12 0.26218733 <a title="9-lsi-12" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>13 0.25996774 <a title="9-lsi-13" href="./jmlr-2010-Bayesian_Learning_in_Sparse_Graphical_Factor_Models_via_Variational_Mean-Field_Annealing.html">17 jmlr-2010-Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing</a></p>
<p>14 0.25336587 <a title="9-lsi-14" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_I%3A_Algorithms_and_Empirical_Evaluation.html">67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</a></p>
<p>15 0.25130635 <a title="9-lsi-15" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>16 0.24808738 <a title="9-lsi-16" href="./jmlr-2010-WEKA%E2%88%92Experiences_with_a_Java_Open-Source_Project.html">116 jmlr-2010-WEKA−Experiences with a Java Open-Source Project</a></p>
<p>17 0.24782629 <a title="9-lsi-17" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>18 0.24687034 <a title="9-lsi-18" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>19 0.23619208 <a title="9-lsi-19" href="./jmlr-2010-Classification_with_Incomplete_Data_Using_Dirichlet_Process_Priors.html">23 jmlr-2010-Classification with Incomplete Data Using Dirichlet Process Priors</a></p>
<p>20 0.23451385 <a title="9-lsi-20" href="./jmlr-2010-On_Finding_Predictors_for_Arbitrary_Families_of_Processes.html">81 jmlr-2010-On Finding Predictors for Arbitrary Families of Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.388), (11, 0.172), (13, 0.094), (22, 0.021), (46, 0.014), (49, 0.014), (54, 0.028), (62, 0.013), (65, 0.057), (68, 0.029), (71, 0.065), (82, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.59957439 <a title="9-lda-1" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>Author: Erik Ĺ trumbelj, Igor Kononenko</p><p>Abstract: We present a general method for explaining individual predictions of classiﬁcation models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method’s initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efﬁcient and that the explanations are intuitive and useful. Keywords: data postprocessing, classiﬁcation, explanation, visualization</p><p>2 0.48042142 <a title="9-lda-2" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>Author: Isabelle Guyon, Amir Saffari, Gideon Dror, Gavin Cawley</p><p>Abstract: The principle of parsimony also known as “Ockham’s razor” has inspired many theories of model selection. Yet such theories, all making arguments in favor of parsimony, are based on very different premises and have developed distinct methodologies to derive algorithms. We have organized challenges and edited a special issue of JMLR and several conference proceedings around the theme of model selection. In this editorial, we revisit the problem of avoiding overﬁtting in light of the latest results. We note the remarkable convergence of theories as different as Bayesian theory, Minimum Description Length, bias/variance tradeoff, Structural Risk Minimization, and regularization, in some approaches. We also present new and interesting examples of the complementarity of theories leading to hybrid algorithms, neither frequentist, nor Bayesian, or perhaps both frequentist and Bayesian! Keywords: model selection, ensemble methods, multilevel inference, multilevel optimization, performance prediction, bias-variance tradeoff, Bayesian priors, structural risk minimization, guaranteed risk minimization, over-ﬁtting, regularization, minimum description length</p><p>3 0.47808069 <a title="9-lda-3" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>Author: Pinar Donmez, Guy Lebanon, Krishnakumar Balasubramanian</p><p>Abstract: Estimating the error rates of classiﬁers or regression models is a fundamental task in machine learning which has thus far been studied exclusively using supervised learning techniques. We propose a novel unsupervised framework for estimating these error rates using only unlabeled data and mild assumptions. We prove consistency results for the framework and demonstrate its practical applicability on both synthetic and real world data. Keywords: classiﬁcation and regression, maximum likelihood, latent variable models</p><p>4 0.4749389 <a title="9-lda-4" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>Author: Gideon S. Mann, Andrew McCallum</p><p>Abstract: In this paper, we present an overview of generalized expectation criteria (GE), a simple, robust, scalable method for semi-supervised training using weakly-labeled data. GE ﬁts model parameters by favoring models that match certain expectation constraints, such as marginal label distributions, on the unlabeled data. This paper shows how to apply generalized expectation criteria to two classes of parametric models: maximum entropy models and conditional random ﬁelds. Experimental results demonstrate accuracy improvements over supervised training and a number of other stateof-the-art semi-supervised learning methods for these models. Keywords: generalized expectation criteria, semi-supervised learning, logistic regression, conditional random ﬁelds</p><p>5 0.47453159 <a title="9-lda-5" href="./jmlr-2010-An_Exponential_Model_for_Infinite_Rankings.html">10 jmlr-2010-An Exponential Model for Infinite Rankings</a></p>
<p>Author: Marina Meilă, Le Bao</p><p>Abstract: This paper presents a statistical model for expressing preferences through rankings, when the number of alternatives (items to rank) is large. A human ranker will then typically rank only the most preferred items, and may not even examine the whole set of items, or know how many they are. Similarly, a user presented with the ranked output of a search engine, will only consider the highest ranked items. We model such situations by introducing a stagewise ranking model that operates with ﬁnite ordered lists called top-t orderings over an inﬁnite space of items. We give algorithms to estimate this model from data, and demonstrate that it has sufﬁcient statistics, being thus an exponential family model with continuous and discrete parameters. We describe its conjugate prior and other statistical properties. Then, we extend the estimation problem to multimodal data by introducing an Exponential-Blurring-Mean-Shift nonparametric clustering algorithm. The experiments highlight the properties of our model and demonstrate that inﬁnite models over permutations can be simple, elegant and practical. Keywords: permutations, partial orderings, Mallows model, distance based ranking model, exponential family, non-parametric clustering, branch-and-bound</p><p>6 0.4721117 <a title="9-lda-6" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>7 0.472027 <a title="9-lda-7" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>8 0.47198203 <a title="9-lda-8" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>9 0.47134241 <a title="9-lda-9" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>10 0.47105378 <a title="9-lda-10" href="./jmlr-2010-Approximate_Tree_Kernels.html">15 jmlr-2010-Approximate Tree Kernels</a></p>
<p>11 0.47085172 <a title="9-lda-11" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>12 0.47040817 <a title="9-lda-12" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>13 0.46967384 <a title="9-lda-13" href="./jmlr-2010-Spectral_Regularization_Algorithms_for_Learning_Large_Incomplete_Matrices.html">105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</a></p>
<p>14 0.46875688 <a title="9-lda-14" href="./jmlr-2010-Maximum_Likelihood_in_Cost-Sensitive_Learning%3A_Model_Specification%2C_Approximations%2C_and_Upper_Bounds.html">73 jmlr-2010-Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds</a></p>
<p>15 0.46832371 <a title="9-lda-15" href="./jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">48 jmlr-2010-How to Explain Individual Classification Decisions</a></p>
<p>16 0.46822548 <a title="9-lda-16" href="./jmlr-2010-Bayesian_Learning_in_Sparse_Graphical_Factor_Models_via_Variational_Mean-Field_Annealing.html">17 jmlr-2010-Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing</a></p>
<p>17 0.46822536 <a title="9-lda-17" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>18 0.46790469 <a title="9-lda-18" href="./jmlr-2010-On_the_Foundations_of_Noise-free_Selective_Classification.html">85 jmlr-2010-On the Foundations of Noise-free Selective Classification</a></p>
<p>19 0.4671953 <a title="9-lda-19" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>20 0.46703598 <a title="9-lda-20" href="./jmlr-2010-Mean_Field_Variational_Approximation_for_Continuous-Time_Bayesian_Networks.html">75 jmlr-2010-Mean Field Variational Approximation for Continuous-Time Bayesian Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
