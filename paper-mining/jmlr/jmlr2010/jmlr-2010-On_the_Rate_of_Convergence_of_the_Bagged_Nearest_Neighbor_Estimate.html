<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>86 jmlr-2010-On the Rate of Convergence of the Bagged Nearest Neighbor Estimate</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-86" href="#">jmlr2010-86</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>86 jmlr-2010-On the Rate of Convergence of the Bagged Nearest Neighbor Estimate</h1>
<br/><p>Source: <a title="jmlr-2010-86-pdf" href="http://jmlr.org/papers/volume11/biau10a/biau10a.pdf">pdf</a></p><p>Author: Gérard Biau, Frédéric Cérou, Arnaud Guyader</p><p>Abstract: Bagging is a simple way to combine estimates in order to improve their performance. This method, suggested by Breiman in 1996, proceeds by resampling from the original data set, constructing a predictor from each subsample, and decide by combining. By bagging an n-sample, the crude nearest neighbor regression estimate is turned into a consistent weighted nearest neighbor regression estimate, which is amenable to statistical analysis. Letting the resampling size kn grows appropriately with n, it is shown that this estimate may achieve optimal rate of convergence, independently from the fact that resampling is done with or without replacement. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, adaptation results by data-splitting are presented. Keywords: bagging, resampling, nearest neighbor estimate, rates of convergence</p><p>Reference: <a title="jmlr-2010-86-reference" href="../jmlr2010_reference/jmlr-2010-On_the_Rate_of_Convergence_of_the_Bagged_Nearest_Neighbor_Estimate_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 By bagging an n-sample, the crude nearest neighbor regression estimate is turned into a consistent weighted nearest neighbor regression estimate, which is amenable to statistical analysis. [sent-9, score-0.565]
</p><p>2 Letting the resampling size kn grows appropriately with n, it is shown that this estimate may achieve optimal rate of convergence, independently from the fact that resampling is done with or without replacement. [sent-10, score-1.049]
</p><p>3 Keywords: bagging, resampling, nearest neighbor estimate, rates of convergence  1. [sent-12, score-0.196]
</p><p>4 It turns out that Breiman’s bagging principle has a simple application in the context of nearest neighbor methods. [sent-24, score-0.287]
</p><p>5 A major attraction of nearest neighbor procedures is their simplicity. [sent-28, score-0.181]
</p><p>6 Before we formalize the link between bagging and nearest neighbors, some deﬁnitions are in order. [sent-32, score-0.199]
</p><p>7 2 Bagging and Nearest Neighbors Recall that the 1-nearest neighbor (1-NN) regression estimate sets rn (x) = Y(1) (x) where Y(1) (x) is the observation of the feature vector X(1) (x) whose Euclidean distance to x is minimal among all X1 , . [sent-45, score-0.21]
</p><p>8 We proceed as follows, via a randomized basic regression estimate rkn in which 1 ≤ kn ≤ n is a parameter. [sent-54, score-0.981]
</p><p>9 Thus, the bagged regression estimate rn is deﬁned by ⋆ rn (x) = E⋆ [rkn (x)] ,  where E⋆ denotes expectation with respect to the resampling distribution, conditionally on the data set Dn . [sent-60, score-0.44]
</p><p>10 The following result, proved in Biau and Devroye (2008), shows that for an appropriate choice of kn , the bagged version of the 1-NN regression estimate is universally consistent: ⋆ Theorem 1 If kn → ∞ and kn /n → 0, then rn is universally consistent. [sent-61, score-3.071]
</p><p>11 ≤ x − X(n) (x) is called a weighted nearest neighbor regression estimate. [sent-73, score-0.215]
</p><p>12 The crux to prove Theorem 1 is to observe that rn is in fact a weighted nearest neighbor estimate with Vi = P(i-th nearest neighbor of x is the 1-NN in a random selection). [sent-78, score-0.474]
</p><p>13 Then, a moment’s thought shows that for the “with replacement” sampling Vi = 1 −  i−1 n  kn  − 1−  i n  kn  ,  whereas for sampling “without replacement”, Vi is hypergeometric:  n−i      kn − 1 , i ≤ n − kn + 1 n Vi =    kn   0, i > n − kn + 1. [sent-79, score-5.454]
</p><p>14 Note also that this new expression for the 1-NN bagged estimate makes any Monte-Carlo approach unnecessary to evaluate the estimate. [sent-82, score-0.207]
</p><p>15 Formally, if Zt = rkn (x) is the prediction in the t-th round of bagging, the bagged regression estimate was approximately evaluated as 1 T ⋆ rn (x) ≈ ∑ Zt , T t=1 where Z1 , . [sent-84, score-0.337]
</p><p>16 Clearly, writing the 1-NN bagged estimate as an (exact) weighted nearest neighbor predictor makes such calculations useless. [sent-88, score-0.419]
</p><p>17 On the other hand, the fact that the bagged 1-NN estimate reduces to a weighted nearest neighbor estimate may seem at ﬁrst sight somehow disappointing. [sent-89, score-0.421]
</p><p>18 Indeed, we get the ordinary kn -NN rule back by the choice 1/kn if i ≤ kn Vi = 0 otherwise, 689  ´ B IAU , C E ROU AND G UYADER  and, with an appropriate choice of the sequence (kn ), this regression estimate is known to have optimal asymptotic properties (see Gy¨ rﬁ et al. [sent-90, score-1.861]
</p><p>19 Thus, o the question is: Why would one care about the bagged nearest neighbor rule then? [sent-93, score-0.367]
</p><p>20 First, bagging the 1-NN is a very popular technique for regression and classiﬁcation in the machine learning community, and most—if not all—empirical studies report practical improvements over the traditional kn -NN method. [sent-95, score-1.037]
</p><p>21 These learning procedures typically involve a resampling step, which may be interpreted as a particular 1-NN bagged procedure based on the so-called “layered nearest neighbor” proximities (Lin and Jeon, 2006; Biau and Devroye, 2008). [sent-98, score-0.332]
</p><p>22 We will start our analysis by stating a comprehensive theorem on the rate of convergence of general weighted nearest neighbor estimates (Section 2. [sent-100, score-0.24]
</p><p>23 3 that, irrespectively of the resampling type, for d ≥ 3 and a suitable choice of ⋆ the sequence (kn ), the estimate rn is of optimum rate for the class F , that is lim sup sup n→∞ (X,Y )∈F  ⋆ E[rn (X) − r(X)]2 2  2  ((ρC)d σ2 ) d+2 n− d+2  ≤Λ  for some positive Λ independent of C, ρ and σ2 . [sent-112, score-0.185]
</p><p>24 Since the parameter kn of the estimate with the optimal rate of convergence depends on the unknown distribution of (X,Y ), especially on the smoothness of the regression function, we present in Section 2. [sent-113, score-0.991]
</p><p>25 , data-dependent) choices of kn which preserve the minimax optimality of the estimate. [sent-116, score-0.929]
</p><p>26 We wish to emphasize that all the results are obtained by letting the resampling size kn grows with n in such a manner that kn → ∞ and kn /n → 0. [sent-117, score-2.791]
</p><p>27 In fact, much of the evidence against the performance of bagged nearest neighbor methods is for full sample size resamples (see the discussion in Breiman, 1996, paragraph 6. [sent-119, score-0.389]
</p><p>28 As put forward in Kulkarni and Posner (1995), these quantities play a key role in the context of nearest neighbor analysis. [sent-129, score-0.181]
</p><p>29 Theorem 4 Let rn (x) = ∑n Vi Y(i) (x) be a weighted nearest neighbor estimate of r(x). [sent-151, score-0.293]
</p><p>30 (2)  i=1  To analyse the bias term in (1), we will need the following result, which bounds the convergence rate of the expected i-th nearest neighbor squared distance in terms of the metric covering radii of the support of the distribution µ of X. [sent-159, score-0.238]
</p><p>31 Taking for example 1/kn if i ≤ kn Vi = 0 otherwise, we get the ordinary kn -NN rule back. [sent-173, score-1.818]
</p><p>32 Here, n  1  ∑ Vi2 = kn  i=1  693  ´ B IAU , C E ROU AND G UYADER  and  ∑ Vi  i=1  n i  −2/d  =  1 kn n ∑ kn i=1 i  ≤  n  1 kn n ∑ kn i=1 kn  n = kn ≤ξ  −2/d −2/d  −2/d  n kn  −2/d  for some positive ξ. [sent-174, score-7.272]
</p><p>33 Therefore, in this context, according to Theorem 4, for d ≥ 3, there exists a 2 sequence (kn ) with kn ∝ n d+2 such that 2 d+2  (ρC)d σ2 E [rn (X) − r(X)] ≤ Λ n 2  ,  for some positive constant Λ independent of ρ, C and σ2 . [sent-175, score-0.909]
</p><p>34 o The adaptation of Theorem 4 to the 1-NN bagged regression estimate needs more careful attention. [sent-181, score-0.244]
</p><p>35 As seen in the introduction, in this case, the weighted nearest neighbor regression estimate takes the form n  ⋆ rn (x) = ∑ Vi Y(i) (x), i=1  where Vi = 1 −  i−1 n  kn  − 1−  i n  kn  . [sent-185, score-2.133]
</p><p>36 , n, let Vi = 1 − Then  kn  i−1 n  n  ∑ Vi2 ≤  i=1  − 1−  2kn n  1+  kn  i n  . [sent-192, score-1.818]
</p><p>37 The message of Proposition 7 is that, when resampling is done with replacement, the variance term of the bagged NN estimate is O(kn /n). [sent-194, score-0.26]
</p><p>38 Then (i) If d = 1, n  i  2  ∑ Vi n ≤ kn  1+  i=1  1 n  kn  . [sent-200, score-1.818]
</p><p>39 (ii) If d = 2, n  ∑ Vi  i=1  (iii) If d ≥ 3,  n i 1 + ln n i n  ∑ Vi  i=1  n i  −2/d  ≤  2 kn  1+  1 n  kn  [1 + ln(kn + 1)] . [sent-201, score-1.857]
</p><p>40 1 1 + αd 1 + kn n n  ≤  where  kn  2  kn − d ,  d +2 d −2 Γ . [sent-202, score-2.727]
</p><p>41 Then (i) If d = 1, ⋆ E [rn (X) − r(X)]2  2σ2 kn ≤ n  1 1+ n  695  2kn  32ρ2C2 + kn  1 1+ n  kn  . [sent-206, score-2.727]
</p><p>42 ´ B IAU , C E ROU AND G UYADER  (ii) If d = 2, ⋆ E [rn (X) − r(X)]2 ≤  2σ2 kn n +  16ρ2C2 kn  2kn  1 n  1+  1+  1 n  kn  [1 + ln(kn + 1)] . [sent-207, score-2.727]
</p><p>43 (iii) If d ≥ 3, ⋆ E [rn (X) − r(X)]2  2σ2 kn ≤ n +  1 1+ n  2kn  kn  8ρ2C2 1 1 + αd 1 + kn 1 − 2/d n n  where αd = 2Γ  2  kn − d ,  d +2 d −2 Γ . [sent-208, score-3.636]
</p><p>44 (ii) If d = 2, there exists a sequence (kn ) such that kn → ∞, kn /n → 0, and ⋆ E [rn (X) − r(X)]2 ≤ (Λ + o(1)) ρCσ  ln n , n  for some positive constant Λ independent of ρ, C and σ2 . [sent-210, score-1.857]
</p><p>45 d  (iii) If d ≥ 3, there exists a sequence (kn ) with kn ∝ n d+2 such that ⋆ E [rn (X) − r(X)]2  (ρC)d σ2 ≤Λ n  2 d+2  ,  for some positive constant Λ independent of ρ, C and σ2 . [sent-211, score-0.909]
</p><p>46 This low-dimensional phenomenon is also known to hold for the traditional kn -NN regression estimate, which does not achieve the optimal rates in dimensions 1 and 2 (see Problems 6. [sent-221, score-0.931]
</p><p>47 3 Bagging Without Replacement We brieﬂy analyse in this subsection the rate of convergence of the bagged 1-NN regression estimate, assuming this time that, at each step, the kn observations are distinctly chosen at random within the sample set Dn . [sent-227, score-1.145]
</p><p>48 We know that, in this case, the weighted nearest neighbor u regression estimate takes the form n  ⋆ rn (x) = ∑ ViY(i) (x), i=1  where Vi =              n−i kn − 1 , i ≤ n − kn + 1 n kn 0, i > n − kn + 1. [sent-229, score-3.951]
</p><p>49 , n, let  Vi =  Then              n  n−i kn − 1 , i ≤ n − kn + 1 n kn 0, i > n − kn + 1. [sent-235, score-3.636]
</p><p>50 n (1 − kn /n + 1/n)2  Thus, as for bagging with replacement, the variance term of the without replacement bagged 1NN estimate is O(kn /n). [sent-237, score-1.271]
</p><p>51 Recall that n  rn (x) = ∑ Vi r(X(i) (x)), ˜⋆ i=1  697  ´ B IAU , C E ROU AND G UYADER  and observe that rn (x) = E⋆ r(X⋆ (x)) , ˜⋆ (1) where X⋆ (x) is the nearest neighbor of x in a random subsample Sn drawn without replacement (1) from {(X1 ,Y1 ), . [sent-240, score-0.416]
</p><p>52 , (Xn ,Yn )} with Card(Sn ) = kn , and E⋆ denotes expectation with respect to the resampling distribution, conditionally on the data set Dn . [sent-243, score-0.962]
</p><p>53 kn  (ii) If d = 2, E [˜n (X) − r(X)]2 ≤ r⋆ (iii) If d ≥ 3,  8ρ2C2 (1 + ln kn ). [sent-248, score-1.857]
</p><p>54 kn  E [˜n (X) − r(X)]2 ≤ r⋆ −2/d  Thus, for d ≥ 3, E [˜n (X) − r(X)]2 = O(kn r⋆ to the desired theorem:  8ρ2C2 − 2 kn d . [sent-249, score-1.818]
</p><p>55 Then (i) If d = 1,  ⋆ E [rn (X) − r(X)]2 ≤  (ii) If d = 2, ⋆ E [rn (X) − r(X)]2 ≤  (iii) If d ≥ 3,  kn σ2 16ρ2C2 + . [sent-253, score-0.909]
</p><p>56 n (1 − kn /n + 1/n)2 kn  σ2 8ρ2C2 kn + (1 + ln kn ). [sent-254, score-3.675]
</p><p>57 n (1 − kn /n + 1/n)2 kn  ⋆ E [rn (X) − r(X)]2 ≤  2 kn σ2 8ρ2C2 − d + kn . [sent-255, score-3.636]
</p><p>58 (ii) If d = 2, there exists a sequence (kn ) such that kn → ∞, kn /n → 0, and ⋆ E [rn (X) − r(X)]2 ≤ (Λ + o(1)) ρCσ  ln n , n  for some positive constant Λ independent of ρ, C and σ2 . [sent-257, score-1.857]
</p><p>59 d  (iii) If d ≥ 3, there exists a sequence (kn ) with kn ∝ n d+2 such that ⋆ E [rn (X) − r(X)]2  (ρC)d σ2 ≤ (Λ + o(1)) n  2 d+2  ,  for some positive constant Λ independent of ρ, C and σ2 . [sent-258, score-0.909]
</p><p>60 4 Adaptation In the previous subsections, the parameter kn of the estimate with the optimal rate of convergence for the class F depends on the unknown distribution of (X,Y ), especially on the smoothness of the regression function measured by the Lipschitz constant C. [sent-261, score-0.991]
</p><p>61 In this subsection, we present a datadependent way of choosing the resampling size kn and show that, for bounded Y , the estimate with parameter chosen in such an adaptive way achieves the optimal rate of convergence (irrespectively of the resampling type). [sent-262, score-1.076]
</p><p>62 The ﬁrst half is denoted by Dn (learning ⋆ ℓ ⋆ ℓ set) and is used to construct the bagged 1-NN estimate r⌊n/2⌋ (x, Dn ) = rk,⌊n/2⌋ (x, Dn ) (for the sake of clarity, we make the dependence of the estimate upon k explicit). [sent-267, score-0.228]
</p><p>63 The second half of the sample, t ˆ denoted by Dn (testing set), is used to choose k by picking kn ∈ K = {1, . [sent-268, score-0.909]
</p><p>64 Immediately from Corollary 10 and Corollary 14 we can conclude: ⋆ Theorem 15 Suppose that |Y | ≤ L almost surely, and let rn be the bagged 1-NN estimate with k ∈ K = {1, . [sent-276, score-0.286]
</p><p>65 3 Proof of Proposition 7 An easy calculation shows that n  ∑  i=1  Vi2  n  kn  i−1 1− n  =∑  i=1 n−1  = 2 ∑ 1− i=0  i n  kn 2  i − 1− n  kn  1−  kn  i n  − 1−  kn  i+1 n  − 1. [sent-317, score-4.545]
</p><p>66 Then, by the mean value theorem, kn  i 0 ≤ 1− n  i+1 − 1− n  kn  i n  1 ≤ − f′ n  kn = n  i 1− n  kn −1  . [sent-319, score-3.636]
</p><p>67 , n, by deﬁnition of the Vi ’s, i−1 1− n  kn  i 1 1− + n n  kn  n i ∑ Vi n = ∑ i=1 i=1 n  i − 1− n  kn  i . [sent-333, score-2.727]
</p><p>68 n  i n  kn  i n  kn − j  i n  Thus n  ∑ Vi  i=1  n i =∑ n i=1 n  =∑  i=1 kn  =  ∑  j=1  kn  ∑  j=1  kn 1 j nj  − 1− 1−  i n  kn 1 i 1 n i 1− j−1 n ∑ n j n n i=1  kn − j  . [sent-334, score-6.375]
</p><p>69 , kn , we use the inequality i 1 n i ∑ n 1− n n i=1  kn − j  ≤2  Z 1 0  x(1 − x)kn − j dx,  which is clearly true for j = kn , without the factor 2 in front of the integral. [sent-338, score-2.727]
</p><p>70 For j < kn , it is illustrated in Figure 1, where we have plotted the function f (x) = x(1 − x)kn − j . [sent-339, score-0.909]
</p><p>71 Consequently, n  ∑ Vi  i=1  kn kn 1 i ≤2∑ j−1 j n n j=1  704  Z 1 0  x(1 − x)kn − j dx. [sent-341, score-1.818]
</p><p>72 BAGGED N EAREST N EIGHBOR E STIMATE  1 2n  i0 n  0 f ( in ) +  +1 f ( i0n )  i0 +1 n  Figure 1: Illustration of  i0 +1 n  ≤ i0 n  n−1 n  1 n i i ∑ 1− n n i=1 n  705  kn − j  ≤2  Z 1 0  1  x(1 − x)kn − j dx. [sent-342, score-0.909]
</p><p>73 j n j ( j + 1)(kn − j + 1)  Observing ﬁnally that ( j + 1)(kn − j + 1) ≥ kn for all j = 0, . [sent-344, score-0.909]
</p><p>74 , kn − 1, we conclude 2 kn −1 kn 1 2 i ≤ ∑ j n j ≤ kn n kn j=0  n  ∑ Vi  i=1  1+  1 n  kn  . [sent-347, score-5.454]
</p><p>75 (ii) For d = 2, a reasoning similar to the one reported in statement (i) above can be followed, to show that n  i n ∑ Vi n 1 + ln i i=1  1 ≤2 kn  1 1+ n kn  −∑  j=1  kn  kn 1 j−1 j n  Z 1 0  x(1 − x)kn − j ln x dx . [sent-348, score-3.753]
</p><p>76 (m + 1)(m + 2)  Thus we may write kn  kn 1 j n j−1  −∑  j=1 kn  =  ∑  j=1  =  kn −1  ∑  j=0  Z 1 0  x(1 − x)kn − j ln x dx  Hkn − j+2 − 1 kn 1 j n j−1 (kn − j + 1)(kn − j + 2) Hkn − j+1 − 1 kn 1 . [sent-355, score-5.532]
</p><p>77 , kn − 1, we have ( j + 1)(kn − j + 1) ≥ kn , as well as 1 1 +. [sent-359, score-1.818]
</p><p>78 + 2 kn − j + 1 Z kn − j+1 dx ≤ x 1 = ln(kn − j + 1)  Hkn − j+1 − 1 =  ≤ ln(kn + 1). [sent-362, score-1.857]
</p><p>79 Therefore, kn  −∑  j=1  kn 1 j−1 j n  Z 1 0  x(1 − x)kn − j ln x dx  ≤  ln(kn + 1) kn −1 kn 1 ∑ j nj kn j=0  ≤  1 ln(kn + 1) 1+ kn n  kn  . [sent-363, score-6.453]
</p><p>80 , n − 1, n i  2 −d  i/n 1 − i/n  ≤  2 d  ,  and set consequently n−1 1 Sn = k + ∑ n n i=1  kn  i−1 1− n  i − 1− n  kn  2 d  i/n 1 − i/n  . [sent-368, score-1.818]
</p><p>81 We obtain n−1 1 Sn = k + ∑ n n i=1  kn  ∑  j=1  kn 1 j nj  i 1− n  kn − j  kn i kn 1 1 n−1 1 1− = k +∑ j−1 n ∑ n n n j n i=1 j=1 kn kn 1 1 ≤ k +2 ∑ j−1 n n j n j=1  Z 1 0  i/n 1 − i/n  2 kn − j− d  2  2  x d (1 − x)kn − j− d dx. [sent-369, score-7.284]
</p><p>82 n dℓ ℓ=1 kn 1 kn − j 2 ∏ 1 − dℓ j − 1 j ℓ=1  1  ∑ n j−1  j=1  kn −1 1 kn 1 + αd ∑ j nkn n j j=0  2 1 kn − j−1 ∏ 1 − dℓ . [sent-373, score-4.564]
</p><p>83 j + 1 ℓ=1  Thus, by technical Lemma 17, −2  kn −1 kn kn d 1 Sn ≤ k + αd ∑ j nj nn j=0  ≤  1 1 + αd 1 + kn n n  kn  −2  kn d . [sent-374, score-5.466]
</p><p>84 , n − kn + 1, Vi =  n−i kn − 1 n kn  =  kn −2 kn i ∏ 1− n− j n − kn + 1 j=0  ≤  kn −2 i kn ∏ 1− n n − kn + 1 j=0  = This yields n  ∑ Vi2 ≤  i=1  ≤  kn i 1− n − kn + 1 n  kn −1  . [sent-380, score-10.908]
</p><p>85 n−kn +1 2 i kn ∑ 1− n (n − kn + 1)2 i=1 2 i 1 n kn n 1− 2 n∑ (n − kn + 1) i=1 n  708  2(kn −1)  2(kn −1)  . [sent-381, score-3.636]
</p><p>86 BAGGED N EAREST N EIGHBOR E STIMATE  Observing ﬁnally that i 1 n ∑ 1− n n i=1  2(kn −1)  ≤ =  Z 1 0  (1 − x)2(kn −1) dx  1 , 2kn − 1  we conclude that n  ∑ Vi2 ≤  i=1  2 kn n 1 kn ≤ . [sent-382, score-1.857]
</p><p>87 6 Proof of Proposition 12 Recall that  n  rn (x) = ∑ Vi r(X(i) (x)), ˜⋆ i=1  and observe that rn (x) = E⋆ r(X⋆ (x)) , ˜⋆ (1) where X⋆ (x) is the nearest neighbor of x in a random subsample Sn drawn without replacement (1) from {(X1 ,Y1 ), . [sent-384, score-0.416]
</p><p>88 , (Xn ,Yn )} with Card(Sn ) = kn , and E⋆ denotes expectation with respect to the resampling distribution, conditionally on the data set Dn . [sent-387, score-0.962]
</p><p>89 (1)  Since Card(Sn ) = kn , we conclude by applying Corollary 6, with i = 1 and replacing n by kn . [sent-389, score-1.818]
</p><p>90 n+1  Lemma 17 For each d ≥ 3, each kn ≥ 1, and j = 0, . [sent-402, score-0.909]
</p><p>91 , kn − 1, we have 2 1 kn − j−1 ∏ 1 − dℓ j + 1 ℓ=1  −2  ≤ kn d . [sent-405, score-2.727]
</p><p>92 Proof First, since 0 ≤ 1 − x ≤ e−x for all x ∈ [0, 1], kn − j−1  ∏  ℓ=1  1−  2 dℓ  ≤ exp −  2 kn − j−1 1 ∑ d ℓ=1 ℓ  . [sent-406, score-1.818]
</p><p>93 + 1/p ≥ ln(p + 1), we deduce kn − j−1  ∏  ℓ=1  1−  2 dℓ  2  ≤ (kn − j)− d . [sent-410, score-0.909]
</p><p>94 j+1  To see this, note that the inequality may be written under the equivalent form j 1− kn  2 −d  ≤ 1 + j = 1 + kn ·  j . [sent-415, score-1.818]
</p><p>95 kn  The result can easily be deduced from a comparison between the maps ϕ : x → (1 − x)−2/d and 2/d ψ : x → 1 + kn x on the interval [0, 1 − 1/kn ]. [sent-416, score-1.818]
</p><p>96 Just note that ϕ(0) = ψ(0), ϕ(1 − 1/kn ) = kn ≤ kn = ψ(1 − 1/kn ) since d ≥ 3, and ϕ is convex while ψ is afﬁne. [sent-417, score-1.818]
</p><p>97 On the rate of convergence of nearest neighbor rules. [sent-511, score-0.209]
</p><p>98 Rates of convergence of nearest neighbor estimation under arbitrary sampling. [sent-559, score-0.196]
</p><p>99 On the ﬁnite sample performance of the nearest neighbor classiﬁer. [sent-572, score-0.181]
</p><p>100 The growth rate of sample complexity with dimension for the nearest neighbor classiﬁer. [sent-591, score-0.194]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kn', 0.909), ('bagged', 0.186), ('rou', 0.11), ('bagging', 0.106), ('iau', 0.095), ('uyader', 0.095), ('nearest', 0.093), ('vi', 0.092), ('earest', 0.088), ('neighbor', 0.088), ('rn', 0.079), ('stimate', 0.075), ('eighbor', 0.075), ('fx', 0.057), ('resampling', 0.053), ('replacement', 0.049), ('dn', 0.047), ('biau', 0.039), ('ln', 0.039), ('dx', 0.039), ('devroye', 0.035), ('gy', 0.033), ('radius', 0.031), ('sn', 0.03), ('ibragimov', 0.029), ('rennes', 0.029), ('rkn', 0.029), ('covering', 0.029), ('proposition', 0.028), ('subsample', 0.028), ('rd', 0.027), ('card', 0.022), ('regression', 0.022), ('hkn', 0.022), ('resamples', 0.022), ('iii', 0.021), ('estimate', 0.021), ('ensemble', 0.02), ('minimax', 0.02), ('corollary', 0.019), ('theorem', 0.019), ('irrespectively', 0.019), ('tihomirov', 0.019), ('nkn', 0.019), ('predictor', 0.019), ('universally', 0.018), ('forests', 0.017), ('arnaud', 0.017), ('neighbour', 0.017), ('kolmogorov', 0.016), ('breiman', 0.015), ('nonparametric', 0.015), ('adaptation', 0.015), ('fr', 0.015), ('aviation', 0.015), ('bretagne', 0.015), ('gradshteyn', 0.015), ('guyader', 0.015), ('posner', 0.015), ('psaltis', 0.015), ('randolph', 0.015), ('samworth', 0.015), ('snapp', 0.015), ('steele', 0.015), ('texas', 0.015), ('usaf', 0.015), ('convergence', 0.015), ('hlmann', 0.015), ('kulkarni', 0.014), ('suppose', 0.013), ('rate', 0.013), ('venkatesh', 0.012), ('discriminatory', 0.012), ('roli', 0.012), ('kittler', 0.012), ('paris', 0.012), ('weighted', 0.012), ('nj', 0.012), ('ii', 0.012), ('bounded', 0.012), ('campus', 0.011), ('rard', 0.011), ('resample', 0.011), ('cedex', 0.011), ('lipschitz', 0.011), ('smoothness', 0.011), ('inf', 0.011), ('letting', 0.011), ('proof', 0.01), ('layered', 0.01), ('curie', 0.01), ('buja', 0.01), ('crude', 0.01), ('ric', 0.01), ('inria', 0.01), ('euclidean', 0.01), ('france', 0.01), ('consistent', 0.01), ('zt', 0.01), ('bootstrap', 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="86-tfidf-1" href="./jmlr-2010-On_the_Rate_of_Convergence_of_the_Bagged_Nearest_Neighbor_Estimate.html">86 jmlr-2010-On the Rate of Convergence of the Bagged Nearest Neighbor Estimate</a></p>
<p>Author: Gérard Biau, Frédéric Cérou, Arnaud Guyader</p><p>Abstract: Bagging is a simple way to combine estimates in order to improve their performance. This method, suggested by Breiman in 1996, proceeds by resampling from the original data set, constructing a predictor from each subsample, and decide by combining. By bagging an n-sample, the crude nearest neighbor regression estimate is turned into a consistent weighted nearest neighbor regression estimate, which is amenable to statistical analysis. Letting the resampling size kn grows appropriately with n, it is shown that this estimate may achieve optimal rate of convergence, independently from the fact that resampling is done with or without replacement. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, adaptation results by data-splitting are presented. Keywords: bagging, resampling, nearest neighbor estimate, rates of convergence</p><p>2 0.045325283 <a title="86-tfidf-2" href="./jmlr-2010-On_Finding_Predictors_for_Arbitrary_Families_of_Processes.html">81 jmlr-2010-On Finding Predictors for Arbitrary Families of Processes</a></p>
<p>Author: Daniil Ryabko</p><p>Abstract: The problem is sequence prediction in the following setting. A sequence x1 , . . . , xn , . . . of discretevalued observations is generated according to some unknown probabilistic law (measure) µ. After observing each outcome, it is required to give the conditional probabilities of the next observation. The measure µ belongs to an arbitrary but known class C of stochastic process measures. We are interested in predictors ρ whose conditional probabilities converge (in some sense) to the “true” µ-conditional probabilities, if any µ ∈ C is chosen to generate the sequence. The contribution of this work is in characterizing the families C for which such predictors exist, and in providing a speciﬁc and simple form in which to look for a solution. We show that if any predictor works, then there exists a Bayesian predictor, whose prior is discrete, and which works too. We also ﬁnd several sufﬁcient and necessary conditions for the existence of a predictor, in terms of topological characterizations of the family C , as well as in terms of local behaviour of the measures in C , which in some cases lead to procedures for constructing such predictors. It should be emphasized that the framework is completely general: the stochastic processes considered are not required to be i.i.d., stationary, or to belong to any parametric or countable family. Keywords: sequence prediction, time series, online prediction, Bayesian prediction</p><p>3 0.03660322 <a title="86-tfidf-3" href="./jmlr-2010-Using_Contextual_Representations_to_Efficiently_Learn_Context-Free_Languages.html">115 jmlr-2010-Using Contextual Representations to Efficiently Learn Context-Free Languages</a></p>
<p>Author: Alexander Clark, Rémi Eyraud, Amaury Habrard</p><p>Abstract: We present a polynomial update time algorithm for the inductive inference of a large class of context-free languages using the paradigm of positive data and a membership oracle. We achieve this result by moving to a novel representation, called Contextual Binary Feature Grammars (CBFGs), which are capable of representing richly structured context-free languages as well as some context sensitive languages. These representations explicitly model the lattice structure of the distribution of a set of substrings and can be inferred using a generalisation of distributional learning. This formalism is an attempt to bridge the gap between simple learnable classes and the sorts of highly expressive representations necessary for linguistic representation: it allows the learnability of a large class of context-free languages, that includes all regular languages and those context-free languages that satisfy two simple constraints. The formalism and the algorithm seem well suited to natural language and in particular to the modeling of ﬁrst language acquisition. Preliminary experimental results conﬁrm the effectiveness of this approach. Keywords: grammatical inference, context-free language, positive data only, membership queries</p><p>4 0.035559595 <a title="86-tfidf-4" href="./jmlr-2010-Graph_Kernels.html">44 jmlr-2010-Graph Kernels</a></p>
<p>Author: S.V.N. Vishwanathan, Nicol N. Schraudolph, Risi Kondor, Karsten M. Borgwardt</p><p>Abstract: We present a uniﬁed framework to study graph kernels, special cases of which include the random a walk (G¨ rtner et al., 2003; Borgwardt et al., 2005) and marginalized (Kashima et al., 2003, 2004; Mah´ et al., 2004) graph kernels. Through reduction to a Sylvester equation we improve the time e complexity of kernel computation between unlabeled graphs with n vertices from O(n6 ) to O(n3 ). We ﬁnd a spectral decomposition approach even more efﬁcient when computing entire kernel matrices. For labeled graphs we develop conjugate gradient and ﬁxed-point methods that take O(dn3 ) time per iteration, where d is the size of the label set. By extending the necessary linear algebra to Reproducing Kernel Hilbert Spaces (RKHS) we obtain the same result for d-dimensional edge kernels, and O(n4 ) in the inﬁnite-dimensional case; on sparse graphs these algorithms only take O(n2 ) time per iteration in all cases. Experiments on graphs from bioinformatics and other application domains show that these techniques can speed up computation of the kernel by an order of magnitude or more. We also show that certain rational kernels (Cortes et al., 2002, 2003, 2004) when specialized to graphs reduce to our random walk graph kernel. Finally, we relate our framework to R-convolution kernels (Haussler, 1999) and provide a kernel that is close to the optimal assignment o kernel of Fr¨ hlich et al. (2006) yet provably positive semi-deﬁnite. Keywords: linear algebra in RKHS, Sylvester equations, spectral decomposition, bioinformatics, rational kernels, transducers, semirings, random walks</p><p>5 0.026285106 <a title="86-tfidf-5" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>Author: Nicola Segata, Enrico Blanzieri</p><p>Abstract: A computationally efﬁcient approach to local learning with kernel methods is presented. The Fast Local Kernel Support Vector Machine (FaLK-SVM) trains a set of local SVMs on redundant neighbourhoods in the training set and an appropriate model for each query point is selected at testing time according to a proximity strategy. Supported by a recent result by Zakai and Ritov (2009) relating consistency and localizability, our approach achieves high classiﬁcation accuracies by dividing the separation function in local optimisation problems that can be handled very efﬁciently from the computational viewpoint. The introduction of a fast local model selection further speeds-up the learning process. Learning and complexity bounds are derived for FaLK-SVM, and the empirical evaluation of the approach (with data sets up to 3 million points) showed that it is much faster and more accurate and scalable than state-of-the-art accurate and approximated SVM solvers at least for non high-dimensional data sets. More generally, we show that locality can be an important factor to sensibly speed-up learning approaches and kernel methods, differently from other recent techniques that tend to dismiss local information in order to improve scalability. Keywords: locality, kernel methods, local learning algorithms, support vector machines, instancebased learning</p><p>6 0.023838377 <a title="86-tfidf-6" href="./jmlr-2010-Hilbert_Space_Embeddings_and_Metrics_on_Probability_Measures.html">47 jmlr-2010-Hilbert Space Embeddings and Metrics on Probability Measures</a></p>
<p>7 0.023790931 <a title="86-tfidf-7" href="./jmlr-2010-An_Exponential_Model_for_Infinite_Rankings.html">10 jmlr-2010-An Exponential Model for Infinite Rankings</a></p>
<p>8 0.023127891 <a title="86-tfidf-8" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>9 0.022782592 <a title="86-tfidf-9" href="./jmlr-2010-Characterization%2C_Stability_and_Convergence_of_Hierarchical_Clustering_Methods.html">19 jmlr-2010-Characterization, Stability and Convergence of Hierarchical Clustering Methods</a></p>
<p>10 0.021519786 <a title="86-tfidf-10" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>11 0.021120705 <a title="86-tfidf-11" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>12 0.02039657 <a title="86-tfidf-12" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>13 0.02035233 <a title="86-tfidf-13" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>14 0.019461773 <a title="86-tfidf-14" href="./jmlr-2010-Consistent_Nonparametric_Tests_of_Independence.html">27 jmlr-2010-Consistent Nonparametric Tests of Independence</a></p>
<p>15 0.01865289 <a title="86-tfidf-15" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>16 0.018086063 <a title="86-tfidf-16" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>17 0.017947307 <a title="86-tfidf-17" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>18 0.017749682 <a title="86-tfidf-18" href="./jmlr-2010-Learning_Translation_Invariant_Kernels_for_Classification.html">65 jmlr-2010-Learning Translation Invariant Kernels for Classification</a></p>
<p>19 0.017693497 <a title="86-tfidf-19" href="./jmlr-2010-Learning_Gradients%3A_Predictive_Models_that_Infer_Geometry_and_Statistical_Dependence.html">62 jmlr-2010-Learning Gradients: Predictive Models that Infer Geometry and Statistical Dependence</a></p>
<p>20 0.016453994 <a title="86-tfidf-20" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.071), (1, -0.028), (2, 0.018), (3, 0.007), (4, -0.052), (5, -0.007), (6, -0.062), (7, 0.02), (8, -0.003), (9, 0.03), (10, 0.021), (11, 0.009), (12, -0.01), (13, -0.072), (14, -0.023), (15, -0.053), (16, -0.065), (17, -0.041), (18, 0.017), (19, -0.04), (20, 0.027), (21, -0.027), (22, 0.048), (23, 0.008), (24, -0.107), (25, 0.067), (26, -0.022), (27, -0.02), (28, -0.059), (29, 0.042), (30, -0.022), (31, -0.009), (32, 0.008), (33, 0.169), (34, 0.158), (35, 0.023), (36, 0.053), (37, 0.273), (38, 0.262), (39, 0.113), (40, 0.048), (41, 0.069), (42, -0.49), (43, -0.122), (44, -0.02), (45, 0.175), (46, 0.036), (47, 0.333), (48, -0.243), (49, -0.198)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99431062 <a title="86-lsi-1" href="./jmlr-2010-On_the_Rate_of_Convergence_of_the_Bagged_Nearest_Neighbor_Estimate.html">86 jmlr-2010-On the Rate of Convergence of the Bagged Nearest Neighbor Estimate</a></p>
<p>Author: Gérard Biau, Frédéric Cérou, Arnaud Guyader</p><p>Abstract: Bagging is a simple way to combine estimates in order to improve their performance. This method, suggested by Breiman in 1996, proceeds by resampling from the original data set, constructing a predictor from each subsample, and decide by combining. By bagging an n-sample, the crude nearest neighbor regression estimate is turned into a consistent weighted nearest neighbor regression estimate, which is amenable to statistical analysis. Letting the resampling size kn grows appropriately with n, it is shown that this estimate may achieve optimal rate of convergence, independently from the fact that resampling is done with or without replacement. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, adaptation results by data-splitting are presented. Keywords: bagging, resampling, nearest neighbor estimate, rates of convergence</p><p>2 0.40026158 <a title="86-lsi-2" href="./jmlr-2010-Using_Contextual_Representations_to_Efficiently_Learn_Context-Free_Languages.html">115 jmlr-2010-Using Contextual Representations to Efficiently Learn Context-Free Languages</a></p>
<p>Author: Alexander Clark, Rémi Eyraud, Amaury Habrard</p><p>Abstract: We present a polynomial update time algorithm for the inductive inference of a large class of context-free languages using the paradigm of positive data and a membership oracle. We achieve this result by moving to a novel representation, called Contextual Binary Feature Grammars (CBFGs), which are capable of representing richly structured context-free languages as well as some context sensitive languages. These representations explicitly model the lattice structure of the distribution of a set of substrings and can be inferred using a generalisation of distributional learning. This formalism is an attempt to bridge the gap between simple learnable classes and the sorts of highly expressive representations necessary for linguistic representation: it allows the learnability of a large class of context-free languages, that includes all regular languages and those context-free languages that satisfy two simple constraints. The formalism and the algorithm seem well suited to natural language and in particular to the modeling of ﬁrst language acquisition. Preliminary experimental results conﬁrm the effectiveness of this approach. Keywords: grammatical inference, context-free language, positive data only, membership queries</p><p>3 0.24715438 <a title="86-lsi-3" href="./jmlr-2010-On_Finding_Predictors_for_Arbitrary_Families_of_Processes.html">81 jmlr-2010-On Finding Predictors for Arbitrary Families of Processes</a></p>
<p>Author: Daniil Ryabko</p><p>Abstract: The problem is sequence prediction in the following setting. A sequence x1 , . . . , xn , . . . of discretevalued observations is generated according to some unknown probabilistic law (measure) µ. After observing each outcome, it is required to give the conditional probabilities of the next observation. The measure µ belongs to an arbitrary but known class C of stochastic process measures. We are interested in predictors ρ whose conditional probabilities converge (in some sense) to the “true” µ-conditional probabilities, if any µ ∈ C is chosen to generate the sequence. The contribution of this work is in characterizing the families C for which such predictors exist, and in providing a speciﬁc and simple form in which to look for a solution. We show that if any predictor works, then there exists a Bayesian predictor, whose prior is discrete, and which works too. We also ﬁnd several sufﬁcient and necessary conditions for the existence of a predictor, in terms of topological characterizations of the family C , as well as in terms of local behaviour of the measures in C , which in some cases lead to procedures for constructing such predictors. It should be emphasized that the framework is completely general: the stochastic processes considered are not required to be i.i.d., stationary, or to belong to any parametric or countable family. Keywords: sequence prediction, time series, online prediction, Bayesian prediction</p><p>4 0.17745033 <a title="86-lsi-4" href="./jmlr-2010-Characterization%2C_Stability_and_Convergence_of_Hierarchical_Clustering_Methods.html">19 jmlr-2010-Characterization, Stability and Convergence of Hierarchical Clustering Methods</a></p>
<p>Author: Gunnar Carlsson, Facundo Mémoli</p><p>Abstract: We study hierarchical clustering schemes under an axiomatic view. We show that within this framework, one can prove a theorem analogous to one of Kleinberg (2002), in which one obtains an existence and uniqueness theorem instead of a non-existence result. We explore further properties of this unique scheme: stability and convergence are established. We represent dendrograms as ultrametric spaces and use tools from metric geometry, namely the Gromov-Hausdorff distance, to quantify the degree to which perturbations in the input metric space affect the result of hierarchical methods. Keywords: clustering, hierarchical clustering, stability of clustering, Gromov-Hausdorff distance</p><p>5 0.16718042 <a title="86-lsi-5" href="./jmlr-2010-Model-based_Boosting_2.0.html">77 jmlr-2010-Model-based Boosting 2.0</a></p>
<p>Author: Torsten Hothorn, Peter Bühlmann, Thomas Kneib, Matthias Schmid, Benjamin Hofner</p><p>Abstract: We describe version 2.0 of the R add-on package mboost. The package implements boosting for optimizing general risk functions using component-wise (penalized) least squares estimates or regression trees as base-learners for ﬁtting generalized linear, additive and interaction models to potentially high-dimensional data. Keywords: component-wise functional gradient descent, splines, decision trees 1. Overview The R add-on package mboost (Hothorn et al., 2010) implements tools for ﬁtting and evaluating a variety of regression and classiﬁcation models that have been suggested in machine learning and statistics. Optimization within the empirical risk minimization framework is performed via a component-wise functional gradient descent algorithm. The algorithm originates from the statistical view on boosting algorithms (Friedman et al., 2000; Bühlmann and Yu, 2003). The theory and its implementation in mboost allow for ﬁtting complex prediction models, taking potentially many interactions of features into account, as well as for ﬁtting additive and linear models. The model class the package deals with is best described by so-called structured additive regression (STAR) models, where some characteristic ξ of the conditional distribution of a response variable Y given features X is modeled through a regression function f of the features ξ(Y |X = x) = f (x). In order to facilitate parsimonious and interpretable models, the regression function f is structured, that is, restricted to additive functions f (x) = ∑ p f j (x). Each model component f j (x) might take only j=1 c 2010 Torsten Hothorn, Peter Bühlmann, Thomas Kneib, Matthias Schmid and Benjamin Hofner. H OTHORN , B ÜHLMANN , K NEIB , S CHMID AND H OFNER a subset of the features into account. Special cases are linear models f (x) = x⊤ β, additive models f (x) = ∑ p f j (x( j) ), where f j is a function of the jth feature x( j) only (smooth functions or j=1 stumps, for example) or a more complex function where f (x) is implicitly deﬁned as the sum of multiple decision trees including higher-order interactions. The latter case corresponds to boosting with trees. Combinations of these structures are also possible. The most important advantage of such a decomposition of the regression function is that each component of a ﬁtted model can be looked at and interpreted separately for gaining a better understanding of the model at hand. The characteristic ξ of the distribution depends on the measurement scale of the response Y and the scientiﬁc question to be answered. For binary or numeric variables, some function of the expectation may be appropriate, but also quantiles or expectiles may be interesting. The deﬁnition of ξ is determined by deﬁning a loss function ρ whose empirical risk is to be minimized under some algorithmic constraints (i.e., limited number of boosting iterations). The model is then ﬁtted using n p ( fˆ1 , . . . , fˆp ) = argmin ∑ wi ρ yi , ∑ f j (x) . ( f1 ,..., f p ) i=1 j=1 Here (yi , xi ), i = 1, . . . , n, are n training samples with responses yi and potentially high-dimensional feature vectors xi , and wi are some weights. The component-wise boosting algorithm starts with some offset for f and iteratively ﬁts residuals deﬁned by the negative gradient of the loss function evaluated at the current ﬁt by updating only the best model component in each iteration. The details have been described by Bühlmann and Yu (2003). Early stopping via resampling approaches or AIC leads to sparse models in the sense that only a subset of important model components f j deﬁnes the ﬁnal model. A more thorough introduction to boosting with applications in statistics based on version 1.0 of mboost is given by Bühlmann and Hothorn (2007). As of version 2.0, the package allows for ﬁtting models to binary, numeric, ordered and censored responses, that is, regression of the mean, robust regression, classiﬁcation (logistic and exponential loss), ordinal regression,1 quantile1 and expectile1 regression, censored regression (including Cox, Weibull1 , log-logistic1 or lognormal1 models) as well as Poisson and negative binomial regression1 for count data can be performed. Because the structure of the regression function f (x) can be chosen independently from the loss function ρ, interesting new models can be ﬁtted (e.g., in geoadditive regression, Kneib et al., 2009). 2. Design and Implementation The package incorporates an infrastructure for representing loss functions (so-called ‘families’), base-learners deﬁning the structure of the regression function and thus the model components f j , and a generic implementation of component-wise functional gradient descent. The main progress in version 2.0 is that only one implementation of the boosting algorithm is applied to all possible models (linear, additive, tree-based) and all families. Earlier versions were based on three implementations, one for linear models, one for additive models, and one for tree-based boosting. In comparison to the 1.0 series, the reduced code basis is easier to maintain, more robust and regression tests have been set-up in a more uniﬁed way. Speciﬁcally, the new code basis results in an enhanced and more user-friendly formula interface. In addition, convenience functions for hyperparameter selection, faster computation of predictions and improved visual model diagnostics are available. 1. Model family is new in version 2.0 or was added after the release of mboost 1.0. 2110 M ODEL - BASED B OOSTING 2.0 Currently implemented base-learners include component-wise linear models (where only one variable is updated in each iteration of the algorithm), additive models with quadratic penalties (e.g., for ﬁtting smooth functions via penalized splines, varying coefﬁcients or bi- and trivariate tensor product splines, Schmid and Hothorn, 2008), and trees. As a major improvement over the 1.0 series, computations on larger data sets (both with respect to the number of observations and the number of variables) are now facilitated by memory efﬁcient implementations of the base-learners, mostly by applying sparse matrix techniques (package Matrix, Bates and Mächler, 2009) and parallelization for a cross-validation-based choice of the number of boosting iterations (per default via package multicore, Urbanek, 2009). A more elaborate description of mboost 2.0 features is available from the mboost vignette.2 3. User Interface by Example We illustrate the main components of the user-interface by a small example on human body fat composition: Garcia et al. (2005) used a linear model for predicting body fat content by means of common anthropometric measurements that were obtained for n = 71 healthy German women. In addition, the women’s body composition was measured by Dual Energy X-Ray Absorptiometry (DXA). The aim is to describe the DXA measurements as a function of the anthropometric features. Here, we extend the linear model by i) an intrinsic variable selection via early stopping, ii) additional terms allowing for smooth deviations from linearity where necessary (by means of penalized splines orthogonalized to the linear effect, Kneib et al., 2009), iii) a possible interaction between two variables with known impact on body fat composition (hip and waist circumference) and iv) using a robust median regression approach instead of L2 risk. For the data (available as data frame bodyfat), the model structure is speciﬁed via a formula involving the base-learners corresponding to the different model components (linear terms: bols(); smooth terms: bbs(); interactions: btree()). The loss function (here, the check function for the 0.5 quantile) along with its negative gradient function are deﬁned by the QuantReg(0.5) family (Fenske et al., 2009). The model structure (speciﬁed using the formula fm), the data and the family are then passed to function mboost() for model ﬁtting:3 R> library(</p><p>6 0.16138819 <a title="86-lsi-6" href="./jmlr-2010-An_Exponential_Model_for_Infinite_Rankings.html">10 jmlr-2010-An Exponential Model for Infinite Rankings</a></p>
<p>7 0.13242954 <a title="86-lsi-7" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>8 0.11821148 <a title="86-lsi-8" href="./jmlr-2010-A_Convergent_Online_Single_Time_Scale_Actor_Critic_Algorithm.html">2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</a></p>
<p>9 0.11116747 <a title="86-lsi-9" href="./jmlr-2010-A_Rotation_Test_to_Verify_Latent_Structure.html">6 jmlr-2010-A Rotation Test to Verify Latent Structure</a></p>
<p>10 0.10350917 <a title="86-lsi-10" href="./jmlr-2010-Learning_Gradients%3A_Predictive_Models_that_Infer_Geometry_and_Statistical_Dependence.html">62 jmlr-2010-Learning Gradients: Predictive Models that Infer Geometry and Statistical Dependence</a></p>
<p>11 0.10343289 <a title="86-lsi-11" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>12 0.10272747 <a title="86-lsi-12" href="./jmlr-2010-Tree_Decomposition_for_Large-Scale_SVM_Problems.html">113 jmlr-2010-Tree Decomposition for Large-Scale SVM Problems</a></p>
<p>13 0.10246074 <a title="86-lsi-13" href="./jmlr-2010-Hilbert_Space_Embeddings_and_Metrics_on_Probability_Measures.html">47 jmlr-2010-Hilbert Space Embeddings and Metrics on Probability Measures</a></p>
<p>14 0.10204204 <a title="86-lsi-14" href="./jmlr-2010-Graph_Kernels.html">44 jmlr-2010-Graph Kernels</a></p>
<p>15 0.099159755 <a title="86-lsi-15" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>16 0.093503639 <a title="86-lsi-16" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>17 0.090956479 <a title="86-lsi-17" href="./jmlr-2010-Collective_Inference_for__Extraction_MRFs_Coupled_with_Symmetric_Clique_Potentials.html">24 jmlr-2010-Collective Inference for  Extraction MRFs Coupled with Symmetric Clique Potentials</a></p>
<p>18 0.09062092 <a title="86-lsi-18" href="./jmlr-2010-Chromatic_PAC-Bayes_Bounds_for_Non-IID_Data%3A_Applications_to_Ranking_and_Stationary_%CE%B2-Mixing_Processes.html">20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</a></p>
<p>19 0.089755654 <a title="86-lsi-19" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>20 0.08384648 <a title="86-lsi-20" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.041), (8, 0.014), (15, 0.025), (21, 0.47), (32, 0.038), (36, 0.02), (37, 0.044), (75, 0.093), (81, 0.014), (83, 0.01), (85, 0.045), (97, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82350546 <a title="86-lda-1" href="./jmlr-2010-On_the_Rate_of_Convergence_of_the_Bagged_Nearest_Neighbor_Estimate.html">86 jmlr-2010-On the Rate of Convergence of the Bagged Nearest Neighbor Estimate</a></p>
<p>Author: Gérard Biau, Frédéric Cérou, Arnaud Guyader</p><p>Abstract: Bagging is a simple way to combine estimates in order to improve their performance. This method, suggested by Breiman in 1996, proceeds by resampling from the original data set, constructing a predictor from each subsample, and decide by combining. By bagging an n-sample, the crude nearest neighbor regression estimate is turned into a consistent weighted nearest neighbor regression estimate, which is amenable to statistical analysis. Letting the resampling size kn grows appropriately with n, it is shown that this estimate may achieve optimal rate of convergence, independently from the fact that resampling is done with or without replacement. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, adaptation results by data-splitting are presented. Keywords: bagging, resampling, nearest neighbor estimate, rates of convergence</p><p>2 0.74955475 <a title="86-lda-2" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>Author: Tong Zhang</p><p>Abstract: We consider learning formulations with non-convex objective functions that often occur in practical applications. There are two approaches to this problem: • Heuristic methods such as gradient descent that only ﬁnd a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L1 -regularization that solves the problem under some conditions. However it often leads to a sub-optimal solution in reality. This paper tries to remedy the above gap between theory and practice. In particular, we present a multi-stage convex relaxation scheme for solving problems with non-convex objective functions. For learning formulations with sparse regularization, we analyze the behavior of a speciﬁc multistage relaxation scheme. Under appropriate conditions, we show that the local solution obtained by this procedure is superior to the global solution of the standard L1 convex relaxation for learning sparse targets. Keywords: sparsity, non-convex optimization, convex relaxation, multi-stage convex relaxation</p><p>3 0.68810368 <a title="86-lda-3" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<p>Author: James Henderson, Ivan Titov</p><p>Abstract: We propose a class of Bayesian networks appropriate for structured prediction problems where the Bayesian network’s model structure is a function of the predicted output structure. These incremental sigmoid belief networks (ISBNs) make decoding possible because inference with partial output structures does not require summing over the unboundedly many compatible model structures, due to their directed edges and incrementally speciﬁed model structure. ISBNs are speciﬁcally targeted at challenging structured prediction problems such as natural language parsing, where learning the domain’s complex statistical dependencies beneﬁts from large numbers of latent variables. While exact inference in ISBNs with large numbers of latent variables is not tractable, we propose two efﬁcient approximations. First, we demonstrate that a previous neural network parsing model can be viewed as a coarse mean-ﬁeld approximation to inference with ISBNs. We then derive a more accurate but still tractable variational approximation, which proves effective in artiﬁcial experiments. We compare the effectiveness of these models on a benchmark natural language parsing task, where they achieve accuracy competitive with the state-of-the-art. The model which is a closer approximation to an ISBN has better parsing accuracy, suggesting that ISBNs are an appropriate abstract model of natural language grammar learning. Keywords: Bayesian networks, dynamic Bayesian networks, grammar learning, natural language parsing, neural networks</p><p>4 0.57700485 <a title="86-lda-4" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>Author: Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, Chih-Jen Lin</p><p>Abstract: Kernel techniques have long been used in SVM to handle linearly inseparable problems by transforming data to a high dimensional space, but training and testing large data sets is often time consuming. In contrast, we can efﬁciently train and test much larger data sets using linear SVM without kernels. In this work, we apply fast linear-SVM methods to the explicit form of polynomially mapped data and investigate implementation issues. The approach enjoys fast training and testing, but may sometimes achieve accuracy close to that of using highly nonlinear kernels. Empirical experiments show that the proposed method is useful for certain large-scale data sets. We successfully apply the proposed method to a natural language processing (NLP) application by improving the testing accuracy under some training/testing speed requirements. Keywords: decomposition methods, low-degree polynomial mapping, kernel functions, support vector machines, dependency parsing, natural language processing</p><p>5 0.34052446 <a title="86-lda-5" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>Author: Shiliang Sun, John Shawe-Taylor</p><p>Abstract: In this paper, we propose a general framework for sparse semi-supervised learning, which concerns using a small portion of unlabeled data and a few labeled data to represent target functions and thus has the merit of accelerating function evaluations when predicting the output of a new example. This framework makes use of Fenchel-Legendre conjugates to rewrite a convex insensitive loss involving a regularization with unlabeled data, and is applicable to a family of semi-supervised learning methods such as multi-view co-regularized least squares and single-view Laplacian support vector machines (SVMs). As an instantiation of this framework, we propose sparse multi-view SVMs which use a squared ε-insensitive loss. The resultant optimization is an inf-sup problem and the optimal solutions have arguably saddle-point properties. We present a globally optimal iterative algorithm to optimize the problem. We give the margin bound on the generalization error of the sparse multi-view SVMs, and derive the empirical Rademacher complexity for the induced function class. Experiments on artiﬁcial and real-world data show their effectiveness. We further give a sequential training approach to show their possibility and potential for uses in large-scale problems and provide encouraging experimental results indicating the efﬁcacy of the margin bound and empirical Rademacher complexity on characterizing the roles of unlabeled data for semi-supervised learning. Keywords: semi-supervised learning, Fenchel-Legendre conjugate, representer theorem, multiview regularization, support vector machine, statistical learning theory</p><p>6 0.33938798 <a title="86-lda-6" href="./jmlr-2010-A_Convergent_Online_Single_Time_Scale_Actor_Critic_Algorithm.html">2 jmlr-2010-A Convergent Online Single Time Scale Actor Critic Algorithm</a></p>
<p>7 0.3374657 <a title="86-lda-7" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>8 0.33668476 <a title="86-lda-8" href="./jmlr-2010-Dual_Averaging_Methods_for_Regularized_Stochastic_Learning_and_Online_Optimization.html">31 jmlr-2010-Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a></p>
<p>9 0.33606148 <a title="86-lda-9" href="./jmlr-2010-Composite_Binary_Losses.html">25 jmlr-2010-Composite Binary Losses</a></p>
<p>10 0.33433661 <a title="86-lda-10" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>11 0.33375689 <a title="86-lda-11" href="./jmlr-2010-Iterative_Scaling_and_Coordinate_Descent_Methods_for_Maximum_Entropy_Models.html">57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</a></p>
<p>12 0.33149892 <a title="86-lda-12" href="./jmlr-2010-Using_Contextual_Representations_to_Efficiently_Learn_Context-Free_Languages.html">115 jmlr-2010-Using Contextual Representations to Efficiently Learn Context-Free Languages</a></p>
<p>13 0.33129081 <a title="86-lda-13" href="./jmlr-2010-A_Comparison_of_Optimization_Methods_and_Software_for_Large-scale_L1-regularized_Linear_Classification.html">1 jmlr-2010-A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</a></p>
<p>14 0.32472199 <a title="86-lda-14" href="./jmlr-2010-Inducing_Tree-Substitution_Grammars.html">53 jmlr-2010-Inducing Tree-Substitution Grammars</a></p>
<p>15 0.32357609 <a title="86-lda-15" href="./jmlr-2010-Rate_Minimaxity_of_the_Lasso_and_Dantzig_Selector_for_thelqLoss_inlrBalls.html">96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</a></p>
<p>16 0.32286188 <a title="86-lda-16" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>17 0.32267985 <a title="86-lda-17" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>18 0.32160777 <a title="86-lda-18" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<p>19 0.32093742 <a title="86-lda-19" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>20 0.31949407 <a title="86-lda-20" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
