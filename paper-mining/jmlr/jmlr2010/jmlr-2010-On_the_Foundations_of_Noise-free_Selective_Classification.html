<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>85 jmlr-2010-On the Foundations of Noise-free Selective Classification</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-85" href="#">jmlr2010-85</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>85 jmlr-2010-On the Foundations of Noise-free Selective Classification</h1>
<br/><p>Source: <a title="jmlr-2010-85-pdf" href="http://jmlr.org/papers/volume11/el-yaniv10a/el-yaniv10a.pdf">pdf</a></p><p>Author: Ran El-Yaniv, Yair Wiener</p><p>Abstract: We consider selective classiﬁcation, a term we adopt here to refer to ‘classiﬁcation with a reject option.’ The essence in selective classiﬁcation is to trade-off classiﬁer coverage for higher accuracy. We term this trade-off the risk-coverage (RC) trade-off. Our main objective is to characterize this trade-off and to construct algorithms that can optimally or near optimally achieve the best possible trade-offs in a controlled manner. For noise-free models we present in this paper a thorough analysis of selective classiﬁcation including characterizations of RC trade-offs in various interesting settings. Keywords: classiﬁcation with a reject option, selective classiﬁcation, perfect learning, high performance classiﬁcation, risk-coverage trade-off</p><p>Reference: <a title="jmlr-2010-85-reference" href="../jmlr2010_reference/jmlr-2010-On_the_Foundations_of_Noise-free_Selective_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 IL  Computer Science Department Technion – Israel Institute of Technology Haifa 32000, Israel  Editor: Gabor Lugosi  Abstract We consider selective classiﬁcation, a term we adopt here to refer to ‘classiﬁcation with a reject option. [sent-7, score-0.493]
</p><p>2 ’ The essence in selective classiﬁcation is to trade-off classiﬁer coverage for higher accuracy. [sent-8, score-0.663]
</p><p>3 For noise-free models we present in this paper a thorough analysis of selective classiﬁcation including characterizations of RC trade-offs in various interesting settings. [sent-11, score-0.375]
</p><p>4 Keywords: classiﬁcation with a reject option, selective classiﬁcation, perfect learning, high performance classiﬁcation, risk-coverage trade-off  1. [sent-12, score-0.635]
</p><p>5 Introduction In this paper we study the trade-off between coverage and accuracy of classiﬁers with a reject option, a trade-off we refer to as the risk-coverage (RC) trade-off. [sent-13, score-0.406]
</p><p>6 Throughout the paper we use the term selective classiﬁcation to refer to ‘classiﬁcation with a reject option. [sent-15, score-0.493]
</p><p>7 Through the years, selective classiﬁcation continued to draw attention and numerous papers have been published. [sent-17, score-0.375]
</p><p>8 The attraction of effective selective classiﬁcation is rather obvious in applications where one is not concerned with, or can afford partial coverage of the domain, and/or in cases where extremely low risk is a must but is not achievable in standard classiﬁcation frameworks. [sent-18, score-0.805]
</p><p>9 Despite the relatively large number of research publications on selective classiﬁcation, the vast majority of these works have been concerned with implementing a reject option within speciﬁc learning schemes, by endowing a learning scheme (e. [sent-21, score-0.519]
</p><p>10 ” While there are many convincing accounts for the potential effectiveness of selective classiﬁcation in reducing the risk, we are not familiar with a thorough or conclusive discussions on the relative power of the numerous rejection mechanisms that have been considered so far. [sent-25, score-0.481]
</p><p>11 E L -YANIV AND W IENER  lective classiﬁcation (see Section 10) do provide some risk or coverage bounds for speciﬁc schemes (e. [sent-27, score-0.397]
</p><p>12 A thorough understanding and effective use of selective classiﬁcation requires characterization of the theoretical and practical boundaries of RC trade-offs, which are essential elements in any discussion of optimality in selective classiﬁcation. [sent-33, score-0.75]
</p><p>13 These missing elements in the current literature are critical when constructing and exploring selective classiﬁcation schemes and selective classiﬁcation algorithms that aim at achieving optimality in controlling the RC trade-off. [sent-34, score-0.75]
</p><p>14 One of our longer term goals is to provide such characterizations and introduce a notion of optimality for selective classiﬁcation in the most general agnostic model. [sent-35, score-0.408]
</p><p>15 In selective classiﬁcation the learner should output a binary selective classiﬁer deﬁned to be a pair ( f , g), with f being a standard binary classiﬁer, and g : X → [0, 1] a selection function whose meaning is as follows. [sent-50, score-0.783]
</p><p>16 When applying the selective classiﬁer to a sample x, its output is: ( f , g)(x)  re ject, w. [sent-51, score-0.394]
</p><p>17 (1)  Thus, in its most general form, the selective classiﬁer is randomized. [sent-56, score-0.375]
</p><p>18 Whenever the selection function is a zero-one rule, g : X → {0, 1}, we say that the selective classiﬁer is deterministic. [sent-57, score-0.375]
</p><p>19 , no rejection is allowed) is the special case of selective classiﬁcation where g(x) selects all points (i. [sent-60, score-0.501]
</p><p>20 The two main characteristics of a selective classiﬁer are its coverage and its risk (or “true error”). [sent-63, score-0.753]
</p><p>21 Deﬁnition 1 (coverage) The coverage of a selective classiﬁer ( f , g) is the mean value of the selection function g(X) taken over the underlying distribution P, Φ( f , g)  E [g(X)] . [sent-64, score-0.663]
</p><p>22 1606  O N THE F OUNDATIONS OF N OISE - FREE S ELECTIVE C LASSIFICATION  Deﬁnition 2 (risk) For a bounded loss function ℓ : Y × Y → [0, 1], we deﬁne the risk of a selective classiﬁer ( f , g) as the average loss on the accepted samples, R( f , g)  E [ℓ( f (X),Y ) · g(X)] . [sent-65, score-0.465]
</p><p>23 Note that (at the outset) both the coverage and risk are unknown quantities because they are deﬁned in terms of the unknown underlying distribution P. [sent-67, score-0.378]
</p><p>24 We deﬁne a learning algorithm ALG to be a (random) function that, given a sample Sm , chooses a selective classiﬁer ( f , g). [sent-68, score-0.394]
</p><p>25 We evaluate learners with respect to their coverage and risk and derive both positive and negative results on achievable risk and coverage. [sent-69, score-0.52]
</p><p>26 ALG  is applied on Sm and outputs a selective classiﬁer ( f , g). [sent-85, score-0.375]
</p><p>27 The result of the game is evaluated in terms of the risk and coverage obtained by the chosen selective classiﬁer and clearly, these are random quantities that trade-off each other. [sent-86, score-0.773]
</p><p>28 For a selective classiﬁer ( f , g) with coverage Φ( f , g) we can specify a Risk-Coverage (RC) trade-off as a bound on the risk R( f , g), expressed in terms of Φ( f , g). [sent-104, score-0.772]
</p><p>29 Using a training sample Sm , the goal in selective classiﬁcation is to output a selective classiﬁer ( f , g) that has sufﬁciently low risk with sufﬁciently high coverage. [sent-116, score-0.879]
</p><p>30 We call the trade-off between risk and coverage the risk-coverage (RC) trade-off. [sent-118, score-0.378]
</p><p>31 The best way to beneﬁt from selective classiﬁcation is to control the creation of the classiﬁer so as to meet a prescribed error/coverage speciﬁcation along the RC trade-off. [sent-119, score-0.375]
</p><p>32 The entire region depicted, called the RC plane, consisting of all (r, c) points in the rectangle of interest, where r is a risk (error) coordinate and c is a coverage coordinate. [sent-127, score-0.425]
</p><p>33 ” We say that (r, c) is (efﬁciently) achievable if there is an (efﬁcient) learning algorithm that will output a selective classiﬁer ( f , g) such that with probability of at least 1 − δ, its coverage is at least c and its risk is at most r. [sent-133, score-0.843]
</p><p>34 ” At this point we require full coverage with certainty and the achievable risk represents the lowest possible risk in our ﬁxed setting (which should be achievable with probability of at least 1 − δ). [sent-135, score-0.633]
</p><p>35 We call point c∗ perfect learning because achievable perfect learning means that we can generate a classiﬁer that never errs with certainty for the problem at hand. [sent-139, score-0.378]
</p><p>36 This curve passes somewhere in the zone labeled with a question mark and represents optimal selective classiﬁcation. [sent-142, score-0.45]
</p><p>37 Given the training set Sm , we are required to generate a “perfect” selective classiﬁer ( f , g) for which 1609  E L -YANIV AND W IENER  it is known with certainty that R( f , g) = 0. [sent-159, score-0.437]
</p><p>38 Our ﬁrst observation is Theorem 8, stating that for any ﬁnite hypothesis class F , perfect learning with guaranteed coverage is achievable by a particular selective classiﬁcation strategy. [sent-162, score-0.998]
</p><p>39 For any tolerance δ, with probability of at least 1 − δ, it is guaranteed that the coverage achieved by this strategy will be at least 1 1 − O(|F | + ln(1/δ)). [sent-163, score-0.384]
</p><p>40 We show in Theorem 7 that any other strategy that achieves perfect learning cannot have larger coverage than CSS. [sent-166, score-0.46]
</p><p>41 This distribution-free coverage guarantee (2) is proven to be nearly tight for CSS and therefore, it is the best possible bound for any selective learner. [sent-171, score-0.71]
</p><p>42 Speciﬁcally, as shown in Theorem 11, there exist a particular ﬁnite hypothesis class and a particular underlying distribution for which a matching negative result (up to multiplicative constants) holds for any consistent selective learner. [sent-172, score-0.515]
</p><p>43 This result is readily extended to any selective learner by the CSS coverage optimality of Theorem 7. [sent-173, score-0.696]
</p><p>44 We show in Theorem 14 that it is impossible to provide any coverage guarantees for perfect learning, in the general case. [sent-175, score-0.43]
</p><p>45 Speciﬁcally, for linear classiﬁers, we show a bad distribution for which any selective learner ensuring zero risk will be forced to reject the entire volume of X , thus failing to guarantee more than zero coverage. [sent-176, score-0.644]
</p><p>46 So the bad news is that perfect learning with guaranteed coverage cannot in general be achieved if the hypothesis space is inﬁnite. [sent-179, score-0.571]
</p><p>47 For any selective hypothesis ( f , g), that is consistent with a sample Sm , Theorem 21 ensures perfect learning with a high probability coverage guarantee of the following form: 1 m m Φ( f , g) ≥ 1 − O γ(F , n) ln , (3) ˆ + ln m γ(F , n) ˆ δ 1. [sent-182, score-1.17]
</p><p>48 The requirement that in perfect learning the risk is zero with certainty is dual to the requirement that the coverage is 100% with certainty in standard learning. [sent-183, score-0.604]
</p><p>49 ˆ This bound immediately yields a coverage guarantee for perfect learning of linear classiﬁers, as stated in Corollary 33. [sent-191, score-0.477]
</p><p>50 This is a powerful result providing strong indication on the potential effectiveness of perfect learning with guaranteed coverage in a variety of applications. [sent-192, score-0.458]
</p><p>51 We generalize the CSS strategy and deﬁne a “controllable selective strategy” (Deﬁnition 34). [sent-195, score-0.405]
</p><p>52 The upper envelop on the RC curve is then derived in Theorem 37 for any selective classiﬁer ( f , g) by constructing a particular bad distribution for which R( f , g) ≥  1 1 16 1 · min 2Φ − 1, 2Φ − 2 + · VCdim(F ) − ln 4Φ 4m 3 1 − 2δ  . [sent-199, score-0.539]
</p><p>53 We show that perfect selective classiﬁcation with guaranteed coverage is achievable (from a learning-theoretic perspective) by a learning strategy termed consistent selective strategy (CSS). [sent-211, score-1.347]
</p><p>54 Deﬁnition 6 (consistent selective strategy (CSS)) Given Sm , a consistent selective strategy (CSS) is a selective classiﬁcation strategy that takes f to be any hypothesis in V SF ,Sm (i. [sent-220, score-1.355]
</p><p>55 An immediate consequence is that any CSS selective hypothesis ( f , g) always satisﬁes R( f , g) = 0. [sent-225, score-0.488]
</p><p>56 The main concern, however, is whether its coverage Φ( f , g) can be bounded from below and whether any other strategy that achieves perfect learning with certainty can achieve better coverage. [sent-226, score-0.502]
</p><p>57 Theorem 7 (CSS coverage optimality) Given Sm , let ( f , g) be a selective classiﬁer chosen by any strategy that ensures zero risk with certainty for any unknown distribution P and any target concept f ∗ ∈ F . [sent-228, score-0.847]
</p><p>58 Let ( fc , gc ) be a selective classiﬁer selected by CSS using Sm . [sent-229, score-0.412]
</p><p>59 Given a hypothetical sample Sm of size ˜c , gc ) be the selective classiﬁer chosen by CSS and let ( f˜, g) be the selective classiﬁer m, let ( f ˜ ˜ ˜ chosen by any competing strategy. [sent-233, score-0.85]
</p><p>60 ˜ ˜ ˜ ˜  The next result establishes the existence of perfect learning with guaranteed coverage in the ﬁnite case. [sent-247, score-0.458]
</p><p>61 Theorem 8 (guaranteed coverage) Assume a ﬁnite F and let ( f , g) be a selective classiﬁer selected by CSS. [sent-248, score-0.397]
</p><p>62 There exist a distribution P, that depends on m and n, and a ﬁnite hypothesis class F of size n, such that for any selective classiﬁer ( f , g), chosen from F by CSS (so R( f , g) = 0) using a training sample Sm drawn i. [sent-277, score-0.527]
</p><p>63 2 2  Since the coverage Φ( f , g) is the volume of the maximal agreement set with respect to the version space V SF ,Sm , it follows that Φ( f , g) = 1 − |V SF ,Sm | ·  Bin m, n , 2δ |F | 1 2 ≤ 1 − · Bin m, , 2δ . [sent-293, score-0.435]
</p><p>64 4 There exist a distribution P, that depends on m and n, and a ﬁnite hypothesis class F of size n, such that for any selective classiﬁer ( f , g), chosen from F by CSS (so R( f , g) = 0) using a training sample Sm drawn i. [sent-298, score-0.527]
</p><p>65 We show that in the general case, perfect selective classiﬁcation with guaranteed (non-zero) coverage is not achievable even when F has a ﬁnite VC-dimension. [sent-306, score-0.885]
</p><p>66 There exist a distribution P, an inﬁnite hypothesis class F with a ﬁnite VC-dimension d, and a target hypothesis in F , such that Φ( f , g) = 0 for any selective classiﬁer ( f , g), chosen from F by CSS using a training sample Sm drawn i. [sent-311, score-0.64]
</p><p>67 A direct corollary of Theorem 14 is that, in the general case, perfect selective classiﬁcation with distribution-free guaranteed coverage is not achievable for inﬁnite hypothesis spaces. [sent-324, score-1.018]
</p><p>68 ˆ ˆ Since a maximal agreement set is a region in X , rather than an hypothesis, we formally deﬁne the dual hypothesis that matches every maximal agreement set. [sent-344, score-0.434]
</p><p>69 For any probability distribution P on X × {±1}, with probability of at least 1 − δ over the choice of Sm from Pm , any hypothesis f ∈ F consistent with Sm satisﬁes 2em 2 2 h ln (5) + ln , R( f ) ≤ ε(h, m, δ) = m h δ where R( f )  E [I( f (x) = f ∗ (x))] is the risk of f . [sent-358, score-0.427]
</p><p>70 Combining this bound with Theorem 21, we immediately obtain a data-dependent compression coverage guarantee, as stated in Corollary 28. [sent-387, score-0.378]
</p><p>71 This powerful result, which is stated in Corollary 33, indicates that consistent selective classiﬁcation might be relevant in various applications of interest. [sent-389, score-0.402]
</p><p>72 As long as the empirical version space compression set size n is sufﬁciently small compared to ˆ m, Corollary 28 provides a meaningful coverage guarantee. [sent-457, score-0.359]
</p><p>73 Is it possible to learn a selective classiﬁer with full control over this trade-off? [sent-502, score-0.375]
</p><p>74 1 Lower Envelop: Controlling the Coverage-risk Trade-off Our lower RC envelop is facilitated by the following strategy, which is a generalization of the consistent selective classiﬁcation strategy (CSS) of Deﬁnition 6. [sent-509, score-0.507]
</p><p>75 Clearly, CSS is a special case of the controllable selective strategy obtained with α = 0. [sent-511, score-0.465]
</p><p>76 The following result provides a distribution independent upper bound on the risk of the controllable selective strategy as a function of its coverage. [sent-517, score-0.574]
</p><p>77 Let ( f , g) be a selective classiﬁer chosen by a controllable selective learner after observing a training sample Sm . [sent-519, score-0.882]
</p><p>78 m δ 1625  E L -YANIV AND W IENER  Proof For any controllable selective learner with a mixing parameter α we have, Φ( f , g) = E [g(X)] = E [I(g(X) = 1)] + αE [I(g(X) = 1)] . [sent-521, score-0.468]
</p><p>79 The statement is a probabilistic lower bound on the risk of any selective classiﬁer expressed as a function of the coverage. [sent-530, score-0.484]
</p><p>80 There exists a distribution P (that depends on F ), such that for any selective classiﬁer ( f , g), chosen using a training sample Sm drawn i. [sent-533, score-0.414]
</p><p>81 There exist a distribution P, that depends on m and n, and a ﬁnite hypothesis class F of size n, such that for any selective classiﬁer ( f , g), chosen using a training sample Sm drawn i. [sent-556, score-0.527]
</p><p>82 The following method, which we term lazy CSS, is very similar to the implicit selective sampling algorithm of Cohn et al. [sent-572, score-0.403]
</p><p>83 1628  O N THE F OUNDATIONS OF N OISE - FREE S ELECTIVE C LASSIFICATION  Remark 40 For the realizable case we can modify any rejection mechanism by restricting rejection only to the region chosen for rejection by CSS. [sent-592, score-0.414]
</p><p>84 The question we discuss in this section is: what would be an appropriate optimization criterion for selective classiﬁcation? [sent-601, score-0.375]
</p><p>85 Speciﬁcally, by bounding the coverage and the risk separately (as we do in this paper) we can in principle optimize any generalized rejective risk function according to any desired rejection model including the cost, the bounded-improvement and bounded-abstention models. [sent-631, score-0.6]
</p><p>86 Herbei and Wegkamp (2006) developed excess risk bounds for the classiﬁcation with a reject option setting where the loss function is the 0-1 loss, extended such that the cost of each reject point is 0 ≤ d ≤ 1/2 (cost model; see Section 9). [sent-655, score-0.391]
</p><p>87 1631  E L -YANIV AND W IENER  Selective classiﬁcation is related to selective sampling (Atlas et al. [sent-684, score-0.375]
</p><p>88 In selective sampling the learner sequentially processes unlabeled examples, and for each one determines whether or not to request a label. [sent-686, score-0.408]
</p><p>89 While coverage bounds and label complexity bounds cannot be directly compared, we conjecture that formal connections between these two settings exist because the disagreement region plays a key role in both. [sent-695, score-0.378]
</p><p>90 Nevertheless, not enough is known about selective classiﬁcation in order to harness its power in a controlled, optimal way, or to avoid its use in cases where it cannot sufﬁciently help. [sent-700, score-0.375]
</p><p>91 In this work we made a ﬁrst step toward a rigorous analysis of selective classiﬁcation by revealing properties of the risk-coverage trade-off, which represents optimal selective classiﬁcation. [sent-701, score-0.75]
</p><p>92 What is the precise relation between selective classiﬁcation and selective sampling? [sent-706, score-0.75]
</p><p>93 Can selective classiﬁcation be rigorously analyzed in transductive, semisupervised or active settings? [sent-708, score-0.375]
</p><p>94 Here we could employ a selective strategy aiming at achieving the error rate of the best hypothesis in the class precisely (and perhaps with certainty). [sent-710, score-0.518]
</p><p>95 Noticing that x1 cos α − x′ 1 · x′ 2 · sin α is continuous in α, and applying the ′ intermediate value theorem, we know that 0 < α < π and x1 2 cos α − x′ 1 · x′ 2 · sin α = 0. [sent-841, score-0.556]
</p><p>96 Recall that all points in Sm ¯ ¯ ¯ Therefore, 0 ∀x′ ∈ Sm ¯  ′ (Rα w′ )T x′ = x1 · cos α − x′ 2 · sin α = w′T x · cos α − v′T x · sin α = 0, ¯ ¯ ¯ ¯ ¯ ¯  and they reside on the boundary of fRα w′ ,0 . [sent-844, score-0.607]
</p><p>97 ¯ ¯ Proof If (Rα w′ )T x′ ≥ 0 and (R−β w′ )T x′ ≥ 0, then ¯ ¯ ¯ ¯ ′ ′ x1 cos α − x2 · sin α ≥ 0, ′ ′ x1 cos β + x2 · sin β ≥ 0. [sent-848, score-0.556]
</p><p>98 Using the trigonometric identities cos(α − β) = cos α cos β + sin α sin β; sin(α − β) = sin α cos β − cos α sin β,  we get that cos β = cos(β + α − α) = cos(α + β) · cos α + sin(α + β) · sin α = − cos α, and sin β = sin(β + α − α) = sin(α + β) · cos α − cos(α + β) · sin α = sin α. [sent-852, score-2.224]
</p><p>99 1638  O N THE F OUNDATIONS OF N OISE - FREE S ELECTIVE C LASSIFICATION  Proof By deﬁnition we get that for all samples in Sm , ′ ′ ′ x1 2 cos α − x1 · x2 · sin α ≥ 0, ′ 2 cos β + x′ · x′ · sin β ≥ 0. [sent-857, score-0.576]
</p><p>100 x1 1 2  Multiplying the ﬁrst inequality by sin β > 0 (0 < β < π), the second inequality by sin α > 0, adding the two, and using the trigonometric identity sin(α + β) = sin α cos β + cos α sin β, we have 2  ′ sin(α + β) · x1 ≥ 0. [sent-858, score-0.882]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sm', 0.461), ('selective', 0.375), ('coverage', 0.288), ('css', 0.281), ('sf', 0.239), ('rc', 0.194), ('sin', 0.163), ('perfect', 0.142), ('elective', 0.135), ('iener', 0.135), ('oise', 0.135), ('oundations', 0.135), ('wt', 0.131), ('reject', 0.118), ('cos', 0.115), ('hypothesis', 0.113), ('fw', 0.106), ('rejection', 0.106), ('agreement', 0.092), ('classi', 0.091), ('risk', 0.09), ('ln', 0.089), ('bin', 0.082), ('characterizing', 0.08), ('envelop', 0.075), ('compression', 0.071), ('realizable', 0.069), ('lassification', 0.065), ('rd', 0.064), ('hn', 0.062), ('sn', 0.061), ('er', 0.061), ('mp', 0.061), ('controllable', 0.06), ('pr', 0.059), ('hull', 0.058), ('maximal', 0.055), ('envelopes', 0.052), ('achievable', 0.052), ('zone', 0.052), ('vcdim', 0.045), ('fv', 0.045), ('hypotheses', 0.043), ('certainty', 0.042), ('chow', 0.04), ('fr', 0.039), ('fumera', 0.037), ('gc', 0.037), ('lemma', 0.037), ('intersection', 0.036), ('ei', 0.035), ('vc', 0.033), ('learner', 0.033), ('agnostic', 0.033), ('facets', 0.032), ('sliced', 0.032), ('boundary', 0.031), ('ers', 0.031), ('strategy', 0.03), ('pietraszek', 0.03), ('wegkamp', 0.029), ('rejects', 0.029), ('guarantee', 0.028), ('lazy', 0.028), ('guaranteed', 0.028), ('consistent', 0.027), ('region', 0.027), ('free', 0.026), ('option', 0.026), ('vertices', 0.026), ('proof', 0.026), ('herbei', 0.026), ('rejective', 0.026), ('theorem', 0.026), ('disagreement', 0.025), ('plane', 0.025), ('labeled', 0.023), ('alg', 0.022), ('elevation', 0.022), ('klee', 0.022), ('tortorella', 0.022), ('binomial', 0.022), ('let', 0.022), ('rotated', 0.021), ('geometry', 0.021), ('depicted', 0.021), ('cost', 0.02), ('points', 0.02), ('samples', 0.02), ('corollary', 0.02), ('training', 0.02), ('game', 0.02), ('least', 0.019), ('bound', 0.019), ('bounds', 0.019), ('br', 0.019), ('technion', 0.019), ('atlas', 0.019), ('sample', 0.019), ('separability', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999905 <a title="85-tfidf-1" href="./jmlr-2010-On_the_Foundations_of_Noise-free_Selective_Classification.html">85 jmlr-2010-On the Foundations of Noise-free Selective Classification</a></p>
<p>Author: Ran El-Yaniv, Yair Wiener</p><p>Abstract: We consider selective classiﬁcation, a term we adopt here to refer to ‘classiﬁcation with a reject option.’ The essence in selective classiﬁcation is to trade-off classiﬁer coverage for higher accuracy. We term this trade-off the risk-coverage (RC) trade-off. Our main objective is to characterize this trade-off and to construct algorithms that can optimally or near optimally achieve the best possible trade-offs in a controlled manner. For noise-free models we present in this paper a thorough analysis of selective classiﬁcation including characterizations of RC trade-offs in various interesting settings. Keywords: classiﬁcation with a reject option, selective classiﬁcation, perfect learning, high performance classiﬁcation, risk-coverage trade-off</p><p>2 0.11510136 <a title="85-tfidf-2" href="./jmlr-2010-Classification_Methods_with_Reject_Option_Based_on_Convex_Risk_Minimization.html">21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</a></p>
<p>Author: Ming Yuan, Marten Wegkamp</p><p>Abstract: In this paper, we investigate the problem of binary classiﬁcation with a reject option in which one can withhold the decision of classifying an observation at a cost lower than that of misclassiﬁcation. Since the natural loss function is non-convex so that empirical risk minimization easily becomes infeasible, the paper proposes minimizing convex risks based on surrogate convex loss functions. A necessary and sufﬁcient condition for inﬁnite sample consistency (both risks share the same minimizer) is provided. Moreover, we show that the excess risk can be bounded through the excess surrogate risk under appropriate conditions. These bounds can be tightened by a generalized margin condition. The impact of the results is illustrated on several commonly used surrogate loss functions. Keywords: classiﬁcation, convex surrogate loss, empirical risk minimization, generalized margin condition, reject option</p><p>3 0.0922556 <a title="85-tfidf-3" href="./jmlr-2010-Bundle_Methods_for_Regularized_Risk_Minimization.html">18 jmlr-2010-Bundle Methods for Regularized Risk Minimization</a></p>
<p>Author: Choon Hui Teo, S.V.N. Vishwanthan, Alex J. Smola, Quoc V. Le</p><p>Abstract: A wide variety of machine learning problems can be described as minimizing a regularized risk functional, with different algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Machines (SVMs), Gaussian Processes, Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper describes the theory and implementation of a scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as L1 and L2 penalties. In addition to the uniﬁed framework we present tight convergence bounds, which show that our algorithm converges in O(1/ε) steps to ε precision for general convex problems and in O(log(1/ε)) steps for continuously differentiable problems. We demonstrate the performance of our general purpose solver on a variety of publicly available data sets. Keywords: optimization, subgradient methods, cutting plane method, bundle methods, regularized risk minimization, parallel optimization ∗. Also at Canberra Research Laboratory, NICTA. c 2010 Choon Hui Teo, S.V. N. Vishwanthan, Alex J. Smola and Quoc V. Le. T EO , V ISHWANATHAN , S MOLA AND L E</p><p>4 0.08406204 <a title="85-tfidf-4" href="./jmlr-2010-Dual_Averaging_Methods_for_Regularized_Stochastic_Learning_and_Online_Optimization.html">31 jmlr-2010-Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a></p>
<p>Author: Lin Xiao</p><p>Abstract: We consider regularized stochastic learning and online optimization problems, where the objective function is the sum of two convex terms: one is the loss function of the learning task, and the other is a simple regularization term such as ℓ1 -norm for promoting sparsity. We develop extensions of Nesterov’s dual averaging method, that can exploit the regularization structure in an online setting. At each iteration of these methods, the learning variables are adjusted by solving a simple minimization problem that involves the running average of all past subgradients of the loss function and the whole regularization term, not just its subgradient. In the case of ℓ1 -regularization, our method is particularly effective in obtaining sparse solutions. We show that these methods achieve the optimal convergence rates or regret bounds that are standard in the literature on stochastic and online convex optimization. For stochastic learning problems in which the loss functions have Lipschitz continuous gradients, we also present an accelerated version of the dual averaging method. Keywords: stochastic learning, online optimization, ℓ1 -regularization, structural convex optimization, dual averaging methods, accelerated gradient methods</p><p>5 0.081598245 <a title="85-tfidf-5" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>Author: Markus Ojala, Gemma C. Garriga</p><p>Abstract: We explore the framework of permutation-based p-values for assessing the performance of classiﬁers. In this paper we study two simple permutation tests. The ﬁrst test assess whether the classiﬁer has found a real class structure in the data; the corresponding null distribution is estimated by permuting the labels in the data. This test has been used extensively in classiﬁcation problems in computational biology. The second test studies whether the classiﬁer is exploiting the dependency between the features in classiﬁcation; the corresponding null distribution is estimated by permuting the features within classes, inspired by restricted randomization techniques traditionally used in statistics. This new test can serve to identify descriptive features which can be valuable information in improving the classiﬁer performance. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classiﬁer performance via permutation tests is effective. In particular, the restricted permutation test clearly reveals whether the classiﬁer exploits the interdependency between the features in the data. Keywords: classiﬁcation, labeled data, permutation tests, restricted randomization, signiﬁcance testing</p><p>6 0.072751984 <a title="85-tfidf-6" href="./jmlr-2010-On-Line_Sequential_Bin_Packing.html">80 jmlr-2010-On-Line Sequential Bin Packing</a></p>
<p>7 0.068397343 <a title="85-tfidf-7" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<p>8 0.060029268 <a title="85-tfidf-8" href="./jmlr-2010-Maximum_Likelihood_in_Cost-Sensitive_Learning%3A_Model_Specification%2C_Approximations%2C_and_Upper_Bounds.html">73 jmlr-2010-Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds</a></p>
<p>9 0.058215879 <a title="85-tfidf-9" href="./jmlr-2010-Learning_Translation_Invariant_Kernels_for_Classification.html">65 jmlr-2010-Learning Translation Invariant Kernels for Classification</a></p>
<p>10 0.057200044 <a title="85-tfidf-10" href="./jmlr-2010-Hilbert_Space_Embeddings_and_Metrics_on_Probability_Measures.html">47 jmlr-2010-Hilbert Space Embeddings and Metrics on Probability Measures</a></p>
<p>11 0.054865614 <a title="85-tfidf-11" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>12 0.054178957 <a title="85-tfidf-12" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>13 0.053807184 <a title="85-tfidf-13" href="./jmlr-2010-A_Quasi-Newton_Approach_to_Nonsmooth_Convex_Optimization_Problems_in_Machine_Learning.html">5 jmlr-2010-A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning</a></p>
<p>14 0.052177005 <a title="85-tfidf-14" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>15 0.048812505 <a title="85-tfidf-15" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>16 0.047662705 <a title="85-tfidf-16" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>17 0.04564745 <a title="85-tfidf-17" href="./jmlr-2010-Analysis_of_Multi-stage_Convex_Relaxation_for_Sparse_Regularization.html">12 jmlr-2010-Analysis of Multi-stage Convex Relaxation for Sparse Regularization</a></p>
<p>18 0.044182945 <a title="85-tfidf-18" href="./jmlr-2010-Learning_From_Crowds.html">61 jmlr-2010-Learning From Crowds</a></p>
<p>19 0.041818872 <a title="85-tfidf-19" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>20 0.039168611 <a title="85-tfidf-20" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.195), (1, -0.072), (2, 0.098), (3, 0.054), (4, 0.08), (5, 0.093), (6, -0.021), (7, 0.171), (8, -0.052), (9, 0.046), (10, -0.003), (11, -0.046), (12, -0.129), (13, 0.0), (14, -0.109), (15, -0.012), (16, -0.161), (17, 0.026), (18, -0.049), (19, 0.036), (20, -0.175), (21, 0.038), (22, -0.11), (23, 0.091), (24, 0.035), (25, 0.0), (26, 0.066), (27, -0.139), (28, 0.03), (29, 0.137), (30, 0.185), (31, -0.267), (32, -0.084), (33, 0.034), (34, 0.062), (35, -0.122), (36, -0.008), (37, -0.104), (38, 0.006), (39, 0.11), (40, -0.124), (41, -0.076), (42, -0.116), (43, 0.017), (44, 0.142), (45, 0.008), (46, -0.008), (47, -0.116), (48, -0.021), (49, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91343212 <a title="85-lsi-1" href="./jmlr-2010-On_the_Foundations_of_Noise-free_Selective_Classification.html">85 jmlr-2010-On the Foundations of Noise-free Selective Classification</a></p>
<p>Author: Ran El-Yaniv, Yair Wiener</p><p>Abstract: We consider selective classiﬁcation, a term we adopt here to refer to ‘classiﬁcation with a reject option.’ The essence in selective classiﬁcation is to trade-off classiﬁer coverage for higher accuracy. We term this trade-off the risk-coverage (RC) trade-off. Our main objective is to characterize this trade-off and to construct algorithms that can optimally or near optimally achieve the best possible trade-offs in a controlled manner. For noise-free models we present in this paper a thorough analysis of selective classiﬁcation including characterizations of RC trade-offs in various interesting settings. Keywords: classiﬁcation with a reject option, selective classiﬁcation, perfect learning, high performance classiﬁcation, risk-coverage trade-off</p><p>2 0.54299587 <a title="85-lsi-2" href="./jmlr-2010-Classification_Methods_with_Reject_Option_Based_on_Convex_Risk_Minimization.html">21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</a></p>
<p>Author: Ming Yuan, Marten Wegkamp</p><p>Abstract: In this paper, we investigate the problem of binary classiﬁcation with a reject option in which one can withhold the decision of classifying an observation at a cost lower than that of misclassiﬁcation. Since the natural loss function is non-convex so that empirical risk minimization easily becomes infeasible, the paper proposes minimizing convex risks based on surrogate convex loss functions. A necessary and sufﬁcient condition for inﬁnite sample consistency (both risks share the same minimizer) is provided. Moreover, we show that the excess risk can be bounded through the excess surrogate risk under appropriate conditions. These bounds can be tightened by a generalized margin condition. The impact of the results is illustrated on several commonly used surrogate loss functions. Keywords: classiﬁcation, convex surrogate loss, empirical risk minimization, generalized margin condition, reject option</p><p>3 0.44326663 <a title="85-lsi-3" href="./jmlr-2010-On-Line_Sequential_Bin_Packing.html">80 jmlr-2010-On-Line Sequential Bin Packing</a></p>
<p>Author: András György, Gábor Lugosi, György Ottucsàk</p><p>Abstract: We consider a sequential version of the classical bin packing problem in which items are received one by one. Before the size of the next item is revealed, the decision maker needs to decide whether the next item is packed in the currently open bin or the bin is closed and a new bin is opened. If the new item does not ﬁt, it is lost. If a bin is closed, the remaining free space in the bin accounts for a loss. The goal of the decision maker is to minimize the loss accumulated over n periods. We present an algorithm that has a cumulative loss not much larger than any strategy in a ﬁnite class of reference strategies for any sequence of items. Special attention is payed to reference strategies that use a ﬁxed threshold at each step to decide whether a new bin is opened. Some positive and negative results are presented for this case. Keywords: bin packing, on-line learning, prediction with expert advice</p><p>4 0.41696781 <a title="85-lsi-4" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<p>Author: Michel Journée, Yurii Nesterov, Peter Richtárik, Rodolphe Sepulchre</p><p>Abstract: In this paper we develop a new approach to sparse principal component analysis (sparse PCA). We propose two single-unit and two block optimization formulations of the sparse PCA problem, aimed at extracting a single sparse dominant principal component of a data matrix, or more components at once, respectively. While the initial formulations involve nonconvex functions, and are therefore computationally intractable, we rewrite them into the form of an optimization program involving maximization of a convex function on a compact set. The dimension of the search space is decreased enormously if the data matrix has many more columns (variables) than rows. We then propose and analyze a simple gradient method suited for the task. It appears that our algorithm has best convergence properties in the case when either the objective function or the feasible set are strongly convex, which is the case with our single-unit formulations and can be enforced in the block case. Finally, we demonstrate numerically on a set of random and gene expression test problems that our approach outperforms existing algorithms both in quality of the obtained solution and in computational speed. Keywords: sparse PCA, power method, gradient ascent, strongly convex sets, block algorithms</p><p>5 0.37598029 <a title="85-lsi-5" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>Author: Markus Ojala, Gemma C. Garriga</p><p>Abstract: We explore the framework of permutation-based p-values for assessing the performance of classiﬁers. In this paper we study two simple permutation tests. The ﬁrst test assess whether the classiﬁer has found a real class structure in the data; the corresponding null distribution is estimated by permuting the labels in the data. This test has been used extensively in classiﬁcation problems in computational biology. The second test studies whether the classiﬁer is exploiting the dependency between the features in classiﬁcation; the corresponding null distribution is estimated by permuting the features within classes, inspired by restricted randomization techniques traditionally used in statistics. This new test can serve to identify descriptive features which can be valuable information in improving the classiﬁer performance. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classiﬁer performance via permutation tests is effective. In particular, the restricted permutation test clearly reveals whether the classiﬁer exploits the interdependency between the features in the data. Keywords: classiﬁcation, labeled data, permutation tests, restricted randomization, signiﬁcance testing</p><p>6 0.36349195 <a title="85-lsi-6" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>7 0.34686577 <a title="85-lsi-7" href="./jmlr-2010-Learning_From_Crowds.html">61 jmlr-2010-Learning From Crowds</a></p>
<p>8 0.33043182 <a title="85-lsi-8" href="./jmlr-2010-Maximum_Likelihood_in_Cost-Sensitive_Learning%3A_Model_Specification%2C_Approximations%2C_and_Upper_Bounds.html">73 jmlr-2010-Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds</a></p>
<p>9 0.3288354 <a title="85-lsi-9" href="./jmlr-2010-Bundle_Methods_for_Regularized_Risk_Minimization.html">18 jmlr-2010-Bundle Methods for Regularized Risk Minimization</a></p>
<p>10 0.30016723 <a title="85-lsi-10" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>11 0.29101813 <a title="85-lsi-11" href="./jmlr-2010-A_Quasi-Newton_Approach_to_Nonsmooth_Convex_Optimization_Problems_in_Machine_Learning.html">5 jmlr-2010-A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning</a></p>
<p>12 0.27617395 <a title="85-lsi-12" href="./jmlr-2010-Dual_Averaging_Methods_for_Regularized_Stochastic_Learning_and_Online_Optimization.html">31 jmlr-2010-Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a></p>
<p>13 0.26113063 <a title="85-lsi-13" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>14 0.2606672 <a title="85-lsi-14" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>15 0.25937313 <a title="85-lsi-15" href="./jmlr-2010-Hilbert_Space_Embeddings_and_Metrics_on_Probability_Measures.html">47 jmlr-2010-Hilbert Space Embeddings and Metrics on Probability Measures</a></p>
<p>16 0.25476804 <a title="85-lsi-16" href="./jmlr-2010-Learnability%2C_Stability_and_Uniform_Convergence.html">60 jmlr-2010-Learnability, Stability and Uniform Convergence</a></p>
<p>17 0.24280174 <a title="85-lsi-17" href="./jmlr-2010-Learning_Translation_Invariant_Kernels_for_Classification.html">65 jmlr-2010-Learning Translation Invariant Kernels for Classification</a></p>
<p>18 0.23354827 <a title="85-lsi-18" href="./jmlr-2010-Model-based_Boosting_2.0.html">77 jmlr-2010-Model-based Boosting 2.0</a></p>
<p>19 0.22696912 <a title="85-lsi-19" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>20 0.22391503 <a title="85-lsi-20" href="./jmlr-2010-Tree_Decomposition_for_Large-Scale_SVM_Problems.html">113 jmlr-2010-Tree Decomposition for Large-Scale SVM Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.019), (3, 0.031), (8, 0.014), (21, 0.015), (32, 0.552), (36, 0.027), (37, 0.039), (75, 0.097), (85, 0.069), (96, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93004835 <a title="85-lda-1" href="./jmlr-2010-Maximum_Likelihood_in_Cost-Sensitive_Learning%3A_Model_Specification%2C_Approximations%2C_and_Upper_Bounds.html">73 jmlr-2010-Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds</a></p>
<p>Author: Jacek P. Dmochowski, Paul Sajda, Lucas C. Parra</p><p>Abstract: The presence of asymmetry in the misclassiﬁcation costs or class prevalences is a common occurrence in the pattern classiﬁcation domain. While much interest has been devoted to the study of cost-sensitive learning techniques, the relationship between cost-sensitive learning and the speciﬁcation of the model set in a parametric estimation framework remains somewhat unclear. To that end, we differentiate between the case of the model including the true posterior, and that in which the model is misspeciﬁed. In the former case, it is shown that thresholding the maximum likelihood (ML) estimate is an asymptotically optimal solution to the risk minimization problem. On the other hand, under model misspeciﬁcation, it is demonstrated that thresholded ML is suboptimal and that the risk-minimizing solution varies with the misclassiﬁcation cost ratio. Moreover, we analytically show that the negative weighted log likelihood (Elkan, 2001) is a tight, convex upper bound of the empirical loss. Coupled with empirical results on several real-world data sets, we argue that weighted ML is the preferred cost-sensitive technique. Keywords: empirical risk minimization, loss function, cost-sensitive learning, imbalanced data sets</p><p>same-paper 2 0.90752614 <a title="85-lda-2" href="./jmlr-2010-On_the_Foundations_of_Noise-free_Selective_Classification.html">85 jmlr-2010-On the Foundations of Noise-free Selective Classification</a></p>
<p>Author: Ran El-Yaniv, Yair Wiener</p><p>Abstract: We consider selective classiﬁcation, a term we adopt here to refer to ‘classiﬁcation with a reject option.’ The essence in selective classiﬁcation is to trade-off classiﬁer coverage for higher accuracy. We term this trade-off the risk-coverage (RC) trade-off. Our main objective is to characterize this trade-off and to construct algorithms that can optimally or near optimally achieve the best possible trade-offs in a controlled manner. For noise-free models we present in this paper a thorough analysis of selective classiﬁcation including characterizations of RC trade-offs in various interesting settings. Keywords: classiﬁcation with a reject option, selective classiﬁcation, perfect learning, high performance classiﬁcation, risk-coverage trade-off</p><p>3 0.8755033 <a title="85-lda-3" href="./jmlr-2010-Expectation_Truncation_and_the_Benefits_of_Preselection_In_Training_Generative_Models.html">38 jmlr-2010-Expectation Truncation and the Benefits of Preselection In Training Generative Models</a></p>
<p>Author: Jörg Lücke, Julian Eggert</p><p>Abstract: We show how a preselection of hidden variables can be used to efﬁciently train generative models with binary hidden variables. The approach is based on Expectation Maximization (EM) and uses an efﬁciently computable approximation to the sufﬁcient statistics of a given model. The computational cost to compute the sufﬁcient statistics is strongly reduced by selecting, for each data point, the relevant hidden causes. The approximation is applicable to a wide range of generative models and provides an interpretation of the beneﬁts of preselection in terms of a variational EM approximation. To empirically show that the method maximizes the data likelihood, it is applied to different types of generative models including: a version of non-negative matrix factorization (NMF), a model for non-linear component extraction (MCA), and a linear generative model similar to sparse coding. The derived algorithms are applied to both artiﬁcial and realistic data, and are compared to other models in the literature. We ﬁnd that the training scheme can reduce computational costs by orders of magnitude and allows for a reliable extraction of hidden causes. Keywords: maximum likelihood, deterministic approximations, variational EM, generative models, component extraction, multiple-cause models</p><p>4 0.86703217 <a title="85-lda-4" href="./jmlr-2010-Regularized_Discriminant_Analysis%2C_Ridge_Regression_and_Beyond.html">98 jmlr-2010-Regularized Discriminant Analysis, Ridge Regression and Beyond</a></p>
<p>Author: Zhihua Zhang, Guang Dai, Congfu Xu, Michael I. Jordan</p><p>Abstract: Fisher linear discriminant analysis (FDA) and its kernel extension—kernel discriminant analysis (KDA)—are well known methods that consider dimensionality reduction and classiﬁcation jointly. While widely deployed in practical problems, there are still unresolved issues surrounding their efﬁcient implementation and their relationship with least mean squares procedures. In this paper we address these issues within the framework of regularized estimation. Our approach leads to a ﬂexible and efﬁcient implementation of FDA as well as KDA. We also uncover a general relationship between regularized discriminant analysis and ridge regression. This relationship yields variations on conventional FDA based on the pseudoinverse and a direct equivalence to an ordinary least squares estimator. Keywords: Fisher discriminant analysis, reproducing kernel, generalized eigenproblems, ridge regression, singular value decomposition, eigenvalue decomposition</p><p>5 0.5824675 <a title="85-lda-5" href="./jmlr-2010-Classification_Methods_with_Reject_Option_Based_on_Convex_Risk_Minimization.html">21 jmlr-2010-Classification Methods with Reject Option Based on Convex Risk Minimization</a></p>
<p>Author: Ming Yuan, Marten Wegkamp</p><p>Abstract: In this paper, we investigate the problem of binary classiﬁcation with a reject option in which one can withhold the decision of classifying an observation at a cost lower than that of misclassiﬁcation. Since the natural loss function is non-convex so that empirical risk minimization easily becomes infeasible, the paper proposes minimizing convex risks based on surrogate convex loss functions. A necessary and sufﬁcient condition for inﬁnite sample consistency (both risks share the same minimizer) is provided. Moreover, we show that the excess risk can be bounded through the excess surrogate risk under appropriate conditions. These bounds can be tightened by a generalized margin condition. The impact of the results is illustrated on several commonly used surrogate loss functions. Keywords: classiﬁcation, convex surrogate loss, empirical risk minimization, generalized margin condition, reject option</p><p>6 0.55096537 <a title="85-lda-6" href="./jmlr-2010-Learnability%2C_Stability_and_Uniform_Convergence.html">60 jmlr-2010-Learnability, Stability and Uniform Convergence</a></p>
<p>7 0.54692018 <a title="85-lda-7" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>8 0.51971996 <a title="85-lda-8" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>9 0.50965267 <a title="85-lda-9" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>10 0.49870417 <a title="85-lda-10" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>11 0.49852693 <a title="85-lda-11" href="./jmlr-2010-Stability_Bounds_for_Stationary_%CF%86-mixing_and_%CE%B2-mixing_Processes.html">106 jmlr-2010-Stability Bounds for Stationary φ-mixing and β-mixing Processes</a></p>
<p>12 0.49537185 <a title="85-lda-12" href="./jmlr-2010-Chromatic_PAC-Bayes_Bounds_for_Non-IID_Data%3A_Applications_to_Ranking_and_Stationary_%CE%B2-Mixing_Processes.html">20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</a></p>
<p>13 0.48643878 <a title="85-lda-13" href="./jmlr-2010-Collective_Inference_for__Extraction_MRFs_Coupled_with_Symmetric_Clique_Potentials.html">24 jmlr-2010-Collective Inference for  Extraction MRFs Coupled with Symmetric Clique Potentials</a></p>
<p>14 0.47965899 <a title="85-lda-14" href="./jmlr-2010-Composite_Binary_Losses.html">25 jmlr-2010-Composite Binary Losses</a></p>
<p>15 0.47775394 <a title="85-lda-15" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>16 0.47703686 <a title="85-lda-16" href="./jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</a></p>
<p>17 0.47609764 <a title="85-lda-17" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>18 0.4741973 <a title="85-lda-18" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>19 0.47274062 <a title="85-lda-19" href="./jmlr-2010-An_Investigation_of_Missing_Data_Methods_for_Classification_Trees_Applied_to_Binary_Response_Data.html">11 jmlr-2010-An Investigation of Missing Data Methods for Classification Trees Applied to Binary Response Data</a></p>
<p>20 0.47101337 <a title="85-lda-20" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
