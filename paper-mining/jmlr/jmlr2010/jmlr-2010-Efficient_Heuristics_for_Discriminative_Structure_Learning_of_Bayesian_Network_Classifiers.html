<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-33" href="#">jmlr2010-33</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</h1>
<br/><p>Source: <a title="jmlr-2010-33-pdf" href="http://jmlr.org/papers/volume11/pernkopf10a/pernkopf10a.pdf">pdf</a></p><p>Author: Franz Pernkopf, Jeff A. Bilmes</p><p>Abstract: We introduce a simple order-based greedy heuristic for learning discriminative structure within generative Bayesian network classiﬁers. We propose two methods for establishing an order of N features. They are based on the conditional mutual information and classiﬁcation rate (i.e., risk), respectively. Given an ordering, we can ﬁnd a discriminative structure with O N k+1 score evaluations (where constant k is the tree-width of the sub-graph over the attributes). We present results on 25 data sets from the UCI repository, for phonetic classiﬁcation using the TIMIT database, for a visual surface inspection task, and for two handwritten digit recognition tasks. We provide classiﬁcation performance for both discriminative and generative parameter learning on both discriminatively and generatively structured networks. The discriminative structure found by our new procedures signiﬁcantly outperforms generatively produced structures, and achieves a classiﬁcation accuracy on par with the best discriminative (greedy) Bayesian network learning approach, but does so with a factor of ∼10-40 speedup. We also show that the advantages of generative discriminatively structured Bayesian network classiﬁers still hold in the case of missing features, a case where generative classiﬁers have an advantage over discriminative classiﬁers. Keywords: Bayesian networks, classiﬁcation, discriminative learning, structure learning, graphical model, missing feature</p><p>Reference: <a title="jmlr-2010-33-reference" href="../jmlr2010_reference/jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Department of Electrical Engineering University of Washington Seattle, WA 98195, USA  Editor: Russ Greiner  Abstract We introduce a simple order-based greedy heuristic for learning discriminative structure within generative Bayesian network classiﬁers. [sent-5, score-0.792]
</p><p>2 Given an ordering, we can ﬁnd a discriminative structure with O N k+1 score evaluations (where constant k is the tree-width of the sub-graph over the attributes). [sent-10, score-0.462]
</p><p>3 We provide classiﬁcation performance for both discriminative and generative parameter learning on both discriminatively and generatively structured networks. [sent-12, score-0.803]
</p><p>4 The discriminative structure found by our new procedures signiﬁcantly outperforms generatively produced structures, and achieves a classiﬁcation accuracy on par with the best discriminative (greedy) Bayesian network learning approach, but does so with a factor of ∼10-40 speedup. [sent-13, score-0.861]
</p><p>5 We also show that the advantages of generative discriminatively structured Bayesian network classiﬁers still hold in the case of missing features, a case where generative classiﬁers have an advantage over discriminative classiﬁers. [sent-14, score-1.097]
</p><p>6 Keywords: Bayesian networks, classiﬁcation, discriminative learning, structure learning, graphical model, missing feature  1. [sent-15, score-0.468]
</p><p>7 Learning the best “discriminative structure” is no less difﬁcult, largely because the cost functions that are needed to be optimized do not in general decompose (Lauritzen, 1996), but there has as of yet not been any formal hardness results in the discriminative case. [sent-27, score-0.341]
</p><p>8 For example, the resulting networks are amenable to interpretation compared to a purely discriminative model (the structure speciﬁes conditional independencies between variables that may indicate distinctive aspects of how best to discern between objects Bilmes et al. [sent-29, score-0.472]
</p><p>9 Since discriminative learning of such networks optimizes for only one inference scenario (e. [sent-31, score-0.367]
</p><p>10 Bilmes (1999, 2000) introduced the explaining away residual (EAR) for discriminative structure learning of dynamic Bayesian networks for speech recognition applications, which also happens to correspond to “synergy” in the neural code (Brenner et al. [sent-37, score-0.482]
</p><p>11 In Grossman and Domingos (2004) the conditional log likelihood (CLL) function is used to learn a discriminative structure. [sent-45, score-0.383]
</p><p>12 In a similar algorithm, the classiﬁcation rate (CR)1 has also been used for discriminative structure learning (Keogh and Pazzani, 1999; Pernkopf, 2005). [sent-50, score-0.404]
</p><p>13 The CR is the discriminative criterion with the fewest approximations, so it is expected to perform well given sufﬁcient data. [sent-52, score-0.341]
</p><p>14 Independence tests may also be used for generative structure learning using, say, mutual information (de Campos, 2006) while other recent 1. [sent-55, score-0.419]
</p><p>15 We use the CR terminology in this paper since it is somewhat more consistent with previous Bayesian network discriminative structure learning literature. [sent-57, score-0.465]
</p><p>16 An experimeny tal comparison of discriminative and generative parameter training on both discriminatively and generatively structured Bayesian network classiﬁers has been performed in Pernkopf and Bilmes (2005). [sent-60, score-0.888]
</p><p>17 An empirical and theoretical comparison of certain discriminative and generative classiﬁers (speciﬁcally logistic regression and NB) is given in Ng and Jordan (2002). [sent-61, score-0.565]
</p><p>18 It is shown that for small sample sizes the generative NB classiﬁer can outperform the discriminative model. [sent-62, score-0.565]
</p><p>19 First, a new case is made for why and when discriminatively structured generative models can be usefully used to solve multi-class classiﬁcation problems. [sent-64, score-0.407]
</p><p>20 We use, however, a discriminative scoring metric and suggest approaches for establishing the variable ordering based on conditional mutual information (CMI) (Cover and Thomas, 1991) and CR. [sent-70, score-0.665]
</p><p>21 We experimentally compare both discriminative and generative parameter training on both discriminative and generatively structured Bayesian network classiﬁers. [sent-73, score-1.085]
</p><p>22 Moreover, one of the key advantages of generative models over discriminative ones is that it is still possible to marginalize away any missing features. [sent-74, score-0.629]
</p><p>23 If it is not known at training time which features might be missing, a typical discriminative model is rendered unusable. [sent-75, score-0.397]
</p><p>24 We provide empirical results showing that discriminatively learned generative models are reasonably insensitive to such missing features and retain their advantages over generative models in such case. [sent-76, score-0.688]
</p><p>25 In Section 3, a practical case is made for why discriminative structure can be desirable. [sent-79, score-0.404]
</p><p>26 The most commonly used approaches for generative and discriminative structure and parameter learning are summarized in Section 4. [sent-80, score-0.628]
</p><p>27 , 1997) to structures that are inherently poor from a generative perspective but good from a discriminative perspective (Bilmes, 2000). [sent-133, score-0.604]
</p><p>28 Each attribute may have at most one other attribute as an additional parent which means that the tree-width of the attribute induced sub-graph is unity, that is, we have to learn a 1-tree over the attributes. [sent-139, score-0.342]
</p><p>29 Discriminative Learning in Generative Models A dichotomy exists between the two primary approaches to statistical pattern classiﬁers, generative and discriminative (Bishop and Lasserre, 2007; Jebara, 2001; Ng and Jordan, 2002; Bishop, 2006; Raina et al. [sent-149, score-0.565]
</p><p>30 , the structure of a generative Bayesian network speciﬁes conditional independencies between variables that might have a useful high-level explanation). [sent-158, score-0.39]
</p><p>31 There are several reasons for using discriminative rather than generative classiﬁers, one of which is that the classiﬁcation problem should be solved most simply and directly, and never via a more general problem such as the intermediate step of estimating the joint distribution (Vapnik, 1998). [sent-165, score-0.565]
</p><p>32 The superior performance of discriminative classiﬁers has been reported in many application domains (Ng and Jordan, 2002; Raina et al. [sent-166, score-0.341]
</p><p>33 In fact, in this paper, we make a clear distinction between learning the parameters of a generative model and learning the structure of a generative model. [sent-173, score-0.511]
</p><p>34 2328  D ISCRIMINATIVE L EARNING FOR BAYESIAN N ETWORK C LASSIFIERS  Moreover, both parameters and structure of a generative model can be learned either generatively or discriminatively. [sent-181, score-0.342]
</p><p>35 In this paper, we are particularly interested in learning the discriminative structure of a generative model. [sent-196, score-0.628]
</p><p>36 With a generative model, even discriminatively structured, some aspect of the joint distribution p(C, X) is still being represented. [sent-197, score-0.368]
</p><p>37 Of course, a discriminatively structured generative model needs only represent that aspect of the joint distribution that is beneﬁcial from a classiﬁcation error rate perspective, and need not “generate” well (Bilmes et al. [sent-198, score-0.407]
</p><p>38 For this reason, it is likely that a discriminatively trained generative model will not need to be as complex as an accurate generatively trained model. [sent-200, score-0.423]
</p><p>39 In other words, the advantage of parsimony of a discriminative model over a generative model will likely be partially if not mostly recovered when one trains a generative model discriminatively. [sent-201, score-0.789]
</p><p>40 This last point is particularly important: a discriminatively structured generative model still has the ability to go from p(C, X) to p(C, X′ ) where X′ is a subset of the features in X. [sent-203, score-0.439]
</p><p>41 A discriminative model, however, is inherently conditional and it is not possible in general when some of the features are missing to go from p(C|X) to p(C|X′ ). [sent-209, score-0.479]
</p><p>42 2329  P ERNKOPF AND B ILMES  Learning a discriminatively structured generative model is inherently a combinatorial optimization problem on a “discriminative” objective function. [sent-211, score-0.407]
</p><p>43 In the case of discriminative parameter learning, CR can be used, but typically alternative continuous and differentiable cost functions, which may upper-bound CR and might be convex (Bartlett et al. [sent-216, score-0.341]
</p><p>44 One may ask, given discriminative parameter learning, is discriminative structure still necessary? [sent-218, score-0.745]
</p><p>45 In the following, we present a simple synthetic example (similar to Narasimhan and Bilmes, 2005) and actual training and test results that indicate when a discriminative structure would be necessary for good classiﬁcation performance in a generative model. [sent-219, score-0.652]
</p><p>46 Sampling from this distribution, we ﬁrst learn structures using generative and discriminative methods, and then we perform parameter training 2330  D ISCRIMINATIVE L EARNING FOR BAYESIAN N ETWORK C LASSIFIERS  on these structures using either ML or CL (Greiner et al. [sent-246, score-0.667]
</p><p>47 For learning a discriminative structure, we apply our order-based algorithm proposed in Section 5 (we note that optimizing the EAR measure (Pernkopf and Bilmes, 2005) leads to similar results in this case). [sent-250, score-0.341]
</p><p>48 Figure 4 shows (a) the generative (b) the discriminative 1-tree over the attributes of the resulting TAN network (the class variable which is the parent of each feature is not shown in this ﬁgure). [sent-258, score-0.802]
</p><p>49 Note that for this example the difference between ML and CL parameter learning is insigniﬁcant and for the generative model, only a discriminative structure enables correct classiﬁcation. [sent-264, score-0.628]
</p><p>50 Therefore, when a generative model is desirable (see the reasons why this might be the case above), there is clearly a need for good discriminative structure learning. [sent-266, score-0.628]
</p><p>51 In this paper, we show that the loss of a “generative meaning” of a generative model (when it is structured discriminatively) does not impair the generative model’s ability to easily deal with missing features (Figure 11). [sent-267, score-0.583]
</p><p>52 4 Discriminative Structure Learning As a baseline discriminative structure learning method, we use a greedy edge augmentation method and also the SuperParent algorithm (Keogh and Pazzani, 1999). [sent-316, score-0.492]
</p><p>53 This approach is computationally expensive since each time an edge is added, the scores for all O N 2 edges need to be re-evaluated due to the discriminative non-decomposable scoring functions we employ. [sent-323, score-0.424]
</p><p>54 The CR is the discriminative criterion that, given sufﬁcient training data, most directly judges what we wish to optimize (error rate), while an alternative would be to use a convex upper-bound on the 0/1-loss function (Bartlett et al. [sent-327, score-0.365]
</p><p>55 This approach has in the literature been shown to be the algorithm that produces the best performing discriminative structure (Keogh and Pazzani, 1999; Pernkopf, 2005) but at the cost of a very expensive optimization procedure. [sent-330, score-0.404]
</p><p>56 2 S UPER PARENT AND ITS k-T REE G ENERALIZATION Keogh and Pazzani (1999) introduced the SuperParent algorithm to efﬁciently learn a discriminative TAN structure. [sent-342, score-0.341]
</p><p>57 Here, we are inspired by these ideas and apply them to the case of learning discriminative structures. [sent-361, score-0.341]
</p><p>58 Also, unlike Teyssier and Koller (2005), we establish only one ordering, and since our scoring cost is discriminative, it does not decompose and the learned discriminative structure is not guaranteed to be optimal. [sent-362, score-0.435]
</p><p>59 1 Step 1: Establishing an Order ≺ We propose three separate heuristics for establishing an ordering ≺ of the attribute nodes prior to parent selection. [sent-374, score-0.368]
</p><p>60 , 1997) and from the discriminative structure perspective (Bilmes, 2000). [sent-418, score-0.404]
</p><p>61 In our case, however, a conditional mutual information query can be computed efﬁciently by making only one pass over the training data, albeit with a potential problem with bias and variance of the mutual information estimate. [sent-425, score-0.33]
</p><p>62 Note that the set of parents is judged using CR, but the model parameters for any given candidate set of parents selected are trained using ML (we did not ﬁnd further advantages, in addition to using CR for parent selection, in also using discriminative parameter training). [sent-473, score-0.607]
</p><p>63 Different combinations of the following parameter/structure learning approaches are used to learn the classiﬁers: • Generative (ML) (Pearl, 1988) and discriminative (CL) (Greiner et al. [sent-491, score-0.341]
</p><p>64 • For OMI-CR, we also evaluate discriminative parameter learning by optimizing CL during the selection of the parent in step 2. [sent-501, score-0.437]
</p><p>65 Discriminative parameter learning while optimizing discriminative structure is computationally feasible only on rather small data sets due to the cost of the conjugate gradient parameter optimization. [sent-503, score-0.404]
</p><p>66 For the ordering heuristics “B” refers to the ordering method, “C” refers to the parent selection and internal parameter learning strategy. [sent-508, score-0.405]
</p><p>67 For discriminative parameter learning, the parameters are 2341  P ERNKOPF AND B ILMES  initialized to the values obtained by the ML approach (Greiner et al. [sent-521, score-0.341]
</p><p>68 As can be seen, our ordering heuristic performs better than both the random and the minimum mutual information orderings on 28 of the 31 cases. [sent-609, score-0.327]
</p><p>69 We compare our OMI-CR heuristic to greedy discriminative structure learning. [sent-702, score-0.507]
</p><p>70 ML and CL denote generative and discriminative parameter learning, respectively. [sent-892, score-0.565]
</p><p>71 CRCL refers to using discriminative parameter learning during structure learning. [sent-894, score-0.404]
</p><p>72 The generative structure learning algorithm is abbreviated as CMI and the greedy discriminative structure learning is TAN-CR and 2-tree-CR. [sent-895, score-0.756]
</p><p>73 , 2005)—the average improvement of discriminative parameter learning over ML estimation on NB and generative TANCMI structures is large. [sent-903, score-0.604]
</p><p>74 It can be shown that the CLL is conj cave when using log θi|h , that is, the global maximum can be found during discriminative parameter learning. [sent-912, score-0.341]
</p><p>75 6, we show that the classiﬁcation performance of a discriminatively structured model may be superior to discriminatively parameterized models in the case of missing features. [sent-916, score-0.391]
</p><p>76 Discriminative parameter learning during discriminative structure learning using our orderbased heuristics can slightly improve the performance. [sent-924, score-0.475]
</p><p>77 In summary, SuperParent achieves a lower classiﬁcation rate compared to other discriminative structure learning algorithms on most of the data sets. [sent-927, score-0.404]
</p><p>78 The TIMIT-39, MNIST, and USPS experiments show that we can perform discriminative structure learning for relatively large classiﬁcation problems (∼140000 samples, 66 features, 39 classes, ∼60000 samples, 196 features, 10 classes, and ∼8000 samples, 256 features, 10 classes, resp. [sent-935, score-0.404]
</p><p>79 CRCL refers to algorithms using discriminative parameter learning during structure learning. [sent-991, score-0.404]
</p><p>80 The generative structure learning algorithm is abbreviated as CMI and the naive greedy discriminative structure learning is TAN-CR and 2-tree-CR. [sent-992, score-0.778]
</p><p>81 CRCL refers to algorithms using discriminative parameter learning during structure learning. [sent-1044, score-0.404]
</p><p>82 The generative structure learning algorithm is abbreviated as CMI and the naive greedy discriminative structure learning is TAN-CR and 2-tree-CR. [sent-1045, score-0.778]
</p><p>83 However, the computationally expensive greedy heuristic TAN-CR and 2-tree-CR do not signiﬁcantly outperform our discriminative order-based heuristics TAN-OMI-CR and 2-tree-OMI-CR, respectively. [sent-1063, score-0.515]
</p><p>84 TAN-CMI is roughly 3-10 times faster than TAN-OMI-CR and TAN-CR takes about 10-40 times longer for establishing the discriminative structure than TAN-OMI-CR. [sent-1068, score-0.404]
</p><p>85 6 Results with Randomly Missing Input Features  As mentioned in Section 3, generative models can easily deal with missing features simply by marginalizing out from the model the missing feature. [sent-1083, score-0.384]
</p><p>86 In Figure 11, we present the classiﬁcation performance of discriminative and generative structures assuming missing features using the Ma+Fe data of TIMIT-4/6. [sent-1097, score-0.7]
</p><p>87 This demonstrates, at least empirically, that discriminatively structured generative models do not lose their ability to impute missing features. [sent-1103, score-0.471]
</p><p>88 In Figure 12, we show for the same data sets and experimental setup that the classiﬁcation performance of a discriminatively structured model may be superior to discriminatively parameterized models in the case of missing features. [sent-1106, score-0.391]
</p><p>89 This might also explain the loss in classiﬁcation performance of discriminative Bayesian networks. [sent-1119, score-0.341]
</p><p>90 Conclusion We introduced a simple order-based heuristic for learning discriminative network structure. [sent-1143, score-0.44]
</p><p>91 Given an ordering, we can ﬁnd a discriminative classiﬁer structure using O N k+1 score evaluations (where constant k is the tree-width of the sub-graph over the attributes). [sent-1146, score-0.462]
</p><p>92 We empirically compare the performance of our algorithms to state-of-the-art discriminative and generative parameter and structure learning algorithms using real data from the TIMIT speech corpus, the UCI repository, a visual surface inspection task, and from handwritten digit recognition tasks. [sent-1147, score-0.763]
</p><p>93 The experiments show that the discriminative structures found by our order-based heuristics achieve on average a signiﬁcantly better classiﬁcation performance than the generative approach. [sent-1148, score-0.675]
</p><p>94 ML and CL denote generative and discriminative parameter learning, respectively. [sent-1548, score-0.565]
</p><p>95 OMI-CRCL refers to OMI-CR using discriminative parameter learning during structure learning. [sent-1550, score-0.404]
</p><p>96 The generative structure learning algorithm is abbreviated as CMI and the greedy discriminative structure learning is TAN-CR and 2-tree-CR. [sent-1551, score-0.756]
</p><p>97 A supermodular-submodular procedure with applications to discriminative structure learning. [sent-1923, score-0.404]
</p><p>98 Discriminative versus generative parameter and structure learning of Bayesian network classiﬁers. [sent-1952, score-0.348]
</p><p>99 Efﬁcient heuristics for discriminative structure learning of Bayesian network classiﬁers. [sent-1959, score-0.536]
</p><p>100 On discriminative Bayesian network u a classiﬁers and logistic regression. [sent-2004, score-0.402]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('discriminative', 0.341), ('cll', 0.332), ('cr', 0.315), ('tan', 0.312), ('generative', 0.224), ('ml', 0.196), ('pernkopf', 0.18), ('nb', 0.162), ('discriminatively', 0.144), ('ernkopf', 0.137), ('ilmes', 0.137), ('mutual', 0.132), ('lassifiers', 0.13), ('cmi', 0.129), ('iscriminative', 0.123), ('superparent', 0.122), ('ordering', 0.119), ('bilmes', 0.111), ('bayesian', 0.109), ('cl', 0.107), ('etwork', 0.1), ('parent', 0.096), ('uper', 0.094), ('omi', 0.094), ('classi', 0.086), ('parents', 0.085), ('attribute', 0.082), ('attributes', 0.08), ('fe', 0.076), ('heuristics', 0.071), ('pazzani', 0.068), ('tree', 0.065), ('phonetic', 0.065), ('greedy', 0.065), ('missing', 0.064), ('structure', 0.063), ('keogh', 0.062), ('network', 0.061), ('earning', 0.059), ('greiner', 0.058), ('juang', 0.058), ('mnist', 0.057), ('uci', 0.056), ('generatively', 0.055), ('xm', 0.052), ('speech', 0.052), ('bs', 0.049), ('usps', 0.045), ('lassifier', 0.043), ('conditional', 0.042), ('ers', 0.041), ('structures', 0.039), ('structured', 0.039), ('heuristic', 0.038), ('orderings', 0.038), ('speakers', 0.037), ('surface', 0.037), ('mfcc', 0.036), ('zm', 0.036), ('xb', 0.035), ('cm', 0.033), ('arrow', 0.033), ('score', 0.033), ('features', 0.032), ('scoring', 0.031), ('glass', 0.031), ('segment', 0.031), ('domingos', 0.031), ('friedman', 0.029), ('node', 0.029), ('edges', 0.029), ('ephraim', 0.029), ('grossman', 0.029), ('parentless', 0.029), ('additionally', 0.028), ('umber', 0.028), ('ear', 0.028), ('timit', 0.028), ('networks', 0.026), ('evaluations', 0.025), ('bahl', 0.025), ('corral', 0.025), ('merz', 0.025), ('roos', 0.025), ('training', 0.024), ('xa', 0.024), ('sentences', 0.024), ('ax', 0.023), ('handwritten', 0.023), ('svms', 0.023), ('edge', 0.023), ('digit', 0.023), ('lecun', 0.022), ('naive', 0.022), ('tructure', 0.022), ('ean', 0.022), ('ng', 0.022), ('crcl', 0.022), ('crnew', 0.022), ('crold', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="33-tfidf-1" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>Author: Franz Pernkopf, Jeff A. Bilmes</p><p>Abstract: We introduce a simple order-based greedy heuristic for learning discriminative structure within generative Bayesian network classiﬁers. We propose two methods for establishing an order of N features. They are based on the conditional mutual information and classiﬁcation rate (i.e., risk), respectively. Given an ordering, we can ﬁnd a discriminative structure with O N k+1 score evaluations (where constant k is the tree-width of the sub-graph over the attributes). We present results on 25 data sets from the UCI repository, for phonetic classiﬁcation using the TIMIT database, for a visual surface inspection task, and for two handwritten digit recognition tasks. We provide classiﬁcation performance for both discriminative and generative parameter learning on both discriminatively and generatively structured networks. The discriminative structure found by our new procedures signiﬁcantly outperforms generatively produced structures, and achieves a classiﬁcation accuracy on par with the best discriminative (greedy) Bayesian network learning approach, but does so with a factor of ∼10-40 speedup. We also show that the advantages of generative discriminatively structured Bayesian network classiﬁers still hold in the case of missing features, a case where generative classiﬁers have an advantage over discriminative classiﬁers. Keywords: Bayesian networks, classiﬁcation, discriminative learning, structure learning, graphical model, missing feature</p><p>2 0.11553397 <a title="33-tfidf-2" href="./jmlr-2010-Maximum_Likelihood_in_Cost-Sensitive_Learning%3A_Model_Specification%2C_Approximations%2C_and_Upper_Bounds.html">73 jmlr-2010-Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds</a></p>
<p>Author: Jacek P. Dmochowski, Paul Sajda, Lucas C. Parra</p><p>Abstract: The presence of asymmetry in the misclassiﬁcation costs or class prevalences is a common occurrence in the pattern classiﬁcation domain. While much interest has been devoted to the study of cost-sensitive learning techniques, the relationship between cost-sensitive learning and the speciﬁcation of the model set in a parametric estimation framework remains somewhat unclear. To that end, we differentiate between the case of the model including the true posterior, and that in which the model is misspeciﬁed. In the former case, it is shown that thresholding the maximum likelihood (ML) estimate is an asymptotically optimal solution to the risk minimization problem. On the other hand, under model misspeciﬁcation, it is demonstrated that thresholded ML is suboptimal and that the risk-minimizing solution varies with the misclassiﬁcation cost ratio. Moreover, we analytically show that the negative weighted log likelihood (Elkan, 2001) is a tight, convex upper bound of the empirical loss. Coupled with empirical results on several real-world data sets, we argue that weighted ML is the preferred cost-sensitive technique. Keywords: empirical risk minimization, loss function, cost-sensitive learning, imbalanced data sets</p><p>3 0.096763581 <a title="33-tfidf-3" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>Author: Jitkomut Songsiri, Lieven Vandenberghe</p><p>Abstract: An algorithm is presented for topology selection in graphical models of autoregressive Gaussian time series. The graph topology of the model represents the sparsity pattern of the inverse spectrum of the time series and characterizes conditional independence relations between the variables. The method proposed in the paper is based on an ℓ1 -type nonsmooth regularization of the conditional maximum likelihood estimation problem. We show that this reduces to a convex optimization problem and describe a large-scale algorithm that solves the dual problem via the gradient projection method. Results of experiments with randomly generated and real data sets are also included. Keywords: graphical models, time series, topology selection, convex optimization</p><p>4 0.082523368 <a title="33-tfidf-4" href="./jmlr-2010-Expectation_Truncation_and_the_Benefits_of_Preselection_In_Training_Generative_Models.html">38 jmlr-2010-Expectation Truncation and the Benefits of Preselection In Training Generative Models</a></p>
<p>Author: Jörg Lücke, Julian Eggert</p><p>Abstract: We show how a preselection of hidden variables can be used to efﬁciently train generative models with binary hidden variables. The approach is based on Expectation Maximization (EM) and uses an efﬁciently computable approximation to the sufﬁcient statistics of a given model. The computational cost to compute the sufﬁcient statistics is strongly reduced by selecting, for each data point, the relevant hidden causes. The approximation is applicable to a wide range of generative models and provides an interpretation of the beneﬁts of preselection in terms of a variational EM approximation. To empirically show that the method maximizes the data likelihood, it is applied to different types of generative models including: a version of non-negative matrix factorization (NMF), a model for non-linear component extraction (MCA), and a linear generative model similar to sparse coding. The derived algorithms are applied to both artiﬁcial and realistic data, and are compared to other models in the literature. We ﬁnd that the training scheme can reduce computational costs by orders of magnitude and allows for a reliable extraction of hidden causes. Keywords: maximum likelihood, deterministic approximations, variational EM, generative models, component extraction, multiple-cause models</p><p>5 0.078973986 <a title="33-tfidf-5" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>Author: Markus Ojala, Gemma C. Garriga</p><p>Abstract: We explore the framework of permutation-based p-values for assessing the performance of classiﬁers. In this paper we study two simple permutation tests. The ﬁrst test assess whether the classiﬁer has found a real class structure in the data; the corresponding null distribution is estimated by permuting the labels in the data. This test has been used extensively in classiﬁcation problems in computational biology. The second test studies whether the classiﬁer is exploiting the dependency between the features in classiﬁcation; the corresponding null distribution is estimated by permuting the features within classes, inspired by restricted randomization techniques traditionally used in statistics. This new test can serve to identify descriptive features which can be valuable information in improving the classiﬁer performance. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classiﬁer performance via permutation tests is effective. In particular, the restricted permutation test clearly reveals whether the classiﬁer exploits the interdependency between the features in the data. Keywords: classiﬁcation, labeled data, permutation tests, restricted randomization, signiﬁcance testing</p><p>6 0.078952335 <a title="33-tfidf-6" href="./jmlr-2010-An_Exponential_Model_for_Infinite_Rankings.html">10 jmlr-2010-An Exponential Model for Infinite Rankings</a></p>
<p>7 0.076801687 <a title="33-tfidf-7" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>8 0.076150239 <a title="33-tfidf-8" href="./jmlr-2010-Posterior_Regularization_for_Structured_Latent_Variable_Models.html">91 jmlr-2010-Posterior Regularization for Structured Latent Variable Models</a></p>
<p>9 0.07010527 <a title="33-tfidf-9" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>10 0.064306214 <a title="33-tfidf-10" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>11 0.058494598 <a title="33-tfidf-11" href="./jmlr-2010-Approximate_Tree_Kernels.html">15 jmlr-2010-Approximate Tree Kernels</a></p>
<p>12 0.056074437 <a title="33-tfidf-12" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>13 0.0549003 <a title="33-tfidf-13" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>14 0.053657249 <a title="33-tfidf-14" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>15 0.052622471 <a title="33-tfidf-15" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>16 0.050977156 <a title="33-tfidf-16" href="./jmlr-2010-An_Investigation_of_Missing_Data_Methods_for_Classification_Trees_Applied_to_Binary_Response_Data.html">11 jmlr-2010-An Investigation of Missing Data Methods for Classification Trees Applied to Binary Response Data</a></p>
<p>17 0.050858472 <a title="33-tfidf-17" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<p>18 0.048479725 <a title="33-tfidf-18" href="./jmlr-2010-Why_Does_Unsupervised_Pre-training_Help_Deep_Learning%3F.html">117 jmlr-2010-Why Does Unsupervised Pre-training Help Deep Learning?</a></p>
<p>19 0.047759227 <a title="33-tfidf-19" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>20 0.047036897 <a title="33-tfidf-20" href="./jmlr-2010-Local_Causal_and_Markov_Blanket_Induction_for_Causal_Discovery_and_Feature_Selection_for_Classification_Part_I%3A_Algorithms_and_Empirical_Evaluation.html">67 jmlr-2010-Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.215), (1, 0.103), (2, -0.097), (3, 0.053), (4, 0.003), (5, 0.1), (6, 0.11), (7, 0.061), (8, -0.085), (9, 0.043), (10, -0.058), (11, 0.069), (12, 0.014), (13, -0.096), (14, -0.094), (15, -0.165), (16, 0.185), (17, 0.089), (18, -0.016), (19, 0.047), (20, -0.024), (21, -0.047), (22, -0.013), (23, -0.085), (24, 0.005), (25, 0.084), (26, 0.055), (27, -0.081), (28, 0.138), (29, -0.278), (30, 0.122), (31, -0.03), (32, 0.079), (33, -0.087), (34, -0.065), (35, 0.186), (36, -0.199), (37, 0.007), (38, 0.052), (39, -0.15), (40, -0.038), (41, 0.017), (42, -0.01), (43, 0.044), (44, -0.084), (45, 0.015), (46, -0.093), (47, 0.021), (48, -0.064), (49, -0.129)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91724372 <a title="33-lsi-1" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>Author: Franz Pernkopf, Jeff A. Bilmes</p><p>Abstract: We introduce a simple order-based greedy heuristic for learning discriminative structure within generative Bayesian network classiﬁers. We propose two methods for establishing an order of N features. They are based on the conditional mutual information and classiﬁcation rate (i.e., risk), respectively. Given an ordering, we can ﬁnd a discriminative structure with O N k+1 score evaluations (where constant k is the tree-width of the sub-graph over the attributes). We present results on 25 data sets from the UCI repository, for phonetic classiﬁcation using the TIMIT database, for a visual surface inspection task, and for two handwritten digit recognition tasks. We provide classiﬁcation performance for both discriminative and generative parameter learning on both discriminatively and generatively structured networks. The discriminative structure found by our new procedures signiﬁcantly outperforms generatively produced structures, and achieves a classiﬁcation accuracy on par with the best discriminative (greedy) Bayesian network learning approach, but does so with a factor of ∼10-40 speedup. We also show that the advantages of generative discriminatively structured Bayesian network classiﬁers still hold in the case of missing features, a case where generative classiﬁers have an advantage over discriminative classiﬁers. Keywords: Bayesian networks, classiﬁcation, discriminative learning, structure learning, graphical model, missing feature</p><p>2 0.5663237 <a title="33-lsi-2" href="./jmlr-2010-Expectation_Truncation_and_the_Benefits_of_Preselection_In_Training_Generative_Models.html">38 jmlr-2010-Expectation Truncation and the Benefits of Preselection In Training Generative Models</a></p>
<p>Author: Jörg Lücke, Julian Eggert</p><p>Abstract: We show how a preselection of hidden variables can be used to efﬁciently train generative models with binary hidden variables. The approach is based on Expectation Maximization (EM) and uses an efﬁciently computable approximation to the sufﬁcient statistics of a given model. The computational cost to compute the sufﬁcient statistics is strongly reduced by selecting, for each data point, the relevant hidden causes. The approximation is applicable to a wide range of generative models and provides an interpretation of the beneﬁts of preselection in terms of a variational EM approximation. To empirically show that the method maximizes the data likelihood, it is applied to different types of generative models including: a version of non-negative matrix factorization (NMF), a model for non-linear component extraction (MCA), and a linear generative model similar to sparse coding. The derived algorithms are applied to both artiﬁcial and realistic data, and are compared to other models in the literature. We ﬁnd that the training scheme can reduce computational costs by orders of magnitude and allows for a reliable extraction of hidden causes. Keywords: maximum likelihood, deterministic approximations, variational EM, generative models, component extraction, multiple-cause models</p><p>3 0.51008874 <a title="33-lsi-3" href="./jmlr-2010-Maximum_Likelihood_in_Cost-Sensitive_Learning%3A_Model_Specification%2C_Approximations%2C_and_Upper_Bounds.html">73 jmlr-2010-Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds</a></p>
<p>Author: Jacek P. Dmochowski, Paul Sajda, Lucas C. Parra</p><p>Abstract: The presence of asymmetry in the misclassiﬁcation costs or class prevalences is a common occurrence in the pattern classiﬁcation domain. While much interest has been devoted to the study of cost-sensitive learning techniques, the relationship between cost-sensitive learning and the speciﬁcation of the model set in a parametric estimation framework remains somewhat unclear. To that end, we differentiate between the case of the model including the true posterior, and that in which the model is misspeciﬁed. In the former case, it is shown that thresholding the maximum likelihood (ML) estimate is an asymptotically optimal solution to the risk minimization problem. On the other hand, under model misspeciﬁcation, it is demonstrated that thresholded ML is suboptimal and that the risk-minimizing solution varies with the misclassiﬁcation cost ratio. Moreover, we analytically show that the negative weighted log likelihood (Elkan, 2001) is a tight, convex upper bound of the empirical loss. Coupled with empirical results on several real-world data sets, we argue that weighted ML is the preferred cost-sensitive technique. Keywords: empirical risk minimization, loss function, cost-sensitive learning, imbalanced data sets</p><p>4 0.46678227 <a title="33-lsi-4" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>Author: Shyam Visweswaran, Gregory F. Cooper</p><p>Abstract: This paper introduces a Bayesian algorithm for constructing predictive models from data that are optimized to predict a target variable well for a particular instance. This algorithm learns Markov blanket models, carries out Bayesian model averaging over a set of models to predict a target variable of the instance at hand, and employs an instance-speciﬁc heuristic to locate a set of suitable models to average over. We call this method the instance-speciﬁc Markov blanket (ISMB) algorithm. The ISMB algorithm was evaluated on 21 UCI data sets using ﬁve different performance measures and its performance was compared to that of several commonly used predictive algorithms, including nave Bayes, C4.5 decision tree, logistic regression, neural networks, k-Nearest Neighbor, Lazy Bayesian Rules, and AdaBoost. Over all the data sets, the ISMB algorithm performed better on average on all performance measures against all the comparison algorithms. Keywords: instance-speciﬁc, Bayesian network, Markov blanket, Bayesian model averaging</p><p>5 0.42799249 <a title="33-lsi-5" href="./jmlr-2010-An_Exponential_Model_for_Infinite_Rankings.html">10 jmlr-2010-An Exponential Model for Infinite Rankings</a></p>
<p>Author: Marina Meilă, Le Bao</p><p>Abstract: This paper presents a statistical model for expressing preferences through rankings, when the number of alternatives (items to rank) is large. A human ranker will then typically rank only the most preferred items, and may not even examine the whole set of items, or know how many they are. Similarly, a user presented with the ranked output of a search engine, will only consider the highest ranked items. We model such situations by introducing a stagewise ranking model that operates with ﬁnite ordered lists called top-t orderings over an inﬁnite space of items. We give algorithms to estimate this model from data, and demonstrate that it has sufﬁcient statistics, being thus an exponential family model with continuous and discrete parameters. We describe its conjugate prior and other statistical properties. Then, we extend the estimation problem to multimodal data by introducing an Exponential-Blurring-Mean-Shift nonparametric clustering algorithm. The experiments highlight the properties of our model and demonstrate that inﬁnite models over permutations can be simple, elegant and practical. Keywords: permutations, partial orderings, Mallows model, distance based ranking model, exponential family, non-parametric clustering, branch-and-bound</p><p>6 0.39596719 <a title="33-lsi-6" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>7 0.33846954 <a title="33-lsi-7" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>8 0.33682555 <a title="33-lsi-8" href="./jmlr-2010-Posterior_Regularization_for_Structured_Latent_Variable_Models.html">91 jmlr-2010-Posterior Regularization for Structured Latent Variable Models</a></p>
<p>9 0.33616021 <a title="33-lsi-9" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>10 0.33291245 <a title="33-lsi-10" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>11 0.30307925 <a title="33-lsi-11" href="./jmlr-2010-A_Streaming_Parallel_Decision_Tree_Algorithm.html">7 jmlr-2010-A Streaming Parallel Decision Tree Algorithm</a></p>
<p>12 0.29533952 <a title="33-lsi-12" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>13 0.28205773 <a title="33-lsi-13" href="./jmlr-2010-Classification_Using_Geometric_Level_Sets.html">22 jmlr-2010-Classification Using Geometric Level Sets</a></p>
<p>14 0.28028843 <a title="33-lsi-14" href="./jmlr-2010-Optimal_Search_on_Clustered_Structural_Constraint_for_Learning_Bayesian_Network_Structure.html">88 jmlr-2010-Optimal Search on Clustered Structural Constraint for Learning Bayesian Network Structure</a></p>
<p>15 0.2663261 <a title="33-lsi-15" href="./jmlr-2010-Approximate_Tree_Kernels.html">15 jmlr-2010-Approximate Tree Kernels</a></p>
<p>16 0.26497212 <a title="33-lsi-16" href="./jmlr-2010-Classification_with_Incomplete_Data_Using_Dirichlet_Process_Priors.html">23 jmlr-2010-Classification with Incomplete Data Using Dirichlet Process Priors</a></p>
<p>17 0.24947716 <a title="33-lsi-17" href="./jmlr-2010-An_Investigation_of_Missing_Data_Methods_for_Classification_Trees_Applied_to_Binary_Response_Data.html">11 jmlr-2010-An Investigation of Missing Data Methods for Classification Trees Applied to Binary Response Data</a></p>
<p>18 0.2490923 <a title="33-lsi-18" href="./jmlr-2010-Incremental_Sigmoid_Belief_Networks_for_Grammar_Learning.html">52 jmlr-2010-Incremental Sigmoid Belief Networks for Grammar Learning</a></p>
<p>19 0.24841951 <a title="33-lsi-19" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>20 0.24428174 <a title="33-lsi-20" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.014), (8, 0.447), (21, 0.011), (32, 0.073), (33, 0.011), (36, 0.054), (37, 0.03), (75, 0.149), (81, 0.012), (85, 0.075), (96, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8547619 <a title="33-lda-1" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>Author: Dapo Omidiran, Martin J. Wainwright</p><p>Abstract: We consider the problem of high-dimensional variable selection: given n noisy observations of a k-sparse vector β∗ ∈ R p , estimate the subset of non-zero entries of β∗ . A signiﬁcant body of work has studied behavior of ℓ1 -relaxations when applied to random measurement matrices that are dense (e.g., Gaussian, Bernoulli). In this paper, we analyze sparsiﬁed measurement ensembles, and consider the trade-off between measurement sparsity, as measured by the fraction γ of nonzero entries, and the statistical efﬁciency, as measured by the minimal number of observations n required for correct variable selection with probability converging to one. Our main result is to prove that it is possible to let the fraction on non-zero entries γ → 0 at some rate, yielding measurement matrices with a vanishing fraction of non-zeros per row, while retaining the same statistical efﬁciency as dense ensembles. A variety of simulation results conﬁrm the sharpness of our theoretical predictions. Keywords: variable selection, sparse random projections, high-dimensional statistics, Lasso, consistency, ℓ1 -regularization</p><p>same-paper 2 0.75498879 <a title="33-lda-2" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>Author: Franz Pernkopf, Jeff A. Bilmes</p><p>Abstract: We introduce a simple order-based greedy heuristic for learning discriminative structure within generative Bayesian network classiﬁers. We propose two methods for establishing an order of N features. They are based on the conditional mutual information and classiﬁcation rate (i.e., risk), respectively. Given an ordering, we can ﬁnd a discriminative structure with O N k+1 score evaluations (where constant k is the tree-width of the sub-graph over the attributes). We present results on 25 data sets from the UCI repository, for phonetic classiﬁcation using the TIMIT database, for a visual surface inspection task, and for two handwritten digit recognition tasks. We provide classiﬁcation performance for both discriminative and generative parameter learning on both discriminatively and generatively structured networks. The discriminative structure found by our new procedures signiﬁcantly outperforms generatively produced structures, and achieves a classiﬁcation accuracy on par with the best discriminative (greedy) Bayesian network learning approach, but does so with a factor of ∼10-40 speedup. We also show that the advantages of generative discriminatively structured Bayesian network classiﬁers still hold in the case of missing features, a case where generative classiﬁers have an advantage over discriminative classiﬁers. Keywords: Bayesian networks, classiﬁcation, discriminative learning, structure learning, graphical model, missing feature</p><p>3 0.58585113 <a title="33-lda-3" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>Author: Jean-Yves Audibert, Sébastien Bubeck</p><p>Abstract: This work deals with four classical prediction settings, namely full information, bandit, label efﬁcient and bandit label efﬁcient as well as four different notions of regret: pseudo-regret, expected regret, high probability regret and tracking the best expert regret. We introduce a new forecaster, INF (Implicitly Normalized Forecaster) based on an arbitrary function ψ for which we propose a uniﬁed γ analysis of its pseudo-regret in the four games we consider. In particular, for ψ(x) = exp(ηx) + K , INF reduces to the classical exponentially weighted average forecaster and our analysis of the pseudo-regret recovers known results while for the expected regret we slightly tighten the bounds. γ η q On the other hand with ψ(x) = −x + K , which deﬁnes a new forecaster, we are able to remove the extraneous logarithmic factor in the pseudo-regret bounds for bandits games, and thus ﬁll in a long open gap in the characterization of the minimax rate for the pseudo-regret in the bandit game. We also provide high probability bounds depending on the cumulative reward of the optimal action. Finally, we consider the stochastic bandit game, and prove that an appropriate modiﬁcation of the upper conﬁdence bound policy UCB1 (Auer et al., 2002a) achieves the distribution-free optimal rate while still having a distribution-dependent rate logarithmic in the number of plays. Keywords: Bandits (adversarial and stochastic), regret bound, minimax rate, label efﬁcient, upper conﬁdence bound (UCB) policy, online learning, prediction with limited feedback.</p><p>4 0.47906226 <a title="33-lda-4" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>Author: Ming Yuan</p><p>Abstract: This paper considers the problem of estimating a high dimensional inverse covariance matrix that can be well approximated by “sparse” matrices. Taking advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix, we propose an estimating procedure that can effectively exploit such “sparsity”. The proposed method can be computed using linear programming and therefore has the potential to be used in very high dimensional problems. Oracle inequalities are established for the estimation error in terms of several operator norms, showing that the method is adaptive to different types of sparsity of the problem. Keywords: covariance selection, Dantzig selector, Gaussian graphical model, inverse covariance matrix, Lasso, linear programming, oracle inequality, sparsity</p><p>5 0.46464002 <a title="33-lda-5" href="./jmlr-2010-Rate_Minimaxity_of_the_Lasso_and_Dantzig_Selector_for_thelqLoss_inlrBalls.html">96 jmlr-2010-Rate Minimaxity of the Lasso and Dantzig Selector for thelqLoss inlrBalls</a></p>
<p>Author: Fei Ye, Cun-Hui Zhang</p><p>Abstract: We consider the estimation of regression coefﬁcients in a high-dimensional linear model. For regression coefﬁcients in ℓr balls, we provide lower bounds for the minimax ℓq risk and minimax quantiles of the ℓq loss for all design matrices. Under an ℓ0 sparsity condition on a target coefﬁcient vector, we sharpen and unify existing oracle inequalities for the Lasso and Dantzig selector. We derive oracle inequalities for target coefﬁcient vectors with many small elements and smaller threshold levels than the universal threshold. These oracle inequalities provide sufﬁcient conditions on the design matrix for the rate minimaxity of the Lasso and Dantzig selector for the ℓq risk and loss in ℓr balls, 0 ≤ r ≤ 1 ≤ q ≤ ∞. By allowing q = ∞, our risk bounds imply the variable selection consistency of threshold Lasso and Dantzig selectors. Keywords: variable selection, estimation, oracle inequality, minimax, linear regression, penalized least squares, linear programming</p><p>6 0.4601934 <a title="33-lda-6" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>7 0.45696145 <a title="33-lda-7" href="./jmlr-2010-Message-passing_for_Graph-structured_Linear_Programs%3A_Proximal_Methods_and_Rounding_Schemes.html">76 jmlr-2010-Message-passing for Graph-structured Linear Programs: Proximal Methods and Rounding Schemes</a></p>
<p>8 0.45478711 <a title="33-lda-8" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>9 0.44003829 <a title="33-lda-9" href="./jmlr-2010-Rademacher_Complexities_and_Bounding_the_Excess_Risk_in_Active_Learning.html">95 jmlr-2010-Rademacher Complexities and Bounding the Excess Risk in Active Learning</a></p>
<p>10 0.43952686 <a title="33-lda-10" href="./jmlr-2010-A_Generalized_Path_Integral_Control_Approach_to_Reinforcement_Learning.html">4 jmlr-2010-A Generalized Path Integral Control Approach to Reinforcement Learning</a></p>
<p>11 0.43948606 <a title="33-lda-11" href="./jmlr-2010-Asymptotic_Equivalence_of_Bayes_Cross_Validation_and_Widely_Applicable_Information_Criterion_in_Singular_Learning_Theory.html">16 jmlr-2010-Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory</a></p>
<p>12 0.43933418 <a title="33-lda-12" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>13 0.43279797 <a title="33-lda-13" href="./jmlr-2010-Stacked_Denoising_Autoencoders%3A_Learning_Useful_Representations_in_a_Deep_Network_with_a_Local_Denoising_Criterion.html">107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</a></p>
<p>14 0.42916724 <a title="33-lda-14" href="./jmlr-2010-Image_Denoising_with_Kernels_Based_on_Natural_Image_Relations.html">50 jmlr-2010-Image Denoising with Kernels Based on Natural Image Relations</a></p>
<p>15 0.42778441 <a title="33-lda-15" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<p>16 0.42735288 <a title="33-lda-16" href="./jmlr-2010-Chromatic_PAC-Bayes_Bounds_for_Non-IID_Data%3A_Applications_to_Ranking_and_Stationary_%CE%B2-Mixing_Processes.html">20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</a></p>
<p>17 0.42630646 <a title="33-lda-17" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>18 0.42435944 <a title="33-lda-18" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>19 0.42397282 <a title="33-lda-19" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>20 0.4223485 <a title="33-lda-20" href="./jmlr-2010-Stability_Bounds_for_Stationary_%CF%86-mixing_and_%CE%B2-mixing_Processes.html">106 jmlr-2010-Stability Bounds for Stationary φ-mixing and β-mixing Processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
