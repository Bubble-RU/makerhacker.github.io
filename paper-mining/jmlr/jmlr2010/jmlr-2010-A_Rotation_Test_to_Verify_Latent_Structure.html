<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 jmlr-2010-A Rotation Test to Verify Latent Structure</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-6" href="#">jmlr2010-6</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6 jmlr-2010-A Rotation Test to Verify Latent Structure</h1>
<br/><p>Source: <a title="jmlr-2010-6-pdf" href="http://jmlr.org/papers/volume11/perry10a/perry10a.pdf">pdf</a></p><p>Author: Patrick O. Perry, Art B. Owen</p><p>Abstract: In multivariate regression models we have the opportunity to look for hidden structure unrelated to the observed predictors. However, when one ﬁts a model involving such latent variables it is important to be able to tell if the structure is real, or just an artifact of correlation in the regression errors. We develop a new statistical test based on random rotations for verifying the existence of latent variables. The rotations are carefully constructed to rotate orthogonally to the column space of the regression model. We ﬁnd that only non-Gaussian latent variables are detectable, a ﬁnding that parallels a well known phenomenon in independent components analysis. We base our test on a measure of non-Gaussianity in the histogram of the principal eigenvector components instead of on the eigenvalue. The method ﬁnds and veriﬁes some latent dichotomies in the microarray data from the AGEMAP consortium. Keywords: independent components analysis, Kronecker covariance, latent variables, projection pursuit, transposable data</p><p>Reference: <a title="jmlr-2010-6-reference" href="../jmlr2010_reference/jmlr-2010-A_Rotation_Test_to_Verify_Latent_Structure_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rot', 0.458), ('mic', 0.341), ('tep', 0.341), ('qt', 0.292), ('agemap', 0.263), ('tissu', 0.175), ('cerebell', 0.17), ('erry', 0.146), ('resid', 0.141), ('cerebr', 0.132), ('ohot', 0.124), ('lat', 0.116), ('histogram', 0.115), ('ow', 0.101), ('xb', 0.1), ('ot', 0.099), ('dichotom', 0.097), ('microarray', 0.092), ('truct', 0.092), ('mous', 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="6-tfidf-1" href="./jmlr-2010-A_Rotation_Test_to_Verify_Latent_Structure.html">6 jmlr-2010-A Rotation Test to Verify Latent Structure</a></p>
<p>Author: Patrick O. Perry, Art B. Owen</p><p>Abstract: In multivariate regression models we have the opportunity to look for hidden structure unrelated to the observed predictors. However, when one ﬁts a model involving such latent variables it is important to be able to tell if the structure is real, or just an artifact of correlation in the regression errors. We develop a new statistical test based on random rotations for verifying the existence of latent variables. The rotations are carefully constructed to rotate orthogonally to the column space of the regression model. We ﬁnd that only non-Gaussian latent variables are detectable, a ﬁnding that parallels a well known phenomenon in independent components analysis. We base our test on a measure of non-Gaussianity in the histogram of the principal eigenvector components instead of on the eigenvalue. The method ﬁnds and veriﬁes some latent dichotomies in the microarray data from the AGEMAP consortium. Keywords: independent components analysis, Kronecker covariance, latent variables, projection pursuit, transposable data</p><p>2 0.072349168 <a title="6-tfidf-2" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>Author: Markus Ojala, Gemma C. Garriga</p><p>Abstract: We explore the framework of permutation-based p-values for assessing the performance of classiﬁers. In this paper we study two simple permutation tests. The ﬁrst test assess whether the classiﬁer has found a real class structure in the data; the corresponding null distribution is estimated by permuting the labels in the data. This test has been used extensively in classiﬁcation problems in computational biology. The second test studies whether the classiﬁer is exploiting the dependency between the features in classiﬁcation; the corresponding null distribution is estimated by permuting the features within classes, inspired by restricted randomization techniques traditionally used in statistics. This new test can serve to identify descriptive features which can be valuable information in improving the classiﬁer performance. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classiﬁer performance via permutation tests is effective. In particular, the restricted permutation test clearly reveals whether the classiﬁer exploits the interdependency between the features in the data. Keywords: classiﬁcation, labeled data, permutation tests, restricted randomization, signiﬁcance testing</p><p>3 0.068413801 <a title="6-tfidf-3" href="./jmlr-2010-A_Streaming_Parallel_Decision_Tree_Algorithm.html">7 jmlr-2010-A Streaming Parallel Decision Tree Algorithm</a></p>
<p>Author: Yael Ben-Haim, Elad Tom-Tov</p><p>Abstract: We propose a new algorithm for building decision tree classiﬁers. The algorithm is executed in a distributed environment and is especially designed for classifying large data sets and streaming data. It is empirically shown to be as accurate as a standard decision tree classiﬁer, while being scalable for processing of streaming data on multiple processors. These ﬁndings are supported by a rigorous analysis of the algorithm’s accuracy. The essence of the algorithm is to quickly construct histograms at the processors, which compress the data to a ﬁxed amount of memory. A master processor uses this information to ﬁnd near-optimal split points to terminal tree nodes. Our analysis shows that guarantees on the local accuracy of split points imply guarantees on the overall tree accuracy. Keywords: decision tree classiﬁers, distributed computing, streaming data, scalability</p><p>4 0.053550716 <a title="6-tfidf-4" href="./jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</a></p>
<p>Author: Alexander Ilin, Tapani Raiko</p><p>Abstract: Principal component analysis (PCA) is a classical data analysis technique that Ä?Ĺš nds linear transformations of data that retain the maximal amount of variance. We study a case where some of the data values are missing, and show that this problem has many features which are usually associated with nonlinear models, such as overÄ?Ĺš tting and bad locally optimal solutions. A probabilistic formulation of PCA provides a good foundation for handling missing values, and we provide formulas for doing that. In case of high dimensional and very sparse data, overÄ?Ĺš tting becomes a severe problem and traditional algorithms for PCA are very slow. We introduce a novel fast algorithm and extend it to variational Bayesian learning. Different versions of PCA are compared in artiÄ?Ĺš cial experiments, demonstrating the effects of regularization and modeling of posterior variance. The scalability of the proposed algorithm is demonstrated by applying it to the NetÄ?Ĺš&sbquo;ix problem. Keywords: principal component analysis, missing values, overÄ?Ĺš tting, regularization, variational Bayes</p><p>5 0.052637871 <a title="6-tfidf-5" href="./jmlr-2010-Posterior_Regularization_for_Structured_Latent_Variable_Models.html">91 jmlr-2010-Posterior Regularization for Structured Latent Variable Models</a></p>
<p>Author: Kuzman Ganchev, João Graça, Jennifer Gillenwater, Ben Taskar</p><p>Abstract: We present posterior regularization, a probabilistic framework for structured, weakly supervised learning. Our framework efﬁciently incorporates indirect supervision via constraints on posterior distributions of probabilistic models with latent variables. Posterior regularization separates model complexity from the complexity of structural constraints it is desired to satisfy. By directly imposing decomposable regularization on the posterior moments of latent variables during learning, we retain the computational efﬁciency of the unconstrained model while ensuring desired constraints hold in expectation. We present an efﬁcient algorithm for learning with posterior regularization and illustrate its versatility on a diverse set of structural constraints such as bijectivity, symmetry and group sparsity in several large scale experiments, including multi-view learning, cross-lingual dependency grammar induction, unsupervised part-of-speech induction, and bitext word alignment.1 Keywords: posterior regularization framework, unsupervised learning, latent variables models, prior knowledge, natural language processing</p><p>6 0.050492436 <a title="6-tfidf-6" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>7 0.044924941 <a title="6-tfidf-7" href="./jmlr-2010-Estimation_of_a_Structural_Vector_Autoregression_Model_Using_Non-Gaussianity.html">36 jmlr-2010-Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity</a></p>
<p>8 0.040985145 <a title="6-tfidf-8" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>9 0.040071607 <a title="6-tfidf-9" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<p>10 0.038712036 <a title="6-tfidf-10" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>11 0.038629476 <a title="6-tfidf-11" href="./jmlr-2010-On_Spectral_Learning.html">84 jmlr-2010-On Spectral Learning</a></p>
<p>12 0.038201362 <a title="6-tfidf-12" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>13 0.038020428 <a title="6-tfidf-13" href="./jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">48 jmlr-2010-How to Explain Individual Classification Decisions</a></p>
<p>14 0.037340511 <a title="6-tfidf-14" href="./jmlr-2010-Optimal_Search_on_Clustered_Structural_Constraint_for_Learning_Bayesian_Network_Structure.html">88 jmlr-2010-Optimal Search on Clustered Structural Constraint for Learning Bayesian Network Structure</a></p>
<p>15 0.037050974 <a title="6-tfidf-15" href="./jmlr-2010-On_the_Foundations_of_Noise-free_Selective_Classification.html">85 jmlr-2010-On the Foundations of Noise-free Selective Classification</a></p>
<p>16 0.035987068 <a title="6-tfidf-16" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>17 0.034993608 <a title="6-tfidf-17" href="./jmlr-2010-On_Learning_with_Integral_Operators.html">82 jmlr-2010-On Learning with Integral Operators</a></p>
<p>18 0.034044169 <a title="6-tfidf-18" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>19 0.033410966 <a title="6-tfidf-19" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>20 0.033324387 <a title="6-tfidf-20" href="./jmlr-2010-Bayesian_Learning_in_Sparse_Graphical_Factor_Models_via_Variational_Mean-Field_Annealing.html">17 jmlr-2010-Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.157), (1, 0.021), (2, 0.03), (3, 0.043), (4, 0.033), (5, -0.005), (6, 0.046), (7, 0.058), (8, -0.004), (9, 0.075), (10, 0.15), (11, 0.031), (12, -0.028), (13, -0.04), (14, 0.09), (15, 0.013), (16, 0.011), (17, -0.115), (18, -0.143), (19, -0.024), (20, 0.062), (21, -0.063), (22, 0.075), (23, 0.239), (24, 0.038), (25, 0.034), (26, 0.074), (27, -0.081), (28, 0.018), (29, -0.01), (30, 0.041), (31, 0.121), (32, 0.134), (33, 0.029), (34, -0.143), (35, 0.092), (36, 0.059), (37, -0.263), (38, -0.003), (39, 0.238), (40, 0.134), (41, -0.068), (42, 0.001), (43, 0.088), (44, -0.07), (45, 0.096), (46, 0.04), (47, -0.158), (48, 0.036), (49, -0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92096287 <a title="6-lsi-1" href="./jmlr-2010-A_Rotation_Test_to_Verify_Latent_Structure.html">6 jmlr-2010-A Rotation Test to Verify Latent Structure</a></p>
<p>Author: Patrick O. Perry, Art B. Owen</p><p>Abstract: In multivariate regression models we have the opportunity to look for hidden structure unrelated to the observed predictors. However, when one ﬁts a model involving such latent variables it is important to be able to tell if the structure is real, or just an artifact of correlation in the regression errors. We develop a new statistical test based on random rotations for verifying the existence of latent variables. The rotations are carefully constructed to rotate orthogonally to the column space of the regression model. We ﬁnd that only non-Gaussian latent variables are detectable, a ﬁnding that parallels a well known phenomenon in independent components analysis. We base our test on a measure of non-Gaussianity in the histogram of the principal eigenvector components instead of on the eigenvalue. The method ﬁnds and veriﬁes some latent dichotomies in the microarray data from the AGEMAP consortium. Keywords: independent components analysis, Kronecker covariance, latent variables, projection pursuit, transposable data</p><p>2 0.45040575 <a title="6-lsi-2" href="./jmlr-2010-A_Streaming_Parallel_Decision_Tree_Algorithm.html">7 jmlr-2010-A Streaming Parallel Decision Tree Algorithm</a></p>
<p>Author: Yael Ben-Haim, Elad Tom-Tov</p><p>Abstract: We propose a new algorithm for building decision tree classiﬁers. The algorithm is executed in a distributed environment and is especially designed for classifying large data sets and streaming data. It is empirically shown to be as accurate as a standard decision tree classiﬁer, while being scalable for processing of streaming data on multiple processors. These ﬁndings are supported by a rigorous analysis of the algorithm’s accuracy. The essence of the algorithm is to quickly construct histograms at the processors, which compress the data to a ﬁxed amount of memory. A master processor uses this information to ﬁnd near-optimal split points to terminal tree nodes. Our analysis shows that guarantees on the local accuracy of split points imply guarantees on the overall tree accuracy. Keywords: decision tree classiﬁers, distributed computing, streaming data, scalability</p><p>3 0.38703066 <a title="6-lsi-3" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>Author: Markus Ojala, Gemma C. Garriga</p><p>Abstract: We explore the framework of permutation-based p-values for assessing the performance of classiﬁers. In this paper we study two simple permutation tests. The ﬁrst test assess whether the classiﬁer has found a real class structure in the data; the corresponding null distribution is estimated by permuting the labels in the data. This test has been used extensively in classiﬁcation problems in computational biology. The second test studies whether the classiﬁer is exploiting the dependency between the features in classiﬁcation; the corresponding null distribution is estimated by permuting the features within classes, inspired by restricted randomization techniques traditionally used in statistics. This new test can serve to identify descriptive features which can be valuable information in improving the classiﬁer performance. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classiﬁer performance via permutation tests is effective. In particular, the restricted permutation test clearly reveals whether the classiﬁer exploits the interdependency between the features in the data. Keywords: classiﬁcation, labeled data, permutation tests, restricted randomization, signiﬁcance testing</p><p>4 0.34473732 <a title="6-lsi-4" href="./jmlr-2010-Estimation_of_a_Structural_Vector_Autoregression_Model_Using_Non-Gaussianity.html">36 jmlr-2010-Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity</a></p>
<p>Author: Aapo Hyvärinen, Kun Zhang, Shohei Shimizu, Patrik O. Hoyer</p><p>Abstract: Analysis of causal effects between continuous-valued variables typically uses either autoregressive models or structural equation models with instantaneous effects. Estimation of Gaussian, linear structural equation models poses serious identiﬁability problems, which is why it was recently proposed to use non-Gaussian models. Here, we show how to combine the non-Gaussian instantaneous model with autoregressive models. This is effectively what is called a structural vector autoregression (SVAR) model, and thus our work contributes to the long-standing problem of how to estimate SVAR’s. We show that such a non-Gaussian model is identiﬁable without prior knowledge of network structure. We propose computationally efﬁcient methods for estimating the model, as well as methods to assess the signiﬁcance of the causal inﬂuences. The model is successfully applied on ﬁnancial and brain imaging data. Keywords: structural vector autoregression, structural equation models, independent component analysis, non-Gaussianity, causality</p><p>5 0.32585469 <a title="6-lsi-5" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<p>Author: Michel Journée, Yurii Nesterov, Peter Richtárik, Rodolphe Sepulchre</p><p>Abstract: In this paper we develop a new approach to sparse principal component analysis (sparse PCA). We propose two single-unit and two block optimization formulations of the sparse PCA problem, aimed at extracting a single sparse dominant principal component of a data matrix, or more components at once, respectively. While the initial formulations involve nonconvex functions, and are therefore computationally intractable, we rewrite them into the form of an optimization program involving maximization of a convex function on a compact set. The dimension of the search space is decreased enormously if the data matrix has many more columns (variables) than rows. We then propose and analyze a simple gradient method suited for the task. It appears that our algorithm has best convergence properties in the case when either the objective function or the feasible set are strongly convex, which is the case with our single-unit formulations and can be enforced in the block case. Finally, we demonstrate numerically on a set of random and gene expression test problems that our approach outperforms existing algorithms both in quality of the obtained solution and in computational speed. Keywords: sparse PCA, power method, gradient ascent, strongly convex sets, block algorithms</p><p>6 0.31275216 <a title="6-lsi-6" href="./jmlr-2010-Iterative_Scaling_and_Coordinate_Descent_Methods_for_Maximum_Entropy_Models.html">57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</a></p>
<p>7 0.3065643 <a title="6-lsi-7" href="./jmlr-2010-On_the_Foundations_of_Noise-free_Selective_Classification.html">85 jmlr-2010-On the Foundations of Noise-free Selective Classification</a></p>
<p>8 0.30440331 <a title="6-lsi-8" href="./jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</a></p>
<p>9 0.26814517 <a title="6-lsi-9" href="./jmlr-2010-Optimal_Search_on_Clustered_Structural_Constraint_for_Learning_Bayesian_Network_Structure.html">88 jmlr-2010-Optimal Search on Clustered Structural Constraint for Learning Bayesian Network Structure</a></p>
<p>10 0.26617831 <a title="6-lsi-10" href="./jmlr-2010-Image_Denoising_with_Kernels_Based_on_Natural_Image_Relations.html">50 jmlr-2010-Image Denoising with Kernels Based on Natural Image Relations</a></p>
<p>11 0.26515445 <a title="6-lsi-11" href="./jmlr-2010-Matched_Gene_Selection_and_Committee_Classifier_for_Molecular_Classification_of_Heterogeneous_Diseases.html">71 jmlr-2010-Matched Gene Selection and Committee Classifier for Molecular Classification of Heterogeneous Diseases</a></p>
<p>12 0.25566697 <a title="6-lsi-12" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>13 0.24956095 <a title="6-lsi-13" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>14 0.2409796 <a title="6-lsi-14" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>15 0.2397451 <a title="6-lsi-15" href="./jmlr-2010-Restricted_Eigenvalue_Properties_for_Correlated_Gaussian_Designs.html">99 jmlr-2010-Restricted Eigenvalue Properties for Correlated Gaussian Designs</a></p>
<p>16 0.23661588 <a title="6-lsi-16" href="./jmlr-2010-Error-Correcting_Output_Codes_Library.html">35 jmlr-2010-Error-Correcting Output Codes Library</a></p>
<p>17 0.23462498 <a title="6-lsi-17" href="./jmlr-2010-Consistent_Nonparametric_Tests_of_Independence.html">27 jmlr-2010-Consistent Nonparametric Tests of Independence</a></p>
<p>18 0.23178601 <a title="6-lsi-18" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>19 0.2270772 <a title="6-lsi-19" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>20 0.20999368 <a title="6-lsi-20" href="./jmlr-2010-Introduction_to_Causal_Inference.html">56 jmlr-2010-Introduction to Causal Inference</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.439), (13, 0.06), (22, 0.038), (62, 0.029), (65, 0.045), (71, 0.066), (74, 0.012), (82, 0.022), (85, 0.15)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95959979 <a title="6-lda-1" href="./jmlr-2010-A_Rotation_Test_to_Verify_Latent_Structure.html">6 jmlr-2010-A Rotation Test to Verify Latent Structure</a></p>
<p>Author: Patrick O. Perry, Art B. Owen</p><p>Abstract: In multivariate regression models we have the opportunity to look for hidden structure unrelated to the observed predictors. However, when one ﬁts a model involving such latent variables it is important to be able to tell if the structure is real, or just an artifact of correlation in the regression errors. We develop a new statistical test based on random rotations for verifying the existence of latent variables. The rotations are carefully constructed to rotate orthogonally to the column space of the regression model. We ﬁnd that only non-Gaussian latent variables are detectable, a ﬁnding that parallels a well known phenomenon in independent components analysis. We base our test on a measure of non-Gaussianity in the histogram of the principal eigenvector components instead of on the eigenvalue. The method ﬁnds and veriﬁes some latent dichotomies in the microarray data from the AGEMAP consortium. Keywords: independent components analysis, Kronecker covariance, latent variables, projection pursuit, transposable data</p><p>2 0.94408202 <a title="6-lda-2" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>Author: Ming Yuan</p><p>Abstract: This paper considers the problem of estimating a high dimensional inverse covariance matrix that can be well approximated by “sparse” matrices. Taking advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix, we propose an estimating procedure that can effectively exploit such “sparsity”. The proposed method can be computed using linear programming and therefore has the potential to be used in very high dimensional problems. Oracle inequalities are established for the estimation error in terms of several operator norms, showing that the method is adaptive to different types of sparsity of the problem. Keywords: covariance selection, Dantzig selector, Gaussian graphical model, inverse covariance matrix, Lasso, linear programming, oracle inequality, sparsity</p><p>3 0.94395161 <a title="6-lda-3" href="./jmlr-2010-Expectation_Truncation_and_the_Benefits_of_Preselection_In_Training_Generative_Models.html">38 jmlr-2010-Expectation Truncation and the Benefits of Preselection In Training Generative Models</a></p>
<p>Author: Jörg Lücke, Julian Eggert</p><p>Abstract: We show how a preselection of hidden variables can be used to efﬁciently train generative models with binary hidden variables. The approach is based on Expectation Maximization (EM) and uses an efﬁciently computable approximation to the sufﬁcient statistics of a given model. The computational cost to compute the sufﬁcient statistics is strongly reduced by selecting, for each data point, the relevant hidden causes. The approximation is applicable to a wide range of generative models and provides an interpretation of the beneﬁts of preselection in terms of a variational EM approximation. To empirically show that the method maximizes the data likelihood, it is applied to different types of generative models including: a version of non-negative matrix factorization (NMF), a model for non-linear component extraction (MCA), and a linear generative model similar to sparse coding. The derived algorithms are applied to both artiﬁcial and realistic data, and are compared to other models in the literature. We ﬁnd that the training scheme can reduce computational costs by orders of magnitude and allows for a reliable extraction of hidden causes. Keywords: maximum likelihood, deterministic approximations, variational EM, generative models, component extraction, multiple-cause models</p><p>4 0.94246686 <a title="6-lda-4" href="./jmlr-2010-Efficient_Heuristics_for_Discriminative_Structure_Learning_of_Bayesian_Network_Classifiers.html">33 jmlr-2010-Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers</a></p>
<p>Author: Franz Pernkopf, Jeff A. Bilmes</p><p>Abstract: We introduce a simple order-based greedy heuristic for learning discriminative structure within generative Bayesian network classiﬁers. We propose two methods for establishing an order of N features. They are based on the conditional mutual information and classiﬁcation rate (i.e., risk), respectively. Given an ordering, we can ﬁnd a discriminative structure with O N k+1 score evaluations (where constant k is the tree-width of the sub-graph over the attributes). We present results on 25 data sets from the UCI repository, for phonetic classiﬁcation using the TIMIT database, for a visual surface inspection task, and for two handwritten digit recognition tasks. We provide classiﬁcation performance for both discriminative and generative parameter learning on both discriminatively and generatively structured networks. The discriminative structure found by our new procedures signiﬁcantly outperforms generatively produced structures, and achieves a classiﬁcation accuracy on par with the best discriminative (greedy) Bayesian network learning approach, but does so with a factor of ∼10-40 speedup. We also show that the advantages of generative discriminatively structured Bayesian network classiﬁers still hold in the case of missing features, a case where generative classiﬁers have an advantage over discriminative classiﬁers. Keywords: Bayesian networks, classiﬁcation, discriminative learning, structure learning, graphical model, missing feature</p><p>5 0.94153118 <a title="6-lda-5" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>Author: Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, Chih-Jen Lin</p><p>Abstract: Kernel techniques have long been used in SVM to handle linearly inseparable problems by transforming data to a high dimensional space, but training and testing large data sets is often time consuming. In contrast, we can efﬁciently train and test much larger data sets using linear SVM without kernels. In this work, we apply fast linear-SVM methods to the explicit form of polynomially mapped data and investigate implementation issues. The approach enjoys fast training and testing, but may sometimes achieve accuracy close to that of using highly nonlinear kernels. Empirical experiments show that the proposed method is useful for certain large-scale data sets. We successfully apply the proposed method to a natural language processing (NLP) application by improving the testing accuracy under some training/testing speed requirements. Keywords: decomposition methods, low-degree polynomial mapping, kernel functions, support vector machines, dependency parsing, natural language processing</p><p>6 0.92771983 <a title="6-lda-6" href="./jmlr-2010-Learning_From_Crowds.html">61 jmlr-2010-Learning From Crowds</a></p>
<p>7 0.91658175 <a title="6-lda-7" href="./jmlr-2010-Permutation_Tests_for_Studying_Classifier_Performance.html">90 jmlr-2010-Permutation Tests for Studying Classifier Performance</a></p>
<p>8 0.91650325 <a title="6-lda-8" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>9 0.9150421 <a title="6-lda-9" href="./jmlr-2010-Tree_Decomposition_for_Large-Scale_SVM_Problems.html">113 jmlr-2010-Tree Decomposition for Large-Scale SVM Problems</a></p>
<p>10 0.9124853 <a title="6-lda-10" href="./jmlr-2010-Fast_and_Scalable_Local_Kernel_Machines.html">40 jmlr-2010-Fast and Scalable Local Kernel Machines</a></p>
<p>11 0.90875459 <a title="6-lda-11" href="./jmlr-2010-How_to_Explain_Individual_Classification_Decisions.html">48 jmlr-2010-How to Explain Individual Classification Decisions</a></p>
<p>12 0.90733719 <a title="6-lda-12" href="./jmlr-2010-Estimation_of_a_Structural_Vector_Autoregression_Model_Using_Non-Gaussianity.html">36 jmlr-2010-Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity</a></p>
<p>13 0.9045738 <a title="6-lda-13" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<p>14 0.9042536 <a title="6-lda-14" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>15 0.90254229 <a title="6-lda-15" href="./jmlr-2010-A_Comparison_of_Optimization_Methods_and_Software_for_Large-scale_L1-regularized_Linear_Classification.html">1 jmlr-2010-A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</a></p>
<p>16 0.90231848 <a title="6-lda-16" href="./jmlr-2010-Stacked_Denoising_Autoencoders%3A_Learning_Useful_Representations_in_a_Deep_Network_with_a_Local_Denoising_Criterion.html">107 jmlr-2010-Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</a></p>
<p>17 0.89575136 <a title="6-lda-17" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>18 0.89455366 <a title="6-lda-18" href="./jmlr-2010-Dimensionality_Estimation%2C_Manifold_Learning_and_Function_Approximation_using_Tensor_Voting.html">30 jmlr-2010-Dimensionality Estimation, Manifold Learning and Function Approximation using Tensor Voting</a></p>
<p>19 0.89155036 <a title="6-lda-19" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>20 0.8903662 <a title="6-lda-20" href="./jmlr-2010-Bayesian_Learning_in_Sparse_Graphical_Factor_Models_via_Variational_Mean-Field_Annealing.html">17 jmlr-2010-Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
