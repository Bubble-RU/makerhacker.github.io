<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>55 jmlr-2010-Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-55" href="#">jmlr2010-55</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>55 jmlr-2010-Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance</h1>
<br/><p>Source: <a title="jmlr-2010-55-pdf" href="http://jmlr.org/papers/volume11/vinh10a/vinh10a.pdf">pdf</a></p><p>Author: Nguyen Xuan Vinh, Julien Epps, James Bailey</p><p>Abstract: Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and inter-relationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing popular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then highlight to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0, 1] range better than other normalized variants. Keywords: clustering comparison, information theory, adjustment for chance, normalized information distance</p><p>Reference: <a title="jmlr-2010-55-reference" href="../jmlr2010_reference/jmlr-2010-Information_Theoretic_Measures_for_Clusterings_Comparison%3A_Variants%2C_Properties%2C_Normalization_and_Correction_for_Chance_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 3010, Australia  Editor: Marina Meil˘ a  Abstract Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. [sent-12, score-0.348]
</p><p>2 In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing popular measures in the literature, as well as some newly proposed ones. [sent-14, score-0.572]
</p><p>3 We then highlight to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. [sent-16, score-0.698]
</p><p>4 Keywords: clustering comparison, information theory, adjustment for chance, normalized information distance  1. [sent-18, score-0.443]
</p><p>5 Most often, such measures are used for external validation, that is, assessing the goodness of clustering solutions according to a “ground truth” clustering. [sent-20, score-0.348]
</p><p>6 Recent advances in cluster analysis have driven new algorithms, in which the clustering comparison measures are used actively in searching for good clustering solutions. [sent-21, score-0.639]
</p><p>7 A possible realization of this scheme is to measure the average pairwise distances between all the clusterings obtained under some sort of perturbations (Vinh and Epps, 2009), hence requiring a clustering comparison measure. [sent-34, score-0.611]
</p><p>8 In the clustering literature, such measures have been employed because of their strong mathematical foundation, and ability to detect non-linear similarities. [sent-37, score-0.348]
</p><p>9 For the particular purpose of clustering comparison, this class of measures has been popularized through the works of Strehl and Ghosh (2002) and Meil˘ (2005), and since then has been employed in various subsea quent research (Fern and Brodley, 2003; He et al. [sent-38, score-0.348]
</p><p>10 Although having received considerable interest, in our opinion, the application of information theoretic measures for comparing clustering has been somewhat scattered. [sent-42, score-0.448]
</p><p>11 Apart from the VI which possesses a fairly comprehensive characterization, less is known about the mutual information and various forms of the so-called normalized mutual information (Strehl and Ghosh, 2002). [sent-43, score-0.24]
</p><p>12 We ﬁrst review and make a coherent categorization of information theoretic similarity and distance measures for clustering comparison. [sent-45, score-0.573]
</p><p>13 We show that among the prospective measures, the normalized information distance (NID) and the normalized variation of information (NVI) satisfy both these desirable properties. [sent-47, score-0.266]
</p><p>14 We draw the attention of the clustering community towards the necessity of correcting information theoretic measures for chance in certain situations, derive analytical forms for the proposed adjusted-for-chance measures, and investigate their properties. [sent-49, score-0.593]
</p><p>15 Preliminary results regarding correcting information theoretic measures for chance have previously appeared in Vinh, Epps, and Bailey (2009). [sent-50, score-0.369]
</p><p>16 The informai=1 tion on the overlap between two clusterings U = {U1 ,U2 , . [sent-60, score-0.387]
</p><p>17 Pair counting based measures are built upon counting pairs of items on which two clusterings agree or disagree. [sent-73, score-0.552]
</p><p>18 For these reasons, the RI has been mostly used in it adjusted form, known as the adjusted Rand index (ARI, Hubert and Arabie 1985):  ARI(U, V) =  2(N00 N11 − N01 N10 ) . [sent-113, score-0.321]
</p><p>19 ai b j /N 2 i=1 j=1 N  ∑∑  The MI measures the information that U and V share: it tells us how much knowing one of these clusterings reduces our uncertainty about the other. [sent-128, score-0.643]
</p><p>20 Also, normalization has been shown to improve the sensitiveness of certain measures, such as the MI, with respect to the difference in cluster distribution in the two clusterings (Wu et al. [sent-145, score-0.498]
</p><p>21 2840  I NFORMATION T HEORETIC M EASURES FOR C LUSTERINGS C OMPARISON  • Constant baseline property: for a similarity measure, its expected value between pairs of independent clusterings, for example, clusterings sampled independently at random, should be a constant. [sent-149, score-0.473]
</p><p>22 The Rand index is an example of a similarity index which does not satisfy this rather intuitive property, the reason why it has been mainly used in its adjusted form. [sent-151, score-0.281]
</p><p>23 (2008)  [0, 1]∗ [0, 1]∗ [0, 1]∗  AMImin † [0, 1]∗ ∗ These measures are normalized in a stochastic sense, being equal to 1 if the (unadjusted) measures equal their value as expected by chance agreement. [sent-155, score-0.445]
</p><p>24 Table 2: Information theoretic-based similarity measures Similarity measures: the mutual information (MI), a non-negative quantity, can be employed as the most basic similarity measure. [sent-157, score-0.31]
</p><p>25 All the normalized variants are bounded in [0,1], equaling 1 when the two clusterings are identical, and 0 when they are independent, that is, sharing no information about each other. [sent-159, score-0.497]
</p><p>26 In the latter case, the contingency table takes the form of the so-called “independence table” where ni j = |Ui ||V j |/N for all i, j. [sent-160, score-0.303]
</p><p>27 The MI and some of its normalized versions have been used in the clustering literature as similarity measures between objects in general (see, for example, Yao, 2003 and references therein). [sent-161, score-0.481]
</p><p>28 Strehl and Ghosh (2002) on the other hand made use of the NMIsqrt normalized version, which has also been used in several follow-up works in the context of ensemble clustering (Fern and Brodley, 2003; He et al. [sent-164, score-0.337]
</p><p>29 However, it can be seen that D joint = 2Dsum ,1 and these two measures have been known in the clustering literature as the variation of information—VI (Meil˘ , 2005). [sent-172, score-0.411]
</p><p>30 These distance measures do not have a ﬁxed upper bound however, and we are therefore seeking some normalized variants. [sent-178, score-0.276]
</p><p>31 By dividing each distance measure by its corresponding upper bound we can deﬁne ﬁve normalized variants as detailed in Table 3, which are actually the unit-complements of the corresponding NMI variants, for example, d joint = 1 − NMI joint . [sent-179, score-0.24]
</p><p>32 We now state the following properties of the normalized distance measures: Theorem 3 The normalized variation of information, d joint , is a metric Theorem 4 The normalized information distance, dmax , is a metric 1. [sent-180, score-0.703]
</p><p>33 2842  I NFORMATION T HEORETIC M EASURES FOR C LUSTERINGS C OMPARISON  Theorem 5 The normalized distance measures dmin , dsum and dsqrt , are not metrics. [sent-182, score-0.54]
</p><p>34 From our discussion so far, we can now identify two promising candidates: d joint and dmax . [sent-188, score-0.245]
</p><p>35 Since the variation of information—D joint —is the unnormalized version of d joint , we shall name d joint the normalized variation of information (NVI). [sent-189, score-0.27]
</p><p>36 We note that Meil˘ (2007) proposed a 1 1 normalized variants for the VI, such as: V (U, V) = log N VI(U, V) or: VK ∗ (U, V) = 2 log K ∗ VI(U, V) √ when the number of clusters in both U and V is bounded by the same constant K ∗ < N. [sent-193, score-0.332]
</p><p>37 It is noted that since max{H(U), H(V)} is yet a tighter upper bound for MI(U, V) than H(U, V), dmax is generally more preferable to d joint since it can even better use the nominal range of [0, 1]. [sent-196, score-0.272]
</p><p>38 For validation purpose for example, if U is the ground-truth, and V is the clustering obtained by some algorithm, then the normalization also depends on V. [sent-198, score-0.268]
</p><p>39 Adjustment for Chance In this section we inspect the proposed information theoretic measures with respect to the third desirable property, that is, the constant baseline property. [sent-201, score-0.257]
</p><p>40 We shall ﬁrst point out that, just like the well-known Rand index, the baseline value of information theoretic measures does not take on a constant value, and thus adjustment for chance will be needed in certain situations. [sent-202, score-0.441]
</p><p>41 Let us consider the following two motivating examples: 1) Example 1 - Distance to a “true” clustering: given a ground-truth clustering U with Ktrue clusters, we need to assess the goodness of two clusterings V with C clusters, and V′ with C′ clusters. [sent-203, score-0.611]
</p><p>42 We set up an experiment as follows: consider a set of N data points, let the number of clusters K vary from 2 to Kmax and suppose that the true clustering has Ktrue = [Kmax /2] clusters. [sent-207, score-0.446]
</p><p>43 Now for each value of K, generate 10,000 random clusterings and calculate the average MI, NMImax , VI, RI and ARI between those clusterings to a ﬁxed, random 2. [sent-208, score-0.774]
</p><p>44 Thus even by selecting totally at random, a 7cluster solution would have a greater chance to outperform a 3-cluster solution, although there isn’t any difference in the clustering generation methodology. [sent-220, score-0.341]
</p><p>45 2  ← NMImax ← ARI  0 10  20 30 40 Number of clusters K  50  Figure 1: (a,b) Average distance between sets of random clusterings to a “true” clustering (c,d) Average pairwise distance in a set of random clusterings. [sent-241, score-0.977]
</p><p>46 2) Example 2 - Determining the number of clusters via consensus (ensemble) clustering: in an era where a huge number of clustering algorithms exist, the consensus clustering idea (Monti et al. [sent-243, score-0.848]
</p><p>47 Consensus clustering is not just another clustering algorithm: it rather provides a framework for unifying the knowledge obtained from other algorithms. [sent-246, score-0.448]
</p><p>48 Given a data set, consensus clustering employs one or several clustering algorithms to generate a set of clustering solutions on either the original data set or its perturbed versions. [sent-247, score-0.761]
</p><p>49 From these clustering solutions, consensus clustering aims to choose a robust and high quality representative clustering. [sent-248, score-0.537]
</p><p>50 Although the main objective of consensus clustering is to discover a high quality cluster structure, closer inspection of the set of clusterings obtained can often give valuable information about the appropriate number of clusters present. [sent-249, score-0.989]
</p><p>51 More speciﬁcally, we have empirically observed the following: in regard to the set of clusterings obtained, when the speciﬁed number of clusters coincides with the true number of clusters, this set has a tendency to be less diverse. [sent-250, score-0.609]
</p><p>52 To quantify this diversity we have recently developed a novel index (Vinh and Epps, 2009), namely the consensus index (CI), which is built upon a suitable clustering similarity measure. [sent-252, score-0.456]
</p><p>53 We deﬁne the consensus index of UK as: ∑i< j AM(Ui , U j ) B(B − 1)/2 where the agreement measure AM is a suitable clustering similarity index. [sent-257, score-0.411]
</p><p>54 We performed an experiment as follows: CI(UK ) =  2844  I NFORMATION T HEORETIC M EASURES FOR C LUSTERINGS C OMPARISON  given N data points, randomly assign each data point into one of the K clusters with equal probability and check to ensure that the ﬁnal clustering contains exactly K clusters. [sent-264, score-0.446]
</p><p>55 It can be observed that for a given data set, the average MI, NMI and RI (VI) values between random clusterings tend to increase (decrease) as the number of clusters increases, while the average value of the ARI is always close to zero. [sent-268, score-0.609]
</p><p>56 214), in which clusterings are generated randomly subject to having a ﬁxed number of clusters and points in each clusters. [sent-275, score-0.609]
</p><p>57 , 2009) that the expected mutual information between two clusterings U and V is: R  E{I(U, V)} =  C  ∑∑  min(ai ,b j )  ∑  i=1 j=1 ni j =max(ai +b j  ni j N. [sent-277, score-0.983]
</p><p>58 −N,0)  (2)  As suggested by Hubert and Arabie (1985), the general form of a similarity index corrected for chance is given by: Index − Expected Index Adjusted Index = , (3) Max Index − Expected Index which is upper-bounded by 1 and equals 0 when the index equals its expected value. [sent-288, score-0.26]
</p><p>59 Having calculated the expectation of the MI, we propose the adjusted form, which we call the adjusted mutual information (AMI), for the normalized mutual information according to (3). [sent-289, score-0.516]
</p><p>60 1 − E{NMImax (U, V)} max {H(U), H(V)} − E{I(U, V)}  Similarly, other adjusted similarity measures are listed in Table 2. [sent-291, score-0.315]
</p><p>61 Speciﬁcally, the AMI equals 1 when the two clusterings are identical, and 0 when the MI between the two clusterings equals its expected value. [sent-293, score-0.774]
</p><p>62 The adjusted forms for the distance measures, listed in Table 2, are again the unitcomplements of the corresponding adjusted similarity measures, for example, Admax = 1 − AMImax , and are also normalized in a stochastic sense. [sent-294, score-0.481]
</p><p>63 It is noted that at this stage, we have not been able to derive an analytical solution for the adjusted form for the normalized variation of information (d joint ) measure. [sent-296, score-0.281]
</p><p>64 01  0  20 40 Number of clusters K  60  Figure 2: (a,b) Average distance between sets of random clusterings to a “true” clustering (c,d) Average pairwise distance in a set of random clusterings. [sent-321, score-0.977]
</p><p>65 Nevertheless, the adjusted measures still exhibit the desired behavior. [sent-324, score-0.262]
</p><p>66 2 Properties of the Adjusted Measures While admitting a constant baseline, the proposed adjusted-for-chance measures are, unfortunately, not proper metrics: Theorem 6 The adjusted measures Admax , Adsum , Adsqrt and Admin are not metrics. [sent-326, score-0.417]
</p><p>67 The following result trivially follows: Corollary 1 Given R and C ﬁxed, limN→∞ E{I(U, V)} = 0, and thus the adjusted measures tend toward the normalized measures. [sent-332, score-0.342]
</p><p>68 For example, on a data set of 100 data items and two clusterings U and V, each having 10 clusters with sizes of [10, 10, 10, 10, 10, 10, 10, 10, 10, 10] and [2, 4, 6, 8, 10, 10, 12, 14, 16, 18] respectively, the expected MI and its upper bounds according to (2) and (4) are E{I(U, V)} = 0. [sent-334, score-0.65]
</p><p>69 However, if the data size increases ten-fold to 1000 items, keeping the same number of clusters and cluster distribution, the two upper bounds are 0. [sent-340, score-0.289]
</p><p>70 3 An Example Application As per our analysis, adjustment for chance for information theoretic measures is mostly needed when the number of data items is relatively small compared to the number of clusters. [sent-344, score-0.449]
</p><p>71 In this section we demonstrate the use of the consensus index to estimate the number of clusters in microarray data. [sent-347, score-0.402]
</p><p>72 (2003) In Vinh and Epps (2009) we have shown that the CI, coupled with sub-sampling as the perturbation method, gives useful information on the appropriate number of clusters in microarray data. [sent-354, score-0.268]
</p><p>73 Agreement by chance inﬂates the CI score of the unadjusted measures (RI, NMI, VI) in such cases, and can lead to incorrect estimation. [sent-366, score-0.301]
</p><p>74 The CI of the adjusted measures (ARI, AMI) correctly estimates the number of clusters in all synthetic data sets with high conﬁdence, whereas on real data sets it gives correct estimations on the Leukemia and Normal tissues data set. [sent-367, score-0.512]
</p><p>75 The CI suggests only 3 clusters on the Novartis while the assumed number of clusters is 4. [sent-368, score-0.444]
</p><p>76 2 Ktrue  true  0  0  5 10 15 Number of clusters K  20  0  0  5 10 15 Number of clusters K  (g) Lung cancer, N=197 data points  K 0  5 10 15 Number of clusters K  20  20  0. [sent-384, score-0.666]
</p><p>77 2 0  true  0  5 10 15 Number of clusters K  20  Figure 3: Consensus Index on microarray data sets. [sent-402, score-0.268]
</p><p>78 Related Work Meil˘ (2005) considered clustering comparison measures with respect to their alignment with the a lattice of partitions. [sent-405, score-0.375]
</p><p>79 Unfortunately, none of the normalized or adjusted variants of the MI is fully aligned with the lattice of partitions in the above sense. [sent-407, score-0.275]
</p><p>80 (2009) considered clustering comparison measures with respect to their sensitivity with class distribution. [sent-412, score-0.348]
</p><p>81 To demonstrate this property, they used the example in Table 5, with a ground-truth clustering U having class sizes of [30, 2, 6, 10, 2], and two clustering solutions: V having cluster sizes of [10, 10, 10, 10, 10]; and V′ having cluster sizes of [29, 2, 6, 11, 2]. [sent-415, score-0.582]
</p><p>82 It can be shown that among the normalized and adjusted variants of the MI considered in this paper, only the NMImin , Dmin , dmin and Admin are defective measures in the above sense. [sent-418, score-0.516]
</p><p>83 Conclusion This paper has presented an organized study of information theoretic measures for clustering comparison. [sent-420, score-0.448]
</p><p>84 We have shown that the normalized information distance (NID) and normalized variation of information (NVI) satisfy both the normalization and the metric properties. [sent-421, score-0.366]
</p><p>85 We highlighted the importance of correcting these measures for chance agreement, especially when the number of data points is relatively small compared with the number of clusters, for example, in the case of microarray data analysis. [sent-423, score-0.315]
</p><p>86 One of the theoretical advantages of the NID over the popular adjusted Rand index is that it can be used in the non-adjusted form (when N/K is large), thus enjoying the property of being a true metric in the space of clusterings. [sent-424, score-0.239]
</p><p>87 We therefore advocate the NID as a “general purpose” measure for clustering validation, comparison and algorithm design, for it possesses concurrently several useful and important properties. [sent-425, score-0.286]
</p><p>88 Note that for the sake of notational simplicity we have dropped the lower and upper values of ni j which runs from max((ai + b j − N), 0) to min(ai , b j ) in the sums. [sent-472, score-0.258]
</p><p>89 From (6) we have: E(ni j ) = ∑ ni j P (M|ni j , a, b) = ni j  ai b j ai b j ni j N ∑ ai b j P (M|ni j , a, b) = N , N ni j n N  n N  therefore: ∑ni j aiijb j P (M|ni j , a, b) = 1. [sent-473, score-1.452]
</p><p>90 Let Q (ni j ) = aiijb j P (M|ni j , a, b), then we can think of Q (ni j ) as a discrete probability distribution on ni j . [sent-474, score-0.282]
</p><p>91 Applying Jensen’s inequality (E( f (x)) ≤ f (E(x)) for f concave) to the concave logarithm function yields: ni j  ∑N  log(  ni j  ai b j N. [sent-475, score-0.674]
</p><p>92 ni j )P (M|ni j , a, b) = ∑ 2 log( )Q (ni j ) ≤ 2 log EQ ( ) . [sent-478, score-0.258]
</p><p>93 ni j ni j N N2 Q (ni j ) = ∑ P (M|ni j , a, b) = 2 2 ∑ n2j P (M|ni j , a, b) ai b j ai b j ni j i ni j ai b j ai b j  N2 a2 b2 i j  ai (ai − 1)b j (b j − 1) ai b j + N(N − 1) N  =  N(ai − 1)(b j − 1) N + . [sent-483, score-1.824]
</p><p>94 (N − 1)ai b j ai b j  Substituting this expression into (7) yields: ni j  ∑N  log(  ni j  N. [sent-484, score-0.648]
</p><p>95 ni j ai b j N(ai − 1)(b j − 1) N )P (M|ni j , a, b) ≤ 2 log + ai b j N (N − 1)ai b j ai b j  . [sent-485, score-0.654]
</p><p>96 k-anmi: A mutual information based clustering algorithm for categorical data. [sent-550, score-0.304]
</p><p>97 Information-theoretic distance measures for clustering validation: Generalization and normalization. [sent-597, score-0.42]
</p><p>98 A novel approach for automatic number of clusters detection in microarray data based on consensus clustering. [sent-667, score-0.357]
</p><p>99 Information theoretic measures for clusterings comparison: Is a correction for chance necessary? [sent-677, score-0.774]
</p><p>100 Graph-based consensus clustering for class discovery from gene expression data. [sent-700, score-0.313]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('clusterings', 0.387), ('ari', 0.287), ('ni', 0.258), ('nid', 0.24), ('clustering', 0.224), ('clusters', 0.222), ('dmax', 0.216), ('nmi', 0.168), ('mi', 0.148), ('adjusted', 0.138), ('bailey', 0.132), ('nmimax', 0.132), ('vinh', 0.132), ('ai', 0.132), ('meil', 0.129), ('measures', 0.124), ('dmin', 0.12), ('chance', 0.117), ('ami', 0.108), ('epps', 0.108), ('inh', 0.108), ('pps', 0.108), ('theoretic', 0.1), ('heoretic', 0.096), ('lusterings', 0.096), ('strehl', 0.092), ('consensus', 0.089), ('amimax', 0.084), ('dsum', 0.084), ('rand', 0.084), ('nformation', 0.082), ('easures', 0.082), ('normalized', 0.08), ('mutual', 0.08), ('vi', 0.079), ('ghosh', 0.076), ('ktrue', 0.072), ('distance', 0.072), ('adjustment', 0.067), ('cluster', 0.067), ('dsqrt', 0.06), ('unadjusted', 0.06), ('ri', 0.058), ('metric', 0.056), ('similarity', 0.053), ('omparison', 0.052), ('hubert', 0.051), ('triangle', 0.05), ('admax', 0.048), ('arabie', 0.048), ('kraskov', 0.048), ('nvi', 0.048), ('hypergeometric', 0.046), ('correction', 0.046), ('ci', 0.046), ('microarray', 0.046), ('index', 0.045), ('contingency', 0.045), ('normalization', 0.044), ('items', 0.041), ('concurrently', 0.037), ('monti', 0.037), ('admin', 0.036), ('albatineh', 0.036), ('asur', 0.036), ('fern', 0.036), ('kvalseth', 0.036), ('tumer', 0.036), ('unnormalized', 0.035), ('variation', 0.034), ('leukemia', 0.034), ('bits', 0.034), ('rc', 0.034), ('baseline', 0.033), ('ensemble', 0.033), ('admitting', 0.031), ('variants', 0.03), ('joint', 0.029), ('correcting', 0.028), ('tissues', 0.028), ('lattice', 0.027), ('nominal', 0.027), ('inequality', 0.026), ('additivity', 0.025), ('advocate', 0.025), ('adsqrt', 0.024), ('adsum', 0.024), ('agogino', 0.024), ('aiijb', 0.024), ('amimin', 0.024), ('amisqrt', 0.024), ('amisum', 0.024), ('charikar', 0.024), ('defective', 0.024), ('herein', 0.024), ('narrower', 0.024), ('nmimin', 0.024), ('nmisqrt', 0.024), ('novartis', 0.024), ('steinley', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="55-tfidf-1" href="./jmlr-2010-Information_Theoretic_Measures_for_Clusterings_Comparison%3A_Variants%2C_Properties%2C_Normalization_and_Correction_for_Chance.html">55 jmlr-2010-Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance</a></p>
<p>Author: Nguyen Xuan Vinh, Julien Epps, James Bailey</p><p>Abstract: Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and inter-relationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing popular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then highlight to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0, 1] range better than other normalized variants. Keywords: clustering comparison, information theory, adjustment for chance, normalized information distance</p><p>2 0.18052708 <a title="55-tfidf-2" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>Author: Yevgeny Seldin, Naftali Tishby</p><p>Abstract: We derive PAC-Bayesian generalization bounds for supervised and unsupervised learning models based on clustering, such as co-clustering, matrix tri-factorization, graphical models, graph clustering, and pairwise clustering.1 We begin with the analysis of co-clustering, which is a widely used approach to the analysis of data matrices. We distinguish among two tasks in matrix data analysis: discriminative prediction of the missing entries in data matrices and estimation of the joint probability distribution of row and column variables in co-occurrence matrices. We derive PAC-Bayesian generalization bounds for the expected out-of-sample performance of co-clustering-based solutions for these two tasks. The analysis yields regularization terms that were absent in the previous formulations of co-clustering. The bounds suggest that the expected performance of co-clustering is governed by a trade-off between its empirical performance and the mutual information preserved by the cluster variables on row and column IDs. We derive an iterative projection algorithm for ﬁnding a local optimum of this trade-off for discriminative prediction tasks. This algorithm achieved stateof-the-art performance in the MovieLens collaborative ﬁltering task. Our co-clustering model can also be seen as matrix tri-factorization and the results provide generalization bounds, regularization terms, and new algorithms for this form of matrix factorization. The analysis of co-clustering is extended to tree-shaped graphical models, which can be used to analyze high dimensional tensors. According to the bounds, the generalization abilities of treeshaped graphical models depend on a trade-off between their empirical data ﬁt and the mutual information that is propagated up the tree levels. We also formulate weighted graph clustering as a prediction problem: given a subset of edge weights we analyze the ability of graph clustering to predict the remaining edge weights. The analysis of co-clustering easily</p><p>3 0.081946298 <a title="55-tfidf-3" href="./jmlr-2010-An_Exponential_Model_for_Infinite_Rankings.html">10 jmlr-2010-An Exponential Model for Infinite Rankings</a></p>
<p>Author: Marina Meilă, Le Bao</p><p>Abstract: This paper presents a statistical model for expressing preferences through rankings, when the number of alternatives (items to rank) is large. A human ranker will then typically rank only the most preferred items, and may not even examine the whole set of items, or know how many they are. Similarly, a user presented with the ranked output of a search engine, will only consider the highest ranked items. We model such situations by introducing a stagewise ranking model that operates with ﬁnite ordered lists called top-t orderings over an inﬁnite space of items. We give algorithms to estimate this model from data, and demonstrate that it has sufﬁcient statistics, being thus an exponential family model with continuous and discrete parameters. We describe its conjugate prior and other statistical properties. Then, we extend the estimation problem to multimodal data by introducing an Exponential-Blurring-Mean-Shift nonparametric clustering algorithm. The experiments highlight the properties of our model and demonstrate that inﬁnite models over permutations can be simple, elegant and practical. Keywords: permutations, partial orderings, Mallows model, distance based ranking model, exponential family, non-parametric clustering, branch-and-bound</p><p>4 0.075182252 <a title="55-tfidf-4" href="./jmlr-2010-Characterization%2C_Stability_and_Convergence_of_Hierarchical_Clustering_Methods.html">19 jmlr-2010-Characterization, Stability and Convergence of Hierarchical Clustering Methods</a></p>
<p>Author: Gunnar Carlsson, Facundo Mémoli</p><p>Abstract: We study hierarchical clustering schemes under an axiomatic view. We show that within this framework, one can prove a theorem analogous to one of Kleinberg (2002), in which one obtains an existence and uniqueness theorem instead of a non-existence result. We explore further properties of this unique scheme: stability and convergence are established. We represent dendrograms as ultrametric spaces and use tools from metric geometry, namely the Gromov-Hausdorff distance, to quantify the degree to which perturbations in the input metric space affect the result of hierarchical methods. Keywords: clustering, hierarchical clustering, stability of clustering, Gromov-Hausdorff distance</p><p>5 0.043278016 <a title="55-tfidf-5" href="./jmlr-2010-Optimal_Search_on_Clustered_Structural_Constraint_for_Learning_Bayesian_Network_Structure.html">88 jmlr-2010-Optimal Search on Clustered Structural Constraint for Learning Bayesian Network Structure</a></p>
<p>Author: Kaname Kojima, Eric Perrier, Seiya Imoto, Satoru Miyano</p><p>Abstract: We study the problem of learning an optimal Bayesian network in a constrained search space; skeletons are compelled to be subgraphs of a given undirected graph called the super-structure. The previously derived constrained optimal search (COS) remains limited even for sparse superstructures. To extend its feasibility, we propose to divide the super-structure into several clusters and perform an optimal search on each of them. Further, to ensure acyclicity, we introduce the concept of ancestral constraints (ACs) and derive an optimal algorithm satisfying a given set of ACs. Finally, we theoretically derive the necessary and sufﬁcient sets of ACs to be considered for ﬁnding an optimal constrained graph. Empirical evaluations demonstrate that our algorithm can learn optimal Bayesian networks for some graphs containing several hundreds of vertices, and even for super-structures having a high average degree (up to four), which is a drastic improvement in feasibility over the previous optimal algorithm. Learnt networks are shown to largely outperform state-of-the-art heuristic algorithms both in terms of score and structural hamming distance. Keywords: Bayesian networks, structure learning, constrained optimal search</p><p>6 0.042976808 <a title="55-tfidf-6" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>7 0.036993209 <a title="55-tfidf-7" href="./jmlr-2010-On_Finding_Predictors_for_Arbitrary_Families_of_Processes.html">81 jmlr-2010-On Finding Predictors for Arbitrary Families of Processes</a></p>
<p>8 0.036149599 <a title="55-tfidf-8" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>9 0.033645406 <a title="55-tfidf-9" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>10 0.033547517 <a title="55-tfidf-10" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>11 0.032509241 <a title="55-tfidf-11" href="./jmlr-2010-A_Streaming_Parallel_Decision_Tree_Algorithm.html">7 jmlr-2010-A Streaming Parallel Decision Tree Algorithm</a></p>
<p>12 0.029969165 <a title="55-tfidf-12" href="./jmlr-2010-High-dimensional_Variable_Selection_with_Sparse_Random_Projections%3A_Measurement_Sparsity_and_Statistical_Efficiency.html">45 jmlr-2010-High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency</a></p>
<p>13 0.029456103 <a title="55-tfidf-13" href="./jmlr-2010-Model_Selection%3A_Beyond_the_Bayesian_Frequentist_Divide.html">78 jmlr-2010-Model Selection: Beyond the Bayesian Frequentist Divide</a></p>
<p>14 0.028858794 <a title="55-tfidf-14" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>15 0.028724719 <a title="55-tfidf-15" href="./jmlr-2010-Learning_Gradients%3A_Predictive_Models_that_Infer_Geometry_and_Statistical_Dependence.html">62 jmlr-2010-Learning Gradients: Predictive Models that Infer Geometry and Statistical Dependence</a></p>
<p>16 0.02767453 <a title="55-tfidf-16" href="./jmlr-2010-Classification_with_Incomplete_Data_Using_Dirichlet_Process_Priors.html">23 jmlr-2010-Classification with Incomplete Data Using Dirichlet Process Priors</a></p>
<p>17 0.02713356 <a title="55-tfidf-17" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>18 0.025334213 <a title="55-tfidf-18" href="./jmlr-2010-Graph_Kernels.html">44 jmlr-2010-Graph Kernels</a></p>
<p>19 0.02509667 <a title="55-tfidf-19" href="./jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</a></p>
<p>20 0.025025519 <a title="55-tfidf-20" href="./jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization.html">54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.134), (1, -0.001), (2, -0.004), (3, 0.016), (4, -0.066), (5, 0.034), (6, -0.026), (7, 0.064), (8, -0.007), (9, 0.176), (10, -0.085), (11, 0.222), (12, 0.134), (13, 0.025), (14, -0.073), (15, -0.246), (16, -0.237), (17, -0.091), (18, 0.206), (19, -0.135), (20, -0.079), (21, 0.004), (22, -0.03), (23, -0.192), (24, 0.082), (25, -0.053), (26, -0.155), (27, -0.021), (28, -0.047), (29, 0.01), (30, -0.186), (31, -0.005), (32, -0.067), (33, -0.065), (34, 0.034), (35, 0.024), (36, 0.022), (37, 0.084), (38, -0.006), (39, -0.066), (40, -0.053), (41, -0.03), (42, 0.044), (43, -0.038), (44, 0.019), (45, -0.075), (46, 0.055), (47, -0.185), (48, 0.017), (49, 0.139)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9731912 <a title="55-lsi-1" href="./jmlr-2010-Information_Theoretic_Measures_for_Clusterings_Comparison%3A_Variants%2C_Properties%2C_Normalization_and_Correction_for_Chance.html">55 jmlr-2010-Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance</a></p>
<p>Author: Nguyen Xuan Vinh, Julien Epps, James Bailey</p><p>Abstract: Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and inter-relationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing popular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then highlight to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0, 1] range better than other normalized variants. Keywords: clustering comparison, information theory, adjustment for chance, normalized information distance</p><p>2 0.68312097 <a title="55-lsi-2" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>Author: Yevgeny Seldin, Naftali Tishby</p><p>Abstract: We derive PAC-Bayesian generalization bounds for supervised and unsupervised learning models based on clustering, such as co-clustering, matrix tri-factorization, graphical models, graph clustering, and pairwise clustering.1 We begin with the analysis of co-clustering, which is a widely used approach to the analysis of data matrices. We distinguish among two tasks in matrix data analysis: discriminative prediction of the missing entries in data matrices and estimation of the joint probability distribution of row and column variables in co-occurrence matrices. We derive PAC-Bayesian generalization bounds for the expected out-of-sample performance of co-clustering-based solutions for these two tasks. The analysis yields regularization terms that were absent in the previous formulations of co-clustering. The bounds suggest that the expected performance of co-clustering is governed by a trade-off between its empirical performance and the mutual information preserved by the cluster variables on row and column IDs. We derive an iterative projection algorithm for ﬁnding a local optimum of this trade-off for discriminative prediction tasks. This algorithm achieved stateof-the-art performance in the MovieLens collaborative ﬁltering task. Our co-clustering model can also be seen as matrix tri-factorization and the results provide generalization bounds, regularization terms, and new algorithms for this form of matrix factorization. The analysis of co-clustering is extended to tree-shaped graphical models, which can be used to analyze high dimensional tensors. According to the bounds, the generalization abilities of treeshaped graphical models depend on a trade-off between their empirical data ﬁt and the mutual information that is propagated up the tree levels. We also formulate weighted graph clustering as a prediction problem: given a subset of edge weights we analyze the ability of graph clustering to predict the remaining edge weights. The analysis of co-clustering easily</p><p>3 0.62325293 <a title="55-lsi-3" href="./jmlr-2010-Characterization%2C_Stability_and_Convergence_of_Hierarchical_Clustering_Methods.html">19 jmlr-2010-Characterization, Stability and Convergence of Hierarchical Clustering Methods</a></p>
<p>Author: Gunnar Carlsson, Facundo Mémoli</p><p>Abstract: We study hierarchical clustering schemes under an axiomatic view. We show that within this framework, one can prove a theorem analogous to one of Kleinberg (2002), in which one obtains an existence and uniqueness theorem instead of a non-existence result. We explore further properties of this unique scheme: stability and convergence are established. We represent dendrograms as ultrametric spaces and use tools from metric geometry, namely the Gromov-Hausdorff distance, to quantify the degree to which perturbations in the input metric space affect the result of hierarchical methods. Keywords: clustering, hierarchical clustering, stability of clustering, Gromov-Hausdorff distance</p><p>4 0.41774583 <a title="55-lsi-4" href="./jmlr-2010-An_Exponential_Model_for_Infinite_Rankings.html">10 jmlr-2010-An Exponential Model for Infinite Rankings</a></p>
<p>Author: Marina Meilă, Le Bao</p><p>Abstract: This paper presents a statistical model for expressing preferences through rankings, when the number of alternatives (items to rank) is large. A human ranker will then typically rank only the most preferred items, and may not even examine the whole set of items, or know how many they are. Similarly, a user presented with the ranked output of a search engine, will only consider the highest ranked items. We model such situations by introducing a stagewise ranking model that operates with ﬁnite ordered lists called top-t orderings over an inﬁnite space of items. We give algorithms to estimate this model from data, and demonstrate that it has sufﬁcient statistics, being thus an exponential family model with continuous and discrete parameters. We describe its conjugate prior and other statistical properties. Then, we extend the estimation problem to multimodal data by introducing an Exponential-Blurring-Mean-Shift nonparametric clustering algorithm. The experiments highlight the properties of our model and demonstrate that inﬁnite models over permutations can be simple, elegant and practical. Keywords: permutations, partial orderings, Mallows model, distance based ranking model, exponential family, non-parametric clustering, branch-and-bound</p><p>5 0.28637591 <a title="55-lsi-5" href="./jmlr-2010-Quadratic_Programming_Feature_Selection.html">94 jmlr-2010-Quadratic Programming Feature Selection</a></p>
<p>Author: Irene Rodriguez-Lujan, Ramon Huerta, Charles Elkan, Carlos Santa Cruz</p><p>Abstract: Identifying a subset of features that preserves classiﬁcation accuracy is a problem of growing importance, because of the increasing size and dimensionality of real-world data sets. We propose a new feature selection method, named Quadratic Programming Feature Selection (QPFS), that reduces the task to a quadratic optimization problem. In order to limit the computational complexity of solving the optimization problem, QPFS uses the Nystr¨ m method for approximate matrix diago onalization. QPFS is thus capable of dealing with very large data sets, for which the use of other methods is computationally expensive. In experiments with small and medium data sets, the QPFS method leads to classiﬁcation accuracy similar to that of other successful techniques. For large data sets, QPFS is superior in terms of computational efﬁciency. Keywords: feature selection, quadratic programming, Nystr¨ m method, large data set, higho dimensional data</p><p>6 0.23513016 <a title="55-lsi-6" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>7 0.23048456 <a title="55-lsi-7" href="./jmlr-2010-Optimal_Search_on_Clustered_Structural_Constraint_for_Learning_Bayesian_Network_Structure.html">88 jmlr-2010-Optimal Search on Clustered Structural Constraint for Learning Bayesian Network Structure</a></p>
<p>8 0.22231083 <a title="55-lsi-8" href="./jmlr-2010-Chromatic_PAC-Bayes_Bounds_for_Non-IID_Data%3A_Applications_to_Ranking_and_Stationary_%CE%B2-Mixing_Processes.html">20 jmlr-2010-Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes</a></p>
<p>9 0.21504913 <a title="55-lsi-9" href="./jmlr-2010-On_Finding_Predictors_for_Arbitrary_Families_of_Processes.html">81 jmlr-2010-On Finding Predictors for Arbitrary Families of Processes</a></p>
<p>10 0.19044223 <a title="55-lsi-10" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>11 0.18057296 <a title="55-lsi-11" href="./jmlr-2010-Matrix_Completion_from__Noisy_Entries.html">72 jmlr-2010-Matrix Completion from  Noisy Entries</a></p>
<p>12 0.16176191 <a title="55-lsi-12" href="./jmlr-2010-Information_Retrieval_Perspective_to_Nonlinear_Dimensionality_Reduction_for_Data_Visualization.html">54 jmlr-2010-Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization</a></p>
<p>13 0.15787506 <a title="55-lsi-13" href="./jmlr-2010-A_Streaming_Parallel_Decision_Tree_Algorithm.html">7 jmlr-2010-A Streaming Parallel Decision Tree Algorithm</a></p>
<p>14 0.15703419 <a title="55-lsi-14" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>15 0.14501937 <a title="55-lsi-15" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>16 0.14276537 <a title="55-lsi-16" href="./jmlr-2010-Approximate_Inference_on_Planar_Graphs_using_Loop_Calculus_and_Belief_Propagation.html">13 jmlr-2010-Approximate Inference on Planar Graphs using Loop Calculus and Belief Propagation</a></p>
<p>17 0.14087744 <a title="55-lsi-17" href="./jmlr-2010-Evolving_Static_Representations_for_Task_Transfer.html">37 jmlr-2010-Evolving Static Representations for Task Transfer</a></p>
<p>18 0.13734552 <a title="55-lsi-18" href="./jmlr-2010-Efficient_Algorithms_for_Conditional_Independence_Inference.html">32 jmlr-2010-Efficient Algorithms for Conditional Independence Inference</a></p>
<p>19 0.13495198 <a title="55-lsi-19" href="./jmlr-2010-Matched_Gene_Selection_and_Committee_Classifier_for_Molecular_Classification_of_Heterogeneous_Diseases.html">71 jmlr-2010-Matched Gene Selection and Committee Classifier for Molecular Classification of Heterogeneous Diseases</a></p>
<p>20 0.13465011 <a title="55-lsi-20" href="./jmlr-2010-Generalized_Power_Method_for_Sparse_Principal_Component_Analysis.html">43 jmlr-2010-Generalized Power Method for Sparse Principal Component Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.02), (4, 0.026), (8, 0.021), (15, 0.01), (21, 0.016), (24, 0.024), (32, 0.039), (33, 0.014), (36, 0.022), (37, 0.039), (47, 0.476), (75, 0.1), (81, 0.036), (85, 0.043), (96, 0.02), (97, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.67488194 <a title="55-lda-1" href="./jmlr-2010-Information_Theoretic_Measures_for_Clusterings_Comparison%3A_Variants%2C_Properties%2C_Normalization_and_Correction_for_Chance.html">55 jmlr-2010-Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance</a></p>
<p>Author: Nguyen Xuan Vinh, Julien Epps, James Bailey</p><p>Abstract: Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and inter-relationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing popular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then highlight to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0, 1] range better than other normalized variants. Keywords: clustering comparison, information theory, adjustment for chance, normalized information distance</p><p>2 0.26120359 <a title="55-lda-2" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>Author: Ming Yuan</p><p>Abstract: This paper considers the problem of estimating a high dimensional inverse covariance matrix that can be well approximated by “sparse” matrices. Taking advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix, we propose an estimating procedure that can effectively exploit such “sparsity”. The proposed method can be computed using linear programming and therefore has the potential to be used in very high dimensional problems. Oracle inequalities are established for the estimation error in terms of several operator norms, showing that the method is adaptive to different types of sparsity of the problem. Keywords: covariance selection, Dantzig selector, Gaussian graphical model, inverse covariance matrix, Lasso, linear programming, oracle inequality, sparsity</p><p>3 0.25942674 <a title="55-lda-3" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>Author: Yevgeny Seldin, Naftali Tishby</p><p>Abstract: We derive PAC-Bayesian generalization bounds for supervised and unsupervised learning models based on clustering, such as co-clustering, matrix tri-factorization, graphical models, graph clustering, and pairwise clustering.1 We begin with the analysis of co-clustering, which is a widely used approach to the analysis of data matrices. We distinguish among two tasks in matrix data analysis: discriminative prediction of the missing entries in data matrices and estimation of the joint probability distribution of row and column variables in co-occurrence matrices. We derive PAC-Bayesian generalization bounds for the expected out-of-sample performance of co-clustering-based solutions for these two tasks. The analysis yields regularization terms that were absent in the previous formulations of co-clustering. The bounds suggest that the expected performance of co-clustering is governed by a trade-off between its empirical performance and the mutual information preserved by the cluster variables on row and column IDs. We derive an iterative projection algorithm for ﬁnding a local optimum of this trade-off for discriminative prediction tasks. This algorithm achieved stateof-the-art performance in the MovieLens collaborative ﬁltering task. Our co-clustering model can also be seen as matrix tri-factorization and the results provide generalization bounds, regularization terms, and new algorithms for this form of matrix factorization. The analysis of co-clustering is extended to tree-shaped graphical models, which can be used to analyze high dimensional tensors. According to the bounds, the generalization abilities of treeshaped graphical models depend on a trade-off between their empirical data ﬁt and the mutual information that is propagated up the tree levels. We also formulate weighted graph clustering as a prediction problem: given a subset of edge weights we analyze the ability of graph clustering to predict the remaining edge weights. The analysis of co-clustering easily</p><p>4 0.25839236 <a title="55-lda-4" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>Author: Miloš Radovanović, Alexandros Nanopoulos, Mirjana Ivanović</p><p>Abstract: Different aspects of the curse of dimensionality are known to present serious challenges to various machine-learning methods and tasks. This paper explores a new aspect of the dimensionality curse, referred to as hubness, that affects the distribution of k-occurrences: the number of times a point appears among the k nearest neighbors of other points in a data set. Through theoretical and empirical analysis involving synthetic and real data sets we show that under commonly used assumptions this distribution becomes considerably skewed as dimensionality increases, causing the emergence of hubs, that is, points with very high k-occurrences which effectively represent “popular” nearest neighbors. We examine the origins of this phenomenon, showing that it is an inherent property of data distributions in high-dimensional vector space, discuss its interaction with dimensionality reduction, and explore its inﬂuence on a wide range of machine-learning tasks directly or indirectly based on measuring distances, belonging to supervised, semi-supervised, and unsupervised learning families. Keywords: nearest neighbors, curse of dimensionality, classiﬁcation, semi-supervised learning, clustering</p><p>5 0.2555871 <a title="55-lda-5" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>Author: Gal Chechik, Varun Sharma, Uri Shalit, Samy Bengio</p><p>Abstract: Learning a measure of similarity between pairs of objects is an important generic problem in machine learning. It is particularly useful in large scale applications like searching for an image that is similar to a given image or ﬁnding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, the approaches that exist today for learning such semantic similarity do not scale to large data sets. This is both because typically their CPU and storage requirements grow quadratically with the sample size, and because many methods impose complex positivity constraints on the space of learned similarity functions. The current paper presents OASIS, an Online Algorithm for Scalable Image Similarity learning that learns a bilinear similarity measure over sparse representations. OASIS is an online dual approach using the passive-aggressive family of learning algorithms with a large margin criterion and an efﬁcient hinge loss cost. Our experiments show that OASIS is both fast and accurate at a wide range of scales: for a data set with thousands of images, it achieves better results than existing state-of-the-art methods, while being an order of magnitude faster. For large, web scale, data sets, OASIS can be trained on more than two million images from 150K text queries within 3 days on a single CPU. On this large scale data set, human evaluations showed that 35% of the ten nearest neighbors of a given test image, as found by OASIS, were semantically relevant to that image. This suggests that query independent similarity could be accurately learned even for large scale data sets that could not be handled before. Keywords: large scale, metric learning, image similarity, online learning ∗. Varun Sharma and Uri Shalit contributed equally to this work. †. Also at ICNC, The Hebrew University of Jerusalem, 91904, Israel. c 2010 Gal Chechik, Varun Sharma, Uri Shalit</p><p>6 0.25530449 <a title="55-lda-6" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>7 0.25524762 <a title="55-lda-7" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>8 0.25497481 <a title="55-lda-8" href="./jmlr-2010-An_Exponential_Model_for_Infinite_Rankings.html">10 jmlr-2010-An Exponential Model for Infinite Rankings</a></p>
<p>9 0.25400233 <a title="55-lda-9" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>10 0.25339586 <a title="55-lda-10" href="./jmlr-2010-Topology_Selection_in_Graphical_Models_of_Autoregressive_Processes.html">111 jmlr-2010-Topology Selection in Graphical Models of Autoregressive Processes</a></p>
<p>11 0.25186688 <a title="55-lda-11" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>12 0.25179356 <a title="55-lda-12" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>13 0.2517173 <a title="55-lda-13" href="./jmlr-2010-Dimensionality_Estimation%2C_Manifold_Learning_and_Function_Approximation_using_Tensor_Voting.html">30 jmlr-2010-Dimensionality Estimation, Manifold Learning and Function Approximation using Tensor Voting</a></p>
<p>14 0.25131717 <a title="55-lda-14" href="./jmlr-2010-Graph_Kernels.html">44 jmlr-2010-Graph Kernels</a></p>
<p>15 0.25108433 <a title="55-lda-15" href="./jmlr-2010-Spectral_Regularization_Algorithms_for_Learning_Large_Incomplete_Matrices.html">105 jmlr-2010-Spectral Regularization Algorithms for Learning Large Incomplete Matrices</a></p>
<p>16 0.25090083 <a title="55-lda-16" href="./jmlr-2010-Lp-Nested_Symmetric_Distributions.html">69 jmlr-2010-Lp-Nested Symmetric Distributions</a></p>
<p>17 0.25026259 <a title="55-lda-17" href="./jmlr-2010-Linear_Algorithms_for_Online_Multitask_Classification.html">66 jmlr-2010-Linear Algorithms for Online Multitask Classification</a></p>
<p>18 0.25002998 <a title="55-lda-18" href="./jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</a></p>
<p>19 0.24974644 <a title="55-lda-19" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>20 0.24929775 <a title="55-lda-20" href="./jmlr-2010-Bayesian_Learning_in_Sparse_Graphical_Factor_Models_via_Variational_Mean-Field_Annealing.html">17 jmlr-2010-Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
