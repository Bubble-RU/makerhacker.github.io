<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 jmlr-2010-Erratum: SGDQN is Less Careful than Expected</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2010" href="../home/jmlr2010_home.html">jmlr2010</a> <a title="jmlr-2010-34" href="#">jmlr2010-34</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>34 jmlr-2010-Erratum: SGDQN is Less Careful than Expected</h1>
<br/><p>Source: <a title="jmlr-2010-34-pdf" href="http://jmlr.org/papers/volume11/bordes10a/bordes10a.pdf">pdf</a></p><p>Author: Antoine Bordes, Léon Bottou, Patrick Gallinari, Jonathan Chang, S. Alex Smith</p><p>Abstract: The SGD-QN algorithm described in Bordes et al. (2009) contains a subtle ﬂaw that prevents it from reaching its design goals. Yet the ﬂawed SGD-QN algorithm has worked well enough to be a winner of the ﬁrst Pascal Large Scale Learning Challenge (Sonnenburg et al., 2008). This document clariﬁes the situation, proposes a corrected algorithm, and evaluates its performance. Keywords: stochastic gradient descent, support vector machine, conditional random ﬁelds</p><p>Reference: <a title="jmlr-2010-34-reference" href="../jmlr2010_reference/jmlr-2010-Erratum%3A_SGDQN_is_Less_Careful_than_Expected_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Yet the ﬂawed SGD-QN algorithm has worked well enough to be a winner of the ﬁrst Pascal Large Scale Learning Challenge (Sonnenburg et al. [sent-13, score-0.012]
</p><p>2 This document clariﬁes the situation, proposes a corrected algorithm, and evaluates its performance. [sent-15, score-0.289]
</p><p>3 Keywords: stochastic gradient descent, support vector machine, conditional random ﬁelds  1. [sent-16, score-0.021]
</p><p>4 (2009) propose to improve the practical speed of stochastic gradient descent by efﬁciently estimating a diagonal matrix for rescaling the gradient estimates. [sent-18, score-0.048]
</p><p>5 The proposed algorithm, SGD-QN, works well enough to be a winner of the ﬁrst Pascal Large Scale Learning Challenge (Sonnenburg et al. [sent-19, score-0.012]
</p><p>6 Then we present a corrected algorithm and evaluate its performance for training both linear Support Vector Machines (SVMs) and Conditional Random Fields (CRFs). [sent-26, score-0.289]
</p><p>7 (xn , yn )}, we obtain a linear SVM classiﬁer by minimizing the cost  Pn (w) =  λ w 2  2  +  1 1 n ∑ ℓ(yi w⊤xi ) = n n i=1  n  ∑  i=1  λ w 2  2  + ℓ(yi w⊤xi ) . [sent-35, score-0.009]
</p><p>8 In the following, expectations and probabilities refer to the discrete distribution describing the training examples randomly picked from the ﬁnite training set at each iteration. [sent-37, score-0.03]
</p><p>9 (xt−1 , yt−1 )} picked before reaching the t-th iteration. [sent-41, score-0.01]
</p><p>10 We would like to ﬁnd B such that ′ ′ wt+1 − wt = B Pn (wt+1 ) − Pn (wt ) + ξt ,  (2)  with an error term ξt verifying E [ ξt | Ft ] = 0. [sent-42, score-0.154]
</p><p>11 The SGD-QN algorithm updates the diagonal elements of matrix B on-the-ﬂy on the basis of the term-by-term ratios of the observed differences wt+1 − wt and gτ (wt+1 ) − gτ (wt ). [sent-46, score-0.172]
</p><p>12 The obvious choices τ = t and τ = t + 1 only require one additional gradient evaluation because the parameter update formula (1) demands the computation of all the gradients gt (wt ) anyway. [sent-47, score-0.092]
</p><p>13 Since gt+1 (wt+1 ) is a function of (xt+1 , yt+1 , xt , yt , Ft ), E [ gτ (wt+1 )| Ft ] = =  gt+1 (wt+1 ) dP(xt+1 , yt+1 , xt , yt | Ft ) gt+1 (wt+1 ) dP(xt+1 , yt+1 ) dP(xt , yt | Ft ) . [sent-51, score-0.369]
</p><p>14 2230  E RRATUM : SGD-QN IS L ESS C AREFUL THAN E XPECTED  Since the variables wt+1 and (xt+1 , yt+1 ) are independent, the inner integral above is simply the average of gt+1 (wt+1 ) for all possible (xt+1 , yt+1 ) picked from the training set. [sent-52, score-0.02]
</p><p>15 Therefore E [ gτ (wt+1 )| Ft ] =  ′ ′ Pn (wt+1 ) dP(xt , yt | Ft ) = E Pn (wt+1 ) Ft . [sent-53, score-0.087]
</p><p>16 Such a derivation is impossible when τ = t because (xt , yt ) and wt+1 are not independent. [sent-56, score-0.087]
</p><p>17 The Consequences In order to take maximal advantage of sparse data sets, the Flawed SGD-QN algorithm (see Figure 2 in the original paper) splits the stochastic parameter update (1) in two halves in order to schedule them separately. [sent-62, score-0.039]
</p><p>18 The ﬁrst half involves only the gradient of the loss, w ← w − (t + t0 )−1 B ℓ′ (yt w⊤ xt ) yt xt . [sent-63, score-0.214]
</p><p>19 The second half involves only the gradient of the regularization term, w←  w most of the time, −1 B w once every skip iterations. [sent-64, score-0.061]
</p><p>20 w − skip λ (t + t0 )  The Flawed SGD-QN algorithm measures the differences wt+1 − wt and gt (wt+1 ) − gt (wt ) during iterations for which the second half does nothing. [sent-65, score-0.342]
</p><p>21 Therefore, using notations [x]i for the i-th coefﬁcient of vector x, and Bii for the terms of the diagonal matrix B, we always have ℓ′ (yt w⊤ xt ) − ℓ′ (yt w⊤xt ) yt [xt ]i [gt (wt+1 ) − gt (wt )]i t t+1 = λ− . [sent-66, score-0.219]
</p><p>22 [wt+1 − wt ]i Bii (t + t0 )−1 ℓ′ (yt w⊤xt ) yt [xt ]i • When [xt ]i is nonzero, we can simplify this expression as ℓ′ (yt w⊤ xt ) − ℓ′ (yt w⊤xt ) [gt (wt+1 ) − gt (wt )]i t t+1 = λ− . [sent-67, score-0.363]
</p><p>23 [wt+1 − wt ]i Bii (t + t0 )−1 ℓ′ (yt w⊤xt ) t  (4)  This ratio is always greater than λ because of the loss function ℓ is convex. [sent-68, score-0.154]
</p><p>24 • When [xt ]i is zero, the original paper uses a continuity argument to justify the equality [gt (wt+1 ) − gt (wt )]i = λ. [sent-70, score-0.068]
</p><p>25 [wt+1 − wt ]i  2231  (5)  B ORDES , B OTTOU , G ALLINARI , C HANG AND S MITH  23. [sent-71, score-0.154]
</p><p>26 5 0  1  2  3 4 Number of epochs  5  6  7  0  D ELTA  1  2 3 Number of epochs  4  5  DATA SET  Figure 1: Plots of the training cost and test misclassiﬁcation percentage versus the number of epochs for SVMSGD2 (red) and Flawed SGD-QN (green) for various values of t0 on the dense Delta data set. [sent-79, score-0.342]
</p><p>27 1 Impact on Dense Data Sets The coefﬁcients [xt ]i for dense data sets are rarely zero. [sent-82, score-0.018]
</p><p>28 Since the Bii coefﬁcients are initially equal, they remain equal all the time, except maybe when encountering an occasional zero in the patterns xt . [sent-86, score-0.054]
</p><p>29 Since the scaling matrix reduces to a scalar gain, similar results could in principle be obtained using the ordinary stochastic gradient descent with a better gain schedule. [sent-88, score-0.029]
</p><p>30 Figure 1 compares the evolutions of the training cost and the test misclassiﬁcation error for the SVMSGD2 and the Flawed SGD-QN algorithms for selected values of the parameter t0 instead of the usual heuristic defaults. [sent-90, score-0.047]
</p><p>31 In some cases, Flawed SGD-QN can even slightly outperforms SVMSGD2 because, despite the ﬂaw, it can still update its learning rate on the course of learning. [sent-94, score-0.008]
</p><p>32 2 Impact on Sparse Data Sets The situation is more complex in the case of sparse data sets because there is special case for updating the Bii coefﬁcients when dealing with zero coefﬁcients (5). [sent-97, score-0.009]
</p><p>33 As a result, the Flawed SGDQN algorithm gives higher values to the scaling coefﬁcients Bii when the i-th feature is more likely 2232  E RRATUM : SGD-QN IS L ESS C AREFUL THAN E XPECTED  0. [sent-98, score-0.008]
</p><p>34 5  4  DATA SET  Figure 2: Plots of the training cost and test misclassiﬁcation error versus the number of epochs for both SVMSGD2 (red) and Flawed SGD-QN (green) running on the RCV1 data set. [sent-117, score-0.128]
</p><p>35 Both algorithms reach optimal performance after seeing half the training set. [sent-118, score-0.02]
</p><p>36 Since this is a sensible scaling for such data sets, the Flawed SGD-QN algorithm works relatively well in the presence of sparse features. [sent-120, score-0.017]
</p><p>37 Figure 2 compares the SVMSGD2 and Flawed SGD-QN algorithms for many choices for the t0 parameter on the Reuters RCV1 data set. [sent-121, score-0.017]
</p><p>38 Both algorithms reach optimal performance after processing only one half of the training set. [sent-123, score-0.02]
</p><p>39 In order to ﬁnd a more challenging sparse data set, we have adapted both the SVMSGD2 and the Flawed SGD-QN algorithms for the optimization of Conditional Random Fields (Lafferty et al. [sent-124, score-0.009]
</p><p>40 This is an interesting case where preconditioning is difﬁcult because the features are generated on the ﬂy on the basis of position-independent templates. [sent-126, score-0.033]
</p><p>41 Figure 3 compares the algorithms on the CoNLL 2000 “chunking” task (Sang and Buchholz, 2000) using the template setup provided as an example with the CRF++ code (Kudo, 2007). [sent-127, score-0.017]
</p><p>42 The Flawed SGD-QN algorithm reaches the best test performance after less epochs than the SVMSGD2 algorithm, but this does not translate into a large improvement in terms of training time. [sent-128, score-0.119]
</p><p>43 Correcting SGD-QN At ﬁrst glance, correcting SGD-QN simply involves computing the difference gτ (wt+1 ) − gτ (wt ) with τ = t + 1 instead of τ = t. [sent-130, score-0.008]
</p><p>44 In fact, during the weeks preceding the Pascal challenge deadline, we tried both versions and found that picking τ = t + 1 performs signiﬁcantly worse! [sent-131, score-0.007]
</p><p>45 5  Test FB1 score  94  2  93 SVMSGD2 eta0=1e-2 SVMSGD2 eta0=3e-2 SVMSGD2 eta0=1e-1 SVMSGD2 eta0=3e-1 SVMSGD2 eta0=1 Flawed SGDQN eta0=1e-2 Flawed SGDQN eta0=3e-2 Flawed SGDQN eta0=1e-1 Flawed SGDQN eta0=3e-1 Flawed SGDQN eta0=1  92. [sent-138, score-0.011]
</p><p>46 5  92 0  2  4  6 8 10 Number of epochs  CRF  12  ON THE  93 SVMSGD2 eta0=1e-2 SVMSGD2 eta0=3e-2 SVMSGD2 eta0=1e-1 SVMSGD2 eta0=3e-1 SVMSGD2 eta0=1 Flawed SGDQN eta0=1e-2 Flawed SGDQN eta0=3e-2 Flawed SGDQN eta0=1e-1 Flawed SGDQN eta0=3e-1 Flawed SGDQN eta0=1  92. [sent-139, score-0.098]
</p><p>47 )  200  250  CONLL 2000 C HUNKING TASK  Figure 3: Plots of the training cost, test loss, and test F1 score for a CRF trained using both 1 SVMSGD2 (red) and Flawed SGD-QN (green) for various initial rates η0 = λt . [sent-141, score-0.058]
</p><p>48 0  In order to form an intuition about the learning rate, we must pay attention to its inﬂuence on the stochastic noise. [sent-142, score-0.012]
</p><p>49 Stochastic gradient descent with a constant learning rate generates a cloud of parameter estimates wt covering a zone whose extent is deﬁned by the learning rate and by the curvature of the cost function. [sent-143, score-0.182]
</p><p>50 When the rates decrease with an appropriate speed, this zone shrinks around the solution. [sent-144, score-0.025]
</p><p>51 t0 + t  (6)  Since the algorithm periodically adapts Bii on the basis of the observed differences wt+1 − wt and gt+1 (wt+1 ) − gt+1 (wt ), the sequence of learning rates ηﬂawQN can occasionally increase. [sent-146, score-0.169]
</p><p>52 This is i,t Bii conﬁrmed by the middle plot of Figure 5 which displays the evolution of the learning rates t0 +t for SGD-QN implementing this straightforward ﬁx. [sent-147, score-0.024]
</p><p>53 2 Managing the Speed of the Learning Rate Decrease The schedule with which the learning rate decreases during training appears to be a key factor, so we propose to ﬁx SGD-QN by using the second-order information to manage this diminution. [sent-152, score-0.02]
</p><p>54 Hence, we use learning rates of the form  ηi,t  corQN  t−1  =  λt0 + ∑ ri,k  −1  where ri,t =  k=1  [gt+1 (wt+1 ) − gt+1 (wt )]i . [sent-153, score-0.015]
</p><p>55 1 It is also interesting to compare the formula ηcorQN with the ﬁrst order version ηSGD = λt +∑t λ i,t i,t 0 k=1 which decreases the learning rate after each iteration by adding λ to the denominator. [sent-155, score-0.007]
</p><p>56 Instead of adding a lower bound of the curvature, the proposed learning rate formula adds a stochastic estimate of the curvature. [sent-156, score-0.019]
</p><p>57 1  1  Number of epochs (logscale)  Number of epochs (logscale)  Flawed SGD-QN  Straightforward Fix  10  Number of epochs (logscale)  Corrected SGD-QN  Figure 5: Plots of the learning rates corresponding to each feature on the course of learning on the Delta data set. [sent-174, score-0.309]
</p><p>58 All rates are equal for Flawed SGD-QN (left plot). [sent-175, score-0.015]
</p><p>59 1, implementing the straightforward ﬁx (middle plot) causes rates to alternatively increase or decrease very fast. [sent-177, score-0.024]
</p><p>60 The Corrected SGD-QN (right plot) proposes learning rates nicely decreasing at different speeds for each feature. [sent-178, score-0.025]
</p><p>61 Interestingly, Equation (7) leads to a convenient recursive formula ηi,t  corQN  =  1 ηcorQN i,t−1  −1  + ri,t−1  =  ηcorQN i,t−1 1 + ri,t−1 ηcorQN i,t−1  . [sent-179, score-0.007]
</p><p>62 (8)  Figure 4 describes the Corrected SGD-QN algorithm and compares it with a slightly reorganized version of the Flawed SGD-QN algorithm. [sent-180, score-0.017]
</p><p>63 The diagonal matrix B is used to store the gains (7). [sent-181, score-0.01]
</p><p>64 The algorithm schedules a gain update (line 14) whenever it performs a regularization update (line 16). [sent-182, score-0.016]
</p><p>65 During the next iteration, the algorithm computes ri,t−1 (line 6) and implements the learning rate update (8) with an additional multiplicative factor (line 8) because this only happens every skip iterations. [sent-183, score-0.05]
</p><p>66 The effect on the learning rates of using Corrected SGD-QN instead of Flawed SGD-QN is illustrated by Figure 5 if we compare the left and the right plots. [sent-184, score-0.015]
</p><p>67 3 Performances on Dense Data Sets Figure 6 (top row) compares the performances of Corrected SGD-QN with the best results of Flawed SGD-QN and SVMSGD2 on the Delta data set. [sent-186, score-0.026]
</p><p>68 Before running the SGD algorithms, we always precondition the dense data sets by centering all the features, normalizing their variances, and rescaling every example to ensure that xk = 1. [sent-188, score-0.037]
</p><p>69 Yet, implementing a strategy involving a single learning rate for all the features appears already very rewarding and, for such cases, the Flawed SGD-QN algorithm is a strong choice because of its capacity to adapt its learning rate. [sent-191, score-0.009]
</p><p>70 Figure 6 (bottom row) compares the performances of SVMSGD2, Flawed SGD-QN and Corrected SGD-QN on this deconditioned data. [sent-194, score-0.052]
</p><p>71 35  0  1  2 3 Number of epochs  4  5  DATA SET ( NORMALIZED ) 25. [sent-204, score-0.098]
</p><p>72 Both normalized (top) and deconditioned (bottom) cases are considered; see the text for details. [sent-214, score-0.026]
</p><p>73 SGD-QN algorithm clearly suffers from the deconditioning operation because it can not assign a different learning rate per feature. [sent-216, score-0.013]
</p><p>74 We also veriﬁed that the estimated learning rates replicate the deconditioning pattern. [sent-218, score-0.028]
</p><p>75 In conclusion, on dense data sets, the Corrected SGD-QN bring little improvement over those associated with a good preconditioning technique. [sent-219, score-0.051]
</p><p>76 Preconditioning was probably the main reason of the good SGD-QN results on dense data sets in the Pascal Large Scale Challenge. [sent-220, score-0.018]
</p><p>77 5  4  DATA SET  Figure 7: Plots of the training cost and test error versus the number of epochs for SVMSGD2 (red) and Flawed SGD-QN (green) with their optimal t0 parameter, and Corrected SGD-QN (brown) running on the RCV1 data set. [sent-240, score-0.128]
</p><p>78 4 Performances on Sparse Data Sets Preconditioning sparse data sets is much more difﬁcult because it is impossible to center sparse features and keep them sparse. [sent-245, score-0.018]
</p><p>79 In addition, normalizing the variance of very rare features generates a small number of coefﬁcients with high values. [sent-246, score-0.011]
</p><p>80 This fat tail distribution usually has very negative impact on the test performance. [sent-247, score-0.011]
</p><p>81 Figure 7 compares the SVMSGD2, Flawed SGD-QN and Corrected SGD-QN algorithms on the Reuters RCV1 data set, but, as we explained for Figure 2, this task is too easy to draw any conclusions. [sent-248, score-0.017]
</p><p>82 Figure 8 then compares the adaptations of SVMSGD2, Flawed SGD-QN (with their best parameters) and Corrected SGD-QN for Conditional Random Fields on the CoNLL 2000 “chunking” task with the setup described in Section 4. [sent-249, score-0.017]
</p><p>83 The Corrected SGD-QN algorithm achieves its optimal test performance after only 75 seconds while SVMSGD2 and Flawed SGD-QN need around twice this time. [sent-251, score-0.011]
</p><p>84 Conclusion Despite its ﬂaw, the original SGD-QN algorithm works well enough to be a winner of the ﬁrst PASCAL Large Scale Learning Challenge (Sonnenburg et al. [sent-254, score-0.012]
</p><p>85 , 2008) because it beneﬁts from our careful preconditioning and handles sparse examples efﬁciently. [sent-255, score-0.042]
</p><p>86 However, as explained in this document, this original version often does not achieve the full beneﬁts of a diagonal scaling approach. [sent-256, score-0.018]
</p><p>87 5  Test FB1 score  94  2  93  Best SVMSGD2 eta0=1e-1 Best Flawed SGDQN eta0=1e-1 Corrected SGDQN eta0=1e-2 Corrected SGDQN eta0=3e-2 Corrected SGDQN eta0=1e-1 Corrected SGDQN eta0=3e-1 Corrected SGDQN eta0=1  92. [sent-259, score-0.011]
</p><p>88 5  92 0  2  4  6 8 10 Number of epochs  CRF  12  ON THE  93  Best SVMSGD2 eta0=1e-1 Best Flawed SGDQN eta0=1e-1 Corrected SGDQN eta0=1e-2 Corrected SGDQN eta0=3e-2 Corrected SGDQN eta0=1e-1 Corrected SGDQN eta0=3e-1 Corrected SGDQN eta0=1  92. [sent-260, score-0.098]
</p><p>89 )  200  250  CONLL 2000 C HUNKING TASK  Figure 8: Plots of the training cost, test loss, and test F1 score for a CRF trained using the best setups of SVMSGD2 (red) and Flawed SGD-QN (green), and Corrected SGD-QN for various 1 initial rates η0 = λt0 (brown). [sent-262, score-0.058]
</p><p>90 Unlike the original SGD-QN algorithm, the Corrected SGDQN algorithm discovers sensible diagonal scaling coefﬁcients. [sent-265, score-0.018]
</p><p>91 However, experiments on dense data sets of intermediate dimensionality show that similar speed improvements can be achieved by simple preconditioning techniques such as normalizing the means and the variances of each feature and normalizing the length of each example. [sent-266, score-0.073]
</p><p>92 A stochastic quasi-Newton method for online convex optiu mization. [sent-316, score-0.012]
</p><p>93 Towards optimal one pass large scale learning with averaged stochastic gradient descent. [sent-336, score-0.021]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sgdqn', 0.717), ('flawed', 0.554), ('corrected', 0.279), ('wt', 0.154), ('bii', 0.111), ('epochs', 0.098), ('yt', 0.087), ('gt', 0.068), ('xt', 0.054), ('updateb', 0.052), ('ft', 0.047), ('corqn', 0.046), ('olbfgs', 0.046), ('skip', 0.042), ('allinari', 0.039), ('ordes', 0.039), ('ottou', 0.039), ('crf', 0.035), ('mith', 0.033), ('preconditioning', 0.033), ('areful', 0.033), ('ess', 0.033), ('rratum', 0.033), ('bordes', 0.03), ('xpected', 0.028), ('deconditioned', 0.026), ('count', 0.022), ('ri', 0.022), ('aw', 0.022), ('pascal', 0.02), ('awqn', 0.02), ('elta', 0.02), ('facebook', 0.02), ('logscale', 0.02), ('hang', 0.019), ('pn', 0.019), ('dense', 0.018), ('conll', 0.017), ('sang', 0.017), ('compares', 0.017), ('sgd', 0.016), ('delta', 0.016), ('green', 0.015), ('rates', 0.015), ('sonnenburg', 0.015), ('dp', 0.014), ('antoine', 0.014), ('jonathan', 0.014), ('claire', 0.013), ('deconditioning', 0.013), ('hunking', 0.013), ('gallinari', 0.013), ('primal', 0.012), ('winner', 0.012), ('stochastic', 0.012), ('schraudolph', 0.012), ('score', 0.011), ('jussieu', 0.011), ('lip', 0.011), ('tjong', 0.011), ('test', 0.011), ('normalizing', 0.011), ('patrick', 0.011), ('cients', 0.011), ('plots', 0.01), ('coef', 0.01), ('picked', 0.01), ('diagonal', 0.01), ('awed', 0.01), ('schedule', 0.01), ('zone', 0.01), ('red', 0.01), ('half', 0.01), ('proposes', 0.01), ('training', 0.01), ('cost', 0.009), ('alex', 0.009), ('fields', 0.009), ('bottou', 0.009), ('reuters', 0.009), ('curie', 0.009), ('gradient', 0.009), ('misclassi', 0.009), ('sparse', 0.009), ('implementing', 0.009), ('performances', 0.009), ('marie', 0.009), ('scaling', 0.008), ('ratios', 0.008), ('brown', 0.008), ('polyak', 0.008), ('franc', 0.008), ('chunking', 0.008), ('false', 0.008), ('rescaling', 0.008), ('correcting', 0.008), ('update', 0.008), ('paris', 0.007), ('challenge', 0.007), ('formula', 0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="34-tfidf-1" href="./jmlr-2010-Erratum%3A_SGDQN_is_Less_Careful_than_Expected.html">34 jmlr-2010-Erratum: SGDQN is Less Careful than Expected</a></p>
<p>Author: Antoine Bordes, Léon Bottou, Patrick Gallinari, Jonathan Chang, S. Alex Smith</p><p>Abstract: The SGD-QN algorithm described in Bordes et al. (2009) contains a subtle ﬂaw that prevents it from reaching its design goals. Yet the ﬂawed SGD-QN algorithm has worked well enough to be a winner of the ﬁrst Pascal Large Scale Learning Challenge (Sonnenburg et al., 2008). This document clariﬁes the situation, proposes a corrected algorithm, and evaluates its performance. Keywords: stochastic gradient descent, support vector machine, conditional random ﬁelds</p><p>2 0.1025421 <a title="34-tfidf-2" href="./jmlr-2010-Dual_Averaging_Methods_for_Regularized_Stochastic_Learning_and_Online_Optimization.html">31 jmlr-2010-Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a></p>
<p>Author: Lin Xiao</p><p>Abstract: We consider regularized stochastic learning and online optimization problems, where the objective function is the sum of two convex terms: one is the loss function of the learning task, and the other is a simple regularization term such as ℓ1 -norm for promoting sparsity. We develop extensions of Nesterov’s dual averaging method, that can exploit the regularization structure in an online setting. At each iteration of these methods, the learning variables are adjusted by solving a simple minimization problem that involves the running average of all past subgradients of the loss function and the whole regularization term, not just its subgradient. In the case of ℓ1 -regularization, our method is particularly effective in obtaining sparse solutions. We show that these methods achieve the optimal convergence rates or regret bounds that are standard in the literature on stochastic and online convex optimization. For stochastic learning problems in which the loss functions have Lipschitz continuous gradients, we also present an accelerated version of the dual averaging method. Keywords: stochastic learning, online optimization, ℓ1 -regularization, structural convex optimization, dual averaging methods, accelerated gradient methods</p><p>3 0.087866172 <a title="34-tfidf-3" href="./jmlr-2010-Bundle_Methods_for_Regularized_Risk_Minimization.html">18 jmlr-2010-Bundle Methods for Regularized Risk Minimization</a></p>
<p>Author: Choon Hui Teo, S.V.N. Vishwanthan, Alex J. Smola, Quoc V. Le</p><p>Abstract: A wide variety of machine learning problems can be described as minimizing a regularized risk functional, with different algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Machines (SVMs), Gaussian Processes, Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper describes the theory and implementation of a scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as L1 and L2 penalties. In addition to the uniﬁed framework we present tight convergence bounds, which show that our algorithm converges in O(1/ε) steps to ε precision for general convex problems and in O(log(1/ε)) steps for continuously differentiable problems. We demonstrate the performance of our general purpose solver on a variety of publicly available data sets. Keywords: optimization, subgradient methods, cutting plane method, bundle methods, regularized risk minimization, parallel optimization ∗. Also at Canberra Research Laboratory, NICTA. c 2010 Choon Hui Teo, S.V. N. Vishwanthan, Alex J. Smola and Quoc V. Le. T EO , V ISHWANATHAN , S MOLA AND L E</p><p>4 0.064932592 <a title="34-tfidf-4" href="./jmlr-2010-A_Quasi-Newton_Approach_to_Nonsmooth_Convex_Optimization_Problems_in_Machine_Learning.html">5 jmlr-2010-A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning</a></p>
<p>Author: Jin Yu, S.V.N. Vishwanathan, Simon Güunter, Nicol N. Schraudolph</p><p>Abstract: We extend the well-known BFGS quasi-Newton method and its memory-limited variant LBFGS to the optimization of nonsmooth convex objectives. This is done in a rigorous fashion by generalizing three components of BFGS to subdifferentials: the local quadratic model, the identiﬁcation of a descent direction, and the Wolfe line search conditions. We prove that under some technical conditions, the resulting subBFGS algorithm is globally convergent in objective function value. We apply its memory-limited variant (subLBFGS) to L2 -regularized risk minimization with the binary hinge loss. To extend our algorithm to the multiclass and multilabel settings, we develop a new, efﬁcient, exact line search algorithm. We prove its worst-case time complexity bounds, and show that our line search can also be used to extend a recently developed bundle method to the multiclass and multilabel settings. We also apply the direction-ﬁnding component of our algorithm to L1 -regularized risk minimization with logistic loss. In all these contexts our methods perform comparable to or better than specialized state-of-the-art solvers on a number of publicly available data sets. An open source implementation of our algorithms is freely available. Keywords: BFGS, variable metric methods, Wolfe conditions, subgradient, risk minimization, hinge loss, multiclass, multilabel, bundle methods, BMRM, OCAS, OWL-QN</p><p>5 0.032617796 <a title="34-tfidf-5" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>Author: Joshua W. Robinson, Alexander J. Hartemink</p><p>Abstract: Learning dynamic Bayesian network structures provides a principled mechanism for identifying conditional dependencies in time-series data. An important assumption of traditional DBN structure learning is that the data are generated by a stationary process, an assumption that is not true in many important settings. In this paper, we introduce a new class of graphical model called a nonstationary dynamic Bayesian network, in which the conditional dependence structure of the underlying data-generation process is permitted to change over time. Non-stationary dynamic Bayesian networks represent a new framework for studying problems in which the structure of a network is evolving over time. Some examples of evolving networks are transcriptional regulatory networks during an organism’s development, neural pathways during learning, and trafﬁc patterns during the day. We deﬁne the non-stationary DBN model, present an MCMC sampling algorithm for learning the structure of the model from time-series data under different assumptions, and demonstrate the effectiveness of the algorithm on both simulated and biological data. Keywords: Bayesian networks, graphical models, model selection, structure learning, Monte Carlo methods</p><p>6 0.025143243 <a title="34-tfidf-6" href="./jmlr-2010-Iterative_Scaling_and_Coordinate_Descent_Methods_for_Maximum_Entropy_Models.html">57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</a></p>
<p>7 0.023045674 <a title="34-tfidf-7" href="./jmlr-2010-On_the_Foundations_of_Noise-free_Selective_Classification.html">85 jmlr-2010-On the Foundations of Noise-free Selective Classification</a></p>
<p>8 0.02234173 <a title="34-tfidf-8" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>9 0.020973381 <a title="34-tfidf-9" href="./jmlr-2010-A_Generalized_Path_Integral_Control_Approach_to_Reinforcement_Learning.html">4 jmlr-2010-A Generalized Path Integral Control Approach to Reinforcement Learning</a></p>
<p>10 0.019634131 <a title="34-tfidf-10" href="./jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</a></p>
<p>11 0.018521484 <a title="34-tfidf-11" href="./jmlr-2010-Online_Learning_for_Matrix_Factorization_and_Sparse_Coding.html">87 jmlr-2010-Online Learning for Matrix Factorization and Sparse Coding</a></p>
<p>12 0.018474139 <a title="34-tfidf-12" href="./jmlr-2010-Linear_Algorithms_for_Online_Multitask_Classification.html">66 jmlr-2010-Linear Algorithms for Online Multitask Classification</a></p>
<p>13 0.013099832 <a title="34-tfidf-13" href="./jmlr-2010-On-Line_Sequential_Bin_Packing.html">80 jmlr-2010-On-Line Sequential Bin Packing</a></p>
<p>14 0.0123512 <a title="34-tfidf-14" href="./jmlr-2010-A_Comparison_of_Optimization_Methods_and_Software_for_Large-scale_L1-regularized_Linear_Classification.html">1 jmlr-2010-A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</a></p>
<p>15 0.012240659 <a title="34-tfidf-15" href="./jmlr-2010-Message-passing_for_Graph-structured_Linear_Programs%3A_Proximal_Methods_and_Rounding_Schemes.html">76 jmlr-2010-Message-passing for Graph-structured Linear Programs: Proximal Methods and Rounding Schemes</a></p>
<p>16 0.011946491 <a title="34-tfidf-16" href="./jmlr-2010-Generalized_Expectation_Criteria_for_Semi-Supervised_Learning_with_Weakly_Labeled_Data.html">42 jmlr-2010-Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data</a></p>
<p>17 0.011253837 <a title="34-tfidf-17" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>18 0.010596981 <a title="34-tfidf-18" href="./jmlr-2010-Consensus-Based_Distributed_Support_Vector_Machines.html">26 jmlr-2010-Consensus-Based Distributed Support Vector Machines</a></p>
<p>19 0.010355277 <a title="34-tfidf-19" href="./jmlr-2010-Estimation_of_a_Structural_Vector_Autoregression_Model_Using_Non-Gaussianity.html">36 jmlr-2010-Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity</a></p>
<p>20 0.0095396489 <a title="34-tfidf-20" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.063), (1, -0.053), (2, 0.082), (3, -0.026), (4, 0.23), (5, 0.017), (6, -0.009), (7, -0.022), (8, -0.019), (9, 0.009), (10, -0.069), (11, 0.031), (12, -0.056), (13, 0.012), (14, 0.007), (15, 0.005), (16, -0.063), (17, 0.089), (18, 0.019), (19, 0.036), (20, 0.031), (21, 0.059), (22, -0.023), (23, -0.074), (24, -0.037), (25, 0.004), (26, -0.017), (27, -0.027), (28, -0.001), (29, -0.066), (30, -0.073), (31, 0.009), (32, -0.066), (33, 0.004), (34, 0.013), (35, -0.04), (36, -0.026), (37, 0.036), (38, 0.125), (39, 0.023), (40, 0.098), (41, -0.076), (42, -0.049), (43, -0.199), (44, 0.04), (45, 0.062), (46, -0.089), (47, -0.054), (48, 0.464), (49, -0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96515048 <a title="34-lsi-1" href="./jmlr-2010-Erratum%3A_SGDQN_is_Less_Careful_than_Expected.html">34 jmlr-2010-Erratum: SGDQN is Less Careful than Expected</a></p>
<p>Author: Antoine Bordes, Léon Bottou, Patrick Gallinari, Jonathan Chang, S. Alex Smith</p><p>Abstract: The SGD-QN algorithm described in Bordes et al. (2009) contains a subtle ﬂaw that prevents it from reaching its design goals. Yet the ﬂawed SGD-QN algorithm has worked well enough to be a winner of the ﬁrst Pascal Large Scale Learning Challenge (Sonnenburg et al., 2008). This document clariﬁes the situation, proposes a corrected algorithm, and evaluates its performance. Keywords: stochastic gradient descent, support vector machine, conditional random ﬁelds</p><p>2 0.42112654 <a title="34-lsi-2" href="./jmlr-2010-Dual_Averaging_Methods_for_Regularized_Stochastic_Learning_and_Online_Optimization.html">31 jmlr-2010-Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a></p>
<p>Author: Lin Xiao</p><p>Abstract: We consider regularized stochastic learning and online optimization problems, where the objective function is the sum of two convex terms: one is the loss function of the learning task, and the other is a simple regularization term such as ℓ1 -norm for promoting sparsity. We develop extensions of Nesterov’s dual averaging method, that can exploit the regularization structure in an online setting. At each iteration of these methods, the learning variables are adjusted by solving a simple minimization problem that involves the running average of all past subgradients of the loss function and the whole regularization term, not just its subgradient. In the case of ℓ1 -regularization, our method is particularly effective in obtaining sparse solutions. We show that these methods achieve the optimal convergence rates or regret bounds that are standard in the literature on stochastic and online convex optimization. For stochastic learning problems in which the loss functions have Lipschitz continuous gradients, we also present an accelerated version of the dual averaging method. Keywords: stochastic learning, online optimization, ℓ1 -regularization, structural convex optimization, dual averaging methods, accelerated gradient methods</p><p>3 0.31791642 <a title="34-lsi-3" href="./jmlr-2010-Bundle_Methods_for_Regularized_Risk_Minimization.html">18 jmlr-2010-Bundle Methods for Regularized Risk Minimization</a></p>
<p>Author: Choon Hui Teo, S.V.N. Vishwanthan, Alex J. Smola, Quoc V. Le</p><p>Abstract: A wide variety of machine learning problems can be described as minimizing a regularized risk functional, with different algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Machines (SVMs), Gaussian Processes, Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper describes the theory and implementation of a scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as L1 and L2 penalties. In addition to the uniﬁed framework we present tight convergence bounds, which show that our algorithm converges in O(1/ε) steps to ε precision for general convex problems and in O(log(1/ε)) steps for continuously differentiable problems. We demonstrate the performance of our general purpose solver on a variety of publicly available data sets. Keywords: optimization, subgradient methods, cutting plane method, bundle methods, regularized risk minimization, parallel optimization ∗. Also at Canberra Research Laboratory, NICTA. c 2010 Choon Hui Teo, S.V. N. Vishwanthan, Alex J. Smola and Quoc V. Le. T EO , V ISHWANATHAN , S MOLA AND L E</p><p>4 0.27156997 <a title="34-lsi-4" href="./jmlr-2010-A_Quasi-Newton_Approach_to_Nonsmooth_Convex_Optimization_Problems_in_Machine_Learning.html">5 jmlr-2010-A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning</a></p>
<p>Author: Jin Yu, S.V.N. Vishwanathan, Simon Güunter, Nicol N. Schraudolph</p><p>Abstract: We extend the well-known BFGS quasi-Newton method and its memory-limited variant LBFGS to the optimization of nonsmooth convex objectives. This is done in a rigorous fashion by generalizing three components of BFGS to subdifferentials: the local quadratic model, the identiﬁcation of a descent direction, and the Wolfe line search conditions. We prove that under some technical conditions, the resulting subBFGS algorithm is globally convergent in objective function value. We apply its memory-limited variant (subLBFGS) to L2 -regularized risk minimization with the binary hinge loss. To extend our algorithm to the multiclass and multilabel settings, we develop a new, efﬁcient, exact line search algorithm. We prove its worst-case time complexity bounds, and show that our line search can also be used to extend a recently developed bundle method to the multiclass and multilabel settings. We also apply the direction-ﬁnding component of our algorithm to L1 -regularized risk minimization with logistic loss. In all these contexts our methods perform comparable to or better than specialized state-of-the-art solvers on a number of publicly available data sets. An open source implementation of our algorithms is freely available. Keywords: BFGS, variable metric methods, Wolfe conditions, subgradient, risk minimization, hinge loss, multiclass, multilabel, bundle methods, BMRM, OCAS, OWL-QN</p><p>5 0.24192986 <a title="34-lsi-5" href="./jmlr-2010-Learning_Non-Stationary_Dynamic_Bayesian_Networks.html">64 jmlr-2010-Learning Non-Stationary Dynamic Bayesian Networks</a></p>
<p>Author: Joshua W. Robinson, Alexander J. Hartemink</p><p>Abstract: Learning dynamic Bayesian network structures provides a principled mechanism for identifying conditional dependencies in time-series data. An important assumption of traditional DBN structure learning is that the data are generated by a stationary process, an assumption that is not true in many important settings. In this paper, we introduce a new class of graphical model called a nonstationary dynamic Bayesian network, in which the conditional dependence structure of the underlying data-generation process is permitted to change over time. Non-stationary dynamic Bayesian networks represent a new framework for studying problems in which the structure of a network is evolving over time. Some examples of evolving networks are transcriptional regulatory networks during an organism’s development, neural pathways during learning, and trafﬁc patterns during the day. We deﬁne the non-stationary DBN model, present an MCMC sampling algorithm for learning the structure of the model from time-series data under different assumptions, and demonstrate the effectiveness of the algorithm on both simulated and biological data. Keywords: Bayesian networks, graphical models, model selection, structure learning, Monte Carlo methods</p><p>6 0.22637473 <a title="34-lsi-6" href="./jmlr-2010-Iterative_Scaling_and_Coordinate_Descent_Methods_for_Maximum_Entropy_Models.html">57 jmlr-2010-Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models</a></p>
<p>7 0.20137188 <a title="34-lsi-7" href="./jmlr-2010-Linear_Algorithms_for_Online_Multitask_Classification.html">66 jmlr-2010-Linear Algorithms for Online Multitask Classification</a></p>
<p>8 0.15140659 <a title="34-lsi-8" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>9 0.14938578 <a title="34-lsi-9" href="./jmlr-2010-A_Generalized_Path_Integral_Control_Approach_to_Reinforcement_Learning.html">4 jmlr-2010-A Generalized Path Integral Control Approach to Reinforcement Learning</a></p>
<p>10 0.14280705 <a title="34-lsi-10" href="./jmlr-2010-Consensus-Based_Distributed_Support_Vector_Machines.html">26 jmlr-2010-Consensus-Based Distributed Support Vector Machines</a></p>
<p>11 0.12168524 <a title="34-lsi-11" href="./jmlr-2010-Matched_Gene_Selection_and_Committee_Classifier_for_Molecular_Classification_of_Heterogeneous_Diseases.html">71 jmlr-2010-Matched Gene Selection and Committee Classifier for Molecular Classification of Heterogeneous Diseases</a></p>
<p>12 0.12111112 <a title="34-lsi-12" href="./jmlr-2010-Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring.html">97 jmlr-2010-Regret Bounds and Minimax Policies under Partial Monitoring</a></p>
<p>13 0.12073279 <a title="34-lsi-13" href="./jmlr-2010-On-Line_Sequential_Bin_Packing.html">80 jmlr-2010-On-Line Sequential Bin Packing</a></p>
<p>14 0.11712565 <a title="34-lsi-14" href="./jmlr-2010-Tree_Decomposition_for_Large-Scale_SVM_Problems.html">113 jmlr-2010-Tree Decomposition for Large-Scale SVM Problems</a></p>
<p>15 0.11672068 <a title="34-lsi-15" href="./jmlr-2010-Expectation_Truncation_and_the_Benefits_of_Preselection_In_Training_Generative_Models.html">38 jmlr-2010-Expectation Truncation and the Benefits of Preselection In Training Generative Models</a></p>
<p>16 0.11531023 <a title="34-lsi-16" href="./jmlr-2010-Classification_with_Incomplete_Data_Using_Dirichlet_Process_Priors.html">23 jmlr-2010-Classification with Incomplete Data Using Dirichlet Process Priors</a></p>
<p>17 0.11521488 <a title="34-lsi-17" href="./jmlr-2010-Bayesian_Learning_in_Sparse_Graphical_Factor_Models_via_Variational_Mean-Field_Annealing.html">17 jmlr-2010-Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing</a></p>
<p>18 0.10868459 <a title="34-lsi-18" href="./jmlr-2010-Using_Contextual_Representations_to_Efficiently_Learn_Context-Free_Languages.html">115 jmlr-2010-Using Contextual Representations to Efficiently Learn Context-Free Languages</a></p>
<p>19 0.10818648 <a title="34-lsi-19" href="./jmlr-2010-An_Exponential_Model_for_Infinite_Rankings.html">10 jmlr-2010-An Exponential Model for Infinite Rankings</a></p>
<p>20 0.10420889 <a title="34-lsi-20" href="./jmlr-2010-A_Surrogate_Modeling_and_Adaptive_Sampling_Toolbox_for_Computer_Based_Design.html">8 jmlr-2010-A Surrogate Modeling and Adaptive Sampling Toolbox for Computer Based Design</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.013), (4, 0.016), (12, 0.45), (15, 0.024), (21, 0.027), (22, 0.013), (24, 0.011), (32, 0.027), (36, 0.034), (37, 0.049), (75, 0.093), (81, 0.012), (85, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69811803 <a title="34-lda-1" href="./jmlr-2010-Erratum%3A_SGDQN_is_Less_Careful_than_Expected.html">34 jmlr-2010-Erratum: SGDQN is Less Careful than Expected</a></p>
<p>Author: Antoine Bordes, Léon Bottou, Patrick Gallinari, Jonathan Chang, S. Alex Smith</p><p>Abstract: The SGD-QN algorithm described in Bordes et al. (2009) contains a subtle ﬂaw that prevents it from reaching its design goals. Yet the ﬂawed SGD-QN algorithm has worked well enough to be a winner of the ﬁrst Pascal Large Scale Learning Challenge (Sonnenburg et al., 2008). This document clariﬁes the situation, proposes a corrected algorithm, and evaluates its performance. Keywords: stochastic gradient descent, support vector machine, conditional random ﬁelds</p><p>2 0.62291372 <a title="34-lda-2" href="./jmlr-2010-Optimal_Search_on_Clustered_Structural_Constraint_for_Learning_Bayesian_Network_Structure.html">88 jmlr-2010-Optimal Search on Clustered Structural Constraint for Learning Bayesian Network Structure</a></p>
<p>Author: Kaname Kojima, Eric Perrier, Seiya Imoto, Satoru Miyano</p><p>Abstract: We study the problem of learning an optimal Bayesian network in a constrained search space; skeletons are compelled to be subgraphs of a given undirected graph called the super-structure. The previously derived constrained optimal search (COS) remains limited even for sparse superstructures. To extend its feasibility, we propose to divide the super-structure into several clusters and perform an optimal search on each of them. Further, to ensure acyclicity, we introduce the concept of ancestral constraints (ACs) and derive an optimal algorithm satisfying a given set of ACs. Finally, we theoretically derive the necessary and sufﬁcient sets of ACs to be considered for ﬁnding an optimal constrained graph. Empirical evaluations demonstrate that our algorithm can learn optimal Bayesian networks for some graphs containing several hundreds of vertices, and even for super-structures having a high average degree (up to four), which is a drastic improvement in feasibility over the previous optimal algorithm. Learnt networks are shown to largely outperform state-of-the-art heuristic algorithms both in terms of score and structural hamming distance. Keywords: Bayesian networks, structure learning, constrained optimal search</p><p>3 0.26881364 <a title="34-lda-3" href="./jmlr-2010-Sparse_Semi-supervised_Learning_Using_Conjugate_Functions.html">103 jmlr-2010-Sparse Semi-supervised Learning Using Conjugate Functions</a></p>
<p>Author: Shiliang Sun, John Shawe-Taylor</p><p>Abstract: In this paper, we propose a general framework for sparse semi-supervised learning, which concerns using a small portion of unlabeled data and a few labeled data to represent target functions and thus has the merit of accelerating function evaluations when predicting the output of a new example. This framework makes use of Fenchel-Legendre conjugates to rewrite a convex insensitive loss involving a regularization with unlabeled data, and is applicable to a family of semi-supervised learning methods such as multi-view co-regularized least squares and single-view Laplacian support vector machines (SVMs). As an instantiation of this framework, we propose sparse multi-view SVMs which use a squared ε-insensitive loss. The resultant optimization is an inf-sup problem and the optimal solutions have arguably saddle-point properties. We present a globally optimal iterative algorithm to optimize the problem. We give the margin bound on the generalization error of the sparse multi-view SVMs, and derive the empirical Rademacher complexity for the induced function class. Experiments on artiﬁcial and real-world data show their effectiveness. We further give a sequential training approach to show their possibility and potential for uses in large-scale problems and provide encouraging experimental results indicating the efﬁcacy of the margin bound and empirical Rademacher complexity on characterizing the roles of unlabeled data for semi-supervised learning. Keywords: semi-supervised learning, Fenchel-Legendre conjugate, representer theorem, multiview regularization, support vector machine, statistical learning theory</p><p>4 0.26459208 <a title="34-lda-4" href="./jmlr-2010-Unsupervised_Supervised_Learning_I%3A_Estimating_Classification_and_Regression_Errors_without_Labels.html">114 jmlr-2010-Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels</a></p>
<p>Author: Pinar Donmez, Guy Lebanon, Krishnakumar Balasubramanian</p><p>Abstract: Estimating the error rates of classiﬁers or regression models is a fundamental task in machine learning which has thus far been studied exclusively using supervised learning techniques. We propose a novel unsupervised framework for estimating these error rates using only unlabeled data and mild assumptions. We prove consistency results for the framework and demonstrate its practical applicability on both synthetic and real world data. Keywords: classiﬁcation and regression, maximum likelihood, latent variable models</p><p>5 0.26380208 <a title="34-lda-5" href="./jmlr-2010-Maximum_Relative_Margin_and_Data-Dependent_Regularization.html">74 jmlr-2010-Maximum Relative Margin and Data-Dependent Regularization</a></p>
<p>Author: Pannagadatta K. Shivaswamy, Tony Jebara</p><p>Abstract: Leading classiﬁcation methods such as support vector machines (SVMs) and their counterparts achieve strong generalization performance by maximizing the margin of separation between data classes. While the maximum margin approach has achieved promising performance, this article identiﬁes its sensitivity to afﬁne transformations of the data and to directions with large data spread. Maximum margin solutions may be misled by the spread of data and preferentially separate classes along large spread directions. This article corrects these weaknesses by measuring margin not in the absolute sense but rather only relative to the spread of data in any projection direction. Maximum relative margin corresponds to a data-dependent regularization on the classiﬁcation function while maximum absolute margin corresponds to an ℓ2 norm constraint on the classiﬁcation function. Interestingly, the proposed improvements only require simple extensions to existing maximum margin formulations and preserve the computational efﬁciency of SVMs. Through the maximization of relative margin, surprising performance gains are achieved on real-world problems such as digit, text classiﬁcation and on several other benchmark data sets. In addition, risk bounds are derived for the new formulation based on Rademacher averages. Keywords: support vector machines, kernel methods, large margin, Rademacher complexity</p><p>6 0.26369599 <a title="34-lda-6" href="./jmlr-2010-PAC-Bayesian_Analysis_of_Co-clustering_and_Beyond.html">89 jmlr-2010-PAC-Bayesian Analysis of Co-clustering and Beyond</a></p>
<p>7 0.26293814 <a title="34-lda-7" href="./jmlr-2010-Large_Scale_Online_Learning_of_Image_Similarity_Through_Ranking.html">59 jmlr-2010-Large Scale Online Learning of Image Similarity Through Ranking</a></p>
<p>8 0.26255754 <a title="34-lda-8" href="./jmlr-2010-Hubs_in_Space%3A_Popular_Nearest_Neighbors_in_High-Dimensional_Data.html">49 jmlr-2010-Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data</a></p>
<p>9 0.26241669 <a title="34-lda-9" href="./jmlr-2010-High_Dimensional_Inverse_Covariance_Matrix_Estimation_via_Linear_Programming.html">46 jmlr-2010-High Dimensional Inverse Covariance Matrix Estimation via Linear Programming</a></p>
<p>10 0.26139414 <a title="34-lda-10" href="./jmlr-2010-Training_and_Testing_Low-degree_Polynomial_Data_Mappings_via_Linear_SVM.html">112 jmlr-2010-Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</a></p>
<p>11 0.26138923 <a title="34-lda-11" href="./jmlr-2010-Dual_Averaging_Methods_for_Regularized_Stochastic_Learning_and_Online_Optimization.html">31 jmlr-2010-Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a></p>
<p>12 0.26130027 <a title="34-lda-12" href="./jmlr-2010-Bundle_Methods_for_Regularized_Risk_Minimization.html">18 jmlr-2010-Bundle Methods for Regularized Risk Minimization</a></p>
<p>13 0.26093882 <a title="34-lda-13" href="./jmlr-2010-Learning_Instance-Specific_Predictive_Models.html">63 jmlr-2010-Learning Instance-Specific Predictive Models</a></p>
<p>14 0.260456 <a title="34-lda-14" href="./jmlr-2010-An_Efficient_Explanation_of_Individual_Classifications_using_Game_Theory.html">9 jmlr-2010-An Efficient Explanation of Individual Classifications using Game Theory</a></p>
<p>15 0.25942454 <a title="34-lda-15" href="./jmlr-2010-Semi-Supervised_Novelty_Detection.html">102 jmlr-2010-Semi-Supervised Novelty Detection</a></p>
<p>16 0.25879729 <a title="34-lda-16" href="./jmlr-2010-Linear_Algorithms_for_Online_Multitask_Classification.html">66 jmlr-2010-Linear Algorithms for Online Multitask Classification</a></p>
<p>17 0.25842008 <a title="34-lda-17" href="./jmlr-2010-Sparse_Spectrum_Gaussian_Process_Regression.html">104 jmlr-2010-Sparse Spectrum Gaussian Process Regression</a></p>
<p>18 0.25840229 <a title="34-lda-18" href="./jmlr-2010-Practical_Approaches_to_Principal_Component_Analysis_in_the_Presence_of_Missing_Values.html">92 jmlr-2010-Practical Approaches to Principal Component Analysis in the Presence of Missing Values</a></p>
<p>19 0.25730556 <a title="34-lda-19" href="./jmlr-2010-Stochastic_Composite_Likelihood.html">109 jmlr-2010-Stochastic Composite Likelihood</a></p>
<p>20 0.2566953 <a title="34-lda-20" href="./jmlr-2010-A_Quasi-Newton_Approach_to_Nonsmooth_Convex_Optimization_Problems_in_Machine_Learning.html">5 jmlr-2010-A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
