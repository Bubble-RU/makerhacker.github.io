<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-4" href="../jmlr2005/jmlr-2005-A_Framework_for_Learning_Predictive_Structures_from_Multiple_Tasks_and_Unlabeled_Data.html">jmlr2005-4</a> <a title="jmlr-2005-4-reference" href="#">jmlr2005-4-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>4 jmlr-2005-A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</h1>
<br/><p>Source: <a title="jmlr-2005-4-pdf" href="http://jmlr.org/papers/volume6/ando05a/ando05a.pdf">pdf</a></p><p>Author: Rie Kubota Ando, Tong Zhang</p><p>Abstract: One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don’t have a complete understanding of their eﬀectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Speciﬁcally we consider learning predictive structures on hypothesis spaces (that is, what kind of classiﬁers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the eﬀectiveness of the proposed algorithms in the semi-supervised learning setting.</p><br/>
<h2>reference text</h2><p>R. K. Ando and T. Zhang. A high-performance semi-supervised learning method for text chunking. In ACL 05, 2005. J. Baxter. A model for inductive bias learning. Journal of Artiﬁcial Intelligent Research, pages 149–198, 2000. M. Belkin and P. Niyogi. Semi-supervised learning on Riemannian manifolds. Machine Learning, Special Issue on Clustering:209–239, 2004. S. Ben-David and R. Schuller. Exploiting task relatedness for multiple task learning. In COLT 03, 2003. A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In proceedings of the eleventh annual conference on Computational learning theory, pages 92–100, 1998. L. Breiman and J. Friedman. Predicting multivariate responses in multiple linear regression. J. Roy. Statist. Soc. B., 59:3–37, 1997. with discussion. R. Caruana. Multi-task learning. Machine Learning, pages 41–75, 1997. H. L. Chieu and H. T. Ng. Named entity recognition with a maximum entropy approach. In Proceedings CoNLL-2003, pages 160–163, 2003. T. Evegniou and M. Pontil. Regularized multi–task learning. In Proc. Conf. on Knowledge Discovery and Data Mining, 2004. R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. Named entity recogintion through classiﬁer combination. In Proceedings CoNLL-2003, pages 168–171, 2003. T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer, 2001. T. Joachims. Transductive inference for text classiﬁcation using support vector machines. In Proceedings of the Sixteenth International Conference on Machine Learning, pages 200–209, 1999. 1852  Learning Predictive Structures  D. Klein, J. Smarr, H. Nguyen, and C. D. Manning. Named entity recognition with character-level models. In Proceedings CoNLL-2003, pages 188–191, 2003. M. Ledoux and M. Talagrand. Probability in Banach spaces. Springer-Verlag, Berlin, 1991. ISBN 3-540-52013-9. Isoperimetry and processes. C. McDiarmid. On the method of bounded diﬀerences. In Surveys in Combinatorics, pages 148–188. Cambridge University Press, 1989. C. A. Micchelli and M. Ponti. Kernels for multi–task learning. In NIPS 2004, 2005. to appear. K. Nigam, A. K. McCallum, S. Thrun, and T. Mitchell. Text classiﬁcation from labeled and unlabeled documents using EM. Machine Learning, Special issue on information retrieval:103–134, 2000. D. Pierce and C. Cardie. Limitations of co-training for natural language learning from large datasets. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP-2001), 2001. M. Szummer and T. Jaakkola. Partially labeled classiﬁcation with Markov random walks. In NIPS 2001, 2002. A. W. van der Vaart and J. A. Wellner. Weak convergence and empirical processes. Springer Series in Statistics. Springer-Verlag, New York, 1996. ISBN 0-387-94640-3. V. N. Vapnik. Statistical learning theory. John Wiley & Sons, New York, 1998. D. Yarowsky. unsupervised word sense disambiguation rivaling supervised methods. In proceedings of ACL 95, pages 189–196, 1995. T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In ICML 04, pages 919–926, 2004. T. Zhang and D. E. Johnson. A robust risk minimization based named entity recognition system. In Proceedings CoNLL-2003, pages 204–207, 2003. T. Zhang and F. J. Oles. A probability analysis on the value of unlabeled data for classiﬁcation problems. In ICML 00, pages 1191–1198, 2000. D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Schlkopf. Learning with local and global consistency. In NIPS 2003, pages 321–328, 2004. X. Zhu, Z. Ghahramani, and J. Laﬀerty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In ICML 2003, 2003.  1853</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
