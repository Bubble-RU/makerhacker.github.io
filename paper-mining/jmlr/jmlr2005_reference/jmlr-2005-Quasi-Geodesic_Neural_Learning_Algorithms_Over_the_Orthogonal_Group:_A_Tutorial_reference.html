<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-63" href="../jmlr2005/jmlr-2005-Quasi-Geodesic_Neural_Learning_Algorithms_Over_the_Orthogonal_Group%3A_A_Tutorial.html">jmlr2005-63</a> <a title="jmlr-2005-63-reference" href="#">jmlr2005-63-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>63 jmlr-2005-Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial</h1>
<br/><p>Source: <a title="jmlr-2005-63-pdf" href="http://jmlr.org/papers/volume6/fiori05a/fiori05a.pdf">pdf</a></p><p>Author: Simone Fiori</p><p>Abstract: The aim of this contribution is to present a tutorial on learning algorithms for a single neural layer whose connection matrix belongs to the orthogonal group. The algorithms exploit geodesics appropriately connected as piece-wise approximate integrals of the exact differential learning equation. The considered learning equations essentially arise from the Riemannian-gradient-based optimization theory with deterministic and diffusion-type gradient. The paper aims speciﬁcally at reviewing the relevant mathematics (and at presenting it in as much transparent way as possible in order to make it accessible to readers that do not possess a background in differential geometry), at bringing together modern optimization methods on manifolds and at comparing the different algorithms on a common machine learning problem. As a numerical case-study, we consider an application to non-negative independent component analysis, although it should be recognized that Riemannian gradient methods give rise to general-purpose algorithms, by no means limited to ICA-related applications. Keywords: differential geometry, diffusion-type gradient, Lie groups, non-negative independent component analysis, Riemannian gradient</p><br/>
<h2>reference text</h2><p>S.-i. Amari. Differential-Geometrical Methods in Statistics. Lecture Notes in Statistics 28, SpringerVerlag, 1989. S.-i. Amari. Natural gradient works efﬁciently in learning. Neural Computation, 10:251–276, 1998. T. Akuzawa. New fast factorization method for multivariate optimization and its realization as ICA algorithm. In Proceedings of the 3rd International Conference on Independent Component Analysis and Blind Signal Separation pages 114–119, San Diego, California, USA, 2001 E. Celledoni and S. Fiori. Neural learning by geometric integration of reduced ‘rigid-body’ equations. Journal of Computational and Applied Mathematics, 172(2):247–269, 2004. A. Cichocki and S.-i. Amari. Adaptive Blind Signal and Image Processing, J. Wiley & Sons, 2002 S. Fiori. A theory for learning by weight ﬂow on Stiefel-Grassman manifold. Neural Computation, 13(7):1625–1647, 2001. S. Fiori. A theory for learning based on rigid bodies dynamics. IEEE Trans. on Neural Networks, 13(3):521–531, 2002. S. Fiori. A fast ﬁxed-point neural blind deconvolution algorithm. IEEE Trans. on Neural Networks, 15(2):455–459, 2004. S. Fiori. Non-linear complex-valued extensions of Hebbian learning: An essay. Neural Computation, 17(4):779–838, 2005. S. Fiori. Formulation and integration of learning differential equations on the Stiefel manifold. IEEE Trans. on Neural Networks, forthcoming. S. Fiori and S.-i. Amari. Editorial: Special issue on “Geometrical Methods in Neural Networks and Learning”, Neurocomputing, forthcoming. W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57:97–109, 1970. A. Hyv¨ rinen, J. Karhunen and E. Oja. Independent Component Analysis, John Wiley & Sons, 2001. a D. J. Higham. An algorithmic introduction to numerical simulation of stochastic differential equations. SIAM Review, 43(3):525–546, 2001. 780  Q UASI -G EODESIC N EURAL L EARNING A LGORITHMS OVER THE O RTHOGONAL G ROUP  R. E. Kass, B. P. Carlin, A. Gelman and R. M. Neal. Markov Chain Monte Carlo in practice: A roundtable discussion. The American Statistician, 52(2):93–100, 1998. N. Keshava and J. F. Mustard. Spectral unmixing. IEEE Signal Processing Magazine, 19(1):44–57, 2002. X. Liu, A. Srivastava and K. Gallivan. Optimal linear representation of images for object recognition. IEEE Trans. on Pattern Analysis and Machine Intelligence, 26(5):662–666, 2004. N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller and E. Teller. Equations of state calculations by fast computing machines. Journal of Chemical Physics, 21:1087–1091, 1953. Y. Nishimori. Learning algorithm for ICA by geodesic ﬂows on orthogonal group. In Proc. of the International Joint Conference on Neural Networks pages 1625–1647, 1999. P. J. Olver. Applications of Lie groups to differential equations. Graduate Texts in Mathematics 107, Second Edition, Springer, 2003. H. Park, S.-i. Amari and K. Fukumizu. Adaptive Natural Gradient Learning Algorithms for Various Stochastic Models. Neural Networks, 13:755–764, 2000. M. D. Plumbley. Conditions for non-negative independent component analysis. IEEE Signal processing Letters, 9(6):177–180, 2002. M. D. Plumbley. Algorithms for nonnegative independent component analysis. IEEE Trans. on Neural Networks, 14(3):534–543, 2003. M. D. Plumbley. Lie group methods for optimization with orthogonality constraints. In Proceedings of the International Conference on Independent Component Analysis and Blind Signal Separation, pages 1245–1252, Granada, Spain, 2004. A. Srivastava, U. Grenander, G. R. Jensen and M. I. Miller. Jump-diffusion Markov processes on orthogonal groups for object recognition. Journal of Statistical Planning and Inference, 103(1/2):15–37, 2002. H. H. Yang and S.-i. Amari. Adaptive online learning algorithms for blind separation: Maximum entropy and minimum mutual information. Neural Computation, 9:1457–1482, 1997. G. R. Warnes. The normal kernel coupler: An adaptive MCMC method for efﬁciently sampling from multi-modal distributions. Technical Report 39, Dept. of Statistics, University of Washington, 2001. D. R. Wilson and T. R. Martinez. The general inefﬁciency of batch training for gradient descent learning. Neural Networks, 16(10):1429–1451, 2003. L.-Q. Zhang, A. Cichocki and S.-i. Amari. Geometrical structures of FIR manifold and multichannel blind deconvolution. Journal of VLSI for Signal Processing Systems, 31:31–44, 2002.  781</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
