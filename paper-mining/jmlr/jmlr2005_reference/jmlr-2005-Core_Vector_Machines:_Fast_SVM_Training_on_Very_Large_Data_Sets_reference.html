<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-24" href="../jmlr2005/jmlr-2005-Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets.html">jmlr2005-24</a> <a title="jmlr-2005-24-reference" href="#">jmlr2005-24-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>24 jmlr-2005-Core Vector Machines: Fast SVM Training on Very Large Data Sets</h1>
<br/><p>Source: <a title="jmlr-2005-24-pdf" href="http://jmlr.org/papers/volume6/tsang05a/tsang05a.pdf">pdf</a></p><p>Author: Ivor W. Tsang, James T. Kwok, Pak-Ming Cheung</p><p>Abstract: O(m3 ) Standard SVM training has time and O(m2 ) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such “approximateness” in this paper. We ﬁrst show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efﬁcient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and realworld data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about ﬁve million training patterns, in only 1.4 seconds on a 3.2GHz Pentium–4 PC. Keywords: kernel methods, approximation algorithm, minimum enclosing ball, core set, scalability</p><br/>
<h2>reference text</h2><p>D. Achlioptas, F. McSherry, and B. Sch¨ lkopf. Sampling techniques for kernel methods. In T. G. o Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, Cambridge, MA, 2002. MIT Press. 388  C ORE V ECTOR M ACHINES  G. H. Bakir, J. Weston, and L. Bottou. Breaking SVM complexity with cross-training. In Advances in Neural Information Processing Systems 17, Cambridge, MA, 2005. MIT Press. D. Boley and D. Cao. Training support vector machine using adaptive clustering. In Proceedings of the SIAM International Conference on Data Mining, pages 126–137, Lake Buena Vista, FL, USA, April 2004. A. P. Bradley. The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern Recognition, 30(7):1145–1159, 1997. M. B˘ doiu and K. L. Clarkson. Optimal core sets for balls. In DIMACS Workshop on Computational a Geometry, 2002. M. B˘ doiu, S. Har-Peled, and P. Indyk. Approximate clustering via core sets. In Proceedings of 34th a Annual ACM Symposium on Theory of Computing, pages 250–257, Montreal, Quebec, Canada, ´ ´ 2002. G. Cauwenberghs and T. Poggio. Incremental and decremental support vector machine learning. In T. Leen, T. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, Cambridge, MA, 2001. MIT Press. T. M. Chan. Approximating the diameter, width, smallest enclosing cylinder, and minimum-width annulus. In Proceedings of the Sixteenth Annual Symposium on Computational Geometry, pages 300–309, Clear Water Bay, Hong Kong, 2000. C.-C. Chang and C.-J. Lin. LIBSVM: a Library for Support Vector Machines, 2004. Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm. O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector machines. Machine Learning, 46(1-3):131–159, 2002. C. S. Chu, I. W. Tsang, and J. T. Kwok. Scaling up support vector data description by using coresets. In Proceedings of the International Joint Conference on Neural Networks, pages 425–430, Budapest, Hungary, July 2004. R. Collobert, S. Bengio, and Y. Bengio. A parallel mixture of SVMs for very large scale problems. Neural Computation, 14(5):1105–1114, May 2002. S. Fine and K. Scheinberg. Efﬁcient SVM training using low-rank kernel representations. Journal of Machine Learning Research, 2:243–264, December 2001. T. Friess, N. Cristianini, and C. Campbell. The Kernel-Adatron algorithm: a fast and simple learning procedure for support vector machines. In Proceedings of the Fifteenth International Conference on Machine Learning, pages 188–196, Madison, Wisconsin, USA, July 1998. G. Fung and O. L. Mangasarian. Incremental support vector machine classiﬁcation. In R. Grossman, H. Mannila, and R. Motwani, editors, Proceedings of the Second SIAM International Conference on Data Mining, pages 247–260, Arlington, Virginia, USA, 2002. G. Fung and O. L. Mangasarian. Finite Newton method for Lagrangian support vector machine classiﬁcation. Neurocomputing, 55:39–55, 2003. 389  T SANG , K WOK AND C HEUNG  M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NPCompleteness. W. H. Freeman, 1979. S. Har-Peled and Y. Wang. Shape ﬁtting with outliers. SIAM Journal on Computing, 33(2):269–285, 2004. B. Heisele, T. Poggio, and M. Pontil. Face detection in still gray images. A.I. memo 1687, Center for Biological and Computational Learning, MIT, Cambridge, MA, 2000. T. Joachims. Making large-scale support vector machine learning practical. In B. Sch¨ lkopf, o C. Burges, and A. Smola, editors, Advances in Kernel Methods – Support Vector Learning, pages 169–184. MIT Press, Cambridge, MA, 1999. W.-C. Kao, K.-M. Chung, C.-L. Sun, and C.-J. Lin. Decomposition methods for linear support vector machines. Neural Computation, 16:1689–1704, 2004. S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and K. R. K. Murthy. A fast iterative nearest point algorithm for support vector machine classiﬁer design. IEEE Transactions on Neural Networks, 11(1):124–136, January 2000. P. Kumar, J. S. B. Mitchell, and A. Yildirim. Approximate minimum enclosing balls in high dimensions using core-sets. ACM Journal of Experimental Algorithmics, 8, January 2003. Y.-J. Lee and O. L. Mangasarian. RSVM: Reduced support vector machines. In Proceeding of the First SIAM International Conference on Data Mining, 2001. O. L. Mangasarian and D. R. Musicant. Active set support vector machine classiﬁcation. In T. Leen, T. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 577–583, Cambridge, MA, 2001a. MIT Press. O. L. Mangasarian and D. R. Musicant. Lagrangian support vector machines. Journal of Machine Learning Research, 1:161–177, 2001b. N. Megiddo. Linear-time algorithms for linear programming in R3 and related problems. SIAM Journal on Computing, 12:759–776, 1983. F. Nielsen and R. Nock. Approximating smallest enclosing balls. In Proceedings of International Conference on Computational Science and Its Applications, volume 3045, pages 147–157, 2004. E. Osuna, R. Freund, and F. Girosi. An improved training algorithm for support vector machines. In Proceedings of the IEEE Workshop on Neural Networks for Signal Processing, pages 276–285, Amelia Island, FL, USA, 1997a. E. Osuna, R. Freund, and F. Girosi. Training support vector machines: an application to face detection. In Proceedings of Computer Vision and Pattern Recognition, pages 130–136, San Juan, Puerto Rico, June 1997b. D. Pavlov, D. Chudova, and P. Smyth. Towards scalable support vector machines using squashing. In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 295–299, Boston, Massachusetts, USA, 2000a. 390  C ORE V ECTOR M ACHINES  D. Pavlov, J. Mao, and B. Dom. Scaling-up support vector machines using boosting algorithm. In Proceedings of the International Conference on Pattern Recognition, volume 2, pages 2219– 2222, Barcelona, Spain, September 2000b. J. C. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Sch¨ lkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods – Support Vector o Learning, pages 185–208. MIT Press, Cambridge, MA, 1999. F. P. Preparata. Computational Geometry: An Introduction. Springer-Verlag, 1985. D. Roobaert. DirectSVM: a simple support vector machine perceptron. In Proceedings of IEEE International Workshop on Neural Networks for Signal Processing, pages 356–365, Sydney, Australia, December 2000. G. Schohn and D. Cohn. Less is more: Active learning with support vector machines. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 839–846, San Francisco, CA, USA, 2000. Morgan Kaufmann. B. Sch¨ lkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Estimating the o support of a high-dimensional distribution. Neural Computation, 13(7):1443–1471, July 2001. B. Sch¨ lkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002. o A. Schwaighofer and V. Tresp. The Bayesian committee support vector machine. In G. Dorffner, H. Bischof, and K. Hornik, editors, Proceedings of the International Conference on Artiﬁcial Neural Networks, pages 411–417. Springer Verlag, 2001. A. Smola and B. Sch¨ lkopf. Sparse greedy matrix approximation for machine learning. In Proceedo ings of the Seventeenth International Conference on Machine Learning, pages 911–918, Stanford, CA, USA, June 2000. A. Smola and B. Sch¨ lkopf. A tutorial on support vector regression. Statistics and Computing, 14 o (3):199–222, August 2004. K.-K. Sung. Learning and Example Selection for Object and Pattern Recognition. PhD thesis, Artiﬁcial Intelligence Laboratory and Center for Biological and Computational Learning, MIT, Cambridge, MA, 1996. J. J. Sylvester. A question in the geometry of situation. Quarterly Journal on Mathematics, 1:79, 1857. D. M. J. Tax and R. P. W. Duin. Support vector domain description. Pattern Recognition Letters, 20 (14):1191–1199, 1999. S. Tong and D. Koller. Support vector machine active learning with applications to text classiﬁcation. In Proceedings of the 17th International Conference on Machine Learning, pages 999–1006, San Francisco, CA, USA, 2000. Morgan Kaufmann. V. Tresp. Scaling kernel-based systems to large data sets. Data Mining and Knowledge Discovery, 5(3):197–211, 2001. 391  T SANG , K WOK AND C HEUNG  I. W. Tsang, J. T. Kwok, and P.-M. Cheung. Very large SVM training using core vector machines. In Proceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics, Barbados, January 2005. V. Vapnik. Statistical Learning Theory. Wiley, New York, 1998. V. V. Vazirani. Approximation Algorithms. Springer, 2001. P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. In Proceedings of the International Conference on Computer Vision and Pattern Recognition, volume 1, pages 1063–6919, 2001. S. V. N. Vishwanathan, A. J. Smola, and M. N. Murty. SimpleSVM. In Proceedings of the Twentieth International Conference on Machine Learning, pages 760–767, Washington, D.C., USA, August 2003. E. Welzl. Smallest enclosing disks (balls and ellipsoids). In H. Maurer, editor, New Results and New Trends in Computer Science, pages 359–370. Springer-Verlag, 1991. J. Weston, B. Sch¨ lkopf, E. Eskin, C. Leslie, and S. W. Noble. Dealing with large diagonals in o kernel matrices. Principles of Data Mining and Knowledge Discovery, Springer Lecture Notes in Computer Science 243, 2002. C. K. I. Williams and M. Seeger. Using the Nystr¨ m method to speed up kernel machines. In o T. Leen, T. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, Cambridge, MA, 2001. MIT Press. C. Yang, R. Duraiswami, and L. Davis. Efﬁcient kernel machines using the improved fast Gauss transform. In Advances in Neural Information Processing Systems 17, Cambridge, MA, 2005. MIT Press. H. Yu, J. Yang, and J. Han. Classifying large data sets using SVM with hierarchical clusters. In Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 306–315, Washington DC, USA, 2003. T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH: An efﬁcient data clustering method for very large databases. In H. V. Jagadish and I. S. Mumick, editors, Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data, pages 103–114, Montreal, Quebec, Canada, June 1996. ACM Press.  392</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
