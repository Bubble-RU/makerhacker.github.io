<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-31" href="../jmlr2005/jmlr-2005-Estimation_of_Non-Normalized_Statistical_Models_by_Score_Matching.html">jmlr2005-31</a> <a title="jmlr-2005-31-reference" href="#">jmlr2005-31-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>31 jmlr-2005-Estimation of Non-Normalized Statistical Models by Score Matching</h1>
<br/><p>Source: <a title="jmlr-2005-31-pdf" href="http://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">pdf</a></p><p>Author: Aapo Hyvärinen</p><p>Abstract: One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data. Keywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov chain Monte Carlo, contrastive divergence</p><br/>
<h2>reference text</h2><p>S.-I. Amari, A. Cichocki, and H. H. Yang. A new learning algorithm for blind source separation. In Advances in Neural Information Processing Systems 8, pages 757–763. MIT Press, 1996. J. Besag. Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical Society, Series B, 36(2):192–236, 1974. J. Besag. Eﬃciency of pseudolikelihood estimation for simple gaussian ﬁelds. Biometrika, 64(3):616–618, 1977. C. Bouman and K. Sauer. A generalized gaussian image model for edge-preserving MAP estimation. IEEE Transactions on Image Processing, 2(3):296–310, 1993. ´ M. A. Carreira-Perpi˜an and G. E. Hinton. On contrastive divergence (CD) learning. n´ Technical report, Dept of Computer Science, University of Toronto, 2005a. In preparation. ´ M. A. Carreira-Perpi˜an and G. E. Hinton. On contrastive divergence learning. In Proceedn´ ings of the Workshop on Artiﬁcial Intelligence and Statistics (AISTATS2005), Barbados, 2005b. G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–1800, 2002. A. Hyv¨rinen and P. O. Hoyer. A two-layer sparse coding model learns simple and complex a cell receptive ﬁelds and topography from natural images. Vision Research, 41(18):2413– 2423, 2001. A. Hyv¨rinen and M. Inki. Estimating overcomplete independent component bases from a image windows. Journal of Mathematical Imaging and Vision, 17:139–152, 2002. 708  Estimation by Score Matching  A. Hyv¨rinen, J. Karhunen, and E. Oja. Independent Component Analysis. Wiley Intera science, 2001. S. Z. Li. Markov Random Field Modeling in Image Analysis. Springer, 2nd edition, 2001. D. J. C. Mackay. Information Theory, Inference and Learning Algorithms. Cambridge University Press, 2003. B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research, 37:3311–3325, 1997. D.-T. Pham and P. Garrat. Blind separation of mixture of independent sources through a quasi-maximum likelihood approach. IEEE Transactions on Signal Processing, 45(7): 1712–1725, 1997. M. Schervish. Theory of Statistics. Springer, 1995. Y. W. Teh, M. Welling, S. Osindero, and G. E. Hinton. Energy-based models for sparse overcomplete representations. Journal of Machine Learning Research, 4:1235–1260, 2003.  709</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
