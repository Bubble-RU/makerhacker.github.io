<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-43" href="../jmlr2005/jmlr-2005-Learning_Hidden_Variable_Networks%3A_The_Information_Bottleneck_Approach.html">jmlr2005-43</a> <a title="jmlr-2005-43-reference" href="#">jmlr2005-43-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>43 jmlr-2005-Learning Hidden Variable Networks: The Information Bottleneck Approach</h1>
<br/><p>Source: <a title="jmlr-2005-43-pdf" href="http://jmlr.org/papers/volume6/elidan05a/elidan05a.pdf">pdf</a></p><p>Author: Gal Elidan, Nir Friedman</p><p>Abstract: A central challenge in learning probabilistic graphical models is dealing with domains that involve hidden variables. The common approach for learning model parameters in such domains is the expectation maximization (EM) algorithm. This algorithm, however, can easily get trapped in suboptimal local maxima. Learning the model structure is even more challenging. The structural EM algorithm can adapt the structure in the presence of hidden variables, but usually performs poorly without prior knowledge about the cardinality and location of the hidden variables. In this work, we present a general approach for learning Bayesian networks with hidden variables that overcomes these problems. The approach builds on the information bottleneck framework of Tishby et al. (1999). We start by proving formal correspondence between the information bottleneck objective and the standard parametric EM functional. We then use this correspondence to construct a learning algorithm that combines an information-theoretic smoothing term with a continuation procedure. Intuitively, the algorithm bypasses local maxima and achieves superior solutions by following a continuous path from a solution of, an easy and smooth, target function, to a solution of the desired likelihood function. As we show, our algorithmic framework allows learning of the parameters as well as the structure of a network. In addition, it also allows us to introduce new hidden variables during model selection and learn their cardinality. We demonstrate the performance of our procedure on several challenging real-life data sets. Keywords: Bayesian networks, hidden variables, information bottleneck, continuation, variational methods</p><br/>
<h2>reference text</h2><p>J. Adachi and M. Hasegawa. Molphy version 2.3, programs for molecular phylogenetics based on maximum likelihood. Technical report, The Institute of Statistical Mathematics, Tokyo, Japan, 1996. S. Becker, S. Thrun, and K. Obermayer, editors. Advances in Neural Information Processing Systems 15. MIT Press, Cambridge, Mass., 2002. C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, Oxford, United Kingdom, 1995. X. Boyen, N. Friedman, and D. Koller. Discovering the hidden structure of complex dynamic systems. In K. Laskey and H. Prade, editors, Proc. Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’99), pages 91–100, San Francisco, 1999. Morgan Kaufmann. 123  E LIDAN AND F RIEDMAN  J.S. Breese and D. Koller, editors. Proc. Seventeenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’01). Morgan Kaufmann, San Francisco, 2001. K. Chang and R. Fung. Reﬁnement and coarsening of bayesian networks. In P. P. Bonissone, M. Henrion, L. N. Kanal, and J. F. Lemmer, editors, Proc. Sixth Annual Conference on Uncertainty Artiﬁcial Intelligence (UAI ’90), pages 475–482, San Francisco, 1990. Morgan Kaufmann. P. Cheeseman, J. Kelly, M. Self, J. Stutz, W. Taylor, and D. Freeman. Autoclass: a Bayesian classiﬁcation system. In Proc. Fifth International Workshop on Machine Learning, pages 54–64. Morgan Kaufmann, San Francisco, 1988. D. M. Chickering. Learning equivalence classes of Bayesian network structures. In E. Horvitz and F. Jensen, editors, Proc. Twelfth Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’96), pages 150–157, San Francisco, 1996. Morgan Kaufmann. D. M. Chickering and D. Heckerman. Efﬁcient approximations for the marginal likelihood of Bayesian networks with hidden variables. Machine Learning, 29:181–212, 1997. A. Corduneanu and T. Jaakkola. Continuation methods for mixing heterogeneous sources. In A. Darwich and N. Friedman, editors, Proc. Eighteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’02), pages 111–118, San Francisco, 2002. Morgan Kaufmann. T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, New York, 1991. M. H. DeGroot. Optimal Statistical Decisions. McGraw-Hill, New York, 1970. A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B 39:1–39, 1977. T. El-Hay and N. Friedman. Incorporating expressive graphical models in variational approximations: Chain-graphs and hidden variables. In Breese and Koller (2001), pages 136–143. G. Elidan and N. Friedman. Learning the dimensionality of hidden variables. In Breese and Koller (2001), pages 144–151. G. Elidan, N. Lotner, N. Friedman, and D. Koller. Discovering hidden variables: A structure-based approach. In Leen et al. (2001), pages 479–485. G. Elidan, M. Ninio, N. Friedman, and D. Schuurmans. Data perturbation for escaping local maxima in learning. In Proc. National Conference on Artiﬁcial Intelligence (AAAI ’02), pages 132–139. AAAI Press, Menlo Park, CA, 2002. N. Friedman. Learning belief networks in the presence of missing values and hidden variables. In D. Fisher, editor, Proc. Fourteenth International Conference on Machine Learning, pages 125– 133. Morgan Kaufmann, San Francisco, 1997. N. Friedman, O. Mosenzon, N. Slonim, and N. Tishby. Multivariate information bottleneck. In Breese and Koller (2001), pages 152–161. 124  L EARNING H IDDEN VARIABLE N ETWORKS  A. P. Gasch, P. T. Spellman, C. M. Kao, O. Carmel-Harel, M. B. Eisen, G. Storz, D. Botstein, and P. O. Brown. Genomic expression program in the response of yeast cells to environmental changes. Molecular Biology of the Cell, 11:4241–4257, 2000. F. Glover and M. Laguna. Tabu search. In C. Reeves, editor, Modern Heuristic Techniques for Combinatorial Problems, Oxford, England, 1993. Blackwell Scientiﬁc Publishing. D. Heckerman. A tutorial on learning with Bayesian networks. In M. I. Jordan, editor, Learning in Graphical Models. Kluwer, Dordrecht, Netherlands, 1998. D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. In R. L´ pez de Mantar´ s and D. Poole, editors, Proc. Tenth o a Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’94), pages 293–301, San Francisco, 1994. Morgan Kaufmann. D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20:197–243, 1995. E. T. Jaynes. Information theory and statistical mechanics. Physical Review, 106:620–630, 1957. M. I. Jordan, Z. Ghahramani, T. Jaakkola, and L. K. Saul. An introduction to variational approximations methods for graphical models. In M. I. Jordan, editor, Learning in Graphical Models. Kluwer, Dordrecht, Netherlands, 1998. S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. Science, 220 (4598):671–680, 1983. W. Lam and F. Bacchus. Learning Bayesian belief networks: An approach based on the MDL principle. Computational Intelligence, 10:269–293, 1994. S. L. Lauritzen. The EM algorithm for graphical association models with missing data. Computational Statistics and Data Analysis, 19:191–201, 1995. T. K. Leen, T. G. Dietterich, and V. Tresp, editors. Advances in Neural Information Processing Systems 13. MIT Press, Cambridge, Mass., 2001. J. Martin and K. VanLehn. Discrete factor analysis: Learning hidden variables in Bayesian networks. Technical report, Department of Computer Science, University of Pittsburgh, 1995. M. Meila and M. I. Jordan. Estimating dependency structure as a hidden variable. In M. I. Jordan, M. J. Kearns, and S. A. Solla, editors, Advances in Neural Information Processing Systems 10, pages 584–590, Cambridge, Mass., 1998. MIT Press. R. M. Neal and G. E. Hinton. A new view of the EM algorithm that justiﬁes incremental and other variants. In M. I. Jordan, editor, Learning in Graphical Models. Kluwer, Dordrecht, Netherlands, 1998. J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, 1988. F. Pereira, N. Tishby, and L. Lee. Distributional clustering of English words. In 31st Annual Meeting of the ACL, pages 183–190, 1993. 125  E LIDAN AND F RIEDMAN  K. Rose. Deterministic annealing for clustering, compression, classiﬁcation, regression, and related optimization problems. Proc. IEEE, 86:2210–2239, 1998. N. Slonim, N.Friedman, and T.Tishby. Agglomerative multivariate information bottleneck. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 929–936, Cambridge, Mass., 2002. MIT Press. N. Slonim and N. Tishby. Agglomerative information bottleneck. In S. A. Solla, T. K. Leen, and K. M¨ ller, editors, Advances in Neural Information Processing Systems 12, pages 617–623, u Cambridge, Mass., 2000. MIT Press. N. Slonim and N. Tishby. Data clustering by markovian relaxation and the information bottleneck method. In Leen et al. (2001), pages 640–646. N. Slonim and Y. Weiss. Maximum likelihood and the information bottleneck. In Becker et al. (2002), pages 351–358. N. A. Smith and J. Eisner. Annealing techniques for unsupervised statistical language learning. In Proc. 42nd Annual Meeting of the Association for Computational Linguistics, 2004. P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction and Search. Number 81 in Lecture Notes in Statistics. Springer-Verlag, New York, 1993. A. Stolcke and S. Omohundro. Hidden Markov Model induction by bayesian model merging. In Stephen Jos´ Hanson, Jack D. Cowan, and C. Lee Giles, editors, Advances in Neural Information e Processing Systems, volume 5, pages 11–18. Morgan Kaufmann, San Mateo, CA, 1993. M. Szummer and T. Jaakkola. Information regularization with partially labeled data. In Becker et al. (2002), pages 640–646. B. Thiesson. Score and information for recursive exponential models with incomplete data. In D. Geiger and P. Shanoy, editors, Proc. Thirteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’97), San Francisco, 1997. Morgan Kaufmann. B. Thiesson, C. Meek, D. M. Chickering, and D. Heckerman. Learning mixtures of Bayesian networks. In G. F. Cooper and S. Moral, editors, Proc. Fourteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’98), pages 504–513, San Francisco, 1998. Morgan Kaufmann. N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. In B. Hajek and R. S. Sreenivas, editors, Proc. 37th Allerton Conference on Communication, Control and Computation, pages 368–377. University of Illinois, 1999. N. Ueda and R. Nakano. Deterministic annealing EM algorithm. Neural Networks, 11(2):271–282, 1998. L. T. Watson. Theory of globally convergent probability-one homotopies for non-linear programming. Technical Report TR-00-04, Department of Computer Science, Virginia Tech, 2000. M. Whiley and D. M. Titterington. Applying the deterministic annealing expectation maximization algorithm to Naive Bayes networks. Technical Report 02-5, Department of Statistics, University of Glasgow, 2002. 126  L EARNING H IDDEN VARIABLE N ETWORKS  N. L. Zhang. Hierarchical latent class models for cluster analysis. Journal of Machine Learning Research, 5:697–723, 2004.  127</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
