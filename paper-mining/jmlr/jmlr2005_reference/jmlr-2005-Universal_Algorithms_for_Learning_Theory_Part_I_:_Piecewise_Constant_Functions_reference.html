<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-70" href="../jmlr2005/jmlr-2005-Universal_Algorithms_for_Learning_Theory_Part_I_%3A_Piecewise_Constant_Functions.html">jmlr2005-70</a> <a title="jmlr-2005-70-reference" href="#">jmlr2005-70-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>70 jmlr-2005-Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions</h1>
<br/><p>Source: <a title="jmlr-2005-70-pdf" href="http://jmlr.org/papers/volume6/binev05a/binev05a.pdf">pdf</a></p><p>Author: Peter Binev, Albert Cohen, Wolfgang Dahmen, Ronald DeVore, Vladimir Temlyakov</p><p>Abstract: This paper is concerned with the construction and analysis of a universal estimator for the regression problem in supervised learning. Universal means that the estimator does not depend on any a priori assumptions about the regression function to be estimated. The universal estimator studied in this paper consists of a least-square ﬁtting procedure using piecewise constant functions on a partition which depends adaptively on the data. The partition is generated by a splitting procedure which differs from those used in CART algorithms. It is proven that this estimator performs at the optimal convergence rate for a wide class of priors on the regression function. Namely, as will be made precise in the text, if the regression function is in any one of a certain class of approximation spaces (or smoothness spaces of order not exceeding one – a limitation resulting because the estimator uses piecewise constants) measured relative to the marginal measure, then the estimator converges to the regression function (in the least squares sense) with an optimal rate of convergence in terms of the number of samples. The estimator is also numerically feasible and can be implemented on-line. Keywords: distribution-free learning theory, nonparametric regression, universal algorithms, adaptive approximation, on-line algorithms c 2005 Peter Binev, Albert Cohen, Wolfgang Dahmen, Ronald DeVore and Vladimir Temlyakov. B INEV, C OHEN , DAHMEN , D E VORE AND T EMLYAKOV</p><br/>
<h2>reference text</h2><p>Y. Baraud. Model selection for regression on a random design. ESAIM Prob. et Stats., 6:127—146, 2002. A. R. Barron. Complexity regularization with application to artiﬁcial neural network. In Nonparametric functional estimation and related topics, G. Roussas (ed.), pages 561—576, Kluwer Academic Publishers, 1991. P. Binev and R. DeVore. Fast computation in adaptive tree approximation. Numerische Math., 97:193—217, 2004. L. Birg´ . Model selection via testing : an alternative to (penalized) maximum likelihood estimators. e Preprint, to appear in Ann. IHP, 2004. L. Birg´ and P. Massart. Gaussian model selection. J. Eur. Math. Soc., 3:203—268, 2001. e L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and regression trees, Wadsworth international, Belmont, CA, 1984. 1320  U NIVERSAL A LGORITHMS FOR L EARNING T HEORY  A. Cohen, W. Dahmen, I. Daubechies, and R. DeVore. Tree-structured approximation and optimal encoding. App. Comp. Harm. Anal., 11:192—226, 2001. S. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin of AMS, 39:1—49, 2001. I. Daubechies. Ten Lectures on Wavelets, SIAM, Philadelphia, 1992. R. A. DeVore. Nonlinear approximation. Acta Numerica, 7:51—150, 1998. R. DeVore, G. Kerkyacharian, D. Picard and V. Temlyakov. On mathematical methods of learning. IMI Preprint, 2004:10, University of South Carolina, 2004a. R. DeVore, G. Kerkyacharian, D. Picard and V. Temlyakov. Lower bounds in learning theory. IMI Preprint, 2004:22, University of South Carolina, 2004b. D. L. Donoho. CART and best-ortho-basis : a connection. Annals of Statistics, 25:1870—1911, 1997. D. L. Donoho and I. M. Johnstone. Adapting to unknown smoothness via Wavelet shrinkage. J. Amer. Statist. Assoc., 90(no. 432):1200—1224, 1995. D. L. Donoho and I. M. Johnstone. Minimax estimation via wavelet shrinkage. Annals of Statistics, 26(no. 3):879—921, 1998. D. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Picard. Wavelet shrinkage: Asymptopia? Journal of the Royal Statistical Society, 57:301—369, 1996a. D. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Picard. Density estimation by wavelet thresholding. Annals of Statistics, 24:508—539, 1996b. L. Gy¨ rfy, M. Kohler, A. Krzyzak, and H. Walk. A distribution-free theory of nonparametric regreso sion, Springer, Berlin, 2002. S. V. Konyagin and V. N. Temlyakov. Some error estimates in learning theory. IMI Preprints, 2004:05, University of South Carolina, 2004a. S. V. Konyagin and V. N. Temlyakov. The entropy in learning theory: Error estimates. IMI Preprints, 2004:09, University of South Carolina, 2004b.  1321</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
