<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-34" href="../jmlr2005/jmlr-2005-Feature_Selection_for_Unsupervised_and_Supervised_Inference%3A_The_Emergence_of_Sparsity_in_a_Weight-Based_Approach.html">jmlr2005-34</a> <a title="jmlr-2005-34-reference" href="#">jmlr2005-34-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>34 jmlr-2005-Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach</h1>
<br/><p>Source: <a title="jmlr-2005-34-pdf" href="http://jmlr.org/papers/volume6/wolf05a/wolf05a.pdf">pdf</a></p><p>Author: Lior Wolf, Amnon Shashua</p><p>Abstract: The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classiﬁcation tasks, for example, it is not uncommon to have 104 to 107 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a deﬁnition of “relevancy” based on spectral properties of the Laplacian of the features’ measurement matrix. The feature selection process is then based on a continuous ranking of the features deﬁned by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a “biased non-negativity” of a key matrix in the process. As a result, a simple leastsquares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.</p><br/>
<h2>reference text</h2><p>H. Almuallim and T .G Dietterich. Learning boolean concepts in the presence of many irrelevant features. AI, 69(1-2):279–305, 1991. A. Ben-Dor, N. Friedman, and Z. Yakhini. Class discovery in gene expression data. In RECOMB, 2001. A. Blum and P. Langley. Selection of relevant features and examples in machine learning. AI, 97 (1-2):245–271, 1997. O. Bousquet and D. J. L. Herrmann. On the complexity of learning the kernel matrix. In NIPS, 2003. P. S. Bradley and O. L. Mangasarian. Feature selection via concave minimization and support vector machines. In ICML, 1998. M. Brand and K. Huang. A unifying theorem for spectral embedding and clustering. In Ninth Int. Workshop on AI and Statistics, 2003. O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector machines. Machine Learning, 46(1-3):131–159, 2002. F. R. K. Chung. Spectral Graph Theory. AMS, 1998. 1884  F EATURE S ELECTION FOR U NSUPERVISED AND S UPERVISED I NFERENCE  Z. Furedi and J. Komlos. The eigenvalues of random symmetric matrices. Combinatorica, 1(3): 233–241, 1981. L. E. Gibbons, D. W. Hearn, P. M. Pardalos, and M. V. Ramana. Continuous characterizations of the maximum clique problem. Math. Oper. Res, 22:754–768, 1997. G. Golub and C. F. V. Loan. Matrix Computations. Johns Hopkins University Press, Baltimore, MD, 1996. T. R. Golub, D. K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J. P. Mesirov, H. Coller, M. L. Loh, J. R. Downing, M. A. Caligiuri, C. D. Bloomﬁeld, and E. S. Lander. Molecular classiﬁcation of cancer: class discovery and class prediction by gene expression monitoring. Science, 286:531– 537, 1999. I. Guyon and A. Elissef. An introduction to variable and feature selection. Journal of Machine Learning Research, Special issue on special feature, 3:389–422, 2003. I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for cancer classiﬁcation using support vector machines. Machine Learning, 46:389–422, 2002. K. Hall. An r-dimensional quadratic placement algorithm. Management Science, 17(3):219–229, 1970. K. Hall. ’gene shaving’ as a method for identifying distinct sets of genes with similar expression patterns. Genome Biology, 1(2):1–21, 2000. W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58(301):13–30, 1963. K. Kira and L. Rendell. A practical approach to feature selection. In Ninth Int, Workshop on Machine Learning, 1992. J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Journal of Information and Computation, 132(1):1–63, 1997. R. Kohavi and G. John. Wrappers for feature subset selection. Artiﬁcial Intelligence, 97(1-2): 273–324, 1997. P. Langley and W. Iba. Average-case analysis of a nearest neighbor algorithm. In 13th Int. Joint Conf. on Artiﬁcial Intelligence, 1993. D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755):788–791, 1999. D. D. Lewis. Feature selection and feature extraction for text categorization. In Speech and Natural Language Workshop, 1992. R. Linsker. Self-organization in a perceptual network. Computer, 21(3):105–117, 1988. ISSN 0018-9162. doi: http://dx.doi.org/10.1109/2.36. P. M. Long and V. B. Vega. Boosting and microarray data. Machine Learning, 52:31–44, 2003. 1885  W OLF AND S HASHUA  M. L. Mehta. Random Matrices. Academic Press, 1991. B. Mohar. The laplacian spectrum of graphs. In Y. Alavi et al., editor, Graph Theory, Combinatorics and Applications. Wiley, New York, 1991. T. S. Motzkin and E. G. Straus. Maxima for graphs and a new proof of a theorem by turan. Canadian Journal of Math., 17:533–540, 1965. A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. In NIPS, 2001. B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381(13):607–609, 1996. M. Pavan and M. Pelillo. A new graph-theoretic approach to clustering and segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2003. P. Perona and W. T. Freeman. A factorization approach to grouping. In European Conference of Computer Vision (ECCV), 1998. S. L. Pomeroy, P. Tamayo, M. Gaasenbeek, L. M. Sturla, M. Angelo, M. E. McLaughlin, J. Y. H. Kim, L. C. Goumnerova, P.McL. Black, C. Lau, J. C. Allen, D. Zagzag, J. M.Olson, T. Curran, C. Wetmore, J. A. Biegel, T. Poggio, S. Mukherjee, R. Rifkin, A.Califano, G.Stolovitzky, D. N. Louis, J. P. Mesirov, E. S. Lander, and T. R. Golub. Gene expression-based classiﬁcation and outcome prediction of central nervous system embryonal tumors. Nature, 415(24):436–442, 2002. S. Ramaswamy. Personal communication. R. Rifkin, G. Yeo, and T. Poggio. Regularized Least Squares Classiﬁcation, volume 190 of NATO Science Series III: Computer and Systems Sciences. IOS Press, Amsterdam, 2003. S. Sarkar and K. L. Boyer. Quantitative measures of change based on feature organization: eigenvalues and eigenvectors. CVIU, 71(1):110–136, 1998. B. Scholkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002. A. Shashua and L. Wolf. Kernel feature selection with side data using a spectral approach. In T. Pajdla and J. Matas, editors, ECCV (3), volume 3023 of Lecture Notes in Computer Science, pages 39–53. Springer, 2004. ISBN 3-540-21982-X. J. Shi and J. Malik. Normalized cuts and image segmentation. PAMI, 22(8):888–905, 2000. M. A. Shipp, K. N. Ross, P. Tamayo, A. P. Weng, J.L Kutok, R.C Aguiar, M. Gaasenbeek, M. Angelo, M. Reich, G. S. Pinkus, T. S. Ray, M.A Koval, K.W Last, A. Norton, T. A. Lister, J. Mesirov, D. S. Neuberg, E. S. Lander, J. C. Aster, and T. R. Golub. Diffuse large b-cell lymphoma outcome prediction by gene expression proﬁling and supervised machine learning. Nature Medicine, 8(1): 68–74, 2002. 1886  F EATURE S ELECTION FOR U NSUPERVISED AND S UPERVISED I NFERENCE  N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. In Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing, pages 368–377, 1999. L. J. van ’t Veer, H. Dai, M. J. van de Vijver, Y. D. He, A.A Hart, M. Mao, H. L. Peterse, K. van der Kooy, M. J. Marton, A. T. Witteveen, G. J. Schreiber, R. M. Kerkhoven, C. Roberts, P. S. Linsley, R. Bernards, and S. H. Friend. Gene expression proﬁling predicts clinical outcome of breast cancer. Nature, 415(6871):530–536, 2002. V. N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, 1998. N. Vasconcelos. Feature selection by maximum marginal diversity: optimality and implications for visual recognition. In CVPR (1), pages 762–772. IEEE Computer Society, 2003. ISBN 0-76951900-8. P. Viola and M. Jones. Robust real-time object detection. Technical Report CRL-2001-1, Compaq Cambridge Research Lab, 2001. Y. Weiss. Segmentation using eigenvectors: A unifying view. In ICCV, 1999. J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Poggio, and V. Vapnik. Feature selection for svms. NIPS, 2001. E. P. Wigner. On the distribution of the roots of certain symmetric matrices. Ann. of Math.(2), 67: 325–327, 1958. L. Wolf and A. Shashua. Kernel principal angles for classiﬁcation machines with applications to image sequence interpretation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2003. L. Wolf, A. Shashua, and S. Mukherjee. Gene selection via a spectral approach. In post CVPR IEEE Workshop on Computer Vision Methods for Bioinformatics (CVMB), 2005.  1887</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
