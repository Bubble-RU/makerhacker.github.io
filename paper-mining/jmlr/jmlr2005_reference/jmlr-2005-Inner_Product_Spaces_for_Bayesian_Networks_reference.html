<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 jmlr-2005-Inner Product Spaces for Bayesian Networks</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-40" href="../jmlr2005/jmlr-2005-Inner_Product_Spaces_for_Bayesian_Networks.html">jmlr2005-40</a> <a title="jmlr-2005-40-reference" href="#">jmlr2005-40-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>40 jmlr-2005-Inner Product Spaces for Bayesian Networks</h1>
<br/><p>Source: <a title="jmlr-2005-40-pdf" href="http://jmlr.org/papers/volume6/nakamura05a/nakamura05a.pdf">pdf</a></p><p>Author: Atsuyoshi Nakamura, Michael Schmitt, Niels Schmitt, Hans Ulrich Simon</p><p>Abstract: Bayesian networks have become one of the major models used for statistical inference. We study the question whether the decisions computed by a Bayesian network can be represented within a low-dimensional inner product space. We focus on two-label classiﬁcation tasks over the Boolean domain. As main results we establish upper and lower bounds on the dimension of the inner product space for Bayesian networks with an explicitly given (full or reduced) parameter collection. In particular, these bounds are tight up to a factor of 2. For some nontrivial cases of Bayesian networks we even determine the exact values of this dimension. We further consider logistic autoregressive Bayesian networks and show that every sufﬁciently expressive inner product space must have dimension at least Ω(n2 ), where n is the number of network nodes. We also derive the bound 2Ω(n) for an artiﬁcial variant of this network, thereby demonstrating the limits of our approach and raising an interesting open question. As a major technical contribution, this work reveals combinatorial and algebraic structures within Bayesian networks such that known methods for the derivation of lower bounds on the dimension of inner product spaces can be brought into play. Keywords: Bayesian network, inner product space, embedding, linear arrangement, Euclidean dimension</p><br/>
<h2>reference text</h2><p>Yasemin Altun, Ioannis Tsochantaridis, and Thomas Hofmann. Hidden Markov support vector machines. In Proceedings of the 20th International Conference on Machine Learning, pages 3–10. AAAI Press, Menlo Park, CA, 2003. Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, Cambridge, 1999. 1400  I NNER P RODUCT S PACES FOR BAYESIAN N ETWORKS  Rosa I. Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and random projection. In Proceedings of the 40th Annual Symposium on Foundations of Computer Science, pages 616–623. IEEE Computer Society Press, Los Alamitos, CA, 1999. Maria-Florina Balcan, Avrim Blum, and Santosh Vempala. On kernels, margins, and lowdimensional mappings. In Shai Ben-David, John Case, and Akira Maruoka, editors, Proceedings of the 15th International Conference on Algorithmic Learning Theory ALT 2004, volume 3244 of Lecture Notes in Artiﬁcial Intelligence, pages 194–205. Springer-Verlag, Berlin, 2004. Shai Ben-David, Nadav Eiron, and Hans Ulrich Simon. Limitations of learning via embeddings in Euclidean half-spaces. Journal of Machine Learning Research, 3:441–461, 2002. Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. A training algorithm for optimal margin classiﬁers. In Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory, pages 144–152. ACM Press, New York, NY, 1992. David Maxwell Chickering, David Heckerman, and Christopher Meek. A Bayesian approach to learning Bayesian networks with local structure. In Proceedings of the Thirteenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 80–89. Morgan Kaufmann, San Francisco, CA, 1997. Luc Devroye, L´ szl´ Gy¨ rﬁ, and G´ bor Lugosi. A Probabilistic Theory of Pattern Recognition. a o o a Springer-Verlag, Berlin, 1996. Richard O. Duda and Peter E. Hart. Pattern Classiﬁcation and Scene Analysis. Wiley & Sons, New York, NY, 1973. R. M. Dudley. Central limit theorems for empirical measures. Annals of Probability, 6:899–929, 1978. J¨ rgen Forster. A linear lower bound on the unbounded error communication complexity. Journal u of Computer and System Sciences, 65:612–625, 2002. J¨ rgen Forster, Matthias Krause, Satyanarayana V. Lokam, Rustam Mubarakzjanov, Niels Schmitt, u and Hans Ulrich Simon. Relations between communication complexity, linear arrangements, and computational complexity. In Ramesh Hariharan, Madhavan Mukund, and V. Vinay, editors, Proceedings of the 21st Annual Conference on the Foundations of Software Technology and Theoretical Computer Science, volume 2245 of Lecture Notes in Computer Science, pages 171–182. Springer-Verlag, Berlin, 2001. J¨ rgen Forster, Niels Schmitt, Hans Ulrich Simon, and Thorsten Suttorp. Estimating the optimal u margins of embeddings in Euclidean halfspaces. Machine Learning, 51:263–281, 2003. J¨ rgen Forster and Hans Ulrich Simon. On the smallest possible dimension and the largest possiu ble margin of linear arrangements representing given concept classes. In Nicol` Cesa-Bianchi, o Masayuki Numao, and R¨ diger Reischuk, editors, Proceedings of the 13th International Worku shop on Algorithmic Learning Theory ALT 2002, volume 2533 of Lecture Notes in Artiﬁcial Intelligence, pages 128–138. Springer-Verlag, Berlin, 2002. P. Frankl and H. Maehara. The Johnson-Lindenstrauss lemma and the sphericity of some graphs. Journal of Combinatorial Theory, Series B, 44:355–362, 1988. 1401  NAKAMURA , S CHMITT, S CHMITT AND S IMON  Brendan J. Frey. Graphical Models for Machine Learning and Digital Communication. MIT Press, Cambridge, MA, 1998. Andras Hajnal, Wolfgang Maass, Pavel Pudl´ k, Mario Szegedy, and Gy¨ rgi Tur´ n. Threshold a o a circuits of bounded depth. Journal of Computer and System Sciences, 46:129–154, 1993. Tommi S. Jaakkola and David Haussler. Exploiting generative models in discriminative classiﬁers. In Michael S. Kearns, Sara A. Solla, and David A. Cohn, editors, Advances in Neural Information Processing Systems 11, pages 487–493. MIT Press, Cambridge, MA, 1999a. Tommi S. Jaakkola and David Haussler. Probabilistic kernel regression models. In David Heckerman and Joe Whittaker, editors, Proceedings of the 7th International Workshop on Artiﬁcial Intelligence and Statistics. Morgan Kaufmann, San Francisco, CA, 1999b. W. B. Johnson and J. Lindenstrauss. Extensions of Lipshitz mapping into Hilbert spaces. Contemporary Mathematics, 26:189–206, 1984. Eike Kiltz. On the representation of Boolean predicates of the Difﬁe-Hellman function. In H. Alt and M. Habib, editors, Proceedings of 20th International Symposium on Theoretical Aspects of Computer Science, volume 2607 of Lecture Notes in Computer Science, pages 223–233. SpringerVerlag, Berlin, 2003. Eike Kiltz and Hans Ulrich Simon. Complexity theoretic aspects of some cryptographic functions. In T. Warnow and B. Zhu, editors, Proceedings of the 9th International Conference on Computing and Combinatorics COCOON 2003, volume 2697 of Lecture Notes in Computer Science, pages 294–303. Springer-Verlag, Berlin, 2003. P. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman and Hall, London, 1983. Atsuyoshi Nakamura, Michael Schmitt, Niels Schmitt, and Hans Ulrich Simon. Bayesian networks and inner product spaces. In John Shawe-Taylor and Yoram Singer, editors, Proceedings of the 17th Annual Conference on Learning Theory COLT 2004, volume 3120 of Lecture Notes in Artiﬁcial Intelligence, pages 518–533. Springer-Verlag, Berlin, 2004. Radford M. Neal. Connectionist learning of belief networks. Artiﬁcial Intelligence, 56:71–113, 1992. Nuria Oliver, Bernhard Sch¨ lkopf, and Alexander J. Smola. Natural regularization from generative o models. In Alexander J. Smola, Peter L. Bartlett, Bernhard Sch¨ lkopf, and Dale Schuurmans, o editors, Advances in Large Margin Classiﬁers, pages 51–60. MIT Press, Cambridge, MA, 2000. Judea Pearl. Reverend Bayes on inference engines: A distributed hierarchical approach. In Proceedings of the National Conference on Artiﬁcial Intelligence, pages 133–136. AAAI Press, Menlo Park, CA, 1982. Laurence K. Saul, Tommi Jaakkola, and Michael I. Jordan. Mean ﬁeld theory for sigmoid belief networks. Journal of Artiﬁcial Intelligence Research, 4:61–76, 1996. Craig Saunders, John Shawe-Taylor, and Alexei Vinokourov. String kernels, Fisher kernels and ﬁnite state automata. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 633–640. MIT Press, Cambridge, MA, 2003. 1402  I NNER P RODUCT S PACES FOR BAYESIAN N ETWORKS  Michael Schmitt. On the complexity of computing and learning with multiplicative neural networks. Neural Computation, 14:241–301, 2002. D. J. Spiegelhalter and R. P. Knill-Jones. Statistical and knowledge-based approaches to clinical decision support systems. Journal of the Royal Statistical Society, Series A, 147:35–77, 1984. Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In Peter Auer and Ron Meir, editors, Proceedings of the 18th Annual Conference on Learning Theory COLT 2005, volume 3559 of Lecture Notes in Artiﬁcial Intelligence, pages 545–560. Springer-Verlag, Berlin, 2005. Ben Taskar, Carlos Guestrin, and Daphne Koller. Max-margin Markov networks. In Sebastian Thrun, Lawrence K. Saul, and Bernhard Sch¨ lkopf, editors, Advances in Neural Information o Processing Systems 16, pages 25–32. MIT Press, Cambridge, MA, 2004. Koji Tsuda, Shotaro Akaho, Motoaki Kawanabe, and Klaus-Robert M¨ ller. Asymptotic properties u of the Fisher kernel. Neural Computation, 16:115–137, 2004. Koji Tsuda and Motoaki Kawanabe. The leave-one-out kernel. In Jose R. Dorronsoro, editor, Proceedings of the International Conference on Artiﬁcial Neural Networks ICANN 2002, volume 2415 of Lecture Notes in Computer Science, pages 727–732. Springer-Verlag, Berlin, 2002. Koji Tsuda, Motoaki Kawanabe, Gunnar R¨ tsch, S¨ ren Sonnenburg, and Klaus-Robert M¨ ller. A a o u new discriminative kernel from probabilistic models. Neural Computation, 14:2397–2414, 2002. Vladimir Vapnik. Statistical Learning Theory. Wiley Series on Adaptive and Learning Systems for Signal Processing, Communications, and Control. Wiley & Sons, New York, NY, 1998. Manfred K. Warmuth and S. V. N. Vishwanathan. Leaving the span. In Peter Auer and Ron Meir, editors, Proceedings of the 18th Annual Conference on Learning Theory COLT 2005, volume 3559 of Lecture Notes in Artiﬁcial Intelligence, pages 366–381. Springer-Verlag, Berlin, 2005.  1403</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
