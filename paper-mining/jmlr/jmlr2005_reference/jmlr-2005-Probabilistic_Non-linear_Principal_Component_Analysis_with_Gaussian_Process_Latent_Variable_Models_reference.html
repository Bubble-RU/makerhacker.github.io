<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-62" href="../jmlr2005/jmlr-2005-Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models.html">jmlr2005-62</a> <a title="jmlr-2005-62-reference" href="#">jmlr2005-62-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 jmlr-2005-Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</h1>
<br/><p>Source: <a title="jmlr-2005-62-pdf" href="http://jmlr.org/papers/volume6/lawrence05a/lawrence05a.pdf">pdf</a></p><p>Author: Neil Lawrence</p><p>Abstract: Summarising a high dimensional data set with a low dimensional embedding is a standard approach for exploring its structure. In this paper we provide an overview of some existing techniques for discovering such embeddings. We then introduce a novel probabilistic interpretation of principal component analysis (PCA) that we term dual probabilistic PCA (DPPCA). The DPPCA model has the additional advantage that the linear mappings from the embedded space can easily be nonlinearised through Gaussian processes. We refer to this model as a Gaussian process latent variable model (GP-LVM). Through analysis of the GP-LVM objective function, we relate the model to popular spectral techniques such as kernel PCA and multidimensional scaling. We then review a practical algorithm for GP-LVMs in the context of large data sets and develop it to also handle discrete valued data and missing attributes. We demonstrate the model on a range of real-world and artiﬁcially generated data sets. Keywords: Gaussian processes, latent variable models, principal component analysis, spectral methods, unsupervised learning, visualisation</p><br/>
<h2>reference text</h2><p>David J. Bartholomew. Latent Variable Models and Factor Analysis. Charles Grifﬁn & Co. Ltd, London, 1987. Alexander Basilevsky. Statistical Factor Analysis and Related Methods. Wiley, New York, 1994. Christopher M. Bishop. Bayesian PCA. In Michael J. Kearns, Sara A. Solla, and David A. Cohn, editors, Advances in Neural Information Processing Systems, volume 11, pages 482–388, Cambridge, MA, 1999. MIT Press. Christopher M. Bishop and Gwilym D. James. Analysis of multiphase ﬂows using dual-energy gamma densitometry and neural networks. Nuclear Instruments and Methods in Physics Research, A327:580–593, 1993. Christopher M. Bishop, Marcus Svensén, and Christopher K. I. Williams. A fast EM algorithm for latent variable density models. In D. S. Touretzky, Michael C. Mozer, and M. E. Hasselmo, editors, Advances in Neural Information Processing Systems, volume 8, pages 465–471. MIT Press, 1996. Christopher M. Bishop, Marcus Svensén, and Christopher K. I. Williams. GTM: a principled alternative to the Self-Organizing Map. In Advances in Neural Information Processing Systems, volume 9, pages 354–360. MIT Press, 1997. Christopher M. Bishop, Marcus Svensén, and Christopher K. I. Williams. GTM: the Generative Topographic Mapping. Neural Computation, 10(1):215–234, 1998. Lehel Csató. Gaussian Processes — Iterative Sparse Approximations. PhD thesis, Aston University, 2002. Keith Grochow, Steven L. Martin, Aaron Hertzmann, and Zoran Popovic. Style-based inverse kinematics. In ACM Transactions on Graphics (SIGGRAPH 2004), 2004. Geoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and Radford M. Neal. The wake-sleep algorithm for unsupervised neural networks. Science, 268:1158–1161, 1995. Geoffrey E. Hinton and Sam T. Roweis. Stochastic neighbor embedding. In Sue Becker, Sebastian Thrun, and Klaus Obermayer, editors, Advances in Neural Information Processing Systems, volume 15, pages 857–864, Cambridge, MA, 2003. MIT Press. Antti Honkela and Harri Valpola. Unsupervised variational Bayesian learning of nonlinear models. In Lawrence Saul, Yair Weiss, and Léon Bouttou, editors, Advances in Neural Information Processing Systems, volume 17, pages 593–600, Cambridge, MA, 2005. MIT Press. Teuvo Kohonen. The self-organizing map. Proceedings of the IEEE, 78(9):1464–1480, 1990. Joseph B. Kruskal. Multidimensional scaling by optimizing goodness-of-ﬁt to a nonmetric hypothesis. Psychometrika, 29(1):1–28, 1964. Solomon Kullback and Richard A. Leibler. On information and sufﬁciency. Annals of Mathematical Statistics, 22:79–86, 1951. 1814  P ROBABILISTIC N ON - LINEAR PCA  Neil D. Lawrence. Gaussian process models for visualisation of high dimensional data. In Sebastian Thrun, Lawrence Saul, and Bernhard Schölkopf, editors, Advances in Neural Information Processing Systems, volume 16, pages 329–336, Cambridge, MA, 2004. MIT Press. Neil D. Lawrence, Matthias Seeger, and Ralf Herbrich. Fast sparse Gaussian process methods: The informative vector machine. In Sue Becker, Sebastian Thrun, and Klaus Obermayer, editors, Advances in Neural Information Processing Systems, volume 15, pages 625–632, Cambridge, MA, 2003. MIT Press. David Lowe and Michael E. Tipping. Feed-forward neural networks and topographic mappings for exploratory data analysis. Neural Computing and Applications, 4(83), 1996. David B. MacKay and J. L. Zinnes. A probabilistic model for the multidimensional scaling of proximity and preference data. Marketing Sciences, 5:325–334, 1986. David J. C. MacKay. Bayesian neural networks and density networks. Nuclear Instruments and Methods in Physics Research, A, 354(1):73–80, 1995. Jan R. Magnus and Heinz Neudecker. Matrix Differential Calculus with Applications in Statistics and Econometrics. John Wiley and Sons, Chichester, West Sussex, 2nd edition, 1999. Kantilal V. Mardia, John T. Kent, and John M. Bibby. Multivariate analysis. Academic Press, London, 1979. Peter S. Maybeck. Stochastic Models, Estimation and Control, Volume 1, volume 141 of Mathematics in Science and Engineering. Academic Press, New York, NY, 1979. ISBN 0-12-4807011. Thomas P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, Massachusetts Institute of Technology, 2001. Martin F. Møller. A scaled conjugate gradient algorithm for fast supervised learning. Neural Networks, 6(4):525–533, 1993. Man-Suk Oh and Adrian E. Raftery. Bayesian multidimensional scaling and choice of dimension. Journal of the American Statistical Association, 96:1031–1044, 2001. Anthony O’Hagan. Some Bayesian numerical analysis. In José M. Bernardo, James O. Berger, A. Phillip Dawid, and Adrian F. M. Smith, editors, Bayesian Statistics 4, pages 345–363, Valencia, 1992. Oxford University Press. Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000. John W. Sammon. A nonlinear mapping for data structure analysis. IEEE Transactions on Computers, C-18(5):401–409, 1969. Bernhard Schölkopf, Alexander Smola, and Klaus-Robert Müller. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation, 10:1299–1319, 1998. Bernhard Schölkopf and Alexander J. Smola. Learning with Kernels. MIT Press, 2001. 1815  L AWRENCE  Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000. Joshua B. Tenenbaum, Virginia de Silva, and John C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. Michael E. Tipping. Topographic Mappings and Feed-Forward Neural Networks. PhD thesis, Aston University, Aston Street, Birmingham B4 7ET, U.K., 1996. Michael E. Tipping. Probabilistic visualisation of high-dimensional binary data. In Michael J. Kearns, Sara A. Solla, and David A. Cohn, editors, Advances in Neural Information Processing Systems, volume 11, pages 592–598, Cambridge, MA, 1999. MIT Press. Michael E. Tipping. Sparse kernel principal component analysis. In Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, Advances in Neural Information Processing Systems, volume 13, pages 633–639, Cambridge, MA, 2001. MIT Press. Michael E. Tipping and Christopher M. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society, B, 6(3):611–622, 1999. Warren S. Torgerson. Multidimensional scaling: I. theory and method. Psychometrika, 17:401–419, 1952. Christopher K. I. Williams. Computing with inﬁnite networks. In Michael C. Mozer, Michael I. Jordan, and Thomas Petsche, editors, Advances in Neural Information Processing Systems, volume 9, Cambridge, MA, 1997. MIT Press. Christopher K. I. Williams. Prediction with Gaussian processes: From linear regression to linear prediction and beyond. In Michael I. Jordan, editor, Learning in Graphical Models, volume 89 of Series D: Behavioural and Social Sciences, Dordrecht, The Netherlands, 1998. Kluwer. Christopher K. I. Williams. On a connection between kernel PCA and metric multidimensional scaling. In Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, Advances in Neural Information Processing Systems, volume 13, pages 675–681, Cambridge, MA, 2001. MIT Press.  1816</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
