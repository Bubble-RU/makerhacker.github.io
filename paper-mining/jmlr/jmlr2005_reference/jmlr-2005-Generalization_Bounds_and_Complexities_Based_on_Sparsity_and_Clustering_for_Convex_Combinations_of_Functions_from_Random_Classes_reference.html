<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 jmlr-2005-Generalization Bounds and Complexities Based on Sparsity and Clustering for Convex Combinations of Functions from Random Classes</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-37" href="../jmlr2005/jmlr-2005-Generalization_Bounds_and_Complexities_Based_on_Sparsity_and_Clustering_for_Convex_Combinations_of_Functions_from_Random_Classes.html">jmlr2005-37</a> <a title="jmlr-2005-37-reference" href="#">jmlr2005-37-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>37 jmlr-2005-Generalization Bounds and Complexities Based on Sparsity and Clustering for Convex Combinations of Functions from Random Classes</h1>
<br/><p>Source: <a title="jmlr-2005-37-pdf" href="http://jmlr.org/papers/volume6/jaeger05a/jaeger05a.pdf">pdf</a></p><p>Author: Savina Andonova Jaeger</p><p>Abstract: A uniﬁed approach is taken for deriving new generalization data dependent bounds for several classes of algorithms explored in the existing literature by different approaches. This uniﬁed approach is based on an extension of Vapnik’s inequality for VC classes of sets to random classes of sets - that is, classes depending on the random data, invariant under permutation of the data and possessing the increasing property. Generalization bounds are derived for convex combinations of functions from random classes with certain properties. Algorithms, such as SVMs (support vector machines), boosting with decision stumps, radial basis function networks, some hierarchies of kernel machines or convex combinations of indicator functions over sets with ﬁnite VC dimension, generate classiﬁer functions that fall into the above category. We also explore the individual complexities of the classiﬁers, such as sparsity of weights and weighted variance over clusters from the convex combination introduced by Koltchinskii and Panchenko (2004), and show sparsity-type and cluster-variance-type generalization bounds for random classes. Keywords: complexities of classiﬁers, generalization bounds, SVM, voting classiﬁers, random classes</p><br/>
<h2>reference text</h2><p>Andonova, S. Theoretical and experimental analysis of the generalization ability of some statistical learning algorithms. PhD thesis, Department of Mathematics and Statistics, Boston University, Boston, MA, 2004. Anthony, M., Shawe-Taylor, J. A result of Vapnik with applications. Discrete Applied Mathematics, 47(3): 207–217, 1993. Bartlett, P., Shawe-Taylor, J. Generalization performance of support vector machines and other pattern classiﬁers. Advances in Kernel Methods. Support Vector Learning. Sch¨ lkopf, Burges and o Smola (Eds.), 1, The MIT Press, Cambridge, 1999. Borowkow, A. A. Wahrscheinlichkeits–theorie. Birkh¨ user Verlag, 1976. a Cannon, A., Ettinger, M., Hush, D., Scovel, C. Machine Learning with Data Dependent Hypothesis Classes. Journal of Machine Learning Research, 2: 335–358, 2002. Breiman, L. Prediction games and arcing algorithms. Neural Computation, 11(7): 1493–1517, 1999. Dudley, R. M. Uniform Central Limit Theorems. Cambridge University Press, 1999. Feller, W. An Introduction to probability theory and its applications, volume II. John Wiley, 1966. Gat, Y. A bound concerning the generalization ability of a certain class of learning algorithms. Technical Report 548, University of California, Berkeley, CA, 1999. Heisele, B., Serre, T., Mukherjee, S., Poggio, T. Feature Reduction and Hierarchy of Classiﬁers for Fast Object Detection in Video Images. In Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2: 18–24, 2001. Kohonen, T. The self-organizing map. In Proceedings of IEEE, 78: 1464–1479, 1990. Koltchinskii, V., Panchenko, D. Empirical margin distributions and bounding the generalization error of combined classiﬁers. The Annals of Statistics, 30(1), 2002. Koltchinskii, V., Panchenko, D. Complexities of convex combinations and bounding the generalization error in classiﬁcation. submitted, 2004. Koltchinskii, V., Panchenko, D., Lozano, F. Bounding the generalization error of convex combinations of classiﬁers: balancing the dimensionality and the margins. The Annals of Applied Probability, 13(1), 2003a. 339  A NDONOVA  Koltchinskii, V., Panchenko, D., Andonova, S. Generalization bounds for voting classiﬁers based on sparsity and clustering. In Proceedings of the Annual Conference on Computational Learning Theory, Lecture Notes in Artiﬁcial Intelligence, M. Warmuth and B. Schoelkopf (eds.). Springer, New York, 2003b. Littlestone, N., Warmuth, M. Relating data compression and learnability. Technical Report, University of California, Santa Cruz, CA, 1986. ˇ Panchenko, D. Some extensions of an inequality of Vapnik and Cervonenkis. Electronic Communications in Probability, 7: 55–65, 2002. Schapire, R., Freund, Y., Bartlett, P., Lee, W. S. Boosting the margin: A new explanation of effectiveness of voting methods. The Annals of Statistics, 26: 1651–1687, 1998. Schapire, R., Singer, Y. Improved Boosting Algorithms using Conﬁdence-Rated Predictions. Machine Learning, 37: 297–336, 1999. Steinwart, I. Sparseness of Support Vector Machines. Journal of Machine Learning Research, 2: 1071–1105, 2003. ˇ Vapnik, V. N., Cervonenkis, A. Ya. On the uniform convergence of relative frequencies of event to their probabilities. Soviet Math. Dokl., 9: 915 – 918, 1968. ˇ Vapnik, V. N., Cervonenkis A. Ya. Theory of Pattern Recognition. Nauka, Moscow (in Russian), 1974. Vapnik, V. Statistical Learning Theory. John Wiley & Sons, New York, 1998. Vapnik, V. Estimation of Dependencies Based on Empirical Data. SpringerVerlag, New York, 1982.  340</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
