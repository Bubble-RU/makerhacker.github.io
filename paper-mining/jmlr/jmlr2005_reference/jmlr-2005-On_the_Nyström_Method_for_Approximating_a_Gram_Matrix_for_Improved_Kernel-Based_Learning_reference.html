<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>60 jmlr-2005-On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-60" href="../jmlr2005/jmlr-2005-On_the_Nystr%C3%B6m_Method_for_Approximating_a_Gram_Matrix_for_Improved_Kernel-Based_Learning.html">jmlr2005-60</a> <a title="jmlr-2005-60-reference" href="#">jmlr2005-60-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>60 jmlr-2005-On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning</h1>
<br/><p>Source: <a title="jmlr-2005-60-pdf" href="http://jmlr.org/papers/volume6/drineas05a/drineas05a.pdf">pdf</a></p><p>Author: Petros Drineas, Michael W. Mahoney</p><p>Abstract: A problem for many kernel-based methods is that the amount of computation required to ﬁnd the solution scales as O(n3 ), where n is the number of training examples. We develop and analyze an algorithm to compute an easily-interpretable low-rank approximation to an n × n Gram matrix G such that computations of interest may be performed more rapidly. The approximation is of ˜ the form Gk = CWk+CT , where C is a matrix consisting of a small number c of columns of G and Wk is the best rank-k approximation to W , the matrix formed by the intersection between those c columns of G and the corresponding c rows of G. An important aspect of the algorithm is the probability distribution used to randomly sample the columns; we will use a judiciously-chosen and data-dependent nonuniform probability distribution. Let · 2 and · F denote the spectral norm and the Frobenius norm, respectively, of a matrix, and let Gk be the best rank-k approximation to G. We prove that by choosing O(k/ε4 ) columns G −CWk+CT ξ ≤ G − Gk ξ +ε n ∑ G2 , ii i=1 both in expectation and with high probability, for both ξ = 2, F, and for all k : 0 ≤ k ≤ rank(W ). This approximation can be computed using O(n) additional space and time, after making two passes over the data from external storage. The relationships between this algorithm, other related matrix decompositions, and the Nystr¨ m method from integral equation theory are discussed.1 o Keywords: kernel methods, randomized algorithms, Gram matrix, Nystr¨ m method o</p><br/>
<h2>reference text</h2><p>D. Achlioptas and F. McSherry. Fast computation of low rank matrix approximations. In Proceedings of the 33rd Annual ACM Symposium on Theory of Computing, pages 611–618, 2001. D. Achlioptas, F. McSherry, and B. Sch¨ lkopf. Sampling techniques for kernel methods. In Annual o Advances in Neural Information Processing Systems 14: Proceedings of the 2001 Conference, pages 335–342, 2002. Y. Azar, A. Fiat, A. R. Karlin, F. McSherry, and J. Saia. Spectral analysis of data. In Proceedings of the 33rd Annual ACM Symposium on Theory of Computing, pages 619–626, 2001. M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373–1396, 2003. A. Ben-Israel and T. N. E. Greville. Generalized Inverses: Theory and Applications. SpringerVerlag, New York, 2003. 2172  ¨ O N THE N YSTR OM M ETHOD FOR A PPROXIMATING A G RAM M ATRIX  Y. Bengio, J. F. Paiement, P. Vincent, O. Delalleau, N. Le Roux, and M. Ouimet. Out-of-sample extensions for LLE, Isomap, MDS, eigenmaps, and spectral clustering. In Annual Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference, pages 177–184, 2004. R. Bhatia. Matrix Analysis. Springer-Verlag, New York, 1997. C. J. C. Burges. Simpliﬁed support vector decision rules. In Proceedings of the 13th International Conference on Machine Learning, pages 71–77, 1996. N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines and Other Kernelbased Learning Methods. Cambridge University Press, Cambridge, 2000. L. M. Delves and J. L. Mohamed. Computational Methods for Integral Equations. Cambridge University Press, Cambridge, 1985. D. L. Donoho and C. Grimes. Hessian eigenmaps: Locally linear embedding techniques for highdimensional data. Proc. Natl. Acad. Sci. USA, 100(10):5591–5596, 2003. P. Drineas, A. Frieze, R. Kannan, S. Vempala, and V. Vinay. Clustering in large graphs and matrices. In Proceedings of the 10th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 291– 299, 1999. P. Drineas and R. Kannan. Fast Monte-Carlo algorithms for approximate matrix multiplication. In Proceedings of the 42nd Annual IEEE Symposium on Foundations of Computer Science, pages 452–459, 2001. P. Drineas and R. Kannan. Pass efﬁcient algorithms for approximating large matrices. In Proceedings of the 14th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 223–232, 2003. P. Drineas, R. Kannan, and M. W. Mahoney. Fast Monte Carlo algorithms for matrices I: Approximating matrix multiplication. Technical Report YALEU/DCS/TR-1269, Yale University Department of Computer Science, New Haven, CT, February 2004a. Accepted for publication in the SIAM Journal on Computing. P. Drineas, R. Kannan, and M. W. Mahoney. Fast Monte Carlo algorithms for matrices II: Computing a low-rank approximation to a matrix. Technical Report YALEU/DCS/TR-1270, Yale University Department of Computer Science, New Haven, CT, February 2004b. Accepted for publication in the SIAM Journal on Computing. P. Drineas, R. Kannan, and M. W. Mahoney. Fast Monte Carlo algorithms for matrices III: Computing a compressed approximate matrix decomposition. Technical Report YALEU/DCS/TR-1271, Yale University Department of Computer Science, New Haven, CT, February 2004c. Accepted for publication in the SIAM Journal on Computing. P. Drineas, R. Kannan, and M. W. Mahoney. Sampling sub-problems of heterogeneous Max-Cut problems and approximation algorithms. Technical Report YALEU/DCS/TR-1283, Yale University Department of Computer Science, New Haven, CT, April 2004d. 2173  D RINEAS AND M AHONEY  P. Drineas, R. Kannan, and M. W. Mahoney. Sampling sub-problems of heterogeneous Max-Cut problems and approximation algorithms. In Proceedings of the 22nd Annual International Symposium on Theoretical Aspects of Computer Science, pages 57–68, 2005. P. Drineas and M. W. Mahoney. Approximating a Gram matrix for improved kernel-based learning. In Proceedings of the 18th Annual Conference on Learning Theory, pages 323–337, 2005a. P. Drineas and M. W. Mahoney. On the Nystr¨ m method for approximating a Gram matrix for o improved kernel-based learning. Technical Report YALEU/DCS/TR-1319, Yale University Department of Computer Science, New Haven, CT, April 2005b. S. Fine and K. Scheinberg. Efﬁcient SVM training using low-rank kernel representations. Journal of Machine Learning Research, 2:243–264, 2001. C. Fowlkes, S. Belongie, F. Chung, and J. Malik. Spectral grouping using the Nystr¨ m method. o IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2):214–225, 2004. A. Frieze, R. Kannan, and S. Vempala. Fast Monte-Carlo algorithms for ﬁnding low-rank approximations. In Proceedings of the 39th Annual IEEE Symposium on Foundations of Computer Science, pages 370–378, 1998. G. H. Golub and C. F. Van Loan. Matrix Computations. Johns Hopkins University Press, Baltimore, 1989. S. A. Goreinov and E. E. Tyrtyshnikov. The maximum-volume concept in approximation by lowrank matrices. Contemporary Mathematics, 280:47–51, 2001. S. A. Goreinov, E. E. Tyrtyshnikov, and N. L. Zamarashkin. A theory of pseudoskeleton approximations. Linear Algebra and Its Applications, 261:1–21, 1997. J. Ham, D. D. Lee, S. Mika, and B. Sch¨ lkopf. A kernel view of the dimensionality reduction o of manifolds. Technical Report TR-110, Max Planck Institute for Biological Cybernetics, July 2003. R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, New York, 1985. S. Lafon. Diffusion Maps and Geometric Harmonics. PhD thesis, Yale University, 2004. M. Z. Nashed, editor. Generalized Inverses and Applications. Academic Press, New York, 1976. E. Osuna, R. Freund, and F. Girosi. An improved training algorithm for support vector machines. In Proceedings of the 1997 IEEE Workshop on Neural Networks for Signal Processing VII, pages 276–285, 1997. L. Rademacher, S. Vempala, and G. Wang. Matrix approximation and projective clustering via iterative sampling. Technical Report MIT-LCS-TR-983, Massachusetts Institute of Technology, Cambridge, MA, March 2005. S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by local linear embedding. Science, 290:2323–2326, 2000. 2174  ¨ O N THE N YSTR OM M ETHOD FOR A PPROXIMATING A G RAM M ATRIX  B. Sch¨ lkopf, A. Smola, and K.-R. M¨ ller. Nonlinear component analysis as a kernel eigenvalue o u problem. Neural Computation, 10:1299–1319, 1998. A. J. Smola and B. Sch¨ lkopf. Sparse greedy matrix approximation for machine learning. In o Proceedings of the 17th International Conference on Machine Learning, pages 911–918, 2000. G. W. Stewart and J. G. Sun. Matrix Perturbation Theory. Academic Press, New York, 1990. J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290:2319–2323, 2000. K. Q. Weinberger, F. Sha, and L. K. Saul. Learning a kernel matrix for nonlinear dimensionality reduction. In Proceedings of the 21st International Conference on Machine Learning, pages 839–846, 2004. C. K. I. Williams, C. E. Rasmussen, A. Schwaighofer, and V. Tresp. Observations on the Nystr¨ m o method for Gaussian process prediction. Technical report, University of Edinburgh, 2002. C. K. I. Williams and M. Seeger. The effect of the input density distribution on kernel-based classiﬁers. In Proceedings of the 17th International Conference on Machine Learning, pages 1159– 1166, 2000. C. K. I. Williams and M. Seeger. Using the Nystr¨ m method to speed up kernel machines. In Annual o Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference, pages 682–688, 2001.  2175</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
