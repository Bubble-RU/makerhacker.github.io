<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-2" href="../jmlr2005/jmlr-2005-A_Bayesian_Model_for_Supervised_Clustering_with_the_Dirichlet_Process_Prior.html">jmlr2005-2</a> <a title="jmlr-2005-2-reference" href="#">jmlr2005-2-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>2 jmlr-2005-A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior</h1>
<br/><p>Source: <a title="jmlr-2005-2-pdf" href="http://jmlr.org/papers/volume6/daume05a/daume05a.pdf">pdf</a></p><p>Author: Hal Daumé III, Daniel Marcu</p><p>Abstract: We develop a Bayesian framework for tackling the supervised clustering problem, the generic problem encountered in tasks such as reference matching, coreference resolution, identity uncertainty and record linkage. Our clustering model is based on the Dirichlet process prior, which enables us to deﬁne distributions over the countably inﬁnite sets that naturally arise in this problem. We add supervision to our model by positing the existence of a set of unobserved random variables (we call these “reference types”) that are generic across all clusters. Inference in our framework, which requires integrating over inﬁnitely many parameters, is solved using Markov chain Monte Carlo techniques. We present algorithms for both conjugate and non-conjugate priors. We present a simple—but general—parameterization of our model based on a Gaussian assumption. We evaluate this model on one artiﬁcial task and three real-world tasks, comparing it against both unsupervised and state-of-the-art supervised algorithms. Our results show that our model is able to outperform other models across a variety of tasks and performance metrics. Keywords: supervised clustering, record linkage, citation matching, coreference, Dirichlet process, non-parametric Bayesian</p><br/>
<h2>reference text</h2><p>Charles E. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. The Annals of Statistics, 2(6):1152–1174, November 1974. 1574  A BAYESIAN M ODEL FOR S UPERVISED C LUSTERING  Aharon Bar-Hillel, Tomer Hertz, Noam Shental, and Daphna Weinshall. Learning distance functions using equivalence relations. In Proceedings of the International Conference on Machine Learning (ICML), 2003. Aharon Bar-Hillel and Daphna Weinshall. Learning with equivalence constraints and the relation to multiclass learning. In Proceedings of the Conference on Computational Learning Theory (COLT), 2003. Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney. Comparing and unifying search-based and similarity-based approaches to semi-supervised clustering. In ICML Workshop on the Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining, pages 42–49, 2003. Matt Beal, Zoubin Ghahramani, and Carl Edward Rasmussen. The inﬁnite hidden Markov model. In Advances in Neural Information Processing Systems (NIPS), 2002. David Blackwell and James B. MacQueen. Ferguson distributions via P` lya urn schemes. The o Annals of Statistics, 1(2):353–355, March 1973. David Blei and Michael I. Jordan. Variational inference for Dirichlet process mixtures. Bayesian Analysis, 1(1):121–144, August 2005. Peter Carbonetto, Jacek Kisy´ ski, Nando de Freitas, and David Poole. Nonparametric bayesian n logic. In Proceedings of the Converence on Uncertainty in Artiﬁcial Intelligence (UAI), July 2005. William Cohen and Jacob Richman. Learning to match and cluster large high-dimensional data sets for data integration. In KDD, 2002. Anhai Doan, Jayant Madhavan, Pedro Domingos, and Alon Halevy. Ontology matching: A machine learning approach. In S. Staab and R. Studer, editors, Handbook on Ontologies in Information Systems, pages 397–416. Springer-Velag, 2004. Michael D. Escobar. Estimating normal means with a Dirichlet process prior. Journal of the American Statistical Association (JASA), 89(425):268–277, March 1994. Thomas S. Ferguson. A Bayesian analysis of some nonparametric problems. The Annals of Statistics, 1(2):209–230, March 1973. Thomas S. Ferguson. Prior distributions on spaces of probability measures. The Annals of Statistics, 2(4):615–629, July 1974. Thomas S. Ferguson. Bayesian density estimation by mixtures of normal distribution. In H. Rizvi and J. Rustagi, editors, Recent Advances in Statistics, pages 287–303. Academic Press, 1983. Thomas Finley and Thorsten Joachims. Supervised clustering with support vector machines. In Proceedings of the International Conference on Machine Learning (ICML), July 2005. Hermant Ishwaran and Lancelot F. James. Gibbs sampling methods for stick-breaking priors. Journal of the American Statistical Association (JASA), 96(453):161–173, March 2001. 1575  ´ DAUM E III AND M ARCU  Sonia Jain and Radford M. Neal. A split-merge markov chain Monte Carlo procedure for the Dirichlet process mixture model. Technical Report 2003, University of Toronto, Department of Statistics, 2003. Toshihiro Kamishima and Fumio Motoyoshi. Learning from cluster examples. Machine Learning (ML), pages 199–233, 2003. Steven N. MacEachern and Peter M¨ ller. Estimating mixture of Dirichlet process models. Journal u of Computational and Graphical Statistics (JCGS), 7:223–238, 1998. Andrew McCallum, Kamal Nigam, and Lyle Ungar. Efﬁcient clustering of high-dimensional data sets with application to reference matching. In KDD, 2000. Andrew McCallum and Ben Wellner. Conditional models of identity uncertainty with application to noun coreference. In Advances in Neural Information Processing Systems (NIPS), 2004. Marina Meila. Comparing clusterings. In Proceedings of the Conference on Computational Learning Theory (COLT), 2003. Thomas Minka and Zoubin Ghahramani. Expectation propagation for inﬁnite mixtures. In NIPS Workshop on Nonparametric Bayesian Methods and Inﬁnite Models, 2004. Alvaro E. Monge and Charles Elkan. An efﬁcient domain-independent algorithm for detecting approximately duplicate database records. In KDD, 1997. Radford M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Technical Report 9815, University of Toronto, September 1998. Vincent Ng and Claire Cardie. Improving machine learning approaches to coreference resolution. In Proceedings of the Conference of the Association for Computational Linguistics (ACL), 2002. Patrick Pantel. Clustering by Committee. PhD thesis, University of Alberta, 2003. Hanna Pasula, Bhaskara Marthi, Brian Milch, Stuart Russell, and Ilya Shpitser. Identity uncertainty and citation matching. In Advances in Neural Information Processing Systems (NIPS), 2003. Jim Pitman. Some developments of the Blackwell-MacQueen urn scheme. In Statistics, Probability and Game Theory; Papers in honor of David Blackwell Lecture Notes, Monograph Series 30: 245–267, 1996. W. M. Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association (JASA), 66:846–850, 1971. Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521 – 544, 2001. Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasmine Altun. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the International Conference on Machine Learning (ICML), 2004. 1576  A BAYESIAN M ODEL FOR S UPERVISED C LUSTERING  USPS digits database. United states postal service handwritten zip code database. Made available by the USPS Ofﬁce of Advanced Technology, 1987. Mike West. Hyperparameter estimation in Dirichlet process mixture models. ISDS Discussion Paper #92-A03, 1992. Duke University. Eric P. Xing, Andrew Ng, Michael I. Jordan, and Stuart Russell. Distance metric learning, with application to clustering with side-information. In Advances in Neural Information Processing Systems (NIPS), 2003. Eric P. Xing, Roded Sharan, and Michael I. Jordan. Bayesian haplotype inference via the Dirichlet process. In Proceedings of the International Conference on Machine Learning (ICML), 2004.  1577</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
