<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-42" href="../jmlr2005/jmlr-2005-Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.html">jmlr2005-42</a> <a title="jmlr-2005-42-reference" href="#">jmlr2005-42-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>42 jmlr-2005-Large Margin Methods for Structured and Interdependent Output Variables</h1>
<br/><p>Source: <a title="jmlr-2005-42-pdf" href="http://jmlr.org/papers/volume6/tsochantaridis05a/tsochantaridis05a.pdf">pdf</a></p><p>Author: Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, Yasemin Altun</p><p>Abstract: Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing ﬂexible and powerful input representations, this paper addresses the complementary issue of designing classiﬁcation algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classiﬁcation problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.</p><br/>
<h2>reference text</h2><p>Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden Markov support vector machines. In Proceedings of the Twentieth International Conference on Machine Learning, 2003. L. Cai and T. Hofmann. Hierarchical document categorization with support vector machines. In Proceedings of the ACM Thirteenth Conference on Information and Knowledge Management, 2004. W. Cohen, R. Shapire, and Y. Singer. Learning to order things. Journal of Artiﬁcial Intelligence Research, 10:243–270, 1999. M. Collins. Discriminative reranking for natural language parsing. In Proceedings of the Seventieth International Conference on Machine Learning, 2000. M. Collins. Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2002. M. Collins and N. Duffy. Convolution kernels for natural language. In Advances in Neural Information Processing Systems 14, pages 625–632, 2002a. M. Collins and N. Duffy. New ranking algorithms for parsing and tagging: kernels over discrete structures, and the voted perceptron. In Proceedings of the Fortieth Annual Meeting of the Association for Computational Linguistics, 2002b. K. Crammer and Y. Singer. On the algorithmic implementation of multi-class kernel-based vector machines. Machine Learning Research, 2:265–292, 2001. K. Crammer and Y. Singer. Pranking with ranking. In Advances in Neural Information Processing Systems 14, 2002. R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. Biological Sequence Analysis. Cambridge University Press, 1998. G. D. Forney Jr. The Viterbi algorithm. Proceedings of the IEEE, 61:268–278, 1973. 1482  L ARGE M ARGIN M ETHODS FOR S TRUCTURED AND I NTERDEPENDENT O UTPUT VARIABLES  M. Gr¨ tschel, L. Lov` tz, and A. Schrijver. The ellipsoid method and its consequences in combinao a torial optimization. Combinatorica, 1(2):169–197, 1981. S. Har-Peled, D. Roth, and D. Zimak. Constraint classiﬁcation for multiclass classiﬁcation and ranking. In Advances in Neural Information Processing Systems 14, 2002. R. Herbrich, T. Graepel, and K. Obermayer. Large margin rank boundaries for ordinal regression. In Advances in Large Margin Classiﬁers. MIT Press, Cambridge, MA, 2000. T. Hofmann, I. Tsochantaridis, and Y. Altun. Learning over structured output spaces via joint kernel functions. In Proceedings of the Sixth Kernel Workshop, 2002. T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining, 2002. T. Joachims. Learning to align sequences: A maximum-margin approach. Technical report, Cornell University, 2003. T. Joachims. A support vector method for multivariate performance measures. In Proceedings of the Twenty-Second International Conference on Machine Learning, 2005. M. Johnson. PCFG models of linguistic tree representations. Computational Linguistics, 24(4): 613–632, 1998. N. Karmarkar. A new polynomial-time algorithm for linear programming. Combinatorica, 4(4): 373–396, 1984. J. E. Kelley. The cutting-plane method for solving convex programs. Journal of the Society for Industrial Applied Mathematics, 8:703–712, 1960. J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289, 2001. C. D. Manning and H. Schuetze. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA, 1999. E. Ristad and P. Yianilos. Learning string edit distance. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 287–295, 1997. R. E. Schapire and Y. Singer. Boostexter: A boosting-based system for text categorization. Machine Learning, 39(2-3):135–168, 2000. R. Schwarz and Y. L. Chow. The n-best algorithm: An efﬁcient and exact procedure for ﬁnding the n most likely hypotheses. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 81–84, 1990. A. Smola and T. Hofmann. Exponential families for generative and discriminative estimation. Technical report, 2003. 1483  T SOCHANTARIDIS , J OACHIMS , H OFMANN AND A LTUN  B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In Advances in Neural Information Processing Systems 16, 2004a. B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning. Max-Margin parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2004b. I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the Twenty-First International Conference on Machine Learning, 2004. V. Vapnik. Statistical Learning Theory. Wiley and Sons Inc., New York, 1998. J. Weston, O. Chapelle, A. Elisseeff, B. Sch¨ lkopf, and V. Vapnik. Kernel dependency estimation. o In Advances in Neural Information Processing Systems 15, 2003. J. Weston and C. Watkins. Multi-class support vector machines. Technical Report CSD-TR-98-04, Department of Computer Science, Royal Holloway, University of London, 1998. D. H. Younger. Recognition and parsing of context-free languages in time n3 . Information and Control, 2(10):189–208, 1967.  1484</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
