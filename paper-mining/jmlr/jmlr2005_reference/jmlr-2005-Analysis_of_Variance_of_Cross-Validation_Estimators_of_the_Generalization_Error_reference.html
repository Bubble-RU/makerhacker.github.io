<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-13" href="../jmlr2005/jmlr-2005-Analysis_of_Variance_of_Cross-Validation_Estimators_of_the_Generalization_Error.html">jmlr2005-13</a> <a title="jmlr-2005-13-reference" href="#">jmlr2005-13-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>13 jmlr-2005-Analysis of Variance of Cross-Validation Estimators of the Generalization Error</h1>
<br/><p>Source: <a title="jmlr-2005-13-pdf" href="http://jmlr.org/papers/volume6/markatou05a/markatou05a.pdf">pdf</a></p><p>Author: Marianthi Markatou, Hong Tian, Shameek Biswas, George Hripcsak</p><p>Abstract: This paper brings together methods from two different disciplines: statistics and machine learning. We address the problem of estimating the variance of cross-validation (CV) estimators of the generalization error. In particular, we approach the problem of variance estimation of the CV estimators of generalization error as a problem in approximating the moments of a statistic. The approximation illustrates the role of training and test sets in the performance of the algorithm. It provides a unifying approach to evaluation of various methods used in obtaining training and test sets and it takes into account the variability due to different training and test sets. For the simple problem of predicting the sample mean and in the case of smooth loss functions, we show that the variance of the CV estimator of the generalization error is a function of the moments of the random T T variables Y = Card(S j S j ) and Y ∗ = Card(Sc Sc ), where S j , S j are two training sets, and Sc , j j j c are the corresponding test sets. We prove that the distribution of Y and Y* is hypergeometric Sj and we compare our estimator with the one proposed by Nadeau and Bengio (2003). We extend these results in the regression case and the case of absolute error loss, and indicate how the methods can be extended to the classiﬁcation case. We illustrate the results through simulation. Keywords: cross-validation, generalization error, moment approximation, prediction, variance estimation</p><br/>
<h2>reference text</h2><p>Y. Bengio and Y. Grandvalet. No unbiased estimator of the variance of k-fold cross validation. Journal of Machine Learning Research, 5: 1089-1105, 2004. P. J. Bickel and K. A. Doksum. Mathematical Statistics, Prentice Hall, 2001. L. Breiman. Heuristics of instability and stabilization in model selection. The Annals of Statistics, 24: 2350-2383, 1996. H. Cramer. Mathematical Methods of Statistics. Princeton University Press, 19th Printing, 1999. T. G. Dietterich. Approximate statistical tests for comparing supervised classiﬁcation learning algorithms. Neural Computation, 10: 1895-1923, 1998. B. Efron. Estimating the error rate of a predication rule: Improvement on cross-validation. Journal of the American Statistical Association, 78: 316-331, 1983. B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap. Chapman and Hall, 1993. B. Efron. The Estimation of Prediction Error: Covariance Penalties and Cross-Validation. Journal of the American Statistical Association, 99: 619-632, 2004. T. Hastie, R. Tibshirani and J. Friedman. The Elements of Statistical Learning: Data Mining, Inference and Prediction. Springer-Verlag, 2001. K. Hitomi and M. Kagihara. Calculation methods for nonlinear dynamic least absolute deviations estimator. Journal of the Japan. Statist. Society, 31: 39-51, 2001. A. D. Ioffe and J-P. Penot. Limiting subhessians, limiting subjets and their calculus. Transactions of the American Mathematical Society , 349: 789-807, 1997. G. M. James. Variance and bias for general loss functions. Machine Learning, 51: 115-135, 2003. M. Kearns. A bound on the error of cross validation with consequences for the training-test split. In Advances in Neural Information Processing Systems, 8: 183-189, 1996.  1167  M ARKATOU , T IAN , B ISWAS AND H RIPCSAK  M. Kearns and D. Ron. Algorithmic stability and sanity-check bounds for leave-one-out cross validation. Neural Computation, 11: 1427-1453, 1999. R. A. Khan. Approximation of the expectation of a function of the sample mean. Statistics, 38: 117-122, 2004. R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection. In The International Joint Conference on Artiﬁcial Intelligence, 1137-1143, 1995. E. L. Lehmann. Theory of Point Estimation. Wiley and Sons, 1983. G. J. McLachlan. An asymptotic expansion for the variance of the errors of misclassiﬁcation of the linear discriminant function. Australian Journal of Statistics, 14: 68-72, 1972. G. J. Mclachlan. An asymptotic expansion of the expectation of the estimated error rate in discriminant analysis. Australian Journal of Statistics, 15: 210-214, 1974. G. J. McLachlan. The asymptotic distributions of the conditional error rate and risk in discriminant analysis. Biometrika, 61: 131-135, 1974. G. J. McLachlan. The bias of the apparent error rate in discriminant analysis. Biometrika, 63: 239244, 1976. C. Nadeau and Y. Bengio. Inference for the generalization error. Machine Learning, 52: 239-281, 2003. R. R. Picard and R. D. Cook. Cross validation of regression models. Journal of the American Statistical Association, 79: 575-583, 1984. J. Piper. Variability and bias in experimentally measured classiﬁer error rates. Pattern Recognition Letters , 13 : 685-692, 1992. E. Ronchetti and L. Ventura. Between stability and higher order asymptotics .Statistics and Computing, 11: 67-73, 2001. P. K. Sen and J. M. Singer. Large Sample Methods in Statistics: An Introduction with Applications. Chapman and Hall, 1993.  1168</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
