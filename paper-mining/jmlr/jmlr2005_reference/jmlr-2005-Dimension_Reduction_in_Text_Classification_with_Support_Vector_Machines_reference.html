<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-27" href="../jmlr2005/jmlr-2005-Dimension_Reduction_in_Text_Classification_with_Support_Vector_Machines.html">jmlr2005-27</a> <a title="jmlr-2005-27-reference" href="#">jmlr2005-27-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>27 jmlr-2005-Dimension Reduction in Text Classification with Support Vector Machines</h1>
<br/><p>Source: <a title="jmlr-2005-27-pdf" href="http://jmlr.org/papers/volume6/kim05a/kim05a.pdf">pdf</a></p><p>Author: Hyunsoo Kim, Peg Howland, Haesun Park</p><p>Abstract: Support vector machines (SVMs) have been recognized as one of the most successful classiﬁcation methods for many applications including text classiﬁcation. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efﬁciently handle a large number of terms in practical applications of text classiﬁcation. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classiﬁcation algorithm and support vector classiﬁers to handle the classiﬁcation problem where a document may belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efﬁciency for both training and testing can be achieved without sacriﬁcing prediction accuracy of text classiﬁcation even when the dimension of the input space is signiﬁcantly reduced. Keywords: dimension reduction, support vector machines, text classiﬁcation, linear discriminant analysis, centroids</p><br/>
<h2>reference text</h2><p>[1] M. W. Berry, Z. Drmac, and E. R. Jessup. Matrices, vector spaces, and information retrieval. SIAM Review, 41:335–362, 1999.</p>
<p>[2] M. W. Berry, S. T. Dumais, and G. W. O’Brien. Using linear algebra for intelligent information retrieval. SIAM Review, 37:573–595, 1995.</p>
<p>[3] M. W. Berry and R. D. Fierro. Low-rank orthogonal decompositions for information retrieval applications. Numerical Linear Algebra with Applications, 3(4):301–327, 1996. ˚</p>
<p>[4] A. Bj¨ rck. Numerical Methods for Least Square Problems. SIAM, Philadelphia, PA, 1996. o</p>
<p>[5] N. Cristianini and J. Shawe-Taylor. Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press, 2000.</p>
<p>[6] S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. Indexing by latent semantic analysis. Journal of the Society for Information Science, 41:391-407, 1990.</p>
<p>[7] K. Fukunaga, Introduction to Statistical Pattern Recognition, Second ed., Academic Press, 1990.</p>
<p>[8] G. H. Golub and C. F. Van Loan. Matrix Computations, third edition. Johns Hopkins University Press, Baltimore, 1996. 51  K IM , H OWLAND AND PARK</p>
<p>[9] M. Heiler. Optimization Criteria and Learning Algorithms for Large Margin Classiﬁers. Diploma Thesis, University of Mannheim., 2002.</p>
<p>[10] P. Howland, M. Jeon, and H. Park. Structure Preserving Dimension Reduction for Clustered Text Data based on the Generalized Singular Value Decomposition. SIAM Journal of Matrix Analysis and Applications, 25(1):165–179, 2003.</p>
<p>[11] P. Howland and H. Park. Generalizing discriminant analysis using the generalized singular value decomposition, IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(8): 995-1006, 2004.</p>
<p>[12] M. Jeon, H. Park, and J. B. Rosen. Dimensional reduction based on centroids and least squares for efﬁcient processing of text data. In Proceedings for the First SIAM International Workshop on Text Mining. Chicago, IL, 2001.</p>
<p>[13] T. Joachims. Text categorization with support vector machines: Learning with many relevant features. In Proceedings of the European Conference on Machine Learning, pages 137–142, Berlin, 1998.</p>
<p>[14] H. Lodhi, N. Cristianini, J. Shawe-Taylor, and C. Watkins. Text classiﬁcation using string kernels. Advances in Neural Information Processing Systems, 13:563–569, 2000.</p>
<p>[15] C. C. Paige and M. A. Saunders, Towards a generalized singular value decomposition, SIAM Journal of Numerical Analysis, 18, pp. 398–405, 1981.</p>
<p>[16] H. Park, M. Jeon, and J. B. Rosen. Lower dimensional representation of text data based on centroids and least squares, BIT Numerical Mathematics, 42(2):1–22, 2003.</p>
<p>[17] H. Park and L. Eld´ n. Downdating the rank-revealing URV decomposition. SIAM Journal of e Matrix Analysis and Applications, 16, pp. 138–155, 1995.</p>
<p>[18] C. Park and H. Park. Nonlinear feature extraction based on centroids and kernel functions. Pattern Recognition, to appear.</p>
<p>[19] C. Park and H. Park. Kernel discriminant analysis based on the generalized singular value decomposition. Technical report 03-017, Department of Computer Science and Engineering, University of Minnesota, 2003.</p>
<p>[20] G. Salton, The SMART Retrieval System, Prentice Hall, 1971.</p>
<p>[21] G. Salton and M. J. McGill, Introduction to Modern Information Retrieval, McGraw-Hill, 1983.</p>
<p>[22] G. W. Stewart. An updating algorithm for subspace tracking. IEEE Transactions on Signal Processing, 40:1535–1541, 1992.</p>
<p>[23] G. W. Stewart. Updating URV decompositions in parallel. Parallel Computing, 20(2):151– 172, 1994.</p>
<p>[24] M. Stewart and P. Van Dooren. Updating a generalized URV decomposition. SIAM Journal of Matrix Analysis and Applications, 22(2):479–500, 2000. 52  D IMENSION R EDUCTION IN T EXT C LASSIFICATION WITH SVM S</p>
<p>[25] S. Theodoridis and K. Koutroumbas, Pattern Recognition, Academic Press, 1999.</p>
<p>[26] C. J. van Rijsbergen. Information Retrieval. Butterworths, London, 1979.</p>
<p>[27] V. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, New York, 1995.</p>
<p>[28] V. Vapnik. Statistical Learning Theory. John Wiley & Sons, New York, 1998.</p>
<p>[29] Y. Yang and X. Liu. A re-examination of text categorization methods. In 22nd Annual International SIGIR, pages 42–49, Berkeley, August 1999.  53</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
