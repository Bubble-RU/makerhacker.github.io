<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>55 jmlr-2005-Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2005" href="../home/jmlr2005_home.html">jmlr2005</a> <a title="jmlr-2005-55" href="../jmlr2005/jmlr-2005-Matrix_Exponentiated_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">jmlr2005-55</a> <a title="jmlr-2005-55-reference" href="#">jmlr2005-55-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>55 jmlr-2005-Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection</h1>
<br/><p>Source: <a title="jmlr-2005-55-pdf" href="http://jmlr.org/papers/volume6/tsuda05a/tsuda05a.pdf">pdf</a></p><p>Author: Koji Tsuda, Gunnar Rätsch, Manfred K. Warmuth</p><p>Abstract: We address the problem of learning a symmetric positive deﬁnite matrix. The central issue is to design parameter updates that preserve positive deﬁniteness. Our updates are motivated with the von Neumann divergence. Rather than treating the most general case, we focus on two key applications that exemplify our methods: on-line learning with a simple square loss, and ﬁnding a symmetric positive deﬁnite matrix subject to linear constraints. The updates generalize the exponentiated gradient (EG) update and AdaBoost, respectively: the parameter is now a symmetric positive deﬁnite matrix of trace one instead of a probability vector (which in this context is a diagonal positive definite matrix with trace one). The generalized updates use matrix logarithms and exponentials to preserve positive deﬁniteness. Most importantly, we show how the derivation and the analyses of the original EG update and AdaBoost generalize to the non-diagonal case. We apply the resulting matrix exponentiated gradient (MEG) update and DeﬁniteBoost to the problem of learning a kernel matrix from distance measurements.</p><br/>
<h2>reference text</h2><p>O. E. Barndorff-Nielsen, R. D. Gill, and P. E. Jupp. On quantum statistical inference. J. R. Statist. Soc. B, 65(4):775–816, 2003. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004. L. M. Bregman. The relaxation method of ﬁnding the common point of convex sets and its application to the solution of problems in convex programming. USSR Computational Mathematics and Physics, 7:200–217, 1967. Y. Censor and A. Lent. An iterative row-action method for interval convex programming. Journal of Optimization Theory and Applications, 34(3):321–353, July 1981. Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997. 1016  M ATRIX E XPONENTIATED G RADIENT U PDATES  S. Golden. Lower bounds for the Helmholtz function. Phys. Rev., 137:B1127–B1128, 1965. D. Helmbold, R. E. Schapire, Y. Singer, and M. K. Warmuth. A comparison of new and old algorithms for amixture estimation problem. Machine Learning, 27(1):97–119, 1997. P. J. Huber. Robust Statistics. John Wiley and Sons, New York, 1981. J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1–63, 1997. J. Kivinen and M. K. Warmuth. Boosting as entropy projection. In Proceedings of the 12th Annual Conference on Computational Learning Theory, pages 134–144. ACM Press, New York, NY, 1999. J. Kivinen and M. K. Warmuth. Relative loss bounds for multidimensional regression problems. Machine Learning, 45(3):301–329, 2001. J. Lafferty. Additive models, boosting, and inference for generalized divergences. In Proceedings of the 12th Annual Conference on Computational Learning Theory, pages 125–133. ACM Press, New York, NY, 1999. N. Littlestone. Learning when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2:285–318, 1988. N. Littlestone. Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. PhD thesis, Technical Report UCSC-CRL-89-11, University of California, Santa Cruz, 1989. N. Littlestone, P. M. Long, and M. K. Warmuth. On-line learning of linear functions. Technical Report UCSC-CRL-91-29, University of California, Santa Cruz, May 1992. M. A. Nielsen and I. L. Chuang. Quantum Computation and Quantum Information. Cambridge University Press, 2000. G. R¨ tsch. Robust Boosting via Convex Optimization. PhD thesis, University of Potsdam, Potsdam, a Germany, October 2001. G. R¨ tsch and M. K. Warmuth. Maximizing the margin with boosting. In Proceedings of the a 15th Annual Conference on Computational Learning Theory, pages 319–333. Springer, Sydney, Australia, 2002. G. R¨ tsch and M. K. Warmuth. Efﬁcient margin maximization with boosting. submitted to Journal a of Machine Learning Research, 2005. R. E. Schapire and Y. Singer. Improved boosting algorithms using conﬁdence-rated predictions. Machine Learning, 37:297–336, 1999. B. Sch¨ lkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002. o S. Shai-Shwartz, Y. Singer, and A. Y. Ng. Online and batch learning of pseudo-metrics. In C. E. Brodley, editor, Machine Learning, Proceedings of the Twenty-ﬁrst International Conference (ICML 2004). ACM Press, New York, NY, 2004. 1017  ¨ T SUDA , R ATSCH AND WARMUTH  Y. Singer and M. K. Warmuth. Batch and on-line parameter estimation of Gaussian mixtures based on the joint entropy. In M. S. Kearns, S. A. Solla, and D. A. Cohn, editors, Advances in Neural Information Processing Systems 11 (NIPS’98), pages 578–584. MIT Press, 1999. I. W. Tsang and J. T. Kwok. Distance metric learning with kernels. In Proceedings of the International Conference on Artiﬁcial Neural Networks (ICANN’03), pages 126–129. Springer Verlag, New York, NY, 2003. K. Tsuda, S. Akaho, and K. Asai. The em algorithm for kernel matrix completion with auxiliary data. Journal of Machine Learning Research, 4:67–81, May 2003. K. Tsuda and W. S. Noble. Learning kernels from biological networks by maximizing entropy. Bioinformatics, 20(Suppl. 1):i326–i333, 2004. E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance metric learning with application to clustering with side-information. In S. Thrun S. Becker and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 505–512. MIT Press, Cambridge, MA, 2003.  1018</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
