<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-14" href="#">jmlr2006-14</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-14-pdf" href="http://jmlr.org/papers/volume7/scheinberg06a/scheinberg06a.pdf">pdf</a></p><p>Author: Katya Scheinberg</p><p>Abstract: We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efﬁcient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims’ SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difﬁcult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight , the generalization properties of these two approaches are identical and we do not discuss them in the paper. Keywords: active set methods, support vector machines, quadratic programming</p><p>Reference: <a title="jmlr-2006-14-reference" href="../jmlr2006_reference/jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Bennett and Emilio Parrado-Hern´ ndez a  Abstract We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. [sent-6, score-0.181]
</p><p>2 Until recently the traditional active set methods were considered impractical for large SVM problems. [sent-9, score-0.148]
</p><p>3 Keywords: active set methods, support vector machines, quadratic programming  1. [sent-17, score-0.181]
</p><p>4 Introduction In this paper we introduce an active set method to solve the following convex quadratic programming (QP) optimization problem which is deﬁned by the classic soft margin SVM formulation (see, for example, Cristianini and Shawe-Taylor, 2000). [sent-18, score-0.196]
</p><p>5 max  T 1 T − α Qα − c ξ 2 −Qα + by + s − ξ = −e,  (1)  0 ≤ α ≤ c, s ≥ 0, ξ ≥ 0, where α ∈ Rn is the vector of the dual variables, b is the bias (scalar) and s and ξ are the ndimensional vectors of the slack and the surplus variables, respectively. [sent-19, score-0.101]
</p><p>6 General convex QPs are typically solved by one of the two approaches: interior point method approach or active set method approach. [sent-34, score-0.175]
</p><p>7 If the problem is of moderate size but the matrices are dense, then active set method is preferable. [sent-36, score-0.148]
</p><p>8 However, if the rank of Q is high, an active set approach seems to remain the only main alternative. [sent-40, score-0.148]
</p><p>9 One of the most “traditional” active set methods in the optimization literature is the Simplex method for linear programming (LP) problems. [sent-41, score-0.165]
</p><p>10 The main idea of this method in the context of SVM is to ﬁx, at each iteration, all variables in the current dual active set1 at their current values (0 or c), and then to solve the reduced dual problem. [sent-47, score-0.317]
</p><p>11 After obtaining a solution - decide whether it is optimal for the overall dual problem (same as being feasible for the overall primal problem), or if any of the dual variables should be released from the active set. [sent-48, score-0.301]
</p><p>12 When applied to SVM, this approach poses the following problem: if the complement of the dual active set (the set of “free” variables) has large cardinality, then solving the restricted subproblems may be too expensive, since Q is completely dense. [sent-49, score-0.214]
</p><p>13 Also determining the next variable to leave the active set may be expensive for the same reason. [sent-50, score-0.148]
</p><p>14 The most common approach to large SVM problems is to use a restricted active set method, such as chunking (Boser et al. [sent-52, score-0.148]
</p><p>15 The dual active set is the set of dual variables α whose values are at their bound. [sent-55, score-0.28]
</p><p>16 There are a few skillfully implemented SVM solvers based on this type of restricted active set methods (Joachims, 1999; Platt, 1999). [sent-58, score-0.148]
</p><p>17 2 A full active set method, such as the one presented in this paper, avoids these disadvantages. [sent-61, score-0.148]
</p><p>18 First we notice that a support vector that violates the margin constraint (that is the ξ surplus variable is positive) corresponds to a variable α which is at its upper bound and therefore is in the dual active set. [sent-64, score-0.264]
</p><p>19 The complement of the dual active set contains variables α that are strictly between the upper and lower bounds. [sent-65, score-0.214]
</p><p>20 The current number of such support vectors, ns , is the size of the reduced QP (RQP). [sent-67, score-0.336]
</p><p>21 Moreover, the active set is incremented only by one variable at a time (either one variable leaving, or one entering the active set), hence we can store and update a factorization of the reduced matrix Q. [sent-70, score-0.41]
</p><p>22 In the latter case this variable is included into the active set and the process repeats. [sent-73, score-0.148]
</p><p>23 If the search for the optimum of the RQP subproblem is terminated then our method determines whether the primal feasibility was achieved and if not, which dual variable should leave the active set. [sent-80, score-0.235]
</p><p>24 The multiple updates to the active set, which are used in “chunking” and “decomposition” methods could still have an advantage if the overall number of iterations were signiﬁcantly smaller than in the case of single updates. [sent-84, score-0.185]
</p><p>25 Essentially SMO is an active set method in which the chunk size is ﬁxed to be the smallest possible, namely 2. [sent-91, score-0.173]
</p><p>26 The most expensive step of our algorithm (and of SVMlight , in fact) is pricing the primal constraints and choosing the next constraint to enter the active set. [sent-99, score-0.227]
</p><p>27 One of these approaches is shrinking, which is used by SVM light , and the other is sprint which is an industry standard in advanced implementations of LP solvers (Bixby et al. [sent-101, score-0.116]
</p><p>28 We observe that sprint appears to work better than shrinking on difﬁcult SVM problems. [sent-103, score-0.118]
</p><p>29 (2001) a randomized active set algorithm for SVM is introduced and shown to have a quasi-linear average complexity. [sent-109, score-0.148]
</p><p>30 Recently, active set methods for SVM similar to ours were used in Cauwenberghs and Poggio (2001) for incremental learning and in Hastie et al. [sent-112, score-0.186]
</p><p>31 Their methods, unlike ours, require primal and dual feasibility to be satisﬁed at every iteration and progress by changing the optimization problem itself (in a manner dictated by the respective uses of their methods). [sent-114, score-0.122]
</p><p>32 Though we choose to focus on one speciﬁc active set method in this paper, we believe the the experience we present here will be useful for other active set methods for SVM problems. [sent-116, score-0.296]
</p><p>33 Some of the ideas presented in the paper to improve the efﬁciency of the active set methods for SVMs were also suggested in Kaufman (1998). [sent-117, score-0.148]
</p><p>34 In the next section we introduce the dual active set method for the soft-margin SVM problem and describe the details of solving the reduced QP problem. [sent-119, score-0.251]
</p><p>35 We will refer to Is as the primal active set and to I0 ∪ Ic as the dual active set. [sent-140, score-0.383]
</p><p>36 cc is the part of vector c indexed by Ic and by e we denote a vector of all ones whose size is determined by the context. [sent-143, score-0.209]
</p><p>37 Then if we ﬁx the variables in the dual active set then our dual problem reduces to minαs s. [sent-150, score-0.28]
</p><p>38 T T 1 T αs Qss αs + cc Qcs αs − e αs 2 T T ys αs = −yc cc ,  0 ≤ αs ≤ c. [sent-152, score-0.447]
</p><p>39 T T 1 T αs Qss αs + c Qcs αs − e αs 2 T T ys αs = −yc cc . [sent-156, score-0.258]
</p><p>40 This solution satisﬁes the KKT conditions: T  T  T  −Qss αs + ys β = −e + cc Qcs αs T  T  ys αs = −cc yc , or, in matrix form, −Qss ys T 0 ys  αs β  =  −e + Qsc cc T −cc yc  . [sent-174, score-0.776]
</p><p>41 (5)  Since we are considering the case when Qss is nonsingular, we can ﬁnd β by taking the Schur complement of the above system T  T  T  (ys Q−1 ys )β = y Q−1 (−e + Qsc cc ) − cc yc . [sent-175, score-0.508]
</p><p>42 ss ss 2242  ACTIVE S ET M ETHOD FOR SVM S  T  T  Consider the Cholesky factorization Qss = Ls Ls and denote Ls −1 ys by r1 and Ls −1 (−e + cc Qcs ) by r2 . [sent-176, score-0.437]
</p><p>43 Then, subject to permutation and without loss of generality, its Cholesky factorization can be written as Ls 0 T ls 0  Qss =  T  Ls 0  ls 0  ,  where Ls ∈ R(ns −1)×(ns −1) and Ls ∈ Rns −1 . [sent-188, score-0.851]
</p><p>44 Let r1 = Ls −1 y1:ns −1 and r2 = Ls −1 (−e + Qsc cc )1:ns −1 . [sent-190, score-0.189]
</p><p>45 By expressing α1:ns −1 in the above system through αns and β, and by consecutively eliminating αns we obtain T  T  (−ls r1 + yns )β = (−e + Qsc cc )ns − ls r2 . [sent-191, score-0.677]
</p><p>46 T  (a) If ls r1 = yns then system (5) still has a unique solution T  β =  (−e + Qsc cc )ns − ls r2 , T −ls r1 + yns cc yc + r1 r2 − βr1 r1 T −r1 ls + yns T  αn s  =  −T  T  T  αs = Ls (−ls αns + r1 β − r2 ). [sent-193, score-1.903]
</p><p>47 2243  S CHEINBERG  T  (b) If ls r1 +yns = 0 then system (5) is singular, hence we are looking for an inﬁnite direction T T d. [sent-194, score-0.399]
</p><p>48 It can be easily shown that Qss ds = 0 from T the form of the factorization of Qss , and it can be easily shown that ys ds = 0 from the T fact that ls r1 + yns = 0. [sent-196, score-0.64]
</p><p>49 , we can write its Cholesky factorization as  T Ls 0 0 Ls T Qss =  ls1 0 0   0 T 0 ls 2 0 0   ls 1 0 0   ls 2 0 , 0  where Ls ∈ Rns −2×ns −2 , ls1 , ls2 ∈ Rns −2 . [sent-202, score-1.25]
</p><p>50 T  (a) If ls2 r1 = yns then the following direction T  d = (Ls  −T  yn −1 − ls1 r1 (ls1 − ρls2 ), −1, ρ), where ρ = s T y ns − l s 2 r 1  is an inﬁnite direction. [sent-205, score-0.388]
</p><p>51 , the factorization of Qss : Ls T  ls Qss =  T1  l s2 Hs   0 0 0 0  0 0 0 0   T 0 Ls 0  0  0  0 0 0 2244  ls 1 0 0 0  ls 2 0 0 0  T  Hs 0  , 0  0  ACTIVE S ET M ETHOD FOR SVM S  where Ls ∈ Rns −k×ns −k , ls1 , ls2 ∈ Rns −k and Hs ∈ Rns −k×k−2 . [sent-222, score-1.25]
</p><p>52 We generate the inﬁnite direction for the ﬁrst ns − k + 2 variables exactly as it is done in Case 2 and we do not change the last k − 2 variables. [sent-223, score-0.299]
</p><p>53 Hence, from each iteration to the next, Q ss changes by an addition and/or a deletion of one row and column. [sent-227, score-0.107]
</p><p>54 Instead of recomputing the Cholesky factorization each time, which would require O(n3 ) operations, it is more efﬁcient to keep the Cholesky factorization of Q ss and update it s accordingly when a row and a column are added to or deleted from Q ss . [sent-228, score-0.273]
</p><p>55 Let qs ∈ Rns +1 be the new row (column) that is added to Qss . [sent-233, score-0.112]
</p><p>56 Aside from possible numerical issues, which we discuss later, qs can be added as the last row and column of Qss . [sent-234, score-0.129]
</p><p>57 Then the Cholesky factorization of the new matrix is Ls Ls −1 (qs )1:ns −T T 0 (qs )2s +1 − (qs )1:ns Ls Ls −1 (qs )1:ns n  ,  where (qs )1:ns are the ﬁrst ns components of the vector qs and (qs )ns +1 is its last component. [sent-235, score-0.44]
</p><p>58 In this case we permute the rows and columns of Qss so that the dependent column and row are at the end of Qss and inserted column and row are placed in the one before last positions. [sent-238, score-0.1]
</p><p>59 In such a case refactorization of several rows of L s might be required even if only one row and column are added to Q ss . [sent-241, score-0.122]
</p><p>60 Remark 2 If ns is very large and is comparable to n then even storing and updating the Cholesky factors of Qss become too expensive compared to solving the entire problem. [sent-265, score-0.299]
</p><p>61 3 Updating Ic Finally we discuss a trivial but useful updates to Qsc cc , Q0c cc and Qcc cc when the set Ic is either increased or decreased by one element. [sent-268, score-0.583]
</p><p>62 We maintain vector Q c cc throughout the algorithm, when index i is added to Ic then a ci multiple of the i-th column of Q is added to Qc cc . [sent-269, score-0.41]
</p><p>63 If index i is removed from Ic , then such a vector is subtracted from Qc cc . [sent-270, score-0.189]
</p><p>64 We examined performances on an arbitrary binary classiﬁcation problem which was set to separate the letter “G” from all the other letters. [sent-283, score-0.161]
</p><p>65 For each data set we used a selection of kernels and parameters to demonstrate how the performance of the methods is affected by ns - the number of support vectors at the margin, and nc - the number of support vectors at the upper bound. [sent-311, score-0.342]
</p><p>66 For instance web 100 10 stands for the web data set with parameter σ = 100 and C = 10. [sent-316, score-0.19]
</p><p>67 Name letter lin 100 stands for the Letter-G set with linear kernel and C = 100, ﬁnally abalone p5 100 stands for the Abalone set with polynomial kernel of degree 5 and C = 100. [sent-317, score-0.35]
</p><p>68 In Table 2 we present the comparison of SVM-QP and SVMlight on the full test sets for adult and web. [sent-336, score-0.167]
</p><p>69 The “*” in the last column of the last row indicates that SVM-QP ran out of memory, since it was trying to store an ns × ns matrix, with ns ≈ 10000. [sent-338, score-0.962]
</p><p>70 For example, the number of active support vectors for web 10 100 reported by SVM-QP was 3446, while the same number reported by SVM light was 4025. [sent-340, score-0.27]
</p><p>71 This discrepancy is due to the fact that SVMlight converges to the optimal active set asymptotically, while SVM-QP steps from one feasible active set to another in an “exact” manner, until the optimal is found. [sent-341, score-0.296]
</p><p>72 Speciﬁcally we need the elements of matrix Q whose column indices are in Is and whose row indices are in Ic and I0 . [sent-349, score-0.133]
</p><p>73 We note that we always store the ns × ns matrix Qss . [sent-350, score-0.622]
</p><p>74 Our algorithm requires storage of the Cholesky factor of Q ss , hence even if we do not store Qss itself, the storage requirement can be reduced at most by half. [sent-352, score-0.154]
</p><p>75 To reduce the computational cost it is best to be able to store the entire Q s matrix (that is the submatrix of Q whose column indices are in Is ). [sent-355, score-0.103]
</p><p>76 We will demonstrate this in the section on the incremental mode, since the incremental mode lacks the ability to “look ahead” and select the maximum violated constraint. [sent-363, score-0.1]
</p><p>77 The primal slack and surplus variables si and ξi are the reduced costs of the associated dual variable αi , whose value is currently at a bound. [sent-366, score-0.205]
</p><p>78 Computing the values of the reduced costs (recall that for each i only one of the reduced costs is not equal to zero) is called pricing of the appropriate dual variable. [sent-367, score-0.236]
</p><p>79 Hence it is important to price all variables with indices in I0 and Ic and maintain these sets in such a way that they contain indices of substantially negative reduced costs. [sent-368, score-0.146]
</p><p>80 When all the reduced costs of variables whose indices are in I0 and Ic are nonnegative, then so are the reduced costs of variables whose indices are in I0 and Ic , due to our assumption about these two subsets. [sent-375, score-0.224]
</p><p>81 Naturally, we usually do not know which indices will be in I0 and Ic at optimality, however, to reduce the workload at each iteration we try to guess which indices are the most likely ones to end up in I0 and Ic at optimality. [sent-376, score-0.112]
</p><p>82 If we guess well, then after all the reduced costs for I0 and Ic become nonnegative, hopefully, only a few reduced costs for I0 and Ic are negative. [sent-378, score-0.132]
</p><p>83 Here we see a trade-off: if we select I0 and Ic too small, then the computational saving is insigniﬁcant, and if we select I0 and Ic too large, then some of large negative reduced costs might be missed and the overall number of iterations might increase. [sent-379, score-0.118]
</p><p>84 Moreover, once all the dual variables with indices in I0 and Ic are priced, then we have to price all variables with indices in I0 and Ic , which are large. [sent-380, score-0.175]
</p><p>85 At the end one still has to price all the dual variables for I0 and Ic , but only a few of such iterations are usually needed. [sent-388, score-0.104]
</p><p>86 Following the sprint strategy we select a relatively small subset of dual variables with the smallest (including the most negative) reduced costs and we form I0 and Ic from the indices of those variables. [sent-392, score-0.267]
</p><p>87 Table 3 below shows that sprint outperforms shrinking in most cases, especially on larger, more difﬁcult problems. [sent-397, score-0.118]
</p><p>88 Luckily sprint provides a natural setting for a memory saving mode. [sent-407, score-0.145]
</p><p>89 3 Warm Start One of the signiﬁcant advantages of the active set methods over the interior point methods is that the former can beneﬁt very well from warm starts. [sent-419, score-0.258]
</p><p>90 For instance, if some additional labeled training data become available, the old optimal solution is used as a starting point for the active set algorithm 2252  ACTIVE S ET M ETHOD FOR SVM S  and the new optimal solution is typically obtained within a few iterations. [sent-420, score-0.148]
</p><p>91 (2004) the whole solution path is generated using an active set method similar to ours. [sent-424, score-0.148]
</p><p>92 (2004) is a parametric active set method, which in practice is usually slower than a purely primal or dual active set method, such as ours. [sent-427, score-0.383]
</p><p>93 Also their method requires that at each iteration an optimal solution of a parametric problem is available, hence there does not seem to be any possibility to use sprint or shrinking. [sent-428, score-0.109]
</p><p>94 The warm starts can also be used when one wants to explore different values of kernel parameters, but the efﬁciency of such application needs a separate computational study. [sent-432, score-0.106]
</p><p>95 The idea we explore here is to use the solution of the approximate problem to warm start the active set method. [sent-437, score-0.231]
</p><p>96 Often the active set method itself is so fast that it outperforms the IPM even for k = 50, for instance on letter x x problems. [sent-442, score-0.309]
</p><p>97 There also cases where Q itself has very low rank and, hence, the problem can be solved to optimality just by the IPM; see letter lin 100, for instance. [sent-444, score-0.191]
</p><p>98 This seem to happen for the problems with relatively large Ic sets, such as the adult x x problems. [sent-446, score-0.167]
</p><p>99 Concluding Remarks Traditional active set methods for convex QPs were considered impractical for large-scale SVM problems. [sent-461, score-0.148]
</p><p>100 In this paper we studied in details an active set method SVM and show that an efﬁcient implementation can outperform other state-of-the-art SVM software. [sent-463, score-0.169]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ic', 0.538), ('ls', 0.399), ('qss', 0.364), ('ns', 0.299), ('cc', 0.189), ('adult', 0.167), ('letter', 0.161), ('active', 0.148), ('cholesky', 0.124), ('abalone', 0.113), ('qsc', 0.098), ('web', 0.095), ('svmlight', 0.092), ('svm', 0.092), ('cheinberg', 0.089), ('sprint', 0.089), ('yns', 0.089), ('qs', 0.088), ('warm', 0.083), ('qcs', 0.08), ('ethod', 0.075), ('ipm', 0.071), ('ys', 0.069), ('dual', 0.066), ('ss', 0.063), ('rns', 0.062), ('rqp', 0.062), ('yc', 0.061), ('spam', 0.057), ('frangioni', 0.053), ('factorization', 0.053), ('indices', 0.046), ('qcc', 0.044), ('nc', 0.043), ('qp', 0.039), ('incremental', 0.038), ('fine', 0.038), ('pricing', 0.038), ('reduced', 0.037), ('goldfarb', 0.035), ('surplus', 0.035), ('scheinberg', 0.034), ('simplex', 0.031), ('saving', 0.031), ('lin', 0.03), ('shrinking', 0.029), ('costs', 0.029), ('interior', 0.027), ('light', 0.027), ('underdetermined', 0.027), ('cpu', 0.025), ('memory', 0.025), ('chunk', 0.025), ('store', 0.024), ('mode', 0.024), ('row', 0.024), ('name', 0.023), ('kernel', 0.023), ('balcazar', 0.023), ('nonsingular', 0.023), ('blake', 0.022), ('subsection', 0.021), ('iterations', 0.021), ('implementation', 0.021), ('primal', 0.021), ('indexed', 0.02), ('step', 0.02), ('nocedal', 0.02), ('fletcher', 0.02), ('bixby', 0.02), ('iteration', 0.02), ('merz', 0.019), ('rows', 0.018), ('repository', 0.018), ('backsolves', 0.018), ('ferris', 0.018), ('forrest', 0.018), ('katya', 0.018), ('priced', 0.018), ('cache', 0.018), ('hs', 0.017), ('programming', 0.017), ('hastie', 0.017), ('price', 0.017), ('column', 0.017), ('si', 0.017), ('uci', 0.017), ('operations', 0.016), ('updates', 0.016), ('quadratic', 0.016), ('submatrix', 0.016), ('joachims', 0.016), ('ci', 0.015), ('margin', 0.015), ('storage', 0.015), ('nonnegative', 0.015), ('ds', 0.015), ('lp', 0.015), ('dictated', 0.015), ('argmini', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="14-tfidf-1" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Katya Scheinberg</p><p>Abstract: We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efﬁcient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims’ SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difﬁcult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight , the generalization properties of these two approaches are identical and we do not discuss them in the paper. Keywords: active set methods, support vector machines, quadratic programming</p><p>2 0.10611251 <a title="14-tfidf-2" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>3 0.093027137 <a title="14-tfidf-3" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>4 0.083138362 <a title="14-tfidf-4" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: S. Sathiya Keerthi, Olivier Chapelle, Dennis DeCoste</p><p>Abstract: Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classiﬁcation speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily ﬁnds a set of kernel basis functions of a speciﬁed maximum size (dmax ) to approximate the SVM primal cost function well; (3) it is 2 efﬁcient and roughly scales as O(ndmax ) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors. Keywords: SVMs, classiﬁcation, sparse design</p><p>5 0.060544558 <a title="14-tfidf-5" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Luca Zanni, Thomas Serafini, Gaetano Zanghirati</p><p>Abstract: Parallel software for solving the quadratic program arising in training support vector machines for classiﬁcation problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-ofthe-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes. Keywords: support vector machines, large scale quadratic programs, decomposition techniques, gradient projection methods, parallel computation</p><p>6 0.05587557 <a title="14-tfidf-6" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>7 0.044062439 <a title="14-tfidf-7" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>8 0.04299318 <a title="14-tfidf-8" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>9 0.039281327 <a title="14-tfidf-9" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.036411047 <a title="14-tfidf-10" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.03631413 <a title="14-tfidf-11" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.035748132 <a title="14-tfidf-12" href="./jmlr-2006-Machine_Learning_for_Computer_Security%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">59 jmlr-2006-Machine Learning for Computer Security    (Special Topic on Machine Learning for Computer Security)</a></p>
<p>13 0.034660459 <a title="14-tfidf-13" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.031006016 <a title="14-tfidf-14" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>15 0.030923229 <a title="14-tfidf-15" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>16 0.030894328 <a title="14-tfidf-16" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>17 0.029618831 <a title="14-tfidf-17" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.028858881 <a title="14-tfidf-18" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>19 0.028232196 <a title="14-tfidf-19" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>20 0.027759947 <a title="14-tfidf-20" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.152), (1, -0.109), (2, 0.098), (3, 0.143), (4, 0.093), (5, 0.033), (6, -0.094), (7, 0.01), (8, 0.02), (9, -0.085), (10, 0.007), (11, -0.02), (12, 0.129), (13, -0.111), (14, 0.007), (15, 0.107), (16, 0.008), (17, 0.043), (18, 0.059), (19, 0.034), (20, 0.035), (21, -0.127), (22, 0.127), (23, 0.187), (24, -0.235), (25, 0.04), (26, 0.203), (27, 0.081), (28, 0.105), (29, 0.249), (30, -0.088), (31, -0.099), (32, -0.07), (33, -0.092), (34, 0.067), (35, -0.146), (36, -0.017), (37, 0.031), (38, -0.088), (39, 0.206), (40, 0.037), (41, 0.133), (42, -0.106), (43, -0.007), (44, 0.005), (45, -0.155), (46, -0.122), (47, 0.094), (48, 0.017), (49, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95038837 <a title="14-lsi-1" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Katya Scheinberg</p><p>Abstract: We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efﬁcient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims’ SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difﬁcult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight , the generalization properties of these two approaches are identical and we do not discuss them in the paper. Keywords: active set methods, support vector machines, quadratic programming</p><p>2 0.5462243 <a title="14-lsi-2" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>3 0.53649151 <a title="14-lsi-3" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: S. Sathiya Keerthi, Olivier Chapelle, Dennis DeCoste</p><p>Abstract: Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classiﬁcation speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily ﬁnds a set of kernel basis functions of a speciﬁed maximum size (dmax ) to approximate the SVM primal cost function well; (3) it is 2 efﬁcient and roughly scales as O(ndmax ) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors. Keywords: SVMs, classiﬁcation, sparse design</p><p>4 0.30059469 <a title="14-lsi-4" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>5 0.29919526 <a title="14-lsi-5" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>Author: Hema Raghavan, Omid Madani, Rosie Jones</p><p>Abstract: We extend the traditional active learning framework to include feedback on features in addition to labeling instances, and we execute a careful study of the effects of feature selection and human feedback on features in the setting of text categorization. Our experiments on a variety of categorization tasks indicate that there is signiﬁcant potential in improving classiﬁer performance by feature re-weighting, beyond that achieved via membership queries alone (traditional active learning) if we have access to an oracle that can point to the important (most predictive) features. Our experiments on human subjects indicate that human feedback on feature relevance can identify a sufﬁcient proportion of the most relevant features (over 50% in our experiments). We ﬁnd that on average, labeling a feature takes much less time than labeling a document. We devise an algorithm that interleaves labeling features and documents which signiﬁcantly accelerates standard active learning in our simulation experiments. Feature feedback can complement traditional active learning in applications such as news ﬁltering, e-mail classiﬁcation, and personalization, where the human teacher can have signiﬁcant knowledge on the relevance of features. Keywords: active learning, feature selection, relevance feedback, term feedback, text classiﬁcation</p><p>6 0.27563754 <a title="14-lsi-6" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.27035201 <a title="14-lsi-7" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>8 0.18534304 <a title="14-lsi-8" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.17111328 <a title="14-lsi-9" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>10 0.17098194 <a title="14-lsi-10" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.16996235 <a title="14-lsi-11" href="./jmlr-2006-Bounds_for_Linear_Multi-Task_Learning.html">16 jmlr-2006-Bounds for Linear Multi-Task Learning</a></p>
<p>12 0.16966879 <a title="14-lsi-12" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.16472951 <a title="14-lsi-13" href="./jmlr-2006-Machine_Learning_for_Computer_Security%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">59 jmlr-2006-Machine Learning for Computer Security    (Special Topic on Machine Learning for Computer Security)</a></p>
<p>14 0.16309629 <a title="14-lsi-14" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.14544487 <a title="14-lsi-15" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>16 0.14342205 <a title="14-lsi-16" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>17 0.13866659 <a title="14-lsi-17" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>18 0.1384514 <a title="14-lsi-18" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>19 0.13768634 <a title="14-lsi-19" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.12653367 <a title="14-lsi-20" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.016), (35, 0.011), (36, 0.058), (45, 0.016), (47, 0.383), (50, 0.03), (63, 0.085), (70, 0.01), (76, 0.021), (78, 0.037), (81, 0.019), (84, 0.017), (90, 0.034), (91, 0.043), (96, 0.1), (99, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72958237 <a title="14-lda-1" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Katya Scheinberg</p><p>Abstract: We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efﬁcient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims’ SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difﬁcult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight , the generalization properties of these two approaches are identical and we do not discuss them in the paper. Keywords: active set methods, support vector machines, quadratic programming</p><p>2 0.36448777 <a title="14-lda-2" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tobias Glasmachers, Christian Igel</p><p>Abstract: Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is signiﬁcantly faster. Keywords: working set selection, sequential minimal optimization, quadratic programming, support vector machines, large scale optimization</p><p>3 0.3583245 <a title="14-lda-3" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Luca Zanni, Thomas Serafini, Gaetano Zanghirati</p><p>Abstract: Parallel software for solving the quadratic program arising in training support vector machines for classiﬁcation problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-ofthe-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes. Keywords: support vector machines, large scale quadratic programs, decomposition techniques, gradient projection methods, parallel computation</p><p>4 0.35814518 <a title="14-lda-4" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>Author: Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou</p><p>Abstract: We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the ﬁrst time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction. html. Keywords: transduction, transductive SVMs, semi-supervised learning, CCCP</p><p>5 0.35289952 <a title="14-lda-5" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram. Keywords: spectral clustering, blind source separation, computational auditory scene analysis</p><p>6 0.3493591 <a title="14-lda-6" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.34796953 <a title="14-lda-7" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.34681675 <a title="14-lda-8" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>9 0.33967614 <a title="14-lda-9" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>10 0.33847889 <a title="14-lda-10" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.33783984 <a title="14-lda-11" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>12 0.33500767 <a title="14-lda-12" href="./jmlr-2006-On_the_Complexity_of_Learning_Lexicographic_Strategies.html">68 jmlr-2006-On the Complexity of Learning Lexicographic Strategies</a></p>
<p>13 0.33497864 <a title="14-lda-13" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.33470181 <a title="14-lda-14" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>15 0.33390072 <a title="14-lda-15" href="./jmlr-2006-Linear_State-Space_Models_for_Blind_Source_Separation.html">57 jmlr-2006-Linear State-Space Models for Blind Source Separation</a></p>
<p>16 0.33151424 <a title="14-lda-16" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.33055335 <a title="14-lda-17" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>18 0.33051851 <a title="14-lda-18" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.32873845 <a title="14-lda-19" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.32597372 <a title="14-lda-20" href="./jmlr-2006-Adaptive_Prototype_Learning_Algorithms%3A_Theoretical_and_Experimental_Studies.html">13 jmlr-2006-Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
