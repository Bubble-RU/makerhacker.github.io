<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-76" href="#">jmlr2006-76</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</h1>
<br/><p>Source: <a title="jmlr-2006-76-pdf" href="http://jmlr.org/papers/volume7/hush06a/hush06a.pdf">pdf</a></p><p>Author: Don Hush, Patrick Kelly, Clint Scovel, Ingo Steinwart</p><p>Abstract: We describe polynomial–time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classiﬁers. These algorithms employ a two–stage process where the ﬁrst stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an √ √ approximate dual solution with accuracy (2 2Kn + 8 λ)−2 λε2 to an approximate primal solution p with accuracy ε p where n is the number of data samples, Kn is the maximum kernel value over the data and λ > 0 is the SVM regularization parameter. For the ﬁrst stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for τ–rate certifying decomposition algorithms we establish the optimality of τ = 1/(n − 1). In addition we extend the recent τ = 1/(n − 1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the τ = 1/(n − 1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the τ–rate certifying property of these algorithms to produce new stopping rules that are computationally efﬁcient and that guarantee a speciﬁed accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classiﬁcation problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n2 ) bound on the overall run time for the ﬁrst stage. Combining the ﬁrst and second stages gives an overall run time of O(n2 (c</p><p>Reference: <a title="jmlr-2006-76-reference" href="../jmlr2006_reference/jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 These algorithms employ a two–stage process where the ﬁrst stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. [sent-6, score-0.583]
</p><p>2 For the ﬁrst stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. [sent-8, score-0.358]
</p><p>3 In particular, for τ–rate certifying decomposition algorithms we establish the optimality of τ = 1/(n − 1). [sent-9, score-0.438]
</p><p>4 In addition we extend the recent τ = 1/(n − 1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the τ = 1/(n − 1) iteration bound of List and Simon (2005), but yield faster run times in practice. [sent-10, score-0.538]
</p><p>5 We also exploit the τ–rate certifying property of these algorithms to produce new stopping rules that are computationally efﬁcient and that guarantee a speciﬁed accuracy for the approximate dual solution. [sent-11, score-0.744]
</p><p>6 Furthermore, for the dual QP problem corresponding to the standard classiﬁcation problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. [sent-12, score-0.677]
</p><p>7 For the Simon and composite algorithms we also establish an O(n2 ) bound on the overall run time for the ﬁrst stage. [sent-14, score-0.53]
</p><p>8 Experiments are included to illustrate the new stopping rules and to compare the Simon and composite decomposition algorithms. [sent-17, score-0.75]
</p><p>9 To guarantee stepwise improvement each working set must contain a certifying pair (Deﬁnition 3 below). [sent-41, score-0.741]
</p><p>10 Hush and Scovel (2003) deﬁne a class of rate certifying algorithms and describe an example al5 gorithm that uses O Kn n εlog n computation to reach an approximate dual solution with accuracy ε, where Kn is the maximum value of the kernel matrix. [sent-57, score-0.57]
</p><p>11 Recently Simon (2004) introduced a new rate certifying algorithm which can be shown, using the results in (List and Simon, 2005), to use λn computation to reach an approximate dual solution with accuracy ε, where O nKn + n2 log Kn λε λ > 0 is the SVM regularization parameter. [sent-58, score-0.57]
</p><p>12 (2001) to obtain a composite algorithm that possesses the same computation bound as Simon’s algorithm, but appears to use far less computation in practice (as illustrated in our experiments). [sent-60, score-0.459]
</p><p>13 We also extend this approach to form a second composite algorithm with similar properties. [sent-61, score-0.416]
</p><p>14 Finally to guarantee that actual implementations of these algorithms produce approximate solutions with accuracy ε we introduce two new stopping rules that terminate the algorithms when an adaptively computed upper bound on the accuracy falls below ε. [sent-63, score-0.427]
</p><p>15 The second stage of the design process maps an approximate dual solution to an approximate primal solution. [sent-64, score-0.346]
</p><p>16 This work studies the accuracy of the approximate primal solution as a function of the accuracy of the approximate dual solution and the map from approximate dual to approximate primal. [sent-70, score-0.591]
</p><p>17 Section 4 describes experiments that illustrate the new stopping rules and compare the run time of different decomposition algorithms. [sent-81, score-0.376]
</p><p>18 The change of variables deﬁned by αi := yi ai + li ,  li =  0 yi = 1 ui yi = −1  (3)  gives the canonical dual QP problem maxα − 1 α · Qα + α · w + w0 2 s. [sent-111, score-0.401]
</p><p>19 It computes an approximate canonˆ ˆ ˆ ˆ ical dual solution α and then maps to an approximate primal solution (ψ, b, ξ) using the map described in the following theorem. [sent-128, score-0.325]
</p><p>20 737  H USH , K ELLY, S COVEL AND S TEINWART  ˆ This theorem gives an expression for ψ that coincides with the standard practice of replacing an ˆ optimal dual solution α∗ by an approximate dual solution α in the expansion for the optimal norˆ ˆ mal vector determined by the KKT conditions. [sent-139, score-0.376]
</p><p>21 To guarantee an accuracy ε p for the primal problem this theorem stipulates that the value of the dual criterion at the approximate solution be non–negative √ √ and that the accuracy for the dual solution satisfy ε = (2 2K + 8 λ)−2 λε2 . [sent-141, score-0.63]
</p><p>22 We will guarantee the second condition by employing an appropriate stopping rule for the decomposition algorithm. [sent-143, score-0.338]
</p><p>23 Deﬁnition 3 A certifying pair (also called a violating pair) for α ∈ A is a pair of indices that witness the non–optimality of α, i. [sent-167, score-0.63]
</p><p>24 Using the approach in (Hush and Scovel, 2003, Section 3) it can be shown that the requirement that working sets contain a certifying pair is both necessary and sufﬁcient to obtain a stepwise improvement in the criterion value. [sent-170, score-0.776]
</p><p>25 Thus, since certifying pairs are deﬁned in terms of the gradient component values it appears that the gradient plays an essential role in determining members of the working sets. [sent-171, score-0.462]
</p><p>26 After computing an initial gradient vector this algorithm iterates the process of determining a working set, solving a QP problem restricted to this working set, updating the gradient vector, and testing a stopping condition. [sent-175, score-0.416]
</p><p>27 The requirement that working sets contain a certifying pair is necessary but not sufﬁcient to guarantee convergence to a solution (e. [sent-176, score-0.553]
</p><p>28 However Lin (2002b) has shown that including a max–violating pair deﬁned by ( j∗ , k∗ ) : j∗ ∈ arg max gi (α), i:αi  0  (9)  in each working set does guarantee convergence to a solution. [sent-180, score-0.372]
</p><p>29 The class of max–violating pair algorithms that include a max–violating 739  H USH , K ELLY, S COVEL AND S TEINWART  Procedure 2 A model decomposition algorithm for the canonical dual QP problem. [sent-182, score-0.447]
</p><p>30 In addition W2 decomposition algorithms require only O(n) computation to update the gradient and have the advantage that the overall algorithm can be quite simple (as demonstrated by the W2 max– violating pair algorithm). [sent-197, score-0.378]
</p><p>31 Furthermore adopting size 2 working sets will allow us to implement our new stopping rules in constant time. [sent-198, score-0.315]
</p><p>32 Hush and Scovel (2003) prove that convergence rates can be guaranteed simply by including a rate certifying pair in each working set. [sent-201, score-0.542]
</p><p>33 Roughly speaking a rate certifying pair is a certifying pair that, when used as the working set, provides a sufﬁcient stepwise improvement. [sent-202, score-1.115]
</p><p>34 The set of feasible solutions for the canonical dual QP sub–problem deﬁned by a feasible value α and a working set W is deﬁned ´ ´ A (α,W ) := {α ∈ A : αi = αi ∀i ∈ W }. [sent-209, score-0.387]
</p><p>35 Deﬁnition 4 For τ > 0 an index pair W2 is called a τ–rate certifying pair for α if σ(α|W2 ) ≥ τσ(α|Wn ). [sent-212, score-0.506]
</p><p>36 A decomposition algorithm that includes a τ–rate certifying pair in the working set at every iteration is called a τ–rate certifying algorithm. [sent-213, score-0.941]
</p><p>37 For a τ–rate certifying algorithm Hush and Scovel (2003) provide an upper bound on the number of iterations as a function of τ. [sent-214, score-0.42]
</p><p>38 (2000) have shown that for every α ∈ A there exists a τ–rate certifying pair with τ ≥ 1/n2 . [sent-221, score-0.401]
</p><p>39 The ﬁrst such algorithm was provided by Hush and Scovel (2003) where the rate certifying pairs satisﬁed τ ≥ 1/n2 . [sent-223, score-0.343]
</p><p>40 However the value τ can be improved and the bound on the number of iterations reduced if the rate certifying pairs are determined differently. [sent-224, score-0.467]
</p><p>41 Corollary 7 Let DECOMP be a realization of the model decomposition algorithm for the canonical dual QP in Procedure 2 and let (αm ) represent a sequence of feasible points produced by this ´ ´ algorithm. [sent-239, score-0.374]
</p><p>42 At each iteration m let W2m be a τ–rate certifying pair and let αm+1 be the feasible m and W m . [sent-240, score-0.47]
</p><p>43 If for every m ≥ 0 the ´2 point determined by solving the restricted QP determined by α ´ stepwise improvement satisﬁes R(αm+1 ) − R(αm ) ≥ R(αm+1 ) − R(αm ) then DECOMP will achieve ∗ − R(αm ) ≤ ε after ⌈m⌉ iterations of the main loop where m is given by Theorem 5. [sent-241, score-0.353]
</p><p>44 R ´ ´ This theorem implies that any pair whose stepwise improvement is at least as good as that produced by a max–lp2 pair yields a decomposition algorithm that inherits the iteration bound in Theorem 5 with τ = 1/(n − 1). [sent-242, score-0.65]
</p><p>45 In particular Simon’s algorithm visits several good candidate pairs in its search for a max–lp2 pair and can therefore be easily extended to form an alternative pair selection algorithm that is computationally efﬁcient and satisﬁes this stepwise improvement property. [sent-245, score-0.427]
</p><p>46 A closer examination reveals that only the nonzero values at the front of the list need to be scanned, since entries with zero values cannot form a certifying pair (i. [sent-261, score-0.467]
</p><p>47 In addition the stepwise improvement for an individual pair can be computed in constant time. [sent-273, score-0.322]
</p><p>48 It adds a negligible amount of computation to the main loop and its stepwise improvement cannot be worse than either the max–violating pair or max–lp2 algorithm alone. [sent-277, score-0.377]
</p><p>49 We can extend this idea further by computing the stepwise improvement for all certifying pairs visited by Simon’s algorithm and then choosing the best. [sent-278, score-0.513]
</p><p>50 This stopping rule is employed by many existing decomposition algorithms (e. [sent-294, score-0.309]
</p><p>51 In contrast we now introduce new stopping rules which guarantee a speciﬁed accuracy for the approximate solutions they produce, and whose convergence rate properties are well understood. [sent-303, score-0.365]
</p><p>52 In addition we will show that these new stopping rules can be computed in constant time when coupled with the pair selection strategies in the previous section. [sent-304, score-0.326]
</p><p>53 The simplest stopping rule that guarantees an ε–optimal solution for a τ–rate certifying algorithm is to stop after m iterations where m is given by Theorem 5 with R∗ − R(α0 ) replaced by a ´ ´ suitable upper bound (e. [sent-305, score-0.645]
</p><p>54 For example the primal-dual gap, which is the difference between the primal criterion value and the dual criterion value, provides such a bound and therefore could be used to terminate the algorithm. [sent-314, score-0.408]
</p><p>55 Instead we develop stopping rules that, when coupled with one of the pair selection methods in the previous section, are simple to compute. [sent-316, score-0.326]
</p><p>56 Theorem 8 Consider the canonical dual QP problem in (4) with Gram matrix Q, constraint vector u, feasible set A , criterion function R, and optimal criterion value R∗ . [sent-319, score-0.389]
</p><p>57 If Wp includes a τ–rate certifying pair for α then R∗ − R(α) ≤  σ(α|Wp ) . [sent-325, score-0.401]
</p><p>58 For any sequence of feasible points (αm ) and corresponding sequence of working sets (W m ) that include τ–rate certifying pairs the following holds: R(αm ) → R∗  ⇔ σ(αm |W m ) → 0. [sent-328, score-0.422]
</p><p>59 Deﬁnition 10 (Stopping Rule 1) For a τ–rate certifying algorithm with τ–rate certifying pair se´ ´ ´ quence (W2m ), stop at the ﬁrst iteration m where σ(αm |W2m ) ≤ τε. [sent-331, score-0.734]
</p><p>60 s0 = σ0 /τ) can be improved using the recursion sm+1 = min  σ(αm+1 |W2m+1 ) m , s − δm R τ  which leads to the following stopping rule: Deﬁnition 11 (Stopping Rule 2) For a τ–rate certifying algorithm with τ–rate certifying pair se´ ´ quence (W2m ), stop at the ﬁrst iteration m where sm ≤ ε. [sent-339, score-0.925]
</p><p>61 ˆ The criterion ∑n ui max 0, yi gi (α) − b is the sum of hinge functions with slopes −ui yi and i=1 ˆ ˆ b–intercepts gi (α). [sent-350, score-0.415]
</p><p>62 Multiple options exist for the Decomposition routine depending on the choice of working set size, pair selection method, and stopping rule. [sent-366, score-0.425]
</p><p>63 Operational Analysis of Decomposition Algorithms In this section we use Theorem 5 and Corollary 7 to determine run time bounds for rate certifying decomposition algorithms that are applied to the L1–SVM and DLD–SVM canonical dual QP problems. [sent-370, score-0.761]
</p><p>64 In the algorithms below each working set contains either a max–lp2 pair or a pair whose stepwise improvement is at least as good as that of a max–lp2 pair. [sent-373, score-0.521]
</p><p>65 Although our emphasis is on rate certifying decomposition algorithms, our ﬁrst theorem establishes a lower bound on the number of iterations for any W2 decomposition algorithm. [sent-397, score-0.723]
</p><p>66 Since the duality gap for the L1–SVM primal and dual QP problems is zero, R∗ is the optimal value of the primal QP problem (e. [sent-405, score-0.367]
</p><p>67 We now continue our analysis by establishing upper bounds on the computation required for rate certifying decomposition algorithms applied to the L1–SVM and the DLD–SVM problems. [sent-419, score-0.49]
</p><p>68 In both examples we ﬁrst consider a general class of rate λ certifying decomposition algorithms whose working sets may be larger than 2. [sent-423, score-0.55]
</p><p>69 Example 1 Consider solving the L1–SVM canonical dual using a decomposition algorithm where each working set includes a certifying pair whose stepwise improvement is at least as good as that produced by a max–lp2 pair. [sent-426, score-1.054]
</p><p>70 Example 2 Consider solving the DLD–SVM canonical dual using a decomposition algorithm where each working set includes a certifying pair whose stepwise improvement is at least as good as that produced by a max–lp2 pair. [sent-439, score-1.054]
</p><p>71 We compare the four rate certifying pair selection methods (max–qp2, max–lp2, Composite–I, Composite–II) described in Section 2. [sent-453, score-0.448]
</p><p>72 More speciﬁcally we compare the actual criterion gap R∗ − R(αm ) to the bounds used by these three stopping rules. [sent-509, score-0.329]
</p><p>73 001 1e-04 1e-05 1e-06  ˆ R∗ − Rm Bound 0 Bound 1 Bound 2 1  10  100  1000  number of iterations ˆ Figure 1: The criterion gap R∗ − Rm and bounds on this gap employed by Stopping Rules 0, 1 and 2 for the Cyber–Security data. [sent-516, score-0.329]
</p><p>74 they are often several orders of magnitude larger than the actual criterion gap, their behavior tracks that of the criterion gap relatively well and therefore the corresponding stopping rules are very effective relative to Rule 0. [sent-523, score-0.424]
</p><p>75 In this case the initial criterion gap is larger so the separation between the criterion gap and the bounds is smaller. [sent-529, score-0.312]
</p><p>76 001 1e-04 1e-05  ˆ R∗ − Rm Bound 0 Bound 1 Bound 2  1e-06 100  1000  10000  100000  1e+06  number of iterations ˆ Figure 2: The criterion gap R∗ − Rm and bounds on this gap employed by Stopping Rules 0, 1 and 2 for the Spambase data. [sent-535, score-0.329]
</p><p>77 Then we ran the decomposition algorithm on each training set and recorded the number of iterations and the wallclock time of the main loop. [sent-546, score-0.335]
</p><p>78 Indeed, it is curious that the max–lp2 method, which chooses a stepwise direction based on a combination of steepness and room to move, has a worse convergence rate than the max–vps method, which chooses a stepwise direction based on steepness alone. [sent-555, score-0.391]
</p><p>79 Although a larger stepwise improvement does not guarantee a faster convergence rate the max–qp2 method, which gives the largest stepwise improvement, also gave the fastest convergence rate. [sent-567, score-0.465]
</p><p>80 To move all n1 + n−1 components of α to their opposite bound using working sets that contain one sample from each class requires n−1 iterations (since n−1 > n1 ) and this is exactly what the algorithms did for all ﬁve pair selection methods on every training set. [sent-584, score-0.323]
</p><p>81 We ran the decomposition algorithm on ten different training sets for each problem size and recorded the number of iterations and the wallclock time of the main loop. [sent-592, score-0.335]
</p><p>82 Indeed the number of iterations is roughly the same for all ﬁve pair selection methods and the wallclock times for the max–lp2, Composite–I and max–vps algorithms are approximately 5 times faster than Composite–II. [sent-621, score-0.327]
</p><p>83 They employ a two–stage process where the ﬁrst stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. [sent-636, score-0.583]
</p><p>84 For the second stage we have √ described a simple O(n log n) algorithm that maps an ap√ proximate dual solution with accuracy (2 2K + 8 λ)−2 λε2 to an approximate primal solution with p accuracy ε p . [sent-637, score-0.427]
</p><p>85 For the ﬁrst stage we have presented new results for decomposition algorithms and we have described decomposition algorithms that employ new pair selection methods and new stopping rules. [sent-638, score-0.537]
</p><p>86 For τ–rate certifying decomposition algorithms we have established the optimality of τ = 1/(n− 1) and described several pair selection methods (max–qp2, max–lp2, Composite–I, Composite–II) that achieve the τ = 1/(n − 1) iteration bound. [sent-639, score-0.551]
</p><p>87 We have also introduced new stopping rules that are computationally efﬁcient and that guarantee a speciﬁed accuracy for the approximate dual solution. [sent-640, score-0.448]
</p><p>88 While these stopping rules can be used by any decomposition algorithm they are especially attractive for the algorithms developed here because they add a negligible amount of computation to the main loop. [sent-641, score-0.334]
</p><p>89 In addition, for the L1–SVM dual QP problem we have described operational conditions for which these W2 decomposition algorithms possess an upper bound of O(n) on the number of iterations. [sent-643, score-0.374]
</p><p>90 Lemma 14 Consider the canonical dual QP problem in (4) with Gram matrix Q, constraint vector u, feasible set A , criterion function R, and optimal criterion value R∗ . [sent-662, score-0.389]
</p><p>91 (20) To obtain a lower bound for the right side we start by writing R(α + ωdq ) − R(α) = ωg(α) · dq −  ω2 ω2 ω2 ¯ dq · Qdq = ωσ(α|Wq ) − dq · Qdq ≥ ωσ − dq · Qdq . [sent-673, score-0.439]
</p><p>92 The basic approach is to obtain an upper bound on the number of iterations by deriving a lower bound on the stepwise improvement. [sent-683, score-0.339]
</p><p>93 Let W2m ⊆ W m be a τ–rate certifying pair for αm . [sent-685, score-0.401]
</p><p>94 The stepwise improvement with W m is at least as good as the stepwise improvement with W2m and therefore ´ R(αm+1 ) − R(αm ) ≥ R(αm+1 ) − R(αm )  (22)  ´ where αm+1 is a solution to the two–variable QP problem at (αm ,W2m ). [sent-686, score-0.463]
</p><p>95 Let q be the index corresponding to the largest of these terms and let σ jq kq = δ jq kq (g jq − gkq ) be its value. [sent-731, score-0.575]
</p><p>96 p n−1  (27)  Furthermore if we combine the fact that σ jq kq > 0 implies g jq − gkq > 0 with the deﬁnitions of d q and ∆ jq kq we obtain δ jq kq = min(d jq , −dkq ) ≤ min(u jq − α jq , αkq ) = ∆ jq kq . [sent-733, score-1.345]
</p><p>97 q  q  761  H USH , K ELLY, S COVEL AND S TEINWART  Finally, combining this result with (27) and (24) gives σ(α|Wn ) ≤ σ jq kq = δ jq kq (g jq − gkq ) ≤ ∆ jq kq (g jq − gkq ) = σ(α|{ jq , kq }) ≤ σ(α|W2∗ ) n−1 which completes the proof of the ﬁrst assertion. [sent-734, score-1.15]
</p><p>98 If Wp contains a τ–rate certifying pair then σ(α|Wn ) ≤  σ(α|Wp ) τ  and the proof is ﬁnished. [sent-756, score-0.401]
</p><p>99 Procedure 6 computes the stepwise improvement for the Wml p2 and Wmv pairs and then updates α according to the pair with the largest improvement. [sent-772, score-0.322]
</p><p>100 , n) do M ← Insert(M, (αi , i, −)) M ← Insert(M, (ui − αi , i, +)) end for s←1 Return(g, M, s)  Procedure 6 This routine computes the stepwise improvements for a max–lp2 pair Wml p2 and a max–violating pair Wmv , and then updates α using the pair with the largest stepwise improvement. [sent-820, score-0.729]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('composite', 0.416), ('certifying', 0.296), ('qp', 0.263), ('vps', 0.224), ('stepwise', 0.172), ('stopping', 0.156), ('covel', 0.149), ('elly', 0.149), ('teinwart', 0.149), ('ush', 0.149), ('simon', 0.144), ('wallclock', 0.141), ('scovel', 0.139), ('dual', 0.13), ('hush', 0.129), ('wn', 0.125), ('violating', 0.124), ('jq', 0.114), ('decomposition', 0.113), ('pair', 0.105), ('kq', 0.1), ('qluq', 0.1), ('wml', 0.1), ('canonical', 0.099), ('dq', 0.099), ('working', 0.094), ('gk', 0.091), ('operational', 0.088), ('lgorithms', 0.082), ('primal', 0.081), ('iterations', 0.081), ('jm', 0.078), ('wp', 0.078), ('max', 0.077), ('ui', 0.076), ('gap', 0.075), ('wq', 0.075), ('routine', 0.07), ('gi', 0.067), ('compositei', 0.066), ('compositeii', 0.066), ('cyber', 0.066), ('wmv', 0.066), ('list', 0.066), ('rules', 0.065), ('svm', 0.065), ('criterion', 0.064), ('offset', 0.063), ('keerthi', 0.062), ('gmax', 0.058), ('gmin', 0.058), ('mv', 0.056), ('loop', 0.055), ('stage', 0.05), ('dld', 0.049), ('gm', 0.048), ('steinwart', 0.048), ('old', 0.047), ('rate', 0.047), ('improvement', 0.045), ('spambase', 0.044), ('bound', 0.043), ('run', 0.042), ('imin', 0.042), ('security', 0.04), ('ml', 0.04), ('accuracy', 0.04), ('rule', 0.04), ('alamos', 0.038), ('iteration', 0.037), ('km', 0.036), ('insert', 0.036), ('gradient', 0.036), ('sm', 0.035), ('qq', 0.035), ('sup', 0.034), ('bounds', 0.034), ('ii', 0.034), ('gkq', 0.033), ('gov', 0.033), ('lanl', 0.033), ('qdq', 0.033), ('yi', 0.032), ('feasible', 0.032), ('imax', 0.032), ('kn', 0.03), ('theorem', 0.03), ('guarantee', 0.029), ('requirements', 0.029), ('solution', 0.029), ('establish', 0.029), ('balcazar', 0.028), ('gik', 0.028), ('gl', 0.028), ('approximate', 0.028), ('url', 0.028), ('los', 0.027), ('chen', 0.027), ('delete', 0.027), ('terminate', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="76-tfidf-1" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>Author: Don Hush, Patrick Kelly, Clint Scovel, Ingo Steinwart</p><p>Abstract: We describe polynomial–time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classiﬁers. These algorithms employ a two–stage process where the ﬁrst stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an √ √ approximate dual solution with accuracy (2 2Kn + 8 λ)−2 λε2 to an approximate primal solution p with accuracy ε p where n is the number of data samples, Kn is the maximum kernel value over the data and λ > 0 is the SVM regularization parameter. For the ﬁrst stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for τ–rate certifying decomposition algorithms we establish the optimality of τ = 1/(n − 1). In addition we extend the recent τ = 1/(n − 1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the τ = 1/(n − 1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the τ–rate certifying property of these algorithms to produce new stopping rules that are computationally efﬁcient and that guarantee a speciﬁed accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classiﬁcation problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n2 ) bound on the overall run time for the ﬁrst stage. Combining the ﬁrst and second stages gives an overall run time of O(n2 (c</p><p>2 0.12387437 <a title="76-tfidf-2" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tobias Glasmachers, Christian Igel</p><p>Abstract: Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is signiﬁcantly faster. Keywords: working set selection, sequential minimal optimization, quadratic programming, support vector machines, large scale optimization</p><p>3 0.1001977 <a title="76-tfidf-3" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>4 0.085909776 <a title="76-tfidf-4" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Luca Zanni, Thomas Serafini, Gaetano Zanghirati</p><p>Abstract: Parallel software for solving the quadratic program arising in training support vector machines for classiﬁcation problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-ofthe-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes. Keywords: support vector machines, large scale quadratic programs, decomposition techniques, gradient projection methods, parallel computation</p><p>5 0.067550428 <a title="76-tfidf-5" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><p>6 0.064053372 <a title="76-tfidf-6" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.060963303 <a title="76-tfidf-7" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.059162721 <a title="76-tfidf-8" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>9 0.058508027 <a title="76-tfidf-9" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.055689551 <a title="76-tfidf-10" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.052373603 <a title="76-tfidf-11" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.051909175 <a title="76-tfidf-12" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.048863459 <a title="76-tfidf-13" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.044249937 <a title="76-tfidf-14" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>15 0.044062439 <a title="76-tfidf-15" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.041779716 <a title="76-tfidf-16" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>17 0.037631214 <a title="76-tfidf-17" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>18 0.037259657 <a title="76-tfidf-18" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>19 0.03632639 <a title="76-tfidf-19" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>20 0.036145121 <a title="76-tfidf-20" href="./jmlr-2006-Quantile_Regression_Forests.html">77 jmlr-2006-Quantile Regression Forests</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.224), (1, -0.126), (2, 0.063), (3, 0.083), (4, 0.091), (5, 0.13), (6, -0.027), (7, 0.073), (8, 0.057), (9, -0.086), (10, 0.032), (11, -0.032), (12, 0.263), (13, 0.017), (14, 0.068), (15, 0.035), (16, -0.002), (17, 0.112), (18, -0.037), (19, -0.119), (20, -0.097), (21, 0.26), (22, 0.009), (23, -0.092), (24, -0.023), (25, -0.007), (26, 0.006), (27, 0.032), (28, -0.134), (29, -0.025), (30, -0.06), (31, 0.058), (32, 0.046), (33, 0.057), (34, -0.016), (35, -0.033), (36, 0.007), (37, 0.09), (38, -0.038), (39, -0.134), (40, 0.056), (41, -0.184), (42, -0.119), (43, -0.032), (44, -0.048), (45, 0.178), (46, 0.056), (47, 0.126), (48, 0.199), (49, 0.116)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94707477 <a title="76-lsi-1" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>Author: Don Hush, Patrick Kelly, Clint Scovel, Ingo Steinwart</p><p>Abstract: We describe polynomial–time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classiﬁers. These algorithms employ a two–stage process where the ﬁrst stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an √ √ approximate dual solution with accuracy (2 2Kn + 8 λ)−2 λε2 to an approximate primal solution p with accuracy ε p where n is the number of data samples, Kn is the maximum kernel value over the data and λ > 0 is the SVM regularization parameter. For the ﬁrst stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for τ–rate certifying decomposition algorithms we establish the optimality of τ = 1/(n − 1). In addition we extend the recent τ = 1/(n − 1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the τ = 1/(n − 1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the τ–rate certifying property of these algorithms to produce new stopping rules that are computationally efﬁcient and that guarantee a speciﬁed accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classiﬁcation problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n2 ) bound on the overall run time for the ﬁrst stage. Combining the ﬁrst and second stages gives an overall run time of O(n2 (c</p><p>2 0.71478623 <a title="76-lsi-2" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tobias Glasmachers, Christian Igel</p><p>Abstract: Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is signiﬁcantly faster. Keywords: working set selection, sequential minimal optimization, quadratic programming, support vector machines, large scale optimization</p><p>3 0.56171072 <a title="76-lsi-3" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Luca Zanni, Thomas Serafini, Gaetano Zanghirati</p><p>Abstract: Parallel software for solving the quadratic program arising in training support vector machines for classiﬁcation problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-ofthe-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes. Keywords: support vector machines, large scale quadratic programs, decomposition techniques, gradient projection methods, parallel computation</p><p>4 0.35317296 <a title="76-lsi-4" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><p>5 0.32527909 <a title="76-lsi-5" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>6 0.31355554 <a title="76-lsi-6" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>7 0.31336892 <a title="76-lsi-7" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.29829657 <a title="76-lsi-8" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.27553773 <a title="76-lsi-9" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>10 0.27514255 <a title="76-lsi-10" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.26754013 <a title="76-lsi-11" href="./jmlr-2006-Adaptive_Prototype_Learning_Algorithms%3A_Theoretical_and_Experimental_Studies.html">13 jmlr-2006-Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies</a></p>
<p>12 0.24933785 <a title="76-lsi-12" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.24515215 <a title="76-lsi-13" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.23734508 <a title="76-lsi-14" href="./jmlr-2006-Action_Elimination_and_Stopping_Conditions_for_the_Multi-Armed_Bandit_and_Reinforcement_Learning_Problems.html">10 jmlr-2006-Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems</a></p>
<p>15 0.21045859 <a title="76-lsi-15" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>16 0.20717722 <a title="76-lsi-16" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>17 0.20129532 <a title="76-lsi-17" href="./jmlr-2006-Segmental_Hidden_Markov_Models_with_Random_Effects_for_Waveform_Modeling.html">80 jmlr-2006-Segmental Hidden Markov Models with Random Effects for Waveform Modeling</a></p>
<p>18 0.19371192 <a title="76-lsi-18" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.18475069 <a title="76-lsi-19" href="./jmlr-2006-Toward_Attribute_Efficient_Learning_of_Decision_Lists_and_Parities.html">92 jmlr-2006-Toward Attribute Efficient Learning of Decision Lists and Parities</a></p>
<p>20 0.18468197 <a title="76-lsi-20" href="./jmlr-2006-Inductive_Synthesis_of_Functional_Programs%3A_An_Explanation_Based_Generalization_Approach_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">39 jmlr-2006-Inductive Synthesis of Functional Programs: An Explanation Based Generalization Approach     (Special Topic on Inductive Programming)</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.012), (35, 0.011), (36, 0.052), (45, 0.011), (50, 0.052), (63, 0.038), (76, 0.54), (78, 0.019), (81, 0.036), (84, 0.018), (90, 0.022), (91, 0.036), (96, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92672771 <a title="76-lda-1" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>Author: Don Hush, Patrick Kelly, Clint Scovel, Ingo Steinwart</p><p>Abstract: We describe polynomial–time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classiﬁers. These algorithms employ a two–stage process where the ﬁrst stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an √ √ approximate dual solution with accuracy (2 2Kn + 8 λ)−2 λε2 to an approximate primal solution p with accuracy ε p where n is the number of data samples, Kn is the maximum kernel value over the data and λ > 0 is the SVM regularization parameter. For the ﬁrst stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for τ–rate certifying decomposition algorithms we establish the optimality of τ = 1/(n − 1). In addition we extend the recent τ = 1/(n − 1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the τ = 1/(n − 1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the τ–rate certifying property of these algorithms to produce new stopping rules that are computationally efﬁcient and that guarantee a speciﬁed accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classiﬁcation problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n2 ) bound on the overall run time for the ﬁrst stage. Combining the ﬁrst and second stages gives an overall run time of O(n2 (c</p><p>2 0.75294697 <a title="76-lda-2" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>Author: Robert Castelo, Alberto Roverato</p><p>Abstract: Learning of large-scale networks of interactions from microarray data is an important and challenging problem in bioinformatics. A widely used approach is to assume that the available data constitute a random sample from a multivariate distribution belonging to a Gaussian graphical model. As a consequence, the prime objects of inference are full-order partial correlations which are partial correlations between two variables given the remaining ones. In the context of microarray data the number of variables exceed the sample size and this precludes the application of traditional structure learning procedures because a sampling version of full-order partial correlations does not exist. In this paper we consider limited-order partial correlations, these are partial correlations computed on marginal distributions of manageable size, and provide a set of rules that allow one to assess the usefulness of these quantities to derive the independence structure of the underlying Gaussian graphical model. Furthermore, we introduce a novel structure learning procedure based on a quantity, obtained from limited-order partial correlations, that we call the non-rejection rate. The applicability and usefulness of the procedure are demonstrated by both simulated and real data. Keywords: Gaussian distribution, gene network, graphical model, microarray data, non-rejection rate, partial correlation, small-sample inference</p><p>3 0.49165565 <a title="76-lda-3" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>Author: Mikio L. Braun</p><p>Abstract: The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to inﬁnity. We derive probabilistic ﬁnite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reﬂecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a signiﬁcant improvement over existing non-scaling bounds. Keywords: kernel matrix, eigenvalues, relative perturbation bounds</p><p>4 0.47521922 <a title="76-lda-4" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>Author: Thomas Kämpke</p><p>Abstract: Similarity of edge labeled graphs is considered in the sense of minimum squared distance between corresponding values. Vertex correspondences are established by isomorphisms if both graphs are of equal size and by subisomorphisms if one graph has fewer vertices than the other. Best ﬁt isomorphisms and subisomorphisms amount to solutions of quadratic assignment problems and are computed exactly as well as approximately by minimum cost ﬂow, linear assignment relaxations and related graph algorithms. Keywords: assignment problem, best approximation, branch and bound, inexact graph matching, model data base</p><p>5 0.4566921 <a title="76-lda-5" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>Author: Dmitry M. Malioutov, Jason K. Johnson, Alan S. Willsky</p><p>Abstract: We present a new framework based on walks in a graph for analysis and inference in Gaussian graphical models. The key idea is to decompose the correlation between each pair of variables as a sum over all walks between those variables in the graph. The weight of each walk is given by a product of edgewise partial correlation coefﬁcients. This representation holds for a large class of Gaussian graphical models which we call walk-summable. We give a precise characterization of this class of models, and relate it to other classes including diagonally dominant, attractive, nonfrustrated, and pairwise-normalizable. We provide a walk-sum interpretation of Gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles. The walk-sum perspective leads to a better understanding of Gaussian belief propagation and to stronger results for its convergence in loopy graphs. Keywords: Gaussian graphical models, walk-sum analysis, convergence of loopy belief propagation</p><p>6 0.39825407 <a title="76-lda-6" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.39521039 <a title="76-lda-7" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.39164761 <a title="76-lda-8" href="./jmlr-2006-A_Graphical_Representation_of_Equivalence_Classes_of_AMP_Chain_Graphs.html">2 jmlr-2006-A Graphical Representation of Equivalence Classes of AMP Chain Graphs</a></p>
<p>9 0.37509763 <a title="76-lda-9" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>10 0.37143129 <a title="76-lda-10" href="./jmlr-2006-Action_Elimination_and_Stopping_Conditions_for_the_Multi-Armed_Bandit_and_Reinforcement_Learning_Problems.html">10 jmlr-2006-Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems</a></p>
<p>11 0.36041698 <a title="76-lda-11" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<p>12 0.34759396 <a title="76-lda-12" href="./jmlr-2006-Toward_Attribute_Efficient_Learning_of_Decision_Lists_and_Parities.html">92 jmlr-2006-Toward Attribute Efficient Learning of Decision Lists and Parities</a></p>
<p>13 0.34516197 <a title="76-lda-13" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.33626324 <a title="76-lda-14" href="./jmlr-2006-Superior_Guarantees_for_Sequential_Prediction_and_Lossless_Compression_via_Alphabet_Decomposition.html">90 jmlr-2006-Superior Guarantees for Sequential Prediction and Lossless Compression via Alphabet Decomposition</a></p>
<p>15 0.33379364 <a title="76-lda-15" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.33323705 <a title="76-lda-16" href="./jmlr-2006-Generalized_Bradley-Terry_Models_and_Multi-Class_Probability_Estimates.html">34 jmlr-2006-Generalized Bradley-Terry Models and Multi-Class Probability Estimates</a></p>
<p>17 0.32777485 <a title="76-lda-17" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>18 0.31898564 <a title="76-lda-18" href="./jmlr-2006-Some_Theory_for_Generalized_Boosting_Algorithms.html">82 jmlr-2006-Some Theory for Generalized Boosting Algorithms</a></p>
<p>19 0.3184064 <a title="76-lda-19" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>20 0.31780389 <a title="76-lda-20" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
