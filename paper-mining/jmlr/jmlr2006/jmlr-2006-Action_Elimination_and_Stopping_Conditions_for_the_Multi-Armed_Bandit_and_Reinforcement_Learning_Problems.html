<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>10 jmlr-2006-Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-10" href="#">jmlr2006-10</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>10 jmlr-2006-Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems</h1>
<br/><p>Source: <a title="jmlr-2006-10-pdf" href="http://jmlr.org/papers/volume7/evendar06a/evendar06a.pdf">pdf</a></p><p>Author: Eyal Even-Dar, Shie Mannor, Yishay Mansour</p><p>Abstract: We incorporate statistical conﬁdence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it sufﬁces to pull the arms a total of O (n/ε2 ) log(1/δ) times to ﬁnd an ε-optimal arm with probability of at least 1 − δ. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the conﬁdence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over ε-greedy Q-learning.</p><p>Reference: <a title="jmlr-2006-10-reference" href="../jmlr2006_reference/jmlr-2006-Action_Elimination_and_Stopping_Conditions_for_the_Multi-Armed_Bandit_and_Reinforcement_Learning_Problems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 IL  School of Computer Science Tel-Aviv University Tel-Aviv, 69978, Israel  Editor: Sridhar Mahadevan  Abstract We incorporate statistical conﬁdence intervals in both the multi-armed bandit and the reinforcement learning problems. [sent-8, score-0.233]
</p><p>2 In the bandit problem we show that given n arms, it sufﬁces to pull the arms a total of O (n/ε2 ) log(1/δ) times to ﬁnd an ε-optimal arm with probability of at least 1 − δ. [sent-9, score-1.022]
</p><p>3 We also devise action elimination procedures in reinforcement learning algorithms. [sent-11, score-0.376]
</p><p>4 Introduction Two of the most studied problems in control, decision theory, and learning in unknown environment are the multi-armed bandit (MAB) and reinforcement learning (RL). [sent-17, score-0.233]
</p><p>5 Since the multi-armed bandit setup is simpler, we start by introducing it and later describe the reinforcement learning problem. [sent-21, score-0.255]
</p><p>6 There is a number of alternative arms, each with a stochastic reward whose probability distribution is initially unknown. [sent-23, score-0.223]
</p><p>7 We try these arms in some order, which may depend on the sequence of rewards ∗. [sent-24, score-0.246]
</p><p>8 A common objective in this context is to ﬁnd a policy for choosing the next arm to be tried, under which the sum of the expected rewards comes as close as possible to the ideal reward, i. [sent-28, score-0.809]
</p><p>9 , the expected reward that would be obtained if we were to try the “best” arm at all times. [sent-30, score-0.827]
</p><p>10 The problem was ﬁrst considered in the 50’s in the seminal work of Robbins (1952) that derives strategies that asymptotically attain an average reward that converges in the limit to the reward of the best arm. [sent-33, score-0.386]
</p><p>11 The multi-armed bandit problem was later studied in discounted, Bayesian, Markovian, expected reward, and adversarial setups. [sent-34, score-0.23]
</p><p>12 The seminal work of Lai and Robbins (1985) provides tight bounds as a function of the Kullback-Leibler divergence between the arms reward distribution, and a logarithmic growth with the number of steps. [sent-37, score-0.383]
</p><p>13 This is in contrast to most of the results for the multi-armed bandit problem, where the main aim is to maximize the expected cumulative reward while both exploring and exploiting. [sent-45, score-0.405]
</p><p>14 When the model is known, and learning is not required, there are several standard methods for calculating the optimal policy - linear programming, value iteration, policy iteration, etc. [sent-51, score-0.238]
</p><p>15 When an action in a certain state can be determined to not belong to the optimal policy in an MDP, it can be discarded and disregarded in both planning and learning. [sent-69, score-0.366]
</p><p>16 This idea, commonly known as action elimination (AE), was proposed by MacQueen (1966) in the context of planning when the MDP parameters are known. [sent-70, score-0.333]
</p><p>17 The ﬁrst algorithm in the bandit settings, Successive Elimination, has the potential to exhibit an improved behavior in cases where the differences between the expected rewards of the optimal arm and sub-optimal arms are much larger than ε. [sent-81, score-1.07]
</p><p>18 Namely, the total number of arm trials is O(n/ε2 log(1/δ)), which improves the naive bound by a factor of log n, and matches the lower bounds given in Mannor and Tsitsiklis (2004). [sent-83, score-0.677]
</p><p>19 When the expected upper estimate of the return of a certain action falls below the expected lower estimate of another action, the obviously inferior action is eliminated. [sent-86, score-0.496]
</p><p>20 When sampling arm a ∈ A a reward which is a random variable R(a) is received. [sent-105, score-0.832]
</p><p>21 , for every arm a ∈ A the reward R(a) ∈ {0, 1} (all the results apply without change if the reward is bounded in [0, 1] and in general as long as the reward is bounded with appropriate modiﬁcations). [sent-108, score-1.213]
</p><p>22 For simplicity of notations we enumerate the arms according to their expected reward p1 > p2 > . [sent-110, score-0.405]
</p><p>23 An arm with the highest expected reward is called the best arm, and denoted by a∗ , and its expected reward r∗ is the optimal reward. [sent-114, score-1.042]
</p><p>24 An arm whose expected reward is strictly less than r∗ , the expected reward of the best arm, is called a non-best arm. [sent-115, score-1.042]
</p><p>25 An arm a is called an ε-optimal arm if its expected reward is at most ε from the optimal reward, i. [sent-116, score-1.439]
</p><p>26 An algorithm for the MAB problem, at each time step t, samples an arm at and receives a reward rt (distributed according to R(at )). [sent-119, score-0.866]
</p><p>27 Eventually the algorithm must commit to a single arm and select it. [sent-123, score-0.612]
</p><p>28 Deﬁnition 1 An algorithm is a (ε, δ)-PAC algorithm for the multi armed bandit with sample complexity T , if it outputs an ε-optimal arm, a′ , with probability at least 1 − δ, when it terminates, and the number of time steps the algorithm performs until it terminates is bounded by T . [sent-125, score-0.276]
</p><p>29 A strategy for an MDP assigns, at each time t, for each state s a probability for performing action a ∈ A, given a history Ft−1 = {s1 , a1 , r1 , . [sent-133, score-0.277]
</p><p>30 While executing a strategy π we perform at time t action at in state st and observe a reward rt (distributed according to R(st , at )), and the next state st+1 distributed according to Psatt,· . [sent-137, score-0.559]
</p><p>31 In this work we focus on the discounted return, which ∞ has a parameter γ ∈ (0, 1), and the discounted return of policy π is V π = ∑t=0 γt rt , where rt is the π = H r for a given reward observed at time t. [sent-140, score-0.526]
</p><p>32 We deﬁne a value function for each state s, under policy π, as V π (s) = IEπ [∑∞ ri γi ], where the expectation is over a run of policy π starting at state s. [sent-146, score-0.332]
</p><p>33 We i=0 further denote the state-action value function as using action a in state s and then following π as: a Qπ (s, a) = R(s, a) + γ ∑ Ps,s′ V π (s′ ). [sent-147, score-0.247]
</p><p>34 We also deﬁne the policy Greedy(Q) as the policy that prescribes in each state the action that maximizes the Q-function in the state, i. [sent-156, score-0.485]
</p><p>35 For a given trajectory let: T s,a be the set of times in which we perform action a in state s and s,a,s′ be a subset of T s,a in which we reached state s′ . [sent-159, score-0.294]
</p><p>36 Also, #(s, a,t) is the number of times action T a is performed in state s up to time t, i. [sent-160, score-0.247]
</p><p>37 We start with a naive solution that samples each arm 1/(ε/2)2 ln(2n/δ) and picks the arm with the highest empirical reward. [sent-195, score-1.273]
</p><p>38 1 we consider an algorithm that eliminates one arm after the other. [sent-199, score-0.631]
</p><p>39 1 Successive Elimination The successive elimination algorithm attempts to sample each arm a minimal number of times and eliminate the arms one after the other. [sent-211, score-1.015]
</p><p>40 To motivate the successive elimination algorithm, we ﬁrst assume that the expected rewards of the arms are known, but the matching of the arms to the expected rewards is unknown. [sent-212, score-0.702]
</p><p>41 Our aim is to sample arm ai for (1/∆2 ) ln(n/δ) i times, and then eliminate it. [sent-214, score-0.659]
</p><p>42 Initially, we sample each arm (1/∆2 ) ln(n/δ) n times. [sent-216, score-0.641]
</p><p>43 Then we eliminate the arm which has the lowest empirical reward (and never sample it again). [sent-217, score-0.852]
</p><p>44 At the i-th phase we sample each of the n − i surviving arms O  1 ∆2 n−i  −  1 ∆2 n−i+1  n log ( ) δ  times and then eliminate the empirically worst arm. [sent-218, score-0.316]
</p><p>45 Then the Successive Elimination with Known Biases algorithm is an (0, δ)-PAC algorithm and its arm sample complexity is n n 1 O log( ) ∑ 2 δ i=2 ∆i  . [sent-228, score-0.641]
</p><p>46 In the second round we sample n − 1 arms tn−1 − tn times. [sent-231, score-0.285]
</p><p>47 In the kth round (1 ≤ k < n) we sample n − k + 1 arms for tn−k − tn−k+1 times. [sent-232, score-0.267]
</p><p>48 The total number of arms samples is therefore t2 + ∑n ti which is of the form (1). [sent-233, score-0.237]
</p><p>49 Consider ﬁrst a simpliﬁed 1085  E VEN -DAR , M ANNOR AND M ANSOUR  algorithm which is similar to the naive algorithm, suppose that each arm is pulled 8/(∆2 ) ln(2n/δ) 2 times. [sent-235, score-0.642]
</p><p>50 j ≥ i , ˆ where pit j is the empirical value the ith arm at time t j . [sent-238, score-0.638]
</p><p>51 If arm 1 is eliminated at time t j for some is implies that some arm i < j has higher empirical value at time t j . [sent-243, score-1.224]
</p><p>52 Next, we relax the requirement that the expected rewards of the arms are known in advance, and introduce the Successive Elimination algorithm that works with any set of biases. [sent-245, score-0.268]
</p><p>53 The algorithm we present as Algorithm 3 ﬁnds the best arm (rather than ε-best) with high probability. [sent-246, score-0.612]
</p><p>54 Proof Our main argument is that, at any time t and for any action a, the observed probability pta is ˆ within αt of the true probability pa . [sent-253, score-0.375]
</p><p>55 For any time t and action a ∈ St we have that, 2  P[| pta − pa | ≥ αt ] ≤ 2e−2αt t ≤ ˆ  2δ . [sent-254, score-0.315]
</p><p>56 cnt 2  By taking the constant c to be greater than 4 and from the union bound we have that with probability at least 1 − δ/n for any time t and any action a ∈ St , | pta − pa | ≤ αt . [sent-255, score-0.369]
</p><p>57 Therefore, with probability ˆ 1 − δ, the best arm is never eliminated. [sent-256, score-0.642]
</p><p>58 Furthermore, since αt goes to zero as t increases, eventually every non-best arm is eliminated. [sent-257, score-0.634]
</p><p>59 To eliminate a non-best arm ai we need to reach a time ti such that, ˆ ˆ ˆ ∆ti = ptai1 − ptaii ≥ 2αti . [sent-260, score-0.658]
</p><p>60 ∆2 i  To conclude, with probability of at least 1 − δ the number of arm samples is 2t2 + ∑n ti , which i=3 completes the proof. [sent-262, score-0.711]
</p><p>61 Instead of stopping when only one arm survives the elimination, it is possible to settle for stopping k when either only one arm remains or when each of the k surviving arms were sampled O( ε12 log( δ )). [sent-264, score-1.534]
</p><p>62 In the latter case the algorithm returns the best arm so far. [sent-265, score-0.643]
</p><p>63 In this case it is not hard to show that the algorithm ﬁnds an ε-optimal arm with probability at least 1 − δ after O  ∑  i:∆i >ε  n log( δ∆i )  ∆2 i  +  N(∆, ε) N(∆, ε) log 2 ε δ  ,  where N(∆, ε) = |{i | ∆i < ε}| is the number of arms which are ε-optimal. [sent-266, score-0.867]
</p><p>64 We do not expect the best arm to be empirically “the best”, we only expect an ε-optimal arm to be above the median. [sent-270, score-1.224]
</p><p>65 1087  E VEN -DAR , M ANNOR AND M ANSOUR  Input : ε > 0, δ > 0 Output : An arm Set S1 = A, ε1 = ε/4, δ1 = δ/2, ℓ = 1. [sent-271, score-0.612]
</p><p>66 First we show that in the ℓ-th phase the expected reward of the best arm in Sℓ drops by at most  Lemma 11 For the Median Elimination(ε, δ) algorithm we have that for every phase ℓ: P[max p j ≤ max pi + εℓ ] ≥ 1 − δℓ . [sent-274, score-0.893]
</p><p>67 We bound the failure probability by looking at the event E1 = { p1 < p1 − ε1 /2}, which is ˆ the case that the empirical estimate of the best arm is pessimistic. [sent-276, score-0.675]
</p><p>68 In case E1 does not hold, we calculate the probability that an arm j which is not an ε1 -optimal arm is empirically better than the best arm. [sent-278, score-1.254]
</p><p>69 Next we prove that arm sample complexity is bounded by O((n/ε2 ) log(1/δ)). [sent-283, score-0.641]
</p><p>70 Proof The number of arm samples in the ℓ-th round is 4nℓ log(3/δℓ )/ε2 . [sent-285, score-0.659]
</p><p>71 In each round we reduce the optimal reward log2 (n) of the surviving arms by at most εi so that the total error is bounded by ∑i=1 εi ≤ ε. [sent-292, score-0.433]
</p><p>72 2 we consider model-free learning and suggest a version of the Q-learning algorithm that incorporates action elimination and stopping conditions. [sent-299, score-0.382]
</p><p>73 , estimate the immediate reward and the next state distribution. [sent-307, score-0.24]
</p><p>74 Then by either value iteration, policy iteration, or linear programming on the learned (empirical) model, we ﬁnd the exact optimal policy for the empirical model. [sent-308, score-0.238]
</p><p>75 We arrive at:   ˆ ˆ P R(s, a) + IEs′ [V k (s′ )] + (k + 1)Rmax   − ln (  ≤e   2  ln ( c|S||A|(k+1) ) k ′ δ ′ [V (s )] < R(s, a) + IEs  |T s,a |  (k+1)Rmax c|S||A|(k+1)2 √ s,a )|T s,a | δ |T | ((k+1)Rmax )2  1090  2  =  δ . [sent-328, score-0.266]
</p><p>76 t  Lemma 14 With probability at least 1 − δ we have that Qδ (s, a) ≥ Q∗ (s, a) ≥ Qtδ (s, a) for every state s, action a and time t. [sent-344, score-0.299]
</p><p>77 For the basis, since V ∗ is not a random variable we can apply Hoeffding’s inequality and obtain that for every state action pair (s, a) 2  ˆ ˆ P R(s, a) + γIEs′ [V (s )] +Vmax ∗  ′  ln( ct |S||A| ) δ < R(s, a) + γIEs′ [V ∗ (s′ )] |T s,a| ct 2 |S||A| δ ≤ e− ln( δ ) = 2 . [sent-354, score-0.345]
</p><p>78 1092  ACTION E LIMINATION FOR R EINFORCEMENT L EARNING  Input : MDP M, ε > 0, δ > 0 Output : A policy for M Choose arbitrarily an initial state s0 , let t = 0, and let U0 = {(s, a)|s ∈ S, a ∈ A} repeat At state st perform any action a s. [sent-364, score-0.443]
</p><p>79 Therefore, with probability of at least 1 − δ the optimal action has not been eliminated in any state in any time t. [sent-372, score-0.277]
</p><p>80 Furthermore, any action b in state s that has not been eliminated satisﬁes Q∗ (s, b) − Qδ (s, b) ≤ Qδ (s, b) − Qδ (s, b) ≤ ε(1 − γ)/2. [sent-373, score-0.247]
</p><p>81 Proposition 18 With probability at least 1 − δ we have that for every state action pair (s, a) and time t: t Qδ (s, a) ≥ Q∗ (s, a) ≥ Qtδ (s, a). [sent-389, score-0.299]
</p><p>82 The setup is that of parallel sampling where the decision maker can sample every state and action pair, as opposed to the typical Q-learning setup where a single trajectory is followed. [sent-411, score-0.369]
</p><p>83 In phased Q-learning the value of Vk (s) is ﬁxed during the kth phased and updated only at the end of the phase. [sent-417, score-0.296]
</p><p>84 This implies that for every state and action (s, a) we can deﬁne a random variable Ys (a) whose value is R(s, a) + γVk (s′ ), where R(s, a) is the random variable representing the reward a and s′ is distributed using Ps,s′ . [sent-418, score-0.462]
</p><p>85 Our aim is to ﬁnd, at each state, the action that maximizes the expected reward, and estimate its expected reward, where the rewards are Ys (a). [sent-419, score-0.3]
</p><p>86 • GetArmB () - returns the arm a that B wants to sample next. [sent-424, score-0.672]
</p><p>87 • U pdateB (a, r) - informs B the latest reward r of arm a. [sent-425, score-0.805]
</p><p>88 (We assume that on termination, with probability at least 1 − δ, the arm a is an ε-optimal arm and |r∗ − v| ≤ ε. [sent-427, score-1.254]
</p><p>89 Suppose that we have some (ε, δ)-PAC MAB algorithm B, and assume B has arm sample complexity TB (ε, δ). [sent-430, score-0.641]
</p><p>90 Then the MAB Phased QLearning(ε, δ) algorithm outputs a policy π which is ε-optimal policy with probability at least 1 − δ, and has sample complexity of ˆ ˆ T (ε, δ) = |S|TB (ε, δ) logγ (  ˆ ε Vmax |S| )=O ln( )TB 2Vmax 1 − γ (1 − γ)ε 1096  δ(1 − γ) ε(1 − γ)2 , 2 |S| ln(Vmax /ε)  . [sent-434, score-0.297]
</p><p>91 Proof First we bound the probability that B outputs an arm which is not ε-optimal. [sent-438, score-0.642]
</p><p>92 Let ms,a denote the number of times the state action pair (s, a) was sampled in the k-th iteration. [sent-448, score-0.247]
</p><p>93 By deﬁnition, the MAB Phased Q-Learning algorithm samples at each state and action during every ˆ ˆ ˆ phase TB (ε, δ). [sent-460, score-0.31]
</p><p>94 We show that by using the median elimination algorithm, the arm sample complexity can be reduced by a factor of log(|A|). [sent-464, score-0.812]
</p><p>95 The reward in each state behaves according to H0 or one of the Hℓ . [sent-491, score-0.24]
</p><p>96 Note that even if L returns a randomized policy, we will determine that the action with the highest reward is the best one (this is the reason ˆ for the factor of 2 in determining ε). [sent-497, score-0.424]
</p><p>97 Both model free AE algorithm and standard Q-learning choose the action in each state uniformly at random. [sent-503, score-0.247]
</p><p>98 We disregard the full queue state in which every action is optimal. [sent-558, score-0.323]
</p><p>99 Future Directions Extending the concept of action elimination to large state spaces is probably the most important direction. [sent-641, score-0.38]
</p><p>100 The sample complexity of exploration in the multi-armed bandit problem. [sent-810, score-0.249]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('arm', 0.612), ('ae', 0.276), ('vmax', 0.215), ('action', 0.2), ('reward', 0.193), ('arms', 0.19), ('bandit', 0.19), ('mab', 0.168), ('ies', 0.161), ('phased', 0.138), ('ln', 0.133), ('elimination', 0.133), ('mdp', 0.121), ('policy', 0.119), ('annor', 0.112), ('ansour', 0.112), ('limination', 0.112), ('ven', 0.112), ('vk', 0.105), ('rmax', 0.095), ('ie', 0.088), ('einforcement', 0.085), ('actions', 0.068), ('horizon', 0.067), ('tsitsiklis', 0.061), ('pta', 0.06), ('mdps', 0.059), ('qt', 0.056), ('rewards', 0.056), ('pa', 0.055), ('queue', 0.054), ('steady', 0.052), ('tb', 0.052), ('discounted', 0.05), ('mannor', 0.05), ('stopping', 0.049), ('state', 0.047), ('howard', 0.044), ('reinforcement', 0.043), ('rl', 0.042), ('maxa', 0.042), ('rt', 0.042), ('mansour', 0.039), ('ct', 0.038), ('median', 0.038), ('tn', 0.038), ('hoeffding', 0.036), ('log', 0.035), ('automobile', 0.034), ('ut', 0.034), ('earning', 0.033), ('singh', 0.033), ('failure', 0.033), ('bertsekas', 0.033), ('successive', 0.033), ('returns', 0.031), ('return', 0.03), ('exploration', 0.03), ('kearns', 0.03), ('naive', 0.03), ('probability', 0.03), ('st', 0.03), ('sample', 0.029), ('discount', 0.028), ('ti', 0.028), ('round', 0.028), ('terminates', 0.027), ('sampling', 0.027), ('packets', 0.026), ('biases', 0.026), ('foreach', 0.026), ('ormoneit', 0.026), ('pit', 0.026), ('queueing', 0.026), ('induction', 0.024), ('union', 0.024), ('expected', 0.022), ('pac', 0.022), ('puterman', 0.022), ('phase', 0.022), ('completes', 0.022), ('every', 0.022), ('supplies', 0.022), ('surviving', 0.022), ('upper', 0.022), ('setup', 0.022), ('kth', 0.02), ('precision', 0.02), ('bars', 0.02), ('lemma', 0.019), ('greedy', 0.019), ('samples', 0.019), ('bi', 0.019), ('agent', 0.019), ('eliminates', 0.019), ('eliminate', 0.018), ('shie', 0.018), ('adversarial', 0.018), ('lai', 0.018), ('regret', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999875 <a title="10-tfidf-1" href="./jmlr-2006-Action_Elimination_and_Stopping_Conditions_for_the_Multi-Armed_Bandit_and_Reinforcement_Learning_Problems.html">10 jmlr-2006-Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems</a></p>
<p>Author: Eyal Even-Dar, Shie Mannor, Yishay Mansour</p><p>Abstract: We incorporate statistical conﬁdence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it sufﬁces to pull the arms a total of O (n/ε2 ) log(1/δ) times to ﬁnd an ε-optimal arm with probability of at least 1 − δ. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the conﬁdence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over ε-greedy Q-learning.</p><p>2 0.16583602 <a title="10-tfidf-2" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<p>Author: Jelle R. Kok, Nikos Vlassis</p><p>Abstract: In this article we describe a set of scalable techniques for learning the behavior of a group of agents in a collaborative multiagent setting. As a basis we use the framework of coordination graphs of Guestrin, Koller, and Parr (2002a) which exploits the dependencies between agents to decompose the global payoff function into a sum of local terms. First, we deal with the single-state case and describe a payoff propagation algorithm that computes the individual actions that approximately maximize the global payoff function. The method can be viewed as the decision-making analogue of belief propagation in Bayesian networks. Second, we focus on learning the behavior of the agents in sequential decision-making tasks. We introduce different model-free reinforcementlearning techniques, unitedly called Sparse Cooperative Q-learning, which approximate the global action-value function based on the topology of a coordination graph, and perform updates using the contribution of the individual agents to the maximal global action value. The combined use of an edge-based decomposition of the action-value function and the payoff propagation algorithm for efﬁcient action selection, result in an approach that scales only linearly in the problem size. We provide experimental evidence that our method outperforms related multiagent reinforcement-learning methods based on temporal differences. Keywords: collaborative multiagent system, coordination graph, reinforcement learning, Qlearning, belief propagation</p><p>3 0.1247794 <a title="10-tfidf-3" href="./jmlr-2006-Point-Based_Value_Iteration_for_Continuous_POMDPs.html">74 jmlr-2006-Point-Based Value Iteration for Continuous POMDPs</a></p>
<p>Author: Josep M. Porta, Nikos Vlassis, Matthijs T.J. Spaan, Pascal Poupart</p><p>Abstract: We propose a novel approach to optimize Partially Observable Markov Decisions Processes (POMDPs) deﬁned on continuous spaces. To date, most algorithms for model-based POMDPs are restricted to discrete states, actions, and observations, but many real-world problems such as, for instance, robot navigation, are naturally deﬁned on continuous spaces. In this work, we demonstrate that the value function for continuous POMDPs is convex in the beliefs over continuous state spaces, and piecewise-linear convex for the particular case of discrete observations and actions but still continuous states. We also demonstrate that continuous Bellman backups are contracting and isotonic ensuring the monotonic convergence of value-iteration algorithms. Relying on those properties, we extend the P ERSEUS algorithm, originally developed for discrete POMDPs, to work in continuous state spaces by representing the observation, transition, and reward models using Gaussian mixtures, and the beliefs using Gaussian mixtures or particle sets. With these representations, the integrals that appear in the Bellman backup can be computed in closed form and, therefore, the algorithm is computationally feasible. Finally, we further extend P ERSEUS to deal with continuous action and observation sets by designing effective sampling approaches. Keywords: planning under uncertainty, partially observable Markov decision processes, continuous state space, continuous action space, continuous observation space, point-based value iteration</p><p>4 0.11869781 <a title="10-tfidf-4" href="./jmlr-2006-Policy_Gradient_in_Continuous_Time.html">75 jmlr-2006-Policy Gradient in Continuous Time</a></p>
<p>Author: Rémi Munos</p><p>Abstract: Policy search is a method for approximately solving an optimal control problem by performing a parametric optimization search in a given class of parameterized policies. In order to process a local optimization technique, such as a gradient method, we wish to evaluate the sensitivity of the performance measure with respect to the policy parameters, the so-called policy gradient. This paper is concerned with the estimation of the policy gradient for continuous-time, deterministic state dynamics, in a reinforcement learning framework, that is, when the decision maker does not have a model of the state dynamics. We show that usual likelihood ratio methods used in discrete-time, fail to proceed the gradient because they are subject to variance explosion when the discretization time-step decreases to 0. We describe an alternative approach based on the approximation of the pathwise derivative, which leads to a policy gradient estimate that converges almost surely to the true gradient when the timestep tends to 0. The underlying idea starts with the derivation of an explicit representation of the policy gradient using pathwise derivation. This derivation makes use of the knowledge of the state dynamics. Then, in order to estimate the gradient from the observable data only, we use a stochastic policy to discretize the continuous deterministic system into a stochastic discrete process, which enables to replace the unknown coefﬁcients by quantities that solely depend on known data. We prove the almost sure convergence of this estimate to the true policy gradient when the discretization time-step goes to zero. The method is illustrated on two target problems, in discrete and continuous control spaces. Keywords: optimal control, reinforcement learning, policy search, sensitivity analysis, parametric optimization, gradient estimate, likelihood ratio method, pathwise derivation 1. Introduction and Statement of the Problem We consider an optimal control problem with continuous state (xt ∈ IRd )t≥0 whose state dynamics is deﬁned according to the controlled differential equation: dxt = f (xt , ut ), dt (1) where the control (ut )t≥0 is a Lebesgue measurable function with values in a control space U. Note that the state-dynamics f may also depend on time, but we omit this dependency in the notation, for simplicity. We intend to maximize a functional J that depends on the trajectory (xt )0≤t≤T over a ﬁnite-time horizon T > 0. For simplicity, in the paper, we illustrate the case of a terminal reward c 2006 Rémi Munos. M UNOS only: J(x; (ut )t≥0 ) := r(xT ), (2) where r : IRd → IR is the reward function. Extension to the case of general functional of the kind J(x; (ut )t≥0 ) = Z T 0 r(t, xt )dt + R(xT ), (3) with r and R being current and terminal reward functions, would easily follow, as indicated in Remark 1. The optimal control problem of ﬁnding a control (ut )t≥0 that maximizes the functional is replaced by a parametric optimization problem for which we search for a good feed-back control law in a given class of parameterized policies {πα : [0, T ] × IRd → U}α , where α ∈ IRm is the parameter. The control ut ∈ U (or action) at time t is ut = πα (t, xt ), and we may write the dynamics of the resulting feed-back system as dxt = fα (xt ), (4) dt where fα (xt ) := f (x, πα (t, x)). In the paper, we will make the assumption that fα is C 2 , with bounded derivatives. Let us deﬁne the performance measure V (α) := J(x; πα (t, xt )t≥0 ), where its dependency with respect to (w.r.t.) the parameter α is emphasized. One may also consider an average performance measure according to some distribution µ for the initial state: V (α) := E[J(x; πα (t, xt )t≥0 )|x ∼ µ]. In order to ﬁnd a local maximum of V (α), one may perform a local search, such as a gradient ascent method α ← α + η∇αV (α), (5) with an adequate step η (see for example (Polyak, 1987; Kushner and Yin, 1997)). The computation of the gradient ∇αV (α) is the object of this paper. A ﬁrst method would be to approximate the gradient by a ﬁnite-difference quotient for each of the m components of the parameter: V (α + εei ) −V (α) , ε for some small value of ε (we use the notation ∂α instead of ∇α to indicate that it is a singledimensional derivative). This ﬁnite-difference method requires the simulation of m + 1 trajectories to compute an approximation of the true gradient. When the number of parameters is large, this may be computationally expensive. However, this simple method may be efﬁcient if the number of parameters is relatively small. In the rest of the paper we will not consider this approach, and will aim at computing the gradient using one trajectory only. ∂αi V (α) ≃ 772 P OLICY G RADIENT IN C ONTINUOUS T IME Pathwise estimation of the gradient. We now illustrate that if the decision-maker has access to a model of the state dynamics, then a pathwise derivation would directly lead to the policy gradient. Indeed, let us deﬁne the gradient of the state with respect to the parameter: zt := ∇α xt (i.e. zt is deﬁned as a d × m-matrix whose (i, j)-component is the derivative of the ith component of xt w.r.t. α j ). Our smoothness assumption on fα allows to differentiate the state dynamics (4) w.r.t. α, which provides the dynamics on (zt ): dzt = ∇α fα (xt ) + ∇x fα (xt )zt , dt (6) where the coefﬁcients ∇α fα and ∇x fα are, respectively, the derivatives of f w.r.t. the parameter (matrix of size d × m) and the state (matrix of size d × d). The initial condition for z is z0 = 0. When the reward function r is smooth (i.e. continuously differentiable), one may apply a pathwise differentiation to derive a gradient formula (see e.g. (Bensoussan, 1988) or (Yang and Kushner, 1991) for an extension to the stochastic case): ∇αV (α) = ∇x r(xT )zT . (7) Remark 1 In the more general setting of a functional (3), the gradient is deduced (by linearity) from the above formula: ∇αV (α) = Z T 0 ∇x r(t, xt )zt dt + ∇x R(xT )zT . What is known from the agent? The decision maker (call it the agent) that intends to design a good controller for the dynamical system may or may not know a model of the state dynamics f . In case the dynamics is known, the state gradient zt = ∇α xt may be computed from (6) along the trajectory and the gradient of the performance measure w.r.t. the parameter α is deduced at time T from (7), which allows to perform the gradient ascent step (5). However, in this paper we consider a Reinforcement Learning (Sutton and Barto, 1998) setting in which the state dynamics is unknown from the agent, but we still assume that the state is fully observable. The agent knows only the response of the system to its control. To be more precise, the available information to the agent at time t is its own control policy πα and the trajectory (xs )0≤s≤t up to time t. At time T , the agent receives the reward r(xT ) and, in this paper, we assume that the gradient ∇r(xT ) is available to the agent. From this point of view, it seems impossible to derive the state gradient zt from (6), since ∇α f and ∇x f are unknown. The term ∇x f (xt ) may be approximated by a least squares method from the observation of past states (xs )s≤t , as this will be explained later on in subsection 3.2. However the term ∇α f (xt ) cannot be calculated analogously. In this paper, we introduce the idea of using stochastic policies to approximate the state (xt ) and the state gradient (zt ) by discrete-time stochastic processes (Xt∆ ) and (Zt∆ ) (with ∆ being some discretization time-step). We show how Zt∆ can be computed without the knowledge of ∇α f , but only from information available to the agent. ∆ ∆ We prove the convergence (with probability one) of the gradient estimate ∇x r(XT )ZT derived from the stochastic processes to ∇αV (α) when ∆ → 0. Here, almost sure convergence is obtained using the concentration of measure phenomenon (Talagrand, 1996; Ledoux, 2001). 773 M UNOS y ∆ XT ∆ X t2 ∆ Xt 0 fα ∆ x Xt 1 Figure 1: A trajectory (Xt∆ )0≤n≤N and the state dynamics vector fα of the continuous process n (xt )0≤t≤T . Likelihood ratio method? It is worth mentioning that this strong convergence result contrasts with the usual likelihood ratio method (also called score method) in discrete time (see e.g. (Reiman and Weiss, 1986; Glynn, 1987) or more recently in the reinforcement learning literature (Williams, 1992; Sutton et al., 2000; Baxter and Bartlett, 2001; Marbach and Tsitsiklis, 2003)) for which the policy gradient estimate is subject to variance explosion when the discretization time-step ∆ tends to 0. The intuitive reason for that problem lies in the fact that the number of decisions before getting the reward grows to inﬁnity when ∆ → 0 (the variance of likelihood ratio estimates being usually linear with the number of decisions). Let us illustrate this problem on a simple 2 dimensional process. Consider the deterministic continuous process (xt )0≤t≤1 deﬁned by the state dynamics: dxt = fα := dt α 1−α , (8) (0 < α < 1) with initial condition x0 = (0 0)′ (where ′ denotes the transpose operator). The performance measure V (α) is the reward at the terminal state at time T = 1, with the reward function being the ﬁrst coordinate of the state r((x y)′ ) := x. Thus V (α) = r(xT =1 ) = α and its derivative is ∇αV (α) = 1. Let (Xt∆ )0≤n≤N ∈ IR2 be a discrete time stochastic process (the discrete times being {tn = n ∆ n∆}n=0...N with the discretization time-step ∆ = 1/N) that starts from initial state X0 = x0 = (0 0)′ and makes N random moves of length ∆ towards the right (action u1 ) or the top (action u2 ) (see Figure 1) according to the stochastic policy (i.e., the probability of choosing the actions in each state x) πα (u1 |x) = α, πα (u2 |x) = 1 − α. The process is thus deﬁned according to the dynamics: Xt∆ = Xt∆ + n n+1 Un 1 −Un ∆, (9) where (Un )0≤n < N and all ∞ N > 0), there exists a constant C that does not depend on N such that dn ≤ C/N. Thus we may take D2 = C2 /N. Now, from the previous paragraph, ||E[XN ] − xN || ≤ e(N), with e(N) → 0 when N → ∞. This means that ||h − E[h]|| + e(N) ≥ ||XN − xN ||, thus P(||h − E[h]|| ≥ ε + e(N)) ≥ P(||XN − xN || ≥ ε), and we deduce from (31) that 2 /(2C 2 ) P(||XN − xN || ≥ ε) ≤ 2e−N(ε+e(N)) . Thus, for all ε > 0, the series ∑N≥0 P(||XN − xN || ≥ ε) converges. Now, from Borel-Cantelli lemma, we deduce that for all ε > 0, there exists Nε such that for all N ≥ Nε , ||XN − xN || < ε, which ∆→0 proves the almost sure convergence of XN to xN as N → ∞ (i.e. XT −→ xT almost surely). Appendix C. Proof of Proposition 8 ′ First, note that Qt = X X ′ − X X is a symmetric, non-negative matrix, since it may be rewritten as 1 nt ∑ (Xs+ − X)(Xs+ − X)′ . s∈S(t) In solving the least squares problem (21), we deduce b = ∆X + AX∆, thus min A,b 1 1 ∑ ∆Xs − b −A(Xs+2 ∆Xs )∆ nt s∈S(t) ≤ 2 = min A 1 ∑ ∆Xs − ∆X − A(Xs+ − X)∆ nt s∈S(t) 1 ∑ ∆Xs− ∆X− ∇x f (X, ut )(Xs+− X)∆ nt s∈S(t) 2 2 . (32) Now, since Xs = X + O(∆) one may obtain like in (19) and (20) (by replacing Xt by X) that: ∆Xs − ∆X − ∇x f (X, ut )(Xs+ − X)∆ = O(∆3 ). (33) We deduce from (32) and (33) that 1 nt ∑ ∇x f (Xt , ut ) − ∇x f (X, ut ) (Xs+ − X)∆ 2 = O(∆6 ). s∈S(t) By developing each component, d ∑ ∇x f (Xt , ut ) − ∇x f (X, ut ) i=1 row i Qt ∇x f (Xt , ut ) − ∇x f (X, ut ) ′ row i = O(∆4 ). Now, from the deﬁnition of ν(∆), for all vector u ∈ IRd , u′ Qt u ≥ ν(∆)||u||2 , thus ν(∆)||∇x f (Xt , ut ) − ∇x f (X, ut )||2 = O(∆4 ). Condition (23) yields ∇x f (Xt , ut ) = ∇x f (X, ut ) + o(1), and since ∇x f (Xt , ut ) = ∇x f (X, ut ) + O(∆), we deduce lim ∇x f (Xt , ut ) = ∇x f (Xt , ut ). ∆→0 789 M UNOS References J. Baxter and P. L. Bartlett. Inﬁnite-horizon gradient-based policy search. Journal of Artiﬁcial Intelligence Research, 15:319–350, 2001. A. Bensoussan. Perturbation methods in optimal control. Wiley/Gauthier-Villars Series in Modern Applied Mathematics. John Wiley & Sons Ltd., Chichester, 1988. Translated from the French by C. Tomson. A. Bogdanov. Optimal control of a double inverted pendulum on a cart. Technical report CSE-04006, CSEE, OGI School of Science and Engineering, OHSU, 2004. P. W. Glynn. Likelihood ratio gradient estimation: an overview. In A. Thesen, H. Grant, and W. D. Kelton, editors, Proceedings of the 1987 Winter Simulation Conference, pages 366–375, 1987. E. Gobet and R. Munos. Sensitivity analysis using Itô-Malliavin calculus and martingales. application to stochastic optimal control. SIAM journal on Control and Optimization, 43(5):1676–1713, 2005. G. H. Golub and C. F. Van Loan. Matrix Computations, 3rd ed. Baltimore, MD: Johns Hopkins, 1996. R. E. Kalman, P. L. Falb, and M. A. Arbib. Topics in Mathematical System Theory. New York: McGraw Hill, 1969. P. E. Kloeden and E. Platen. Numerical Solutions of Stochastic Differential Equations. SpringerVerlag, 1995. H. J. Kushner and G. Yin. Stochastic Approximation Algorithms and Applications. Springer-Verlag, Berlin and New York, 1997. S. M. LaValle. Planning Algorithms. Cambridge University Press, 2006. M. Ledoux. The concentration of measure phenomenon. American Mathematical Society, Providence, RI, 2001. P. Marbach and J. N. Tsitsiklis. Approximate gradient methods in policy-space optimization of Markov reward processes. Journal of Discrete Event Dynamical Systems, 13:111–148, 2003. B. T. Polyak. Introduction to Optimization. Optimization Software Inc., New York, 1987. M. I. Reiman and A. Weiss. Sensitivity analysis via likelihood ratios. In J. Wilson, J. Henriksen, and S. Roberts, editors, Proceedings of the 1986 Winter Simulation Conference, pages 285–289, 1986. R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. Bradford Book, 1998. R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. Neural Information Processing Systems. MIT Press, pages 1057–1063, 2000. 790 P OLICY G RADIENT IN C ONTINUOUS T IME M. Talagrand. A new look at independence. Annals of Probability, 24:1–34, 1996. R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229–256, 1992. J. Yang and H. J. Kushner. A Monte Carlo method for sensitivity analysis and parametric optimization of nonlinear stochastic systems. SIAM J. Control Optim., 29(5):1216–1249, 1991. 791</p><p>5 0.090853095 <a title="10-tfidf-5" href="./jmlr-2006-Causal_Graph_Based_Decomposition_of_Factored_MDPs.html">19 jmlr-2006-Causal Graph Based Decomposition of Factored MDPs</a></p>
<p>Author: Anders Jonsson, Andrew Barto</p><p>Abstract: We present Variable Inﬂuence Structure Analysis, or VISA, an algorithm that performs hierarchical decomposition of factored Markov decision processes. VISA uses a dynamic Bayesian network model of actions, and constructs a causal graph that captures relationships between state variables. In tasks with sparse causal graphs VISA exploits structure by introducing activities that cause the values of state variables to change. The result is a hierarchy of activities that together represent a solution to the original task. VISA performs state abstraction for each activity by ignoring irrelevant state variables and lower-level activities. In addition, we describe an algorithm for constructing compact models of the activities introduced. State abstraction and compact activity models enable VISA to apply efﬁcient algorithms to solve the stand-alone subtask associated with each activity. Experimental results show that the decomposition introduced by VISA can signiﬁcantly accelerate construction of an optimal, or near-optimal, policy. Keywords: Markov decision processes, hierarchical decomposition, state abstraction</p><p>6 0.087092176 <a title="10-tfidf-6" href="./jmlr-2006-A_Simulation-Based_Algorithm_for_Ergodic_Control_of_Markov_Chains_Conditioned_on_Rare_Events.html">7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</a></p>
<p>7 0.063754037 <a title="10-tfidf-7" href="./jmlr-2006-Evolutionary_Function_Approximation_for_Reinforcement_Learning.html">30 jmlr-2006-Evolutionary Function Approximation for Reinforcement Learning</a></p>
<p>8 0.047354314 <a title="10-tfidf-8" href="./jmlr-2006-Geometric_Variance_Reduction_in_Markov_Chains%3A_Application_to_Value_Function_and_Gradient_Estimation.html">35 jmlr-2006-Geometric Variance Reduction in Markov Chains: Application to Value Function and Gradient Estimation</a></p>
<p>9 0.032964237 <a title="10-tfidf-9" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>10 0.032747176 <a title="10-tfidf-10" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>11 0.030484172 <a title="10-tfidf-11" href="./jmlr-2006-Worst-Case_Analysis_of_Selective_Sampling_for_Linear_Classification.html">96 jmlr-2006-Worst-Case Analysis of Selective Sampling for Linear Classification</a></p>
<p>12 0.027825044 <a title="10-tfidf-12" href="./jmlr-2006-Toward_Attribute_Efficient_Learning_of_Decision_Lists_and_Parities.html">92 jmlr-2006-Toward Attribute Efficient Learning of Decision Lists and Parities</a></p>
<p>13 0.027364047 <a title="10-tfidf-13" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>14 0.026364412 <a title="10-tfidf-14" href="./jmlr-2006-Expectation_Correction_for_Smoothed_Inference_in_Switching_Linear_Dynamical_Systems.html">32 jmlr-2006-Expectation Correction for Smoothed Inference in Switching Linear Dynamical Systems</a></p>
<p>15 0.024842946 <a title="10-tfidf-15" href="./jmlr-2006-Some_Discriminant-Based_PAC_Algorithms.html">81 jmlr-2006-Some Discriminant-Based PAC Algorithms</a></p>
<p>16 0.024783906 <a title="10-tfidf-16" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>17 0.024050567 <a title="10-tfidf-17" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>18 0.023311161 <a title="10-tfidf-18" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>19 0.022715436 <a title="10-tfidf-19" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>20 0.021067888 <a title="10-tfidf-20" href="./jmlr-2006-A_Linear_Non-Gaussian_Acyclic_Model_for_Causal_Discovery.html">4 jmlr-2006-A Linear Non-Gaussian Acyclic Model for Causal Discovery</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.157), (1, 0.222), (2, -0.262), (3, 0.027), (4, 0.248), (5, 0.04), (6, -0.063), (7, -0.014), (8, -0.042), (9, -0.05), (10, 0.028), (11, 0.059), (12, -0.007), (13, 0.03), (14, -0.031), (15, -0.039), (16, 0.053), (17, 0.181), (18, -0.058), (19, -0.124), (20, -0.067), (21, -0.046), (22, 0.085), (23, -0.038), (24, -0.039), (25, 0.067), (26, -0.055), (27, -0.123), (28, 0.02), (29, -0.003), (30, -0.005), (31, 0.133), (32, -0.001), (33, -0.04), (34, -0.024), (35, -0.051), (36, 0.042), (37, -0.052), (38, -0.078), (39, -0.004), (40, 0.027), (41, 0.024), (42, 0.042), (43, 0.07), (44, 0.088), (45, 0.102), (46, 0.079), (47, 0.031), (48, -0.058), (49, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94418275 <a title="10-lsi-1" href="./jmlr-2006-Action_Elimination_and_Stopping_Conditions_for_the_Multi-Armed_Bandit_and_Reinforcement_Learning_Problems.html">10 jmlr-2006-Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems</a></p>
<p>Author: Eyal Even-Dar, Shie Mannor, Yishay Mansour</p><p>Abstract: We incorporate statistical conﬁdence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it sufﬁces to pull the arms a total of O (n/ε2 ) log(1/δ) times to ﬁnd an ε-optimal arm with probability of at least 1 − δ. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the conﬁdence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over ε-greedy Q-learning.</p><p>2 0.75761634 <a title="10-lsi-2" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<p>Author: Jelle R. Kok, Nikos Vlassis</p><p>Abstract: In this article we describe a set of scalable techniques for learning the behavior of a group of agents in a collaborative multiagent setting. As a basis we use the framework of coordination graphs of Guestrin, Koller, and Parr (2002a) which exploits the dependencies between agents to decompose the global payoff function into a sum of local terms. First, we deal with the single-state case and describe a payoff propagation algorithm that computes the individual actions that approximately maximize the global payoff function. The method can be viewed as the decision-making analogue of belief propagation in Bayesian networks. Second, we focus on learning the behavior of the agents in sequential decision-making tasks. We introduce different model-free reinforcementlearning techniques, unitedly called Sparse Cooperative Q-learning, which approximate the global action-value function based on the topology of a coordination graph, and perform updates using the contribution of the individual agents to the maximal global action value. The combined use of an edge-based decomposition of the action-value function and the payoff propagation algorithm for efﬁcient action selection, result in an approach that scales only linearly in the problem size. We provide experimental evidence that our method outperforms related multiagent reinforcement-learning methods based on temporal differences. Keywords: collaborative multiagent system, coordination graph, reinforcement learning, Qlearning, belief propagation</p><p>3 0.65353221 <a title="10-lsi-3" href="./jmlr-2006-Causal_Graph_Based_Decomposition_of_Factored_MDPs.html">19 jmlr-2006-Causal Graph Based Decomposition of Factored MDPs</a></p>
<p>Author: Anders Jonsson, Andrew Barto</p><p>Abstract: We present Variable Inﬂuence Structure Analysis, or VISA, an algorithm that performs hierarchical decomposition of factored Markov decision processes. VISA uses a dynamic Bayesian network model of actions, and constructs a causal graph that captures relationships between state variables. In tasks with sparse causal graphs VISA exploits structure by introducing activities that cause the values of state variables to change. The result is a hierarchy of activities that together represent a solution to the original task. VISA performs state abstraction for each activity by ignoring irrelevant state variables and lower-level activities. In addition, we describe an algorithm for constructing compact models of the activities introduced. State abstraction and compact activity models enable VISA to apply efﬁcient algorithms to solve the stand-alone subtask associated with each activity. Experimental results show that the decomposition introduced by VISA can signiﬁcantly accelerate construction of an optimal, or near-optimal, policy. Keywords: Markov decision processes, hierarchical decomposition, state abstraction</p><p>4 0.57373762 <a title="10-lsi-4" href="./jmlr-2006-Point-Based_Value_Iteration_for_Continuous_POMDPs.html">74 jmlr-2006-Point-Based Value Iteration for Continuous POMDPs</a></p>
<p>Author: Josep M. Porta, Nikos Vlassis, Matthijs T.J. Spaan, Pascal Poupart</p><p>Abstract: We propose a novel approach to optimize Partially Observable Markov Decisions Processes (POMDPs) deﬁned on continuous spaces. To date, most algorithms for model-based POMDPs are restricted to discrete states, actions, and observations, but many real-world problems such as, for instance, robot navigation, are naturally deﬁned on continuous spaces. In this work, we demonstrate that the value function for continuous POMDPs is convex in the beliefs over continuous state spaces, and piecewise-linear convex for the particular case of discrete observations and actions but still continuous states. We also demonstrate that continuous Bellman backups are contracting and isotonic ensuring the monotonic convergence of value-iteration algorithms. Relying on those properties, we extend the P ERSEUS algorithm, originally developed for discrete POMDPs, to work in continuous state spaces by representing the observation, transition, and reward models using Gaussian mixtures, and the beliefs using Gaussian mixtures or particle sets. With these representations, the integrals that appear in the Bellman backup can be computed in closed form and, therefore, the algorithm is computationally feasible. Finally, we further extend P ERSEUS to deal with continuous action and observation sets by designing effective sampling approaches. Keywords: planning under uncertainty, partially observable Markov decision processes, continuous state space, continuous action space, continuous observation space, point-based value iteration</p><p>5 0.47187948 <a title="10-lsi-5" href="./jmlr-2006-Evolutionary_Function_Approximation_for_Reinforcement_Learning.html">30 jmlr-2006-Evolutionary Function Approximation for Reinforcement Learning</a></p>
<p>Author: Shimon Whiteson, Peter Stone</p><p>Abstract: Temporal difference methods are theoretically grounded and empirically effective methods for addressing reinforcement learning problems. In most real-world reinforcement learning tasks, TD methods require a function approximator to represent the value function. However, using function approximators requires manually making crucial representational decisions. This paper investigates evolutionary function approximation, a novel approach to automatically selecting function approximator representations that enable efﬁcient individual learning. This method evolves individuals that are better able to learn. We present a fully implemented instantiation of evolutionary function approximation which combines NEAT, a neuroevolutionary optimization technique, with Q-learning, a popular TD method. The resulting NEAT+Q algorithm automatically discovers effective representations for neural network function approximators. This paper also presents on-line evolutionary computation, which improves the on-line performance of evolutionary computation by borrowing selection mechanisms used in TD methods to choose individual actions and using them in evolutionary computation to select policies for evaluation. We evaluate these contributions with extended empirical studies in two domains: 1) the mountain car task, a standard reinforcement learning benchmark on which neural network function approximators have previously performed poorly and 2) server job scheduling, a large probabilistic domain drawn from the ﬁeld of autonomic computing. The results demonstrate that evolutionary function approximation can signiﬁcantly improve the performance of TD methods and on-line evolutionary computation can signiﬁcantly improve evolutionary methods. This paper also presents additional tests that offer insight into what factors can make neural network function approximation difﬁcult in practice. Keywords: reinforcement learning, temporal difference methods, evolutionary computation, neuroevolution, on-</p><p>6 0.36992821 <a title="10-lsi-6" href="./jmlr-2006-Policy_Gradient_in_Continuous_Time.html">75 jmlr-2006-Policy Gradient in Continuous Time</a></p>
<p>7 0.35769865 <a title="10-lsi-7" href="./jmlr-2006-A_Simulation-Based_Algorithm_for_Ergodic_Control_of_Markov_Chains_Conditioned_on_Rare_Events.html">7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</a></p>
<p>8 0.2660419 <a title="10-lsi-8" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>9 0.201472 <a title="10-lsi-9" href="./jmlr-2006-Toward_Attribute_Efficient_Learning_of_Decision_Lists_and_Parities.html">92 jmlr-2006-Toward Attribute Efficient Learning of Decision Lists and Parities</a></p>
<p>10 0.20146893 <a title="10-lsi-10" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>11 0.19511804 <a title="10-lsi-11" href="./jmlr-2006-Adaptive_Prototype_Learning_Algorithms%3A_Theoretical_and_Experimental_Studies.html">13 jmlr-2006-Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies</a></p>
<p>12 0.19367906 <a title="10-lsi-12" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>13 0.18562564 <a title="10-lsi-13" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>14 0.1735093 <a title="10-lsi-14" href="./jmlr-2006-Superior_Guarantees_for_Sequential_Prediction_and_Lossless_Compression_via_Alphabet_Decomposition.html">90 jmlr-2006-Superior Guarantees for Sequential Prediction and Lossless Compression via Alphabet Decomposition</a></p>
<p>15 0.16450289 <a title="10-lsi-15" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>16 0.16266173 <a title="10-lsi-16" href="./jmlr-2006-Learning_Minimum_Volume_Sets.html">48 jmlr-2006-Learning Minimum Volume Sets</a></p>
<p>17 0.13801925 <a title="10-lsi-17" href="./jmlr-2006-Worst-Case_Analysis_of_Selective_Sampling_for_Linear_Classification.html">96 jmlr-2006-Worst-Case Analysis of Selective Sampling for Linear Classification</a></p>
<p>18 0.13716316 <a title="10-lsi-18" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.1367382 <a title="10-lsi-19" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.13383998 <a title="10-lsi-20" href="./jmlr-2006-Bounds_for_Linear_Multi-Task_Learning.html">16 jmlr-2006-Bounds for Linear Multi-Task Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.015), (21, 0.458), (36, 0.081), (45, 0.023), (50, 0.03), (63, 0.033), (67, 0.049), (76, 0.034), (78, 0.014), (79, 0.013), (81, 0.031), (84, 0.014), (90, 0.02), (91, 0.025), (96, 0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73520339 <a title="10-lda-1" href="./jmlr-2006-Action_Elimination_and_Stopping_Conditions_for_the_Multi-Armed_Bandit_and_Reinforcement_Learning_Problems.html">10 jmlr-2006-Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems</a></p>
<p>Author: Eyal Even-Dar, Shie Mannor, Yishay Mansour</p><p>Abstract: We incorporate statistical conﬁdence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it sufﬁces to pull the arms a total of O (n/ε2 ) log(1/δ) times to ﬁnd an ε-optimal arm with probability of at least 1 − δ. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the conﬁdence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over ε-greedy Q-learning.</p><p>2 0.26859114 <a title="10-lda-2" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<p>Author: Jelle R. Kok, Nikos Vlassis</p><p>Abstract: In this article we describe a set of scalable techniques for learning the behavior of a group of agents in a collaborative multiagent setting. As a basis we use the framework of coordination graphs of Guestrin, Koller, and Parr (2002a) which exploits the dependencies between agents to decompose the global payoff function into a sum of local terms. First, we deal with the single-state case and describe a payoff propagation algorithm that computes the individual actions that approximately maximize the global payoff function. The method can be viewed as the decision-making analogue of belief propagation in Bayesian networks. Second, we focus on learning the behavior of the agents in sequential decision-making tasks. We introduce different model-free reinforcementlearning techniques, unitedly called Sparse Cooperative Q-learning, which approximate the global action-value function based on the topology of a coordination graph, and perform updates using the contribution of the individual agents to the maximal global action value. The combined use of an edge-based decomposition of the action-value function and the payoff propagation algorithm for efﬁcient action selection, result in an approach that scales only linearly in the problem size. We provide experimental evidence that our method outperforms related multiagent reinforcement-learning methods based on temporal differences. Keywords: collaborative multiagent system, coordination graph, reinforcement learning, Qlearning, belief propagation</p><p>3 0.24202824 <a title="10-lda-3" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>4 0.23873062 <a title="10-lda-4" href="./jmlr-2006-Consistency_of_Multiclass_Empirical_Risk_Minimization_Methods_Based_on_Convex_Loss.html">24 jmlr-2006-Consistency of Multiclass Empirical Risk Minimization Methods Based on Convex Loss</a></p>
<p>Author: Di-Rong Chen, Tao Sun</p><p>Abstract: The consistency of classiﬁcation algorithm plays a central role in statistical learning theory. A consistent algorithm guarantees us that taking more samples essentially sufﬁces to roughly reconstruct the unknown distribution. We consider the consistency of ERM scheme over classes of combinations of very simple rules (base classiﬁers) in multiclass classiﬁcation. Our approach is, under some mild conditions, to establish a quantitative relationship between classiﬁcation errors and convex risks. In comparison with the related previous work, the feature of our result is that the conditions are mainly expressed in terms of the differences between some values of the convex function. Keywords: multiclass classiﬁcation, classiﬁer, consistency, empirical risk minimization, constrained comparison method, Tsybakov noise condition</p><p>5 0.23747073 <a title="10-lda-5" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>Author: Dana Angluin, Jiang Chen</p><p>Abstract: We consider the problem of learning a hypergraph using edge-detecting queries. In this model, the learner may query whether a set of vertices induces an edge of the hidden hypergraph or not. We show that an r-uniform hypergraph with m edges and n vertices is learnable with O(2 4r m · poly(r, log n)) queries with high probability. The queries can be made in O(min(2 r (log m + r)2 , (log m + r)3 )) rounds. We also give an algorithm that learns an almost uniform hypergraph of ∆ ∆ dimension r using O(2O((1+ 2 )r) · m1+ 2 · poly(log n)) queries with high probability, where ∆ is the difference between the maximum and the minimum edge sizes. This upper bound matches our ∆ lower bound of Ω(( m∆ )1+ 2 ) for this class of hypergraphs in terms of dependence on m. The 1+ 2 queries can also be made in O((1 + ∆) · min(2r (log m + r)2 , (log m + r)3 )) rounds. Keywords: query learning, hypergraph, multiple round algorithm, sampling, chemical reaction network</p><p>6 0.2356791 <a title="10-lda-6" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>7 0.23515525 <a title="10-lda-7" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>8 0.23508611 <a title="10-lda-8" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>9 0.23493862 <a title="10-lda-9" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>10 0.23470047 <a title="10-lda-10" href="./jmlr-2006-Policy_Gradient_in_Continuous_Time.html">75 jmlr-2006-Policy Gradient in Continuous Time</a></p>
<p>11 0.23455772 <a title="10-lda-11" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>12 0.23391025 <a title="10-lda-12" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>13 0.23335481 <a title="10-lda-13" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>14 0.23201959 <a title="10-lda-14" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>15 0.23143116 <a title="10-lda-15" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.23085764 <a title="10-lda-16" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>17 0.230552 <a title="10-lda-17" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.23023427 <a title="10-lda-18" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>19 0.22945534 <a title="10-lda-19" href="./jmlr-2006-On_Model_Selection_Consistency_of_Lasso.html">66 jmlr-2006-On Model Selection Consistency of Lasso</a></p>
<p>20 0.22903724 <a title="10-lda-20" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
