<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-79" href="#">jmlr2006-79</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-79-pdf" href="http://jmlr.org/papers/volume7/shivaswamy06a/shivaswamy06a.pdf">pdf</a></p><p>Author: Pannagadatta K. Shivaswamy, Chiranjib Bhattacharyya, Alexander J. Smola</p><p>Abstract: We propose a novel second order cone programming formulation for designing robust classiﬁers which can handle uncertainty in observations. Similar formulations are also derived for designing regression functions which are robust to uncertainties in the regression setting. The proposed formulations are independent of the underlying distribution, requiring only the existence of second order moments. These formulations are then specialized to the case of missing values in observations for both classiﬁcation and regression problems. Experiments show that the proposed formulations outperform imputation.</p><p>Reference: <a title="jmlr-2006-79-reference" href="../jmlr2006_reference/jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Bennett and Emilio Parrado-Hern´ ndez a  Abstract We propose a novel second order cone programming formulation for designing robust classiﬁers which can handle uncertainty in observations. [sent-12, score-0.493]
</p><p>2 Similar formulations are also derived for designing regression functions which are robust to uncertainties in the regression setting. [sent-13, score-0.536]
</p><p>3 These formulations are then specialized to the case of missing values in observations for both classiﬁcation and regression problems. [sent-15, score-0.443]
</p><p>4 Quite often, however, this is not the case — for instance in the case of missing values we may be able (using a secondary estimation procedure) to estimate the values of the missing variables, albeit with a certain degree of uncertainty. [sent-20, score-0.376]
</p><p>5 , 2004b), where a second order cone programming (SOCP) formulation was derived to design a robust linear classiﬁer when the uncertainty was described by multivariate normal distributions. [sent-37, score-0.494]
</p><p>6 (2004b) by proposing a SOCP formulation for designing robust binary classiﬁers for arbitrary distributions having ﬁnite mean and covariance. [sent-40, score-0.33]
</p><p>7 We also show that the formulation achieves robustness by requiring that for every uncertain datapoint an ellipsoid should lie in the correct halfspace. [sent-42, score-0.437]
</p><p>8 Using Chebyshev inequalities two SOCP fromulations are derived, namely Close to Mean formulation and Small Residual formulation, which give linear regression functions robust to the uncertainty in x. [sent-46, score-0.476]
</p><p>9 The proposed formulations are then applied to the problem of patterns having missing values both in the case of classiﬁcation and regression. [sent-49, score-0.47]
</p><p>10 Again using Chebyshev inequalities two different formulations are derived for regression in section 5 for handling uncertainty in x. [sent-61, score-0.359]
</p><p>11 In section 6 we specialize the formulations to the missing value problem both in the case of classiﬁcation and regression. [sent-63, score-0.335]
</p><p>12 Linear Classiﬁcation by Hyperplanes Assume that we have n observations (xi , yi ) drawn iid (independently and identically distributed) from a distribution over X × Y, where xi is the ith pattern and yi is the corresponding label. [sent-69, score-0.436]
</p><p>13 One can compute the parameters of the hyperplane (w, b) by solving a quadratic optimization problem (see Cortes and Vapnik (1995)) 1 w 2 w,b 2 subject to yi ( w, xi + b) ≥ 1 minimize  (1a) for all 1 ≤ i ≤ n,  (1b)  where w is the euclidean norm. [sent-74, score-0.5]
</p><p>14 This leads to the following soft margin formulation with L1 regularization (Bennett and Mangasarian, 1993; Cortes and Vapnik, 1995): minimize w,b,ξ  1 w 2  2  n  +C ∑ ξi  (2a)  i=1  subject to yi ( w, xi + b) ≥ 1 − ξi ξi ≥ 0  for all 1 ≤ i ≤ n  for all 1 ≤ i ≤ n. [sent-78, score-0.414]
</p><p>15 The quantity Cξi is the “penalty” for any data point xi that either lies within the margin on the correct side of the hyperplane (ξi ≤ 1) or on the wrong side of the hyperplane (ξi > 1). [sent-81, score-0.368]
</p><p>16 This yields n  minimize w,b,ξ  ∑ ξi  (3a)  i=1  subject to yi ( w, xi + b) ≥ 1 − ξi ξi ≥ 0  w ≤ W. [sent-83, score-0.342]
</p><p>17 Suppose that instead of the pattern (xi , yi ) we only have a distribution over xi , that is xi is a random variable. [sent-103, score-0.348]
</p><p>18 xi  (4)  In other words, we require that the random variable xi lies on the correct side of the hyperplane with probability greater than κi . [sent-105, score-0.315]
</p><p>19 inf  (8)  xi ∼(xi ,Σi ) xi ¯  This means that even for the worst distribution we still classify xi correctly with high probability 1 − κi . [sent-133, score-0.38]
</p><p>20 What this means in terms of our formulation is that, by making Gaussian assumption we only scale down the size of the uncertainty ellipsoid with respect to the Chebyshev bound. [sent-151, score-0.408]
</p><p>21 In case xi satisﬁes yi ( w, xi + b) ≥ 1 − ξi then clearly the inﬁmum in (13b) is zero. [sent-159, score-0.348]
</p><p>22 If not, d 2 is just the distance of the mean xi from the hyperplane yi ( w, xi + b) = 1−ξi , that is yi ( w, xi + b − 1 + ξi ) d2 = . [sent-160, score-0.752]
</p><p>23 We have Pr  zi − zi yi b + ξi − 1 − zi ≥ σzi σzi  ≤ κi ,  (15)  where zi := −yi w, xi is a normal random variable with mean zi and variance σ2i := w⊤ Σi w. [sent-163, score-0.614]
</p><p>24 The centroid is classiﬁed correctly and the hyperplane does not cut the ellipsoid: The error is zero as all the points within the ellipsoid are classiﬁed correctly. [sent-183, score-0.435]
</p><p>25 The centroid is misclassiﬁed and the hyperplane does not cut the ellipsoid: Here the error is 1 as all the points within the ellipsoid are misclassiﬁed. [sent-185, score-0.435]
</p><p>26 Here the worst case error is one as we can always ﬁnd points within the uncertainty ellipsoid that get misclassiﬁed. [sent-188, score-0.474]
</p><p>27 We ﬁnd out the volume of the ellipsoid on the wrong side of the hyperplane and use the ratio of this volume to the entire volume of the ellipsoid as the expected error measure. [sent-204, score-0.708]
</p><p>28 When the hyperplane doesn’t cut the ellipsoid, expected error is either zero or one depending on whether the ellipsoid lies entirely on the correct side or entirely on the wrong side of the hyperplane. [sent-205, score-0.467]
</p><p>29 In ﬁgure 1 we essentially take the fraction of the area of the shaded portion of the ellipsoid as the expected error measure. [sent-207, score-0.318]
</p><p>30 In all our experiments, this was done by generating large number of uniformly distributed points in the ellipsoid and then taking the fraction of the number of points on the correct side of the hyperplane to the total number of points generated. [sent-208, score-0.364]
</p><p>31 , 2003): a n  minimize w,ξ  ∑ ξi  (19a)  i=1  subject to wyi , xi − max wy , xi ≥ 1 − ξi and ξi ≥ 0 y=yi  |Y|  ∑  i=1  wyi  2  ≤ W 2. [sent-215, score-0.4]
</p><p>32 2 Set Constraints The formulations presented so far can be broadly understood in the context of robust convex optimization (see Ben-Tal and Nemirovski (1998, 2001)). [sent-226, score-0.412]
</p><p>33 These inequalities are used to derive two SOCP formulations for designing robust estimators useful for regression with missing variables. [sent-303, score-0.681]
</p><p>34 In penalized linear regression settings one assumes that there is a function f (x) = w, x + b, (25) which is used to minimize a regularized risk n  minimize ∑ c(ei ) subject to w ≤ W and ei = f (xi ) − yi . [sent-310, score-0.412]
</p><p>35 We do so in order to obtain second order cone programs for the robust formulation more easily without the need to dualize immediately. [sent-319, score-0.361]
</p><p>36 3 we derive two formulations which render estimates robust to the stochastic variations in xi . [sent-325, score-0.435]
</p><p>37 (1997) we obtain i n  n  minimize ∑ (ξi + ξ∗ ) + D ∑ θi i w,b,ξ,ξ∗ ,θ i=1  (35a)  i=1  subject to w ≤ W and θi , ξi , ξ∗ ≥ 0 i  xi , w + b − yi ≤ ε + ξi and yi − xi , w − b ≤ ¯ ¯ 1 √ Σi2 w ≤ θi ηi  ε + ξ∗ i  for all 1 ≤ i ≤ n  (35b)  for all 1 ≤ i ≤ n  (35c)  for all 1 ≤ i ≤ n. [sent-355, score-0.594]
</p><p>38 Hence the optimization problem becomes n  minimize ∑ ξi w,b,ξ  (36a)  i=1  subject to w ≤ W and ξi ≥ 0  √ w⊤ Σi w + ( w, xi + b − yi )2 ≤ (ξi + ε) ηi ¯  for all 1 ≤ i ≤ n  (36b)  for all 1 ≤ i ≤ n. [sent-358, score-0.377]
</p><p>39 4 Geometrical Interpretation and Error Measures The CTM formulation can be motivated by a similar geometrical interpretation to the one in the classiﬁcation case, using an ellipsoid with center x, shape and size determined by Σ and γ. [sent-362, score-0.348]
</p><p>40 Robust Formulation For Missing Values In this section we discuss how to apply the robust formulations to the problem of estimation with missing values. [sent-376, score-0.527]
</p><p>41 While we use a linear regression model to ﬁll in the missing values, the linear assumption is not really necessary: as long as we have information on the ﬁrst and second moments of the distribution we can use the robust programming formulation for estimation. [sent-377, score-0.57]
</p><p>42 , 1977) to take care of missing variables wherever appropriate: 1297  S HIVASWAMY, B HATTACHARYYA AND S MOLA  Let (x, y) have parts xm and xa , corresponding to missing and available components respectively. [sent-380, score-0.414]
</p><p>43 aa ma  (42) (43)  In standard EM fashion one begins with initial estimates for mean and covariance, uses the latter to impute the missing values for the entire class of data and iterates by re-estimating mean and covariance until convergence. [sent-383, score-0.401]
</p><p>44 Optimization Problem Without loss of generality, suppose that the patterns 1 to c are complete and that patterns c + 1 to n have missing components. [sent-384, score-0.458]
</p><p>45 The quantities γi ’s are deﬁned only for the patterns with missing components. [sent-387, score-0.323]
</p><p>46 Fill in the missing values xm in x using the parameters (mean and the covariance) of each class, call the resulting patterns x+ and x− corresponding to classes +1 and −1 respectively. [sent-392, score-0.361]
</p><p>47 After using the same linear model an imputation strategy as above we now propose to use the CTM and SR formulations to exploit the covariance information to design robust prediction functions for the missing values. [sent-402, score-0.686]
</p><p>48 As before, quantities θi ’s are deﬁned only for patterns with missing components. [sent-406, score-0.323]
</p><p>49 A similar SR formulation could be easily obtained for the case of missing values:  c  minimize ∑ (ξi + ξ∗ ) + i w,b,ξ,ξ∗  i=1  n  ∑  ξi  i=c+1  subject to w, xi + b − yi ≤ ε + ξi , yi − w, xi − b ≤ ε + ξ∗ i √ w⊤ Σi w + ( w, xi + b − yi )2 ≤ (ε + ξi ) ηi ξ∗ ≥ 0 for all 1 ≤ i ≤ c and ξi ≥ 0 i w ≤ W. [sent-407, score-1.106]
</p><p>50 Kernelized Robust Formulations In this section we propose robust formulations for designing nonlinear classiﬁers by using kernel function. [sent-409, score-0.376]
</p><p>51 n  maximize ∑ λi −W δ,  (47a)  ∑ λi yi = 0,  (47b)  λ,δ,β,u  subject to  i=1 n  i=1 c  ∑ λi yi xi +  i=1  n  ∑  i=c+1  1  T  λi yi (xi + γi Σi2 ui ) ≤ δ,  λi + βi = 1  (47c) for all 1 ≤ i ≤ n  for all 1 ≤ i ≤ n. [sent-414, score-0.673]
</p><p>52 Substituting for un+1 in (48a) gives the following expression for w, w=  W δ  c  n  i=1  i=c+1  ∑ λi yi xi + ∑  1  λi yi xi + γi Σi2 ui  . [sent-427, score-0.557]
</p><p>53 The vector w has been expressed as a combination of complete patterns and vectors from the uncertainty ellipsoid of the incomplete patterns. [sent-429, score-0.496]
</p><p>54 The ellipsoid around the pattern denotes the uncertainty ellipsoid. [sent-431, score-0.336]
</p><p>55 The vertical solid line represents the optimal hyperplane obtained by nominal SVM while the thick dotted line represents the optimal hyperplane obtained by the robust classiﬁer  Kernelized Formulation It is not simple to solve the dual (47) as a kernelized formulation. [sent-433, score-1.007]
</p><p>56 The difﬁculty arises from the fact that the constraint containing the dot products of the patterns (47c) involves terms such as  1 2  xi + γi Σi ui  T  1  x j + γ jΣ 2 u j j  for some i and j. [sent-434, score-0.333]
</p><p>57 When the shape of the uncertainty ellipsoid for a pattern with missing values is determined by the covariance matrix of the imputed values, any point in the ellipsoid is in the span of the patterns used in estimating the covariance matrix. [sent-437, score-1.263]
</p><p>58 The eigenvectors of a covariance matrix are in the span of the patterns from which the covariance matrix is estimated. [sent-439, score-0.348]
</p><p>59 Since eigenvectors are in the span of the patterns and they span the entire ellipsoid, any vector in the ellipsoid is in the span of the patterns from which the covariance matrix is estimated. [sent-440, score-0.7]
</p><p>60 The above fact and the equation to construct w from the dual variables (49) imply w is in the span of the imputed data ( all the patterns: complete and the incomplete patterns with missing values imputed). [sent-441, score-0.531]
</p><p>61 Following the same steps as in Section 3, it can be shown that the above probabilistic constraint is equivalent to yi  ˜ α, K (xi ) + b ≥ 1 − ξi +  κi 1 − κi  αT Σk α, i  ˜ ˜ ˜ where Σk and K (xi ) are the covariance and the mean of K (xi ) (in K-space). [sent-460, score-0.324]
</p><p>62 In view of this, the i following is the non-linear version of the formulation: n  minimize ∑ ξi α,b,ξ  (51a)  i=1  subject to yi yi  ˜ α, K(xi ) + b ≥ 1 − ξi  α ≤ W ξi ≥ 0  (51b)  for all c + 1 ≤ j ≤ n  (51c)  for all 1 ≤ i ≤ n. [sent-461, score-0.402]
</p><p>63 j In the original lower dimensional space we had a closed form formula to estimate the covariance for patterns with missing values. [sent-465, score-0.413]
</p><p>64 Similarly, the kernelized version of formulation SR is given by, n  minimize ∑ ξi α,b,ξ  i=1  subject to  √ ˜ α⊤ Σk α + ( α, K(xi ) + b − yi )2 ≤ (ε + ξi ) ηi i  α ≤ W and ξi ≥ 0  for all 1 ≤ i ≤ n for all 1 ≤ i ≤ n. [sent-496, score-0.505]
</p><p>65 If the patterns 1 through c i are complete and the patterns c + 1 through n have missing values, then assuming ηi = 1 and Σk = 0 i for i from 1 through c, would make the above formulations directly applicable to the case. [sent-498, score-0.605]
</p><p>66 Experiments In this section we empirically test the derived formulations for both classiﬁcation and regression problems which have missing values in the observations. [sent-500, score-0.415]
</p><p>67 The missing values are ﬁlled in by imputation and subsequently a 1303  S HIVASWAMY, B HATTACHARYYA AND S MOLA  SVM classiﬁer was trained on the complete data to obtain the nominal classiﬁer. [sent-505, score-0.639]
</p><p>68 We compared the proposed formulations with the nominal classiﬁers by performing numerical experiments on real life data bench mark datasets. [sent-506, score-0.529]
</p><p>69 For evaluating the results of robust classiﬁer we used the worst case error and the expected error along with the actual error. [sent-509, score-0.447]
</p><p>70 In case it has missing values, we ﬁrst impute the missing values and then classify the pattern. [sent-511, score-0.411]
</p><p>71 We trained a SVM on this imputed data, to obtain the nominal classiﬁer. [sent-517, score-0.532]
</p><p>72 Figure 3 shows some of the digits that were misclassiﬁed by the nominal classiﬁer but were correctly classiﬁed by the robust classiﬁer. [sent-521, score-0.607]
</p><p>73 With only partial pixels available, our formulation did better than the nominal classiﬁer. [sent-523, score-0.454]
</p><p>74 In all the three measures, the robust classiﬁer outperformed the nominal classiﬁer. [sent-525, score-0.574]
</p><p>75 Experiments were done with low noise (50% patterns with missing values) and high noise (90% patterns with missing values). [sent-559, score-0.738]
</p><p>76 The data sets were divided in the ratio 9:1, the larger set was used for training the nominal and robust classiﬁers while the smaller set was used as test data set. [sent-560, score-0.574]
</p><p>77 50% of the feature values (chosen at random) were deleted from 50% of the training patterns (in the low noise case) and 90% of the training patterns (in the high noise case). [sent-561, score-0.39]
</p><p>78 It can be seen that the robust classiﬁer, with suitable amount of robustness comes very close to the error rates on the clean data set. [sent-571, score-0.389]
</p><p>79 Amongst the three error measures the worst case error, the last column of Figure 7, brings out the advantage of the robust classiﬁer over the nominal classiﬁer. [sent-572, score-0.712]
</p><p>80 Clearly with increasing γ the robust formulation gives dividends over the nominal classiﬁer. [sent-573, score-0.646]
</p><p>81 Spherical uncertainty was ˜ assumed in K-space for samples with missing values in case of kernelized robust formulations. [sent-590, score-0.662]
</p><p>82 Figure 8 shows actual error rates with linear nominal, linear robust, kernelized nominal and kernelized robust. [sent-591, score-0.867]
</p><p>83 It can also be observed that the robust kernelized classiﬁer has the least error rate. [sent-593, score-0.425]
</p><p>84 2 Regression Given a regression problem with training data having missing values in the observations we obtained the nominal regression function by training a Support Vector Regression(SVR) formulation over the 1305  S HIVASWAMY, B HATTACHARYYA AND S MOLA  Heart − Low Noise  Heart − Low Noise  0. [sent-595, score-0.83]
</p><p>85 The obtained regression function will be called the nominal SVR. [sent-659, score-0.462]
</p><p>86 In this section we compare our formulations with nominal SVR on a toy dataset and one real world dataset in the linear setting. [sent-660, score-0.577]
</p><p>87 We also compared the kernelized formulations with the linear formulations. [sent-661, score-0.334]
</p><p>88 The ﬁrst row of Figure 9 shows robustness error (38), worst case error (40) for CTM and expected residual (39) and worst case error (40) for SR. [sent-772, score-0.468]
</p><p>89 The performance of our formulation over nominal regression is evident. [sent-774, score-0.534]
</p><p>90 Essentially, we calculate ∑n e( f (xi , yi )) for all the test i=1 samples where the missing values are ﬁlled in using the training data set parameters using a linear 6. [sent-784, score-0.344]
</p><p>91 4  Figure 8: The left ﬁgure shows how the data set looks in two dimensions, the right ﬁgure gives the actual error rate for linear and kernelized formulations for the robust and nominal cases. [sent-871, score-0.994]
</p><p>92 The ﬁgures show that the kernelized version of the robust formulation does a better job than the linear version when the underlying function is non-linear. [sent-874, score-0.451]
</p><p>93 5  1−η  Figure 9: Top row — toy data set, Bottom row — Boston Housing estimation problem; From left to right: robustness (CTM), worst case error (CTM), expected residual (SR), and worst case error (SR). [sent-995, score-0.422]
</p><p>94 Conclusions In this paper we have proposed SOCP formulations for designing robust linear prediction functions which are capable of tackling uncertainty in the patterns both in classiﬁcation and regression setting. [sent-998, score-0.686]
</p><p>95 When applied to the missing variables problem the formulations outperform the imputation based classiﬁers and regression functions. [sent-1000, score-0.484]
</p><p>96 Note that the u’s are deﬁned only for patterns with 1310  SOCP A PPROACHES FOR M ISSING AND U NCERTAIN DATA  missing values and un+1 is deﬁned for the constraint w ≤ W . [sent-1058, score-0.372]
</p><p>97 Taking partial derivatives of L with respect to w, b, and ξ yields n  c  ∂w L (w, ξ, b, λ, β, δ, u) = − ∑ λi yi xi − i=1  ∑  j=c+1  1  λ j y j x j − γi Σ j 2 T u j + δun+1  (57a)  ∂ξi L (w, ξ, b, λ, β, δ, u) = 1 − λi − βi  (57b)  ∂b L (w, ξ, b, λ, β, δ, u) = ∑ λi yi . [sent-1066, score-0.408]
</p><p>98 Substituting −u j in (57a) by y j u j and then equating (57a) , (57b) and (57c) to zero gives c  n  i=1  j=c+1  ∑ λi yi xi + ∑  1  λy j x j + γi Σ j 2 T u j = δun+1  (58a)  1 − λi − βi = 0  (58b)  ∑ λi yi = 0. [sent-1068, score-0.408]
</p><p>99 A second order cone programming formulation for classifying missing data. [sent-1107, score-0.357]
</p><p>100 Analysis of incomplete climate data: Estimation of mean values and covariance matrices and imputation of missing values. [sent-1264, score-0.401]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nominal', 0.382), ('ctm', 0.3), ('ellipsoid', 0.241), ('socp', 0.217), ('robust', 0.192), ('missing', 0.188), ('kernelized', 0.187), ('hattacharyya', 0.185), ('hivaswamy', 0.185), ('issing', 0.173), ('ncertain', 0.173), ('pproaches', 0.173), ('yi', 0.156), ('imputed', 0.15), ('mola', 0.149), ('formulations', 0.147), ('patterns', 0.135), ('sr', 0.129), ('hyperplane', 0.123), ('cone', 0.097), ('chebyshev', 0.096), ('xi', 0.096), ('uncertainty', 0.095), ('worst', 0.092), ('covariance', 0.09), ('regression', 0.08), ('clean', 0.072), ('formulation', 0.072), ('uncertain', 0.07), ('imputation', 0.069), ('ion', 0.069), ('sonar', 0.069), ('classi', 0.065), ('pr', 0.064), ('residual', 0.061), ('zi', 0.059), ('subject', 0.056), ('ocr', 0.056), ('un', 0.056), ('robustness', 0.054), ('ui', 0.053), ('ei', 0.052), ('si', 0.051), ('bhattacharyya', 0.049), ('constraint', 0.049), ('wyi', 0.046), ('noise', 0.046), ('lled', 0.046), ('error', 0.046), ('deleting', 0.044), ('constraints', 0.042), ('actual', 0.04), ('ghaoui', 0.039), ('marshall', 0.039), ('normal', 0.038), ('xm', 0.038), ('moments', 0.038), ('convex', 0.038), ('inequalities', 0.037), ('designing', 0.037), ('heart', 0.037), ('sgn', 0.035), ('geometrical', 0.035), ('graepel', 0.035), ('optimization', 0.035), ('cala', 0.035), ('impute', 0.035), ('neighbours', 0.035), ('ore', 0.035), ('pannagadatta', 0.035), ('minimize', 0.034), ('australia', 0.033), ('span', 0.033), ('digits', 0.033), ('expected', 0.031), ('aa', 0.03), ('polyhedral', 0.029), ('lobo', 0.029), ('el', 0.029), ('svm', 0.029), ('mean', 0.029), ('deleted', 0.028), ('observations', 0.028), ('wt', 0.027), ('er', 0.027), ('sampling', 0.027), ('wrong', 0.026), ('olkin', 0.026), ('wy', 0.026), ('ict', 0.026), ('incomplete', 0.025), ('rates', 0.025), ('centroid', 0.025), ('dataset', 0.024), ('boyd', 0.024), ('chiranjib', 0.023), ('eexp', 0.023), ('ellipsoids', 0.023), ('erobust', 0.023), ('farias', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="79-tfidf-1" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pannagadatta K. Shivaswamy, Chiranjib Bhattacharyya, Alexander J. Smola</p><p>Abstract: We propose a novel second order cone programming formulation for designing robust classiﬁers which can handle uncertainty in observations. Similar formulations are also derived for designing regression functions which are robust to uncertainties in the regression setting. The proposed formulations are independent of the underlying distribution, requiring only the existence of second order moments. These formulations are then specialized to the case of missing values in observations for both classiﬁcation and regression problems. Experiments show that the proposed formulations outperform imputation.</p><p>2 0.11477505 <a title="79-tfidf-2" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>3 0.065745883 <a title="79-tfidf-3" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Matthias Heiler, Christoph Schnörr</p><p>Abstract: We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. We show that for ordinary NMF, our approach performs as well as existing stateof-the-art algorithms, while for sparsity-constrained NMF, as recently proposed by P. O. Hoyer in JMLR 5 (2004), it outperforms previous methods. In addition, we show how to extend NMF learning within the same optimization framework in order to make use of class membership information in supervised learning problems. Keywords: non-negative matrix factorization, second-order cone programming, sequential convex optimization, reverse-convex programming, sparsity</p><p>4 0.064936146 <a title="79-tfidf-4" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>Author: Robert Castelo, Alberto Roverato</p><p>Abstract: Learning of large-scale networks of interactions from microarray data is an important and challenging problem in bioinformatics. A widely used approach is to assume that the available data constitute a random sample from a multivariate distribution belonging to a Gaussian graphical model. As a consequence, the prime objects of inference are full-order partial correlations which are partial correlations between two variables given the remaining ones. In the context of microarray data the number of variables exceed the sample size and this precludes the application of traditional structure learning procedures because a sampling version of full-order partial correlations does not exist. In this paper we consider limited-order partial correlations, these are partial correlations computed on marginal distributions of manageable size, and provide a set of rules that allow one to assess the usefulness of these quantities to derive the independence structure of the underlying Gaussian graphical model. Furthermore, we introduce a novel structure learning procedure based on a quantity, obtained from limited-order partial correlations, that we call the non-rejection rate. The applicability and usefulness of the procedure are demonstrated by both simulated and real data. Keywords: Gaussian distribution, gene network, graphical model, microarray data, non-rejection rate, partial correlation, small-sample inference</p><p>5 0.059376981 <a title="79-tfidf-5" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>Author: Sayan Mukherjee, Ding-Xuan Zhou</p><p>Abstract: We introduce an algorithm that learns gradients from samples in the supervised learning framework. An error analysis is given for the convergence of the gradient estimated by the algorithm to the true gradient. The utility of the algorithm for the problem of variable selection as well as determining variable covariance is illustrated on simulated data as well as two gene expression data sets. For square loss we provide a very efﬁcient implementation with respect to both memory and time. Keywords: Tikhnonov regularization, variable selection, reproducing kernel Hilbert space, generalization bounds</p><p>6 0.053300168 <a title="79-tfidf-6" href="./jmlr-2006-Nonparametric_Quantile_Estimation.html">65 jmlr-2006-Nonparametric Quantile Estimation</a></p>
<p>7 0.052721087 <a title="79-tfidf-7" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>8 0.051846508 <a title="79-tfidf-8" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.051255401 <a title="79-tfidf-9" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.049634553 <a title="79-tfidf-10" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>11 0.047866311 <a title="79-tfidf-11" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.046837643 <a title="79-tfidf-12" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>13 0.046037681 <a title="79-tfidf-13" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>14 0.04539489 <a title="79-tfidf-14" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.045001723 <a title="79-tfidf-15" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>16 0.04374183 <a title="79-tfidf-16" href="./jmlr-2006-Segmental_Hidden_Markov_Models_with_Random_Effects_for_Waveform_Modeling.html">80 jmlr-2006-Segmental Hidden Markov Models with Random Effects for Waveform Modeling</a></p>
<p>17 0.041683692 <a title="79-tfidf-17" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.041443348 <a title="79-tfidf-18" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>19 0.040247049 <a title="79-tfidf-19" href="./jmlr-2006-Stability_Properties_of_Empirical_Risk_Minimization_over_Donsker_Classes.html">84 jmlr-2006-Stability Properties of Empirical Risk Minimization over Donsker Classes</a></p>
<p>20 0.039834451 <a title="79-tfidf-20" href="./jmlr-2006-Some_Discriminant-Based_PAC_Algorithms.html">81 jmlr-2006-Some Discriminant-Based PAC Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.234), (1, -0.103), (2, 0.043), (3, 0.008), (4, 0.022), (5, -0.01), (6, 0.007), (7, 0.06), (8, 0.113), (9, 0.048), (10, -0.038), (11, 0.041), (12, -0.027), (13, -0.007), (14, -0.025), (15, -0.178), (16, -0.051), (17, 0.021), (18, -0.033), (19, -0.074), (20, 0.157), (21, -0.115), (22, -0.128), (23, -0.131), (24, 0.035), (25, -0.063), (26, -0.116), (27, 0.101), (28, 0.072), (29, -0.122), (30, -0.093), (31, 0.039), (32, -0.075), (33, 0.064), (34, 0.118), (35, -0.017), (36, 0.021), (37, -0.128), (38, -0.13), (39, 0.046), (40, -0.073), (41, -0.101), (42, 0.183), (43, -0.181), (44, 0.269), (45, -0.232), (46, 0.192), (47, -0.113), (48, 0.166), (49, 0.163)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94014454 <a title="79-lsi-1" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pannagadatta K. Shivaswamy, Chiranjib Bhattacharyya, Alexander J. Smola</p><p>Abstract: We propose a novel second order cone programming formulation for designing robust classiﬁers which can handle uncertainty in observations. Similar formulations are also derived for designing regression functions which are robust to uncertainties in the regression setting. The proposed formulations are independent of the underlying distribution, requiring only the existence of second order moments. These formulations are then specialized to the case of missing values in observations for both classiﬁcation and regression problems. Experiments show that the proposed formulations outperform imputation.</p><p>2 0.39689669 <a title="79-lsi-2" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>Author: S. V. N. Vishwanathan, Nicol N. Schraudolph, Alex J. Smola</p><p>Abstract: This paper presents an online support vector machine (SVM) that uses the stochastic meta-descent (SMD) algorithm to adapt its step size automatically. We formulate the online learning problem as a stochastic gradient descent in reproducing kernel Hilbert space (RKHS) and translate SMD to the nonparametric setting, where its gradient trace parameter is no longer a coefﬁcient vector but an element of the RKHS. We derive efﬁcient updates that allow us to perform the step size adaptation in linear time. We apply the online SVM framework to a variety of loss functions, and in particular show how to handle structured output spaces and achieve efﬁcient online multiclass classiﬁcation. Experiments show that our algorithm outperforms more primitive methods for setting the gradient step size. Keywords: online SVM, stochastic meta-descent, structured output spaces</p><p>3 0.38877439 <a title="79-lsi-3" href="./jmlr-2006-Segmental_Hidden_Markov_Models_with_Random_Effects_for_Waveform_Modeling.html">80 jmlr-2006-Segmental Hidden Markov Models with Random Effects for Waveform Modeling</a></p>
<p>Author: Seyoung Kim, Padhraic Smyth</p><p>Abstract: This paper proposes a general probabilistic framework for shape-based modeling and classiﬁcation of waveform data. A segmental hidden Markov model (HMM) is used to characterize waveform shape and shape variation is captured by adding random effects to the segmental model. The resulting probabilistic framework provides a basis for learning of waveform models from data as well as parsing and recognition of new waveforms. Expectation-maximization (EM) algorithms are derived and investigated for ﬁtting such models to data. In particular, the “expectation conditional maximization either” (ECME) algorithm is shown to provide signiﬁcantly faster convergence than a standard EM procedure. Experimental results on two real-world data sets demonstrate that the proposed approach leads to improved accuracy in classiﬁcation and segmentation when compared to alternatives such as Euclidean distance matching, dynamic time warping, and segmental HMMs without random effects. Keywords: waveform recognition, random effects, segmental hidden Markov models, EM algorithm, ECME</p><p>4 0.37494138 <a title="79-lsi-4" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer, Bernhard Schölkopf</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun. Keywords: multiple kernel learning, string kernels, large scale optimization, support vector machines, support vector regression, column generation, semi-inﬁnite linear programming</p><p>5 0.37156397 <a title="79-lsi-5" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>6 0.3228085 <a title="79-lsi-6" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>7 0.31590068 <a title="79-lsi-7" href="./jmlr-2006-Nonparametric_Quantile_Estimation.html">65 jmlr-2006-Nonparametric Quantile Estimation</a></p>
<p>8 0.29382113 <a title="79-lsi-8" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.28601241 <a title="79-lsi-9" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>10 0.27826285 <a title="79-lsi-10" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>11 0.26823199 <a title="79-lsi-11" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>12 0.25447196 <a title="79-lsi-12" href="./jmlr-2006-Adaptive_Prototype_Learning_Algorithms%3A_Theoretical_and_Experimental_Studies.html">13 jmlr-2006-Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies</a></p>
<p>13 0.25399509 <a title="79-lsi-13" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>14 0.24923667 <a title="79-lsi-14" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>15 0.22770691 <a title="79-lsi-15" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.21584697 <a title="79-lsi-16" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.21559699 <a title="79-lsi-17" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>18 0.20805301 <a title="79-lsi-18" href="./jmlr-2006-In_Search_of_Non-Gaussian_Components_of_a_High-Dimensional_Distribution.html">36 jmlr-2006-In Search of Non-Gaussian Components of a High-Dimensional Distribution</a></p>
<p>19 0.20799986 <a title="79-lsi-19" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>20 0.20551205 <a title="79-lsi-20" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.033), (35, 0.014), (36, 0.064), (45, 0.025), (50, 0.05), (63, 0.03), (76, 0.013), (78, 0.013), (81, 0.033), (84, 0.496), (90, 0.028), (91, 0.029), (96, 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91631377 <a title="79-lda-1" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pannagadatta K. Shivaswamy, Chiranjib Bhattacharyya, Alexander J. Smola</p><p>Abstract: We propose a novel second order cone programming formulation for designing robust classiﬁers which can handle uncertainty in observations. Similar formulations are also derived for designing regression functions which are robust to uncertainties in the regression setting. The proposed formulations are independent of the underlying distribution, requiring only the existence of second order moments. These formulations are then specialized to the case of missing values in observations for both classiﬁcation and regression problems. Experiments show that the proposed formulations outperform imputation.</p><p>2 0.81515962 <a title="79-lda-2" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>Author: Ross A. Lippert, Ryan M. Rifkin</p><p>Abstract: We consider the problem of Tikhonov regularization with a general convex loss function: this formalism includes support vector machines and regularized least squares. For a family of kernels that includes the Gaussian, parameterized by a “bandwidth” parameter σ, we characterize the limiting ˜ solution as σ → ∞. In particular, we show that if we set the regularization parameter λ = λσ−2p , the regularization term of the Tikhonov problem tends to an indicator function on polynomials of degree ⌊p⌋ (with residual regularization in the case where p ∈ Z). The proof rests on two key ideas: epi-convergence, a notion of functional convergence under which limits of minimizers converge to minimizers of limits, and a value-based formulation of learning, where we work with regularization on the function output values (y) as opposed to the function expansion coefﬁcients in the RKHS. Our result generalizes and uniﬁes previous results in this area. Keywords: Tikhonov regularization, Gaussian kernel, theory, kernel machines</p><p>3 0.44970328 <a title="79-lda-3" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>4 0.44454211 <a title="79-lda-4" href="./jmlr-2006-Quantile_Regression_Forests.html">77 jmlr-2006-Quantile Regression Forests</a></p>
<p>Author: Nicolai Meinshausen</p><p>Abstract: Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classiﬁcation. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power. Keywords: quantile regression, random forests, adaptive neighborhood regression</p><p>5 0.42627057 <a title="79-lda-5" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>Author: Mikio L. Braun</p><p>Abstract: The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to inﬁnity. We derive probabilistic ﬁnite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reﬂecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a signiﬁcant improvement over existing non-scaling bounds. Keywords: kernel matrix, eigenvalues, relative perturbation bounds</p><p>6 0.4167679 <a title="79-lda-6" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>7 0.41644919 <a title="79-lda-7" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.41210717 <a title="79-lda-8" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.4060598 <a title="79-lda-9" href="./jmlr-2006-Nonparametric_Quantile_Estimation.html">65 jmlr-2006-Nonparametric Quantile Estimation</a></p>
<p>10 0.4055959 <a title="79-lda-10" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>11 0.40556037 <a title="79-lda-11" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>12 0.39829606 <a title="79-lda-12" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>13 0.39558464 <a title="79-lda-13" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>14 0.39392924 <a title="79-lda-14" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>15 0.39356285 <a title="79-lda-15" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>16 0.39341176 <a title="79-lda-16" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>17 0.38624996 <a title="79-lda-17" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.37695092 <a title="79-lda-18" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>19 0.37004259 <a title="79-lda-19" href="./jmlr-2006-Pattern_Recognition_for__Conditionally_Independent_Data.html">73 jmlr-2006-Pattern Recognition for  Conditionally Independent Data</a></p>
<p>20 0.36887074 <a title="79-lda-20" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
