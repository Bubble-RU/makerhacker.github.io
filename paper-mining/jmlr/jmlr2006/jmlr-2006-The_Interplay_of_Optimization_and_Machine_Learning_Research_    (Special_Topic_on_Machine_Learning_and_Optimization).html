<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-91" href="#">jmlr2006-91</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-91-pdf" href="http://jmlr.org/papers/volume7/MLOPT-intro06a/MLOPT-intro06a.pdf">pdf</a></p><p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>Reference: <a title="jmlr-2006-91-reference" href="../jmlr2006_reference/jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Bennett and Emilio Parrado-Hern´ ndez a  Abstract The ﬁelds of machine learning and mathematical programming are increasingly intertwined. [sent-6, score-0.28]
</p><p>2 Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. [sent-9, score-0.41]
</p><p>3 We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. [sent-11, score-0.264]
</p><p>4 Mathematical programming puts a premium on accuracy, speed, and robustness. [sent-12, score-0.231]
</p><p>5 Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. [sent-15, score-0.566]
</p><p>6 The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. [sent-17, score-0.49]
</p><p>7 Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. [sent-18, score-0.385]
</p><p>8 Keywords: machine learning, mathematical programming, convex optimization  1. [sent-19, score-0.331]
</p><p>9 Introduction The special topic on “Large Scale Optimization and Machine Learning” focuses on the core optimization problems underlying machine learning algorithms. [sent-20, score-0.329]
</p><p>10 We seek to examine the interaction of state-of-the-art machine learning and mathematical programming, soliciting papers that either enhanced the scalability and efﬁciency of existing machine learning models or that promoted new uses of mathematical programming in machine learning. [sent-21, score-0.583]
</p><p>11 Then the model is typically trained by solving a core optimization problem that optimizes the variables or parameters of the model with respect to the selected loss function and possibly some regularization function. [sent-29, score-0.213]
</p><p>12 In the process of model selection and validation, the core optimization problem may be solved many times. [sent-30, score-0.302]
</p><p>13 The research area of mathematical programming intersects with machine learning through these core optimization problems. [sent-31, score-0.493]
</p><p>14 On one hand, mathematical programming theory supplies a deﬁnition of what constitutes an optimal solution – the optimality conditions. [sent-32, score-0.28]
</p><p>15 On the other hand, mathematical programming algorithms equip machine learning researchers with tools for training large families of models. [sent-33, score-0.348]
</p><p>16 In general, a mathematical program is a problem of the form mins f (s) subject to g(s) ≤ 0 . [sent-34, score-0.325]
</p><p>17 Convex programs minimize convex optimization functions subject to convex constraints ensuring that every local minimum is always a global minimum. [sent-38, score-0.558]
</p><p>18 Integer or discrete optimization considers nonconvex problems with integer constraints. [sent-48, score-0.244]
</p><p>19 A taxonomy of mathematical programs exists based on the types of objectives and constraints. [sent-49, score-0.223]
</p><p>20 A more complete description of these problems can be obtained from the mathematical programming glossary (www. [sent-52, score-0.28]
</p><p>21 Very brief descriptions of the mathematical programs used in this special issue can be found in the Appendix. [sent-59, score-0.265]
</p><p>22 An introduction to convex optimization including semi-deﬁnite programming can be found in (Boyd and Vandenberghe, 2004). [sent-62, score-0.393]
</p><p>23 Information about o integer programming can be found in (Nemhauser and Wolsey, 1999). [sent-64, score-0.21]
</p><p>24 We observe that the relationship between available mathematical programming models and machine learning models has been increasingly coupled. [sent-65, score-0.404]
</p><p>25 The adaptation of mathematical programming models and algorithms has helped machine learning research advance. [sent-66, score-0.342]
</p><p>26 , 1986) to exploring the use of various unconstrained nonlinear programming techniques such as discussed in (Bishop, 1996). [sent-68, score-0.228]
</p><p>27 With the advent of kernel methods (Cortes and Vapnik, 1995), mathematical programming terms such as quadratic program, Lagrange multipliers and duality are now very familiar to well-versed machine learning students. [sent-70, score-0.344]
</p><p>28 Machine learning researchers are designing novel models and methods to exploit more branches of the mathematical programming tree with a special emphasis on constrained convex optimization. [sent-71, score-0.738]
</p><p>29 The special topic reﬂects the diversity of mathematical programming models being employed in machine learning. [sent-72, score-0.458]
</p><p>30 We see how recent advances in mathematical programming have allowed rich new sets of machine learning models to be explored without initial worries about the underlying algorithm. [sent-73, score-0.342]
</p><p>31 In turn, machine learning has motivated advances in mathematical programming: the optimization problems arising from large scale machine learning and data mining far exceed the size of the problem typically reported in the mathematical programming literature. [sent-74, score-0.576]
</p><p>32 This special topic investigates two majors themes in the interplay of machine learning (ML) and mathematical programming (MP). [sent-75, score-0.595]
</p><p>33 A wide range of convex programming methods is used to create novel models for problems such as uncertain and missing data, and hypothesis selection. [sent-77, score-0.456]
</p><p>34 Methods that exploit the properties of learning problems can outperform generic mathematical programming algorithms. [sent-81, score-0.347]
</p><p>35 Many of the included papers deal with well-known convex optimization problems present in ML tools such as the quadratic and linear programs at the core of the ubiquitous support vector machines (SVM) in either primal or dual forms. [sent-82, score-0.665]
</p><p>36 Tree re-weighted belief propagation is used to solve LP relaxations of large scale real-world belief nets. [sent-83, score-0.249]
</p><p>37 To summarize, in this special issue we see novel approaches to machine learning models that require solution of continuous optimization problems including: unconstrained, quadratic, linear, second-order cone, semi-deﬁnite, and semi-inﬁnite convex programs. [sent-89, score-0.413]
</p><p>38 We ﬁrst examine the interplay of machine learning and mathematical programming to understand the desirable properties of optimization methods used for training a machine learning model. [sent-90, score-0.531]
</p><p>39 We observe that the desirable properties of an optimization algorithm from a machine learning perspective can differ quite markedly from those typically seen in mathematical programming papers. [sent-91, score-0.412]
</p><p>40 Interplay of Optimization and Machine Learning The interplay of optimization and machine learning is complicated by the fact that machine learning mixes modeling and methods. [sent-94, score-0.251]
</p><p>41 Both OR and ML analysts address real world problems by formulating a model, deriving the core optimization problem, and using mathematical programming to solve it. [sent-99, score-0.533]
</p><p>42 So at a high level the OR and ML analysts face the same validity and tractability dilemmas and it is not surprising that both can exploit the same optimization toolbox. [sent-101, score-0.304]
</p><p>43 Reducing the problem to a convex optimization by appropriate choices of loss and constraints or relaxations can greatly help the search problem. [sent-111, score-0.41]
</p><p>44 For example, ridge regression for a ﬁxed ridge parameter is a convex unconstrained quadratic program. [sent-113, score-0.303]
</p><p>45 Little or no attention is paid to how well the underlying optimization problem was solved by any of the metrics typically used in mathematical programming. [sent-128, score-0.289]
</p><p>46 Thus not only is “good” optimization not necessary, but “bad” optimization algorithms can lead to better machine learning models. [sent-141, score-0.264]
</p><p>47 However, the ML optimization can be tailored to exploit the structure of the optimization model. [sent-153, score-0.331]
</p><p>48 New Machine Learning Models Using Existing Optimization Methods The special topic papers include novel machine learning models based on existing primarily convex programs such as linear, second order cone, and semi-deﬁnite programming. [sent-157, score-0.549]
</p><p>49 In these papers, the authors develop novel modeling approaches to uncertainty, hypothesis selection, incorporation of domain constraints, and graph clustering, and they use off-the-shelf optimization packages to solve the models. [sent-159, score-0.256]
</p><p>50 , 2006) propose a chain of transformations of the quadratic integer program towards a convex semi-deﬁnite program (SDP). [sent-181, score-0.349]
</p><p>51 They exploit the biconvex nature of Euclidean NMF and the reverse-convex structure of the corresponding sparsity constraints to derive an efﬁcient optimization algorithm. [sent-201, score-0.295]
</p><p>52 The mathematical formulation of this problem leads to an intractable combinatorial optimization problem. [sent-206, score-0.29]
</p><p>53 Reﬁning the Classics: Improvements in Algorithms for Widely Ssed Models Widely used methods such as SVM and Bayesian networks have well-accepted core optimization problems and algorithms. [sent-213, score-0.213]
</p><p>54 The immediate answer to this demand from the optimization and machine learning communities is to try to come up with more efﬁcient implementations of these solid and reliable optimization methods. [sent-215, score-0.32]
</p><p>55 Now the demand for more scalable and easier to implement algorithms makes novel algorithms for SVMs an active and dynamic research area. [sent-219, score-0.34]
</p><p>56 Thus identiﬁcation of the active constraints, or active set, represents a key step in LP and QP algorithms. [sent-225, score-0.394]
</p><p>57 An active set strategy estimates the active set, solves the problem with respect to the estimated active set, uses the result to update the active set by adding and dropping constraints, and then repeats until an optimal solution is found. [sent-227, score-0.788]
</p><p>58 For example in SVM classiﬁcation, the active set in the primal corresponds to data points that are on the margin or in error. [sent-229, score-0.29]
</p><p>59 The paper “An Efﬁcient Implementation of an Active Set Method for SVMs” (Scheinberg, 2006) adapts “traditional” active set methods to the special structure of SVMs. [sent-232, score-0.239]
</p><p>60 Traditional active set methods were not thought to be tractable for large scale SVMs, but the paper concludes that they are competitive with popular methods such as SVMlight (Joachims, 1999). [sent-233, score-0.252]
</p><p>61 The restricted active set method in SVMLight decomposes the QP into subproblems, each identiﬁed by a group of variables that form an active set. [sent-235, score-0.394]
</p><p>62 When full active sets are used, there is a corresponding speedup in the convergence of the optimization method. [sent-240, score-0.329]
</p><p>63 The full active set method offers a speed scalability tradeoff, it performs faster that SVMLight but may reach memory limitations sooner since it requires storage of a matrix of the size of the active set. [sent-244, score-0.446]
</p><p>64 Reduced active set methods are taken to the extreme result in the popular sequential minimal optimization (SMO) method (Platt, 1999). [sent-245, score-0.329]
</p><p>65 The authors examine incremental SVM learning in two scenarios: active learning and limited resource learning. [sent-262, score-0.24]
</p><p>66 The efﬁciency of the method relies on both a primal method approach to the optimization and a cheap and accurate selection criterion for kernel basis functions. [sent-273, score-0.227]
</p><p>67 Primal or direct kernel SVM models formulated with absolute value type losses and one-norm regularization produce LP core optimization problems. [sent-276, score-0.275]
</p><p>68 Robust general purpose LP optimization tools that exploit advanced numerical analysis are available that can reliably and accurately solve massive problems. [sent-278, score-0.248]
</p><p>69 , 2006) and (Shalev-Shwartz and Singer, 2006), the revised formulations and novel algorithms can more effectively exploit special structure thus reducing the problem to a series of familiar, more easily solved problems. [sent-297, score-0.244]
</p><p>70 , 2006) proposes a novel semi-inﬁnite linear program (SILP) for the problem of learning with multiple kernels. [sent-302, score-0.207]
</p><p>71 3 Max Margin Methods for Structured Output Two of the papers tackled maximum margin methods for outputs deﬁned on graphs by reformulating the problem and developing algorithms that could exploit the special structure. [sent-323, score-0.228]
</p><p>72 Tractability from the optimization point of view is achieved through the grouping of the variables of the optimization problem into marginals deﬁned by the graphical model. [sent-330, score-0.264]
</p><p>73 Moreover, the optimization is enhanced by a dynamic program that computes the best update directions in the feasible set. [sent-335, score-0.21]
</p><p>74 , 2006b) proposes simple scalable maximimum margin algorithms for structured output models including Markov networks and combinatorial models. [sent-337, score-0.248]
</p><p>75 By thinking of the problem one level up as a convex concave saddle point model, the authors can capitalize on the recent advances in optimization on extragradient methods (Nesterov, 2003). [sent-341, score-0.307]
</p><p>76 This demonstrates the interplay of the optimization algorithm and regularization: the path of the optimization algorithm is part of the regularization and there is no need to accurately solve the model. [sent-346, score-0.383]
</p><p>77 In this issue we see MP researchers using convex optimization methods including linear, nonlinear, saddle point, semi-inﬁnite, second order cone, and semi-deﬁnite programming models. [sent-350, score-0.461]
</p><p>78 The resulting ML models challenge the capacity of general purpose solvers resulting in the development of novel special purpose algorithms that exploit problem structure. [sent-352, score-0.356]
</p><p>79 These special purpose solvers 1275  ´ B ENNETT AND PARRADO -H ERN ANDEZ  do not necessarily possess the traits associated with good optimization algorithms. [sent-353, score-0.223]
</p><p>80 In this special topic large scale problems were successfully tackled by methods that exploited both the novel MP models and their special structure and state-of-the-art MP methods. [sent-360, score-0.362]
</p><p>81 The special issue illustrates the many forms of convex programs that can be used in ML. [sent-361, score-0.246]
</p><p>82 But we expect the interplay of MP and ML will increase as more branches of the MP tree are incorporated into ML and the demands of large scale ML models exceed the capacity of existing solvers. [sent-362, score-0.278]
</p><p>83 Appendix: Standard Convex Programs This section reviews the basic convex optimization mathematical programming models used in this special issue. [sent-366, score-0.606]
</p><p>84 The use of the least squares loss function in methods such as ridge regression and the 2-norm regularization in most support vector machine models both lead to quadratic programming models. [sent-368, score-0.343]
</p><p>85 Based on (Nocedal and Wright, 1999), we provide a brief review of quadratic programming and the reader can see (Nocedal and Wright, 1999) for more details. [sent-370, score-0.235]
</p><p>86 The general quadratic program can be stated as 1 mins 2 s′ Qs + c′ s (2) subject to ai s ≤ bi i∈I a js = b j j∈ε where the Hessian Q is a n × n symmetric matrix, I and ε are ﬁnite sets of indices and ai , i ∈ I ∪ ε are n × 1 vectors. [sent-371, score-0.369]
</p><p>87 We focus on the latter since active set algorithms are a key component of this special topic. [sent-380, score-0.239]
</p><p>88 The optimal active set is the set of constraints satisﬁed as equalities at the optimal solutions. [sent-381, score-0.293]
</p><p>89 Active set methods work by making educated guesses as to the active set and solving the resulting equality constrained QP. [sent-382, score-0.237]
</p><p>90 If the guesses are wrong, the method uses gradient and Lagrangian multiplier information to determine constraints to add to or subtract from the active set. [sent-383, score-0.333]
</p><p>91 Linear Programming Linear programming optimizes a linear function subject to linear constraints. [sent-385, score-0.207]
</p><p>92 Linear programming can be thought of as a special case of the QP with the Hessian Q equal to 0. [sent-387, score-0.213]
</p><p>93 The general linear program can be stated as mins c′ s subject to ai s ≤ bi i ∈ I (4) a js = b j j ∈ ε Interior point methods and simplex methods (active set methods) are both widely used within general purpose LP solvers. [sent-388, score-0.354]
</p><p>94 Second-Order Cone Programming The second-order cone program (SOCP) problems have a linear objective, second-order cone constraints, and possibly additional linear constraints: mins c′ s subject to ||Ri s + di ||2 ≤ ai s + bi i ∈ C a js = b j j∈ε  (5)  where Ri ∈ Rni ×n and di ∈ Rni . [sent-389, score-0.617]
</p><p>95 Semideﬁnite Programming Semideﬁnite programs (SDPs) are the generalization of linear programs to matrices. [sent-393, score-0.228]
</p><p>96 In standard 1277  ´ B ENNETT AND PARRADO -H ERN ANDEZ  form an SDP minimizes a linear function of a matrix subject to linear equality constraints and a matrix nonnegativity constraint: minS subject to  C, S Ai , S = bi i ∈ I S 0  (6)  where S, C, and Ai are in Rn×n and bi ∈ R. [sent-394, score-0.244]
</p><p>97 SDPs are most commonly solved via interior programming methods. [sent-396, score-0.219]
</p><p>98 Semi-inﬁnite Programming Semi-inﬁnite linear programs (SILPs) are linear programs with inﬁnitely many constraints. [sent-398, score-0.228]
</p><p>99 Second order cone programming approaches for handling missing and uncertain data. [sent-658, score-0.373]
</p><p>100 Linear programming relaxations and belief propagation- an empirical study. [sent-691, score-0.314]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ml', 0.265), ('lp', 0.243), ('mp', 0.235), ('active', 0.197), ('programming', 0.171), ('andez', 0.161), ('ennett', 0.161), ('ern', 0.161), ('parrado', 0.161), ('cone', 0.156), ('onstrained', 0.136), ('ptimization', 0.136), ('qp', 0.132), ('optimization', 0.132), ('sdp', 0.122), ('achine', 0.122), ('interplay', 0.119), ('programs', 0.114), ('mathematical', 0.109), ('mins', 0.102), ('constraints', 0.096), ('relaxations', 0.092), ('taskar', 0.091), ('convex', 0.09), ('novel', 0.087), ('extragradient', 0.085), ('core', 0.081), ('analyst', 0.08), ('themes', 0.08), ('papers', 0.08), ('mangasarian', 0.079), ('program', 0.078), ('svm', 0.076), ('topic', 0.074), ('nonconvex', 0.073), ('socp', 0.07), ('svms', 0.07), ('silp', 0.068), ('researchers', 0.068), ('exploit', 0.067), ('tractability', 0.065), ('subproblems', 0.064), ('quadratic', 0.064), ('models', 0.062), ('nocedal', 0.061), ('dolan', 0.06), ('goberna', 0.06), ('mor', 0.06), ('premium', 0.06), ('rummelhart', 0.06), ('unconstrained', 0.057), ('structured', 0.056), ('demand', 0.056), ('bradley', 0.056), ('scale', 0.055), ('primal', 0.054), ('scalability', 0.052), ('js', 0.051), ('trbp', 0.051), ('bergkvist', 0.051), ('belief', 0.051), ('ranking', 0.051), ('dual', 0.05), ('purpose', 0.049), ('combinatorial', 0.049), ('solved', 0.048), ('earning', 0.048), ('ridge', 0.046), ('uncertain', 0.046), ('qs', 0.046), ('multiprocessor', 0.046), ('sonnenburg', 0.046), ('svmlight', 0.044), ('cplex', 0.044), ('incremental', 0.043), ('branches', 0.042), ('backpropagation', 0.042), ('benchmarking', 0.042), ('bie', 0.042), ('nmf', 0.042), ('proposes', 0.042), ('special', 0.042), ('selection', 0.041), ('analysts', 0.04), ('bazaraa', 0.04), ('guesses', 0.04), ('madrid', 0.04), ('modeler', 0.04), ('nemhauser', 0.04), ('pez', 0.04), ('reemtsen', 0.04), ('rni', 0.04), ('integer', 0.039), ('margin', 0.039), ('bi', 0.038), ('develop', 0.037), ('relaxation', 0.037), ('wright', 0.037), ('reformulated', 0.037), ('subject', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="91-tfidf-1" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>2 0.18639438 <a title="91-tfidf-2" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Chen Yanover, Talya Meltzer, Yair Weiss</p><p>Abstract: The problem of ﬁnding the most probable (MAP) conﬁguration in graphical models comes up in a wide range of applications. In a general graphical model this problem is NP hard, but various approximate algorithms have been developed. Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for ﬁnding the most probable assignment in small graphical models. However, applying this powerful method to real-world problems is extremely challenging due to the large numbers of variables and constraints in the linear program. Tree-Reweighted Belief Propagation is a promising recent algorithm for solving LP relaxations, but little is known about its running time on large problems. In this paper we compare tree-reweighted belief propagation (TRBP) and powerful generalpurpose LP solvers (CPLEX) on relaxations of real-world graphical models from the ﬁelds of computer vision and computational biology. We ﬁnd that TRBP almost always ﬁnds the solution signiﬁcantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. Using TRBP we can ﬁnd the MAP conﬁgurations in a matter of minutes for a large range of real world problems.</p><p>3 0.17429011 <a title="91-tfidf-3" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tijl De Bie, Nello Cristianini</p><p>Abstract: The rise of convex programming has changed the face of many research ﬁelds in recent years, machine learning being one of the ones that beneﬁtted the most. A very recent developement, the relaxation of combinatorial problems to semi-deﬁnite programs (SDP), has gained considerable attention over the last decade (Helmberg, 2000; De Bie and Cristianini, 2004a). Although SDP problems can be solved in polynomial time, for many relaxations the exponent in the polynomial complexity bounds is too high for scaling to large problem sizes. This has hampered their uptake as a powerful new tool in machine learning. In this paper, we present a new and fast SDP relaxation of the normalized graph cut problem, and investigate its usefulness in unsupervised and semi-supervised learning. In particular, this provides a convex algorithm for transduction, as well as approaches to clustering. We further propose a whole cascade of fast relaxations that all hold the middle between older spectral relaxations and the new SDP relaxation, allowing one to trade oﬀ computational cost versus relaxation accuracy. Finally, we discuss how the methodology developed in this paper can be applied to other combinatorial problems in machine learning, and we treat the max-cut problem as an example. Keywords: convex transduction, normalized graph cut, semi-deﬁnite programming, semisupervised learning, relaxation, combinatorial optimization, max-cut</p><p>4 0.14926283 <a title="91-tfidf-4" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Olvi L. Mangasarian</p><p>Abstract: Support vector machines utilizing the 1-norm, typically set up as linear programs (Mangasarian, 2000; Bradley and Mangasarian, 1998), are formulated here as a completely unconstrained minimization of a convex differentiable piecewise-quadratic objective function in the dual space. The objective function, which has a Lipschitz continuous gradient and contains only one additional ﬁnite parameter, can be minimized by a generalized Newton method and leads to an exact solution of the support vector machine problem. The approach here is based on a formulation of a very general linear program as an unconstrained minimization problem and its application to support vector machine classiﬁcation problems. The present approach which generalizes both (Mangasarian, 2004) and (Fung and Mangasarian, 2004) is also applied to nonlinear approximation where a minimal number of nonlinear kernel functions are utilized to approximate a function from a given number of function values.</p><p>5 0.13780206 <a title="91-tfidf-5" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Matthias Heiler, Christoph Schnörr</p><p>Abstract: We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. We show that for ordinary NMF, our approach performs as well as existing stateof-the-art algorithms, while for sparsity-constrained NMF, as recently proposed by P. O. Hoyer in JMLR 5 (2004), it outperforms previous methods. In addition, we show how to extend NMF learning within the same optimization framework in order to make use of class membership information in supervised learning problems. Keywords: non-negative matrix factorization, second-order cone programming, sequential convex optimization, reverse-convex programming, sparsity</p><p>6 0.12827808 <a title="91-tfidf-6" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.11477505 <a title="91-tfidf-7" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.11239772 <a title="91-tfidf-8" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.10768165 <a title="91-tfidf-9" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.1001977 <a title="91-tfidf-10" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>11 0.096897945 <a title="91-tfidf-11" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>12 0.095905803 <a title="91-tfidf-12" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.095349796 <a title="91-tfidf-13" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.09531673 <a title="91-tfidf-14" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.093027137 <a title="91-tfidf-15" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.081187077 <a title="91-tfidf-16" href="./jmlr-2006-Ensemble_Pruning_Via_Semi-definite_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">27 jmlr-2006-Ensemble Pruning Via Semi-definite Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.080379687 <a title="91-tfidf-17" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.077938624 <a title="91-tfidf-18" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.067759834 <a title="91-tfidf-19" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.064353652 <a title="91-tfidf-20" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.35), (1, -0.227), (2, 0.084), (3, 0.336), (4, 0.129), (5, -0.005), (6, -0.014), (7, 0.316), (8, 0.157), (9, 0.038), (10, -0.136), (11, -0.043), (12, -0.038), (13, 0.044), (14, -0.127), (15, 0.049), (16, -0.098), (17, -0.086), (18, 0.032), (19, -0.068), (20, 0.006), (21, -0.088), (22, 0.049), (23, 0.072), (24, -0.005), (25, 0.029), (26, -0.046), (27, 0.108), (28, 0.029), (29, -0.07), (30, 0.012), (31, 0.074), (32, -0.044), (33, 0.019), (34, -0.033), (35, 0.003), (36, -0.043), (37, -0.044), (38, -0.019), (39, -0.03), (40, -0.009), (41, -0.011), (42, 0.018), (43, 0.037), (44, 0.057), (45, 0.002), (46, -0.002), (47, -0.001), (48, 0.037), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96123469 <a title="91-lsi-1" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>2 0.70068377 <a title="91-lsi-2" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Chen Yanover, Talya Meltzer, Yair Weiss</p><p>Abstract: The problem of ﬁnding the most probable (MAP) conﬁguration in graphical models comes up in a wide range of applications. In a general graphical model this problem is NP hard, but various approximate algorithms have been developed. Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for ﬁnding the most probable assignment in small graphical models. However, applying this powerful method to real-world problems is extremely challenging due to the large numbers of variables and constraints in the linear program. Tree-Reweighted Belief Propagation is a promising recent algorithm for solving LP relaxations, but little is known about its running time on large problems. In this paper we compare tree-reweighted belief propagation (TRBP) and powerful generalpurpose LP solvers (CPLEX) on relaxations of real-world graphical models from the ﬁelds of computer vision and computational biology. We ﬁnd that TRBP almost always ﬁnds the solution signiﬁcantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. Using TRBP we can ﬁnd the MAP conﬁgurations in a matter of minutes for a large range of real world problems.</p><p>3 0.59447479 <a title="91-lsi-3" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Olvi L. Mangasarian</p><p>Abstract: Support vector machines utilizing the 1-norm, typically set up as linear programs (Mangasarian, 2000; Bradley and Mangasarian, 1998), are formulated here as a completely unconstrained minimization of a convex differentiable piecewise-quadratic objective function in the dual space. The objective function, which has a Lipschitz continuous gradient and contains only one additional ﬁnite parameter, can be minimized by a generalized Newton method and leads to an exact solution of the support vector machine problem. The approach here is based on a formulation of a very general linear program as an unconstrained minimization problem and its application to support vector machine classiﬁcation problems. The present approach which generalizes both (Mangasarian, 2004) and (Fung and Mangasarian, 2004) is also applied to nonlinear approximation where a minimal number of nonlinear kernel functions are utilized to approximate a function from a given number of function values.</p><p>4 0.57217824 <a title="91-lsi-4" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tijl De Bie, Nello Cristianini</p><p>Abstract: The rise of convex programming has changed the face of many research ﬁelds in recent years, machine learning being one of the ones that beneﬁtted the most. A very recent developement, the relaxation of combinatorial problems to semi-deﬁnite programs (SDP), has gained considerable attention over the last decade (Helmberg, 2000; De Bie and Cristianini, 2004a). Although SDP problems can be solved in polynomial time, for many relaxations the exponent in the polynomial complexity bounds is too high for scaling to large problem sizes. This has hampered their uptake as a powerful new tool in machine learning. In this paper, we present a new and fast SDP relaxation of the normalized graph cut problem, and investigate its usefulness in unsupervised and semi-supervised learning. In particular, this provides a convex algorithm for transduction, as well as approaches to clustering. We further propose a whole cascade of fast relaxations that all hold the middle between older spectral relaxations and the new SDP relaxation, allowing one to trade oﬀ computational cost versus relaxation accuracy. Finally, we discuss how the methodology developed in this paper can be applied to other combinatorial problems in machine learning, and we treat the max-cut problem as an example. Keywords: convex transduction, normalized graph cut, semi-deﬁnite programming, semisupervised learning, relaxation, combinatorial optimization, max-cut</p><p>5 0.51986945 <a title="91-lsi-5" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Matthias Heiler, Christoph Schnörr</p><p>Abstract: We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. We show that for ordinary NMF, our approach performs as well as existing stateof-the-art algorithms, while for sparsity-constrained NMF, as recently proposed by P. O. Hoyer in JMLR 5 (2004), it outperforms previous methods. In addition, we show how to extend NMF learning within the same optimization framework in order to make use of class membership information in supervised learning problems. Keywords: non-negative matrix factorization, second-order cone programming, sequential convex optimization, reverse-convex programming, sparsity</p><p>6 0.43834195 <a title="91-lsi-6" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.42912048 <a title="91-lsi-7" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.42159989 <a title="91-lsi-8" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.41594225 <a title="91-lsi-9" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.4108814 <a title="91-lsi-10" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.40827194 <a title="91-lsi-11" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.4033249 <a title="91-lsi-12" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.40150386 <a title="91-lsi-13" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.35489497 <a title="91-lsi-14" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>15 0.34233493 <a title="91-lsi-15" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>16 0.33901715 <a title="91-lsi-16" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.33355013 <a title="91-lsi-17" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.33152938 <a title="91-lsi-18" href="./jmlr-2006-Ensemble_Pruning_Via_Semi-definite_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">27 jmlr-2006-Ensemble Pruning Via Semi-definite Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.30144832 <a title="91-lsi-19" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>20 0.29653636 <a title="91-lsi-20" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.012), (35, 0.022), (36, 0.041), (38, 0.018), (44, 0.011), (45, 0.013), (50, 0.027), (63, 0.039), (68, 0.012), (76, 0.018), (78, 0.019), (81, 0.025), (84, 0.028), (90, 0.021), (91, 0.049), (96, 0.573), (99, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99046659 <a title="91-lda-1" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>2 0.98307741 <a title="91-lda-2" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>Author: Hema Raghavan, Omid Madani, Rosie Jones</p><p>Abstract: We extend the traditional active learning framework to include feedback on features in addition to labeling instances, and we execute a careful study of the effects of feature selection and human feedback on features in the setting of text categorization. Our experiments on a variety of categorization tasks indicate that there is signiﬁcant potential in improving classiﬁer performance by feature re-weighting, beyond that achieved via membership queries alone (traditional active learning) if we have access to an oracle that can point to the important (most predictive) features. Our experiments on human subjects indicate that human feedback on feature relevance can identify a sufﬁcient proportion of the most relevant features (over 50% in our experiments). We ﬁnd that on average, labeling a feature takes much less time than labeling a document. We devise an algorithm that interleaves labeling features and documents which signiﬁcantly accelerates standard active learning in our simulation experiments. Feature feedback can complement traditional active learning in applications such as news ﬁltering, e-mail classiﬁcation, and personalization, where the human teacher can have signiﬁcant knowledge on the relevance of features. Keywords: active learning, feature selection, relevance feedback, term feedback, text classiﬁcation</p><p>3 0.96352637 <a title="91-lda-3" href="./jmlr-2006-Adaptive_Prototype_Learning_Algorithms%3A_Theoretical_and_Experimental_Studies.html">13 jmlr-2006-Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies</a></p>
<p>Author: Fu Chang, Chin-Chin Lin, Chi-Jen Lu</p><p>Abstract: In this paper, we propose a number of adaptive prototype learning (APL) algorithms. They employ the same algorithmic scheme to determine the number and location of prototypes, but differ in the use of samples or the weighted averages of samples as prototypes, and also in the assumption of distance measures. To understand these algorithms from a theoretical viewpoint, we address their convergence properties, as well as their consistency under certain conditions. We also present a soft version of APL, in which a non-zero training error is allowed in order to enhance the generalization power of the resultant classifier. Applying the proposed algorithms to twelve UCI benchmark data sets, we demonstrate that they outperform many instance-based learning algorithms, the k-nearest neighbor rule, and support vector machines in terms of average test accuracy. Keywords: adaptive prototype learning, cluster-based prototypes, consistency, instance-based prototype, pattern classification 1</p><p>4 0.72622472 <a title="91-lda-4" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>Author: Jing Zhou, Dean P. Foster, Robert A. Stine, Lyle H. Ungar</p><p>Abstract: In streamwise feature selection, new features are sequentially considered for addition to a predictive model. When the space of potential features is large, streamwise feature selection offers many advantages over traditional feature selection methods, which assume that all features are known in advance. Features can be generated dynamically, focusing the search for new features on promising subspaces, and overﬁtting can be controlled by dynamically adjusting the threshold for adding features to the model. In contrast to traditional forward feature selection algorithms such as stepwise regression in which at each step all possible features are evaluated and the best one is selected, streamwise feature selection only evaluates each feature once when it is generated. We describe information-investing and α-investing, two adaptive complexity penalty methods for streamwise feature selection which dynamically adjust the threshold on the error reduction required for adding a new feature. These two methods give false discovery rate style guarantees against overﬁtting. They differ from standard penalty methods such as AIC, BIC and RIC, which always drastically over- or under-ﬁt in the limit of inﬁnite numbers of non-predictive features. Empirical results show that streamwise regression is competitive with (on small data sets) and superior to (on large data sets) much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with millions of potential features. Keywords: classiﬁcation, stepwise regression, multiple regression, feature selection, false discovery rate</p><p>5 0.72444892 <a title="91-lda-5" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>Author: Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou</p><p>Abstract: We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the ﬁrst time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction. html. Keywords: transduction, transductive SVMs, semi-supervised learning, CCCP</p><p>6 0.72205287 <a title="91-lda-6" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.71572399 <a title="91-lda-7" href="./jmlr-2006-Rearrangement_Clustering%3A_Pitfalls%2C_Remedies%2C_and_Applications.html">78 jmlr-2006-Rearrangement Clustering: Pitfalls, Remedies, and Applications</a></p>
<p>8 0.71423882 <a title="91-lda-8" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.71063691 <a title="91-lda-9" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.70769072 <a title="91-lda-10" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.70607609 <a title="91-lda-11" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>12 0.70312536 <a title="91-lda-12" href="./jmlr-2006-Ensemble_Pruning_Via_Semi-definite_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">27 jmlr-2006-Ensemble Pruning Via Semi-definite Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.69161302 <a title="91-lda-13" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.64798689 <a title="91-lda-14" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.64351976 <a title="91-lda-15" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.64164811 <a title="91-lda-16" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>17 0.63944346 <a title="91-lda-17" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>18 0.63226599 <a title="91-lda-18" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>19 0.62482029 <a title="91-lda-19" href="./jmlr-2006-Evolutionary_Function_Approximation_for_Reinforcement_Learning.html">30 jmlr-2006-Evolutionary Function Approximation for Reinforcement Learning</a></p>
<p>20 0.61829436 <a title="91-lda-20" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
