<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-43" href="#">jmlr2006-43</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-43-pdf" href="http://jmlr.org/papers/volume7/sonnenburg06a/sonnenburg06a.pdf">pdf</a></p><p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer, Bernhard Schölkopf</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun. Keywords: multiple kernel learning, string kernels, large scale optimization, support vector machines, support vector regression, column generation, semi-inﬁnite linear programming</p><p>Reference: <a title="jmlr-2006-43-reference" href="../jmlr2006_reference/jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constrained quadratic program. [sent-17, score-0.229]
</p><p>2 In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. [sent-21, score-0.507]
</p><p>3 While this so-called “multiple kernel learning” (MKL) problem can in principle be solved via cross-validation, several recent papers have focused on more efﬁcient methods for multiple kernel learning (Chapelle et al. [sent-38, score-0.37]
</p><p>4 K  k(xi , x j ) =  ∑ βk kk (xi , x j )  (2)  k=1  with βk ≥ 0 and ∑K βk = 1, where each kernel kk uses only a distinct set of features. [sent-48, score-0.331]
</p><p>5 Our proposed wrapper method works for any kernel and many loss functions: In order to obtain an efﬁcient MKL 1532  L ARGE S CALE MKL  algorithm for a new loss function, it now sufﬁces to have an LP solver and the corresponding single kernel algorithm (which is assumed to be efﬁcient). [sent-70, score-0.48]
</p><p>6 1 We also consider a chunking algorithm that can be considerably more efﬁcient, since it optimizes the SVM α multipliers and the kernel coefﬁcients β at the same time. [sent-72, score-0.313]
</p><p>7 However, for large scale problems it needs to compute and cache the K kernels separately, instead of only one kernel as in the single kernel algorithm. [sent-73, score-0.483]
</p><p>8 If, on the other hand, the number of kernels K is large, then the amount of memory available for caching is drastically reduced and, hence, kernel caching is not effective anymore. [sent-75, score-0.366]
</p><p>9 ) Since kernel caching cannot help to solve large scale MKL problems, we sought for ways to avoid kernel caching. [sent-78, score-0.418]
</p><p>10 In the experimental part we show that the resulting algorithm is more than 70 times faster than the plain chunking algorithm (for 50,000 examples), even though large kernel caches were used. [sent-90, score-0.313]
</p><p>11 Also, we were able to solve MKL problems with up to one million examples and 20 kernels and a 10 million real-world splice site classiﬁcation problem from computational biology. [sent-91, score-0.426]
</p><p>12 1 Multiple Kernel Learning for Classiﬁcation Using SILP In the multiple kernel learning problem for binary classiﬁcation one is given N data points (xi , yi ) (yi ∈ {±1}), where xi is translated via K mappings Φk (x) → RDk , k = 1, . [sent-110, score-0.318]
</p><p>13 1 2  2  K  ∑  wk  N  +C ∑ ξi  2  (3)  i=1  k=1  wk ∈ RDk , ξ ∈ RN , b ∈ R, ξi ≥ 0 and yi  K  ∑  wk , Φk (xi ) + b  ≥ 1 − ξi , ∀i = 1, . [sent-125, score-0.497]
</p><p>14 γ ∈ R, α ∈ RN 0 ≤ α ≤ 1C,  N  ∑ αi yi = 0  i=1 N  1 αi α j yi y j kk (xi , x j ) ≤ γ, ∀k = 1, . [sent-146, score-0.263]
</p><p>15 0 ≤ α ≤ 1C,  N N  ∑ αi yi = 0  i=1  N 1 ∑ αi α j yi y j kk (xi , x j ) − ∑ αi ≤ γ, ∀k = 1, . [sent-164, score-0.263]
</p><p>16 (2004) it is argued that the approximation quality of composite kernels is inferior to mixtures of kernels where a weight is assigned per example and kernel as in Bennett et al. [sent-204, score-0.385]
</p><p>17 For that reason and as no efﬁcient methods were available to solve the composite kernel problem, they only considered mixtures of kernels and in the experimental validation used a uniform weighting in the composite kernel experiment. [sent-206, score-0.582]
</p><p>18 We deﬁne the MKL primal formulation for a strictly convex and differentiable loss function L( f (x), y) as: MKL Primal for Generic Loss Functions  min  2  K  1 2  ∑  wk  N  + ∑ L( f (xi ), yi )  (9)  i=1  k=1  w = (w1 , . [sent-222, score-0.287]
</p><p>19 ∑ αi = 0  and  i=1  1 2  N  2  i=1  N  N  i=1  i=1  − ∑ L(L′−1 (αi , yi ), yi ) + ∑ αi L′−1 (αi , yi ) ≤ γ, ∀k = 1, . [sent-239, score-0.285]
</p><p>20 (11) K  0 ≤ β,  K  K  ∑ βk = 1 and  k=1  ∑ βk Sk (α) ≥ θ, ∀α ∈ RN ,  k=1  N  ∑ αi = 0,  i=1  where N  Sk (α) = − ∑ L(L i=1  ′−1  N  (αi , yi ), yi ) + ∑ αi L  ′−1  i=1  1 (αi , yi ) + 2  2  N  ∑ αi Φk (xi )  i=1  . [sent-252, score-0.285]
</p><p>21 Substituting αi = −αi yi , we obtain N  ˜ ˜ Sk (α) = − ∑ αi + i=1  1 2  2  N  ˜ ∑ αi yi Φk (xi )  i=1  N  and  ˜ ∑ αi yi = 0,  i=1  2  ˜ with 0 ≤ αi ≤ C (i = 1, . [sent-263, score-0.285]
</p><p>22 For instance for binary classiﬁcation with soft-margin loss, (13) reduces to the standard SVM dual using the kernel k(xi , x j ) = ∑k βk kk (xi , x j ): N  v=  min  ∑  α∈RN i, j=1  s. [sent-324, score-0.298]
</p><p>23 Also, if the kernels are computed on-theﬂy within the SVM still only a single kernel cache is required. [sent-329, score-0.298]
</p><p>24 Here we would like to propose an extension of the chunking algorithm to optimize the kernel weights β and the example weights α at the same time. [sent-343, score-0.359]
</p><p>25 , K K  Compute αt = argmin ∑ βtk Sk (α) by single kernel algorithm with k = α∈C  K  ∑  t  S =  βtk Stk ,  where  k=1  Stk  = Sk  K  ∑ βtk kk  k=1  (αt )  k=1  St ≤ εMKL then break θt t+1 t+1 (β , θ ) = argmax θ w. [sent-354, score-0.258]
</p><p>26 ,t  end for  be considerably faster if for any newly obtained α in the chunking iterations, we could efﬁciently recompute the optimal β and then continue optimizing the α’s using the new kernel weighting. [sent-362, score-0.313]
</p><p>27 However, if one changes the kernel weights, then the stored gi values become invalid and ˆ need to be recomputed. [sent-370, score-0.239]
</p><p>28 3 D ISCUSSION The Wrapper as well as the chunking algorithm have both their merits: The Wrapper algorithm only relies on the repeated efﬁcient computation of the single kernel solution, for which typically large scale algorithms exist. [sent-378, score-0.313]
</p><p>29 If, on the other hand, K is large, then the amount of memory available for caching is drastically reduced and, hence, kernel caching is not effective anymore. [sent-417, score-0.281]
</p><p>30 3 we discuss how we can exploit these properties to accelerate chunking algorithms, such as SVMlight , by a factor of up to Q (the chunking subproblem size). [sent-440, score-0.256]
</p><p>31 1 S TRING K ERNELS The Spectrum Kernel The spectrum kernel (Leslie et al. [sent-443, score-0.263]
</p><p>32 Then the spectrum kernel is deﬁned as the inner product of k(x, x′ ) = Φ(x), Φ(x′ ) , where Φ(x) = (#u(x))u∈Σd . [sent-447, score-0.263]
</p><p>33 The spectrum kernel can be efﬁciently computed in O (d(|x| + |x′ |)) using tries (Leslie et al. [sent-451, score-0.289]
</p><p>34 The Weighted Degree Kernel The so-called weighted degree (WD) kernel (Rätsch and Sonnenburg, 2004) efﬁciently computes similarities between sequences while taking positional information of kmers into account. [sent-457, score-0.246]
</p><p>35 The WD kernel of order d compares two sequences xi and x j of length L by summing all contributions of k-mer matches of lengths k ∈ {1, . [sent-459, score-0.26]
</p><p>36 Exploiting this  1542  L ARGE S CALE MKL  Note that the WD kernel can be understood as a Spectrum kernel where the k-mers starting at different positions are treated independently of each other. [sent-468, score-0.37]
</p><p>37 7 This approach has expensive memory requirements (O (|Σ|d )), but is very fast and best suited for instance for the Spectrum kernel on DNA sequences with d ≤ 14 and on protein sequences with d ≤ 6. [sent-487, score-0.259]
</p><p>38 L ARGE S CALE MKL  As a result the effort decreases to O (QN) kernel computations, which can be further speed up by using kernel caching (e. [sent-536, score-0.418]
</p><p>39 However kernel caching is not efﬁcient enough for large scale problems8 and thus most time is spend computing kernel rows for the updates of g on the working set W . [sent-539, score-0.44]
</p><p>40 For instance for the WD kernel one kernel computation requires O (Ld) operations (L is the length of the sequence). [sent-550, score-0.399]
</p><p>41 Finally note that kernel caching is no longer required and as Q is small in practice (e. [sent-555, score-0.233]
</p><p>42 The pseudo-code of our linadd SVM chunking algorithm is given in Algorithm 3. [sent-558, score-0.391]
</p><p>43 , K: one for each kernel in order to avoid full recomputation of g if a kernel weight βk is updated. [sent-576, score-0.397]
</p><p>44 When using the linadd algorithm, one ﬁrst constructs the trie (or any of the other possible more appropriate data structures) and then performs parallel lookup operations using several CPUs (e. [sent-597, score-0.421]
</p><p>45 9 The choice of the kernel width of the Gaussian RBF (below, denoted by RBF) kernel used for classiﬁcation is expected to depend on the separation distance of the learning problem: An increased distance between the stars will correspond to a larger optimal kernel width. [sent-652, score-0.653]
</p><p>46 In Figure 2 we show the obtained kernel weightings for the ﬁve kernels and the test error (circled line) which quickly drops to zero as the problem becomes separable. [sent-656, score-0.27]
</p><p>47 The courses of the kernel weightings reﬂect the development of the learning problem: as long as the problem is difﬁcult the best separation can be obtained when using the kernel with smallest width. [sent-658, score-0.37]
</p><p>48 The low width kernel looses importance when the distance between the stars increases and larger kernel widths obtain a larger weight in MKL. [sent-659, score-0.516]
</p><p>49 Note that the RBF kernel with largest width was not appropriate and thus never chosen. [sent-661, score-0.25]
</p><p>50 For every frequency the computed weights for each kernel width are shown. [sent-687, score-0.303]
</p><p>51 The largest selected width (100) models the linear component (since RBF kernels with large widths are effectively linear) and the medium width (1) corresponds to the lower frequency sine. [sent-698, score-0.293]
</p><p>52 One observes that MKL determines an appropriate combination of kernels of low and high widths, while decreasing the RBF kernel width with increased frequency. [sent-700, score-0.335]
</p><p>53 Additionally one can observe that MKL leads to sparse solutions since most of the kernel weights in Figure 4 are depicted in blue, that is they are zero. [sent-709, score-0.231]
</p><p>54 11 On that data set we were able to solve the classiﬁcation MKL SILP for N = 1, 000, 000 examples and K = 20 kernels, as well as for N = 10, 000 examples and K = 550 kernels, using the linadd optimizations with the weighted degree kernel. [sent-723, score-0.263]
</p><p>55 As an example we learned the weighting of a WD kernel of degree 20, which consist of a weighted sum of 20 sub-kernels each counting matching d-mers, for d = 1, . [sent-727, score-0.252]
</p><p>56 2 we will use a human splice data set containing 15 million examples, and train WD kernel based SVM classiﬁers on up to 10 million examples using the parallelized linadd algorithm. [sent-735, score-0.76]
</p><p>57 05  0  0  5  10  15  20  25  kernel index d (length of substring)  Figure 5: The learned WD kernel weighting on a million of examples. [sent-758, score-0.502]
</p><p>58 the 9th and 10th kernel leads to a combined kernel matrix that is most diagonally dominant (since the sequences are only similar to themselves but not to other sequences), which we believe is the reason for having a large weight. [sent-765, score-0.407]
</p><p>59 005 0 −50  −40  −30  −20  −10  Exon Start  +10  +20  +30  +40  +50  position relative to the exon start  Figure 6: The ﬁgure shows an importance weighting for each position in a DNA sequence (around a so called splice site). [sent-778, score-0.247]
</p><p>60 We can identify several interesting regions that we can match to current biological knowledge about splice site recognition: a) The region −50 nucleotides (nt) to −40nt, which corresponds to the donor splice site of the previous exon (many introns in C. [sent-785, score-0.454]
</p><p>61 The splice data set contains 159,771 true acceptor splice site sequences and 14,868,555 decoys, leading to a total of 15,028,326 sequences each 141 base pairs in length. [sent-795, score-0.433]
</p><p>62 We set the degree parameter to d = 20 for the WD kernel and to d = 8 for the spectrum kernel ﬁxing the SVMs regularization parameter to C = 5. [sent-803, score-0.448]
</p><p>63 A kernel cache of 1GB was used for all kernels except the precomputed kernel and algorithms using the linadd-SMO extension for which the kernel-cache was disabled. [sent-806, score-0.525]
</p><p>64 First, SVMs were trained using standard SVMlight with the Weighted Degree Kernel precomputed (WDPre), the standard WD kernel (WD1) and the precomputed (SpecPre) and standard spectrum kernel (Spec). [sent-814, score-0.532]
</p><p>65 Then SVMs utilizing the linadd extension14 were trained using the WD (LinWD) and spectrum (LinSpec) kernel. [sent-815, score-0.341]
</p><p>66 Finally SVMs were trained on four and eight CPUs using the parallel version of the linadd algorithm (LinWD4, LinWD8). [sent-816, score-0.263]
</p><p>67 WD4 and WD8 demonstrate the effect of a simple parallelization strategy where the computation of kernel rows and updates on the working set are parallelized, which works with any kernel. [sent-817, score-0.247]
</p><p>68 The training times obtained when precomputing the kernel matrix (which includes the time needed to precompute the full kernel matrix) is lower when no more than 1, 000 examples are used. [sent-818, score-0.421]
</p><p>69 More precisely the linadd and O (L) block formulation of the WD kernel as proposed in Sonnenburg et al. [sent-826, score-0.448]
</p><p>70 The picture is different for, say, Q = 42 (data not shown) where the WDPre training time is in all cases larger than the times obtained using the original WD kernel demonstrating the effectiveness of SVMlight ’s kernel cache. [sent-829, score-0.394]
</p><p>71 The overhead of constructing a trie on Q = 112 examples becomes even more visible: only starting from 50,000 examples linadd optimization becomes more efﬁcient than the original WD kernel algorithm as the kernel cache cannot hold all kernel elements anymore. [sent-830, score-0.88]
</p><p>72 The linadd formulation outperforms the original WD kernel by a factor of 3. [sent-832, score-0.448]
</p><p>73 The picture is similar for the spectrum kernel, here speedups of factor 64 on 500, 000 examples are reached which stems from the fact that explicit maps (and not tries as in the WD kernel case) as discussed in Section 3. [sent-834, score-0.317]
</p><p>74 For that reason the parallelization effort beneﬁts the WD kernel more than the Spectrum kernel: on one million examples the parallelization using 4 CPUs (8 CPUs) leads to a speedup of factor 3. [sent-837, score-0.367]
</p><p>75 Training with the original WD kernel with a sample size of 1, 000, 000 takes about 28 hours, the linadd version still requires 7 hours while with the 8 CPU parallel implementation only about 6 hours and in conjunction with the linadd optimization a single hour and 20 minutes are needed. [sent-843, score-0.711]
</p><p>76 3 and Algorithm 3, using the linadd algorithm for computing the output for all training examples w. [sent-865, score-0.287]
</p><p>77 When single precision 4-byte ﬂoating point numbers are used, caching all kernel elements is possible when training with up to 16384 examples. [sent-876, score-0.286]
</p><p>78 In the upper ﬁgure the Weighted Degree kernel training times are measured, the lower ﬁgure displays Spectrum kernel training times. [sent-879, score-0.418]
</p><p>79 (bottom) Speed Comparison of the spectrum kernel without (Spec) and with linadd (LinSpec1, LinSpec4, LinSpec8 using 1,4 and 8 processors). [sent-883, score-0.526]
</p><p>80 Using MKL we learned the weighting on the splice site recognition task for one million examples as displayed in Figure 5 and discussed in Section 4. [sent-897, score-0.369]
</p><p>81 Focusing on a speed comparison we now show the obtained training times for the different MKL algorithms applied to learning weightings of the WD kernel on the splice site classiﬁcation task. [sent-900, score-0.42]
</p><p>82 To do so, several MKL-SVMs were trained using precomputed kernel matrices (PreMKL), kernel matrices which are computed on the ﬂy employing kernel caching (MKL16 ), MKL using the linadd extension (LinMKL1) and linadd with its parallel implementation17 (LinMKL4 and LinMKL8 - on 4 and 8 CPUs). [sent-901, score-1.171]
</p><p>83 18 On-the-ﬂy-computation of the kernel matrices is computationally extremely demanding, but since kernel caching19 is used, it is still possible on 50,000 examples in about 57 hours. [sent-906, score-0.37]
</p><p>84 Algorithm 2 with the linadd extensions including parallelization of Algorithm 4. [sent-911, score-0.303]
</p><p>85 The linadd variants outperform the other algorithms by far (speedup factor 53 on 50,000 examples) and are still applicable to data sets of size up to one million. [sent-963, score-0.263]
</p><p>86 and one-class classiﬁcation and to try different losses on the kernel weighting β (such as L2 ). [sent-985, score-0.252]
</p><p>87 In the second part we proposed performance enhancements to make large scale MKL practical: the SILP wrapper, SILP chunking and (for the special case of kernels that can be written as an inner product of sparse feature vectors, e. [sent-986, score-0.236]
</p><p>88 For the standalone SVM using the spectrum kernel we achieved speedups of factor 64 (for the weighted degree kernel, about 4). [sent-991, score-0.291]
</p><p>89 Finally we proposed a parallel version of the linadd algorithm running on a 8 CPU multiprocessor system which lead to additional speedups of factor up to 5. [sent-993, score-0.291]
</p><p>90 N  + ∑ L( f (xi ), yi )  f (xi ) =  ∑  Φk (xi ), wk + b, ∀i = 1, . [sent-1009, score-0.229]
</p><p>91 1 2 N u + ∑ L( f (xi ), yi ) 2 i=1 u ∈ R, tk ∈ R, wk ∈ RDk , ∀k = 1, . [sent-1029, score-0.301]
</p><p>92 Then the conic 1560  L ARGE S CALE MKL  Lagrangian is given as N 1 2 N u + ∑ L( f (xi ), yi ) − ∑ αi f (xi ) + 2 i=1 i=1  L (w, b, t, u, α, γ, λ, µ) = N  i=1  k=1  K  K  + ∑ αi ∑ ( Φk (xi ), wk + b) + γ  ∑ tk − u  k=1  K  − ∑ ( λk , wk + µk tk ) . [sent-1057, score-0.551]
</p><p>93 Thus the dual function is N N 1 D(α, γ) = − γ2 + ∑ L(L′−1 (αi , yi ), yi ) − ∑ αi L′−1 (αi , yi ) + 2 i=1 i=1 N  i=1  K  K  k=1 N  N  + ∑ αi ∑ Φk (xi ), wk − ∑ ∑ αi Φk (xi ), wk k=1 i=1 N  1 = − γ2 + ∑ L(L′−1 (αi , yi ), yi ) − ∑ αi L′−1 (αi , yi ). [sent-1070, score-0.878]
</p><p>94 N N 1 − γ2 + ∑ L(L′−1 (αi , yi ), yi ) − ∑ αi L′−1 (αi , yi ) 2 i=1 i=1  γ ∈ R, α ∈ RN γ ≥ 0,  N  ∑ αi = 0  i=1 N  ∑ αi Φk (xi )  i=1  ≤ γ, ∀k = 1, . [sent-1080, score-0.285]
</p><p>95 Conic Dual N  N  i=1  i=1  γ − ∑ L(L′−1 (αi , yi ), yi ) + ∑ αi L′−1 (αi , yi )  min  :=T  γ ∈ R, α ∈ R  w. [sent-1084, score-0.285]
</p><p>96 Recall the deﬁnition of N  Sk (α) = − ∑ L(L i=1  ′−1  N  (αi , yi ), yi ) + ∑ αi L i=1  ′−1  1 (αi , yi ) + 2  2  N  ∑ αi Φk (xi )  i=1  . [sent-1097, score-0.285]
</p><p>97 2  Plugging in L, L′−1 leads to N  N 1 1 1 Sk (α) = − ∑ ( αi + yi − yi )2 + ∑ αi ( αi + yi ) + 2C 2C 2 i=1 i=1 N  =  N  1 1 ∑ α2 + ∑ αi yi + 2 4C i=1 i i=1  N  2  ∑ αi Φk (xi )  i=1  2  2  ∑ αi Φk (xi )  i=1  N  . [sent-1098, score-0.38]
</p><p>98 1 + e−xy 1 + e(1−xy) The inverse function for y = 0 and y + z = 0 is given by z 1 L′−1 (z, y) = − log − y y+z 1562  L ARGE S CALE MKL  and ﬁnally we obtain αi αi − ∑ log − yi + αi i=1 yi  αi Sk (α) = ∑ log 1 − yi + αi i=1  N  N  1 + 2  N  2  ∑ αi Φk (xi )  i=1  . [sent-1102, score-0.285]
</p><p>99 Multiple kernel learning, conic duality, and the SMO algorithm. [sent-1115, score-0.229]
</p><p>100 The spectrum kernel: A string kernel for SVM protein classiﬁcation. [sent-1222, score-0.306]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mkl', 0.648), ('wd', 0.266), ('linadd', 0.263), ('kernel', 0.185), ('ch', 0.165), ('splice', 0.148), ('onnenburg', 0.143), ('tsch', 0.141), ('wk', 0.134), ('chunking', 0.128), ('sk', 0.109), ('arge', 0.103), ('cale', 0.103), ('silp', 0.101), ('fer', 0.098), ('lookup', 0.095), ('yi', 0.095), ('cy', 0.088), ('xy', 0.087), ('kernels', 0.085), ('sonnenburg', 0.085), ('spectrum', 0.078), ('svm', 0.077), ('kk', 0.073), ('tk', 0.072), ('cpus', 0.067), ('weighting', 0.067), ('million', 0.065), ('width', 0.065), ('site', 0.063), ('spec', 0.054), ('gi', 0.054), ('roc', 0.052), ('wrapper', 0.048), ('cyi', 0.048), ('widths', 0.048), ('ww', 0.048), ('svmlight', 0.048), ('bach', 0.048), ('caching', 0.048), ('conic', 0.044), ('string', 0.043), ('sine', 0.042), ('precomputed', 0.042), ('parallelization', 0.04), ('rdk', 0.04), ('elegans', 0.04), ('prc', 0.04), ('dual', 0.04), ('old', 0.039), ('xi', 0.038), ('sequences', 0.037), ('lanckriet', 0.037), ('speedup', 0.037), ('lkopf', 0.036), ('trie', 0.034), ('parallelized', 0.034), ('stars', 0.033), ('vu', 0.033), ('exon', 0.032), ('slave', 0.032), ('stk', 0.032), ('loss', 0.031), ('area', 0.031), ('arrays', 0.03), ('composite', 0.03), ('svms', 0.03), ('frequency', 0.03), ('operations', 0.029), ('curve', 0.029), ('precision', 0.029), ('speedups', 0.028), ('cache', 0.028), ('lp', 0.027), ('precompute', 0.027), ('recomputation', 0.027), ('classi', 0.027), ('primal', 0.027), ('displayed', 0.026), ('cone', 0.026), ('tries', 0.026), ('training', 0.024), ('substrings', 0.024), ('auprc', 0.024), ('cye', 0.024), ('davis', 0.024), ('iq', 0.024), ('positional', 0.024), ('qpsize', 0.024), ('shogun', 0.024), ('sites', 0.024), ('specpre', 0.024), ('wdpre', 0.024), ('master', 0.024), ('rbf', 0.024), ('weights', 0.023), ('sparse', 0.023), ('working', 0.022), ('fraunhofer', 0.022), ('dna', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="43-tfidf-1" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer, Bernhard Schölkopf</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun. Keywords: multiple kernel learning, string kernels, large scale optimization, support vector machines, support vector regression, column generation, semi-inﬁnite linear programming</p><p>2 0.11239772 <a title="43-tfidf-2" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>3 0.090000391 <a title="43-tfidf-3" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Luca Zanni, Thomas Serafini, Gaetano Zanghirati</p><p>Abstract: Parallel software for solving the quadratic program arising in training support vector machines for classiﬁcation problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-ofthe-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes. Keywords: support vector machines, large scale quadratic programs, decomposition techniques, gradient projection methods, parallel computation</p><p>4 0.088874541 <a title="43-tfidf-4" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>5 0.071591437 <a title="43-tfidf-5" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>6 0.069576733 <a title="43-tfidf-6" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.064831376 <a title="43-tfidf-7" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>8 0.062138811 <a title="43-tfidf-8" href="./jmlr-2006-Universal_Kernels.html">93 jmlr-2006-Universal Kernels</a></p>
<p>9 0.058508027 <a title="43-tfidf-9" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>10 0.054715231 <a title="43-tfidf-10" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.054459859 <a title="43-tfidf-11" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>12 0.053139567 <a title="43-tfidf-12" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>13 0.051846508 <a title="43-tfidf-13" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.050479032 <a title="43-tfidf-14" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>15 0.050128296 <a title="43-tfidf-15" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>16 0.049682438 <a title="43-tfidf-16" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>17 0.04844559 <a title="43-tfidf-17" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>18 0.04811471 <a title="43-tfidf-18" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.044560242 <a title="43-tfidf-19" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.043443866 <a title="43-tfidf-20" href="./jmlr-2006-Considering_Cost_Asymmetry_in_Learning_Classifiers.html">22 jmlr-2006-Considering Cost Asymmetry in Learning Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.222), (1, -0.153), (2, 0.117), (3, 0.094), (4, 0.126), (5, 0.028), (6, -0.046), (7, 0.023), (8, -0.052), (9, -0.088), (10, 0.136), (11, -0.045), (12, 0.071), (13, 0.016), (14, 0.058), (15, 0.028), (16, 0.028), (17, -0.014), (18, -0.052), (19, -0.08), (20, 0.125), (21, 0.015), (22, -0.155), (23, 0.125), (24, 0.081), (25, 0.119), (26, -0.038), (27, -0.052), (28, -0.043), (29, -0.146), (30, 0.03), (31, -0.066), (32, -0.036), (33, -0.026), (34, 0.067), (35, 0.003), (36, 0.002), (37, -0.06), (38, -0.028), (39, 0.042), (40, 0.095), (41, 0.065), (42, 0.335), (43, -0.037), (44, 0.069), (45, -0.089), (46, -0.025), (47, -0.144), (48, 0.054), (49, -0.277)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91331333 <a title="43-lsi-1" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer, Bernhard Schölkopf</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun. Keywords: multiple kernel learning, string kernels, large scale optimization, support vector machines, support vector regression, column generation, semi-inﬁnite linear programming</p><p>2 0.53353161 <a title="43-lsi-2" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Luca Zanni, Thomas Serafini, Gaetano Zanghirati</p><p>Abstract: Parallel software for solving the quadratic program arising in training support vector machines for classiﬁcation problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-ofthe-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes. Keywords: support vector machines, large scale quadratic programs, decomposition techniques, gradient projection methods, parallel computation</p><p>3 0.43056741 <a title="43-lsi-3" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>4 0.39935309 <a title="43-lsi-4" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>Author: Andrea Passerini, Paolo Frasconi, Luc De Raedt</p><p>Abstract: We develop kernels for measuring the similarity between relational instances using background knowledge expressed in ﬁrst-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are ﬁrst used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then deﬁned over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classiﬁcation as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difﬁcult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets. Keywords: kernel methods, inductive logic programming, Prolog, learning from program traces</p><p>5 0.37988397 <a title="43-lsi-5" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: S. Sathiya Keerthi, Olivier Chapelle, Dennis DeCoste</p><p>Abstract: Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classiﬁcation speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily ﬁnds a set of kernel basis functions of a speciﬁed maximum size (dmax ) to approximate the SVM primal cost function well; (3) it is 2 efﬁcient and roughly scales as O(ndmax ) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors. Keywords: SVMs, classiﬁcation, sparse design</p><p>6 0.36696094 <a title="43-lsi-6" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.36648974 <a title="43-lsi-7" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>8 0.35351238 <a title="43-lsi-8" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.34411561 <a title="43-lsi-9" href="./jmlr-2006-Universal_Kernels.html">93 jmlr-2006-Universal Kernels</a></p>
<p>10 0.33860055 <a title="43-lsi-10" href="./jmlr-2006-Considering_Cost_Asymmetry_in_Learning_Classifiers.html">22 jmlr-2006-Considering Cost Asymmetry in Learning Classifiers</a></p>
<p>11 0.33294323 <a title="43-lsi-11" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>12 0.3293364 <a title="43-lsi-12" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.30760115 <a title="43-lsi-13" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.29435521 <a title="43-lsi-14" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>15 0.28176874 <a title="43-lsi-15" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>16 0.26578969 <a title="43-lsi-16" href="./jmlr-2006-On_Representing_and_Generating_Kernels_by_Fuzzy_Equivalence_Relations.html">67 jmlr-2006-On Representing and Generating Kernels by Fuzzy Equivalence Relations</a></p>
<p>17 0.26263896 <a title="43-lsi-17" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>18 0.24211571 <a title="43-lsi-18" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>19 0.23983808 <a title="43-lsi-19" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>20 0.22170079 <a title="43-lsi-20" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.022), (35, 0.012), (36, 0.046), (45, 0.011), (50, 0.038), (63, 0.053), (76, 0.028), (78, 0.029), (81, 0.036), (84, 0.024), (90, 0.033), (91, 0.482), (96, 0.081), (99, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92711121 <a title="43-lda-1" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><p>same-paper 2 0.84687555 <a title="43-lda-2" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer, Bernhard Schölkopf</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun. Keywords: multiple kernel learning, string kernels, large scale optimization, support vector machines, support vector regression, column generation, semi-inﬁnite linear programming</p><p>3 0.5093767 <a title="43-lda-3" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>Author: Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We present a family of margin based online learning algorithms for various prediction tasks. In particular we derive and analyze algorithms for binary and multiclass categorization, regression, uniclass prediction and sequence prediction. The update steps of our different algorithms are all based on analytical solutions to simple constrained optimization problems. This uniﬁed view allows us to prove worst-case loss bounds for the different algorithms and for the various decision problems based on a single lemma. Our bounds on the cumulative loss of the algorithms are relative to the smallest loss that can be attained by any ﬁxed hypothesis, and as such are applicable to both realizable and unrealizable settings. We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real data sets.</p><p>4 0.45891827 <a title="43-lda-4" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>5 0.45532793 <a title="43-lda-5" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Anders Bergkvist, Peter Damaschke,  Marcel Lüthi</p><p>Abstract: We consider an optimization problem in probabilistic inference: Given n hypotheses H j , m possible observations Ok , their conditional probabilities pk j , and a particular Ok , select a possibly small subset of hypotheses excluding the true target only with some error probability ε. After specifying the optimization goal we show that this problem can be solved through a linear program in mn variables that indicate the probabilities to discard a hypothesis given an observation. Moreover, we can compute optimal strategies where only O(m + n) of these variables get fractional values. The manageable size of the linear programs and the mostly deterministic shape of optimal strategies makes the method practicable. We interpret the dual variables as worst-case distributions of hypotheses, and we point out some counterintuitive nonmonotonic behaviour of the variables as a function of the error bound ε. One of the open problems is the existence of a purely combinatorial algorithm that is faster than generic linear programming. Keywords: probabilistic inference, error probability, linear programming, cycle-free graphs, network ﬂows</p><p>6 0.44799513 <a title="43-lda-6" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.44070849 <a title="43-lda-7" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.43627 <a title="43-lda-8" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.41761813 <a title="43-lda-9" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>10 0.41713327 <a title="43-lda-10" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<p>11 0.41712445 <a title="43-lda-11" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.40988648 <a title="43-lda-12" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>13 0.40643537 <a title="43-lda-13" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>14 0.40484118 <a title="43-lda-14" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.40446436 <a title="43-lda-15" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.39180455 <a title="43-lda-16" href="./jmlr-2006-Superior_Guarantees_for_Sequential_Prediction_and_Lossless_Compression_via_Alphabet_Decomposition.html">90 jmlr-2006-Superior Guarantees for Sequential Prediction and Lossless Compression via Alphabet Decomposition</a></p>
<p>17 0.38887712 <a title="43-lda-17" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>18 0.3871353 <a title="43-lda-18" href="./jmlr-2006-Causal_Graph_Based_Decomposition_of_Factored_MDPs.html">19 jmlr-2006-Causal Graph Based Decomposition of Factored MDPs</a></p>
<p>19 0.38649851 <a title="43-lda-19" href="./jmlr-2006-Point-Based_Value_Iteration_for_Continuous_POMDPs.html">74 jmlr-2006-Point-Based Value Iteration for Continuous POMDPs</a></p>
<p>20 0.38257885 <a title="43-lda-20" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
