<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-31" href="#">jmlr2006-31</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-31-pdf" href="http://jmlr.org/papers/volume7/mangasarian06a/mangasarian06a.pdf">pdf</a></p><p>Author: Olvi L. Mangasarian</p><p>Abstract: Support vector machines utilizing the 1-norm, typically set up as linear programs (Mangasarian, 2000; Bradley and Mangasarian, 1998), are formulated here as a completely unconstrained minimization of a convex differentiable piecewise-quadratic objective function in the dual space. The objective function, which has a Lipschitz continuous gradient and contains only one additional ﬁnite parameter, can be minimized by a generalized Newton method and leads to an exact solution of the support vector machine problem. The approach here is based on a formulation of a very general linear program as an unconstrained minimization problem and its application to support vector machine classiﬁcation problems. The present approach which generalizes both (Mangasarian, 2004) and (Fung and Mangasarian, 2004) is also applied to nonlinear approximation where a minimal number of nonlinear kernel functions are utilized to approximate a function from a given number of function values.</p><p>Reference: <a title="jmlr-2006-31-reference" href="../jmlr2006_reference/jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The objective function, which has a Lipschitz continuous gradient and contains only one additional ﬁnite parameter, can be minimized by a generalized Newton method and leads to an exact solution of the support vector machine problem. [sent-6, score-0.152]
</p><p>2 The approach here is based on a formulation of a very general linear program as an unconstrained minimization problem and its application to support vector machine classiﬁcation problems. [sent-7, score-0.464]
</p><p>3 The present approach which generalizes both (Mangasarian, 2004) and (Fung and Mangasarian, 2004) is also applied to nonlinear approximation where a minimal number of nonlinear kernel functions are utilized to approximate a function from a given number of function values. [sent-8, score-0.298]
</p><p>4 , 2004) which treats the 1norm SVM uses standard linear programming packages for solving their formulation. [sent-13, score-0.058]
</p><p>5 To the best of our knowledge there has not been an exact completely unconstrained differentiable minimization formulation of 1-norm SVMs, which is the principal concern of the present rather theoretical contribution which we outline now. [sent-14, score-0.432]
</p><p>6 In Section 2 we show how a very general linear program can be solved as the minimization of a completely unconstrained differentiable piecewise-quadratic convex function that contains a single ﬁnite parameter. [sent-15, score-0.519]
</p><p>7 This result generalizes (Mangasarian, 2004) where linear programs with millions of constraints were solved as unconstrained minimization problems by a generalized Newton method. [sent-16, score-0.471]
</p><p>8 In Section 3 we show how to set up 1-norm SVMs, with linear and nonlinear kernels as unconstrained minimization problems and state a generalized Newton method for their solution. [sent-17, score-0.502]
</p><p>9 In Section 4 we show how to solve the problem of approximating an unknown function based on a given number of function values using a minimal number of kernel functions. [sent-18, score-0.06]
</p><p>10 M ANGASARIAN  by again converting a 1-norm approximation problem to an unconstrained minimization problem. [sent-23, score-0.34]
</p><p>11 However, one can deﬁne its generalized Hessian (Hiriart-Urruty et al. [sent-39, score-0.051]
</p><p>12 The generalized Hessian has many of the properties of the regular Hessian (Hiriart-Urruty et al. [sent-44, score-0.051]
</p><p>13 A separating plane, with respect to two given point sets A and B in Rn , is a plane that attempts to separate Rn into two halfspaces such that each open halfspace contains points mostly of A or B . [sent-47, score-0.078]
</p><p>14 Linear Programs as Exact Unconstrained Differentiable Minimization Problems We consider in this section a very general linear program (LP) that contains nonnegative and unrestricted variables as well as inequality and equality constraints. [sent-50, score-0.124]
</p><p>15 We will show how to obtain an exact solution of this LP by a single minimization of a completely unconstrained differentiable piecewise-quadratic function that contains a single ﬁnite parameter. [sent-51, score-0.471]
</p><p>16 We begin with the primal linear program: min c′ x + d ′ y s. [sent-52, score-0.09]
</p><p>17 (2)  The exterior penalty problem for the dual linear program is: min  (u,v)∈Rm+k  1 ε(−b′ u − h′ v) + ( (A′ u + E ′ v − c)+ 2 1518  2  + B′ u + G′ v − d  2  + (−u)+ 2 ). [sent-57, score-0.521]
</p><p>18 (3)  SVM-1 AS U NCONSTRAINED M INIMIZATION  Solving the exterior penalty problem for a positive sequence {εi } converging to zero will yield a solution to the dual linear program (2) (Fiacco and McCormick, 1968; Bertsekas, 1999). [sent-58, score-0.56]
</p><p>19 However, we will not do that here because of the inherent inaccuracies associated with asymptotic exterior penalty methods and the fact that this would merely yield an approximate dual solution but not a primal solution. [sent-59, score-0.494]
</p><p>20 Instead, we will solve the exterior penalty problem for some ﬁnite value of the penalty parameter ε and from this inexact dual solution we shall easily extract an exact primal solution by using the following proposition. [sent-60, score-0.69]
</p><p>21 Proposition 1 Exact Primal Solution Computation Let the primal LP (1) be solvable. [sent-61, score-0.058]
</p><p>22 Then the ¯ ¯ dual exterior penalty problem (3) is solvable for all ε > 0. [sent-62, score-0.479]
</p><p>23 For any ε ∈ (0, ε] for some ε > 0, any solution (u, v) of (3) generates an exact solution to primal LP (1) as follows: 1 1 x = (A′ u + E ′ v − c)+ , y = (B′ u + G′ v − d). [sent-63, score-0.199]
</p><p>24 ε ε  (4)  In addition, this (x, y) minimizes: x  2  2  + y  + Ax + By − b 2 ,  (5)  over the solution set of the primal LP (1). [sent-64, score-0.097]
</p><p>25 Proof The dual exterior penalty minimization problem (3) can be written in the equivalent form: min  (u,v,z1 ,z2  )∈Rm+k+n+m  1 ε(−b′ u − h′ v) + ( z1 2 + B′ u + G′ v − d 2 + z2 2 ) 2 s. [sent-65, score-0.49]
</p><p>26 (6)  The justiﬁcation for this is that at a minimum of (6) the variables z1 and z2 are nonnegative, else if any component of these variables is negative the objective function can be strictly decreased by setting that component to zero while maintaining constraint feasibility. [sent-68, score-0.025]
</p><p>27 Hence, at a solution of (6), z1 = (A′ u + E ′ v − c)+ and z2 = (−u)+ . [sent-69, score-0.039]
</p><p>28 2) for the convex quadratic program (6) is: 1 − (( z1 2 (u,v,z1 ,z2 ,r,s)∈Rm+k+n+m+n+m max  2  + B′ u  2  + G′ v  2  + 2v′ GB′ u − d  2  + z2 2 ) − c′ r  s. [sent-72, score-0.121]
</p><p>29 1519  (8)  M ANGASARIAN  Note that at a solution of the exterior penalty problem (6) and the corresponding Wolfe dual (7) we have that: r = z1 = (A′ u + E ′ v − c)+ (9) s = z2 = (−u)+ . [sent-77, score-0.436]
</p><p>30 (11)  This convex quadratic program (11) is feasible, because the linear program (1) is feasible. [sent-82, score-0.245]
</p><p>31 It is solvable for any ε > 0 (Frank and Wolfe, 1956) because its objective function is bounded below since it is a strongly convex quadratic function in (x, y). [sent-83, score-0.136]
</p><p>32 Since the dual exterior penalty minimization problem objective (3) or equivalently (6) is bounded below by the negative of the objective function of (11) by the weak duality theorem (Mangasarian, 1994, Theorem 8. [sent-84, score-0.54]
</p><p>33 We do not give that proof here because it does not justify how the quadratic perturbation terms of (11) arose, but it merely starts with these terms as given. [sent-89, score-0.084]
</p><p>34 1-Norm SVMs as Unconstrained Minimization Problems We consider ﬁrst the 1-norm linear SVM binary classiﬁcation problem (Mangasarian, 2000; Bradley and Mangasarian, 1998; Fung and Mangasarian, 2004): min  (w,γ,y)  ν y  1+  w  1  s. [sent-92, score-0.032]
</p><p>35 The objective term y 1 minimizes the classiﬁcation error weighted with the positive parameter ν while the term w 1 maximizes the ∞-norm margin (Mangasarian, 1999) between the bounding planes x′ w = γ ± 1 that approximately bound each of the two classes of points represented by A. [sent-95, score-0.051]
</p><p>36 We convert (12) to an explicit linear program as in (Fung and Mangasarian, 2004) by setting: w = p − q, p ≥ 0, q ≥ 0, (14) which results in the linear program: min  (p,q,γ,y)  νe′ y + e′ (p + q) (15)  s. [sent-97, score-0.156]
</p><p>37 We note immediately that this linear program is solvable because it is feasible and its objective function is bounded below by zero. [sent-100, score-0.231]
</p><p>38 Hence, Proposition 1 can be utilized to yield the following unconstrained reformulation of the problem. [sent-101, score-0.367]
</p><p>39 Proposition 2 Exact 1-Norm SVM Solution via Unconstrained Minimization The unconstrained dual exterior penalty problem for the 1-norm SVM (15): 1 min − εe′ u + ( (A′ Du − e)+ m u∈R 2  2  + (−A′ Du − e)+  2  + (−e′ Du)2 + (u − νe)+  2  + (−u)+ 2 ),  (16) ¯ ¯ is solvable for all ε > 0. [sent-102, score-0.726]
</p><p>40 For any ε ∈ (0, ε] for some ε > 0, any solution u of (16) generates an exact solution of the 1-norm SVM classiﬁcation problem (12) as follows: w = p − q = = 1 ((A′ Du − e)+ − (−A′ Du − e)+ ), ε γ = − 1 e′ Du, ε y = 1 (u − νe)+ . [sent-103, score-0.141]
</p><p>41 ε  (17)  In addition this (w, γ, y) minimizes: w  2  + γ2 + y  2  + D(Aw − eγ) + y − e 2 ,  (18)  over the solution set of the 1-norm SVM classiﬁcation problem (12). [sent-104, score-0.039]
</p><p>42 We note here the similarity between our unconstrained penalty minimization problem (16) and the corresponding problem of (Fung and Mangasarian, 2004, Equation 23). [sent-105, score-0.437]
</p><p>43 In the latter, a penalty parameter α multiplies the term (−u)+ 2 of equation (16) above and is required to approach ∞ in order to obtain an exact solution to the original problem (12). [sent-107, score-0.173]
</p><p>44 Thus, the solution obtained by (Fung and Mangasarian, 2004, Equation 23) for any ﬁnite α is only approximate, as pointed out there. [sent-108, score-0.039]
</p><p>45 Furthermore, our solution to (16) here minimizes the expression (18) rather than being merely an approximate least 2-norm solution as is the case in (Fung and 1521  M ANGASARIAN  Mangasarian, 2004, Equation 11). [sent-109, score-0.104]
</p><p>46 However the generalized Newton method prescribed in (Fung and Mangasarian, 2004) for a sequence {α ↑ ∞}, is applicable here with α = 1. [sent-110, score-0.051]
</p><p>47 To do that we let f (u) denote the exterior penalty function (16). [sent-112, score-0.325]
</p><p>48 Then the gradient and generalized Hessian as deﬁned in the Introduction are given as follows. [sent-113, score-0.051]
</p><p>49 For a linear kernel K(A, A′ ) = AA′ , we have that w = A′ Dv, where v is a dual variable (Mangasarian, 2000) and the primal linear programming SVM (15) becomes upon using w = p − q = A′ Dv and minimizing the 1-norm of v in the objective instead that of w: min  (v,γ,y)  s. [sent-117, score-0.305]
</p><p>50 (24)  Setting: v = r − s, r ≥ 0, s ≥ 0,  (25)  the linear program (24) becomes: min  (r,s,γ,y)  νe′ y + e′ (r + s)  s. [sent-120, score-0.124]
</p><p>51 D(AA′ D(r − s) − eγ) + y ≥ e r, s, y ≥ 0,  (26)  which is the linear kernel SVM in terms of the dual variable v = r − s. [sent-122, score-0.164]
</p><p>52 If we replace the linear kernel AA′ in (26) by the nonlinear kernel K(A, A′ ) we obtain the nonlinear kernel linear program: min  (r,s,γ,y)  νe′ y + e′ (r + s)  s. [sent-123, score-0.402]
</p><p>53 1522  (27)  SVM-1 AS U NCONSTRAINED M INIMIZATION  We immediately note that the linear program (15) is identical to the linear program (27) if we make the replacement (23). [sent-126, score-0.277]
</p><p>54 This result can be implemented computationally by using an ε, which when decreased by some factor yields the same solution to (1) or (12). [sent-131, score-0.039]
</p><p>55 We state now our generalized Newton algorithm for solving the unconstrained minimization problem (16) as follows. [sent-133, score-0.391]
</p><p>56 Set the parameter values ν, ε, δ, tolerance tol, and imax (typically: ε ∈ [10−6 , 4 × 10−4 ] for linear SVMs and ε ∈ [10−9 , 1] nonlinear SVMs, tol = 10−3 , imax = 50, while ν and δ are set by a tuning procedure). [sent-135, score-0.264]
</p><p>57 : (I) ui+1 = ui − λi (∂2 f (ui ) + δI)−1 ∇ f (ui ) = ui + λi d i , where the Armijo stepsize λi = max{1, 1 , 1 , . [sent-140, score-0.319]
</p><p>58 (III) Deﬁne the solution of the 1-norm SVM (12) with least quadratic perturbation (18) by (17) with u = ui . [sent-147, score-0.269]
</p><p>59 Proposition 4 Let tol = 0, imax = ∞ and let ε > 0 be sufﬁciently small. [sent-149, score-0.109]
</p><p>60 Each accumulation point u of the sequence {ui } generated by Algorithm 3 solves the exterior penalty problem (16). [sent-150, score-0.35]
</p><p>61 The ¯ corresponding (w, γ, y) obtained by setting u to u in (17) is an exact solution to the primal 1-norm ¯ ¯ ¯ ¯ SVM (12) which in addition minimizes the quadratic perturbation (18) over the solution set of (12). [sent-151, score-0.283]
</p><p>62 Proof That each accumulation point u of the sequence {ui } solves the minimization problem (13) ¯ follows from exterior penalty results (Fiacco and McCormick, 1968; Bertsekas, 1999) and standard unconstrained descent methods such as (Mangasarian, 1995, Theorem 2. [sent-152, score-0.69]
</p><p>63 2(iv)) and the facts that the direction choice d i of (24) satisﬁes, for some c > 0: −∇ f (ui )′ d i = ∇ f (ui )′ (δI + ∂2 f (ui ))−1 ∇ f (ui ) ≥ c ∇ f (ui ) 2 ,  (30)  and that we are using an Armijo stepsize (28). [sent-155, score-0.027]
</p><p>64 Utilizing the 1-norm in minimizing the kernel weights suppresses unnecessary kernel functions similar to the approach of (Mangasarian et al. [sent-160, score-0.12]
</p><p>65 , 2004) except that we shall solve the resulting linear program here through an unconstrained minimization reformulation. [sent-161, score-0.487]
</p><p>66 Also, for simplicity we shall not incorporate prior knowledge as was done in (Mangasarian et al. [sent-162, score-0.023]
</p><p>67 (32)  Setting v = r − s, r ≥ 0, s ≥ 0, = y − z, y ≥ 0, z ≥ 0,  K(A, A′ )v + eγ − b  (33)  we obtain the following linear program: min  (r,s,γ,y,z)  νe′ (y + z) + e′ (r + s)  s. [sent-166, score-0.032]
</p><p>68 K(A, A′ )(r − s) + eγ − y + z = b r, s, y, z ≥ 0,  (34)  which is similar to the nonlinear kernel SVM classiﬁer linear programming formulation (27) with equality constraints replacing inequality constraints. [sent-168, score-0.197]
</p><p>69 We also note that this linear program is solvable because it is feasible and its objective function is bounded below by zero. [sent-169, score-0.231]
</p><p>70 Hence, Proposition 1 can be utilized to yield the following unconstrained reformulation of the problem. [sent-170, score-0.367]
</p><p>71 Computational results utilizing the linear programming formulation (32) with prior knowledge in (Mangasarian et al. [sent-173, score-0.1]
</p><p>72 , 2004) but using the simplex method of solution is effective for solving approximation problems. [sent-174, score-0.039]
</p><p>73 The unconstrained minimization formulation (35) is another method of solution which can also handle such problems without prior knowledge as well as with prior knowledge with appropriate but straightforward modiﬁcations. [sent-175, score-0.379]
</p><p>74 Computational Results Computational testing was carried on a 3 Ghz Pentium 4 machine with 2GB of memory running CentOS 4 Linux and utilizing the CPLEX 7. [sent-178, score-0.042]
</p><p>75 We tested our algorithm on six publicly available data sets. [sent-181, score-0.023]
</p><p>76 For the linear classiﬁer (13) we compare in Table 1, NLPSVM (Fung and Mangasarian, 2004), CPLEX (ILO, 2003) and our Generalized LPNewton Algorithm for (16), on six public data sets using ten-fold cross validation. [sent-186, score-0.105]
</p><p>77 NLPSVM is essentially identical to our algorithm, except that it requires a penalty parameter multiplying the last term of (16) to approach inﬁnity. [sent-187, score-0.097]
</p><p>78 CPLEX uses the standard linear programming package CPLEX (ILO, 2003) to solve (26). [sent-188, score-0.058]
</p><p>79 We note that our method LPNewton is faster than both NLPSVM and CPLEX on all six data sets and gives the best feature suppression based on the average number of features used by the linear classiﬁer (13). [sent-189, score-0.113]
</p><p>80 NLPSVM has the best test set correctness on two of the data sets, and comparable correctness on the other four. [sent-190, score-0.058]
</p><p>81 For the nonlinear classiﬁer (22) we compare in Table 2, NLPSVM (Fung and Mangasarian, 2004), CPLEX (ILO, 2003) and our Generalized LPNewton Algorithm 3 for (27), on three public data sets using ten-fold cross validation. [sent-203, score-0.129]
</p><p>82 We note again that our method LPNewton is faster than both NLPSVM and CPLEX on all three data sets and gives the best reduction in the number of kernel functions utilized, on two of the data sets, based on the cardinality of v = r − s as deﬁned in (25) and (27). [sent-204, score-0.06]
</p><p>83 Best test set correctness was achieved on two data sets by our method and it was a close second on the third data set. [sent-205, score-0.029]
</p><p>84 Tuning and choice of the parameters ν and ε was done as for the linear classiﬁer above. [sent-207, score-0.032]
</p><p>85 A Gaussian kernel was used for all three methods and data sets with the Gaussian parameter tuned from the set {2−12 , . [sent-208, score-0.06]
</p><p>86 2  1e-6  4e-4 1e-6  4e-4 1e-6  4e-4 1e-6  4e-4 1e-6  4e-4 1e-6  Table 1: Comparison of the Linear Classiﬁer (13) obtained by NLPSVM (Fung and Mangasarian, 2004), CPLEX (ILO, 2003) and our Generalized LPNewton Algorithm 3 for (16) on six public data sets. [sent-293, score-0.073]
</p><p>87 Time is for one fold in seconds, Train and Test corectness is the average over ten folds and Features (Feat) denote the average number over ten folds of input space features utilized by the linear classiﬁer. [sent-294, score-0.15]
</p><p>88 6  Eps 4e-4 1e-6  4e-4 1e+0  4e-4 1e-9  Table 2: Comparison of the Nonlinear Classiﬁer (22) obtained by NLPSVM (Fung and Mangasarian, 2004), CPLEX (ILO, 2003) and our Generalized LPNewton Algorithm 3 for (27) on three public data sets. [sent-340, score-0.05]
</p><p>89 Time for one fold is in seconds, Train and Test corectness is on ten folds. [sent-341, score-0.038]
</p><p>90 Card(v) denotes the average number of nonzero components of v = r − s as deﬁned in (25) and (27) and hence that is the number of kernel functions utilized by the nonlinear classiﬁer (22). [sent-342, score-0.219]
</p><p>91 Epsilon (Eps) is the ﬁnite parameter deﬁned in (16) with the replacement (23) of A by K(A, A′ )D. [sent-343, score-0.029]
</p><p>92 Reduced SVM (RSVM) (Lee and Mangasarian, 2001) was used to speed all computations by using the reduced m ¯ ¯ kernel K(A, A′ ) where 10 randomly chosen rows of A constitute the rows of rows of A. [sent-345, score-0.06]
</p><p>93 Conclusion and Outlook We have derived an unconstrained differentiable convex minimization reformulation of a most general linear program and have applied it to 1-norm classiﬁcation and approximation problems. [sent-349, score-0.559]
</p><p>94 Feature selection via concave minimization and support vector machines. [sent-366, score-0.093]
</p><p>95 Generalized hessian matrix and second-order optimality conditions for problems with CL1 data. [sent-427, score-0.053]
</p><p>96 A new result in the theory and computation of the least-norm solution of a linear program. [sent-449, score-0.071]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mangasarian', 0.372), ('nlpsvm', 0.365), ('lpnewton', 0.346), ('unconstrained', 0.247), ('exterior', 0.228), ('cplex', 0.21), ('fung', 0.199), ('ftp', 0.147), ('ui', 0.146), ('angasarian', 0.135), ('ilo', 0.135), ('nconstrained', 0.115), ('bupa', 0.114), ('cleveland', 0.114), ('du', 0.11), ('rm', 0.106), ('ax', 0.105), ('penalty', 0.097), ('newton', 0.095), ('minimization', 0.093), ('program', 0.092), ('inimization', 0.088), ('ionosphere', 0.082), ('solvable', 0.082), ('utilized', 0.08), ('nonlinear', 0.079), ('armijo', 0.077), ('galaxy', 0.077), ('rn', 0.075), ('dual', 0.072), ('svm', 0.069), ('bradley', 0.067), ('indians', 0.065), ('tol', 0.065), ('kernel', 0.06), ('eps', 0.058), ('dv', 0.058), ('primal', 0.058), ('fiacco', 0.058), ('suppression', 0.058), ('perturbation', 0.055), ('lp', 0.055), ('differentiable', 0.055), ('pima', 0.054), ('liver', 0.054), ('hessian', 0.053), ('generalized', 0.051), ('aa', 0.05), ('public', 0.05), ('da', 0.05), ('dee', 0.049), ('feat', 0.049), ('olvi', 0.049), ('programs', 0.048), ('diag', 0.047), ('dim', 0.044), ('imax', 0.044), ('svms', 0.042), ('utilizing', 0.042), ('separating', 0.041), ('epsilon', 0.04), ('housing', 0.04), ('reformulation', 0.04), ('proposition', 0.039), ('solution', 0.039), ('corectness', 0.038), ('facchinei', 0.038), ('iters', 0.038), ('odewahn', 0.038), ('wolfe', 0.037), ('exact', 0.037), ('plane', 0.037), ('frank', 0.033), ('ilog', 0.033), ('mccormick', 0.033), ('linear', 0.032), ('classi', 0.032), ('rsvm', 0.029), ('px', 0.029), ('card', 0.029), ('quadratic', 0.029), ('replacement', 0.029), ('correctness', 0.029), ('wisconsin', 0.029), ('lipschitz', 0.027), ('stepsize', 0.027), ('gb', 0.027), ('minimizes', 0.026), ('rk', 0.026), ('er', 0.026), ('programming', 0.026), ('generates', 0.026), ('objective', 0.025), ('aw', 0.025), ('accumulation', 0.025), ('shavlik', 0.025), ('six', 0.023), ('shall', 0.023), ('zhu', 0.023), ('madison', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="31-tfidf-1" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Olvi L. Mangasarian</p><p>Abstract: Support vector machines utilizing the 1-norm, typically set up as linear programs (Mangasarian, 2000; Bradley and Mangasarian, 1998), are formulated here as a completely unconstrained minimization of a convex differentiable piecewise-quadratic objective function in the dual space. The objective function, which has a Lipschitz continuous gradient and contains only one additional ﬁnite parameter, can be minimized by a generalized Newton method and leads to an exact solution of the support vector machine problem. The approach here is based on a formulation of a very general linear program as an unconstrained minimization problem and its application to support vector machine classiﬁcation problems. The present approach which generalizes both (Mangasarian, 2004) and (Fung and Mangasarian, 2004) is also applied to nonlinear approximation where a minimal number of nonlinear kernel functions are utilized to approximate a function from a given number of function values.</p><p>2 0.14926283 <a title="31-tfidf-2" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>3 0.067840464 <a title="31-tfidf-3" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Chen Yanover, Talya Meltzer, Yair Weiss</p><p>Abstract: The problem of ﬁnding the most probable (MAP) conﬁguration in graphical models comes up in a wide range of applications. In a general graphical model this problem is NP hard, but various approximate algorithms have been developed. Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for ﬁnding the most probable assignment in small graphical models. However, applying this powerful method to real-world problems is extremely challenging due to the large numbers of variables and constraints in the linear program. Tree-Reweighted Belief Propagation is a promising recent algorithm for solving LP relaxations, but little is known about its running time on large problems. In this paper we compare tree-reweighted belief propagation (TRBP) and powerful generalpurpose LP solvers (CPLEX) on relaxations of real-world graphical models from the ﬁelds of computer vision and computational biology. We ﬁnd that TRBP almost always ﬁnds the solution signiﬁcantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. Using TRBP we can ﬁnd the MAP conﬁgurations in a matter of minutes for a large range of real world problems.</p><p>4 0.059696298 <a title="31-tfidf-4" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: S. Sathiya Keerthi, Olivier Chapelle, Dennis DeCoste</p><p>Abstract: Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classiﬁcation speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily ﬁnds a set of kernel basis functions of a speciﬁed maximum size (dmax ) to approximate the SVM primal cost function well; (3) it is 2 efﬁcient and roughly scales as O(ndmax ) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors. Keywords: SVMs, classiﬁcation, sparse design</p><p>5 0.056569982 <a title="31-tfidf-5" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><p>6 0.052373603 <a title="31-tfidf-6" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>7 0.045567591 <a title="31-tfidf-7" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>8 0.04539489 <a title="31-tfidf-8" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.044560242 <a title="31-tfidf-9" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.043765105 <a title="31-tfidf-10" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.043499202 <a title="31-tfidf-11" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.040806994 <a title="31-tfidf-12" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>13 0.039818943 <a title="31-tfidf-13" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.037421662 <a title="31-tfidf-14" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>15 0.034977231 <a title="31-tfidf-15" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>16 0.034629013 <a title="31-tfidf-16" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>17 0.034617715 <a title="31-tfidf-17" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>18 0.033759173 <a title="31-tfidf-18" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>19 0.033658091 <a title="31-tfidf-19" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.032010376 <a title="31-tfidf-20" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.181), (1, -0.145), (2, 0.085), (3, 0.1), (4, 0.12), (5, -0.008), (6, -0.031), (7, 0.158), (8, 0.084), (9, -0.008), (10, -0.077), (11, -0.123), (12, -0.024), (13, 0.019), (14, -0.06), (15, -0.013), (16, -0.05), (17, -0.055), (18, -0.034), (19, -0.093), (20, 0.004), (21, -0.057), (22, -0.108), (23, -0.008), (24, 0.046), (25, -0.024), (26, -0.152), (27, 0.123), (28, -0.015), (29, -0.155), (30, -0.064), (31, 0.033), (32, 0.087), (33, -0.049), (34, -0.126), (35, -0.254), (36, 0.083), (37, -0.175), (38, -0.096), (39, -0.119), (40, 0.004), (41, 0.196), (42, -0.289), (43, 0.085), (44, -0.023), (45, 0.202), (46, 0.073), (47, 0.054), (48, -0.227), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95142919 <a title="31-lsi-1" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Olvi L. Mangasarian</p><p>Abstract: Support vector machines utilizing the 1-norm, typically set up as linear programs (Mangasarian, 2000; Bradley and Mangasarian, 1998), are formulated here as a completely unconstrained minimization of a convex differentiable piecewise-quadratic objective function in the dual space. The objective function, which has a Lipschitz continuous gradient and contains only one additional ﬁnite parameter, can be minimized by a generalized Newton method and leads to an exact solution of the support vector machine problem. The approach here is based on a formulation of a very general linear program as an unconstrained minimization problem and its application to support vector machine classiﬁcation problems. The present approach which generalizes both (Mangasarian, 2004) and (Fung and Mangasarian, 2004) is also applied to nonlinear approximation where a minimal number of nonlinear kernel functions are utilized to approximate a function from a given number of function values.</p><p>2 0.46344522 <a title="31-lsi-2" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>3 0.43595558 <a title="31-lsi-3" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Chen Yanover, Talya Meltzer, Yair Weiss</p><p>Abstract: The problem of ﬁnding the most probable (MAP) conﬁguration in graphical models comes up in a wide range of applications. In a general graphical model this problem is NP hard, but various approximate algorithms have been developed. Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for ﬁnding the most probable assignment in small graphical models. However, applying this powerful method to real-world problems is extremely challenging due to the large numbers of variables and constraints in the linear program. Tree-Reweighted Belief Propagation is a promising recent algorithm for solving LP relaxations, but little is known about its running time on large problems. In this paper we compare tree-reweighted belief propagation (TRBP) and powerful generalpurpose LP solvers (CPLEX) on relaxations of real-world graphical models from the ﬁelds of computer vision and computational biology. We ﬁnd that TRBP almost always ﬁnds the solution signiﬁcantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. Using TRBP we can ﬁnd the MAP conﬁgurations in a matter of minutes for a large range of real world problems.</p><p>4 0.36467394 <a title="31-lsi-4" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: S. Sathiya Keerthi, Olivier Chapelle, Dennis DeCoste</p><p>Abstract: Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classiﬁcation speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily ﬁnds a set of kernel basis functions of a speciﬁed maximum size (dmax ) to approximate the SVM primal cost function well; (3) it is 2 efﬁcient and roughly scales as O(ndmax ) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors. Keywords: SVMs, classiﬁcation, sparse design</p><p>5 0.25018439 <a title="31-lsi-5" href="./jmlr-2006-Learning_Minimum_Volume_Sets.html">48 jmlr-2006-Learning Minimum Volume Sets</a></p>
<p>Author: Clayton D. Scott, Robert D. Nowak</p><p>Abstract: Given a probability measure P and a reference measure µ, one is often interested in the minimum µ-measure set with P-measure at least α. Minimum volume sets of this type summarize the regions of greatest probability mass of P, and are useful for detecting anomalies and constructing conﬁdence regions. This paper addresses the problem of estimating minimum volume sets based on independent samples distributed according to P. Other than these samples, no other information is available regarding P, but the reference measure µ is assumed to be known. We introduce rules for estimating minimum volume sets that parallel the empirical risk minimization and structural risk minimization principles in classiﬁcation. As in classiﬁcation, we show that the performances of our estimators are controlled by the rate of uniform convergence of empirical to true probabilities over the class from which the estimator is drawn. Thus we obtain ﬁnite sample size performance bounds in terms of VC dimension and related quantities. We also demonstrate strong universal consistency, an oracle inequality, and rates of convergence. The proposed estimators are illustrated with histogram and decision tree set estimation rules. Keywords: minimum volume sets, anomaly detection, statistical learning theory, uniform deviation bounds, sample complexity, universal consistency</p><p>6 0.23653534 <a title="31-lsi-6" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.23081286 <a title="31-lsi-7" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>8 0.22436415 <a title="31-lsi-8" href="./jmlr-2006-Segmental_Hidden_Markov_Models_with_Random_Effects_for_Waveform_Modeling.html">80 jmlr-2006-Segmental Hidden Markov Models with Random Effects for Waveform Modeling</a></p>
<p>9 0.22202823 <a title="31-lsi-9" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>10 0.2172325 <a title="31-lsi-10" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.19701491 <a title="31-lsi-11" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.18192036 <a title="31-lsi-12" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>13 0.17791495 <a title="31-lsi-13" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.17572036 <a title="31-lsi-14" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>15 0.16471967 <a title="31-lsi-15" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>16 0.16001038 <a title="31-lsi-16" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>17 0.15517415 <a title="31-lsi-17" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>18 0.15048331 <a title="31-lsi-18" href="./jmlr-2006-Action_Elimination_and_Stopping_Conditions_for_the_Multi-Armed_Bandit_and_Reinforcement_Learning_Problems.html">10 jmlr-2006-Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems</a></p>
<p>19 0.14932232 <a title="31-lsi-19" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.14846431 <a title="31-lsi-20" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.014), (35, 0.555), (36, 0.034), (45, 0.028), (50, 0.032), (63, 0.034), (78, 0.022), (81, 0.036), (84, 0.015), (90, 0.037), (91, 0.036), (96, 0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83729798 <a title="31-lda-1" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Olvi L. Mangasarian</p><p>Abstract: Support vector machines utilizing the 1-norm, typically set up as linear programs (Mangasarian, 2000; Bradley and Mangasarian, 1998), are formulated here as a completely unconstrained minimization of a convex differentiable piecewise-quadratic objective function in the dual space. The objective function, which has a Lipschitz continuous gradient and contains only one additional ﬁnite parameter, can be minimized by a generalized Newton method and leads to an exact solution of the support vector machine problem. The approach here is based on a formulation of a very general linear program as an unconstrained minimization problem and its application to support vector machine classiﬁcation problems. The present approach which generalizes both (Mangasarian, 2004) and (Fung and Mangasarian, 2004) is also applied to nonlinear approximation where a minimal number of nonlinear kernel functions are utilized to approximate a function from a given number of function values.</p><p>2 0.66104907 <a title="31-lda-2" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>Author: Jing Zhou, Dean P. Foster, Robert A. Stine, Lyle H. Ungar</p><p>Abstract: In streamwise feature selection, new features are sequentially considered for addition to a predictive model. When the space of potential features is large, streamwise feature selection offers many advantages over traditional feature selection methods, which assume that all features are known in advance. Features can be generated dynamically, focusing the search for new features on promising subspaces, and overﬁtting can be controlled by dynamically adjusting the threshold for adding features to the model. In contrast to traditional forward feature selection algorithms such as stepwise regression in which at each step all possible features are evaluated and the best one is selected, streamwise feature selection only evaluates each feature once when it is generated. We describe information-investing and α-investing, two adaptive complexity penalty methods for streamwise feature selection which dynamically adjust the threshold on the error reduction required for adding a new feature. These two methods give false discovery rate style guarantees against overﬁtting. They differ from standard penalty methods such as AIC, BIC and RIC, which always drastically over- or under-ﬁt in the limit of inﬁnite numbers of non-predictive features. Empirical results show that streamwise regression is competitive with (on small data sets) and superior to (on large data sets) much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with millions of potential features. Keywords: classiﬁcation, stepwise regression, multiple regression, feature selection, false discovery rate</p><p>3 0.23240495 <a title="31-lda-3" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Matthias Heiler, Christoph Schnörr</p><p>Abstract: We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. We show that for ordinary NMF, our approach performs as well as existing stateof-the-art algorithms, while for sparsity-constrained NMF, as recently proposed by P. O. Hoyer in JMLR 5 (2004), it outperforms previous methods. In addition, we show how to extend NMF learning within the same optimization framework in order to make use of class membership information in supervised learning problems. Keywords: non-negative matrix factorization, second-order cone programming, sequential convex optimization, reverse-convex programming, sparsity</p><p>4 0.21710148 <a title="31-lda-4" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>Author: Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou</p><p>Abstract: We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the ﬁrst time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction. html. Keywords: transduction, transductive SVMs, semi-supervised learning, CCCP</p><p>5 0.21619046 <a title="31-lda-5" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>Author: Greg Hamerly, Erez Perelman, Jeremy Lau, Brad Calder, Timothy Sherwood</p><p>Abstract: An essential step in designing a new computer architecture is the careful examination of different design options. It is critical that computer architects have efﬁcient means by which they may estimate the impact of various design options on the overall machine. This task is complicated by the fact that different programs, and even different parts of the same program, may have distinct behaviors that interact with the hardware in different ways. Researchers use very detailed simulators to estimate processor performance, which models every cycle of an executing program. Unfortunately, simulating every cycle of a real program can take weeks or months. To address this problem we have created a tool called SimPoint that uses data clustering algorithms from machine learning to automatically ﬁnd repetitive patterns in a program’s execution. By simulating one representative of each repetitive behavior pattern, simulation time can be reduced to minutes instead of weeks for standard benchmark programs, with very little cost in terms of accuracy. We describe this important problem, the data representation and preprocessing methods used by SimPoint, the clustering algorithm at the core of SimPoint, and we evaluate different options for tuning SimPoint. Keywords: k-means, random projection, Bayesian information criterion, simulation, SimPoint</p><p>6 0.21580672 <a title="31-lda-6" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.21465893 <a title="31-lda-7" href="./jmlr-2006-Generalized_Bradley-Terry_Models_and_Multi-Class_Probability_Estimates.html">34 jmlr-2006-Generalized Bradley-Terry Models and Multi-Class Probability Estimates</a></p>
<p>8 0.21034265 <a title="31-lda-8" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>9 0.20781279 <a title="31-lda-9" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.20503274 <a title="31-lda-10" href="./jmlr-2006-Geometric_Variance_Reduction_in_Markov_Chains%3A_Application_to_Value_Function_and_Gradient_Estimation.html">35 jmlr-2006-Geometric Variance Reduction in Markov Chains: Application to Value Function and Gradient Estimation</a></p>
<p>11 0.20165746 <a title="31-lda-11" href="./jmlr-2006-MinReg%3A_A_Scalable_Algorithm_for_Learning_Parsimonious_Regulatory_Networks_in_Yeast_and_Mammals.html">62 jmlr-2006-MinReg: A Scalable Algorithm for Learning Parsimonious Regulatory Networks in Yeast and Mammals</a></p>
<p>12 0.20010619 <a title="31-lda-12" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.19798124 <a title="31-lda-13" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.19524069 <a title="31-lda-14" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.19324467 <a title="31-lda-15" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.18704706 <a title="31-lda-16" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>17 0.18580389 <a title="31-lda-17" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>18 0.18340921 <a title="31-lda-18" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.18331328 <a title="31-lda-19" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>20 0.18149704 <a title="31-lda-20" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
