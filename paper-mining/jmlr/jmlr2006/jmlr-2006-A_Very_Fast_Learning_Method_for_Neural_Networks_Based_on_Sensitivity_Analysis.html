<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 jmlr-2006-A Very Fast Learning Method for Neural Networks Based on Sensitivity Analysis</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-8" href="#">jmlr2006-8</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>8 jmlr-2006-A Very Fast Learning Method for Neural Networks Based on Sensitivity Analysis</h1>
<br/><p>Source: <a title="jmlr-2006-8-pdf" href="http://jmlr.org/papers/volume7/castillo06a/castillo06a.pdf">pdf</a></p><p>Author: Enrique Castillo, Bertha Guijarro-Berdiñas, Oscar Fontenla-Romero, Amparo Alonso-Betanzos</p><p>Abstract: This paper introduces a learning method for two-layer feedforward neural networks based on sensitivity analysis, which uses a linear training algorithm for each of the two layers. First, random values are assigned to the outputs of the ﬁrst layer; later, these initial values are updated based on sensitivity formulas, which use the weights in each of the layers; the process is repeated until convergence. Since these weights are learnt solving a linear system of equations, there is an important saving in computational time. The method also gives the local sensitivities of the least square errors with respect to input and output data, with no extra computational cost, because the necessary information becomes available without extra calculations. This method, called the Sensitivity-Based Linear Learning Method, can also be used to provide an initial set of weights, which signiﬁcantly improves the behavior of other learning algorithms. The theoretical basis for the method is given and its performance is illustrated by its application to several examples in which it is compared with several learning algorithms and well known data sets. The results have shown a learning speed generally faster than other existing methods. In addition, it can be used as an initialization tool for other well known methods with signiﬁcant improvements. Keywords: supervised learning, neural networks, linear optimization, least-squares, initialization method, sensitivity analysis</p><p>Reference: <a title="jmlr-2006-8-reference" href="../jmlr2006_reference/jmlr-2006-A_Very_Fast_Learning_Method_for_Neural_Networks_Based_on_Sensitivity_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 First, random values are assigned to the outputs of the ﬁrst layer; later, these initial values are updated based on sensitivity formulas, which use the weights in each of the layers; the process is repeated until convergence. [sent-6, score-0.093]
</p><p>2 The method also gives the local sensitivities of the least square errors with respect to input and output data, with no extra computational cost, because the necessary information becomes available without extra calculations. [sent-8, score-0.084]
</p><p>3 In addition, it can be used as an initialization tool for other well known methods with signiﬁcant improvements. [sent-12, score-0.066]
</p><p>4 Keywords: supervised learning, neural networks, linear optimization, least-squares, initialization method, sensitivity analysis  1. [sent-13, score-0.121]
</p><p>5 In the case of feedforward multilayer networks the ﬁrst successful algorithm was the classical backpropagation (Rumelhart et al. [sent-15, score-0.146]
</p><p>6 Although this approach is very useful for the learning process of this kind of neural networks it has two main drawbacks: • Convergence to local minima. [sent-17, score-0.059]
</p><p>7 Sperduti and Antonina (1993) extend the backpropagation framework by adding a gradient descent to the sigmoids steepness parameters. [sent-22, score-0.108]
</p><p>8 Also, stochastic backpropagation—which is opposite to batch learning and updates the weights in each iteration—often decreases the convergence time, and is specially recommended when dealing with large data sets on classiﬁcation problems (see LeCun et al. [sent-25, score-0.05]
</p><p>9 • Methods based on linear least-squares: Some algorithms based on linear least-squares methods have been proposed to initialize or train feedforward neural networks (Biegler-K¨ nig and o B¨ rmann, 1993; Pethel et al. [sent-27, score-0.124]
</p><p>10 These methods are mostly based on minimizing the mean squared error (MSE) between the signal of an output neuron, before the output nonlinearity, and a modiﬁed desired output, which is exactly the actual desired output passed through the inverse of the nonlinearity. [sent-32, score-0.049]
</p><p>11 , 2002) a method for learning a single layer neural network by solving a linear system of equations is proposed. [sent-34, score-0.102]
</p><p>12 , 2003) to learn the last layer of a neural network, while the rest of the layers are updated employing any other non-linear algorithm (for example, conjugate gradient). [sent-36, score-0.118]
</p><p>13 , 1991) that these methods are more efﬁcient, in terms of learning speed, than the methods based only on the gradient descent technique. [sent-41, score-0.066]
</p><p>14 Some of the most relevant examples of this type of methods are the quasi-Newton, Levenberg-Marquardt (Hagan and Menhaj, 1994; Levenberg, 1944; Marquardt, 1963) and the conjugate gradient algorithms (Beale, 1972). [sent-43, score-0.073]
</p><p>15 Regarding the conjugate gradient methods, they use, at each iteration of the algorithm, different search directions in a way that the component of the gradient is parallel to the previous search direction. [sent-48, score-0.116]
</p><p>16 , 1996), Powelle Beale (Powell, 1977) and scaled conjugate gradient algorithms (Moller, 1993). [sent-51, score-0.073]
</p><p>17 Nevertheless, second-order methods are not practicable for large neural networks trained in batch mode, although some attempts to reduce their computational cost or to obtain stochastic versions have appeared (LeCun et al. [sent-55, score-0.077]
</p><p>18 • Adaptive step size: In the standard backpropagation method the learning rate, which determines the magnitude of the changes in the weights for each iteration of the algorithm, is ﬁxed at the beginning of the learning process. [sent-57, score-0.074]
</p><p>19 This method is an adaptive acceleration strategy for error backpropagation learning that converges faster than the gradient descent with optimal step size value, reducing the sensitivity to parameter values. [sent-61, score-0.143]
</p><p>20 More recently, in Orr and Leen (1996), an algorithm for fast stochastic gradient descent, which uses a nonlinear adaptive momentum scheme to optimize the slow convergence rate was proposed. [sent-63, score-0.085]
</p><p>21 (1999), a new method for step size adaptation in stochastic gradient optimization was presented. [sent-65, score-0.084]
</p><p>22 Thus, several solutions for the appropriate initialization of weights have been proposed. [sent-69, score-0.098]
</p><p>23 , 1997), an analytical technique, to initialize the weights of a multilayer perceptron with vector quantization (VQ) prototypes given the equivalence between circular backpropagation networks and VQ classiﬁers, has been proposed. [sent-72, score-0.113]
</p><p>24 However, in this paper we show that sensitivity formulas can also be used for learning, and a novel supervised learning algorithm for two-layer feedforward neural networks that presents a high convergence speed is proposed. [sent-79, score-0.189]
</p><p>25 In addition, this algorithm gives the sensitivities of the sum of squared errors with respect to the input and output data. [sent-81, score-0.103]
</p><p>26 In Section 2 a method for learning one layer neural networks that consists of solving a system of linear equations is presented, and formulas for the sensitivities of the sum of squared errors with respect to the input and output data are derived. [sent-83, score-0.225]
</p><p>27 In Section 5 the SBLLM method is presented as an initialization tool to be used with other learning methods. [sent-86, score-0.066]
</p><p>28 The set of equations relating inputs and outputs is given by I  y js = f j  ∑ w ji xis  ; j = 1, 2, . [sent-91, score-0.284]
</p><p>29 , S,  i=0  where I is the number of inputs, J the number of outputs, x0s = 1, w ji are the weights associated with neuron j and S is the number of data points. [sent-97, score-0.096]
</p><p>30 w J2 w 1I  +  w 2I  fJ  y JS  w JI x IS  Figure 1: One-layer feedforward neural network. [sent-104, score-0.085]
</p><p>31 To learn the weights w ji , the following sum of squared errors between the real and the desired output of the networks is usually minimized: 1162  A V ERY FAST L EARNING M ETHOD FOR N EURAL N ETWORKS BASED ON S ENSITIVITY A NALYSIS  J  S  ∑∑  P=  δ2 js  =  ∑∑  y js − f j  ∑ w ji xis  . [sent-105, score-0.471]
</p><p>32 i=0  s=1 j=1  s=1 j=1  2  I  J  S  Assuming that the nonlinear activation functions, f j , are invertible (as it is the case for the most commonly employed functions), alternatively, one can minimize the sum of squared errors before the nonlinear activation functions (Castillo et al. [sent-106, score-0.077]
</p><p>33 , 2002), that is,  S  J  S  Q = ∑ ∑ ε2 = js  ∑ w ji xis − f j−1 (y js )  ,  (1)  i=0  s=1 j=1  s=1 j=1  2  I  J  ∑ ∑  which leads to the system of equations: S ∂Q =2∑ ∂w jp s=1  I  ∑ w ji xis − f j−1 (y js )  x ps = 0;  p = 0, 1, . [sent-107, score-0.614]
</p><p>34 , I;  ∀ j,  i=0  that is, I  S  S  ∑ w ji ∑ xis x ps  i=0  ∑ f j−1 (y js )x ps ;  =  p = 0, 1, . [sent-110, score-0.302]
</p><p>35 , I;  ∀ j,  (2)  i=0  where S  ∑ xis x ps ;  A pi =  p = 0, 1, . [sent-116, score-0.13]
</p><p>36 , I;  ∀i  s=1 S  ∑ f j−1 (y js )x ps ;  bp j =  p = 0, 1, . [sent-119, score-0.134]
</p><p>37 s=1  Moreover, for the neural network shown in Figure 1, the sensitivities (see Castillo et al. [sent-123, score-0.123]
</p><p>38 , 2001, 2004, 2006) of the new cost function, Q, with respect to the output and input data can be obtained as: I  2  ∂Q ∂y pq  = −  ∂Q ∂x pq  = 2∑  −1 ∑ w pi xiq − f p (y pq )  i=0  ′ f p (y pq ) J  j=1  ;  ∀p, q  (3)  I  ∑ w ji xiq − f j−1 (y jq )  i=0  1163  w jp ;  ∀p, q. [sent-124, score-0.122]
</p><p>39 The Proposed Sensitivity-Based Linear Learning Method The learning method and the sensitivity formulas given in the previous section can be used to develop a new learning method for two-layer feedforward neural networks, as it is described below. [sent-126, score-0.12]
</p><p>40 Consider the two-layer feedforward neural network in Figure 2 where I is the number of inputs, J the number of outputs, K the number of hidden units, x0s = 1, z0s = 1, S the number of data samples and the superscripts (1) and (2) are used to refer to the ﬁrst and second layer, respectively. [sent-127, score-0.104]
</p><p>41 (1)  Thus, using the outputs zks we can learn, for each layer independently, the weights wki and (2) w jk by solving the corresponding linear system of equations (2). [sent-130, score-0.338]
</p><p>42 After that, the sensitivities (see equations (3) and (4)) with respect to zks are calculated as: ∂Q ∂zks  =  ∂Q(1) ∂Q(2) + = ∂zks ∂zks I  2 =−  (1)−1  (1)  ∑ wki xis − fk  (zks )  i=0  ′ (1)  fk (zks )  J  +2 ∑  j=1  K  ∑ w jr zrs − f j  (2)−1  (2)  (2)  (y js ) w jk  r=0  with k = 1, . [sent-131, score-0.587]
</p><p>43 z 0s  x 0s x 1s  x Is  w ki (1)  f1  f2  (1)  (1)  fK  (1)  z 1s  w jk (2)  z 2s  f1  z Ks  fJ  (2)  y 1s  (2)  y Js  Figure 2: Two-layer feedforward neural network. [sent-135, score-0.104]
</p><p>44 The data set (input, xis , and desired data, y js ), two threshold errors (ε and ε′ ) to control convergence, and a step size ρ . [sent-139, score-0.202]
</p><p>45 The weights of the two layers and the sensitivities of the sum of squared errors with respect to input and output data. [sent-141, score-0.158]
</p><p>46 Assign to the outputs of the intermediate layer the output associated with some random weights w(1) (0) plus a small random error, that is: I  ∑ wki  (1)  zks = fk  (1)  (0)xis + εks ;  εks ∼ U(−η, η); k = 1, . [sent-143, score-0.355]
</p><p>47 , K  s=1  (2)  S  (2)−1  bq j = ∑ f j  (y js )zqs ;  q = 0, 1, . [sent-154, score-0.103]
</p><p>48 Evaluate Q using Q(z)  = Q(1) (z) + Q(2) (z)  S  =  K  I  ∑  ∑ ∑ wki  s=1  k=1  (1)  2 (1)−1  xis − fk  (zks )  i=0  J  +∑  j=1  and evaluate also the MSE. [sent-159, score-0.185]
</p><p>49 1165  K  2  ∑ w jk zks − f j  k=0  (2)  (2)−1  (y js )     ˜ C ASTILLO , G UIJARRO -B ERDI NAS , F ONTENLA -ROMERO AND A LONSO -B ETANZOS  Step 3: Convergence checking. [sent-160, score-0.267]
</p><p>50 Otherwise, store the values of Q and z, that is, Q previous = Q, MSE previous = MSE and z previous = z and obtain the sensitivities using: I  ∂Q =− ∂zks  2  (1)−1  (1)  ∑ wki xis − fk  i=0  ′ (1)  fk (zks )  (zks )  K  J  +2 ∑  j=1  ∑ w jr zrs − f j (2)  (2)−1  (2)  (y js ) w jk ; k = 1, . [sent-165, score-0.424]
</p><p>51 Using the Taylor series approximation in equation (5), update the intermediate outputs as z = z−ρ  Q(z) ∇Q ||∇Q||2  and go to Step 1. [sent-170, score-0.065]
</p><p>52 Two of them are small/medium size problems (Dow-Jones and Leuven competition time series), while the other three used large data sets and networks (Lorenz time series, and the MNIST and UCI Forest databases). [sent-176, score-0.074]
</p><p>53 Three of these methods are the gradient descent (GD), the gradient descent with adaptive momentum and step sizes (GDX), and the stochastic gradient descent (SGD), whose complexity is O(n). [sent-178, score-0.216]
</p><p>54 For each experiment all the learning methods shared the following conditions: • The network topology and neural functions. [sent-181, score-0.067]
</p><p>55 In all cases, the logistic function was used for hidden neurons, while for output neurons the linear function was used for regression problems and the logistic function was used for classiﬁcation problems. [sent-182, score-0.046]
</p><p>56 This initial set was the same for all the algorithms (except for the SBLLM), and was obtained by the Nguyen-Widrow (Nguyen and Widrow, 1990) initialization method. [sent-198, score-0.066]
</p><p>57 For this data set a 5-7-1 topology (5 inputs, 7 hidden neurons and 1 output neuron) was used. [sent-207, score-0.074]
</p><p>58 In order to obtain the MSE curves during the learning process 100 simulations of 3000 iterations each, were done. [sent-209, score-0.098]
</p><p>59 • M2 : Mean epoch and corresponding standard deviation in which each of the other methods reaches the minimum MSE obtained by the SBLLM. [sent-216, score-0.083]
</p><p>60 • M3 : MSE and standard deviation for each of the other methods at the epoch in which the SBLLM gets its minimum. [sent-217, score-0.083]
</p><p>61 The Ratio column contains the relation between the ttotalmean of each algorithm and the fastest one. [sent-224, score-0.114]
</p><p>62 Again, the multiple comparison test applied over the ttotalmean revealed that the speed of the fastest method, that is the SBLLM, was only comparable to that of the SCG. [sent-225, score-0.144]
</p><p>63 01 SBLLM  −4  10  0  10  1  2  10  10 Epoch  3  0  4  10  10  GD  (a) Mean error curves over 100 simulations  SCG  GDX  LM  SBLLM  SGD  (b) Boxplot of the 100 MSE values obtained at the end of the training  Figure 3: Results of the learning process for the Dow-Jones data. [sent-232, score-0.098]
</p><p>64 356 × 10−2  (∗1) 4% of the curves did not get the minimum of SBLLM (∗2) 99. [sent-260, score-0.06]
</p><p>65 5% of the curves did not get the minimum of SBLLM  Table 1: Comparative measures for the Dow-Jones data. [sent-261, score-0.06]
</p><p>66 Leuven time series prediction competition data (Suykens and Vandewalle, 1998) were generated from a computer simulated 5-scroll attractor, resulting from a generalized Chua’s circuit which is a paradigm for chaos. [sent-297, score-0.053]
</p><p>67 Regarding the ttotalmean , the multiple comparison test showed that the speed of the fastest method, that is the SBLLM, was only comparable to those of the GDX and the GD. [sent-309, score-0.144]
</p><p>68 8% of the curves did not get the minimum of SBLLM  Table 3: Comparative measures for the Leuven competition data. [sent-340, score-0.095]
</p><p>69 1169  ˜ C ASTILLO , G UIJARRO -B ERDI NAS , F ONTENLA -ROMERO AND A LONSO -B ETANZOS  GDX GD SBLLM SCG LM SGD  tepochmean 0. [sent-341, score-0.046]
</p><p>70 In this case, and due to the large size of both the data set and the neural networks, the conditions of the experiments were the following:  • The number of simulations, which were carried out to obtain the MSE curves during the learning process was reduced to 30, of 1000 iterations each. [sent-376, score-0.08]
</p><p>71 For this data set we used 784 inputs neurons fed by the 28 × 28 pixels of each input pattern, and one output neuron per class. [sent-431, score-0.072]
</p><p>72 Regarding the total CPU time, again the fastest method is the SBLLM, with a ttotalmean signiﬁcantly different from the other two methods. [sent-447, score-0.114]
</p><p>73 Regarding the total CPU time, the fastest method is the SBLLM, with a ttotalmean signiﬁcantly different from the other two methods. [sent-501, score-0.114]
</p><p>74 In this section, the results of the SBLLM used as an initialization method instead of as a learning algorithm are presented. [sent-548, score-0.066]
</p><p>75 Thus, several experiments were accomplished using the SBLLM only to get the initial values of the weights of the neural network. [sent-549, score-0.052]
</p><p>76 Moreover, in order to accomplish a comparative study, the obtained results were confronted with the ones achieved by the same learning methods (LM and SCG) but using the Nguyen-Widrow (NW) initialization method (Nguyen and Widrow, 1990), one of the most popular, to obtain the initial weights. [sent-555, score-0.086]
</p><p>77 1173  ˜ C ASTILLO , G UIJARRO -B ERDI NAS , F ONTENLA -ROMERO AND A LONSO -B ETANZOS  Figures 6(a), 7(a) and 8(a) show the corresponding mean curves (over the 100 simulations) of the learning process using the SBLLM and the NW as initialization methods and the LM as the learning algorithm. [sent-556, score-0.156]
</p><p>78 Figures 6(b), 7(b) and 8(b) show the same mean curves of the learning process using this time the SCG as learning algorithm. [sent-557, score-0.09]
</p><p>79 Figures 9(a), 10(a) and 11(a) contain the boxplots of the methods in the last epoch of training (3000) using the SBLLM and NW as initialization methods and the LM as the learning algorithm. [sent-561, score-0.165]
</p><p>80 7  NW + LM  SBLLM + LM  NW + SCG  (a) Boxplot for the LM method  SBLLM + SCG  (b) Boxplot for the SCG method  Figure 9: Boxplot of the 100 MSE values at the last epoch of training for the Dow-Jones time series using the SBLLM and NW as initialization methods. [sent-573, score-0.167]
</p><p>81 2 NW + LM  SBLLM + LM  NW + SCG  (a) Boxplot for the LM method  SBLLM + SCG  (b) Boxplot for the SCG method  Figure 10: Boxplot of the 100 MSE values at the last epoch of training for the Leuven competition time series using the SBLLM and NW as initialization methods. [sent-590, score-0.202]
</p><p>82 5  0  0  NW + LM  SBLLM + LM  NW + SCG  (a) Boxplot for the LM method  SBLLM + SCG  (b) Boxplot for the SCG method  Figure 11: Boxplot of the 100 MSE values at the last epoch of training for the Lorenz time series using the SBLLM and NW as initialization methods. [sent-595, score-0.167]
</p><p>83 Moreover, and generally speaking, measure M3 reﬂects that the SBLLM gets its minimum in an epoch for which the other algorithms are far from similar MSE values. [sent-603, score-0.083]
</p><p>84 It is also important to remark that despite of the advantages of the LM method, it could not be applied in the experiments that involved large data sets and neural networks as it is impractical for such cases (LeCun et al. [sent-610, score-0.059]
</p><p>85 This behavior could be explained by the initialization method and the updating rule for the step size employed. [sent-629, score-0.066]
</p><p>86 • The SBLLM behaves homogeneously not only if we consider just the end of the learning process, as commented, but also during the whole process, in such a way that very similar learning curves where obtained for all iterations of the ﬁrst three experiments. [sent-636, score-0.06]
</p><p>87 With respect to the use of the SBLLM as initialization method, as it can be observed in Figures 6, 7 and 8, the SBLLM combined with the LM or the SCG achieves a faster convergence speed than the same methods using the NW as initialization method. [sent-641, score-0.162]
</p><p>88 Moreover, in this case, most of the times the ﬁnal MSE achieved is smaller than the one obtained using the NW initialization method. [sent-643, score-0.066]
</p><p>89 Thus, experiments conﬁrm the utility of the SBLLM as an initialization method, which effect is to speed up the convergence. [sent-645, score-0.096]
</p><p>90 The sensitivities of the sum of squared errors with respect to the outputs of the intermediate layer allow an efﬁcient and fast gradient method to be applied. [sent-648, score-0.262]
</p><p>91 Besides, the stochastic gradient (SGD) is the one that obtains the lowest classiﬁcation error. [sent-655, score-0.061]
</p><p>92 The SBLLM used as an initialization method signiﬁcantly improves the performance of a learning algorithm. [sent-659, score-0.066]
</p><p>93 A more appropriate method to set the initial values of the outputs z of hidden neurons (step 0 of the proposed algorithm). [sent-661, score-0.072]
</p><p>94 A learning algorithm for multilayered neural networks based on o a linear least-squares problems. [sent-695, score-0.059]
</p><p>95 Supervised learning for feed-forward neural networks: a new minimax approach for fast convergence. [sent-768, score-0.044]
</p><p>96 Acceleration of learning speed in neural networks by reducing weight oscillations. [sent-848, score-0.089]
</p><p>97 A scaled conjugate gradient algorithm for fast supervised learning. [sent-906, score-0.097]
</p><p>98 Improving the learning speed of 2-layer neural networks by choosing initial values of the adaptive weights. [sent-911, score-0.089]
</p><p>99 An algorithm for fast convergence in training neural networks. [sent-1014, score-0.044]
</p><p>100 A new method in determining the initial weights of feedforward neural networks. [sent-1023, score-0.117]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sbllm', 0.648), ('scg', 0.304), ('lm', 0.237), ('mse', 0.232), ('sgd', 0.198), ('nw', 0.183), ('zks', 0.145), ('castillo', 0.122), ('gdx', 0.122), ('js', 0.103), ('gd', 0.099), ('xis', 0.099), ('astillo', 0.091), ('boxplot', 0.091), ('erdi', 0.091), ('etanzos', 0.091), ('lonso', 0.091), ('nas', 0.091), ('ontenla', 0.091), ('uijarro', 0.091), ('eural', 0.084), ('lorenz', 0.084), ('sensitivities', 0.084), ('ttotalmean', 0.084), ('epoch', 0.083), ('ensitivity', 0.071), ('ery', 0.071), ('ethod', 0.071), ('std', 0.069), ('initialization', 0.066), ('feedforward', 0.065), ('leuven', 0.065), ('etworks', 0.064), ('curves', 0.06), ('nalysis', 0.058), ('wki', 0.053), ('neurons', 0.046), ('tepochmean', 0.046), ('layer', 0.045), ('gradient', 0.043), ('backpropagation', 0.042), ('lecun', 0.042), ('cpu', 0.04), ('networks', 0.039), ('simulations', 0.038), ('ji', 0.038), ('forest', 0.037), ('sensitivity', 0.035), ('competition', 0.035), ('fk', 0.033), ('weights', 0.032), ('hagan', 0.032), ('nguyen', 0.032), ('ps', 0.031), ('ridella', 0.03), ('tepochstd', 0.03), ('ttotalstd', 0.03), ('widrow', 0.03), ('mean', 0.03), ('fastest', 0.03), ('conjugate', 0.03), ('speed', 0.03), ('topology', 0.028), ('mnist', 0.028), ('outputs', 0.026), ('neuron', 0.026), ('tables', 0.026), ('earning', 0.025), ('fast', 0.024), ('layers', 0.023), ('descent', 0.023), ('conejo', 0.023), ('minmse', 0.023), ('rigler', 0.023), ('rrez', 0.023), ('udc', 0.023), ('adaptation', 0.023), ('figures', 0.022), ('pq', 0.021), ('orr', 0.021), ('intermediate', 0.021), ('regarding', 0.021), ('comparative', 0.02), ('activation', 0.02), ('neural', 0.02), ('guti', 0.019), ('drago', 0.019), ('ks', 0.019), ('network', 0.019), ('jk', 0.019), ('squared', 0.019), ('epochs', 0.019), ('equations', 0.018), ('stochastic', 0.018), ('series', 0.018), ('employed', 0.018), ('fletcher', 0.017), ('boxplots', 0.016), ('rescaling', 0.016), ('amparo', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="8-tfidf-1" href="./jmlr-2006-A_Very_Fast_Learning_Method_for_Neural_Networks_Based_on_Sensitivity_Analysis.html">8 jmlr-2006-A Very Fast Learning Method for Neural Networks Based on Sensitivity Analysis</a></p>
<p>Author: Enrique Castillo, Bertha Guijarro-Berdiñas, Oscar Fontenla-Romero, Amparo Alonso-Betanzos</p><p>Abstract: This paper introduces a learning method for two-layer feedforward neural networks based on sensitivity analysis, which uses a linear training algorithm for each of the two layers. First, random values are assigned to the outputs of the ﬁrst layer; later, these initial values are updated based on sensitivity formulas, which use the weights in each of the layers; the process is repeated until convergence. Since these weights are learnt solving a linear system of equations, there is an important saving in computational time. The method also gives the local sensitivities of the least square errors with respect to input and output data, with no extra computational cost, because the necessary information becomes available without extra calculations. This method, called the Sensitivity-Based Linear Learning Method, can also be used to provide an initial set of weights, which signiﬁcantly improves the behavior of other learning algorithms. The theoretical basis for the method is given and its performance is illustrated by its application to several examples in which it is compared with several learning algorithms and well known data sets. The results have shown a learning speed generally faster than other existing methods. In addition, it can be used as an initialization tool for other well known methods with signiﬁcant improvements. Keywords: supervised learning, neural networks, linear optimization, least-squares, initialization method, sensitivity analysis</p><p>2 0.037802506 <a title="8-tfidf-2" href="./jmlr-2006-Generalized_Bradley-Terry_Models_and_Multi-Class_Probability_Estimates.html">34 jmlr-2006-Generalized Bradley-Terry Models and Multi-Class Probability Estimates</a></p>
<p>Author: Tzu-Kuo Huang, Ruby C. Weng, Chih-Jen Lin</p><p>Abstract: The Bradley-Terry model for obtaining individual skill from paired comparisons has been popular in many areas. In machine learning, this model is related to multi-class probability estimates by coupling all pairwise classiﬁcation results. Error correcting output codes (ECOC) are a general framework to decompose a multi-class problem to several binary problems. To obtain probability estimates under this framework, this paper introduces a generalized Bradley-Terry model in which paired individual comparisons are extended to paired team comparisons. We propose a simple algorithm with convergence proofs to solve the model and obtain individual skill. Experiments on synthetic and real data demonstrate that the algorithm is useful for obtaining multi-class probability estimates. Moreover, we discuss four extensions of the proposed model: 1) weighted individual skill, 2) home-ﬁeld advantage, 3) ties, and 4) comparisons with more than two teams. Keywords: Bradley-Terry model, probability estimates, error correcting output codes, support vector machines</p><p>3 0.036133993 <a title="8-tfidf-3" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>Author: Martin J. Wainwright</p><p>Abstract: Consider the problem of joint parameter estimation and prediction in a Markov random ﬁeld: that is, the model parameters are estimated on the basis of an initial set of data, and then the ﬁtted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working under the restriction of limited computation, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for ﬁtting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the “wrong” model even in the inﬁnite data limit) is provably beneﬁcial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of convex variational methods. This stability result provides additional incentive, apart from the obvious beneﬁt of unique global optima, for using message-passing methods based on convex variational relaxations. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. Keywords: graphical model, Markov random ﬁeld, belief propagation, variational method, parameter estimation, prediction error, algorithmic stability</p><p>4 0.023929093 <a title="8-tfidf-4" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>5 0.023779387 <a title="8-tfidf-5" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Radu Stefan Niculescu, Tom M. Mitchell, R. Bharat Rao</p><p>Abstract: The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. This paper considers a variety of types of domain knowledge for constraining parameter estimates when learning Bayesian networks. In particular, we consider domain knowledge that constrains the values or relationships among subsets of parameters in a Bayesian network with known structure. We incorporate a wide variety of parameter constraints into learning procedures for Bayesian networks, by formulating this task as a constrained optimization problem. The assumptions made in module networks, dynamic Bayes nets and context speciﬁc independence models can be viewed as particular cases of such parameter constraints. We present closed form solutions or fast iterative algorithms for estimating parameters subject to several speciﬁc classes of parameter constraints, including equalities and inequalities among parameters, constraints on individual parameters, and constraints on sums and ratios of parameters, for discrete and continuous variables. Our methods cover learning from both frequentist and Bayesian points of view, from both complete and incomplete data. We present formal guarantees for our estimators, as well as methods for automatically learning useful parameter constraints from data. To validate our approach, we apply it to the domain of fMRI brain image analysis. Here we demonstrate the ability of our system to ﬁrst learn useful relationships among parameters, and then to use them to constrain the training of the Bayesian network, resulting in improved cross-validated accuracy of the learned model. Experiments on synthetic data are also presented. Keywords: Bayesian networks, constrained optimization, domain knowledge c 2006 Radu Stefan Niculescu, Tom Mitchell and Bharat Rao. N ICULESCU , M ITCHELL AND R AO</p><p>6 0.023391597 <a title="8-tfidf-6" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>7 0.022361817 <a title="8-tfidf-7" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.020991717 <a title="8-tfidf-8" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.01947286 <a title="8-tfidf-9" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>10 0.019242998 <a title="8-tfidf-10" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.018872537 <a title="8-tfidf-11" href="./jmlr-2006-Learning_Image_Components_for_Object_Recognition.html">47 jmlr-2006-Learning Image Components for Object Recognition</a></p>
<p>12 0.01866349 <a title="8-tfidf-12" href="./jmlr-2006-Policy_Gradient_in_Continuous_Time.html">75 jmlr-2006-Policy Gradient in Continuous Time</a></p>
<p>13 0.016238032 <a title="8-tfidf-13" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>14 0.015147234 <a title="8-tfidf-14" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.013875511 <a title="8-tfidf-15" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>16 0.013649447 <a title="8-tfidf-16" href="./jmlr-2006-A_Simulation-Based_Algorithm_for_Ergodic_Control_of_Markov_Chains_Conditioned_on_Rare_Events.html">7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</a></p>
<p>17 0.013466302 <a title="8-tfidf-17" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>18 0.013122114 <a title="8-tfidf-18" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.012877612 <a title="8-tfidf-19" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>20 0.012663729 <a title="8-tfidf-20" href="./jmlr-2006-Noisy-OR_Component_Analysis_and_its_Application_to_Link_Analysis.html">64 jmlr-2006-Noisy-OR Component Analysis and its Application to Link Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.079), (1, -0.005), (2, -0.014), (3, 0.027), (4, -0.012), (5, -0.007), (6, -0.03), (7, 0.014), (8, 0.057), (9, -0.047), (10, -0.005), (11, -0.004), (12, 0.025), (13, -0.048), (14, -0.03), (15, 0.028), (16, -0.019), (17, -0.006), (18, 0.087), (19, 0.111), (20, 0.042), (21, 0.046), (22, -0.011), (23, -0.014), (24, -0.005), (25, 0.047), (26, 0.088), (27, -0.084), (28, 0.008), (29, 0.201), (30, -0.259), (31, -0.138), (32, 0.122), (33, -0.17), (34, 0.089), (35, 0.302), (36, 0.23), (37, -0.212), (38, 0.278), (39, -0.545), (40, 0.012), (41, 0.109), (42, -0.012), (43, 0.181), (44, 0.2), (45, -0.141), (46, 0.083), (47, 0.105), (48, -0.034), (49, 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97229171 <a title="8-lsi-1" href="./jmlr-2006-A_Very_Fast_Learning_Method_for_Neural_Networks_Based_on_Sensitivity_Analysis.html">8 jmlr-2006-A Very Fast Learning Method for Neural Networks Based on Sensitivity Analysis</a></p>
<p>Author: Enrique Castillo, Bertha Guijarro-Berdiñas, Oscar Fontenla-Romero, Amparo Alonso-Betanzos</p><p>Abstract: This paper introduces a learning method for two-layer feedforward neural networks based on sensitivity analysis, which uses a linear training algorithm for each of the two layers. First, random values are assigned to the outputs of the ﬁrst layer; later, these initial values are updated based on sensitivity formulas, which use the weights in each of the layers; the process is repeated until convergence. Since these weights are learnt solving a linear system of equations, there is an important saving in computational time. The method also gives the local sensitivities of the least square errors with respect to input and output data, with no extra computational cost, because the necessary information becomes available without extra calculations. This method, called the Sensitivity-Based Linear Learning Method, can also be used to provide an initial set of weights, which signiﬁcantly improves the behavior of other learning algorithms. The theoretical basis for the method is given and its performance is illustrated by its application to several examples in which it is compared with several learning algorithms and well known data sets. The results have shown a learning speed generally faster than other existing methods. In addition, it can be used as an initialization tool for other well known methods with signiﬁcant improvements. Keywords: supervised learning, neural networks, linear optimization, least-squares, initialization method, sensitivity analysis</p><p>2 0.14655182 <a title="8-lsi-2" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Radu Stefan Niculescu, Tom M. Mitchell, R. Bharat Rao</p><p>Abstract: The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. This paper considers a variety of types of domain knowledge for constraining parameter estimates when learning Bayesian networks. In particular, we consider domain knowledge that constrains the values or relationships among subsets of parameters in a Bayesian network with known structure. We incorporate a wide variety of parameter constraints into learning procedures for Bayesian networks, by formulating this task as a constrained optimization problem. The assumptions made in module networks, dynamic Bayes nets and context speciﬁc independence models can be viewed as particular cases of such parameter constraints. We present closed form solutions or fast iterative algorithms for estimating parameters subject to several speciﬁc classes of parameter constraints, including equalities and inequalities among parameters, constraints on individual parameters, and constraints on sums and ratios of parameters, for discrete and continuous variables. Our methods cover learning from both frequentist and Bayesian points of view, from both complete and incomplete data. We present formal guarantees for our estimators, as well as methods for automatically learning useful parameter constraints from data. To validate our approach, we apply it to the domain of fMRI brain image analysis. Here we demonstrate the ability of our system to ﬁrst learn useful relationships among parameters, and then to use them to constrain the training of the Bayesian network, resulting in improved cross-validated accuracy of the learned model. Experiments on synthetic data are also presented. Keywords: Bayesian networks, constrained optimization, domain knowledge c 2006 Radu Stefan Niculescu, Tom Mitchell and Bharat Rao. N ICULESCU , M ITCHELL AND R AO</p><p>3 0.13350186 <a title="8-lsi-3" href="./jmlr-2006-Generalized_Bradley-Terry_Models_and_Multi-Class_Probability_Estimates.html">34 jmlr-2006-Generalized Bradley-Terry Models and Multi-Class Probability Estimates</a></p>
<p>Author: Tzu-Kuo Huang, Ruby C. Weng, Chih-Jen Lin</p><p>Abstract: The Bradley-Terry model for obtaining individual skill from paired comparisons has been popular in many areas. In machine learning, this model is related to multi-class probability estimates by coupling all pairwise classiﬁcation results. Error correcting output codes (ECOC) are a general framework to decompose a multi-class problem to several binary problems. To obtain probability estimates under this framework, this paper introduces a generalized Bradley-Terry model in which paired individual comparisons are extended to paired team comparisons. We propose a simple algorithm with convergence proofs to solve the model and obtain individual skill. Experiments on synthetic and real data demonstrate that the algorithm is useful for obtaining multi-class probability estimates. Moreover, we discuss four extensions of the proposed model: 1) weighted individual skill, 2) home-ﬁeld advantage, 3) ties, and 4) comparisons with more than two teams. Keywords: Bradley-Terry model, probability estimates, error correcting output codes, support vector machines</p><p>4 0.11957308 <a title="8-lsi-4" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>Author: S. V. N. Vishwanathan, Nicol N. Schraudolph, Alex J. Smola</p><p>Abstract: This paper presents an online support vector machine (SVM) that uses the stochastic meta-descent (SMD) algorithm to adapt its step size automatically. We formulate the online learning problem as a stochastic gradient descent in reproducing kernel Hilbert space (RKHS) and translate SMD to the nonparametric setting, where its gradient trace parameter is no longer a coefﬁcient vector but an element of the RKHS. We derive efﬁcient updates that allow us to perform the step size adaptation in linear time. We apply the online SVM framework to a variety of loss functions, and in particular show how to handle structured output spaces and achieve efﬁcient online multiclass classiﬁcation. Experiments show that our algorithm outperforms more primitive methods for setting the gradient step size. Keywords: online SVM, stochastic meta-descent, structured output spaces</p><p>5 0.11298583 <a title="8-lsi-5" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>Author: Peter Bühlmann, Bin Yu</p><p>Abstract: We propose Sparse Boosting (the SparseL2 Boost algorithm), a variant on boosting with the squared error loss. SparseL2 Boost yields sparser solutions than the previously proposed L2 Boosting by minimizing some penalized L2 -loss functions, the FPE model selection criteria, through smallstep gradient descent. Although boosting may give already relatively sparse solutions, for example corresponding to the soft-thresholding estimator in orthogonal linear models, there is sometimes a desire for more sparseness to increase prediction accuracy and ability for better variable selection: such goals can be achieved with SparseL2 Boost. We prove an equivalence of SparseL2 Boost to Breiman’s nonnegative garrote estimator for orthogonal linear models and demonstrate the generic nature of SparseL2 Boost for nonparametric interaction modeling. For an automatic selection of the tuning parameter in SparseL2 Boost we propose to employ the gMDL model selection criterion which can also be used for early stopping of L2 Boosting. Consequently, we can select between SparseL2 Boost and L2 Boosting by comparing their gMDL scores. Keywords: lasso, minimum description length (MDL), model selection, nonnegative garrote, regression</p><p>6 0.10175275 <a title="8-lsi-6" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>7 0.09844017 <a title="8-lsi-7" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.093574964 <a title="8-lsi-8" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.092503816 <a title="8-lsi-9" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.084787935 <a title="8-lsi-10" href="./jmlr-2006-Evolutionary_Function_Approximation_for_Reinforcement_Learning.html">30 jmlr-2006-Evolutionary Function Approximation for Reinforcement Learning</a></p>
<p>11 0.083586246 <a title="8-lsi-11" href="./jmlr-2006-Learning_Image_Components_for_Object_Recognition.html">47 jmlr-2006-Learning Image Components for Object Recognition</a></p>
<p>12 0.082811765 <a title="8-lsi-12" href="./jmlr-2006-Policy_Gradient_in_Continuous_Time.html">75 jmlr-2006-Policy Gradient in Continuous Time</a></p>
<p>13 0.08158271 <a title="8-lsi-13" href="./jmlr-2006-Learning_Minimum_Volume_Sets.html">48 jmlr-2006-Learning Minimum Volume Sets</a></p>
<p>14 0.076897137 <a title="8-lsi-14" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>15 0.07467854 <a title="8-lsi-15" href="./jmlr-2006-Learning_the_Structure_of_Linear_Latent_Variable_Models.html">54 jmlr-2006-Learning the Structure of Linear Latent Variable Models</a></p>
<p>16 0.072643898 <a title="8-lsi-16" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.065881699 <a title="8-lsi-17" href="./jmlr-2006-%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">97 jmlr-2006- (Special Topic on Machine Learning for Computer Security)</a></p>
<p>18 0.063136026 <a title="8-lsi-18" href="./jmlr-2006-On_Representing_and_Generating_Kernels_by_Fuzzy_Equivalence_Relations.html">67 jmlr-2006-On Representing and Generating Kernels by Fuzzy Equivalence Relations</a></p>
<p>19 0.06300047 <a title="8-lsi-19" href="./jmlr-2006-A_Simulation-Based_Algorithm_for_Ergodic_Control_of_Markov_Chains_Conditioned_on_Rare_Events.html">7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</a></p>
<p>20 0.060597964 <a title="8-lsi-20" href="./jmlr-2006-Considering_Cost_Asymmetry_in_Learning_Classifiers.html">22 jmlr-2006-Considering Cost Asymmetry in Learning Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.012), (22, 0.537), (36, 0.046), (45, 0.014), (50, 0.046), (63, 0.039), (76, 0.018), (78, 0.024), (79, 0.011), (81, 0.025), (90, 0.034), (91, 0.017), (96, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74143571 <a title="8-lda-1" href="./jmlr-2006-A_Very_Fast_Learning_Method_for_Neural_Networks_Based_on_Sensitivity_Analysis.html">8 jmlr-2006-A Very Fast Learning Method for Neural Networks Based on Sensitivity Analysis</a></p>
<p>Author: Enrique Castillo, Bertha Guijarro-Berdiñas, Oscar Fontenla-Romero, Amparo Alonso-Betanzos</p><p>Abstract: This paper introduces a learning method for two-layer feedforward neural networks based on sensitivity analysis, which uses a linear training algorithm for each of the two layers. First, random values are assigned to the outputs of the ﬁrst layer; later, these initial values are updated based on sensitivity formulas, which use the weights in each of the layers; the process is repeated until convergence. Since these weights are learnt solving a linear system of equations, there is an important saving in computational time. The method also gives the local sensitivities of the least square errors with respect to input and output data, with no extra computational cost, because the necessary information becomes available without extra calculations. This method, called the Sensitivity-Based Linear Learning Method, can also be used to provide an initial set of weights, which signiﬁcantly improves the behavior of other learning algorithms. The theoretical basis for the method is given and its performance is illustrated by its application to several examples in which it is compared with several learning algorithms and well known data sets. The results have shown a learning speed generally faster than other existing methods. In addition, it can be used as an initialization tool for other well known methods with signiﬁcant improvements. Keywords: supervised learning, neural networks, linear optimization, least-squares, initialization method, sensitivity analysis</p><p>2 0.45105359 <a title="8-lda-2" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>Author: Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We present a family of margin based online learning algorithms for various prediction tasks. In particular we derive and analyze algorithms for binary and multiclass categorization, regression, uniclass prediction and sequence prediction. The update steps of our different algorithms are all based on analytical solutions to simple constrained optimization problems. This uniﬁed view allows us to prove worst-case loss bounds for the different algorithms and for the various decision problems based on a single lemma. Our bounds on the cumulative loss of the algorithms are relative to the smallest loss that can be attained by any ﬁxed hypothesis, and as such are applicable to both realizable and unrealizable settings. We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real data sets.</p><p>3 0.18318352 <a title="8-lda-3" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram. Keywords: spectral clustering, blind source separation, computational auditory scene analysis</p><p>4 0.1822017 <a title="8-lda-4" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>Author: Mikio L. Braun</p><p>Abstract: The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to inﬁnity. We derive probabilistic ﬁnite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reﬂecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a signiﬁcant improvement over existing non-scaling bounds. Keywords: kernel matrix, eigenvalues, relative perturbation bounds</p><p>5 0.18026683 <a title="8-lda-5" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>6 0.17927183 <a title="8-lda-6" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>7 0.1790334 <a title="8-lda-7" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>8 0.17838992 <a title="8-lda-8" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.17784458 <a title="8-lda-9" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>10 0.17609334 <a title="8-lda-10" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>11 0.1758537 <a title="8-lda-11" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>12 0.17534493 <a title="8-lda-12" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>13 0.17381823 <a title="8-lda-13" href="./jmlr-2006-On_Model_Selection_Consistency_of_Lasso.html">66 jmlr-2006-On Model Selection Consistency of Lasso</a></p>
<p>14 0.17364621 <a title="8-lda-14" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>15 0.1720998 <a title="8-lda-15" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>16 0.17188054 <a title="8-lda-16" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>17 0.17124645 <a title="8-lda-17" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.17076409 <a title="8-lda-18" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.16999416 <a title="8-lda-19" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.16974378 <a title="8-lda-20" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
