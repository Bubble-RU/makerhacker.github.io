<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 jmlr-2006-MinReg: A Scalable Algorithm for Learning Parsimonious Regulatory Networks in Yeast and Mammals</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-62" href="#">jmlr2006-62</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 jmlr-2006-MinReg: A Scalable Algorithm for Learning Parsimonious Regulatory Networks in Yeast and Mammals</h1>
<br/><p>Source: <a title="jmlr-2006-62-pdf" href="http://jmlr.org/papers/volume7/peer06a/peer06a.pdf">pdf</a></p><p>Author: Dana Pe'er, Amos Tanay, Aviv Regev</p><p>Abstract: In recent years, there has been a growing interest in applying Bayesian networks and their extensions to reconstruct regulatory networks from gene expression data. Since the gene expression domain involves a large number of variables and a limited number of samples, it poses both computational and statistical challenges to Bayesian network learning algorithms. Here we deﬁne a constrained family of Bayesian network structures suitable for this domain and devise an efﬁcient search algorithm that utilizes these structural constraints to ﬁnd high scoring networks from data. Interestingly, under reasonable assumptions on the underlying probability distribution, we can provide performance guarantees on our algorithm. Evaluation on real data from yeast and mouse, demonstrates that our method cannot only reconstruct a high quality model of the yeast regulatory network, but is also the ﬁrst method to scale to the complexity of mammalian networks and successfully reconstructs a reasonable model over thousands of variables. Keywords: Bayesian networks, structure learning, gene networks, gene expression, approximation algorithms</p><p>Reference: <a title="jmlr-2006-62-reference" href="../jmlr2006_reference/jmlr-2006-MinReg%3A_A_Scalable_Algorithm_for_Learning_Parsimonious_Regulatory_Networks_in_Yeast_and_Mammals_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The combined network of transcription factors and signaling proteins forms a regulatory program controlling the expression of individual genes directly (by regulator TFs) and indirectly (by regulator SPs). [sent-30, score-1.379]
</p><p>2 Importantly, microarrays measure not only the expression levels of target genes, but also of genes encoding regulators - TFs and SPs. [sent-34, score-0.829]
</p><p>3 , 2003), in many cases a TF’s expression level is a good proxy to its activity, allowing us to construct a network that relates the gene expression of a target gene to the gene expression of its regulators. [sent-37, score-0.564]
</p><p>4 Fortunately, in some of these cases, a change in the expression of indirect regulators (such as SPs that control the TF’s activity) may be observed in microarray measurements, allowing us to detect an indirect regulatory relation in lieu of the direct event. [sent-39, score-0.945]
</p><p>5 These organisms have considerably more complex regulatory systems, with a larger number of regulators and target genes, and much more complex combinatorial regulatory functions. [sent-58, score-1.115]
</p><p>6 A central question toward these important applications is ﬁnding a parsimonious set of major regulators at the center of a given response, and distinguishing them from additional redundant regulators or by product effects. [sent-60, score-1.296]
</p><p>7 Regulation Model Our gene regulation model is a Bayesian network that describes regulatory relations between genes. [sent-70, score-0.602]
</p><p>8 We denote by PaX the set of all regulators (parents) of the gene (variable) X. [sent-73, score-0.766]
</p><p>9 The key point behind to our approach is that we enforce a number of biologically motivated constraints to limit these regulators and the graph structure. [sent-75, score-0.658]
</p><p>10 Unlike a standard Bayesian network, we limit the possible regulators (parents) in the network to a set of candidate regulators C . [sent-76, score-1.377]
</p><p>11 Our candidate set C is chosen based on prior biological knowledge, and contains known and putative regulators in the organism being studied. [sent-77, score-0.806]
</p><p>12 Notice this includes small top layer of regulators and many targets for each. [sent-80, score-0.681]
</p><p>13 which genes in a genome may function as regulators in general is often tractable, ﬁnding which regulators are active in a data set is difﬁcult. [sent-81, score-1.441]
</p><p>14 In fact, previous biological studies indicate that only a small fraction of all potential regulators may be active in a given data set. [sent-83, score-0.736]
</p><p>15 , 2002) (Figure 2) indicate that each such “master regulatory gene” may affect the transcription of many genes (indeed, only 36% of the yeast and human genes respectively encode TFs). [sent-89, score-0.721]
</p><p>16 These constraints result in a graph of small depth, in which layers containing a small number of regulators control a large bottom layer of target genes, consistent with current biological understanding. [sent-91, score-0.755]
</p><p>17 In fact, for most biological applications, false positives are signiﬁcantly more “costly” than false negatives, and ﬁnding a robust set of key regulators whom are most strongly supported by the data (as offered by our model) is a more important goal then discovering their complete set of targets. [sent-95, score-0.736]
</p><p>18 The top layer is associated with the regulators and the bottom layer is associated with all other variables. [sent-101, score-0.639]
</p><p>19 The graph structure is best visualized as a graph with a shallow depth: in the top layers, a small set of regulators (chosen from a large set C ), possibly regulating each other and in the bottom layer, all other variables (see Figure 3). [sent-107, score-0.701]
</p><p>20 Unfortunately, the standard approach is likely to fail as the limited number of regulators speciﬁed by the regulation graph could be quickly used up. [sent-112, score-0.834]
</p><p>21 In many of these iterations, a new regulator is added to the regulator set R . [sent-115, score-0.778]
</p><p>22 Therefore, after little over 30 iterations, no new regulators could be added to R and all subsequent legal steps would only involve adding edges from regulators already in R . [sent-116, score-1.278]
</p><p>23 While these regulators might be the best parents for a small set of variables, thousands of other variables remain unexplained in the model. [sent-117, score-0.721]
</p><p>24 Fortunately, once a regulator set R is given, ﬁnding the optimal regulation graph constrained to R is polynomial, and for most practical cases efﬁcient. [sent-122, score-0.584]
</p><p>25 Deﬁnition 2 We deﬁne the utility, F(R) of a regulator set R as: n  F(R) = ∑  max  i=1 P⊂R,|P|≤d  Score(Xi ; P)  (2)  The utility of a regulator set R can be computed quickly and closely approximates the score of the optimal network constrained to R, denoted SCORE(R). [sent-126, score-0.892]
</p><p>26 The challenge is that the regulator set R must be chosen from a much larger candidate set C and there are |C | possible regulator sets. [sent-142, score-0.832]
</p><p>27 MinReg Learning Algorithm We now turn to the task of learning a regulation model (speciﬁed by a set of regulators R , and a parent structure on the variables in X ) from a training set (D = {x[1], . [sent-145, score-0.857]
</p><p>28 Our goal is to choose the set of regulators R and learn the regulatory graph structure that best explains this distribution. [sent-149, score-0.896]
</p><p>29 Our novel greedy algorithm for this task, MinReg (sketched in Figure 4), begins with an empty set of regulators and an empty graph structure. [sent-154, score-0.686]
</p><p>30 At each iteration, for each possible candidate, we construct an increment regulator set by adding that candidate to the current regulator set. [sent-155, score-0.832]
</p><p>31 Each time R is updated, we calculate the optimal regulation graph restricted to the current regulator set R . [sent-157, score-0.584]
</p><p>32 Deﬁnition 4 We deﬁne the marginal utility of adding a regulator set C to an already chosen regulator set R as F(C | R) = F(C ∪ R) − F(R) (4)  Thus, at each iteration, we add the candidate regulator with the largest marginal utility. [sent-163, score-1.253]
</p><p>33 Importantly, this is a biologically-plausible scenario, since synergy between regulators is a well-documented phenomenon. [sent-170, score-0.675]
</p><p>34 , Xn approximate its best parents restricted to R ∪ c maxP⊂R ∪c,|P|≤d Score(Xi ; P) Add local score of Xi to utility of c Add best candidate regulator to R and update the regulation graph until stopping criterion (3. [sent-174, score-0.765]
</p><p>35 This ﬁts our intuition that as R grows larger, the utility of adding new regulators diminishes. [sent-199, score-0.639]
</p><p>36 For k = 1, the optimal solution is the best single regulator and this is exactly the regulator found by the MinReg algorithm therefore MINREG1 = OPT1 . [sent-205, score-0.778]
</p><p>37 Set J = argmaxI∈C F(I), the best single regulator in C and Jˆ = argmaxI∈OPTk F(I), the best single regulator in OPTk . [sent-207, score-0.778]
</p><p>38 Let ˆ ˆ F(Y) = F(Y ∪ {J}) − F(J), our goal is to ﬁnd a set of k − 1 regulators that optimize F on C \ {J}. [sent-210, score-0.639]
</p><p>39 Fortunately, while regulators are synergistic for speciﬁc targets, F is a sum over thousands of variables. [sent-231, score-0.663]
</p><p>40 Even if the synergy between two regulators is very high, this synergy would need to hold for many targets, otherwise it would average out when summing over all of X . [sent-232, score-0.711]
</p><p>41 We empirically tested the synergy between regulators and groups of regulators in both yeast and mammalian data sets, the worst factor we encountered was 1. [sent-233, score-1.54]
</p><p>42 We store the candidate regulators in a heap sorted by Util(c). [sent-257, score-0.73]
</p><p>43 1 S TOPPING C RITERION Formally, the best regulator set problem requires a predeﬁned constant k, specifying the number of regulators in the model. [sent-281, score-1.028]
</p><p>44 We devise a stopping criterion for the addition of new regulators to our model. [sent-284, score-0.656]
</p><p>45 We continue to add regulators as long as their contribution to the score is signiﬁcantly better than random regulators. [sent-285, score-0.708]
</p><p>46 We generate a set of m random regulators with similar properties to the real candidate regulators in our data. [sent-286, score-1.332]
</p><p>47 We construct these by sampling regulators with replacement from the original candidate regulator set. [sent-287, score-1.082]
</p><p>48 Thus, the random regulators have the same distribution over their values, but these are independent of the target variables. [sent-289, score-0.639]
</p><p>49 We calculate the score for these random regulators in a manner similar to the real candidate set and store these in a heap. [sent-290, score-0.762]
</p><p>50 We continue to add regulators to our model as long as they score greater than the random candidates. [sent-292, score-0.724]
</p><p>51 4 We compiled a set C of 466 candidate regulators for yeast, which includes any gene with a potential regulatory role based on annotation or sequence homology. [sent-317, score-1.11]
</p><p>52 We compiled a list of 684 candidate regulators using criterion similar to the yeast candidate regulator set. [sent-327, score-1.263]
</p><p>53 To demonstrate the accuracy of our algorithm in reconstructing the real yeast and mammalian regulatory network, we devise an approach to infer regulator function from our model, and compare that to the known central regulators in the relevant biological processes. [sent-333, score-1.634]
</p><p>54 1 A LPHA M ODULARITY MinReg employs a greedy approach, evaluating only the addition of single regulators to the model at each iteration, potentially missing a better combination of regulators. [sent-352, score-0.683]
</p><p>55 Furthermore, to improve the speed performance, the α-modularity of F is used strongly by the implementation to bound the number of the regulators that are evaluated from the heap at each iteration. [sent-355, score-0.676]
</p><p>56 9 To empirically evaluate the α-modularity of F in the two data sets used, we calculated the pairwise gain in score for all pairs of regulators in the candidate set C . [sent-356, score-0.762]
</p><p>57 In addition, we calculated the worst α for 10,000 pairs of random subsets, C1 , C2 ⊂ C , ranging between 2 to 8 regulators each, using F(C1 ∪ C2 ),F(C1 ) and F(C2 ). [sent-358, score-0.639]
</p><p>58 We then used the inferred model and gene expression of inferred regulators in the test data to predict the expression levels of all 3755 variables in each test sample. [sent-374, score-0.938]
</p><p>59 That is, given the expression of regulators in the mth sample, we use P(X | PaX [m]) to predict the value of X in that sample. [sent-375, score-0.685]
</p><p>60 Typically, after the ﬁrst few iterations, only a few regulators are evaluated at each iteration. [sent-388, score-0.639]
</p><p>61 As for false positives, on average 74% of the reconstructed regulators were correct (worst case 71% and best case 77%). [sent-413, score-0.669]
</p><p>62 Even when we did not limit to 181  P E ’ ER , TANAY, AND R EGEV  candidate regulators (setting C = X ) our reconstruction of regulators was surprisingly good, 42/47 of the regulators were correct and 76% of the individual edges were correctly reconstructed. [sent-418, score-1.996]
</p><p>63 In a real biological system, there are more regulators working together in more complex functions, feedback loops, and unobserved events. [sent-423, score-0.736]
</p><p>64 Importantly, such functional characterization of key regulators is a critical biological task in its own right. [sent-430, score-0.736]
</p><p>65 More generally, we expect that the function associated with a regulator based on its set of targets in the model (as reﬂected by signiﬁcant enrichment for a particular annotation) will ﬁt with the known function of this regulator (as reﬂected by its own annotation). [sent-440, score-0.863]
</p><p>66 Speciﬁcally, the model derived functions for 8 of the top 10 regulators (sorted by p-value) coincided with their known biological roles. [sent-447, score-0.752]
</p><p>67 To assess the impact of this prior knowledge on MinReg’s success, we examined MinReg’s biological accuracy when run on the yeast data set, in the absence of a pre-deﬁned candidate regulator set (i. [sent-463, score-0.667]
</p><p>68 This lack of “true” regulators suggests the expression of co-regulated genes is often at least as predictive (and sometimes even more) than that of the true regulating gene. [sent-467, score-0.853]
</p><p>69 The fact that our procedure results in a statistically signiﬁcant enhancement of regulators is encouraging. [sent-471, score-0.639]
</p><p>70 3 Analysis of Mouse Data In contrast to the signiﬁcant success of several methods in reconstructing yeast regulatory networks, no algorithm has so far successfully reconstructed a mammalian regulatory network. [sent-474, score-0.76]
</p><p>71 The results for the top 20 regulators are of similar quality: the associations for 13 regulators correspond their known function,four regulators were previously uncharacterized and the associations for three regulators are unsupported. [sent-476, score-2.604]
</p><p>72 These are 8/10 regulators we evaluated for gene function above, excluding 2 regulators for which no motif is currently known. [sent-478, score-1.432]
</p><p>73 It is important to note that in our data set, the expression of many candidate regulators remains almost constant. [sent-482, score-0.739]
</p><p>74 Only 148 genes from our original candidate set of known and putative regulators were included in the 2828 genes. [sent-483, score-0.853]
</p><p>75 We ﬁrst examine whether the key regulators identiﬁed by the algorithm are known to participate in the main biological process taking place in B lymphocytes under the tested stimuli- the decision between cell proliferation and cell death. [sent-485, score-0.877]
</p><p>76 Indeed, the inferred regulator set R includes the top ﬁve genes - Trp53, Nfkb1, Jun, Fos and Bak1 - known to play a pivotal role in this decision. [sent-486, score-0.565]
</p><p>77 16 additional inferred regulators are known to be directly involved in the regulation of proliferation and cell death13 and seven others are involved in the regulation of the cell division cycle. [sent-487, score-1.143]
</p><p>78 14 Overall, 28/75 regulators are known to participate in regulation of the central process occurring in these cells. [sent-488, score-0.815]
</p><p>79 Taken together, this analysis indicates that 44 of the 75 inferred regulators are known regulators of lymphocytes, cell proliferation and death or both. [sent-491, score-1.402]
</p><p>80 We next examined the quality of our model structure, based on its ability to predict the detailed function of individual regulators (as described above for yeast). [sent-494, score-0.655]
</p><p>81 Based on these criteria the predicted function for over half (45/75) the regulators had at least some support in prior biological knowledge. [sent-498, score-0.736]
</p><p>82 Speciﬁcally, the predicted functional annotation of 6/75 regulators was “very good” (P < 10−18 ), “good” for 28/75 additional regulators (P < 10−35 ), and “weak” for 11/75 genes. [sent-499, score-1.33]
</p><p>83 12/75 genes had “no match” to any annotations, but many of them were genes encoding relatively uncharacterized regulators with little or no known annotations. [sent-500, score-0.927]
</p><p>84 Importantly, only 16 of 75 regulators were assigned no signiﬁcant annotation, indicating the biological coherence of our reconstructed model, where regulators are associated with functionally related targets. [sent-501, score-1.405]
</p><p>85 We expect that many of the other inferred regulators may be just as correct, and are simply not characterized by current biological knowledge. [sent-511, score-0.768]
</p><p>86 Indeed, our method detects and correctly associates a whole range of regulators - including transcription factors, kinases and phosphotases. [sent-517, score-0.707]
</p><p>87 However, while MinReg considers each target gene separately, Module Networks groups targets into sets (“modules”), such that all module genes share exactly the same regulatory program. [sent-530, score-0.633]
</p><p>88 Here, we applied Module networks to the B-lymphocyte data and learn 75 modules and their associated regulation programs, involving 216 regulators overall. [sent-533, score-0.88]
</p><p>89 While many of the regulators overlapped those chosen by the MinReg algorithm,17 these did not include 3 of the 5 known central regulators of cell proliferation and death (Nfkb1, Fos, nor Bak1) identiﬁed by MinReg. [sent-534, score-1.37]
</p><p>90 For comparison, we can evaluate the Module Networks model by annotating each of its 206 regulators based on its associated targets (compiled across all 75 modules), resulting in 196 signiﬁcantly annotated regulators. [sent-535, score-0.697]
</p><p>91 When evaluating the annotation quality of the top 75 regulators (sorted by p-value) by the same scale described above, we did not receive similarly signiﬁcant results. [sent-536, score-0.691]
</p><p>92 Furthermore, when examining only regulators identiﬁed by both algorithms, MinReg’s associations outperform Module networks on 23 regulators, while Module networks only found a better association for one regulator (Gnai13). [sent-538, score-1.118]
</p><p>93 In contrast, MinReg focuses on ﬁnding the most dominant regulators and their targets in the data. [sent-546, score-0.681]
</p><p>94 Our results are validated by statistical criteria (synthetic data, cross-validation) and biological ones (our ability to correctly infer a correct set of key regulators and their detailed regulatory functions). [sent-558, score-0.974]
</p><p>95 Importantly, unlike previous approaches, our method scales well to complex mammalian systems, discovering key mammalian regulators (both signaling proteins and transcription factors) solely from expression data. [sent-559, score-1.011]
</p><p>96 While constraining the number of regulators carries obvious statistical and computational advantages, what does it cost us in biological accuracy? [sent-560, score-0.736]
</p><p>97 Indeed, our analysis of the B lymphocyte data set indicates that MinReg is able to focus on the very key regulators of a complex process (cell proliferation and death) as well as on a signiﬁcant number of cell speciﬁc regulators. [sent-565, score-0.715]
</p><p>98 In contrast, MinReg and Module Networks can reconstruct a network over thousands of variables, based on the assumption that a small number of regulators can be chosen from a pre-deﬁned candidate set. [sent-587, score-0.784]
</p><p>99 Identifying global regulators in transcriptional regulatory networks in bacteria. [sent-871, score-0.937]
</p><p>100 Module networks: identifying regulatory modules and their condition speciﬁc regulators from gene expression data. [sent-938, score-1.082]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('regulators', 0.639), ('regulator', 0.389), ('minreg', 0.352), ('regulatory', 0.238), ('regulation', 0.176), ('optk', 0.154), ('genes', 0.144), ('yeast', 0.127), ('gene', 0.127), ('mammalian', 0.099), ('biological', 0.097), ('minregk', 0.091), ('module', 0.082), ('tanay', 0.069), ('score', 0.069), ('transcription', 0.068), ('fx', 0.059), ('annotations', 0.059), ('egev', 0.059), ('parents', 0.058), ('candidate', 0.054), ('annotation', 0.052), ('eg', 0.05), ('segal', 0.05), ('expression', 0.046), ('pax', 0.045), ('network', 0.045), ('cell', 0.044), ('util', 0.043), ('bayesian', 0.042), ('targets', 0.042), ('pe', 0.041), ('mouse', 0.041), ('heap', 0.037), ('signaling', 0.037), ('synergy', 0.036), ('networks', 0.033), ('scoring', 0.033), ('proliferation', 0.032), ('inferred', 0.032), ('modules', 0.032), ('reconstructed', 0.03), ('tf', 0.028), ('greedy', 0.028), ('importantly', 0.028), ('genetics', 0.028), ('reconstructing', 0.028), ('enrichment', 0.027), ('transcriptional', 0.027), ('motif', 0.027), ('parent', 0.026), ('reconstruction', 0.025), ('associations', 0.024), ('regulating', 0.024), ('thousands', 0.024), ('binding', 0.023), ('proteins', 0.023), ('regev', 0.023), ('maxp', 0.023), ('microarray', 0.022), ('reconstruct', 0.022), ('heckerman', 0.021), ('fos', 0.021), ('harbison', 0.021), ('hartemink', 0.021), ('jun', 0.021), ('lymphocytes', 0.021), ('tfs', 0.021), ('utilx', 0.021), ('yoo', 0.021), ('monotone', 0.021), ('hughes', 0.02), ('scored', 0.02), ('genome', 0.019), ('graph', 0.019), ('aviv', 0.018), ('harvard', 0.018), ('metabolism', 0.018), ('parsimonious', 0.018), ('saccharomyces', 0.018), ('candidates', 0.018), ('activity', 0.018), ('devise', 0.017), ('pro', 0.017), ('model', 0.016), ('marginal', 0.016), ('events', 0.016), ('lehmann', 0.016), ('pathway', 0.016), ('compendium', 0.016), ('putative', 0.016), ('amos', 0.016), ('bussemaker', 0.016), ('death', 0.016), ('gasch', 0.016), ('gifford', 0.016), ('ihmels', 0.016), ('imoto', 0.016), ('indegree', 0.016), ('modular', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="62-tfidf-1" href="./jmlr-2006-MinReg%3A_A_Scalable_Algorithm_for_Learning_Parsimonious_Regulatory_Networks_in_Yeast_and_Mammals.html">62 jmlr-2006-MinReg: A Scalable Algorithm for Learning Parsimonious Regulatory Networks in Yeast and Mammals</a></p>
<p>Author: Dana Pe'er, Amos Tanay, Aviv Regev</p><p>Abstract: In recent years, there has been a growing interest in applying Bayesian networks and their extensions to reconstruct regulatory networks from gene expression data. Since the gene expression domain involves a large number of variables and a limited number of samples, it poses both computational and statistical challenges to Bayesian network learning algorithms. Here we deﬁne a constrained family of Bayesian network structures suitable for this domain and devise an efﬁcient search algorithm that utilizes these structural constraints to ﬁnd high scoring networks from data. Interestingly, under reasonable assumptions on the underlying probability distribution, we can provide performance guarantees on our algorithm. Evaluation on real data from yeast and mouse, demonstrates that our method cannot only reconstruct a high quality model of the yeast regulatory network, but is also the ﬁrst method to scale to the complexity of mammalian networks and successfully reconstructs a reasonable model over thousands of variables. Keywords: Bayesian networks, structure learning, gene networks, gene expression, approximation algorithms</p><p>2 0.055307619 <a title="62-tfidf-2" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>Author: Robert Castelo, Alberto Roverato</p><p>Abstract: Learning of large-scale networks of interactions from microarray data is an important and challenging problem in bioinformatics. A widely used approach is to assume that the available data constitute a random sample from a multivariate distribution belonging to a Gaussian graphical model. As a consequence, the prime objects of inference are full-order partial correlations which are partial correlations between two variables given the remaining ones. In the context of microarray data the number of variables exceed the sample size and this precludes the application of traditional structure learning procedures because a sampling version of full-order partial correlations does not exist. In this paper we consider limited-order partial correlations, these are partial correlations computed on marginal distributions of manageable size, and provide a set of rules that allow one to assess the usefulness of these quantities to derive the independence structure of the underlying Gaussian graphical model. Furthermore, we introduce a novel structure learning procedure based on a quantity, obtained from limited-order partial correlations, that we call the non-rejection rate. The applicability and usefulness of the procedure are demonstrated by both simulated and real data. Keywords: Gaussian distribution, gene network, graphical model, microarray data, non-rejection rate, partial correlation, small-sample inference</p><p>3 0.046401989 <a title="62-tfidf-3" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>Author: Sayan Mukherjee, Ding-Xuan Zhou</p><p>Abstract: We introduce an algorithm that learns gradients from samples in the supervised learning framework. An error analysis is given for the convergence of the gradient estimated by the algorithm to the true gradient. The utility of the algorithm for the problem of variable selection as well as determining variable covariance is illustrated on simulated data as well as two gene expression data sets. For square loss we provide a very efﬁcient implementation with respect to both memory and time. Keywords: Tikhnonov regularization, variable selection, reproducing kernel Hilbert space, generalization bounds</p><p>4 0.041261874 <a title="62-tfidf-4" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>Author: Luis M. de Campos</p><p>Abstract: We propose a new scoring function for learning Bayesian networks from data using score+search algorithms. This is based on the concept of mutual information and exploits some well-known properties of this measure in a novel way. Essentially, a statistical independence test based on the chi-square distribution, associated with the mutual information measure, together with a property of additive decomposition of this measure, are combined in order to measure the degree of interaction between each variable and its parent variables in the network. The result is a non-Bayesian scoring function called MIT (mutual information tests) which belongs to the family of scores based on information theory. The MIT score also represents a penalization of the Kullback-Leibler divergence between the joint probability distributions associated with a candidate network and with the available data set. Detailed results of a complete experimental evaluation of the proposed scoring function and its comparison with the well-known K2, BDeu and BIC/MDL scores are also presented. Keywords: Bayesian networks, scoring functions, learning, mutual information, conditional independence tests</p><p>5 0.040275298 <a title="62-tfidf-5" href="./jmlr-2006-Rearrangement_Clustering%3A_Pitfalls%2C_Remedies%2C_and_Applications.html">78 jmlr-2006-Rearrangement Clustering: Pitfalls, Remedies, and Applications</a></p>
<p>Author: Sharlee Climer, Weixiong Zhang</p><p>Abstract: Given a matrix of values in which the rows correspond to objects and the columns correspond to features of the objects, rearrangement clustering is the problem of rearranging the rows of the matrix such that the sum of the similarities between adjacent rows is maximized. Referred to by various names and reinvented several times, this clustering technique has been extensively used in many ﬁelds over the last three decades. In this paper, we point out two critical pitfalls that have been previously overlooked. The ﬁrst pitfall is deleterious when rearrangement clustering is applied to objects that form natural clusters. The second concerns a similarity metric that is commonly used. We present an algorithm that overcomes these pitfalls. This algorithm is based on a variation of the Traveling Salesman Problem. It offers an extra beneﬁt as it automatically determines cluster boundaries. Using this algorithm, we optimally solve four benchmark problems and a 2,467-gene expression data clustering problem. As expected, our new algorithm identiﬁes better clusters than those found by previous approaches in all ﬁve cases. Overall, our results demonstrate the beneﬁts of rectifying the pitfalls and exemplify the usefulness of this clustering technique. Our code is available at our websites. Keywords: clustering, visualization of patterns in data, bond energy algorithm, traveling salesman problem, asymmetric clustering</p><p>6 0.037414167 <a title="62-tfidf-6" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.031609919 <a title="62-tfidf-7" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>8 0.020655764 <a title="62-tfidf-8" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>9 0.018987956 <a title="62-tfidf-9" href="./jmlr-2006-Evolutionary_Function_Approximation_for_Reinforcement_Learning.html">30 jmlr-2006-Evolutionary Function Approximation for Reinforcement Learning</a></p>
<p>10 0.018926881 <a title="62-tfidf-10" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.018775038 <a title="62-tfidf-11" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>12 0.018065004 <a title="62-tfidf-12" href="./jmlr-2006-Noisy-OR_Component_Analysis_and_its_Application_to_Link_Analysis.html">64 jmlr-2006-Noisy-OR Component Analysis and its Application to Link Analysis</a></p>
<p>13 0.017723676 <a title="62-tfidf-13" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>14 0.017102756 <a title="62-tfidf-14" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>15 0.015920926 <a title="62-tfidf-15" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>16 0.015640616 <a title="62-tfidf-16" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>17 0.014510902 <a title="62-tfidf-17" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.014367867 <a title="62-tfidf-18" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.014296596 <a title="62-tfidf-19" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>20 0.014179343 <a title="62-tfidf-20" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.085), (1, -0.044), (2, -0.064), (3, -0.011), (4, -0.031), (5, -0.079), (6, 0.068), (7, -0.088), (8, 0.097), (9, 0.0), (10, 0.009), (11, -0.006), (12, -0.006), (13, 0.034), (14, 0.065), (15, 0.047), (16, -0.053), (17, 0.035), (18, 0.127), (19, 0.049), (20, 0.083), (21, -0.025), (22, 0.208), (23, -0.031), (24, 0.204), (25, -0.077), (26, -0.055), (27, 0.001), (28, -0.005), (29, -0.022), (30, -0.098), (31, -0.059), (32, -0.077), (33, -0.258), (34, 0.183), (35, -0.107), (36, -0.287), (37, -0.135), (38, -0.031), (39, 0.09), (40, 0.396), (41, -0.343), (42, -0.103), (43, 0.2), (44, 0.002), (45, 0.079), (46, -0.011), (47, 0.061), (48, -0.073), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96699309 <a title="62-lsi-1" href="./jmlr-2006-MinReg%3A_A_Scalable_Algorithm_for_Learning_Parsimonious_Regulatory_Networks_in_Yeast_and_Mammals.html">62 jmlr-2006-MinReg: A Scalable Algorithm for Learning Parsimonious Regulatory Networks in Yeast and Mammals</a></p>
<p>Author: Dana Pe'er, Amos Tanay, Aviv Regev</p><p>Abstract: In recent years, there has been a growing interest in applying Bayesian networks and their extensions to reconstruct regulatory networks from gene expression data. Since the gene expression domain involves a large number of variables and a limited number of samples, it poses both computational and statistical challenges to Bayesian network learning algorithms. Here we deﬁne a constrained family of Bayesian network structures suitable for this domain and devise an efﬁcient search algorithm that utilizes these structural constraints to ﬁnd high scoring networks from data. Interestingly, under reasonable assumptions on the underlying probability distribution, we can provide performance guarantees on our algorithm. Evaluation on real data from yeast and mouse, demonstrates that our method cannot only reconstruct a high quality model of the yeast regulatory network, but is also the ﬁrst method to scale to the complexity of mammalian networks and successfully reconstructs a reasonable model over thousands of variables. Keywords: Bayesian networks, structure learning, gene networks, gene expression, approximation algorithms</p><p>2 0.34970865 <a title="62-lsi-2" href="./jmlr-2006-Rearrangement_Clustering%3A_Pitfalls%2C_Remedies%2C_and_Applications.html">78 jmlr-2006-Rearrangement Clustering: Pitfalls, Remedies, and Applications</a></p>
<p>Author: Sharlee Climer, Weixiong Zhang</p><p>Abstract: Given a matrix of values in which the rows correspond to objects and the columns correspond to features of the objects, rearrangement clustering is the problem of rearranging the rows of the matrix such that the sum of the similarities between adjacent rows is maximized. Referred to by various names and reinvented several times, this clustering technique has been extensively used in many ﬁelds over the last three decades. In this paper, we point out two critical pitfalls that have been previously overlooked. The ﬁrst pitfall is deleterious when rearrangement clustering is applied to objects that form natural clusters. The second concerns a similarity metric that is commonly used. We present an algorithm that overcomes these pitfalls. This algorithm is based on a variation of the Traveling Salesman Problem. It offers an extra beneﬁt as it automatically determines cluster boundaries. Using this algorithm, we optimally solve four benchmark problems and a 2,467-gene expression data clustering problem. As expected, our new algorithm identiﬁes better clusters than those found by previous approaches in all ﬁve cases. Overall, our results demonstrate the beneﬁts of rectifying the pitfalls and exemplify the usefulness of this clustering technique. Our code is available at our websites. Keywords: clustering, visualization of patterns in data, bond energy algorithm, traveling salesman problem, asymmetric clustering</p><p>3 0.31991586 <a title="62-lsi-3" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Radu Stefan Niculescu, Tom M. Mitchell, R. Bharat Rao</p><p>Abstract: The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. This paper considers a variety of types of domain knowledge for constraining parameter estimates when learning Bayesian networks. In particular, we consider domain knowledge that constrains the values or relationships among subsets of parameters in a Bayesian network with known structure. We incorporate a wide variety of parameter constraints into learning procedures for Bayesian networks, by formulating this task as a constrained optimization problem. The assumptions made in module networks, dynamic Bayes nets and context speciﬁc independence models can be viewed as particular cases of such parameter constraints. We present closed form solutions or fast iterative algorithms for estimating parameters subject to several speciﬁc classes of parameter constraints, including equalities and inequalities among parameters, constraints on individual parameters, and constraints on sums and ratios of parameters, for discrete and continuous variables. Our methods cover learning from both frequentist and Bayesian points of view, from both complete and incomplete data. We present formal guarantees for our estimators, as well as methods for automatically learning useful parameter constraints from data. To validate our approach, we apply it to the domain of fMRI brain image analysis. Here we demonstrate the ability of our system to ﬁrst learn useful relationships among parameters, and then to use them to constrain the training of the Bayesian network, resulting in improved cross-validated accuracy of the learned model. Experiments on synthetic data are also presented. Keywords: Bayesian networks, constrained optimization, domain knowledge c 2006 Radu Stefan Niculescu, Tom Mitchell and Bharat Rao. N ICULESCU , M ITCHELL AND R AO</p><p>4 0.21700159 <a title="62-lsi-4" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>Author: Robert Castelo, Alberto Roverato</p><p>Abstract: Learning of large-scale networks of interactions from microarray data is an important and challenging problem in bioinformatics. A widely used approach is to assume that the available data constitute a random sample from a multivariate distribution belonging to a Gaussian graphical model. As a consequence, the prime objects of inference are full-order partial correlations which are partial correlations between two variables given the remaining ones. In the context of microarray data the number of variables exceed the sample size and this precludes the application of traditional structure learning procedures because a sampling version of full-order partial correlations does not exist. In this paper we consider limited-order partial correlations, these are partial correlations computed on marginal distributions of manageable size, and provide a set of rules that allow one to assess the usefulness of these quantities to derive the independence structure of the underlying Gaussian graphical model. Furthermore, we introduce a novel structure learning procedure based on a quantity, obtained from limited-order partial correlations, that we call the non-rejection rate. The applicability and usefulness of the procedure are demonstrated by both simulated and real data. Keywords: Gaussian distribution, gene network, graphical model, microarray data, non-rejection rate, partial correlation, small-sample inference</p><p>5 0.13511309 <a title="62-lsi-5" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>Author: Luis M. de Campos</p><p>Abstract: We propose a new scoring function for learning Bayesian networks from data using score+search algorithms. This is based on the concept of mutual information and exploits some well-known properties of this measure in a novel way. Essentially, a statistical independence test based on the chi-square distribution, associated with the mutual information measure, together with a property of additive decomposition of this measure, are combined in order to measure the degree of interaction between each variable and its parent variables in the network. The result is a non-Bayesian scoring function called MIT (mutual information tests) which belongs to the family of scores based on information theory. The MIT score also represents a penalization of the Kullback-Leibler divergence between the joint probability distributions associated with a candidate network and with the available data set. Detailed results of a complete experimental evaluation of the proposed scoring function and its comparison with the well-known K2, BDeu and BIC/MDL scores are also presented. Keywords: Bayesian networks, scoring functions, learning, mutual information, conditional independence tests</p><p>6 0.13080852 <a title="62-lsi-6" href="./jmlr-2006-Evolutionary_Function_Approximation_for_Reinforcement_Learning.html">30 jmlr-2006-Evolutionary Function Approximation for Reinforcement Learning</a></p>
<p>7 0.11437006 <a title="62-lsi-7" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>8 0.11088341 <a title="62-lsi-8" href="./jmlr-2006-One-Class_Novelty_Detection_for_Seizure_Analysis_from_Intracranial_EEG.html">69 jmlr-2006-One-Class Novelty Detection for Seizure Analysis from Intracranial EEG</a></p>
<p>9 0.10862838 <a title="62-lsi-9" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>10 0.10342529 <a title="62-lsi-10" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>11 0.1034149 <a title="62-lsi-11" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>12 0.10273971 <a title="62-lsi-12" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>13 0.099047042 <a title="62-lsi-13" href="./jmlr-2006-Computational_and_Theoretical_Analysis_of__Null_Space__and_Orthogonal_Linear_Discriminant_Analysis.html">21 jmlr-2006-Computational and Theoretical Analysis of  Null Space  and Orthogonal Linear Discriminant Analysis</a></p>
<p>14 0.097657815 <a title="62-lsi-14" href="./jmlr-2006-Learning_the_Structure_of_Linear_Latent_Variable_Models.html">54 jmlr-2006-Learning the Structure of Linear Latent Variable Models</a></p>
<p>15 0.095714331 <a title="62-lsi-15" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.089450739 <a title="62-lsi-16" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>17 0.085650124 <a title="62-lsi-17" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>18 0.081664965 <a title="62-lsi-18" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>19 0.074733555 <a title="62-lsi-19" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>20 0.073753797 <a title="62-lsi-20" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.392), (8, 0.014), (35, 0.015), (36, 0.069), (45, 0.012), (46, 0.036), (50, 0.071), (61, 0.016), (63, 0.03), (68, 0.019), (74, 0.023), (76, 0.024), (78, 0.017), (79, 0.014), (81, 0.025), (84, 0.012), (90, 0.014), (91, 0.031), (96, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74197066 <a title="62-lda-1" href="./jmlr-2006-MinReg%3A_A_Scalable_Algorithm_for_Learning_Parsimonious_Regulatory_Networks_in_Yeast_and_Mammals.html">62 jmlr-2006-MinReg: A Scalable Algorithm for Learning Parsimonious Regulatory Networks in Yeast and Mammals</a></p>
<p>Author: Dana Pe'er, Amos Tanay, Aviv Regev</p><p>Abstract: In recent years, there has been a growing interest in applying Bayesian networks and their extensions to reconstruct regulatory networks from gene expression data. Since the gene expression domain involves a large number of variables and a limited number of samples, it poses both computational and statistical challenges to Bayesian network learning algorithms. Here we deﬁne a constrained family of Bayesian network structures suitable for this domain and devise an efﬁcient search algorithm that utilizes these structural constraints to ﬁnd high scoring networks from data. Interestingly, under reasonable assumptions on the underlying probability distribution, we can provide performance guarantees on our algorithm. Evaluation on real data from yeast and mouse, demonstrates that our method cannot only reconstruct a high quality model of the yeast regulatory network, but is also the ﬁrst method to scale to the complexity of mammalian networks and successfully reconstructs a reasonable model over thousands of variables. Keywords: Bayesian networks, structure learning, gene networks, gene expression, approximation algorithms</p><p>2 0.31228662 <a title="62-lda-2" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>Author: Sayan Mukherjee, Qiang Wu</p><p>Abstract: We introduce an algorithm that simultaneously estimates a classiﬁcation function as well as its gradient in the supervised learning framework. The motivation for the algorithm is to ﬁnd salient variables and estimate how they covary. An efﬁcient implementation with respect to both memory and time is given. The utility of the algorithm is illustrated on simulated data as well as a gene expression data set. An error analysis is given for the convergence of the estimate of the classiﬁcation function and its gradient to the true classiﬁcation function and true gradient. Keywords: Tikhnonov regularization, variable selection, reproducing kernel Hilbert space, generalization bounds, classiﬁcation</p><p>3 0.29923522 <a title="62-lda-3" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Ben Taskar, Simon Lacoste-Julien, Michael I. Jordan</p><p>Abstract: We present a simple and scalable algorithm for maximum-margin estimation of structured output models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). The projection step can be solved using dynamic programming or combinatorial algorithms for min-cost convex ﬂow, depending on the structure of the problem. We show that this approach provides a memoryefﬁcient alternative to formulations based on reductions to a quadratic program (QP). We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.1 Keywords: Markov networks, large-margin methods, structured prediction, extragradient, Bregman projections</p><p>4 0.29382536 <a title="62-lda-4" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>Author: Sayan Mukherjee, Ding-Xuan Zhou</p><p>Abstract: We introduce an algorithm that learns gradients from samples in the supervised learning framework. An error analysis is given for the convergence of the gradient estimated by the algorithm to the true gradient. The utility of the algorithm for the problem of variable selection as well as determining variable covariance is illustrated on simulated data as well as two gene expression data sets. For square loss we provide a very efﬁcient implementation with respect to both memory and time. Keywords: Tikhnonov regularization, variable selection, reproducing kernel Hilbert space, generalization bounds</p><p>5 0.29323852 <a title="62-lda-5" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>Author: Magnus Ekdahl, Timo Koski</p><p>Abstract: In many pattern recognition/classiﬁcation problem the true class conditional model and class probabilities are approximated for reasons of reducing complexity and/or of statistical estimation. The approximated classiﬁer is expected to have worse performance, here measured by the probability of correct classiﬁcation. We present an analysis valid in general, and easily computable formulas for estimating the degradation in probability of correct classiﬁcation when compared to the optimal classiﬁer. An example of an approximation is the Na¨ve Bayes classiﬁer. We show that the perforı mance of the Na¨ve Bayes depends on the degree of functional dependence between the features ı and labels. We provide a sufﬁcient condition for zero loss of performance, too. Keywords: Bayesian networks, na¨ve Bayes, plug-in classiﬁer, Kolmogorov distance of variation, ı variational learning</p><p>6 0.29098204 <a title="62-lda-6" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>7 0.28690627 <a title="62-lda-7" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>8 0.28587905 <a title="62-lda-8" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>9 0.28167701 <a title="62-lda-9" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>10 0.2804296 <a title="62-lda-10" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>11 0.27929157 <a title="62-lda-11" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.27902913 <a title="62-lda-12" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>13 0.27901459 <a title="62-lda-13" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>14 0.27891093 <a title="62-lda-14" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>15 0.27878711 <a title="62-lda-15" href="./jmlr-2006-On_Model_Selection_Consistency_of_Lasso.html">66 jmlr-2006-On Model Selection Consistency of Lasso</a></p>
<p>16 0.27842921 <a title="62-lda-16" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>17 0.27799308 <a title="62-lda-17" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.27788004 <a title="62-lda-18" href="./jmlr-2006-Pattern_Recognition_for__Conditionally_Independent_Data.html">73 jmlr-2006-Pattern Recognition for  Conditionally Independent Data</a></p>
<p>19 0.27765617 <a title="62-lda-19" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>20 0.27658349 <a title="62-lda-20" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
