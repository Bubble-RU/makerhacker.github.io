<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>69 jmlr-2006-One-Class Novelty Detection for Seizure Analysis from Intracranial EEG</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-69" href="#">jmlr2006-69</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>69 jmlr-2006-One-Class Novelty Detection for Seizure Analysis from Intracranial EEG</h1>
<br/><p>Source: <a title="jmlr-2006-69-pdf" href="http://jmlr.org/papers/volume7/gardner06a/gardner06a.pdf">pdf</a></p><p>Author: Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos, Brian Litt</p><p>Abstract: This paper describes an application of one-class support vector machine (SVM) novelty detection for detecting seizures in humans. Our technique maps intracranial electroencephalogram (EEG) time series into corresponding novelty sequences by classifying short-time, energy-based statistics computed from one-second windows of data. We train a classifier on epochs of interictal (normal) EEG. During ictal (seizure) epochs of EEG, seizure activity induces distributional changes in feature space that increase the empirical outlier fraction. A hypothesis test determines when the parameter change differs significantly from its nominal value, signaling a seizure detection event. Outputs are gated in a “one-shot” manner using persistence to reduce the false alarm rate of the system. The detector was validated using leave-one-out cross-validation (LOO-CV) on a sample of 41 interictal and 29 ictal epochs, and achieved 97.1% sensitivity, a mean detection latency of ©2005 Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos and Brian Litt GARDNER, KRIEGER, VACHTSEVANOS AND LITT -7.58 seconds, and an asymptotic false positive rate (FPR) of 1.56 false positives per hour (Fp/hr). These results are better than those obtained from a novelty detection technique based on Mahalanobis distance outlier detection, and comparable to the performance of a supervised learning technique used in experimental implantable devices (Echauz et al., 2001). The novelty detection paradigm overcomes three significant limitations of competing methods: the need to collect seizure data, precisely mark seizure onset and offset times, and perform patient-specific parameter tuning for detector training. Keywords: seizure detection, novelty detection, one-class SVM, epilepsy, unsupervised learning 1</p><p>Reference: <a title="jmlr-2006-69-reference" href="../jmlr2006_reference/jmlr-2006-One-Class_Novelty_Detection_for_Seizure_Analysis_from_Intracranial_EEG_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We train a classifier on epochs of interictal (normal) EEG. [sent-13, score-0.349]
</p><p>2 During ictal (seizure) epochs of EEG, seizure activity induces distributional changes in feature space that increase the empirical outlier fraction. [sent-14, score-1.128]
</p><p>3 A hypothesis test determines when the parameter change differs significantly from its nominal value, signaling a seizure detection event. [sent-15, score-0.903]
</p><p>4 The detector was validated using leave-one-out cross-validation (LOO-CV) on a sample of 41 interictal and 29 ictal epochs, and achieved 97. [sent-17, score-0.687]
</p><p>5 1% sensitivity, a mean detection latency of  ©2005 Andrew B. [sent-18, score-0.326]
</p><p>6 These results are better than those obtained from a novelty detection technique based on Mahalanobis distance outlier detection, and comparable to the performance of a supervised learning technique used in experimental implantable devices (Echauz et al. [sent-23, score-0.503]
</p><p>7 The novelty detection paradigm overcomes three significant limitations of competing methods: the need to collect seizure data, precisely mark seizure onset and offset times, and perform patient-specific parameter tuning for detector training. [sent-25, score-2.079]
</p><p>8 Keywords: seizure detection, novelty detection, one-class SVM, epilepsy, unsupervised learning  1  Introduction  Epilepsy, a neurological disorder in which patients suffer from recurring seizures, affects approximately 1% of the world population. [sent-26, score-0.959]
</p><p>9 In spite of available dietary, drug, and surgical treatment options, more than 25% of individuals with epilepsy have seizures that are uncontrollable (Kandel, Schwartz & Jessel, 1991). [sent-30, score-0.292]
</p><p>10 These treatments rely on robust algorithms for seizure detection to perform effectively. [sent-34, score-0.883]
</p><p>11 Over the past 30 years seizure detection technology has matured. [sent-35, score-0.883]
</p><p>12 This paper presents one technique for improving the state of the art in seizure detection by reformulating the task as a time-series novelty detection problem. [sent-39, score-1.283]
</p><p>13 While seizure detection is traditionally considered a supervised learning problem (e. [sent-40, score-0.883]
</p><p>14 , binary classification), an unsupervised approach allows for uniform treatment of seizure detection and prediction, and offers four key advantages for implementation. [sent-42, score-0.883]
</p><p>15 Third, there is no need to collect seizure data for training. [sent-45, score-0.696]
</p><p>16 Finally, there is no need to precisely mark seizure intervals. [sent-49, score-0.677]
</p><p>17 For “properly chosen” features, novelties correspond well with the ictal events of interest, and our EEG time series are successfully segmented in a one-class-from-many manner. [sent-53, score-0.295]
</p><p>18 2  Background  In this section, we present a brief review of seizure-related terminology, the seizure detection literature, and the one-class SVM. [sent-54, score-0.883]
</p><p>19 1 Seizure-Related Terminology Seizure analysis refers collectively to algorithms for seizure detection, seizure prediction, and automatic focus channel identification. [sent-56, score-1.37]
</p><p>20 When multiple channels are considered, the electrode location that exhibits the earliest evidence of seizure activity is labeled the focus channel. [sent-59, score-0.729]
</p><p>21 It is convenient to describe segments of the EEG signals by their temporal proximity to seizure activity. [sent-60, score-0.701]
</p><p>22 The ictal period refers to the time during which a seizure occurs. [sent-61, score-0.947]
</p><p>23 The interictal period is the time between successive seizures. [sent-62, score-0.262]
</p><p>24 2 Seizure Detection Early attempts to detect seizures began in the 1970s (Viglione, Ordon & Risch, 1970; Liss, 1973) and primarily considered scalp EEG recordings to detect the clinical (and less frequently) electrographic onset of seizures. [sent-67, score-0.411]
</p><p>25 In 1990, Gotman reported a technique for automated seizure detection that achieved 76% detection accuracy at 1 Fp/hr for 293 seizures recorded from 49 patients (Gotman, 1990). [sent-68, score-1.377]
</p><p>26 Their detector achieved 100% detection accuracy on an 11-seizure database. [sent-70, score-0.393]
</p><p>27 In 1995, Qu and Gotman presented an early seizure warning system trained on template EEG activity that achieved 100% detection accuracy at a mean detection latency of 9. [sent-71, score-1.258]
</p><p>28 claimed 100% detection sensitivity with a mean detection latency of 2. [sent-76, score-0.561]
</p><p>29 Several successful attempts at seizure detection using artificial neural network classifiers have been reported since 1996 (Khorasani & Weng, 1996; Webber et al. [sent-83, score-0.898]
</p><p>30 Evaluation of 31 distinct features (Esteller, 2000) showed that fractal dimension, wavelet packet energy, and mean Teager energy were especially promising for seizure detection. [sent-85, score-0.716]
</p><p>31 In 2001, Esteller reported a detector based on the line length feature that achieved a mean 1027  GARDNER, KRIEGER, VACHTSEVANOS AND LITT  detection latency of 4. [sent-86, score-0.536]
</p><p>32 , subsequently reported a similar detector based upon this work that achieved 97% sensitivity at a mean detection latency of 5. [sent-92, score-0.542]
</p><p>33 The NeuroPace detector claims represent the state of the art in seizure detection performance. [sent-96, score-1.07]
</p><p>34 More complete reviews of the seizure detection and prediction literature are available elsewhere (Litt & Echauz, 2002; Gardner, 2004). [sent-97, score-0.883]
</p><p>35 3  Methodology  In this section, we describe and discuss the experimental methods for detecting seizures under a novelty detection framework. [sent-140, score-0.6]
</p><p>36 1029  GARDNER, KRIEGER, VACHTSEVANOS AND LITT  model parameters Π = {ν , γ , p , N , T }  persistence z [ n]  preprocess  feature extraction  novelty detection  parameter estimation  IEEG  Figure 2:  The seizure analysis architecture. [sent-142, score-1.165]
</p><p>37 1 Human Data Preparation The data analyzed were selected from intracranial EEG recordings of epilepsy patients implanted as part of standard evaluation for epilepsy surgery. [sent-145, score-0.349]
</p><p>38 Five consecutive patients with seizures arising from the temporal lobe(s) were selected for review, and the corresponding data were expertly and independently marked by two certified epileptologists to indicate UEO and UCO times. [sent-150, score-0.312]
</p><p>39 Ictal epochs were selected from the focus channel for each temporal lobe seizure that a patient exhibited. [sent-153, score-0.865]
</p><p>40 Two patients exhibited some seizures with extra-temporal focal regions: those events were excluded from further analysis. [sent-154, score-0.306]
</p><p>41 2 Feature Extraction Many features have been proposed for seizure analysis (Esteller, 2000; D’Alessandro, 2001; Esteller et al. [sent-160, score-0.677]
</p><p>42 3 One-Class SVM Feature extraction was performed on interictal epochs to generate feature vectors for training. [sent-171, score-0.334]
</p><p>43 1 was chosen consistent with the estimated fraction of ictal data. [sent-179, score-0.296]
</p><p>44 4 Parameter Estimation For a stationary process, the one-class SVM novelty parameter, ν , asymptotically equals the outlier fraction. [sent-182, score-0.274]
</p><p>45 We exploit this property by training on features which strongly discriminate interictal from ictal EEG: features are stationary during interictal periods, but change markedly during periods of seizure activity, causing significant changes in the empirical outlier fraction. [sent-183, score-1.55]
</p><p>46 We assumed that ν = ν 0 for interictal EEG, and ν = ν 1 > ν 0 for ictal EEG. [sent-185, score-0.5]
</p><p>47 We then used this estimate to compute a seizure event indicator variable,  ˆ z [ k ] = sgn (ν − C )  (8)  where z = +1 if a seizure is indicated or z = −1 otherwise, and C ∈ [ 0,1] is a threshold parameter. [sent-188, score-1.373]
</p><p>48 5 Persistence (Detector Refractory Period) During early experiments we observed that the detector tended to generate novelty events (i. [sent-204, score-0.399]
</p><p>49 , “fire”) in bursts, with increasing frequency near seizure onset. [sent-206, score-0.677]
</p><p>50 This behavior may indicate the presence of preictal states, periods of EEG activity that are likely to transition from interictal to ictal state. [sent-207, score-0.555]
</p><p>51 Persistence offers an improvement to the basic system beyond false positive rate improvement: it allows for the characterization of the detector over a range of detection time horizons. [sent-213, score-0.466]
</p><p>52 As persistence decreases, one expects the false positive rate to increase and the detection latency to approach zero seconds. [sent-214, score-0.448]
</p><p>53 Conversely, as persistence increases, one expects the false positive rate to decrease, asymptotically approaching a value determined jointly by the novelty parameters of the system (some fraction of the data will always be novel) and the actual novelty rate due to epileptiform activity. [sent-215, score-0.568]
</p><p>54 We heuristically set the detector persistence to TR = 180 seconds for our experiments. [sent-217, score-0.285]
</p><p>55 1032  ONE-CLASS NOVELTY DETECTION FOR SEIZURE ANALYSIS FROM IEEG  TR  Figure 3:  Examples of persistence for improving detector false alarm performance. [sent-218, score-0.344]
</p><p>56 (Top) Ictal epoch showing seizure activity (red, diagonal hatching). [sent-219, score-0.734]
</p><p>57 Training was only performed using interictal epochs, however, testing was performed on each ictal segment, in addition to the withheld interictal epoch. [sent-224, score-0.746]
</p><p>58 This scheme yields C ( N BL ,1)  interictal- and C ( N BL ,1) × N SZ ictal statistics per patient, where N BL , N SZ are the patient-specific  number of interictal and ictal epochs, respectively. [sent-225, score-0.754]
</p><p>59 From these statistics we estimate three key performance metrics: sensitivity, false positive rate, and mean detection latency. [sent-226, score-0.295]
</p><p>60 A block true positive occurs when the detector output, after applying persistence, correctly identifies an interval containing a seizure onset (c. [sent-228, score-0.962]
</p><p>61 Block false negatives and false positives occur when the detector incorrectly labels interictal and ictal intervals, respectively. [sent-231, score-0.86]
</p><p>62 1033  GARDNER, KRIEGER, VACHTSEVANOS AND LITT  τ (i)  τ (ii) T  (iii)  (iv)  (v)  (vi)  Figure 4:  Temporal relationships considered in detector evaluation: intervals representing detected novelty (blue, vertical hatching) and ictal activity (red, diagonal hatching). [sent-232, score-0.685]
</p><p>63 Mean detection latency (11) measures detector responsiveness:  µτ =  1 N  N  ∑τ i =1  i  (11)  where τ i is the detection latency of each detected seizure. [sent-242, score-0.828]
</p><p>64 A negative latency indicates seizure event detection prior to the expert-labeled onset time. [sent-243, score-1.104]
</p><p>65 7 Benchmark Novelty Detection To provide a reference for the relative performance of our algorithm, and the general application of unsupervised learning to the seizure detection problem, we implemented a simple benchmark novelty detection algorithm. [sent-245, score-1.306]
</p><p>66 4  Results  In this section we present the results of both seizure detection approaches. [sent-259, score-0.883]
</p><p>67 76  a  All false positives occurred on a single ictal epoch. [sent-317, score-0.354]
</p><p>68 Several seizure onsets were originally mislabeled by as much as 110 seconds. [sent-318, score-0.677]
</p><p>69 Bottom row of table summarizes aggregate  We estimate the FPR over interictal EEG from the data in Table 1 by dividing FPBL by the epoch duration (0. [sent-323, score-0.274]
</p><p>70 Since ictal events are rare, and the aggregate false positive rate on ictal segments is lower than the corresponding rate on interictal segments, we take the interictal FPR as an asymptotic measure of the overall FPR. [sent-327, score-1.091]
</p><p>71 We reviewed the results for those patients (2, 4, and 5) with negative mean detection latencies. [sent-328, score-0.31]
</p><p>72 For each of these patients we found that the distribution of detection latencies was skewed, and a fraction (less than one-third) of the models detected seizures early. [sent-329, score-0.577]
</p><p>73 Representative IEEG time series, novelty sequences, and estimated outlier fractions for interictal- and ictal epochs are shown in Figures 5 and 6. [sent-337, score-0.593]
</p><p>74 As expected, the outlier fraction remained near its (small) nominal value except during periods of seizure activity. [sent-338, score-0.845]
</p><p>75 Onsets were detected quickly, and the entire seizure event—not just the onset— was correctly identified as novel. [sent-339, score-0.698]
</p><p>76 The near-zero false negative rate (FNR) of the detector was surprising because the data used for training originated from unknown states of consciousness (e. [sent-340, score-0.26]
</p><p>77 Typically, seizure detection performance is drastically affected by patient state-of-consciousness; evaluation on larger data sets with concomitant sleep staging information will provide a better estimate of the true FNR. [sent-343, score-0.943]
</p><p>78 (Top) IEEG signal, (Middle) frame-wise output of the novelty detector, z , (Bottom) estimated outlier fraction (dashed line is 0. [sent-345, score-0.316]
</p><p>79 The earliest electrographic change is visible as the beginning of the pinched region prior to the high-amplitude seizure onset. [sent-351, score-0.723]
</p><p>80 The UEO occurs at time zero, (Middle) frame-wise output of the novelty detector, z , (Bottom) estimated outlier fraction and 0. [sent-352, score-0.316]
</p><p>81 The detector has a latency of about 3 seconds in this example. [sent-354, score-0.324]
</p><p>82 The SVM detector’s mean detection latency outperformed all previously reported seizure detection algorithms. [sent-355, score-1.209]
</p><p>83 It should be noted, however, that this result is attributable to the large fraction of seizures (27%) that were detected early. [sent-356, score-0.263]
</p><p>84 While they did not perform cross-validation, and optimized in-sample for each patient, their reported results—a  1038  ONE-CLASS NOVELTY DETECTION FOR SEIZURE ANALYSIS FROM IEEG  mean detection latency of 5. [sent-362, score-0.326]
</p><p>85 Both techniques are surprisingly effective at seizure detection, but the one-class SVM method performed consistently better, especially with respect to false positive rate. [sent-369, score-0.75]
</p><p>86 Figure 7 shows the marginal distributions of features for both interictal and ictal data. [sent-371, score-0.5]
</p><p>87 05  0  0  6 feature value  Figure 7:  1 2  Representative marginals of the feature vector—E (solid blue), TE (dashed red), CL (dotted green)—for patient 5 corresponding to interictal (top) and ictal (bottom) frames. [sent-380, score-0.606]
</p><p>88 1039  GARDNER, KRIEGER, VACHTSEVANOS AND LITT  Figure 8:  Representative isosurfaces in interictal feature space produced by each method. [sent-381, score-0.269]
</p><p>89 Both approaches, SVM and Mahalanobis, find regions, S1 and S2 , in feature space that include 90% of the observations from interictal data. [sent-385, score-0.269]
</p><p>90 2 Detector Output Analysis We analyzed a sample of 850 interictal detector outputs and confirmed that the empirical outlier fraction equaled its nominal value, 0. [sent-392, score-0.595]
</p><p>91 5 for the probabilities of a novelty occurrence during ictal epochs. [sent-397, score-0.448]
</p><p>92 Empirically, the conditional probability of a novel detector output given a previous novelty output increases dramatically from 0. [sent-405, score-0.381]
</p><p>93 This analysis suggests that the detector output sequence obeys a Markov process where the probability at each point in time of a novelty is P ( zt = −1) = ν 0 = 0. [sent-408, score-0.4]
</p><p>94 5  Conclusions  Traditional approaches to seizure detection rely on binary classification. [sent-434, score-0.883]
</p><p>95 They require seizure data for training, which is difficult and invasive to collect, and do not address the class imbalance problem between interictal and ictal EEG, as less than 1% of EEG data from epileptic patients is seizure-related. [sent-435, score-1.296]
</p><p>96 These approaches assume that seizures develop in a consistent manner and seek to identify features and architectures that discriminate seizure EEG from “other” EEG. [sent-436, score-0.877]
</p><p>97 In contrast, we have presented a technique for seizure detection based on novelty detection that operates by modeling the dominant data class, interictal EEG, and declaring outliers to this class as seizure events. [sent-437, score-2.206]
</p><p>98 The success of our method relies on detecting change points in the empirical outlier fraction with respect to a feature space that strongly discriminates interictal from ictal EEG. [sent-438, score-0.645]
</p><p>99 While the false positive performance of the detector is not as good as other reported algorithms, this may be attributable to the presence of subclinical seizures, or other nonictal anomalies in the data (e. [sent-441, score-0.26]
</p><p>100 In this setting, the need to prevent seizures (avoid false negative events), and the apparent relative harmlessness of false positive stimulations, encourage making the detector hypersensitive. [sent-447, score-0.533]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('seizure', 0.677), ('ictal', 0.254), ('interictal', 0.246), ('detection', 0.206), ('seizures', 0.2), ('novelty', 0.194), ('detector', 0.187), ('eeg', 0.17), ('ieeg', 0.161), ('gardner', 0.123), ('litt', 0.123), ('latency', 0.104), ('krieger', 0.1), ('onset', 0.098), ('epilepsy', 0.092), ('vachtsevanos', 0.092), ('patients', 0.088), ('outlier', 0.08), ('esteller', 0.077), ('false', 0.073), ('epochs', 0.065), ('persistence', 0.065), ('gotman', 0.062), ('patient', 0.06), ('echauz', 0.054), ('intracranial', 0.054), ('svm', 0.052), ('fraction', 0.042), ('classification', 0.04), ('fpr', 0.039), ('bl', 0.038), ('neurology', 0.038), ('osorio', 0.038), ('ueo', 0.038), ('classifier', 0.038), ('clinical', 0.035), ('seconds', 0.033), ('qu', 0.033), ('mahalanobis', 0.033), ('atlanta', 0.031), ('epileptic', 0.031), ('fpbl', 0.031), ('refractory', 0.031), ('sz', 0.031), ('sensitivity', 0.029), ('activity', 0.029), ('detections', 0.029), ('epoch', 0.028), ('positives', 0.027), ('periods', 0.026), ('temporal', 0.024), ('earliest', 0.023), ('georgia', 0.023), ('feature', 0.023), ('electrodes', 0.023), ('electroencephalography', 0.023), ('electrographic', 0.023), ('epilepsia', 0.023), ('fpsz', 0.023), ('frei', 0.023), ('hatching', 0.023), ('implantable', 0.023), ('lobe', 0.023), ('mfp', 0.023), ('neuropace', 0.023), ('novelties', 0.023), ('recordings', 0.023), ('stimulation', 0.023), ('teager', 0.023), ('uco', 0.023), ('electrical', 0.023), ('benchmark', 0.023), ('energy', 0.023), ('significant', 0.021), ('detected', 0.021), ('nominal', 0.02), ('iid', 0.02), ('outputs', 0.02), ('warning', 0.02), ('efficient', 0.02), ('therapy', 0.02), ('dissertation', 0.02), ('te', 0.02), ('latencies', 0.02), ('zt', 0.019), ('alarm', 0.019), ('collect', 0.019), ('event', 0.019), ('producing', 0.018), ('events', 0.018), ('tp', 0.018), ('detect', 0.016), ('mean', 0.016), ('channel', 0.016), ('period', 0.016), ('five', 0.016), ('device', 0.016), ('fn', 0.016), ('abba', 0.015), ('artificial', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="69-tfidf-1" href="./jmlr-2006-One-Class_Novelty_Detection_for_Seizure_Analysis_from_Intracranial_EEG.html">69 jmlr-2006-One-Class Novelty Detection for Seizure Analysis from Intracranial EEG</a></p>
<p>Author: Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos, Brian Litt</p><p>Abstract: This paper describes an application of one-class support vector machine (SVM) novelty detection for detecting seizures in humans. Our technique maps intracranial electroencephalogram (EEG) time series into corresponding novelty sequences by classifying short-time, energy-based statistics computed from one-second windows of data. We train a classifier on epochs of interictal (normal) EEG. During ictal (seizure) epochs of EEG, seizure activity induces distributional changes in feature space that increase the empirical outlier fraction. A hypothesis test determines when the parameter change differs significantly from its nominal value, signaling a seizure detection event. Outputs are gated in a “one-shot” manner using persistence to reduce the false alarm rate of the system. The detector was validated using leave-one-out cross-validation (LOO-CV) on a sample of 41 interictal and 29 ictal epochs, and achieved 97.1% sensitivity, a mean detection latency of ©2005 Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos and Brian Litt GARDNER, KRIEGER, VACHTSEVANOS AND LITT -7.58 seconds, and an asymptotic false positive rate (FPR) of 1.56 false positives per hour (Fp/hr). These results are better than those obtained from a novelty detection technique based on Mahalanobis distance outlier detection, and comparable to the performance of a supervised learning technique used in experimental implantable devices (Echauz et al., 2001). The novelty detection paradigm overcomes three significant limitations of competing methods: the need to collect seizure data, precisely mark seizure onset and offset times, and perform patient-specific parameter tuning for detector training. Keywords: seizure detection, novelty detection, one-class SVM, epilepsy, unsupervised learning 1</p><p>2 0.074632525 <a title="69-tfidf-2" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>Author: Hichem Sahbi, Donald Geman</p><p>Abstract: We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly ﬁner subsets. The hierarchy is traversed coarse-to-ﬁne and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This “free network” is then perturbed, cell by cell, into another network, which is “graded” in two ways: ﬁrst, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-speciﬁc SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network. Keywords: statistical learning, hierarchy of classiﬁers, coarse-to-ﬁne computation, support vector machines, face detection</p><p>3 0.054993711 <a title="69-tfidf-3" href="./jmlr-2006-%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">97 jmlr-2006- (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Charles V. Wright, Fabian Monrose, Gerald M. Masson</p><p>Abstract: Several fundamental security mechanisms for restricting access to network resources rely on the ability of a reference monitor to inspect the contents of trafﬁc as it traverses the network. However, with the increasing popularity of cryptographic protocols, the traditional means of inspecting packet contents to enforce security policies is no longer a viable approach as message contents are concealed by encryption. In this paper, we investigate the extent to which common application protocols can be identiﬁed using only the features that remain intact after encryption—namely packet size, timing, and direction. We ﬁrst present what we believe to be the ﬁrst exploratory look at protocol identiﬁcation in encrypted tunnels which carry trafﬁc from many TCP connections simultaneously, using only post-encryption observable features. We then explore the problem of protocol identiﬁcation in individual encrypted TCP connections, using much less data than in other recent approaches. The results of our evaluation show that our classiﬁers achieve accuracy greater than 90% for several protocols in aggregate trafﬁc, and, for most protocols, greater than 80% when making ﬁne-grained classiﬁcations on single connections. Moreover, perhaps most surprisingly, we show that one can even estimate the number of live connections in certain classes of encrypted tunnels to within, on average, better than 20%. Keywords: trafﬁc classiﬁcation, hidden Markov models, network security</p><p>4 0.047386181 <a title="69-tfidf-4" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>5 0.024029331 <a title="69-tfidf-5" href="./jmlr-2006-Machine_Learning_for_Computer_Security%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">59 jmlr-2006-Machine Learning for Computer Security    (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Philip K. Chan, Richard P. Lippmann</p><p>Abstract: The prevalent use of computers and internet has enhanced the quality of life for many people, but it has also attracted undesired attempts to undermine these systems. This special topic contains several research studies on how machine learning algorithms can help improve the security of computer systems. Keywords: computer security, spam, images with embedded text, malicious executables, network protocols, encrypted trafﬁc</p><p>6 0.0232797 <a title="69-tfidf-6" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>7 0.019549653 <a title="69-tfidf-7" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.018033454 <a title="69-tfidf-8" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.017349068 <a title="69-tfidf-9" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.016389085 <a title="69-tfidf-10" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>11 0.016165838 <a title="69-tfidf-11" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.015923049 <a title="69-tfidf-12" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>13 0.0158868 <a title="69-tfidf-13" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>14 0.015639005 <a title="69-tfidf-14" href="./jmlr-2006-Linear_State-Space_Models_for_Blind_Source_Separation.html">57 jmlr-2006-Linear State-Space Models for Blind Source Separation</a></p>
<p>15 0.014504707 <a title="69-tfidf-15" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>16 0.014484707 <a title="69-tfidf-16" href="./jmlr-2006-Adaptive_Prototype_Learning_Algorithms%3A_Theoretical_and_Experimental_Studies.html">13 jmlr-2006-Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies</a></p>
<p>17 0.014167499 <a title="69-tfidf-17" href="./jmlr-2006-Nonparametric_Quantile_Estimation.html">65 jmlr-2006-Nonparametric Quantile Estimation</a></p>
<p>18 0.013942054 <a title="69-tfidf-18" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>19 0.013673181 <a title="69-tfidf-19" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.013242706 <a title="69-tfidf-20" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.081), (1, -0.023), (2, 0.027), (3, 0.059), (4, -0.001), (5, -0.008), (6, -0.105), (7, -0.174), (8, 0.012), (9, -0.006), (10, -0.102), (11, -0.056), (12, 0.032), (13, -0.039), (14, 0.047), (15, -0.056), (16, 0.053), (17, 0.007), (18, -0.005), (19, 0.12), (20, -0.017), (21, -0.084), (22, -0.025), (23, 0.043), (24, 0.204), (25, 0.049), (26, -0.047), (27, 0.061), (28, 0.235), (29, 0.081), (30, -0.055), (31, 0.158), (32, 0.406), (33, 0.099), (34, -0.015), (35, 0.12), (36, 0.122), (37, -0.117), (38, 0.041), (39, 0.142), (40, -0.112), (41, -0.371), (42, 0.115), (43, 0.083), (44, -0.005), (45, 0.206), (46, -0.041), (47, 0.014), (48, 0.012), (49, -0.227)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97670883 <a title="69-lsi-1" href="./jmlr-2006-One-Class_Novelty_Detection_for_Seizure_Analysis_from_Intracranial_EEG.html">69 jmlr-2006-One-Class Novelty Detection for Seizure Analysis from Intracranial EEG</a></p>
<p>Author: Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos, Brian Litt</p><p>Abstract: This paper describes an application of one-class support vector machine (SVM) novelty detection for detecting seizures in humans. Our technique maps intracranial electroencephalogram (EEG) time series into corresponding novelty sequences by classifying short-time, energy-based statistics computed from one-second windows of data. We train a classifier on epochs of interictal (normal) EEG. During ictal (seizure) epochs of EEG, seizure activity induces distributional changes in feature space that increase the empirical outlier fraction. A hypothesis test determines when the parameter change differs significantly from its nominal value, signaling a seizure detection event. Outputs are gated in a “one-shot” manner using persistence to reduce the false alarm rate of the system. The detector was validated using leave-one-out cross-validation (LOO-CV) on a sample of 41 interictal and 29 ictal epochs, and achieved 97.1% sensitivity, a mean detection latency of ©2005 Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos and Brian Litt GARDNER, KRIEGER, VACHTSEVANOS AND LITT -7.58 seconds, and an asymptotic false positive rate (FPR) of 1.56 false positives per hour (Fp/hr). These results are better than those obtained from a novelty detection technique based on Mahalanobis distance outlier detection, and comparable to the performance of a supervised learning technique used in experimental implantable devices (Echauz et al., 2001). The novelty detection paradigm overcomes three significant limitations of competing methods: the need to collect seizure data, precisely mark seizure onset and offset times, and perform patient-specific parameter tuning for detector training. Keywords: seizure detection, novelty detection, one-class SVM, epilepsy, unsupervised learning 1</p><p>2 0.4276481 <a title="69-lsi-2" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>Author: Hichem Sahbi, Donald Geman</p><p>Abstract: We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly ﬁner subsets. The hierarchy is traversed coarse-to-ﬁne and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This “free network” is then perturbed, cell by cell, into another network, which is “graded” in two ways: ﬁrst, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-speciﬁc SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network. Keywords: statistical learning, hierarchy of classiﬁers, coarse-to-ﬁne computation, support vector machines, face detection</p><p>3 0.23849991 <a title="69-lsi-3" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>4 0.20034996 <a title="69-lsi-4" href="./jmlr-2006-%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">97 jmlr-2006- (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Charles V. Wright, Fabian Monrose, Gerald M. Masson</p><p>Abstract: Several fundamental security mechanisms for restricting access to network resources rely on the ability of a reference monitor to inspect the contents of trafﬁc as it traverses the network. However, with the increasing popularity of cryptographic protocols, the traditional means of inspecting packet contents to enforce security policies is no longer a viable approach as message contents are concealed by encryption. In this paper, we investigate the extent to which common application protocols can be identiﬁed using only the features that remain intact after encryption—namely packet size, timing, and direction. We ﬁrst present what we believe to be the ﬁrst exploratory look at protocol identiﬁcation in encrypted tunnels which carry trafﬁc from many TCP connections simultaneously, using only post-encryption observable features. We then explore the problem of protocol identiﬁcation in individual encrypted TCP connections, using much less data than in other recent approaches. The results of our evaluation show that our classiﬁers achieve accuracy greater than 90% for several protocols in aggregate trafﬁc, and, for most protocols, greater than 80% when making ﬁne-grained classiﬁcations on single connections. Moreover, perhaps most surprisingly, we show that one can even estimate the number of live connections in certain classes of encrypted tunnels to within, on average, better than 20%. Keywords: trafﬁc classiﬁcation, hidden Markov models, network security</p><p>5 0.13909164 <a title="69-lsi-5" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>Author: Mingrui Wu, Bernhard Schölkopf, Gökhan Bakır</p><p>Abstract: Many kernel learning algorithms, including support vector machines, result in a kernel machine, such as a kernel classiﬁer, whose key component is a weight vector in a feature space implicitly introduced by a positive deﬁnite kernel function. This weight vector is usually obtained by solving a convex optimization problem. Based on this fact we present a direct method to build sparse kernel learning algorithms by adding one more constraint to the original convex optimization problem, such that the sparseness of the resulting kernel machine is explicitly controlled while at the same time performance is kept as high as possible. A gradient based approach is provided to solve this modiﬁed optimization problem. Applying this method to the support vectom machine results in a concrete algorithm for building sparse large margin classiﬁers. These classiﬁers essentially ﬁnd a discriminating subspace that can be spanned by a small number of vectors, and in this subspace, the diﬀerent classes of data are linearly well separated. Experimental results over several classiﬁcation benchmarks demonstrate the eﬀectiveness of our approach. Keywords: sparse learning, sparse large margin classiﬁers, kernel learning algorithms, support vector machine, kernel Fisher discriminant</p><p>6 0.12752698 <a title="69-lsi-6" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>7 0.12660974 <a title="69-lsi-7" href="./jmlr-2006-A_Linear_Non-Gaussian_Acyclic_Model_for_Causal_Discovery.html">4 jmlr-2006-A Linear Non-Gaussian Acyclic Model for Causal Discovery</a></p>
<p>8 0.11914607 <a title="69-lsi-8" href="./jmlr-2006-Linear_State-Space_Models_for_Blind_Source_Separation.html">57 jmlr-2006-Linear State-Space Models for Blind Source Separation</a></p>
<p>9 0.10987862 <a title="69-lsi-9" href="./jmlr-2006-MinReg%3A_A_Scalable_Algorithm_for_Learning_Parsimonious_Regulatory_Networks_in_Yeast_and_Mammals.html">62 jmlr-2006-MinReg: A Scalable Algorithm for Learning Parsimonious Regulatory Networks in Yeast and Mammals</a></p>
<p>10 0.088191956 <a title="69-lsi-10" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.086009644 <a title="69-lsi-11" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.085572824 <a title="69-lsi-12" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>13 0.085179031 <a title="69-lsi-13" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>14 0.081431732 <a title="69-lsi-14" href="./jmlr-2006-Adaptive_Prototype_Learning_Algorithms%3A_Theoretical_and_Experimental_Studies.html">13 jmlr-2006-Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies</a></p>
<p>15 0.08064419 <a title="69-lsi-15" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.080005802 <a title="69-lsi-16" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.071619906 <a title="69-lsi-17" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>18 0.069775149 <a title="69-lsi-18" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>19 0.06203904 <a title="69-lsi-19" href="./jmlr-2006-On_Model_Selection_Consistency_of_Lasso.html">66 jmlr-2006-On Model Selection Consistency of Lasso</a></p>
<p>20 0.059602026 <a title="69-lsi-20" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.014), (36, 0.051), (45, 0.021), (50, 0.018), (61, 0.03), (63, 0.045), (64, 0.526), (68, 0.014), (70, 0.012), (76, 0.01), (78, 0.018), (81, 0.028), (84, 0.015), (90, 0.018), (91, 0.018), (96, 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85622871 <a title="69-lda-1" href="./jmlr-2006-One-Class_Novelty_Detection_for_Seizure_Analysis_from_Intracranial_EEG.html">69 jmlr-2006-One-Class Novelty Detection for Seizure Analysis from Intracranial EEG</a></p>
<p>Author: Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos, Brian Litt</p><p>Abstract: This paper describes an application of one-class support vector machine (SVM) novelty detection for detecting seizures in humans. Our technique maps intracranial electroencephalogram (EEG) time series into corresponding novelty sequences by classifying short-time, energy-based statistics computed from one-second windows of data. We train a classifier on epochs of interictal (normal) EEG. During ictal (seizure) epochs of EEG, seizure activity induces distributional changes in feature space that increase the empirical outlier fraction. A hypothesis test determines when the parameter change differs significantly from its nominal value, signaling a seizure detection event. Outputs are gated in a “one-shot” manner using persistence to reduce the false alarm rate of the system. The detector was validated using leave-one-out cross-validation (LOO-CV) on a sample of 41 interictal and 29 ictal epochs, and achieved 97.1% sensitivity, a mean detection latency of ©2005 Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos and Brian Litt GARDNER, KRIEGER, VACHTSEVANOS AND LITT -7.58 seconds, and an asymptotic false positive rate (FPR) of 1.56 false positives per hour (Fp/hr). These results are better than those obtained from a novelty detection technique based on Mahalanobis distance outlier detection, and comparable to the performance of a supervised learning technique used in experimental implantable devices (Echauz et al., 2001). The novelty detection paradigm overcomes three significant limitations of competing methods: the need to collect seizure data, precisely mark seizure onset and offset times, and perform patient-specific parameter tuning for detector training. Keywords: seizure detection, novelty detection, one-class SVM, epilepsy, unsupervised learning 1</p><p>2 0.3756724 <a title="69-lda-2" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>Author: Robert Castelo, Alberto Roverato</p><p>Abstract: Learning of large-scale networks of interactions from microarray data is an important and challenging problem in bioinformatics. A widely used approach is to assume that the available data constitute a random sample from a multivariate distribution belonging to a Gaussian graphical model. As a consequence, the prime objects of inference are full-order partial correlations which are partial correlations between two variables given the remaining ones. In the context of microarray data the number of variables exceed the sample size and this precludes the application of traditional structure learning procedures because a sampling version of full-order partial correlations does not exist. In this paper we consider limited-order partial correlations, these are partial correlations computed on marginal distributions of manageable size, and provide a set of rules that allow one to assess the usefulness of these quantities to derive the independence structure of the underlying Gaussian graphical model. Furthermore, we introduce a novel structure learning procedure based on a quantity, obtained from limited-order partial correlations, that we call the non-rejection rate. The applicability and usefulness of the procedure are demonstrated by both simulated and real data. Keywords: Gaussian distribution, gene network, graphical model, microarray data, non-rejection rate, partial correlation, small-sample inference</p><p>3 0.19262519 <a title="69-lda-3" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>4 0.18912464 <a title="69-lda-4" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram. Keywords: spectral clustering, blind source separation, computational auditory scene analysis</p><p>5 0.18569721 <a title="69-lda-5" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>Author: Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou</p><p>Abstract: We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the ﬁrst time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction. html. Keywords: transduction, transductive SVMs, semi-supervised learning, CCCP</p><p>6 0.18282729 <a title="69-lda-6" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.18279839 <a title="69-lda-7" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>8 0.18192837 <a title="69-lda-8" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.18164445 <a title="69-lda-9" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>10 0.18141741 <a title="69-lda-10" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>11 0.18016556 <a title="69-lda-11" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.17961505 <a title="69-lda-12" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>13 0.17740154 <a title="69-lda-13" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.17710194 <a title="69-lda-14" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.17669347 <a title="69-lda-15" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>16 0.17653242 <a title="69-lda-16" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>17 0.17571692 <a title="69-lda-17" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>18 0.17509162 <a title="69-lda-18" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<p>19 0.17490906 <a title="69-lda-19" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.17398654 <a title="69-lda-20" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
