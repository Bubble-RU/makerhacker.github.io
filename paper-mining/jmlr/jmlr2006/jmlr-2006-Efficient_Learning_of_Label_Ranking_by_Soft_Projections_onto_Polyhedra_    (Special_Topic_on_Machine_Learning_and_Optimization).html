<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-26" href="#">jmlr2006-26</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-26-pdf" href="http://jmlr.org/papers/volume7/shalev-shwartz06a/shalev-shwartz06a.pdf">pdf</a></p><p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><p>Reference: <a title="jmlr-2006-26-reference" href="../jmlr2006_reference/jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. [sent-8, score-0.496]
</p><p>2 This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. [sent-10, score-0.252]
</p><p>3 We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. [sent-13, score-0.347]
</p><p>4 The preferences over the labels can also be described as a weighted directed graph: the nodes of the graph are the labels and weighted edges encode pairwise preferences over pairs of labels. [sent-43, score-0.433]
</p><p>5 Our generalized hinge loss contrasts the predicted preferences graph and the target preferences graph by decomposing the target graph into bipartite sub-graphs. [sent-51, score-0.588]
</p><p>6 Generalizing the iterative algorithm proposed by Hildreth (1957) (see also Censor and Zenios (1997)) from half-space constraints to polyhedra constraints, we also derive and analyze an iterative algorithm which on each iteration performs a soft projection onto a single polyhedron. [sent-55, score-0.47]
</p><p>7 The end result is a fast optimization procedure for label ranking from general real-valued feedback. [sent-56, score-0.283]
</p><p>8 The learning goal is to learn a ranking function of the form f : X → Rk which takes x as an input instance and returns a ranking vector f(x) ∈ Rk . [sent-103, score-0.393]
</p><p>9 Analogous to the target vector, γ, we say that label y is more relevant than label y′ with respect to the predicted ranking if fy (x) > fy′ (x). [sent-105, score-0.356]
</p><p>10 We assume that the label-ranking functions are linear, namely, fr (x) = wr · x , where each wr is a vector in Rn and X ⊆ Rn . [sent-106, score-0.409]
</p><p>11 Clearly, we want the loss of a predicted ranking to be small if it expresses similar preferences over pairs as the given label-ranking. [sent-112, score-0.318]
</p><p>12 The multilabel classiﬁcation problem is a special case of the label ranking problem discussed in this paper and can be realized by setting γr = 1 if the r’th label is relevant and otherwise deﬁning γr = 0. [sent-145, score-0.357]
</p><p>13 For instance, suppose we need to induce a ranking over 4 labels where the target label ranking is (−1, 2, 0, 0). [sent-194, score-0.5]
</p><p>14 In most ranking and search applications such a predicted ranking would be perceived as being right on target since the preferences it expresses over pairs are on par with the target ranking. [sent-196, score-0.539]
</p><p>15 Proceeding to more complex decision problems, the task of multilabel classiﬁcation or ranking is concerned with predicting a set or relevant labels or ranking the labels in accordance to their relevance to the input instance. [sent-213, score-0.489]
</p><p>16 Elisseeff and Weston focus on a feedback vector γ which constitutes a bipartite graph by itself and deﬁne a constrained optimization problem with a separate slack variable for each edge in the graph. [sent-217, score-0.296]
</p><p>17 Our approach is substantially more general as it allows much richer and ﬂexible ways to decompose the multilabel problem as well as more general label ranking problems. [sent-221, score-0.276]
</p><p>18 Our goal now is to derive and analyze an efﬁcient algorithm for solving the label ranking problem. [sent-224, score-0.268]
</p><p>19 We ﬁrst derive a dual version of the problem deﬁned by Eq. [sent-253, score-0.24]
</p><p>20 Each variable in the dual problem corresponds to an edge in E. [sent-255, score-0.274]
</p><p>21 Thus, the total number of dual variables can be as large as k2 /4. [sent-256, score-0.24]
</p><p>22 We prove that the reduced problem nonetheless attains the same optimum as the original dual problem. [sent-258, score-0.365]
</p><p>23 We next show that the reduced problem can be decoupled into two simpler constrained optimization problems each of which corresponds to one layer in the bipartite graph induced by E. [sent-260, score-0.284]
</p><p>24 1 The Dual Problem To start, we would like to note that the primal objective function is convex and all the primal constraints are linear. [sent-265, score-0.247]
</p><p>25 Therefore, strong duality holds and we can obtain a solution to the primal problem by ﬁnding the solution of its dual problem. [sent-268, score-0.427]
</p><p>26 (7), which amounts to,  L = =  1 k ∑ wy − uy 2 y=1  2  + Cξ +  ∑  τr,s (γr − γs − ξ + ws · x − wr · x) − ζξ  (r,s)∈E  1 k ∑ wy − uy 2 y=1  2  +ξ C−  ∑  τr,s − ζ +  ∑  τr,s (γr − wr · x − γs + ws · x) ,  (r,s)∈E  (r,s)∈E  where τr,s ≥ 0 for all (r, s) ∈ E and ζ ≥ 0. [sent-270, score-0.996]
</p><p>27 To derive the dual problem we now can use the strong duality. [sent-271, score-0.24]
</p><p>28 Since we need to maximize the dual we can rule out the latter case and pose the following constraint on the dual variables, C−  ∑  τr,s − ζ = 0 . [sent-275, score-0.48]
</p><p>29 Using the deﬁnition of the sets A and B we can rewrite the last sum of the Lagrangian as,  ∑  τr,s (γr − wr · x − γs + ws · x) =  r∈A,s∈B  ∑ (γr − wr · x) ∑ τr,s  r∈A  −  s∈B  ∑ (γs − ws · x) ∑ τr,s  s∈B  . [sent-279, score-0.498]
</p><p>30 For all y ∈ A, the above gives the set of constraints, ∇wy L = wy − uy −  ∑ τy,s  s∈B  1574  x = 0 . [sent-284, score-0.268]
</p><p>31 (9)  SOPOPO - S OFT P ROJECTIONS ONTO P OLYHEDRA  Similarly, for y ∈ B we get that,  ∑ τr,y  ∇wy L = wy − uy +  x = 0 . [sent-285, score-0.318]
</p><p>32 (10)  r∈A  Finally, we would like to note that for any label y ∈ A ∪ B we get that wy − uy = 0. [sent-286, score-0.374]
</p><p>33 Summing up, we get that,   uy + (∑s∈B τy,s ) x y ∈ A wy = u −( τ )x y ∈ B  y ∑r∈A r,y uy otherwise  . [sent-288, score-0.459]
</p><p>34 (8) into the Lagrangian and rearranging terms give the following dual objective function, D(τ)  =  1 − x 2 +  2 2  ∑ ∑ τy,s  y∈A  s∈B  1 − x 2  2 2  ∑ ∑ τr,y  y∈B  (12)  r∈A  ∑ (γy − uy · x) ∑ τy,s − ∑ (γy − uy · x) ∑ τr,y s∈B  y∈A  y∈B  . [sent-291, score-0.585]
</p><p>35 r∈A  In summary, the resulting dual problem is, max D(τ) |E|  ∑  s. [sent-292, score-0.24]
</p><p>36 2 Reparametrization of the Dual Problem Each dual variable τr,s corresponds to an edge in E. [sent-296, score-0.274]
</p><p>37 Thus, the number of dual variables may be as large as k2 /4. [sent-297, score-0.24]
</p><p>38 However, the dual objective function depends only on sums of variables τr,s . [sent-298, score-0.303]
</p><p>39 Furthermore, each primal vector wy also depends on sums of dual variables (see Eq. [sent-299, score-0.439]
</p><p>40 (11) can be rewritten using αy and βy as follows,   uy + α y x y ∈ A wy = u − βy x y ∈ B  y uy otherwise  . [sent-304, score-0.409]
</p><p>41 (15)  Overloading our notation and using D(α, β) to denote dual objective function in terms of α and β, we can rewrite the dual objective of Eq. [sent-305, score-0.644]
</p><p>42 (12) as follows, D(α, β) = −  1 x 2  2  ∑ α2 + ∑ β2 y y  y∈A  y∈B  + ∑ (γy − uy · x) αy − ∑ (γy − uy · x) βy . [sent-306, score-0.282]
</p><p>43 In appendix A we show that the reduced problem and the original dual problem from Eq. [sent-314, score-0.288]
</p><p>44 the argument attaining the minimum of g(z; µ) + g(z; ν) we could have calculated the optimal dual variables α⋆ and β⋆ by ﬁrst ﬁnding ρ(z, µ) and ρ(z, ν) and then ﬁnding α and β using Eq. [sent-392, score-0.24]
</p><p>45 In the next sub-section we exploit the properties of the function g to devise an efﬁcient procedure for ﬁnding the optimal value of z and from there the road to the optimal dual variables is clear and simple. [sent-411, score-0.24]
</p><p>46 1578  SOPOPO - S OFT P ROJECTIONS ONTO P OLYHEDRA  I NPUT:  ; target ranking γ ; sets A, B  instance x ∈ X  current prototypes u1 , . [sent-412, score-0.25]
</p><p>47 We now summarize the full algorithm for ﬁnding the optimum of the dual variables and wrap up with its pseudo-code. [sent-427, score-0.274]
</p><p>48 For each i ∈ [p], deﬁne the knots derived from µ i  zi =  ∑ µr − iµi  ,  r=1  and similarly, for each j ∈ [q] deﬁne j  zj = ˜  ∑ νr − jν j  . [sent-443, score-0.308]
</p><p>49 Therefore, as already argued above, the function g(z; µ) + g(z; ν) is also z ˜ piecewise quadratic in [0,C] and its knots are the points in the set,  Q = {zi : zi < C} ∪ {˜ j : z j < C} ∪ {C} . [sent-445, score-0.348]
</p><p>50 Given a knot z we deﬁne R(z) = |{zi : zi ≤ z}| and S(z) = |{˜i : zi ≤ z}| . [sent-448, score-0.535]
</p><p>51 4 we get that the value of the dual objective function at z is, g(z; µ) + g(z; ν) = 1 R(z′ )  2  R(z′ )  ∑ µr − z  p  ∑  + µ2 + r ′ )+1 r=R(z  r=1  1 S(z′ )  S(z′ )  ∑ νr − z  2  p  ∑  +  r=1  ν2 . [sent-451, score-0.353]
</p><p>52 i=1  Therefore, if O(z′ ) ∈ [z′ , N(z′ )], then the global minimum of the dual objective function is attained at O(z′ ). [sent-453, score-0.347]
</p><p>53 We ﬁrst derive the dual of the problem given in Eq. [sent-528, score-0.24]
</p><p>54 1 The Dual Problem First, note that the primal objective function of the general problem is convex and all the primal constraints are linear. [sent-534, score-0.247]
</p><p>55 1 it is simple to show that strong duality holds and a solution to the primal problem can be obtained from the solution of its dual problem. [sent-537, score-0.427]
</p><p>56 To derive the dual problem, we ﬁrst write the Lagrangian,  L=  1 ¯ w 2  2  ki  m  m  +  m  ¯ ∑ Ci ξi + ∑ ∑ λi, j bi, j − ξi − w · ai, j − ∑ ζi ξi , i=1  i=1 j=1  i=1  where λi, j and ζi are non-negative Lagrange multipliers. [sent-538, score-0.301]
</p><p>57 (28)  i=1 j=1  As in the derivation of the dual objective function for a single soft projection, we get that the following must hold at the optimum, ki  ∀i ∈ [m],  ∑ λi, j −Ci − ζi = 0  . [sent-540, score-0.502]
</p><p>58 (29)  j=1  Since λi, j and ζi are non-negative Lagrange multipliers we get that the set of feasible solutions of the dual problem is, S =  λ  ki  ∀i,  ∑ λi, j  ≤ Ci and ∀i, j, λi, j ≥ 0  . [sent-541, score-0.403]
</p><p>59 (29) to further rewrite the Lagrangian gives the dual objective function, 1 D(λ) = − 2  m  ki  2  ∑ ∑ λi, j a  i, j  i=1 j=1  m  +  ki  ∑ ∑ λi, j bi, j  . [sent-544, score-0.463]
</p><p>60 The iterative procedure works in rounds and operates on the dual form of the objective function. [sent-552, score-0.362]
</p><p>61 Let λt denote the vector of dual variables before the tth iteration of the iterative algorithm. [sent-554, score-0.368]
</p><p>62 On the tth iteration of the algorithm, we choose a single example whose index is denoted r and update its dual variables. [sent-557, score-0.309]
</p><p>63 (32)  SOPOPO - S OFT P ROJECTIONS ONTO P OLYHEDRA  ¯ The vector u is equal to the current estimate of w excluding the contribution of the rth set of dual variables. [sent-565, score-0.28]
</p><p>64 j=1  As in the previous derivations of the dual objective functions we also get that, kr  Cr − ζr − ∑ λr, j = 0 , j=1  and thus the Lagrange multipliers must satisfy, kr  ∑ λr, j  ≤ Cr . [sent-580, score-0.585]
</p><p>65 (35) becomes, max λr,·  1 − 2 kr  kr  2  ∑ λr, j a  j=1  ∑ λr, j ≤ Cr  r, j  kr  +  ∑ λr, j (br, j − u · ar, j )  j=1  s. [sent-582, score-0.348]
</p><p>66 (31) we clearly get that on each round we are guaranteed to increase the dual objective function unless we are already at the optimum. [sent-594, score-0.353]
</p><p>67 In the next subsection we show that this iterative paradigm converges to the global optimum of the dual objective function. [sent-595, score-0.396]
</p><p>68 In addition, as we have shown in the previous section, the solution of each soft projection takes the form wa = ua + αa xi and wb = ub − βb xi . [sent-600, score-0.272]
</p><p>69 We denote by Dt the value of the dual objective function before the tth iteration and by ∆t = Dt+1 − Dt the increase in the dual on the tth iteration. [sent-605, score-0.657]
</p><p>70 Last, let D⋆ and λ⋆ denote the optimal value and argument of the dual objective function. [sent-608, score-0.303]
</p><p>71 Our algorithm maximizes the dual objective on each iteration subject to the constraint that for all i = r and j ∈ [ki ], the variables λi, j are kept intact. [sent-609, score-0.327]
</p><p>72 To prove convergence we need the following lemma which says that if the algorithm is at suboptimal solution then it will keep increasing the dual objective on the subsequent iteration. [sent-614, score-0.363]
</p><p>73 Let v = λ⋆ − λ denote the difference between the optimal solution and the current solution and denote h(θ) = D(λ + θv) the value of the dual obtained by moving along the direction  1586  SOPOPO - S OFT P ROJECTIONS ONTO P OLYHEDRA  v from λ. [sent-619, score-0.306]
</p><p>74 Let ∇D denote the gradient of the dual objective at λ. [sent-623, score-0.303]
</p><p>75 (37)  We now rewrite v as the sum of vectors, m  vr, j r = i 0 r=i  v = ∑ zi where zi j = r, i=1  . [sent-625, score-0.512]
</p><p>76 In words, we rewrite v as the sum of vectors each of which corresponds to the dual variables appearing in a single soft-projection problem induced by the ith example. [sent-626, score-0.304]
</p><p>77 From the deﬁnition of zi together with the form of the dual constraints we get that the vector λ + zi is also a feasible solution for the dual problem. [sent-627, score-1.129]
</p><p>78 Since hi is derived from the dual problem by constraining the dual variables to reside on the line λ + θzi , then as the function D, hi is also continuously differentiable. [sent-630, score-0.528]
</p><p>79 Furthermore, ∇D · zi = h′ (0) ≤ 0 for all i which gives, i i m  ∇D · v = ∇D · ∑ zi = i=1  m  ∑ ∇D · zi  ≤ 0 ,  i=1  which contradicts Eq. [sent-632, score-0.736]
</p><p>80 Theorem 6 Let Dt denote the value of the dual objective after the t’th iteration of the algorithm deﬁned in Eq. [sent-635, score-0.327]
</p><p>81 Since the value of the dual problem cannot exceed the value of the primal problem we get that D⋆ < ∞. [sent-648, score-0.362]
</p><p>82 Therefore, the sequence of dual objective values is a monotonic, non-decreasing, and upper bounded sequence, D1 ≤ D2 ≤ . [sent-649, score-0.303]
</p><p>83 The set of feasible dual solutions, S, is a compact set. [sent-659, score-0.292]
</p><p>84 5 when the gap between the primal and dual objective functions went below 0. [sent-743, score-0.375]
</p><p>85 After each iteration of the algorithm, we examined both the increase in the dual objective after the update and the difference between the primal and dual values. [sent-769, score-0.639]
</p><p>86 After about 1000 iterations, which is also the number of examples, the increase in the dual objective becomes miniscule. [sent-773, score-0.303]
</p><p>87 Note in addition that as the number of epochs increases, the increase of the dual objective becomes very small relatively to the duality gap. [sent-775, score-0.352]
</p><p>88 It is common to use the increase of the dual objective as a stopping criterion and the last experiment indicates that this criterion does not necessarily imply convergence. [sent-776, score-0.303]
</p><p>89 Previous large margin approaches for label ranking associate a unique slack variable with each constraint which is induced by a pair of labels. [sent-779, score-0.263]
</p><p>90 Using the all-pair decomposition, the label ranking problem is reduced to a binary classiﬁcation problem. [sent-783, score-0.285]
</p><p>91 005  0 0  5  1000  2000 3000 iterations  4000  0 0  5000  1000  2000 3000 iterations  4000  5000  Figure 8: The increase in the dual objective (left) and the primal-dual gap (right) as a function of the number of iterations of the iterative algorithm in Fig. [sent-793, score-0.362]
</p><p>92 A main obstacle is attributed to the fact that the set of feasible solutions for the dual problem must satisfy the constraint ∑a αa = ∑b βb ≤ C. [sent-796, score-0.292]
</p><p>93 Thus, a sequential minimization algorithm must update at least 4 dual variables on each iteration in order to preserve the feasibility of the dual solution. [sent-797, score-0.504]
</p><p>94 Recall that SOPOPO is designed for projecting onto a polyhedron which is deﬁned according to a complete bipartite graph. [sent-805, score-0.266]
</p><p>95 In addition to the original edges of the bipartite graph supported by A and B we add edges from s to all the nodes in A and from all the nodes in B to t and thus E ′ = (A × B) ∪ ({s} × A) ∪ (B × {t}). [sent-849, score-0.34]
</p><p>96 Therefore, we get that i  i+1  zi+1 =  ∑ µr − (i + 1)µi+1  r=1 i  =  ∑ µr − i µi+1  ∑ µr + µi+1 − µi+1 − i µi+1  =  r=1 i  ∑ µr − i µi  ≥  = zi . [sent-936, score-0.287]
</p><p>97 To show that g is continuous in [0,C] we need to examine all of its knots zi . [sent-944, score-0.308]
</p><p>98 The continuity of the derivative of g is shown by using the same technique of examining the right and left limits at each knot zi for the function, i 2 z − ∑ µr . [sent-947, score-0.298]
</p><p>99 r  i:r∈Bi  i:r∈Ai  Therefore, the dual objective can be rewritten as, 1 k D = − ∑ 2 r=1  2  ∑  αi xi r  −  ∑  βi xi r  i:r∈Bi  i:r∈Ai  k  +  ∑  r=1  ∑  αi γi − r r  i:r∈Ai  ∑  βi γi r r  . [sent-969, score-0.303]
</p><p>100 b∈Bi  Combining the dual deﬁnition with the above constraints gives the reduced problem from Eq. [sent-972, score-0.328]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sopopo', 0.535), ('dual', 0.24), ('zi', 0.237), ('oft', 0.19), ('olyhedra', 0.19), ('loqo', 0.19), ('ranking', 0.181), ('wr', 0.172), ('hwartz', 0.161), ('inger', 0.161), ('rojections', 0.161), ('uy', 0.141), ('wy', 0.127), ('kr', 0.116), ('bipartite', 0.109), ('onto', 0.109), ('preferences', 0.101), ('bi', 0.101), ('val', 0.099), ('ow', 0.094), ('crammer', 0.072), ('primal', 0.072), ('knots', 0.071), ('ai', 0.069), ('rk', 0.065), ('fr', 0.065), ('objective', 0.063), ('wk', 0.062), ('ki', 0.061), ('knot', 0.061), ('ua', 0.061), ('iterative', 0.059), ('ws', 0.058), ('cut', 0.057), ('label', 0.056), ('fs', 0.055), ('graph', 0.055), ('polyhedra', 0.054), ('ub', 0.054), ('edges', 0.053), ('feedback', 0.052), ('feasible', 0.052), ('get', 0.05), ('soft', 0.049), ('duality', 0.049), ('reduced', 0.048), ('censor', 0.048), ('polyhedron', 0.048), ('optimization', 0.046), ('projection', 0.045), ('tth', 0.045), ('labels', 0.044), ('attained', 0.044), ('elisseeff', 0.043), ('ar', 0.043), ('attains', 0.043), ('batch', 0.042), ('cr', 0.041), ('rth', 0.04), ('quadratic', 0.04), ('constraints', 0.04), ('weston', 0.039), ('derivation', 0.039), ('multilabel', 0.039), ('lagrangian', 0.038), ('target', 0.038), ('rewrite', 0.038), ('dt', 0.037), ('loss', 0.036), ('br', 0.036), ('outgoing', 0.036), ('wrapper', 0.036), ('tag', 0.036), ('nodes', 0.035), ('edge', 0.034), ('contradiction', 0.034), ('optimum', 0.034), ('solution', 0.033), ('analyze', 0.031), ('singer', 0.031), ('instance', 0.031), ('decompositions', 0.031), ('monotonically', 0.031), ('shai', 0.03), ('sor', 0.03), ('wb', 0.03), ('exible', 0.03), ('news', 0.029), ('multiclass', 0.028), ('prede', 0.027), ('eighteenth', 0.027), ('musicant', 0.027), ('lemma', 0.027), ('mangasarian', 0.026), ('induced', 0.026), ('fy', 0.025), ('contradicts', 0.025), ('realized', 0.025), ('iteration', 0.024), ('hi', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="26-tfidf-1" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><p>2 0.13015626 <a title="26-tfidf-2" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Ben Taskar, Simon Lacoste-Julien, Michael I. Jordan</p><p>Abstract: We present a simple and scalable algorithm for maximum-margin estimation of structured output models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). The projection step can be solved using dynamic programming or combinatorial algorithms for min-cost convex ﬂow, depending on the structure of the problem. We show that this approach provides a memoryefﬁcient alternative to formulations based on reductions to a quadratic program (QP). We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.1 Keywords: Markov networks, large-margin methods, structured prediction, extragradient, Bregman projections</p><p>3 0.09531673 <a title="26-tfidf-3" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>4 0.090542927 <a title="26-tfidf-4" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>5 0.083849102 <a title="26-tfidf-5" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>Author: Thomas Kämpke</p><p>Abstract: Similarity of edge labeled graphs is considered in the sense of minimum squared distance between corresponding values. Vertex correspondences are established by isomorphisms if both graphs are of equal size and by subisomorphisms if one graph has fewer vertices than the other. Best ﬁt isomorphisms and subisomorphisms amount to solutions of quadratic assignment problems and are computed exactly as well as approximately by minimum cost ﬂow, linear assignment relaxations and related graph algorithms. Keywords: assignment problem, best approximation, branch and bound, inexact graph matching, model data base</p><p>6 0.07672976 <a title="26-tfidf-6" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>7 0.069618337 <a title="26-tfidf-7" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.067550428 <a title="26-tfidf-8" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>9 0.060674656 <a title="26-tfidf-9" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>10 0.056569982 <a title="26-tfidf-10" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.052370723 <a title="26-tfidf-11" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>12 0.051255401 <a title="26-tfidf-12" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.04811471 <a title="26-tfidf-13" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.046229638 <a title="26-tfidf-14" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>15 0.046062283 <a title="26-tfidf-15" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>16 0.045373827 <a title="26-tfidf-16" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.044718817 <a title="26-tfidf-17" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.044091742 <a title="26-tfidf-18" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>19 0.043577258 <a title="26-tfidf-19" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>20 0.041418716 <a title="26-tfidf-20" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.245), (1, -0.089), (2, 0.04), (3, 0.11), (4, 0.027), (5, 0.068), (6, 0.215), (7, 0.141), (8, -0.012), (9, 0.033), (10, -0.089), (11, 0.011), (12, 0.04), (13, 0.055), (14, 0.102), (15, -0.004), (16, 0.019), (17, -0.003), (18, -0.069), (19, -0.273), (20, -0.044), (21, -0.004), (22, -0.031), (23, -0.151), (24, 0.013), (25, -0.212), (26, 0.093), (27, -0.152), (28, 0.13), (29, 0.136), (30, -0.11), (31, 0.013), (32, 0.032), (33, 0.076), (34, 0.0), (35, 0.139), (36, 0.003), (37, 0.072), (38, -0.057), (39, 0.065), (40, 0.059), (41, 0.058), (42, -0.032), (43, 0.042), (44, -0.177), (45, 0.012), (46, 0.095), (47, -0.133), (48, -0.072), (49, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93767095 <a title="26-lsi-1" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><p>2 0.765984 <a title="26-lsi-2" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Ben Taskar, Simon Lacoste-Julien, Michael I. Jordan</p><p>Abstract: We present a simple and scalable algorithm for maximum-margin estimation of structured output models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). The projection step can be solved using dynamic programming or combinatorial algorithms for min-cost convex ﬂow, depending on the structure of the problem. We show that this approach provides a memoryefﬁcient alternative to formulations based on reductions to a quadratic program (QP). We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.1 Keywords: Markov networks, large-margin methods, structured prediction, extragradient, Bregman projections</p><p>3 0.40052474 <a title="26-lsi-3" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>Author: Ross A. Lippert, Ryan M. Rifkin</p><p>Abstract: We consider the problem of Tikhonov regularization with a general convex loss function: this formalism includes support vector machines and regularized least squares. For a family of kernels that includes the Gaussian, parameterized by a “bandwidth” parameter σ, we characterize the limiting ˜ solution as σ → ∞. In particular, we show that if we set the regularization parameter λ = λσ−2p , the regularization term of the Tikhonov problem tends to an indicator function on polynomials of degree ⌊p⌋ (with residual regularization in the case where p ∈ Z). The proof rests on two key ideas: epi-convergence, a notion of functional convergence under which limits of minimizers converge to minimizers of limits, and a value-based formulation of learning, where we work with regularization on the function output values (y) as opposed to the function expansion coefﬁcients in the RKHS. Our result generalizes and uniﬁes previous results in this area. Keywords: Tikhonov regularization, Gaussian kernel, theory, kernel machines</p><p>4 0.36066601 <a title="26-lsi-4" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>5 0.34959933 <a title="26-lsi-5" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>Author: Thomas Kämpke</p><p>Abstract: Similarity of edge labeled graphs is considered in the sense of minimum squared distance between corresponding values. Vertex correspondences are established by isomorphisms if both graphs are of equal size and by subisomorphisms if one graph has fewer vertices than the other. Best ﬁt isomorphisms and subisomorphisms amount to solutions of quadratic assignment problems and are computed exactly as well as approximately by minimum cost ﬂow, linear assignment relaxations and related graph algorithms. Keywords: assignment problem, best approximation, branch and bound, inexact graph matching, model data base</p><p>6 0.33826107 <a title="26-lsi-6" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.32480058 <a title="26-lsi-7" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>8 0.30498594 <a title="26-lsi-8" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>9 0.27666727 <a title="26-lsi-9" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>10 0.24477799 <a title="26-lsi-10" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.23482379 <a title="26-lsi-11" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>12 0.2304621 <a title="26-lsi-12" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.22746454 <a title="26-lsi-13" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.21538183 <a title="26-lsi-14" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.21106471 <a title="26-lsi-15" href="./jmlr-2006-Adaptive_Prototype_Learning_Algorithms%3A_Theoretical_and_Experimental_Studies.html">13 jmlr-2006-Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies</a></p>
<p>16 0.20790841 <a title="26-lsi-16" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>17 0.20562527 <a title="26-lsi-17" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>18 0.19634537 <a title="26-lsi-18" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.19322228 <a title="26-lsi-19" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>20 0.19189239 <a title="26-lsi-20" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.021), (35, 0.011), (36, 0.047), (45, 0.014), (50, 0.05), (61, 0.011), (63, 0.033), (76, 0.03), (78, 0.013), (81, 0.025), (84, 0.019), (90, 0.024), (91, 0.552), (96, 0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91489285 <a title="26-lda-1" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><p>2 0.81729615 <a title="26-lda-2" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer, Bernhard Schölkopf</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun. Keywords: multiple kernel learning, string kernels, large scale optimization, support vector machines, support vector regression, column generation, semi-inﬁnite linear programming</p><p>3 0.46844134 <a title="26-lda-3" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>Author: Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We present a family of margin based online learning algorithms for various prediction tasks. In particular we derive and analyze algorithms for binary and multiclass categorization, regression, uniclass prediction and sequence prediction. The update steps of our different algorithms are all based on analytical solutions to simple constrained optimization problems. This uniﬁed view allows us to prove worst-case loss bounds for the different algorithms and for the various decision problems based on a single lemma. Our bounds on the cumulative loss of the algorithms are relative to the smallest loss that can be attained by any ﬁxed hypothesis, and as such are applicable to both realizable and unrealizable settings. We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real data sets.</p><p>4 0.41815284 <a title="26-lda-4" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Anders Bergkvist, Peter Damaschke,  Marcel Lüthi</p><p>Abstract: We consider an optimization problem in probabilistic inference: Given n hypotheses H j , m possible observations Ok , their conditional probabilities pk j , and a particular Ok , select a possibly small subset of hypotheses excluding the true target only with some error probability ε. After specifying the optimization goal we show that this problem can be solved through a linear program in mn variables that indicate the probabilities to discard a hypothesis given an observation. Moreover, we can compute optimal strategies where only O(m + n) of these variables get fractional values. The manageable size of the linear programs and the mostly deterministic shape of optimal strategies makes the method practicable. We interpret the dual variables as worst-case distributions of hypotheses, and we point out some counterintuitive nonmonotonic behaviour of the variables as a function of the error bound ε. One of the open problems is the existence of a purely combinatorial algorithm that is faster than generic linear programming. Keywords: probabilistic inference, error probability, linear programming, cycle-free graphs, network ﬂows</p><p>5 0.41004038 <a title="26-lda-5" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>6 0.40594274 <a title="26-lda-6" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.40251672 <a title="26-lda-7" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.40115264 <a title="26-lda-8" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.38860619 <a title="26-lda-9" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>10 0.38505054 <a title="26-lda-10" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<p>11 0.37441748 <a title="26-lda-11" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>12 0.36846906 <a title="26-lda-12" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.36100993 <a title="26-lda-13" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>14 0.35885686 <a title="26-lda-14" href="./jmlr-2006-Superior_Guarantees_for_Sequential_Prediction_and_Lossless_Compression_via_Alphabet_Decomposition.html">90 jmlr-2006-Superior Guarantees for Sequential Prediction and Lossless Compression via Alphabet Decomposition</a></p>
<p>15 0.35767633 <a title="26-lda-15" href="./jmlr-2006-Causal_Graph_Based_Decomposition_of_Factored_MDPs.html">19 jmlr-2006-Causal Graph Based Decomposition of Factored MDPs</a></p>
<p>16 0.35660848 <a title="26-lda-16" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.35185573 <a title="26-lda-17" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>18 0.34981176 <a title="26-lda-18" href="./jmlr-2006-Point-Based_Value_Iteration_for_Continuous_POMDPs.html">74 jmlr-2006-Point-Based Value Iteration for Continuous POMDPs</a></p>
<p>19 0.34906346 <a title="26-lda-19" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.34308025 <a title="26-lda-20" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
