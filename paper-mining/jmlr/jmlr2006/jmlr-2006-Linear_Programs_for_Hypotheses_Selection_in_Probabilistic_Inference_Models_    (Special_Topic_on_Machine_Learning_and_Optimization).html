<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-56" href="#">jmlr2006-56</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-56-pdf" href="http://jmlr.org/papers/volume7/bergkvist06a/bergkvist06a.pdf">pdf</a></p><p>Author: Anders Bergkvist, Peter Damaschke,  Marcel Lüthi</p><p>Abstract: We consider an optimization problem in probabilistic inference: Given n hypotheses H j , m possible observations Ok , their conditional probabilities pk j , and a particular Ok , select a possibly small subset of hypotheses excluding the true target only with some error probability ε. After specifying the optimization goal we show that this problem can be solved through a linear program in mn variables that indicate the probabilities to discard a hypothesis given an observation. Moreover, we can compute optimal strategies where only O(m + n) of these variables get fractional values. The manageable size of the linear programs and the mostly deterministic shape of optimal strategies makes the method practicable. We interpret the dual variables as worst-case distributions of hypotheses, and we point out some counterintuitive nonmonotonic behaviour of the variables as a function of the error bound ε. One of the open problems is the existence of a purely combinatorial algorithm that is faster than generic linear programming. Keywords: probabilistic inference, error probability, linear programming, cycle-free graphs, network ﬂows</p><p>Reference: <a title="jmlr-2006-56-reference" href="../jmlr2006_reference/jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 After specifying the optimization goal we show that this problem can be solved through a linear program in mn variables that indicate the probabilities to discard a hypothesis given an observation. [sent-12, score-0.365]
</p><p>2 Moreover, we can compute optimal strategies where only O(m + n) of these variables get fractional values. [sent-13, score-0.321]
</p><p>3 Moreover we know the conditional probabilities pk j = P(Ok |H j ) to observe Ok if H j is the true hypothesis, also called the target. [sent-25, score-0.275]
</p><p>4 Since exactly one Ok is observed, the pk j must satisfy ∑m pk j = 1 for every j. [sent-26, score-0.39]
</p><p>5 Our aim is to devise a strategy that, for any observed Ok , selects a subset of hypotheses so as to minimize two conﬂicting parameters at the same time: the probability to discard (that is, not to select) the target, and the size of the selection. [sent-28, score-0.417]
</p><p>6 u  ¨ B ERGKVIST, DAMASCHKE AND L UTHI  then examined closer, in order to identify the target, whereas we would come back to discarded hypotheses only if we missed the target in our selection. [sent-30, score-0.28]
</p><p>7 The 3D structure of the protein backbone is uniquely determined by its torsion angles. [sent-41, score-0.517]
</p><p>8 Since it is difﬁcult and costly to determine them experimentally, various methods have been developed to infer torsion angles and other structure elements from easier measurable, correlated data, partly with help of sequence homology. [sent-42, score-0.47]
</p><p>9 Due to the correlations, it is a natural idea to infer torsion angles from measured chemical shifts. [sent-45, score-0.567]
</p><p>10 Torsion angle restraints that are narrow but still contain the (unknown) true torsion angle values in the majority of cases are important for correct 3D structure reconstruction of whole protein sequences. [sent-46, score-0.851]
</p><p>11 We have to treat in a semi-automated way a huge number of problem instances: for 6 different nuclei, 20 different amino acids, and 2 torsion angles we get nearly 240 data sets. [sent-55, score-0.494]
</p><p>12 We can work, for example, with larger error probabilities for hypotheses that are unlikely to appear as target, or hard to discriminate from others. [sent-67, score-0.285]
</p><p>13 In interval prediction applications like protein torsion angle prediction, it is sensible to choose the weight of each interval proportional to the interval length. [sent-78, score-0.714]
</p><p>14 Moreover we disprove in Section 4 the tempting conjecture that the exclusion probabilities in optimal strategies are always monotone in the error bounds. [sent-81, score-0.29]
</p><p>15 One may compare our optimization to very common and simple heuristic inference rules such as the maximum likelihood (ML) and the maximum a-posteriori (MAP) rule: For an observed Ok , ML selects the desired number of hypotheses H j with the highest pk j . [sent-90, score-0.377]
</p><p>16 MAP proceeds similarly with the posterior probabilities of the H j , for prior probabilities given along with the pk j , whereas ML ignores prior probabilities. [sent-91, score-0.449]
</p><p>17 Based on an observed Ok , a player wants to discard a set of hypotheses that should have large weight but should not contain the target. [sent-100, score-0.408]
</p><p>18 The exclusiveness of σ for any ﬁxed target H j is the expected total weight of the hypotheses discarded by σ. [sent-110, score-0.676]
</p><p>19 ) Finally, the exclusiveness of σ is deﬁned to be the worst (smallest) exclusiveness for all H j . [sent-112, score-0.792]
</p><p>20 (3) For entries with pk j = 0 we would immediately discard hypothesis H j upon observation Ok . [sent-120, score-0.405]
</p><p>21 In applications, typically the pk j are estimated from statistical data, and instead of setting pk j = 0 in the absence of cases, it is common in statistical learning methods to apply some correction rules that yield small positive values. [sent-122, score-0.39]
</p><p>22 However, for maximizing exclusiveness we actually need only mn variables, and this makes the approach feasible. [sent-124, score-0.471]
</p><p>23 max u  (1)  m  ∀j :  ∑ pk j xk j ≤ ε j  (2)  k=1 m  ∀j :  ∑  k=1  n  pk j ∑ wi xki ≥ u  (3)  i=1  ∀k, j : 0 ≤ xk j ≤ 1  (4)  Proof The left-hand side of (2) is obviously the probability to discard H j if H j is the target. [sent-130, score-1.138]
</p><p>24 The left-hand side of (3) is the exclusiveness for H j , hence (3) says that the exclusiveness for every H j is at least some u that is maximized in (1). [sent-131, score-0.792]
</p><p>25 That is, we are maximizing the exclusiveness of the strategy as desired. [sent-132, score-0.463]
</p><p>26 We remark that, because of (3), the exclusiveness actually depends only on the weighted sum of variables in each row of X, deﬁned by xk := ∑n wi xki . [sent-136, score-0.823]
</p><p>27 To mention only two natural options: We may take a random number rand uniformly from interval [0, 1] and discard all H j with xk j ≥ rand, or we may discard the H j independently with probabilities xk j . [sent-139, score-0.827]
</p><p>28 Thus we will henceforth consider the exclusion probabilities xk j as the strategy variables. [sent-142, score-0.408]
</p><p>29 Some applications may prefer hypotheses of some guaranteed weight for every Ok (although this can be rather unnatural, especially when rows of P contain very different numbers of safely discarded small entries pk j ). [sent-145, score-0.503]
</p><p>30 Game-theoretic Interpretation, Knapsack Strategies, and the Dual Our linear program from Theorem 2 is equivalent to a matrix game between a player who selects hypotheses and an adversary (“Nature”) which tries to make a successful choice as difﬁcult as possible. [sent-148, score-0.362]
</p><p>31 More precisely, the player can choose a strategy X that respects (2) and (4), the adversary chooses a hypothesis, and the payoff to the player is exclusiveness u in (3). [sent-149, score-0.658]
</p><p>32 The set of possible strategies X is inﬁnite, but we can turn the game into an equivalent ﬁnite game, by observing that (a) exclusiveness is linear in X, and (b) all feasible X build a polytope with ﬁnitely many vertices. [sent-150, score-0.52]
</p><p>33 In every column j of X we set up an instance of the fractional knapsack problem, with capacity ε j and items k = 1, . [sent-164, score-0.566]
</p><p>34 , m having sizes pk j and utilities w j ∑n pki qi ; see Martello and Toth (1990) for an introduction to knapsack problems. [sent-167, score-0.577]
</p><p>35 i=1 Note that the fractional knapsack problem is trivially solved by a greedy algorithm: Start from xk j := 0 for all k, and then set xk j := 1 for k with decreasing utility-to-size ratio rk j := w j ∑n pki qi /pk j , i=1 until the capacity is exhausted. [sent-168, score-1.077]
</p><p>36 ) Now, we call a matrix X a knapsack strategy against prior q if each column of X is an optimal solution to the fractional knapsack problem introduced above. [sent-174, score-1.022]
</p><p>37 Proposition 4 The optimal strategies X against a prior q are exactly the knapsack strategies against that prior q. [sent-175, score-0.566]
</p><p>38 In particular, if X ∗ is optimal then X ∗ is a knapsack strategy against every optimal q∗ . [sent-176, score-0.437]
</p><p>39 Proof The ﬁrst assertion is obvious, since the utility term w j ∑n pki qi is the coefﬁcient of xk j in i=1 the exclusiveness. [sent-177, score-0.283]
</p><p>40 But since the latter strategies are knapsack strategies against q∗ , the second assertion follows. [sent-180, score-0.444]
</p><p>41 We remark that the converse cannot be concluded: A knapsack strategy against the optimal q∗ is not necessarily optimal in the whole game, since it may be worse against other priors. [sent-181, score-0.437]
</p><p>42 1344  L INEAR P ROGRAMS FOR H YPOTHESES S ELECTION  Proof The Lagrange function is given by n  L(X, u, λ) = u + ∑ λ j j=1  m  n  k=1  i=1  ∑ pk j ∑ wi xki − u  . [sent-184, score-0.37]
</p><p>43 , λn ), the Lagrangian subproblem θ(λ) = maxX,u L(X, u, λ) can be separated for u and X: n  n  m  n  j=1  k=1  i=1  θ(λ) = max u(1 − ∑ λ j ) + max ∑ λ j ∑ pk j ∑ wi xki . [sent-188, score-0.37]
</p><p>44 Thus j=1 the Lagrangian dual simpliﬁes to n  m  n  j=1  k=1  i=1  min θ(λ) = min max ∑ λ j ∑ pk j ∑ wi xki λ  λ  X  subject to ∑n λ j = 1 and the original constraints (2),(4). [sent-192, score-0.439]
</p><p>45 Theorem 6 (X, q) is a pair of optimal solutions if and only if: X is a knapsack strategy against q, and X has its lowest exclusiveness for all H j where q j > 0. [sent-194, score-0.805]
</p><p>46 Note that this optimality criterion can be checked in O(mn) time for given X and q: One just has to solve the fractional knapsack instances for all columns j and to compare the left-hand sides of constraints (3). [sent-197, score-0.659]
</p><p>47 This would be valuable for applications with many instances like our torsion angle prediction project. [sent-200, score-0.567]
</p><p>48 (Recall that X ∗ has the worst exclusiveness even for all H j with positive q∗ . [sent-203, score-0.396]
</p><p>49 ) We may j then drop constraint (3) for such indices j and optimize again, in order to raise the exclusiveness for the more frequent targets only. [sent-204, score-0.437]
</p><p>50 Regarding the dependency of u from this parameter we have: Proposition 7 For any ﬁxed likelihood matrix P, the optimal exclusiveness u is a monotone increasing and concave function in ε. [sent-209, score-0.472]
</p><p>51 Concavity: Consider the (mn+2)-dimensional space with the mn variables xk j and, additionally, ε and u as coordinates. [sent-212, score-0.29]
</p><p>52 One might expect that also every single variable xk j in the strategy matrix X is monotone in the error bound ε, but this is not true in general. [sent-225, score-0.353]
</p><p>53 Recall the notations pk j for the probability to observe Ok given H j , the weighted row sums xk := ∑n wi xki , i=1 and the utility-to-size ratios rk j := w j ∑n pki qi /pk j from the fractional knapsack problems. [sent-227, score-1.269]
</p><p>54 Consider the following matrix P of conditional probabilities pk j : 0. [sent-229, score-0.275]
</p><p>55 For the prior (q1 , q2 , q3 ) = (1, 0, 0) it is easy to check that any knapsack solution has exclusiveness u = 0. [sent-238, score-0.757]
</p><p>56 Moreover, against this prior, every knapsack solution with x1 ≥ x2 1346  L INEAR P ROGRAMS FOR H YPOTHESES S ELECTION  satisﬁes the criterion in Theorem 6. [sent-240, score-0.314]
</p><p>57 This in turn implies that every optimal X must be a knapsack solution against (1, 0, 0). [sent-247, score-0.342]
</p><p>58 Then, every knapsack solution against (1, 0, 0) has x1 < x2 , so that prior (0, 0, 1) would be worse. [sent-252, score-0.361]
</p><p>59 But, similarly, every knapsack solution against (0, 0, 1) has x1 > x2 , so that prior (1, 0, 0) would be worse. [sent-253, score-0.361]
</p><p>60 As announced, we show that our linear programs from Theorem 2 have optimal solutions where only a minority of the mn variables xk j is fractional, that is, properly between 0 and 1. [sent-263, score-0.357]
</p><p>61 Theorem 8 Any optimal solution being a vertex of the feasible region has at most 2n fractional variables. [sent-265, score-0.306]
</p><p>62 Furthermore, the number of binding constraints in a vertex X is at least the dimension mn, but only one of any two constraints xk j ≥ 0, xk j ≤ 1 can be binding. [sent-268, score-0.52]
</p><p>63 Thus, in a vertex X with more than 2n fractional coordinates, more than 2n other constraints must be binding. [sent-269, score-0.286]
</p><p>64 We can also say something about the positions of fractional entries in optimal strategy matrices X and get a better bound in case that m ≤ n. [sent-271, score-0.356]
</p><p>65 Let B(X) be the bipartite graph with vertices rk for all rows k, and vertices c j for all columns j, where an edge between rk and c j exists iff xk j is a fractional value. [sent-272, score-0.683]
</p><p>66 The effect is that n xk := ∑i=1 wi xki remains unchanged for row k = 2. [sent-289, score-0.427]
</p><p>67 Note that all these changes neither affect the left-hand sides of constraints (2) nor the weighted row sums xk deﬁned above, with x1 as the only exception. [sent-293, score-0.284]
</p><p>68 Since the optimal value u is monotone in the xk , the new solution X is no worse. [sent-300, score-0.291]
</p><p>69 The 3 × 2 instances from Example 1 admit optimal solutions with n = 3 fractional entries. [sent-305, score-0.306]
</p><p>70 (1) Example 1 in the previous section shows (besides non-monotonicity of the xk j in the error bounds) that, in an optimal solution, the xk j in a row k are in general not simply ﬁlled up to 1 in increasing order of the pk j . [sent-315, score-0.713]
</p><p>71 Nevertheless, intuition tells that larger xk j are mostly assigned to smaller pk j . [sent-317, score-0.41]
</p><p>72 For an input P and a strategy X, deﬁne a directed graph C(X) whose vertices are the columns, with a directed arc from i to j if pki > pk j and xki > xk j holds for some k. [sent-320, score-0.716]
</p><p>73 However, the idea works only for a variant of H YPOTHESIS S ELECTION WITH E RROR B OUNDS with “observation-wise” exclusiveness demands instead of a global exclusiveness objective: Recall again the weighted row sums xk := ∑n wi xki . [sent-329, score-1.219]
</p><p>74 For given parameters ε j and yk i=1 for all j and k, respectively, we may raise the following existence problem: Is there a solution X with error probabilities at most ε j for all H j , and xk ≥ yk for all Ok ? [sent-330, score-0.359]
</p><p>75 of conditional probabilities for the same set of hypotheses but for different types of observations, such as different groups of symptomes in diagnosis, or chemical shifts of several nuclei in protein torsion angle prediction. [sent-339, score-1.119]
</p><p>76 , m′ ), we deﬁne for all H j the exclusion probabilities xk j + xl′ j − xk j xl′ j . [sent-357, score-0.556]
</p><p>77 Then the resulting mm′ × n matrix is a strategy for combined observations with upper bound ε j + ε′j on the probability to wrongly discard target H j . [sent-358, score-0.278]
</p><p>78 Hence, if Ok , O′ are observed, we keep H j with probability l (1 − xk j )(1 − xl′ j ) = 1 − (xk j + xl′ j − xk j xl′ j ). [sent-361, score-0.43]
</p><p>79 However, concavity of exclusiveness (see Proposition 7) suggests that combining two predictors with half error bound in general improves the exclusiveness. [sent-364, score-0.419]
</p><p>80 We also remark that, since by Theorem 8 and 9 most strategy variables xk j , xl′ j are 0 or 1, the calculations are fast. [sent-366, score-0.282]
</p><p>81 In the ﬁrst instance, the exclusiveness is only 0. [sent-380, score-0.396]
</p><p>82 6 if H1 is the target (since always O1 is observed), and for H2 we get exclusiveness 0. [sent-381, score-0.437]
</p><p>83 If we use the information from both instances, we can improve the exclusiveness for the same ε = 0. [sent-383, score-0.396]
</p><p>84 83, and target H2 yields the same exclusiveness by symmetry. [sent-401, score-0.437]
</p><p>85 We were led to the problem class by a concrete challenge: a project where we are comparing different methods for predicting protein torsion angles from NMR chemical shifts, see Section 1. [sent-405, score-0.671]
</p><p>86 Optimization assists in the creation of a predictor: Any prediction heuristic has to take a measured chemical shift value and output predicted torsion angle values. [sent-407, score-0.644]
</p><p>87 The main relevant question for spectroscopists is the achievable conﬁdence when predicting torsion angle intervals of a prescribed length (error probability vs. [sent-410, score-0.566]
</p><p>88 Basic heuristics working purely “row-wise” (MAP, ML, or similar) do not pay attention to error probabilities for speciﬁc hypothesis intervals and easily discard certain torsion angles completely, despite a considerable frequency of occurrence. [sent-414, score-0.855]
</p><p>89 Hence such heuristics generate systematically misleading predictions when these neglected ranges of torsion angles appear. [sent-415, score-0.495]
</p><p>90 Even worse, they can appear more frequently in a protein under consideration than in the database: Recall that the scatterplots are sampled from a large collection of various proteins so that we know only average torsion angle frequencies. [sent-416, score-0.676]
</p><p>91 A more even distribution of errors to different torsion angles gives more robustness against varying torsion angle frequencies. [sent-417, score-0.987]
</p><p>92 A simple MAP heuristics, for example, would take the measured chemical shift value and select the torsion angle ranges (columns) with highest point densities in the row containing the measured value. [sent-420, score-0.681]
</p><p>93 Now we can adjust error probabilities for individual torsion angle intervals in any desired direction and re-optimize. [sent-423, score-0.669]
</p><p>94 As an illustration we discuss an arbitrary example (point count matrix) from the real data: Aspartic Acid, nucleus Cα , and torsion angle φ, partitioned into homogeneous regions using the method from Christin (2006):  1. [sent-424, score-0.517]
</p><p>95 The bottom line gives the torsion angle interval lengths in degrees, that is, our weights. [sent-427, score-0.548]
</p><p>96 Suppose we want to predict torsion angle intervals of about 60 degrees and start with a naive MAP heuristic that takes the intervals of exactly 60 degrees with maximum density in each row. [sent-437, score-0.673]
</p><p>97 4, optimization (followed by raising some sporadic xk j < 1 to 1 when pk j is small) yields this X:   1 1 0 0 0 1 1 1  1 1 1 0 0 0. [sent-472, score-0.41]
</p><p>98 exclusiveness tradeoff) also by other 1353  ¨ B ERGKVIST, DAMASCHKE AND L UTHI  plausible heuristics: ε j proportional to the weight-by-frequency ratio, equal ε j , and combinations of them. [sent-502, score-0.396]
</p><p>99 Protein φ and ψ dihedral restraints determined from multidimensional hypersurface correlations of backbone chemical shifts and their use in the determination of protein tertiary structures. [sent-513, score-0.371]
</p><p>100 Protein backbone angle restraints from searching a database for chemical shift and sequence homology. [sent-555, score-0.361]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('exclusiveness', 0.396), ('torsion', 0.382), ('knapsack', 0.314), ('fractional', 0.228), ('ok', 0.228), ('xk', 0.215), ('pk', 0.195), ('hypotheses', 0.182), ('discard', 0.143), ('xki', 0.136), ('angle', 0.135), ('election', 0.124), ('damaschke', 0.114), ('ergkvist', 0.109), ('uthi', 0.109), ('ypotheses', 0.109), ('protein', 0.104), ('chemical', 0.097), ('ypothesis', 0.096), ('rograms', 0.093), ('angles', 0.088), ('player', 0.083), ('rror', 0.081), ('probabilities', 0.08), ('mn', 0.075), ('shifts', 0.071), ('nmr', 0.068), ('nuclei', 0.068), ('pki', 0.068), ('restraints', 0.068), ('strategy', 0.067), ('ounds', 0.067), ('strategies', 0.065), ('inear', 0.063), ('discarded', 0.057), ('scatterplots', 0.055), ('teborg', 0.055), ('wayne', 0.055), ('instances', 0.05), ('xl', 0.05), ('intervals', 0.049), ('monotone', 0.048), ('prior', 0.047), ('exclusion', 0.046), ('raise', 0.041), ('lossy', 0.041), ('chalmers', 0.041), ('marcel', 0.041), ('target', 0.041), ('wi', 0.039), ('programs', 0.039), ('rounded', 0.038), ('dual', 0.037), ('rk', 0.037), ('row', 0.037), ('rows', 0.036), ('ml', 0.036), ('columns', 0.035), ('game', 0.035), ('vertices', 0.035), ('tardos', 0.035), ('scatterplot', 0.035), ('hypothesis', 0.034), ('entries', 0.033), ('combinatorial', 0.033), ('program', 0.033), ('constraints', 0.032), ('interval', 0.031), ('purely', 0.031), ('sweden', 0.031), ('backbone', 0.031), ('shift', 0.03), ('lagrangian', 0.029), ('degrees', 0.029), ('adversary', 0.029), ('optimal', 0.028), ('basel', 0.027), ('beger', 0.027), ('cornilescu', 0.027), ('destroy', 0.027), ('martello', 0.027), ('szolovits', 0.027), ('observations', 0.027), ('ow', 0.027), ('narrow', 0.027), ('scheduling', 0.027), ('dii', 0.027), ('vertex', 0.026), ('heuristics', 0.025), ('devise', 0.025), ('diagnosis', 0.025), ('bipartite', 0.025), ('proposition', 0.024), ('bennett', 0.024), ('column', 0.024), ('feasible', 0.024), ('amino', 0.024), ('error', 0.023), ('biomolecular', 0.023), ('christin', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999899 <a title="56-tfidf-1" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Anders Bergkvist, Peter Damaschke,  Marcel Lüthi</p><p>Abstract: We consider an optimization problem in probabilistic inference: Given n hypotheses H j , m possible observations Ok , their conditional probabilities pk j , and a particular Ok , select a possibly small subset of hypotheses excluding the true target only with some error probability ε. After specifying the optimization goal we show that this problem can be solved through a linear program in mn variables that indicate the probabilities to discard a hypothesis given an observation. Moreover, we can compute optimal strategies where only O(m + n) of these variables get fractional values. The manageable size of the linear programs and the mostly deterministic shape of optimal strategies makes the method practicable. We interpret the dual variables as worst-case distributions of hypotheses, and we point out some counterintuitive nonmonotonic behaviour of the variables as a function of the error bound ε. One of the open problems is the existence of a purely combinatorial algorithm that is faster than generic linear programming. Keywords: probabilistic inference, error probability, linear programming, cycle-free graphs, network ﬂows</p><p>2 0.067759834 <a title="56-tfidf-2" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>3 0.044718817 <a title="56-tfidf-3" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><p>4 0.044596042 <a title="56-tfidf-4" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>Author: Robert Castelo, Alberto Roverato</p><p>Abstract: Learning of large-scale networks of interactions from microarray data is an important and challenging problem in bioinformatics. A widely used approach is to assume that the available data constitute a random sample from a multivariate distribution belonging to a Gaussian graphical model. As a consequence, the prime objects of inference are full-order partial correlations which are partial correlations between two variables given the remaining ones. In the context of microarray data the number of variables exceed the sample size and this precludes the application of traditional structure learning procedures because a sampling version of full-order partial correlations does not exist. In this paper we consider limited-order partial correlations, these are partial correlations computed on marginal distributions of manageable size, and provide a set of rules that allow one to assess the usefulness of these quantities to derive the independence structure of the underlying Gaussian graphical model. Furthermore, we introduce a novel structure learning procedure based on a quantity, obtained from limited-order partial correlations, that we call the non-rejection rate. The applicability and usefulness of the procedure are demonstrated by both simulated and real data. Keywords: Gaussian distribution, gene network, graphical model, microarray data, non-rejection rate, partial correlation, small-sample inference</p><p>5 0.0418224 <a title="56-tfidf-5" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>Author: Thomas Kämpke</p><p>Abstract: Similarity of edge labeled graphs is considered in the sense of minimum squared distance between corresponding values. Vertex correspondences are established by isomorphisms if both graphs are of equal size and by subisomorphisms if one graph has fewer vertices than the other. Best ﬁt isomorphisms and subisomorphisms amount to solutions of quadratic assignment problems and are computed exactly as well as approximately by minimum cost ﬂow, linear assignment relaxations and related graph algorithms. Keywords: assignment problem, best approximation, branch and bound, inexact graph matching, model data base</p><p>6 0.04080211 <a title="56-tfidf-6" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>7 0.039011478 <a title="56-tfidf-7" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.03876318 <a title="56-tfidf-8" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.038151342 <a title="56-tfidf-9" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.035426438 <a title="56-tfidf-10" href="./jmlr-2006-Lower_Bounds_and_Aggregation_in_Density_Estimation.html">58 jmlr-2006-Lower Bounds and Aggregation in Density Estimation</a></p>
<p>11 0.033804514 <a title="56-tfidf-11" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>12 0.033240966 <a title="56-tfidf-12" href="./jmlr-2006-Universal_Kernels.html">93 jmlr-2006-Universal Kernels</a></p>
<p>13 0.032895189 <a title="56-tfidf-13" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>14 0.032277137 <a title="56-tfidf-14" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>15 0.03224428 <a title="56-tfidf-15" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>16 0.030303244 <a title="56-tfidf-16" href="./jmlr-2006-Generalized_Bradley-Terry_Models_and_Multi-Class_Probability_Estimates.html">34 jmlr-2006-Generalized Bradley-Terry Models and Multi-Class Probability Estimates</a></p>
<p>17 0.029281525 <a title="56-tfidf-17" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.029228102 <a title="56-tfidf-18" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.028993417 <a title="56-tfidf-19" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>20 0.028982509 <a title="56-tfidf-20" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.159), (1, -0.078), (2, -0.068), (3, 0.056), (4, -0.022), (5, 0.06), (6, 0.057), (7, 0.018), (8, 0.035), (9, 0.01), (10, -0.012), (11, -0.035), (12, -0.002), (13, 0.088), (14, -0.008), (15, 0.037), (16, -0.025), (17, 0.036), (18, 0.009), (19, 0.009), (20, 0.041), (21, -0.044), (22, 0.121), (23, -0.042), (24, -0.098), (25, -0.026), (26, -0.076), (27, 0.033), (28, 0.152), (29, -0.242), (30, 0.095), (31, -0.041), (32, 0.298), (33, -0.193), (34, -0.051), (35, 0.263), (36, -0.21), (37, 0.048), (38, 0.192), (39, 0.07), (40, -0.06), (41, 0.173), (42, -0.029), (43, -0.368), (44, -0.161), (45, -0.011), (46, -0.012), (47, 0.134), (48, -0.284), (49, 0.111)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9573859 <a title="56-lsi-1" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Anders Bergkvist, Peter Damaschke,  Marcel Lüthi</p><p>Abstract: We consider an optimization problem in probabilistic inference: Given n hypotheses H j , m possible observations Ok , their conditional probabilities pk j , and a particular Ok , select a possibly small subset of hypotheses excluding the true target only with some error probability ε. After specifying the optimization goal we show that this problem can be solved through a linear program in mn variables that indicate the probabilities to discard a hypothesis given an observation. Moreover, we can compute optimal strategies where only O(m + n) of these variables get fractional values. The manageable size of the linear programs and the mostly deterministic shape of optimal strategies makes the method practicable. We interpret the dual variables as worst-case distributions of hypotheses, and we point out some counterintuitive nonmonotonic behaviour of the variables as a function of the error bound ε. One of the open problems is the existence of a purely combinatorial algorithm that is faster than generic linear programming. Keywords: probabilistic inference, error probability, linear programming, cycle-free graphs, network ﬂows</p><p>2 0.23686324 <a title="56-lsi-2" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Radu Stefan Niculescu, Tom M. Mitchell, R. Bharat Rao</p><p>Abstract: The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. This paper considers a variety of types of domain knowledge for constraining parameter estimates when learning Bayesian networks. In particular, we consider domain knowledge that constrains the values or relationships among subsets of parameters in a Bayesian network with known structure. We incorporate a wide variety of parameter constraints into learning procedures for Bayesian networks, by formulating this task as a constrained optimization problem. The assumptions made in module networks, dynamic Bayes nets and context speciﬁc independence models can be viewed as particular cases of such parameter constraints. We present closed form solutions or fast iterative algorithms for estimating parameters subject to several speciﬁc classes of parameter constraints, including equalities and inequalities among parameters, constraints on individual parameters, and constraints on sums and ratios of parameters, for discrete and continuous variables. Our methods cover learning from both frequentist and Bayesian points of view, from both complete and incomplete data. We present formal guarantees for our estimators, as well as methods for automatically learning useful parameter constraints from data. To validate our approach, we apply it to the domain of fMRI brain image analysis. Here we demonstrate the ability of our system to ﬁrst learn useful relationships among parameters, and then to use them to constrain the training of the Bayesian network, resulting in improved cross-validated accuracy of the learned model. Experiments on synthetic data are also presented. Keywords: Bayesian networks, constrained optimization, domain knowledge c 2006 Radu Stefan Niculescu, Tom Mitchell and Bharat Rao. N ICULESCU , M ITCHELL AND R AO</p><p>3 0.22120076 <a title="56-lsi-3" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>Author: Janez Demšar</p><p>Abstract: While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classiﬁers: the Wilcoxon signed ranks test for comparison of two classiﬁers and the Friedman test with the corresponding post-hoc tests for comparison of more classiﬁers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams. Keywords: comparative studies, statistical methods, Wilcoxon signed ranks test, Friedman test, multiple comparisons tests</p><p>4 0.21014045 <a title="56-lsi-4" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tobias Glasmachers, Christian Igel</p><p>Abstract: Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is signiﬁcantly faster. Keywords: working set selection, sequential minimal optimization, quadratic programming, support vector machines, large scale optimization</p><p>5 0.20417735 <a title="56-lsi-5" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>Author: Hichem Sahbi, Donald Geman</p><p>Abstract: We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly ﬁner subsets. The hierarchy is traversed coarse-to-ﬁne and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This “free network” is then perturbed, cell by cell, into another network, which is “graded” in two ways: ﬁrst, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-speciﬁc SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network. Keywords: statistical learning, hierarchy of classiﬁers, coarse-to-ﬁne computation, support vector machines, face detection</p><p>6 0.19596419 <a title="56-lsi-6" href="./jmlr-2006-Evolutionary_Function_Approximation_for_Reinforcement_Learning.html">30 jmlr-2006-Evolutionary Function Approximation for Reinforcement Learning</a></p>
<p>7 0.19183248 <a title="56-lsi-7" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.19064346 <a title="56-lsi-8" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>9 0.18210283 <a title="56-lsi-9" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.18157567 <a title="56-lsi-10" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.17585517 <a title="56-lsi-11" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>12 0.17167637 <a title="56-lsi-12" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>13 0.16306013 <a title="56-lsi-13" href="./jmlr-2006-Universal_Kernels.html">93 jmlr-2006-Universal Kernels</a></p>
<p>14 0.16104741 <a title="56-lsi-14" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>15 0.15369594 <a title="56-lsi-15" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>16 0.15264691 <a title="56-lsi-16" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.13830149 <a title="56-lsi-17" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>18 0.13604967 <a title="56-lsi-18" href="./jmlr-2006-Lower_Bounds_and_Aggregation_in_Density_Estimation.html">58 jmlr-2006-Lower Bounds and Aggregation in Density Estimation</a></p>
<p>19 0.12433475 <a title="56-lsi-19" href="./jmlr-2006-Some_Discriminant-Based_PAC_Algorithms.html">81 jmlr-2006-Some Discriminant-Based PAC Algorithms</a></p>
<p>20 0.11869453 <a title="56-lsi-20" href="./jmlr-2006-Causal_Graph_Based_Decomposition_of_Factored_MDPs.html">19 jmlr-2006-Causal Graph Based Decomposition of Factored MDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.02), (23, 0.389), (35, 0.012), (36, 0.076), (45, 0.021), (50, 0.052), (61, 0.017), (63, 0.052), (68, 0.014), (76, 0.02), (78, 0.026), (81, 0.033), (84, 0.029), (90, 0.027), (91, 0.051), (96, 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72805613 <a title="56-lda-1" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Anders Bergkvist, Peter Damaschke,  Marcel Lüthi</p><p>Abstract: We consider an optimization problem in probabilistic inference: Given n hypotheses H j , m possible observations Ok , their conditional probabilities pk j , and a particular Ok , select a possibly small subset of hypotheses excluding the true target only with some error probability ε. After specifying the optimization goal we show that this problem can be solved through a linear program in mn variables that indicate the probabilities to discard a hypothesis given an observation. Moreover, we can compute optimal strategies where only O(m + n) of these variables get fractional values. The manageable size of the linear programs and the mostly deterministic shape of optimal strategies makes the method practicable. We interpret the dual variables as worst-case distributions of hypotheses, and we point out some counterintuitive nonmonotonic behaviour of the variables as a function of the error bound ε. One of the open problems is the existence of a purely combinatorial algorithm that is faster than generic linear programming. Keywords: probabilistic inference, error probability, linear programming, cycle-free graphs, network ﬂows</p><p>2 0.35536113 <a title="56-lda-2" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>3 0.35008934 <a title="56-lda-3" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram. Keywords: spectral clustering, blind source separation, computational auditory scene analysis</p><p>4 0.3483015 <a title="56-lda-4" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>Author: Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou</p><p>Abstract: We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the ﬁrst time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction. html. Keywords: transduction, transductive SVMs, semi-supervised learning, CCCP</p><p>5 0.34215772 <a title="56-lda-5" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Matthias Heiler, Christoph Schnörr</p><p>Abstract: We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. We show that for ordinary NMF, our approach performs as well as existing stateof-the-art algorithms, while for sparsity-constrained NMF, as recently proposed by P. O. Hoyer in JMLR 5 (2004), it outperforms previous methods. In addition, we show how to extend NMF learning within the same optimization framework in order to make use of class membership information in supervised learning problems. Keywords: non-negative matrix factorization, second-order cone programming, sequential convex optimization, reverse-convex programming, sparsity</p><p>6 0.34210834 <a title="56-lda-6" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>7 0.34152392 <a title="56-lda-7" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.34010804 <a title="56-lda-8" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>9 0.33961236 <a title="56-lda-9" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>10 0.33944982 <a title="56-lda-10" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.33887789 <a title="56-lda-11" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>12 0.33597252 <a title="56-lda-12" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>13 0.33566034 <a title="56-lda-13" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>14 0.33373687 <a title="56-lda-14" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>15 0.33333868 <a title="56-lda-15" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>16 0.33150607 <a title="56-lda-16" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>17 0.32970017 <a title="56-lda-17" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.32803071 <a title="56-lda-18" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.32731 <a title="56-lda-19" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>20 0.32577151 <a title="56-lda-20" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
