<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-41" href="#">jmlr2006-41</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-41-pdf" href="http://jmlr.org/papers/volume7/rousu06a/rousu06a.pdf">pdf</a></p><p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>Reference: <a title="jmlr-2006-41-reference" href="../jmlr2006_reference/jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Bennett and Emilio Parrado-Hern´ ndez a  Abstract We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. [sent-14, score-0.229]
</p><p>2 The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. [sent-15, score-0.207]
</p><p>3 We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. [sent-16, score-0.411]
</p><p>4 The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. [sent-17, score-0.188]
</p><p>5 The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. [sent-20, score-0.309]
</p><p>6 Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs  1. [sent-21, score-0.277]
</p><p>7 Very recently, new hierarchical classiﬁcation approaches utilizing kernel methods have been introduced (Hofmann et al. [sent-32, score-0.196]
</p><p>8 The main idea behind these methods is to map the documents (or document–labeling pairs) into a potentially high-dimensional feature space where linear maximum margin separation of the documents becomes possible. [sent-35, score-0.186]
</p><p>9 In this paper we consider the more general case where a single object can be classiﬁed into several categories in the hierarchy, to be speciﬁc, the multilabel is a union of partial paths in the hierarchy. [sent-37, score-0.207]
</p><p>10 This technique used a marginalization trick to obtain a polynomial sized quadratic program using marginal dual variables. [sent-43, score-0.241]
</p><p>11 In Section 3 we present an efﬁcient learning algorithm relying a decomposition of the problem into single training example subproblems and conducting iterative conditional gradient ascent in marginal dual variable subspaces corresponding to single training examples. [sent-53, score-0.435]
</p><p>12 We compare the new algorithm in Section 5 to ﬂat and hierarchical SVM learning approaches and the hierarchical regularized least squares algorithm recently proposed by Cesa-Bianchi et al. [sent-56, score-0.28]
</p><p>13 , yk ) ∈ Y is called the multilabel and the components y j are called the microlabels. [sent-67, score-0.21]
</p><p>14 We assume that a training set {(xi , yi )}m ⊂ X × Y has been given, consisting of training exi=1 amples (xi , yi ) of a training pattern xi and multilabel yi . [sent-68, score-0.388]
</p><p>15 Figure 1: Examples of classiﬁcation hierarchies: An excerpt from the WIPO patent classiﬁcation hierarchy (top) and an excerpt from the Enzyme Classiﬁcation scheme (bottom). [sent-108, score-0.206]
</p><p>16 By ye = (y j , y j′ ) we denote the restriction of the multilabel y = (y1 , . [sent-111, score-0.524]
</p><p>17 1 Loss Functions for Hierarchical Multilabel Classiﬁcation There are many ways to deﬁne loss functions for multilabel classiﬁcation setting, and it depends on the application which loss function is the most suitable. [sent-122, score-0.285]
</p><p>18 The loss function between two multilabel vectors y and u should obviously fulﬁll some basic conditions: ℓ(u, y) = 0 if and only if u = y, ℓ(u, y) is maximum when u j = y j for every 1 ≤ j ≤ k, and ℓ should be monotonically non-decreasing with respect to the sets of incorrect microlabels. [sent-124, score-0.227]
</p><p>19 However, it gives loss of 1 if the complete hierarchy is not labeled correctly, even if only a single microlabel was predicted incorrectly. [sent-126, score-0.365]
</p><p>20 In multilabel classiﬁcation, we would like the loss to increase smoothly so that we can make a difference between ’nearly correct’ and ’clearly incorrect’ multilabel predictions. [sent-127, score-0.396]
</p><p>21 (2004) use as a loss function the length of the path (i1 , · · · , ik ) between the the true and predicted nodes with positive microlabels ℓPAT H (y, u) = |path(i : yi = 1, j : u j = 1)|. [sent-133, score-0.217]
</p><p>22 Another possibility is to scale the loss by the proportion of the hierarchy that is in the subtree T ( j) rooted by j, that is, to deﬁne c j = |T ( j)|/|T (root)|. [sent-146, score-0.298]
</p><p>23 If we just use a uniform weighting (c j = 1) in conjunction with the hierarchical loss above this is denoted as ℓ-uni f . [sent-148, score-0.198]
</p><p>24 This choice leads the loss function to capture some of the hierarchical dependencies (between the parent and the child) but allows us deﬁne the loss in terms of edges, which is crucial for the efﬁciency of our learning algorithm. [sent-151, score-0.297]
</p><p>25 This is achieved by deﬁning an edge-loss ℓe (ye , ue ) = ℓ j (y j , u j )/N ( j) + ℓ j′ (y j′ , u j′ )/N ( j′ ) for each e = ( j, j′ ), where ℓ j is the term regarding microlabel j, ye = (y j , y j′ ) is a labeling of the edge e and N ( j) denotes the neighbors of node j in the hierarchy (i. [sent-153, score-1.304]
</p><p>26 Intuitively, the edges adjacent to node j ’share the blame’ of the microlabel loss ℓ j . [sent-156, score-0.314]
</p><p>27 The multilabel loss (ℓ∆ or ℓH ) is then written as a sum over the edges: ℓ(y, u) = ∑e∈E ℓe (ye , ue ). [sent-157, score-0.669]
</p><p>28 Gartner (2003)), we will concentrate to the important case of hierarchical classiﬁcation of text or, in general, sequence data. [sent-171, score-0.178]
</p><p>29 The substring kernels can be generalized in many ways, for example • Gapped substring spectrum kernels allow gaps in the subsequence occurrences. [sent-197, score-0.216]
</p><p>30 3 Feature Representations for Hierarchical Outputs When the input features are used in hierarchical classiﬁcation, they need to be associated with the labelings of the hierarchy. [sent-204, score-0.223]
</p><p>31 There are important design choices to be made in how the hierarchical structure should reﬂect in the feature representation. [sent-206, score-0.18]
</p><p>32 For example, the bag-of-words or the substring spectrum of a document is not tied to a single class of documents in a hierarchy, but a given word can relate to different classes with different importances. [sent-209, score-0.228]
</p><p>33 A simple choice is to deﬁne φue (x, ye ) = [ue = ye ] φx (x)T , φx (x)T e e  T  that incorporates both the global and local features if the edge is labeled ye = ue , and a zero vector otherwise. [sent-216, score-1.57]
</p><p>34 Additive feature representation is deﬁned as φ(x, y) =  ∑ ∑  [ye = ue ]φue (x), e  e∈E ue ∈Y e  where φue contains features speciﬁc to the pair (e, ue ). [sent-218, score-1.366]
</p><p>35 The kernel induced by the above feature map decomposes as K(x, y; x′ , y′ ) =  ∑ φe (x, ye )T φe (x′ , y′e ) = ∑ Ke (x, ye ; x′ , y′e ),  e∈E  e∈E  which means that there is no crosstalk between the edges: φe (x, ye )T φe′ (x, ye′ ) = 0 if e = e′ , hence the name ’orthogonal’. [sent-223, score-1.161]
</p><p>36 The dimension of the feature vector using the additive feature representation is independent of the size of the hierarchy, thus optimization in primal representation (1) is more feasible for large structures. [sent-225, score-0.209]
</p><p>37 Second, the complexity of the kernel will grow quadratically in the size of the hierarchy rather than linearly as is the case with orthogonal features. [sent-230, score-0.221]
</p><p>38 This is another reason why a primal optimization approach for this representation might be more justiﬁed than a dual approach. [sent-231, score-0.204]
</p><p>39 In the sequel, we describe a method that relies on the orthogonal feature representation which will give us a dual formulation with complexity growing linearly in the number of edges in E. [sent-232, score-0.232]
</p><p>40 2003), inspired by support vector machines, is to estimate parameters that in some sense maximize the ratio P(yi |xi ; w) P(y|xi ; w) between the probability of the correct labeling yi and the worst competing labeling y. [sent-244, score-0.19]
</p><p>41 w  (1)  This optimization problem suffers from the possible high-dimensionality of the feature vectors, for example with string kernels, and from the exponential-sized constraint set (in the length of the multilabel vector). [sent-256, score-0.275]
</p><p>42 However, in the dual problem there are exponentially many dual variables α(i, y), one for each pseudo-example. [sent-260, score-0.284]
</p><p>43 There are a few basic routes by which the exponential complexity can be circumvented: • Dual working set methods where the constraint set is grown incrementally by adding the worst margin violator argmini,y wT ∆φ(xi , y) − ℓ(yi , y) to the dual problem. [sent-261, score-0.222]
</p><p>44 • Marginal dual methods, where the problem is translated to a polynomial-sized form via considering the marginals of the dual variables (Taskar et al. [sent-267, score-0.284]
</p><p>45 5 Marginalized Dual Problem The feasible set of the dual problem (2) is a Cartesian product  A = A1 ×···×Am  (3)  A i = {αi ∈ R|Y | | αi ≥ 0, ||αi ||1 ≤ C},  (4)  of identical closed polytopes  with a vertex set V i = {0,Ce1 , . [sent-271, score-0.307]
</p><p>46 Fortunately by utilizing the structure of T , the set A can be mapped to a set M of polynomial dimension, called the marginal polytope of H, where optimization becomes more feasible (Taskar et al. [sent-278, score-0.258]
</p><p>47 For an edge e ∈ E of the Markov tree T , and an associated labeling ye , the marginal of α(i, y) for the pair (e, ye ) is given by  ∑  µe (i, ye ) =  [ye = ue ]α(i, u)  (5)  {u∈Y i }  where the sum picks up those dual variables α(i, y) that have equal value ue = ye on the edge e. [sent-280, score-2.786]
</p><p>48 For the hierarchy T , the vector containing the edge marginals of the example xi , the marginal dual vector, is given by µi = (µe (i, ue ))e∈E,ue ∈Y e . [sent-282, score-0.911]
</p><p>49 The marginal vector of the whole training set is the concatenation of the single example marginal dual vectors µ = (µi )m . [sent-283, score-0.369]
</p><p>50 The indicator functions in (5) can be collectively represented by the the matrix ME , ME (e, ue ; y) = [ue = ye ], and the relationship between a dual vector alpha and the corresponding marginal vector µ is given by the linear map ME · αi = µi and µ = (ME · αi )m . [sent-286, score-1.038]
</p><p>51 These properties underlie the efﬁcient solution of the dual problem on the marginal polytope. [sent-291, score-0.241]
</p><p>52 The exponential size of the dual problem (2) can be tackled via the relationship between its feasible set A = A 1 × · · · × A m and the marginal polytopes M i of each A i . [sent-292, score-0.344]
</p><p>53 The objective should be maximized with respect to µ whilst ensuring that there exist α ∈ A satisfying Mαi = µi for all i, so that the marginal dual solution represents a feasible solution of the original dual. [sent-295, score-0.336]
</p><p>54 t ∑ µe (i, ye ) ≤ C, ∀i, e ∈ E, ye  ∑ µe (i, (y′ , y)) = ∑ µe (i, (y, y′)), ∀i, y, (e, e′) ∈ E2 , ′  y′  y′  , While the above formulation is closely related to that described in Taskar et al. [sent-305, score-0.71]
</p><p>55 Secondly, single-node marginal dual variables—the µ j ’s in (7)—become redundant when the constraints are given in terms of the edges. [sent-308, score-0.241]
</p><p>56 Thirdly, we have utilized the fact that in our feature representation the ’cross-edge’ values ∆φe (x, ye )T ∆φe′ (x′ , y′ ′ ), where e = e′ , do not contribute to the kernel, hence we have a blocke diagonal kernel KE = diag (Ke1 , . [sent-309, score-0.451]
</p><p>57 , Ke|E| ), KE (i, e, ue ; j, e, ve ) = Ke (i, ue ; j, ve ) with the number of non-zero entries thus scaling linearly rather than quadratically in the number of edges. [sent-312, score-0.884]
</p><p>58 To arrive at a more tractable problem, we notice from (3) and (4) that the constraint set decomposes by the examples: to satisfy a single box constraint (6) or a marginal consistency constraint (7) one only needs to change the marginal dual variables of a single example. [sent-324, score-0.369]
</p><p>59 Let us consider optimizing the dual variables µi = (µe (i, ye ))e∈E,ye ∈Y e of example xi where ℓi denotes the corresponding loss vector and Ki j = Ke (i, ue ; j, ve )e∈E,ue ,ve ∈Y e denotes the block of kernel values between examples i and j, and by Ki· = (Ki j ) j∈{1,. [sent-329, score-1.053]
</p><p>60 1612  H IERARCHICAL M ULTILABEL C LASSIFICATION  Obtaining the gradient for the xi -subspace requires computing the corresponding part of the gradient of the objective function in (8) which is gi = ℓi − Ki· µ where ℓi = (ℓe (i, ue ))e∈E,ue ∈Y e is the corresponding loss vector for xi . [sent-333, score-0.705]
</p><p>61 On the other hand, we do not want to spend too much time in optimizing a single example: When the dual variables of the other examples are non-optimal, so is the initial gradient gi . [sent-336, score-0.263]
</p><p>62 In our approach, we have chosen to conduct a few optimization steps for each training example using a conditional gradient ascent (see Algorithm 2) before moving on to the next example. [sent-342, score-0.199]
</p><p>63 It takes as input the training data, the edge set of the hierarchy, the loss vector ℓ = (ℓi )m and the constraints deﬁning the feasible region. [sent-346, score-0.217]
</p><p>64 The algorithm takes as input the current dual variables, gradient, constraints and the kernel block for the example xi , and an iteration limit. [sent-359, score-0.198]
</p><p>65 1613  ROUSU , S AUNDERS , S ZEDMAK AND S HAWE -TAYLOR  Algorithm 1 Maximum margin optimization algorithm for the H-M3 hierarchical classiﬁcation model. [sent-362, score-0.218]
</p><p>66 The second condition states that pseudo-examples with non-zero dual variables are those that have the minimum margin, that is, need the full slack ξi . [sent-379, score-0.185]
</p><p>67 This observation leads to the following heuristics for the working set update: 1614  H IERARCHICAL M ULTILABEL C LASSIFICATION  Algorithm 2 Conditional subspace gradient ascent optimization step. [sent-381, score-0.206]
</p><p>68 CSGA(µi , gi , Kii , F i , maxiteri ) Require: Initial dual variable vector µi , gradient gi , constraints of the feasible region F i , a joint kernel block Kii for the subspace, and an iteration limit maxiteri . [sent-382, score-0.451]
</p><p>69 • Saturated examples (∑e,ye µe (i, ye ) = C) are added if there are not enough non-saturated ones. [sent-385, score-0.355]
</p><p>70 • Inactive (∑e,ye µe (i, ye ) = 0) non-violators (ξi = 0) are removed from the working set, as they do not constrain the objective. [sent-387, score-0.391]
</p><p>71 i i  As feasible primal solutions always are least as large as feasible dual solutions, the duality gap gives an upper bound to the distance from the dual solution to the optimum. [sent-389, score-0.446]
</p><p>72 e Thus instead of solving (11) directly, we can search for the labeling y∗ of the hierarchy corresponding to an optimal vertex vmu(y∗ ) of the feasible set: argmaxy∈Y gT µ(y) (12) i This problem can be solved efﬁciently using a dynamic programming inference algorithm, reviewed in the next section. [sent-402, score-0.396]
</p><p>73 4 Solving the Inference Problem in Linear Time When dealing with structured output models, one needs to solve the inference problem argmaxy∈Y gT µ(y) i  (13)  to ﬁnd a multilabel y maximizing the inner product between some (gradient) vector h and the marginal dual variables µ(y) corresponding to y. [sent-404, score-0.453]
</p><p>74 However, for a hierarchical model dynamic programming can be used: starting from the leaves of the hierarchy we compute bottom-up 1616  H IERARCHICAL M ULTILABEL C LASSIFICATION  for each subtree the optimal labeling of the subtree, conditioned on ﬁxing the label of the subtree root to +1 or −1. [sent-410, score-0.557]
</p><p>75 • The best objective value that can be obtained for the subtree rooted by the edge e = ( j, j′ ) when the root node j is ﬁxed to y j . [sent-414, score-0.23]
</p><p>76 The corresponding vertex v(y∗ ) is found in making a top-down pass over the hierarchy: one looks for best label for a child of a node given the parent has been ﬁxed. [sent-417, score-0.243]
</p><p>77 The dynamic programming scheme can be implemented in vectorized form so that all examples and all nodes on a level of the hierarchy are handled at the same time, thus eliminating the need for loops going over examples and nodes, which in MATLAB implementation are to be avoided. [sent-419, score-0.226]
</p><p>78 Firstly, we notice that for a normalized x-kernel, the entries of the joint kernel are given as sums of indicators Kii (e, ue ; e, u′ ) = 1 − [yie = u′ ] − [y′ = ue ] + [ue = u′ ]. [sent-425, score-0.94]
</p><p>79 Secondly, since µ∗ is an extreme point of e e e ie the feasible set, µ∗ (e, ue ) = C for exactly one of the components ue ∈ Y e . [sent-426, score-0.951]
</p><p>80 (2004)) results in the dual equality constraint ∑ye µe (i, ye ) = C instead of the box constraint. [sent-438, score-0.526]
</p><p>81 Allotting a separate slack variable for each edge is a possibility when the data for some edges can be considered less reliable than the data for others; in such case the unreliable edge can consume required slack without affecting the other edges. [sent-440, score-0.262]
</p><p>82 As the number of partial paths in the hierarchy equals the number of nodes, the resulting feature vectors are actually smaller than the ones deﬁned by edge-labelings. [sent-450, score-0.243]
</p><p>83 First, ensuring global consistency of the marginalized dual is more involved as local consistency of edge-marginals does not guarantee existence of a dual variable α(i, y) with those marginals. [sent-455, score-0.322]
</p><p>84 As the label hierarchy we used the ’CCAT’ family of categories (Corporate/Industrial news articles), which had a total of 34 nodes, organized in a tree with maximum depth 3. [sent-465, score-0.207]
</p><p>85 The number of nodes in the hierarchy was 188, with maximum depth 3. [sent-469, score-0.197]
</p><p>86 The hierarchy consisted of 236 nodes organized into a tree of depth three. [sent-472, score-0.239]
</p><p>87 In all datasets, the membership of examples in the nodes of the hierarchy is indicated by binary vectors y ∈ {+1,−1}k . [sent-474, score-0.197]
</p><p>88 We compared the performance of the presented learning approach—below denoted by H - M3 — to three algorithms: SVM denotes an SVM trained for each microlabel separately, H - SVM denotes the case where the SVM for a microlabel is trained only with examples for which the ancestor labels are positive. [sent-479, score-0.284]
</p><p>89 As the learning algorithms compared here all decompose the hierarchy for learning, the multilabel composed of naively combining the microlabel predictions may be inconsistent, that is, they may predict a document as part of the child but not as part of the parent. [sent-492, score-0.545]
</p><p>90 For SVM and H - SVM consistent labelings were produced by post-processing the predicted labelings as follows: start at the root and traverse the tree in a breadth-ﬁrst fashion. [sent-493, score-0.208]
</p><p>91 For H - M3 models, we computed by dynamic programming the consistent multilabel with maximum 1619  ROUSU , S AUNDERS , S ZEDMAK AND S HAWE -TAYLOR  1 objective tr. [sent-497, score-0.226]
</p><p>92 The number of dual variables for this training set is just over one million with a joint kernel matrix with approx 5 billion entries. [sent-513, score-0.227]
</p><p>93 Note that the solutions for this optimization are not sparse, typically less than 25% of the marginal dual variables are zero. [sent-514, score-0.275]
</p><p>94 We also suspect that early stopping for SVM may be more costly than for H - M3 , due to the fact that the latter predicts whole labelings for the trees where the weight of a single microlabel is small, and in fact the predicted multilabels may contain microlabels that are not locally optimal. [sent-519, score-0.348]
</p><p>95 In other words, the inference procedure for multilabels may correct poor microlabel predictions. [sent-520, score-0.182]
</p><p>96 The WIPO dataset gives an indication that using a hierarchical loss function during training (e. [sent-538, score-0.258]
</p><p>97 Conclusions and Future Work In this paper we have proposed a new method for training variants of the Maximum Margin Markov Network framework for hierarchical multi-category text classiﬁcation models. [sent-685, score-0.207]
</p><p>98 Experimental results on three classiﬁcation tasks show that using the hierarchical structure of multi-category labelings leads to improved performance over the more traditional approach of combining individual binary classiﬁers. [sent-688, score-0.223]
</p><p>99 7  Table 3: Precision/Recall/F1 statistics for each level of the hierarchy for different algorithms on Reuters RCV1 (top), WIPO-alpha (middle), and ENZYME datasets (bottom). [sent-838, score-0.208]
</p><p>100 Improving text classiﬁcation by shrinkage in a hierarchy of classes. [sent-961, score-0.203]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ue', 0.442), ('ye', 0.355), ('kii', 0.249), ('rousu', 0.191), ('ke', 0.181), ('multilabel', 0.169), ('hierarchy', 0.165), ('aunders', 0.154), ('hawe', 0.154), ('zedmak', 0.154), ('microlabel', 0.142), ('ultilabel', 0.142), ('dual', 0.142), ('hierarchical', 0.14), ('ierarchical', 0.121), ('enzyme', 0.107), ('marginal', 0.099), ('rls', 0.091), ('svm', 0.09), ('taskar', 0.084), ('microlabels', 0.083), ('ob', 0.083), ('labelings', 0.083), ('ascent', 0.08), ('subtree', 0.075), ('lassification', 0.074), ('labeling', 0.073), ('substring', 0.072), ('iter', 0.071), ('wt', 0.068), ('feasible', 0.067), ('gi', 0.065), ('reuters', 0.065), ('node', 0.064), ('edge', 0.063), ('vertex', 0.062), ('wipo', 0.06), ('loss', 0.058), ('dp', 0.058), ('polytope', 0.058), ('kernel', 0.056), ('gradient', 0.056), ('documents', 0.051), ('edges', 0.05), ('hofmann', 0.048), ('argmaxy', 0.047), ('juho', 0.047), ('lipsol', 0.047), ('hierarchies', 0.046), ('yi', 0.044), ('margin', 0.044), ('article', 0.043), ('slack', 0.043), ('datasets', 0.043), ('structured', 0.043), ('tree', 0.042), ('classi', 0.041), ('patent', 0.041), ('saunders', 0.041), ('tsochantaridis', 0.041), ('parent', 0.041), ('yk', 0.041), ('feature', 0.04), ('sy', 0.04), ('multilabels', 0.04), ('gt', 0.039), ('spectrum', 0.039), ('wainwright', 0.039), ('pass', 0.039), ('losses', 0.039), ('marginalized', 0.038), ('strings', 0.038), ('paths', 0.038), ('text', 0.038), ('child', 0.037), ('working', 0.036), ('cartesian', 0.036), ('csga', 0.036), ('football', 0.036), ('polytopes', 0.036), ('soton', 0.036), ('optimization', 0.034), ('word', 0.034), ('cai', 0.033), ('altun', 0.033), ('subsequence', 0.033), ('precision', 0.032), ('nodes', 0.032), ('string', 0.032), ('document', 0.032), ('dataset', 0.031), ('optimum', 0.03), ('training', 0.029), ('box', 0.029), ('dynamic', 0.029), ('directions', 0.029), ('dekel', 0.029), ('update', 0.029), ('objective', 0.028), ('primal', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="41-tfidf-1" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>2 0.21313386 <a title="41-tfidf-2" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>Author: Nicolò Cesa-Bianchi, Claudio Gentile, Luca Zaniboni</p><p>Abstract: We study the problem of classifying data in a given taxonomy when classiﬁcations associated with multiple and/or partial paths are allowed. We introduce a new algorithm that incrementally learns a linear-threshold classiﬁer for each node of the taxonomy. A hierarchical classiﬁcation is obtained by evaluating the trained node classiﬁers in a top-down fashion. To evaluate classiﬁers in our multipath framework, we deﬁne a new hierarchical loss function, the H-loss, capturing the intuition that whenever a classiﬁcation mistake is made on a node of the taxonomy, then no loss should be charged for any additional mistake occurring in the subtree of that node. Making no assumptions on the mechanism generating the data instances, and assuming a linear noise model for the labels, we bound the H-loss of our on-line algorithm in terms of the H-loss of a reference classiﬁer knowing the true parameters of the label-generating process. We show that, in expectation, the excess cumulative H-loss grows at most logarithmically in the length of the data sequence. Furthermore, our analysis reveals the precise dependence of the rate of convergence on the eigenstructure of the data each node observes. Our theoretical results are complemented by a number of experiments on texual corpora. In these experiments we show that, after only one epoch of training, our algorithm performs much better than Perceptron-based hierarchical classiﬁers, and reasonably close to a hierarchical support vector machine. Keywords: incremental algorithms, online learning, hierarchical classiﬁcation, second order perceptron, support vector machines, regret bound, loss function</p><p>3 0.095905803 <a title="41-tfidf-3" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>4 0.093002088 <a title="41-tfidf-4" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Ben Taskar, Simon Lacoste-Julien, Michael I. Jordan</p><p>Abstract: We present a simple and scalable algorithm for maximum-margin estimation of structured output models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). The projection step can be solved using dynamic programming or combinatorial algorithms for min-cost convex ﬂow, depending on the structure of the problem. We show that this approach provides a memoryefﬁcient alternative to formulations based on reductions to a quadratic program (QP). We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.1 Keywords: Markov networks, large-margin methods, structured prediction, extragradient, Bregman projections</p><p>5 0.090542927 <a title="41-tfidf-5" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><p>6 0.081567362 <a title="41-tfidf-6" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>7 0.078279749 <a title="41-tfidf-7" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>8 0.073404588 <a title="41-tfidf-8" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>9 0.071591437 <a title="41-tfidf-9" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.067102753 <a title="41-tfidf-10" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>11 0.067060441 <a title="41-tfidf-11" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>12 0.066431835 <a title="41-tfidf-12" href="./jmlr-2006-On_the_Complexity_of_Learning_Lexicographic_Strategies.html">68 jmlr-2006-On the Complexity of Learning Lexicographic Strategies</a></p>
<p>13 0.066177078 <a title="41-tfidf-13" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>14 0.065566763 <a title="41-tfidf-14" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.064053372 <a title="41-tfidf-15" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>16 0.062062729 <a title="41-tfidf-16" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>17 0.061024327 <a title="41-tfidf-17" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>18 0.054375414 <a title="41-tfidf-18" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.047116078 <a title="41-tfidf-19" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.043128625 <a title="41-tfidf-20" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.29), (1, -0.004), (2, 0.161), (3, 0.157), (4, -0.001), (5, 0.027), (6, 0.183), (7, -0.048), (8, -0.083), (9, -0.014), (10, -0.039), (11, 0.119), (12, 0.071), (13, 0.107), (14, 0.164), (15, -0.04), (16, 0.062), (17, -0.22), (18, 0.142), (19, -0.128), (20, -0.292), (21, -0.047), (22, -0.112), (23, -0.086), (24, 0.071), (25, 0.043), (26, 0.002), (27, -0.002), (28, -0.045), (29, 0.096), (30, 0.09), (31, -0.08), (32, -0.025), (33, -0.069), (34, 0.031), (35, -0.092), (36, -0.032), (37, 0.094), (38, 0.118), (39, 0.012), (40, -0.01), (41, 0.042), (42, 0.043), (43, -0.087), (44, 0.115), (45, -0.003), (46, 0.013), (47, 0.244), (48, 0.068), (49, -0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91419959 <a title="41-lsi-1" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>2 0.68524146 <a title="41-lsi-2" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>Author: Nicolò Cesa-Bianchi, Claudio Gentile, Luca Zaniboni</p><p>Abstract: We study the problem of classifying data in a given taxonomy when classiﬁcations associated with multiple and/or partial paths are allowed. We introduce a new algorithm that incrementally learns a linear-threshold classiﬁer for each node of the taxonomy. A hierarchical classiﬁcation is obtained by evaluating the trained node classiﬁers in a top-down fashion. To evaluate classiﬁers in our multipath framework, we deﬁne a new hierarchical loss function, the H-loss, capturing the intuition that whenever a classiﬁcation mistake is made on a node of the taxonomy, then no loss should be charged for any additional mistake occurring in the subtree of that node. Making no assumptions on the mechanism generating the data instances, and assuming a linear noise model for the labels, we bound the H-loss of our on-line algorithm in terms of the H-loss of a reference classiﬁer knowing the true parameters of the label-generating process. We show that, in expectation, the excess cumulative H-loss grows at most logarithmically in the length of the data sequence. Furthermore, our analysis reveals the precise dependence of the rate of convergence on the eigenstructure of the data each node observes. Our theoretical results are complemented by a number of experiments on texual corpora. In these experiments we show that, after only one epoch of training, our algorithm performs much better than Perceptron-based hierarchical classiﬁers, and reasonably close to a hierarchical support vector machine. Keywords: incremental algorithms, online learning, hierarchical classiﬁcation, second order perceptron, support vector machines, regret bound, loss function</p><p>3 0.35687107 <a title="41-lsi-3" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Ben Taskar, Simon Lacoste-Julien, Michael I. Jordan</p><p>Abstract: We present a simple and scalable algorithm for maximum-margin estimation of structured output models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). The projection step can be solved using dynamic programming or combinatorial algorithms for min-cost convex ﬂow, depending on the structure of the problem. We show that this approach provides a memoryefﬁcient alternative to formulations based on reductions to a quadratic program (QP). We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.1 Keywords: Markov networks, large-margin methods, structured prediction, extragradient, Bregman projections</p><p>4 0.351735 <a title="41-lsi-4" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><p>5 0.33466703 <a title="41-lsi-5" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>Author: Hichem Sahbi, Donald Geman</p><p>Abstract: We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly ﬁner subsets. The hierarchy is traversed coarse-to-ﬁne and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This “free network” is then perturbed, cell by cell, into another network, which is “graded” in two ways: ﬁrst, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-speciﬁc SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network. Keywords: statistical learning, hierarchy of classiﬁers, coarse-to-ﬁne computation, support vector machines, face detection</p><p>6 0.32845175 <a title="41-lsi-6" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>7 0.31852001 <a title="41-lsi-7" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>8 0.29676527 <a title="41-lsi-8" href="./jmlr-2006-On_the_Complexity_of_Learning_Lexicographic_Strategies.html">68 jmlr-2006-On the Complexity of Learning Lexicographic Strategies</a></p>
<p>9 0.2963416 <a title="41-lsi-9" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.29246128 <a title="41-lsi-10" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>11 0.28824374 <a title="41-lsi-11" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.28243756 <a title="41-lsi-12" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.2758334 <a title="41-lsi-13" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>14 0.26355004 <a title="41-lsi-14" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>15 0.23553959 <a title="41-lsi-15" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>16 0.23482126 <a title="41-lsi-16" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>17 0.21705596 <a title="41-lsi-17" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>18 0.2142648 <a title="41-lsi-18" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.21237618 <a title="41-lsi-19" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.20866604 <a title="41-lsi-20" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.026), (36, 0.055), (40, 0.056), (45, 0.022), (50, 0.034), (61, 0.017), (63, 0.041), (68, 0.012), (76, 0.032), (78, 0.018), (79, 0.011), (81, 0.312), (84, 0.08), (89, 0.012), (90, 0.044), (91, 0.058), (96, 0.101)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96098894 <a title="41-lda-1" href="./jmlr-2006-Segmental_Hidden_Markov_Models_with_Random_Effects_for_Waveform_Modeling.html">80 jmlr-2006-Segmental Hidden Markov Models with Random Effects for Waveform Modeling</a></p>
<p>Author: Seyoung Kim, Padhraic Smyth</p><p>Abstract: This paper proposes a general probabilistic framework for shape-based modeling and classiﬁcation of waveform data. A segmental hidden Markov model (HMM) is used to characterize waveform shape and shape variation is captured by adding random effects to the segmental model. The resulting probabilistic framework provides a basis for learning of waveform models from data as well as parsing and recognition of new waveforms. Expectation-maximization (EM) algorithms are derived and investigated for ﬁtting such models to data. In particular, the “expectation conditional maximization either” (ECME) algorithm is shown to provide signiﬁcantly faster convergence than a standard EM procedure. Experimental results on two real-world data sets demonstrate that the proposed approach leads to improved accuracy in classiﬁcation and segmentation when compared to alternatives such as Euclidean distance matching, dynamic time warping, and segmental HMMs without random effects. Keywords: waveform recognition, random effects, segmental hidden Markov models, EM algorithm, ECME</p><p>2 0.91490752 <a title="41-lda-2" href="./jmlr-2006-Stability_Properties_of_Empirical_Risk_Minimization_over_Donsker_Classes.html">84 jmlr-2006-Stability Properties of Empirical Risk Minimization over Donsker Classes</a></p>
<p>Author: Andrea Caponnetto, Alexander Rakhlin</p><p>Abstract: We study some stability properties of algorithms which minimize (or almost-minimize) empirical error over Donsker classes of functions. We show that, as the number n of samples grows, the L 2 1 diameter of the set of almost-minimizers of empirical error with tolerance ξ(n) = o(n − 2 ) converges to zero in probability. Hence, even in the case of multiple minimizers of expected error, as n increases it becomes less and less likely that adding a sample (or a number of samples) to the training set will result in a large jump to a new hypothesis. Moreover, under some assumptions on the entropy of the class, along with an assumption of Komlos-Major-Tusnady type, we derive a power rate of decay for the diameter of almost-minimizers. This rate, through an application of a uniform ratio limit inequality, is shown to govern the closeness of the expected errors of the almost-minimizers. In fact, under the above assumptions, the expected errors of almost-minimizers become closer with a rate strictly faster than n−1/2 . Keywords: empirical risk minimization, empirical processes, stability, Donsker classes</p><p>3 0.90641963 <a title="41-lda-3" href="./jmlr-2006-Considering_Cost_Asymmetry_in_Learning_Classifiers.html">22 jmlr-2006-Considering Cost Asymmetry in Learning Classifiers</a></p>
<p>Author: Francis R. Bach, David Heckerman, Eric Horvitz</p><p>Abstract: Receiver Operating Characteristic (ROC) curves are a standard way to display the performance of a set of binary classiﬁers for all feasible ratios of the costs associated with false positives and false negatives. For linear classiﬁers, the set of classiﬁers is typically obtained by training once, holding constant the estimated slope and then varying the intercept to obtain a parameterized set of classiﬁers whose performances can be plotted in the ROC plane. We consider the alternative of varying the asymmetry of the cost function used for training. We show that the ROC curve obtained by varying both the intercept and the asymmetry, and hence the slope, always outperforms the ROC curve obtained by varying only the intercept. In addition, we present a path-following algorithm for the support vector machine (SVM) that can compute efﬁciently the entire ROC curve, and that has the same computational complexity as training a single classiﬁer. Finally, we provide a theoretical analysis of the relationship between the asymmetric cost model assumed when training a classiﬁer and the cost model assumed in applying the classiﬁer. In particular, we show that the mismatch between the step function used for testing and its convex upper bounds, usually used for training, leads to a provable and quantiﬁable difference around extreme asymmetries. Keywords: support vector machines, receiver operating characteristic (ROC) analysis, linear classiﬁcation</p><p>same-paper 4 0.82470584 <a title="41-lda-4" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>5 0.69271916 <a title="41-lda-5" href="./jmlr-2006-On_Model_Selection_Consistency_of_Lasso.html">66 jmlr-2006-On Model Selection Consistency of Lasso</a></p>
<p>Author: Peng Zhao, Bin Yu</p><p>Abstract: Sparsity or parsimony of statistical models is crucial for their proper interpretations, as in sciences and social sciences. Model selection is a commonly used method to ﬁnd such models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani, 1996) is now being used as a computationally feasible alternative to model selection. Therefore it is important to study Lasso for model selection purposes. In this paper, we prove that a single condition, which we call the Irrepresentable Condition, is almost necessary and sufﬁcient for Lasso to select the true model both in the classical ﬁxed p setting and in the large p setting as the sample size n gets large. Based on these results, sufﬁcient conditions that are veriﬁable in practice are given to relate to previous works and help applications of Lasso for feature selection and sparse representation. This Irrepresentable Condition, which depends mainly on the covariance of the predictor variables, states that Lasso selects the true model consistently if and (almost) only if the predictors that are not in the true model are “irrepresentable” (in a sense to be clariﬁed) by predictors that are in the true model. Furthermore, simulations are carried out to provide insights and understanding of this result. Keywords: Lasso, regularization, sparsity, model selection, consistency</p><p>6 0.61649013 <a title="41-lda-6" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>7 0.60854793 <a title="41-lda-7" href="./jmlr-2006-Pattern_Recognition_for__Conditionally_Independent_Data.html">73 jmlr-2006-Pattern Recognition for  Conditionally Independent Data</a></p>
<p>8 0.60354245 <a title="41-lda-8" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>9 0.58146417 <a title="41-lda-9" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>10 0.57866395 <a title="41-lda-10" href="./jmlr-2006-Some_Theory_for_Generalized_Boosting_Algorithms.html">82 jmlr-2006-Some Theory for Generalized Boosting Algorithms</a></p>
<p>11 0.57721913 <a title="41-lda-11" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>12 0.57038623 <a title="41-lda-12" href="./jmlr-2006-Generalized_Bradley-Terry_Models_and_Multi-Class_Probability_Estimates.html">34 jmlr-2006-Generalized Bradley-Terry Models and Multi-Class Probability Estimates</a></p>
<p>13 0.56129396 <a title="41-lda-13" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<p>14 0.55553293 <a title="41-lda-14" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>15 0.55515319 <a title="41-lda-15" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>16 0.54689682 <a title="41-lda-16" href="./jmlr-2006-Noisy-OR_Component_Analysis_and_its_Application_to_Link_Analysis.html">64 jmlr-2006-Noisy-OR Component Analysis and its Application to Link Analysis</a></p>
<p>17 0.54602385 <a title="41-lda-17" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>18 0.54301363 <a title="41-lda-18" href="./jmlr-2006-Nonparametric_Quantile_Estimation.html">65 jmlr-2006-Nonparametric Quantile Estimation</a></p>
<p>19 0.54175436 <a title="41-lda-19" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>20 0.54022062 <a title="41-lda-20" href="./jmlr-2006-Lower_Bounds_and_Aggregation_in_Density_Estimation.html">58 jmlr-2006-Lower Bounds and Aggregation in Density Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
