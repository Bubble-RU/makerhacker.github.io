<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>27 jmlr-2006-Ensemble Pruning Via Semi-definite Programming     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-27" href="#">jmlr2006-27</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>27 jmlr-2006-Ensemble Pruning Via Semi-definite Programming     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-27-pdf" href="http://jmlr.org/papers/volume7/zhang06a/zhang06a.pdf">pdf</a></p><p>Author: Yi Zhang, Samuel Burer, W. Nick Street</p><p>Abstract: An ensemble is a group of learning models that jointly solve a problem. However, the ensembles generated by existing techniques are sometimes unnecessarily large, which can lead to extra memory usage, computational costs, and occasional decreases in eﬀectiveness. The purpose of ensemble pruning is to search for a good subset of ensemble members that performs as well as, or better than, the original ensemble. This subset selection problem is a combinatorial optimization problem and thus ﬁnding the exact optimal solution is computationally prohibitive. Various heuristic methods have been developed to obtain an approximate solution. However, most of the existing heuristics use simple greedy search as the optimization method, which lacks either theoretical or empirical quality guarantees. In this paper, the ensemble subset selection problem is formulated as a quadratic integer programming problem. By applying semi-deﬁnite programming (SDP) as a solution technique, we are able to get better approximate solutions. Computational experiments show that this SDP-based pruning algorithm outperforms other heuristics in the literature. Its application in a classiﬁer-sharing study also demonstrates the eﬀectiveness of the method. Keywords: ensemble pruning, semi-deﬁnite programming, heuristics, knowledge sharing</p><p>Reference: <a title="jmlr-2006-27-reference" href="../jmlr2006_reference/jmlr-2006-Ensemble_Pruning_Via_Semi-definite_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Bennett and Emilio Parrado-Hern´ndez a  Abstract An ensemble is a group of learning models that jointly solve a problem. [sent-6, score-0.805]
</p><p>2 However, the ensembles generated by existing techniques are sometimes unnecessarily large, which can lead to extra memory usage, computational costs, and occasional decreases in eﬀectiveness. [sent-7, score-0.098]
</p><p>3 The purpose of ensemble pruning is to search for a good subset of ensemble members that performs as well as, or better than, the original ensemble. [sent-8, score-1.92]
</p><p>4 This subset selection problem is a combinatorial optimization problem and thus ﬁnding the exact optimal solution is computationally prohibitive. [sent-9, score-0.03]
</p><p>5 Various heuristic methods have been developed to obtain an approximate solution. [sent-10, score-0.025]
</p><p>6 However, most of the existing heuristics use simple greedy search as the optimization method, which lacks either theoretical or empirical quality guarantees. [sent-11, score-0.028]
</p><p>7 In this paper, the ensemble subset selection problem is formulated as a quadratic integer programming problem. [sent-12, score-0.921]
</p><p>8 By applying semi-deﬁnite programming (SDP) as a solution technique, we are able to get better approximate solutions. [sent-13, score-0.046]
</p><p>9 Computational experiments show that this SDP-based pruning algorithm outperforms other heuristics in the literature. [sent-14, score-0.308]
</p><p>10 Keywords: ensemble pruning, semi-deﬁnite programming, heuristics, knowledge sharing  1. [sent-16, score-0.852]
</p><p>11 Introduction Ensemble methods are gaining more and more attention in the machine-learning and datamining communities. [sent-17, score-0.019]
</p><p>12 By deﬁnition, an ensemble is a group of learning models whose predictions are aggregated to give the ﬁnal prediction. [sent-18, score-0.805]
</p><p>13 It is widely accepted that an ensemble is usually better than a single classiﬁer given the same amount of training information. [sent-19, score-0.805]
</p><p>14 A number of eﬀective ensemble generation algorithms have been invented during the past decade, such as bagging (Breiman, 1996), boosting (Freund and Schapire, 1996), arcing (Breiman, 1998) and random forest (Breiman, 2001). [sent-20, score-0.949]
</p><p>15 The eﬀectiveness of the ensemble methods relies on creating a collection of diverse, yet accurate learning models. [sent-21, score-0.805]
</p><p>16 Two costs associated with ensemble methods are that they require much more memory to store all the learning models, and it takes much more computation time to get a prediction for an unlabeled data point. [sent-22, score-0.869]
</p><p>17 Although these extra costs may seem to be negligible with a small research data set, they may become serious when the ensemble method is applied to a large scale real-world data set. [sent-23, score-0.83]
</p><p>18 In fact, a large scale implementation of ensemble learning can easily generate an ensemble with thousands of learning models (Street and Kim, 2001; c 2006 Yi Zhang, Samuel Burer and Nick Street. [sent-24, score-1.61]
</p><p>19 For example, ensemble-based distributed data-mining techniques enable large companies (like WalMart) that store data at hundreds of diﬀerent locations to build learning models locally and then combine all the models for future prediction and knowledge discovery. [sent-27, score-0.061]
</p><p>20 For example, the boosting algorithm focuses on those training samples that are misclassiﬁed by the previous classiﬁer in each round of training and ﬁnally squeezes the training error to zero. [sent-30, score-0.039]
</p><p>21 If there is a certain amount of noise in the training data, the boosting ensemble will overﬁt (Opitz and Maclin, 1999; Dietterich, 2000). [sent-31, score-0.844]
</p><p>22 In such cases, it will be better to reduce the complexity of the learning model in order to correct the overﬁtting, like pruning a decision tree. [sent-32, score-0.28]
</p><p>23 For a boosting ensemble, selecting a subset of classiﬁers may improve the generalization performance. [sent-33, score-0.092]
</p><p>24 Ensemble methods have also been applied to mine streaming data (Street and Kim, 2001; Wang et al. [sent-34, score-0.044]
</p><p>25 The ensemble classiﬁers are trained from sequential chunks of the data stream. [sent-36, score-0.827]
</p><p>26 It is better to have a screening process that only keeps classiﬁers that match the current form of the drifting concept. [sent-38, score-0.022]
</p><p>27 For example, in a peer-to-peer spam email ﬁltering system, each email user can introduce spam ﬁlters from other users and construct an ensemble-ﬁlter. [sent-40, score-0.208]
</p><p>28 However, because of the diﬀerence of interest among email users, sharing ﬁlters indiscriminately is not a good solution. [sent-41, score-0.105]
</p><p>29 The sharing system should be able to pick ﬁlters that ﬁt the individuality of each user. [sent-42, score-0.047]
</p><p>30 All of the above reasons motivate the appearance of various ensemble pruning algorithms. [sent-43, score-1.085]
</p><p>31 A straightforward pruning method is to rank the classiﬁers according to their individual performance on a held-out test set and pick the best ones (Caruana et al. [sent-44, score-0.302]
</p><p>32 This simple approach may sometimes work well but is theoretically unsound. [sent-46, score-0.027]
</p><p>33 For example, an ensemble of three identical classiﬁers with 95% accuracy is worse than an ensemble of three classiﬁers with 67% accuracy and least pairwise correlated error (which is perfect! [sent-47, score-1.663]
</p><p>34 Margineantu and Dietterich (1997) proposed four approaches to prune ensembles generated by Adaboost. [sent-49, score-0.033]
</p><p>35 KL-divergence pruning and Kappa pruning aim at maximizing the pairwise diﬀerence between the selected ensemble members. [sent-50, score-1.439]
</p><p>36 Kappa-error convex hull pruning is a diagram-based heuristic targeting a good accuracy-divergence trade-oﬀ among the selected subset. [sent-51, score-0.348]
</p><p>37 Back-ﬁtting pruning is essentially enumerating all the possible subsets, which is computationally too costly for large ensembles. [sent-52, score-0.28]
</p><p>38 invented several pruning algorithms for their distributed data mining system (Prodromidis and Chan, 2000; Chan et al. [sent-54, score-0.331]
</p><p>39 One of the two algorithms they implemented is based on a diversity measure they deﬁned, and the other is based on class specialty metrics. [sent-56, score-0.099]
</p><p>40 The major problem with the above algorithms is that when it comes to optimizing some criteria of the selected subset, they all resort to greedy search, which is on the lower end of optimization techniques and usually without either theoretical or empirical quality guarantees. [sent-57, score-0.021]
</p><p>41 used an evolutionary algorithm for ensemble pruning and it turned out to be eﬀective (Kim et al. [sent-59, score-1.085]
</p><p>42 Unlike previous heuristic approaches, we formulate the ensemble pruning problem as a quadratic integer programming problem to look for a subset of classiﬁers that has the opti1316  Ensemble Pruning Via Semi-definite Programming  mal accuracy-diversity trade-oﬀ. [sent-63, score-1.249]
</p><p>43 Using a state-of-the-art semi-deﬁnite programming (SDP) solution technique, we are able to get a good approximate solution eﬃciently, although the original problem is NP-hard. [sent-64, score-0.046]
</p><p>44 Our new SDP-based ensemble pruning method is tested on a number of UCI repository data sets with Adaboost as the ensemble generation technique and compares favorably to two other metric-based pruning algorithms: diversity-based pruning and Kappa pruning. [sent-69, score-2.507]
</p><p>45 The same subset selection procedure is also applied to a classiﬁer sharing study. [sent-70, score-0.077]
</p><p>46 In that study, classiﬁers trained from diﬀerent but closely related problem domains are pooled together and then a subset of them is selected and assigned to each problem domain. [sent-71, score-0.051]
</p><p>47 Computational results show that the selected subset performs as well as, and sometimes better than, including all elements of the ensemble. [sent-72, score-0.097]
</p><p>48 Ensemble pruning can be viewed as a discrete version of weight-based ensemble optimization. [sent-73, score-1.085]
</p><p>49 The more general weight-based ensemble optimization aims to improve the generalization performance of the ensemble by tuning the weight on each ensemble member. [sent-74, score-2.438]
</p><p>50 If the prediction target is continuous, derivative methods can be applied to obtain the optimal weight on each ensemble model (Krogh and Vedelsby, 1995; Zhou et al. [sent-75, score-0.826]
</p><p>51 Those optimization approaches are eﬀective in performance enhancement according to empirical results and are sometimes able to signiﬁcantly reduce the size the ensemble when there are many zeros in the weights (Demiriz et al. [sent-80, score-0.832]
</p><p>52 The proposed ensemble pruning method distinguishes from the above methods by explicitly constraining the weights to be binary and using a cardinality constraint to set the size of the ﬁnal ensemble. [sent-83, score-1.104]
</p><p>53 The goal of ensemble pruning is to contain the size of the ensemble without compromising its performance, which is subtly diﬀerent from that of general weight-based ensemble optimization. [sent-84, score-2.717]
</p><p>54 Section 2 describes the pruning algorithm in detail, including the mathematical formulation and the solution technique. [sent-86, score-0.298]
</p><p>55 Section 3 shows the experimental results on the UCI repository data sets and compares our method with other pruning algorithms. [sent-87, score-0.301]
</p><p>56 Section 4 is devoted to the algorithm’s application in a classiﬁer-sharing case study with a direct marketing data set. [sent-88, score-0.022]
</p><p>57 The individual accuracy and pairwise independence of classiﬁers in an ensemble are often referred to as strength and divergence of the ensemble. [sent-93, score-0.983]
</p><p>58 Breiman (2001) 1317  Zhang, Burer and Street  ¯ showed that the generalization error of an ensemble is loosely bounded by sρ , where ρ is ¯ 2 the average correlation between classiﬁers and s is the overall strength of the classiﬁers. [sent-94, score-0.903]
</p><p>59 For continuous prediction problems, there are even closed-form representations for the ensemble generalization performance based on individual error and diversity. [sent-95, score-0.871]
</p><p>60 Krogh and Vedelsby (Krogh and Vedelsby, 1995) showed that for a neural network ensemble, the generalization error ¯ ¯ E = E − A,  ¯ ¯ where E is the weighted average of the error of the individual networks and A is the variance among the networks. [sent-96, score-0.045]
</p><p>61 (2001) give another form, E=  Cij , i,j  where Cij =  p(x) fi (x) − d(x)  fj (x) − d(x) dx,  p(x) is the density of input x, fi (x) is the output of the ith network and d(x) is the true output. [sent-98, score-0.069]
</p><p>62 Note that Cii is the error of the ith network and Cij, i=j is a pairwise correlation-like measurement. [sent-99, score-0.086]
</p><p>63 Therefore, there must be a trade-oﬀ between the strength and the divergence of an ensemble. [sent-101, score-0.103]
</p><p>64 What we are looking for is a subset of classiﬁers with the best trade-oﬀ so that the generalization performance can be optimized. [sent-102, score-0.071]
</p><p>65 In order to get the mathematical formulation of the ensemble pruning problem, we need to represent the error structure of the existing ensemble in a nice way. [sent-103, score-1.908]
</p><p>66 Unlike the case of continuous prediction, there is no exact closed-form representation for the ensemble error in terms of strength and diversity for a discrete classiﬁcation problem. [sent-104, score-0.979]
</p><p>67 From the error analysis of continuous problems, we notice that the ensemble error can be represented by a linear combination of the individual accuracy terms and pairwise diversity terms. [sent-106, score-0.979]
</p><p>68 Therefore, if we are able to ﬁnd strength and diversity measurements for a classiﬁcation ensemble, a linear combination of them should serve as a good approximation of the overall ensemble error. [sent-107, score-1.003]
</p><p>69 Minimizing this approximate ensemble error function will be the objective of the mathematical programming formulation. [sent-108, score-0.851]
</p><p>70 Thus, the diagonal term Gii is the total number of errors made by classiﬁer i and the oﬀ-diagonal term Gij is the number of common errors of classiﬁer pair i and j. [sent-111, score-0.023]
</p><p>71 To put all the elements of the G matrix on the same scale, we normalize them by Gii ˜ , Gii = N Gij 1 Gij ˜ Gij, i=j = + , 2 Gii Gjj 1318  (2)  Ensemble Pruning Via Semi-definite Programming  ˜ where N is the number of training points. [sent-112, score-0.019]
</p><p>72 After normalization, all elements of the G matrix ˜ ˜ are between 0 and 1. [sent-113, score-0.019]
</p><p>73 Taking the average of Gij and ii Gij ˜ ˜ as the oﬀ-diagonal elements of G makes the matrix symmetric. [sent-116, score-0.019]
</p><p>74 The constructed G Gjj  matrix captures both the strength (diagonal elements) and the pairwise divergence (oﬀdiagonal elements) of the ensemble classiﬁers. [sent-117, score-0.961]
</p><p>75 It is self-evident that for a good ensemble, all ˜ ˜ elements of the G matrix should be small. [sent-118, score-0.019]
</p><p>76 Intuitively, i Gii measures the overall strength ˜ ij measures the diversity. [sent-119, score-0.11]
</p><p>77 A combination of these of the ensemble classiﬁers and ij, i=j G ˜ two terms, ij Gij should be a good approximation of the ensemble error. [sent-120, score-1.645]
</p><p>78 We have noticed that there exist many other heuristic pairwise measurements for ensemble diversity. [sent-122, score-0.907]
</p><p>79 For instance, the disagreement measure (Ho, 1998), κ statistic (Fleiss, 1981), Yule’s Q statistic (Yule, 1900), and so on. [sent-123, score-0.072]
</p><p>80 However, we found that the choice of the diversity measurement did not make a signiﬁcant diﬀerence in terms of performance according to our computational experiments. [sent-124, score-0.099]
</p><p>81 Now we can formulate the subset selection problem as a quadratic integer programming problem. [sent-126, score-0.12]
</p><p>82 Essentially, we are looking for a ﬁxed-size subset of classiﬁers, with the sum of ˜ the corresponding elements in the G matrix minimized. [sent-127, score-0.067]
</p><p>83 The mathematical programming formulation is as follows, ˜ min xT Gx x  s. [sent-128, score-0.064]
</p><p>84 The binary variable xi represents whether the ith classiﬁer will be chosen. [sent-131, score-0.033]
</p><p>85 If xi = 1, which means that the ith classiﬁer is included in the selected set, its corresponding diagonal and oﬀ-diagonal elements will be counted in the objective function. [sent-132, score-0.114]
</p><p>86 Note that the cardinality constraint i xi = k is mathematically important because without it, there is only one trivial solution to the problem with none of the classiﬁers picked. [sent-133, score-0.037]
</p><p>87 In addition, it gives us control over the size of the selected subset. [sent-134, score-0.021]
</p><p>88 This quadratic integer programming problem is a standard 0-1 optimization problem, which is NP-hard in general. [sent-135, score-0.068]
</p><p>89 Fortunately, we found that this formulation is close to that of the so-called “max cut with size k” problem (written MC-k), in which one partitions the vertices of an edge-weighted graph into two sets, one of which has size k, so that the total weight of edges crossing the partition is maximized. [sent-136, score-0.018]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ensemble', 0.805), ('pruning', 0.28), ('gij', 0.229), ('gii', 0.178), ('vedelsby', 0.102), ('diversity', 0.099), ('erent', 0.093), ('breiman', 0.082), ('burer', 0.077), ('krogh', 0.077), ('cij', 0.076), ('strength', 0.075), ('ers', 0.07), ('classi', 0.069), ('street', 0.066), ('ective', 0.065), ('di', 0.062), ('email', 0.058), ('erence', 0.053), ('pairwise', 0.053), ('gjj', 0.051), ('invented', 0.051), ('iowa', 0.051), ('margineantu', 0.051), ('pij', 0.051), ('yule', 0.051), ('kim', 0.051), ('sharing', 0.047), ('sdp', 0.046), ('programming', 0.046), ('er', 0.045), ('lters', 0.044), ('nick', 0.043), ('prodromidis', 0.043), ('zhou', 0.04), ('boosting', 0.039), ('kappa', 0.039), ('samuel', 0.039), ('ectiveness', 0.035), ('ij', 0.035), ('ith', 0.033), ('spam', 0.033), ('demiriz', 0.033), ('ensembles', 0.033), ('dietterich', 0.032), ('chan', 0.031), ('misclassi', 0.03), ('subset', 0.03), ('divergence', 0.028), ('heuristics', 0.028), ('sometimes', 0.027), ('users', 0.026), ('zhang', 0.025), ('statistic', 0.025), ('costs', 0.025), ('heuristic', 0.025), ('measurements', 0.024), ('generalization', 0.023), ('diagonal', 0.023), ('formulate', 0.022), ('integer', 0.022), ('individual', 0.022), ('chunks', 0.022), ('caruana', 0.022), ('gx', 0.022), ('companies', 0.022), ('compromising', 0.022), ('disagreement', 0.022), ('drifting', 0.022), ('marketing', 0.022), ('mine', 0.022), ('streaming', 0.022), ('targeting', 0.022), ('wij', 0.022), ('wolpert', 0.022), ('selected', 0.021), ('repository', 0.021), ('prediction', 0.021), ('cardinality', 0.019), ('ho', 0.019), ('chawla', 0.019), ('gaining', 0.019), ('mal', 0.019), ('occasional', 0.019), ('unnecessarily', 0.019), ('elements', 0.019), ('formulation', 0.018), ('formulated', 0.018), ('fi', 0.018), ('generation', 0.018), ('yj', 0.018), ('mathematically', 0.018), ('favorably', 0.018), ('adaboost', 0.018), ('arcing', 0.018), ('bagging', 0.018), ('ia', 0.018), ('counted', 0.018), ('looking', 0.018), ('store', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="27-tfidf-1" href="./jmlr-2006-Ensemble_Pruning_Via_Semi-definite_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">27 jmlr-2006-Ensemble Pruning Via Semi-definite Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Yi Zhang, Samuel Burer, W. Nick Street</p><p>Abstract: An ensemble is a group of learning models that jointly solve a problem. However, the ensembles generated by existing techniques are sometimes unnecessarily large, which can lead to extra memory usage, computational costs, and occasional decreases in eﬀectiveness. The purpose of ensemble pruning is to search for a good subset of ensemble members that performs as well as, or better than, the original ensemble. This subset selection problem is a combinatorial optimization problem and thus ﬁnding the exact optimal solution is computationally prohibitive. Various heuristic methods have been developed to obtain an approximate solution. However, most of the existing heuristics use simple greedy search as the optimization method, which lacks either theoretical or empirical quality guarantees. In this paper, the ensemble subset selection problem is formulated as a quadratic integer programming problem. By applying semi-deﬁnite programming (SDP) as a solution technique, we are able to get better approximate solutions. Computational experiments show that this SDP-based pruning algorithm outperforms other heuristics in the literature. Its application in a classiﬁer-sharing study also demonstrates the eﬀectiveness of the method. Keywords: ensemble pruning, semi-deﬁnite programming, heuristics, knowledge sharing</p><p>2 0.081187077 <a title="27-tfidf-2" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>3 0.066336043 <a title="27-tfidf-3" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>Author: Martin J. Wainwright</p><p>Abstract: Consider the problem of joint parameter estimation and prediction in a Markov random ﬁeld: that is, the model parameters are estimated on the basis of an initial set of data, and then the ﬁtted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working under the restriction of limited computation, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for ﬁtting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the “wrong” model even in the inﬁnite data limit) is provably beneﬁcial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of convex variational methods. This stability result provides additional incentive, apart from the obvious beneﬁt of unique global optima, for using message-passing methods based on convex variational relaxations. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. Keywords: graphical model, Markov random ﬁeld, belief propagation, variational method, parameter estimation, prediction error, algorithmic stability</p><p>4 0.045529429 <a title="27-tfidf-4" href="./jmlr-2006-Machine_Learning_for_Computer_Security%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">59 jmlr-2006-Machine Learning for Computer Security    (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Philip K. Chan, Richard P. Lippmann</p><p>Abstract: The prevalent use of computers and internet has enhanced the quality of life for many people, but it has also attracted undesired attempts to undermine these systems. This special topic contains several research studies on how machine learning algorithms can help improve the security of computer systems. Keywords: computer security, spam, images with embedded text, malicious executables, network protocols, encrypted trafﬁc</p><p>5 0.043323498 <a title="27-tfidf-5" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tijl De Bie, Nello Cristianini</p><p>Abstract: The rise of convex programming has changed the face of many research ﬁelds in recent years, machine learning being one of the ones that beneﬁtted the most. A very recent developement, the relaxation of combinatorial problems to semi-deﬁnite programs (SDP), has gained considerable attention over the last decade (Helmberg, 2000; De Bie and Cristianini, 2004a). Although SDP problems can be solved in polynomial time, for many relaxations the exponent in the polynomial complexity bounds is too high for scaling to large problem sizes. This has hampered their uptake as a powerful new tool in machine learning. In this paper, we present a new and fast SDP relaxation of the normalized graph cut problem, and investigate its usefulness in unsupervised and semi-supervised learning. In particular, this provides a convex algorithm for transduction, as well as approaches to clustering. We further propose a whole cascade of fast relaxations that all hold the middle between older spectral relaxations and the new SDP relaxation, allowing one to trade oﬀ computational cost versus relaxation accuracy. Finally, we discuss how the methodology developed in this paper can be applied to other combinatorial problems in machine learning, and we treat the max-cut problem as an example. Keywords: convex transduction, normalized graph cut, semi-deﬁnite programming, semisupervised learning, relaxation, combinatorial optimization, max-cut</p><p>6 0.038984299 <a title="27-tfidf-6" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>7 0.034123205 <a title="27-tfidf-7" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>8 0.026604876 <a title="27-tfidf-8" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.024056023 <a title="27-tfidf-9" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>10 0.021751015 <a title="27-tfidf-10" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.020280749 <a title="27-tfidf-11" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>12 0.020117693 <a title="27-tfidf-12" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>13 0.01942715 <a title="27-tfidf-13" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>14 0.018986804 <a title="27-tfidf-14" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>15 0.01898217 <a title="27-tfidf-15" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>16 0.016573487 <a title="27-tfidf-16" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.016506221 <a title="27-tfidf-17" href="./jmlr-2006-Generalized_Bradley-Terry_Models_and_Multi-Class_Probability_Estimates.html">34 jmlr-2006-Generalized Bradley-Terry Models and Multi-Class Probability Estimates</a></p>
<p>18 0.016131645 <a title="27-tfidf-18" href="./jmlr-2006-Considering_Cost_Asymmetry_in_Learning_Classifiers.html">22 jmlr-2006-Considering Cost Asymmetry in Learning Classifiers</a></p>
<p>19 0.016099045 <a title="27-tfidf-19" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.015898995 <a title="27-tfidf-20" href="./jmlr-2006-Quantile_Regression_Forests.html">77 jmlr-2006-Quantile Regression Forests</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.111), (1, -0.047), (2, -0.011), (3, 0.063), (4, -0.026), (5, -0.011), (6, -0.039), (7, 0.033), (8, 0.036), (9, -0.028), (10, -0.186), (11, -0.048), (12, -0.037), (13, 0.009), (14, -0.074), (15, 0.001), (16, -0.115), (17, -0.044), (18, 0.062), (19, 0.065), (20, 0.119), (21, -0.099), (22, 0.006), (23, -0.007), (24, -0.115), (25, 0.082), (26, -0.101), (27, -0.091), (28, -0.299), (29, 0.198), (30, 0.202), (31, 0.16), (32, -0.34), (33, 0.091), (34, 0.014), (35, 0.304), (36, -0.064), (37, -0.088), (38, 0.11), (39, 0.14), (40, -0.01), (41, -0.122), (42, -0.073), (43, -0.17), (44, 0.136), (45, 0.184), (46, 0.135), (47, -0.013), (48, -0.103), (49, -0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97242856 <a title="27-lsi-1" href="./jmlr-2006-Ensemble_Pruning_Via_Semi-definite_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">27 jmlr-2006-Ensemble Pruning Via Semi-definite Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Yi Zhang, Samuel Burer, W. Nick Street</p><p>Abstract: An ensemble is a group of learning models that jointly solve a problem. However, the ensembles generated by existing techniques are sometimes unnecessarily large, which can lead to extra memory usage, computational costs, and occasional decreases in eﬀectiveness. The purpose of ensemble pruning is to search for a good subset of ensemble members that performs as well as, or better than, the original ensemble. This subset selection problem is a combinatorial optimization problem and thus ﬁnding the exact optimal solution is computationally prohibitive. Various heuristic methods have been developed to obtain an approximate solution. However, most of the existing heuristics use simple greedy search as the optimization method, which lacks either theoretical or empirical quality guarantees. In this paper, the ensemble subset selection problem is formulated as a quadratic integer programming problem. By applying semi-deﬁnite programming (SDP) as a solution technique, we are able to get better approximate solutions. Computational experiments show that this SDP-based pruning algorithm outperforms other heuristics in the literature. Its application in a classiﬁer-sharing study also demonstrates the eﬀectiveness of the method. Keywords: ensemble pruning, semi-deﬁnite programming, heuristics, knowledge sharing</p><p>2 0.28927678 <a title="27-lsi-2" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>Author: Martin J. Wainwright</p><p>Abstract: Consider the problem of joint parameter estimation and prediction in a Markov random ﬁeld: that is, the model parameters are estimated on the basis of an initial set of data, and then the ﬁtted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working under the restriction of limited computation, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for ﬁtting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the “wrong” model even in the inﬁnite data limit) is provably beneﬁcial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of convex variational methods. This stability result provides additional incentive, apart from the obvious beneﬁt of unique global optima, for using message-passing methods based on convex variational relaxations. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. Keywords: graphical model, Markov random ﬁeld, belief propagation, variational method, parameter estimation, prediction error, algorithmic stability</p><p>3 0.26245776 <a title="27-lsi-3" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>4 0.25969574 <a title="27-lsi-4" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>Author: Janez Demšar</p><p>Abstract: While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classiﬁers: the Wilcoxon signed ranks test for comparison of two classiﬁers and the Friedman test with the corresponding post-hoc tests for comparison of more classiﬁers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams. Keywords: comparative studies, statistical methods, Wilcoxon signed ranks test, Friedman test, multiple comparisons tests</p><p>5 0.24693641 <a title="27-lsi-5" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tijl De Bie, Nello Cristianini</p><p>Abstract: The rise of convex programming has changed the face of many research ﬁelds in recent years, machine learning being one of the ones that beneﬁtted the most. A very recent developement, the relaxation of combinatorial problems to semi-deﬁnite programs (SDP), has gained considerable attention over the last decade (Helmberg, 2000; De Bie and Cristianini, 2004a). Although SDP problems can be solved in polynomial time, for many relaxations the exponent in the polynomial complexity bounds is too high for scaling to large problem sizes. This has hampered their uptake as a powerful new tool in machine learning. In this paper, we present a new and fast SDP relaxation of the normalized graph cut problem, and investigate its usefulness in unsupervised and semi-supervised learning. In particular, this provides a convex algorithm for transduction, as well as approaches to clustering. We further propose a whole cascade of fast relaxations that all hold the middle between older spectral relaxations and the new SDP relaxation, allowing one to trade oﬀ computational cost versus relaxation accuracy. Finally, we discuss how the methodology developed in this paper can be applied to other combinatorial problems in machine learning, and we treat the max-cut problem as an example. Keywords: convex transduction, normalized graph cut, semi-deﬁnite programming, semisupervised learning, relaxation, combinatorial optimization, max-cut</p><p>6 0.23665248 <a title="27-lsi-6" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>7 0.19128174 <a title="27-lsi-7" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>8 0.17511691 <a title="27-lsi-8" href="./jmlr-2006-Machine_Learning_for_Computer_Security%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">59 jmlr-2006-Machine Learning for Computer Security    (Special Topic on Machine Learning for Computer Security)</a></p>
<p>9 0.16474825 <a title="27-lsi-9" href="./jmlr-2006-Consistency_of_Multiclass_Empirical_Risk_Minimization_Methods_Based_on_Convex_Loss.html">24 jmlr-2006-Consistency of Multiclass Empirical Risk Minimization Methods Based on Convex Loss</a></p>
<p>10 0.15954208 <a title="27-lsi-10" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>11 0.15445298 <a title="27-lsi-11" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.12784652 <a title="27-lsi-12" href="./jmlr-2006-Evolutionary_Function_Approximation_for_Reinforcement_Learning.html">30 jmlr-2006-Evolutionary Function Approximation for Reinforcement Learning</a></p>
<p>13 0.11825299 <a title="27-lsi-13" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>14 0.1135006 <a title="27-lsi-14" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.11088824 <a title="27-lsi-15" href="./jmlr-2006-Considering_Cost_Asymmetry_in_Learning_Classifiers.html">22 jmlr-2006-Considering Cost Asymmetry in Learning Classifiers</a></p>
<p>16 0.11024188 <a title="27-lsi-16" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>17 0.1066319 <a title="27-lsi-17" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>18 0.10634395 <a title="27-lsi-18" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>19 0.10218611 <a title="27-lsi-19" href="./jmlr-2006-Inductive_Synthesis_of_Functional_Programs%3A_An_Explanation_Based_Generalization_Approach_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">39 jmlr-2006-Inductive Synthesis of Functional Programs: An Explanation Based Generalization Approach     (Special Topic on Inductive Programming)</a></p>
<p>20 0.10024182 <a title="27-lsi-20" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.012), (36, 0.101), (50, 0.031), (63, 0.037), (68, 0.015), (70, 0.021), (71, 0.428), (76, 0.015), (78, 0.022), (81, 0.027), (84, 0.022), (90, 0.017), (91, 0.03), (96, 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74540156 <a title="27-lda-1" href="./jmlr-2006-Ensemble_Pruning_Via_Semi-definite_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">27 jmlr-2006-Ensemble Pruning Via Semi-definite Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Yi Zhang, Samuel Burer, W. Nick Street</p><p>Abstract: An ensemble is a group of learning models that jointly solve a problem. However, the ensembles generated by existing techniques are sometimes unnecessarily large, which can lead to extra memory usage, computational costs, and occasional decreases in eﬀectiveness. The purpose of ensemble pruning is to search for a good subset of ensemble members that performs as well as, or better than, the original ensemble. This subset selection problem is a combinatorial optimization problem and thus ﬁnding the exact optimal solution is computationally prohibitive. Various heuristic methods have been developed to obtain an approximate solution. However, most of the existing heuristics use simple greedy search as the optimization method, which lacks either theoretical or empirical quality guarantees. In this paper, the ensemble subset selection problem is formulated as a quadratic integer programming problem. By applying semi-deﬁnite programming (SDP) as a solution technique, we are able to get better approximate solutions. Computational experiments show that this SDP-based pruning algorithm outperforms other heuristics in the literature. Its application in a classiﬁer-sharing study also demonstrates the eﬀectiveness of the method. Keywords: ensemble pruning, semi-deﬁnite programming, heuristics, knowledge sharing</p><p>2 0.64331555 <a title="27-lda-2" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>Author: Dmitry M. Malioutov, Jason K. Johnson, Alan S. Willsky</p><p>Abstract: We present a new framework based on walks in a graph for analysis and inference in Gaussian graphical models. The key idea is to decompose the correlation between each pair of variables as a sum over all walks between those variables in the graph. The weight of each walk is given by a product of edgewise partial correlation coefﬁcients. This representation holds for a large class of Gaussian graphical models which we call walk-summable. We give a precise characterization of this class of models, and relate it to other classes including diagonally dominant, attractive, nonfrustrated, and pairwise-normalizable. We provide a walk-sum interpretation of Gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles. The walk-sum perspective leads to a better understanding of Gaussian belief propagation and to stronger results for its convergence in loopy graphs. Keywords: Gaussian graphical models, walk-sum analysis, convergence of loopy belief propagation</p><p>3 0.32832405 <a title="27-lda-3" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>Author: Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou</p><p>Abstract: We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the ﬁrst time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction. html. Keywords: transduction, transductive SVMs, semi-supervised learning, CCCP</p><p>4 0.32447177 <a title="27-lda-4" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>Author: Greg Hamerly, Erez Perelman, Jeremy Lau, Brad Calder, Timothy Sherwood</p><p>Abstract: An essential step in designing a new computer architecture is the careful examination of different design options. It is critical that computer architects have efﬁcient means by which they may estimate the impact of various design options on the overall machine. This task is complicated by the fact that different programs, and even different parts of the same program, may have distinct behaviors that interact with the hardware in different ways. Researchers use very detailed simulators to estimate processor performance, which models every cycle of an executing program. Unfortunately, simulating every cycle of a real program can take weeks or months. To address this problem we have created a tool called SimPoint that uses data clustering algorithms from machine learning to automatically ﬁnd repetitive patterns in a program’s execution. By simulating one representative of each repetitive behavior pattern, simulation time can be reduced to minutes instead of weeks for standard benchmark programs, with very little cost in terms of accuracy. We describe this important problem, the data representation and preprocessing methods used by SimPoint, the clustering algorithm at the core of SimPoint, and we evaluate different options for tuning SimPoint. Keywords: k-means, random projection, Bayesian information criterion, simulation, SimPoint</p><p>5 0.32097679 <a title="27-lda-5" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>Author: Masashi Sugiyama</p><p>Abstract: The goal of active learning is to determine the locations of training input points so that the generalization error is minimized. We discuss the problem of active learning in linear regression scenarios. Traditional active learning methods using least-squares learning often assume that the model used for learning is correctly speciﬁed. In many practical situations, however, this assumption may not be fulﬁlled. Recently, active learning methods using “importance”-weighted least-squares learning have been proposed, which are shown to be robust against misspeciﬁcation of models. In this paper, we propose a new active learning method also using the weighted least-squares learning, which we call ALICE (Active Learning using the Importance-weighted least-squares learning based on Conditional Expectation of the generalization error). An important difference from existing methods is that we predict the conditional expectation of the generalization error given training input points, while existing methods predict the full expectation of the generalization error. Due to this difference, the training input design can be ﬁne-tuned depending on the realization of training input points. Theoretically, we prove that the proposed active learning criterion is a more accurate predictor of the single-trial generalization error than the existing criterion. Numerical studies with toy and benchmark data sets show that the proposed method compares favorably to existing methods. Keywords: Active Learning, Conditional Expectation of Generalization Error, Misspeciﬁcation of Models, Importance-Weighted Least-Squares Learning, Covariate Shift.</p><p>6 0.32000366 <a title="27-lda-6" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.31345701 <a title="27-lda-7" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>8 0.313151 <a title="27-lda-8" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>9 0.31289735 <a title="27-lda-9" href="./jmlr-2006-Adaptive_Prototype_Learning_Algorithms%3A_Theoretical_and_Experimental_Studies.html">13 jmlr-2006-Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies</a></p>
<p>10 0.31199354 <a title="27-lda-10" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.31076577 <a title="27-lda-11" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.31040031 <a title="27-lda-12" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>13 0.31012768 <a title="27-lda-13" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.30909586 <a title="27-lda-14" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.30806404 <a title="27-lda-15" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>16 0.30332893 <a title="27-lda-16" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.30168462 <a title="27-lda-17" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>18 0.30107927 <a title="27-lda-18" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>19 0.30105105 <a title="27-lda-19" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>20 0.3001551 <a title="27-lda-20" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
