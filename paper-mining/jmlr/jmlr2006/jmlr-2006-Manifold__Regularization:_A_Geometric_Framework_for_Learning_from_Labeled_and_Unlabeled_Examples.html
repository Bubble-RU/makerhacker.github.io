<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-60" href="#">jmlr2006-60</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</h1>
<br/><p>Source: <a title="jmlr-2006-60-pdf" href="http://jmlr.org/papers/volume7/belkin06a/belkin06a.pdf">pdf</a></p><p>Author: Mikhail Belkin, Partha Niyogi, Vikas Sindhwani</p><p>Abstract: We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework. Keywords: semi-supervised learning, graph transduction, regularization, kernel methods, manifold learning, spectral graph theory, unlabeled data, support vector machines</p><p>Reference: <a title="jmlr-2006-60-reference" href="../jmlr2006_reference/jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. [sent-10, score-0.47]
</p><p>2 Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. [sent-11, score-0.225]
</p><p>3 We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. [sent-14, score-0.315]
</p><p>4 Keywords: semi-supervised learning, graph transduction, regularization, kernel methods, manifold learning, spectral graph theory, unlabeled data, support vector machines  1. [sent-16, score-0.695]
</p><p>5 The problem of learning from labeled and unlabeled data (semi-supervised and transductive learning) has attracted considerable attention in recent years. [sent-21, score-0.612]
</p><p>6 Hence, there are two regularization terms— one controlling the complexity of the classiﬁer in the ambient space and the other controlling the complexity as measured by the geometry of the distribution. [sent-42, score-0.248]
</p><p>7 The second is the geometric point of view embodied in a class of algorithms that can be termed as manifold learning. [sent-51, score-0.227]
</p><p>8 In particular, a new Representer theorem provides a functional form of the solution when the distribution is known; its empirical version involves an expansion over labeled and unlabeled points when the distribution is unknown. [sent-78, score-0.47]
</p><p>9 1 The Signiﬁcance of Semi-Supervised Learning From an engineering standpoint, it is clear that collecting labeled data is generally more involved than collecting unlabeled data. [sent-90, score-0.47]
</p><p>10 As a result, an approach to pattern recognition that is able to make better use of unlabeled data to improve recognition performance is of potentially great practical signiﬁcance. [sent-91, score-0.315]
</p><p>11 These stimuli comprise the unlabeled data that we have easy access to. [sent-95, score-0.315]
</p><p>12 , learning clusters and categories of objects) suggests that unlabeled data can be usefully processed to learn natural invariances, to form categories, and to develop classiﬁers. [sent-102, score-0.315]
</p><p>13 Therefore the success of human learning in this “small sample” regime is plausibly due to effective utilization of the large amounts of unlabeled data to extract information that is useful for generalization. [sent-104, score-0.315]
</p><p>14 Figure 1 illustrates how unlabeled 2401  B ELKIN , N IYOGI AND S INDHWANI  Figure 1: Unlabeled data and prior beliefs  examples may force us to restructure our hypotheses during learning. [sent-106, score-0.315]
</p><p>15 Now consider the situation where one is given additional unlabeled examples as shown in the right panel. [sent-111, score-0.315]
</p><p>16 We argue that it is self-evident that in the light of this new unlabeled set, one must re-evaluate one’s prior notion of simplicity. [sent-112, score-0.315]
</p><p>17 2 Outline of the Paper The paper is organized as follows: in Section 2, we develop the basic framework for semi-supervised learning where we ultimately formulate an objective function that can use both labeled and unlabeled data. [sent-118, score-0.47]
</p><p>18 In Section 5, we describe experiments that evaluate the algorithms and demonstrate the usefulness of unlabeled data. [sent-124, score-0.315]
</p><p>19 The Representer Theorem above allows us to express the solution f ∗ directly in terms of the labeled data, the (ambient) kernel K, and the marginal P X . [sent-162, score-0.263]
</p><p>20 In the next section, we consider a particular approximation scheme that leads to a simple algorithmic framework for learning from labeled and unlabeled data. [sent-165, score-0.47]
</p><p>21 Note that in order to get such empirical estimates it is sufﬁcient to have unlabeled examples. [sent-169, score-0.315]
</p><p>22 In R that case, one natural choice for f I is x∈M ∇M f 2 d PX (x), where ∇M is the gradient (see, for example Do Carmo, 1992, for an introduction to differential geometry) of f along the manifold M and the integral is taken over the marginal distribution. [sent-173, score-0.308]
</p><p>23 The term x∈M ∇M f 2 d PX (x) may be approximated on the basis of labeled and unlabeled data using the graph Laplacian associated to the data. [sent-175, score-0.517]
</p><p>24 i, j=1 The following version of the Representer Theorem shows that the minimizer has an expansion in terms of both labeled and unlabeled examples and is a key to our algorithms. [sent-188, score-0.505]
</p><p>25 Theorem 2 The minimizer of optimization problem 4 admits an expansion l+u  f ∗ (x) = ∑ αi K(xi , x)  (5)  i=1  in terms of the labeled and unlabeled examples. [sent-189, score-0.537]
</p><p>26 Therefore regularization with respect only to the sampled manifold is ill-posed. [sent-214, score-0.297]
</p><p>27 There may be situations when regularization with respect to the ambient space yields a better solution, for example, when the manifold assumption does not hold (or holds to a lesser degree). [sent-217, score-0.381]
</p><p>28 On the other hand, a different ambient kernel restricted to M can potentially serve as the intrinsic regularization term. [sent-228, score-0.333]
</p><p>29 Thus one (sharper) kernel may be used in conjunction with unlabeled data to estimate the heat kernel on M and a wider kernel for inference. [sent-230, score-0.539]
</p><p>30 Writing a function f in that basis, we have f = ∑ ai ei (x) and K(x, · ), f (·) µ = ∑i λi ai ei (x). [sent-274, score-0.228]
</p><p>31 Thus we see that e j (x) = K(x, ·), e j (·) K = ∑ λi ei (x) ei , e j K . [sent-278, score-0.228]
</p><p>32 i  1 Therefore ei , e j K = 0, if i = j, and ei , ei K = λi . [sent-279, score-0.342]
</p><p>33 On the other hand ei , e j µ = 0, if i = j, and ei , ei µ = 1. [sent-280, score-0.342]
</p><p>34 i a2  Since LK (∑i bi ei ) = ∑i bi λi ei = ∑i ai ei , we obtain ai = bi λi . [sent-294, score-0.342]
</p><p>35 minimizer f  As a direct corollary of these consideration, we obtain the following Proposition 6 If f I = f KM then the minimizer of Equation 2 is identical to that of the usual regularization problem (Equation 1) although with a different regularization parameter (λ A + λI ). [sent-334, score-0.292]
</p><p>36 We have ∑i ai ei (x) ating with respect to the coefﬁcients ai yields the following set of equations: 0=  ∂H( f ∗ ) 1 = ∂ak l  l  ak  ∑ ek (x j )∂3V (x j , y j , ∑ ai ei ) + 2γA λk + γI  2 K  = ∑i  a2 i λi . [sent-365, score-0.32]
</p><p>37 D f , ek + f , Dek = (D + D∗ ) f , ek and hence ak = −  λk 2γA l  γI  l  ∑ ek (x j )∂3V (x j , y j , f ∗ ) − 2γA λk  D f ∗ + D ∗ f ∗ , ek . [sent-367, score-0.368]
</p><p>38 j=1  Since f ∗ (x) = ∑k ak ek (x) and recalling that K(x, y) = ∑i λi ei (x)ei (y) f ∗ (x) = −  l 1 γI ∑ ∑ λk ek (x)ek (x j )∂3V (x j , y j , f ∗ ) − 2γA ∑ λk D f ∗ + D∗ f ∗ , ek ek , 2γA l k j=1 k  =−  1 2γA l  γI  l  ∑ K(x, x j )∂3V (x j , y j , f ∗ ) − 2γA ∑ λk  j=1  D f ∗ + D ∗ f ∗ , ek e k . [sent-368, score-0.574]
</p><p>39 3 Manifold Setting2 We now show that for the case when M is a manifold and D is a differential operator, such as the Laplace-Beltrami operator ∆M , the boundedness condition of Theorem 7 is satisﬁed. [sent-375, score-0.314]
</p><p>40 While we consider the case when the manifold has no boundary, the same argument goes through for manifold with boundary, with, for example, Dirichlet’s boundary conditions (vanishing at the boundary). [sent-376, score-0.372]
</p><p>41 Let M be a C ∞ manifold without boundary with an inﬁnitely differentiable embedding in some ambient space X, D a differential operator with C ∞ coefﬁcients and let µ, be the measure corresponding to some C ∞ nowhere vanishing volume form on M . [sent-378, score-0.398]
</p><p>42 Roughly speaking, a degree k differential operator D is bounded as an operator H K → Lµ , if the kernel K(x, y) has 2k derivatives. [sent-385, score-0.272]
</p><p>43 Before ﬁnishing the theoretical discussion we obtain a useful 1/2  2 2 Corollary 10 The operator T = LK on Lµ is a bounded (and in fact compact) operator Lµ → H sob , where H sob is an arbitrary Sobolev space. [sent-404, score-0.232]
</p><p>44 2 2 Proof Follows from the fact that DT is bounded operator Lµ → Lµ for an arbitrary differential operator D and standard results on compact embeddings of Sobolev spaces (see, for example, Adams, 1975). [sent-405, score-0.242]
</p><p>45 4 The Representer Theorem for the Empirical Case In the case when M is unknown and sampled via labeled and unlabeled examples, the LaplaceBeltrami operator on M may be approximated by the Laplacian of the data adjacency graph (see Belkin, 2003; Bousquet et al. [sent-407, score-0.633]
</p><p>46 We now provide a proof of Theorem 2 which states that the solution to this problem admits a representation in terms of an expansion over labeled and unlabeled points. [sent-410, score-0.502]
</p><p>47 i=1 The simple form of the minimizer, given by this theorem, allows us to translate our extrinsic and intrinsic regularization framework into optimization problems over the ﬁnite dimensional space of l+u coefﬁcients {αi }i=1 , and invoke the machinery of kernel based algorithms. [sent-421, score-0.277]
</p><p>48 To ﬁx notation, we assume we have l labeled examples {(x i , yi )}l and u unlabeled examples i=1 j=l+u {x j } j=l+1 . [sent-426, score-0.52]
</p><p>49 (u + l)2  As before, the Representer Theorem can be used to show that the solution is an expansion of kernel functions over both the labeled and the unlabeled data: l+u  f (x) = ∑ αi K(x, xi ). [sent-441, score-0.562]
</p><p>50 αl+u ]T : 1 γI α∗ = argmin (Y − JKα)T (Y − JKα) + γA αT Kα + αT KLKα, (u + l)2 l+u l α∈R where K is the (l + u) × (l + u) Gram matrix over labeled and unlabeled points; Y is an (l + u) dimensional label vector given by: Y = [y1 , . [sent-445, score-0.53]
</p><p>51 (u + l)2  (8)  Note that when γI = 0, Equation 8) gives zero coefﬁcients over unlabeled data, and the coefﬁcients over the labeled data are exactly those for standard RLS. [sent-459, score-0.47]
</p><p>52 Note that when γI = 0, the SVM QP and Equations 11 and 10, give zero expansion coefﬁcients over the unlabeled data. [sent-512, score-0.315]
</p><p>53 The manifold regularization algorithms are summarized in the Table 1. [sent-514, score-0.297]
</p><p>54 5 Related Work and Connections to Other Algorithms In this section we survey various approaches to semi-supervised and transductive learning and highlight connections of manifold regularization to other algorithms. [sent-538, score-0.469]
</p><p>55 yl+u  i=1  l+u  ∑  (1 − yi f (xi ))+ + f  2 K,  i=l+1  which proposes a joint optimization of the SVM objective function over binary-valued labels on the unlabeled data and functions in the RKHS. [sent-542, score-0.365]
</p><p>56 Here, C,C ∗ are parameters that control the relative hingeloss over labeled and unlabeled sets. [sent-543, score-0.47]
</p><p>57 The joint optimization is implemented in Joachims (1999) by ﬁrst using an inductive SVM to label the unlabeled data and then iteratively solving SVM quadratic programs, at each step switching labels to improve the objective function. [sent-544, score-0.37]
</p><p>58 Semi-Supervised SVMs (S3 VM) (Bennett and Demiriz, 1999; Fung and Mangasarian, 2001): 3 VM incorporate unlabeled data by including the minimum hinge-loss for the two choices of S 2418  M ANIFOLD R EGULARIZATION  labels for each unlabeled example. [sent-547, score-0.63]
</p><p>59 This is formulated as a mixed-integer program for linear SVMs in Bennett and Demiriz (1999) and is found to be intractable for large amounts of unlabeled data. [sent-548, score-0.315]
</p><p>60 In a situation where the data truly lies on or near a submanifold M , the difference between these two penalizers can be signiﬁcant since smoothness in the normal direction to the data manifold is irrelevant to classiﬁcation or regression. [sent-560, score-0.253]
</p><p>61 (2003), nearest neighbor labeling for test examples is proposed once unlabeled examples have been labeled by transductive learning. [sent-568, score-0.612]
</p><p>62 (2003), test points are approximately represented as a linear combination of training and unlabeled points in the feature space induced by the kernel. [sent-570, score-0.315]
</p><p>63 , 2005) on out-of-sample extensions for semisupervised learning where an induction formula is derived by assuming that the addition of a test point to the graph does not change the transductive solution over the unlabeled data. [sent-580, score-0.539]
</p><p>64 Cotraining (Blum and Mitchell, 1998): The cotraining algorithm was developed to integrate abundance of unlabeled data with availability of multiple sources of information in domains like web-page classiﬁcation. [sent-581, score-0.371]
</p><p>65 Weak learners are trained on labeled examples and their predictions on 2419  B ELKIN , N IYOGI AND S INDHWANI  subsets of unlabeled examples are used to mutually expand the training set. [sent-582, score-0.47]
</p><p>66 (2000) where a combination of EM algorithm and Naive-Bayes classiﬁcation is proposed to incorporate unlabeled data. [sent-587, score-0.315]
</p><p>67 When γI = 0, Laplacian SVM disregards unlabeled data and returns the SVM decision boundary which is ﬁxed by the location of the two labeled points. [sent-606, score-0.47]
</p><p>68 As γI is increased, the intrinsic regularizer incorporates unlabeled data and causes the decision surface to appropriately adjust according to the geometry of the two classes. [sent-607, score-0.487]
</p><p>69 2 images for each class were randomly labeled (l=2) and the rest were left unlabeled (u=398). [sent-638, score-0.47]
</p><p>70 In Figure 4, we compare the error rates of manifold regularization algorithms, inductive classiﬁers and TSVM, at the break-even points in the precision-recall curves for the 45 binary classi2421  B ELKIN , N IYOGI AND S INDHWANI  ﬁcation problems. [sent-646, score-0.385]
</p><p>71 The following comments can be made: (a) manifold regularization results in signiﬁcant improvements over inductive classiﬁcation, for both RLS and SVM, and either compares well or signiﬁcantly outperforms TSVM across the 45 classiﬁcation problems. [sent-648, score-0.352]
</p><p>72 Note that TSVM solves multiple quadratic programs in the size of the labeled and unlabeled sets whereas LapSVM solves a single QP (Equation 11) in the size of the labeled set, followed by a linear system (Equation 10). [sent-649, score-0.625]
</p><p>73 (b) Scatter plots of performance on test and unlabeled data sets, in the bottom row of Figure 4, conﬁrm that the out-of-sample extension is good for both LapRLS and LapSVM. [sent-651, score-0.315]
</p><p>74 In Figure 5, we demonstrate the beneﬁt of unlabeled data as a function of the number of labeled examples. [sent-654, score-0.47]
</p><p>75 (Bottom row) Scatter plots of error rates on test and unlabeled data for Laplacian RLS, Laplacian SVM; and standard deviations in test errors of Laplacian SVM and TSVM. [sent-656, score-0.348]
</p><p>76 We considered 30 binary classiﬁcation problems corresponding to 30 splits of the training data where all 52 utterances of one speaker were labeled and all the rest were left unlabeled. [sent-673, score-0.256]
</p><p>77 The following comments can be made: (a) LapSVM and LapRLS make signiﬁcant performance improvements over inductive methods and TSVM, for predictions on unlabeled speakers that come from the same group as the labeled speaker, over all choices of the labeled speaker. [sent-681, score-0.736]
</p><p>78 As before, we considered 30 binary classiﬁcation problems arising when all utterances of one speaker are labeled and other training speakers are left unlabeled. [sent-687, score-0.312]
</p><p>79 It is encouraging to note performance improvements with unlabeled data in Experiment 1 where the test data comes from a slightly different distribution. [sent-689, score-0.315]
</p><p>80 2425  B ELKIN , N IYOGI AND S INDHWANI  For the ﬁrst experiment, we ran LapRLS and LapSVM in a transductive setting, with 12 randomly labeled examples (3 course and 9 non-course) and the rest unlabeled. [sent-717, score-0.297]
</p><p>81 We noted the performance of inductive and semi-supervised classiﬁers on unlabeled and test sets as a function of the number of labeled examples in the training set. [sent-764, score-0.525]
</p><p>82 The bottom panel presents performance curves of Laplacian SVM for different number of unlabeled points. [sent-766, score-0.315]
</p><p>83 The beneﬁt of unlabeled data can be seen by comparing the performance curves of inductive and semi-supervised classiﬁers. [sent-769, score-0.37]
</p><p>84 The bottom panel in Figure 8 presents the quality of transduction and semi-supervised learning with Laplacian SVM (Laplacian RLS performed similarly) as a function of the number of labeled examples for different amounts of unlabeled data. [sent-771, score-0.47]
</p><p>85 We ﬁnd that transduction improves with increasing unlabeled data. [sent-772, score-0.315]
</p><p>86 We expect this to be true for test set performance as well, but do not observe this consistently possibly since we use a ﬁxed set of parameters that become suboptimal as unlabeled data is increased. [sent-773, score-0.315]
</p><p>87 The optimal choice of the regularization parameters depends on the amount of labeled and unlabeled data, and should be adjusted by the model selection protocol accordingly. [sent-774, score-0.581]
</p><p>88 1 Unsupervised Learning: Clustering and Data Representation In the unsupervised case one is given a collection of unlabeled data points x 1 , . [sent-779, score-0.351]
</p><p>89 Our basic algorithmic framework embodied in the optimization problem in Equation 2 has three terms: (i) ﬁt to labeled data, (ii) extrinsic regularization and (iii) intrinsic regularization. [sent-783, score-0.376]
</p><p>90 As before f 2 can be approximated using the unlabeled I R data. [sent-786, score-0.315]
</p><p>91 Since standard supervised algorithms (SVM and RLS) are special cases of manifold regularization, our framework is also able to deal with a labeled data set containing no unlabeled examples. [sent-823, score-0.697]
</p><p>92 Additionally, manifold regularization can augment supervised learning with intrinsic regularization, possibly in a classdependent manner, which suggests the following algorithm: f ∗ = argmin f ∈HK γ+ I  1 l ∑ V (xi , yi , f ) + γA f l i=1  (u + l)  fT L f + 2 + + +  2 K  +  γ− I f T L− f − . [sent-824, score-0.53]
</p><p>93 (u + l)2 −  Here we introduce two intrinsic regularization parameters γ + , γ− and regularize separately for the I I two classes: f+ , f− are the vectors of evaluations of the function f , and L+ , L− are the graph Laplacians, on positive and negative examples respectively. [sent-825, score-0.24]
</p><p>94 The solution to the above problem for 2429  B ELKIN , N IYOGI AND S INDHWANI  RLS and SVM can be obtained by replacing γI L by the block-diagonal matrix  γ + L+ 0 I 0 γ − L− I  in the manifold regularization formulas given in Section 4. [sent-826, score-0.297]
</p><p>95 This framework brings together ideas from the theory of regularization in reproducing kernel Hilbert spaces, manifold learning and spectral methods. [sent-831, score-0.442]
</p><p>96 Convergence and generalization error: The crucial issue of dependence of generalization error on the number of labeled and unlabeled examples is still very poorly understood. [sent-833, score-0.47]
</p><p>97 Efﬁcient algorithms: The naive implementations of our algorithms have cubic complexity in the number of labeled and unlabeled examples, which is restrictive for large scale real-world applications. [sent-841, score-0.47]
</p><p>98 Learning from labeled and unlabeled data using graph mincuts. [sent-908, score-0.517]
</p><p>99 Text classiﬁcation from labeled and unlabeled documents using EM. [sent-1063, score-0.47]
</p><p>100 Very large scale manifold regularization using core vector machines. [sent-1145, score-0.297]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lapsvm', 0.316), ('unlabeled', 0.315), ('laprls', 0.251), ('laplacian', 0.232), ('rls', 0.227), ('hk', 0.193), ('tsvm', 0.189), ('manifold', 0.186), ('px', 0.176), ('elkin', 0.167), ('indhwani', 0.167), ('iyogi', 0.167), ('anifold', 0.158), ('labeled', 0.155), ('transductive', 0.142), ('svm', 0.141), ('egularization', 0.134), ('belkin', 0.126), ('ei', 0.114), ('regularization', 0.111), ('lk', 0.107), ('representer', 0.106), ('ek', 0.092), ('operator', 0.088), ('ambient', 0.084), ('intrinsic', 0.082), ('rkhs', 0.078), ('niyogi', 0.068), ('prbep', 0.065), ('sindhwani', 0.063), ('klk', 0.063), ('km', 0.061), ('argmin', 0.06), ('speakers', 0.056), ('cotraining', 0.056), ('heat', 0.056), ('utterances', 0.056), ('kernel', 0.056), ('inductive', 0.055), ('geometry', 0.053), ('marginal', 0.052), ('yi', 0.05), ('chicago', 0.049), ('blum', 0.048), ('graph', 0.047), ('isolet', 0.046), ('kondor', 0.046), ('lkm', 0.046), ('chapelle', 0.046), ('speaker', 0.045), ('zhu', 0.045), ('vs', 0.045), ('reproducing', 0.045), ('spectral', 0.044), ('gram', 0.043), ('geometric', 0.041), ('supervised', 0.041), ('differential', 0.04), ('scholkopf', 0.04), ('hx', 0.039), ('experiment', 0.038), ('regularizer', 0.037), ('hilbert', 0.037), ('corduneanu', 0.037), ('moons', 0.037), ('unsupervised', 0.036), ('xi', 0.036), ('regularized', 0.036), ('fm', 0.035), ('minimizer', 0.035), ('smoothness', 0.035), ('semisupervised', 0.035), ('norm', 0.034), ('kernels', 0.034), ('rates', 0.033), ('yl', 0.032), ('sobolev', 0.032), ('admits', 0.032), ('fs', 0.032), ('submanifold', 0.032), ('eigenmaps', 0.032), ('riemannian', 0.032), ('diffusion', 0.032), ('hkm', 0.032), ('connections', 0.03), ('integral', 0.03), ('joachims', 0.03), ('adjacency', 0.028), ('coifman', 0.028), ('extrinsic', 0.028), ('kemp', 0.028), ('lafon', 0.028), ('laplacians', 0.028), ('nigam', 0.028), ('sob', 0.028), ('webkb', 0.028), ('text', 0.027), ('vanishes', 0.027), ('compact', 0.026), ('classi', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="60-tfidf-1" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>Author: Mikhail Belkin, Partha Niyogi, Vikas Sindhwani</p><p>Abstract: We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework. Keywords: semi-supervised learning, graph transduction, regularization, kernel methods, manifold learning, spectral graph theory, unlabeled data, support vector machines</p><p>2 0.31287616 <a title="60-tfidf-2" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>Author: Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou</p><p>Abstract: We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the ﬁrst time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction. html. Keywords: transduction, transductive SVMs, semi-supervised learning, CCCP</p><p>3 0.12094339 <a title="60-tfidf-3" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>Author: Ross A. Lippert, Ryan M. Rifkin</p><p>Abstract: We consider the problem of Tikhonov regularization with a general convex loss function: this formalism includes support vector machines and regularized least squares. For a family of kernels that includes the Gaussian, parameterized by a “bandwidth” parameter σ, we characterize the limiting ˜ solution as σ → ∞. In particular, we show that if we set the regularization parameter λ = λσ−2p , the regularization term of the Tikhonov problem tends to an indicator function on polynomials of degree ⌊p⌋ (with residual regularization in the case where p ∈ Z). The proof rests on two key ideas: epi-convergence, a notion of functional convergence under which limits of minimizers converge to minimizers of limits, and a value-based formulation of learning, where we work with regularization on the function output values (y) as opposed to the function expansion coefﬁcients in the RKHS. Our result generalizes and uniﬁes previous results in this area. Keywords: Tikhonov regularization, Gaussian kernel, theory, kernel machines</p><p>4 0.11153743 <a title="60-tfidf-4" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>Author: Sayan Mukherjee, Ding-Xuan Zhou</p><p>Abstract: We introduce an algorithm that learns gradients from samples in the supervised learning framework. An error analysis is given for the convergence of the gradient estimated by the algorithm to the true gradient. The utility of the algorithm for the problem of variable selection as well as determining variable covariance is illustrated on simulated data as well as two gene expression data sets. For square loss we provide a very efﬁcient implementation with respect to both memory and time. Keywords: Tikhnonov regularization, variable selection, reproducing kernel Hilbert space, generalization bounds</p><p>5 0.096002266 <a title="60-tfidf-5" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>Author: Sayan Mukherjee, Qiang Wu</p><p>Abstract: We introduce an algorithm that simultaneously estimates a classiﬁcation function as well as its gradient in the supervised learning framework. The motivation for the algorithm is to ﬁnd salient variables and estimate how they covary. An efﬁcient implementation with respect to both memory and time is given. The utility of the algorithm is illustrated on simulated data as well as a gene expression data set. An error analysis is given for the convergence of the estimate of the classiﬁcation function and its gradient to the true classiﬁcation function and true gradient. Keywords: Tikhnonov regularization, variable selection, reproducing kernel Hilbert space, generalization bounds, classiﬁcation</p><p>6 0.094751604 <a title="60-tfidf-6" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>7 0.084963568 <a title="60-tfidf-7" href="./jmlr-2006-Universal_Kernels.html">93 jmlr-2006-Universal Kernels</a></p>
<p>8 0.078009777 <a title="60-tfidf-8" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>9 0.073404588 <a title="60-tfidf-9" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.072080113 <a title="60-tfidf-10" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.066747993 <a title="60-tfidf-11" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>12 0.066162772 <a title="60-tfidf-12" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.063625999 <a title="60-tfidf-13" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>14 0.060991254 <a title="60-tfidf-14" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.057349429 <a title="60-tfidf-15" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>16 0.057135481 <a title="60-tfidf-16" href="./jmlr-2006-Some_Discriminant-Based_PAC_Algorithms.html">81 jmlr-2006-Some Discriminant-Based PAC Algorithms</a></p>
<p>17 0.054086186 <a title="60-tfidf-17" href="./jmlr-2006-In_Search_of_Non-Gaussian_Components_of_a_High-Dimensional_Distribution.html">36 jmlr-2006-In Search of Non-Gaussian Components of a High-Dimensional Distribution</a></p>
<p>18 0.050128296 <a title="60-tfidf-18" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.049908709 <a title="60-tfidf-19" href="./jmlr-2006-Some_Theory_for_Generalized_Boosting_Algorithms.html">82 jmlr-2006-Some Theory for Generalized Boosting Algorithms</a></p>
<p>20 0.048647564 <a title="60-tfidf-20" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.288), (1, -0.173), (2, 0.206), (3, -0.143), (4, 0.202), (5, -0.096), (6, 0.026), (7, -0.083), (8, -0.339), (9, -0.061), (10, -0.014), (11, 0.291), (12, -0.322), (13, -0.16), (14, -0.082), (15, -0.024), (16, -0.048), (17, -0.088), (18, -0.019), (19, 0.086), (20, -0.04), (21, 0.146), (22, 0.045), (23, -0.062), (24, 0.003), (25, -0.038), (26, -0.041), (27, -0.012), (28, -0.015), (29, -0.038), (30, -0.006), (31, 0.011), (32, 0.031), (33, 0.014), (34, -0.02), (35, -0.02), (36, -0.029), (37, -0.041), (38, 0.037), (39, -0.034), (40, 0.019), (41, -0.016), (42, 0.0), (43, 0.012), (44, -0.054), (45, 0.041), (46, -0.016), (47, 0.043), (48, -0.02), (49, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92958349 <a title="60-lsi-1" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>Author: Mikhail Belkin, Partha Niyogi, Vikas Sindhwani</p><p>Abstract: We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework. Keywords: semi-supervised learning, graph transduction, regularization, kernel methods, manifold learning, spectral graph theory, unlabeled data, support vector machines</p><p>2 0.89996231 <a title="60-lsi-2" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>Author: Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou</p><p>Abstract: We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the ﬁrst time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction. html. Keywords: transduction, transductive SVMs, semi-supervised learning, CCCP</p><p>3 0.40328118 <a title="60-lsi-3" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>Author: Ross A. Lippert, Ryan M. Rifkin</p><p>Abstract: We consider the problem of Tikhonov regularization with a general convex loss function: this formalism includes support vector machines and regularized least squares. For a family of kernels that includes the Gaussian, parameterized by a “bandwidth” parameter σ, we characterize the limiting ˜ solution as σ → ∞. In particular, we show that if we set the regularization parameter λ = λσ−2p , the regularization term of the Tikhonov problem tends to an indicator function on polynomials of degree ⌊p⌋ (with residual regularization in the case where p ∈ Z). The proof rests on two key ideas: epi-convergence, a notion of functional convergence under which limits of minimizers converge to minimizers of limits, and a value-based formulation of learning, where we work with regularization on the function output values (y) as opposed to the function expansion coefﬁcients in the RKHS. Our result generalizes and uniﬁes previous results in this area. Keywords: Tikhonov regularization, Gaussian kernel, theory, kernel machines</p><p>4 0.32271835 <a title="60-lsi-4" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: S. Sathiya Keerthi, Olivier Chapelle, Dennis DeCoste</p><p>Abstract: Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classiﬁcation speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily ﬁnds a set of kernel basis functions of a speciﬁed maximum size (dmax ) to approximate the SVM primal cost function well; (3) it is 2 efﬁcient and roughly scales as O(ndmax ) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors. Keywords: SVMs, classiﬁcation, sparse design</p><p>5 0.30465013 <a title="60-lsi-5" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>Author: Sayan Mukherjee, Ding-Xuan Zhou</p><p>Abstract: We introduce an algorithm that learns gradients from samples in the supervised learning framework. An error analysis is given for the convergence of the gradient estimated by the algorithm to the true gradient. The utility of the algorithm for the problem of variable selection as well as determining variable covariance is illustrated on simulated data as well as two gene expression data sets. For square loss we provide a very efﬁcient implementation with respect to both memory and time. Keywords: Tikhnonov regularization, variable selection, reproducing kernel Hilbert space, generalization bounds</p><p>6 0.29735312 <a title="60-lsi-6" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>7 0.29228091 <a title="60-lsi-7" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.27386847 <a title="60-lsi-8" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>9 0.25450841 <a title="60-lsi-9" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.25019556 <a title="60-lsi-10" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.24799272 <a title="60-lsi-11" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>12 0.2417112 <a title="60-lsi-12" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>13 0.24149388 <a title="60-lsi-13" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>14 0.23617201 <a title="60-lsi-14" href="./jmlr-2006-Universal_Kernels.html">93 jmlr-2006-Universal Kernels</a></p>
<p>15 0.22645958 <a title="60-lsi-15" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>16 0.22384389 <a title="60-lsi-16" href="./jmlr-2006-In_Search_of_Non-Gaussian_Components_of_a_High-Dimensional_Distribution.html">36 jmlr-2006-In Search of Non-Gaussian Components of a High-Dimensional Distribution</a></p>
<p>17 0.20109996 <a title="60-lsi-17" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>18 0.20073016 <a title="60-lsi-18" href="./jmlr-2006-Some_Discriminant-Based_PAC_Algorithms.html">81 jmlr-2006-Some Discriminant-Based PAC Algorithms</a></p>
<p>19 0.20020927 <a title="60-lsi-19" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.19918066 <a title="60-lsi-20" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.028), (17, 0.022), (35, 0.011), (36, 0.067), (45, 0.015), (50, 0.052), (63, 0.044), (75, 0.356), (76, 0.018), (78, 0.024), (81, 0.04), (84, 0.03), (90, 0.067), (91, 0.033), (96, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71159011 <a title="60-lda-1" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>Author: Mikhail Belkin, Partha Niyogi, Vikas Sindhwani</p><p>Abstract: We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework. Keywords: semi-supervised learning, graph transduction, regularization, kernel methods, manifold learning, spectral graph theory, unlabeled data, support vector machines</p><p>2 0.38295552 <a title="60-lda-2" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>Author: Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We present a family of margin based online learning algorithms for various prediction tasks. In particular we derive and analyze algorithms for binary and multiclass categorization, regression, uniclass prediction and sequence prediction. The update steps of our different algorithms are all based on analytical solutions to simple constrained optimization problems. This uniﬁed view allows us to prove worst-case loss bounds for the different algorithms and for the various decision problems based on a single lemma. Our bounds on the cumulative loss of the algorithms are relative to the smallest loss that can be attained by any ﬁxed hypothesis, and as such are applicable to both realizable and unrealizable settings. We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real data sets.</p><p>3 0.38031083 <a title="60-lda-3" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>4 0.37952638 <a title="60-lda-4" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>Author: Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou</p><p>Abstract: We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the ﬁrst time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction. html. Keywords: transduction, transductive SVMs, semi-supervised learning, CCCP</p><p>5 0.37070176 <a title="60-lda-5" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>Author: Mingrui Wu, Bernhard Schölkopf, Gökhan Bakır</p><p>Abstract: Many kernel learning algorithms, including support vector machines, result in a kernel machine, such as a kernel classiﬁer, whose key component is a weight vector in a feature space implicitly introduced by a positive deﬁnite kernel function. This weight vector is usually obtained by solving a convex optimization problem. Based on this fact we present a direct method to build sparse kernel learning algorithms by adding one more constraint to the original convex optimization problem, such that the sparseness of the resulting kernel machine is explicitly controlled while at the same time performance is kept as high as possible. A gradient based approach is provided to solve this modiﬁed optimization problem. Applying this method to the support vectom machine results in a concrete algorithm for building sparse large margin classiﬁers. These classiﬁers essentially ﬁnd a discriminating subspace that can be spanned by a small number of vectors, and in this subspace, the diﬀerent classes of data are linearly well separated. Experimental results over several classiﬁcation benchmarks demonstrate the eﬀectiveness of our approach. Keywords: sparse learning, sparse large margin classiﬁers, kernel learning algorithms, support vector machine, kernel Fisher discriminant</p><p>6 0.36731291 <a title="60-lda-6" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>7 0.36629149 <a title="60-lda-7" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>8 0.36537722 <a title="60-lda-8" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>9 0.35876715 <a title="60-lda-9" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>10 0.3575668 <a title="60-lda-10" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.35586283 <a title="60-lda-11" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.35210332 <a title="60-lda-12" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>13 0.34950137 <a title="60-lda-13" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.34686691 <a title="60-lda-14" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>15 0.34629396 <a title="60-lda-15" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>16 0.34624946 <a title="60-lda-16" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>17 0.34574422 <a title="60-lda-17" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>18 0.34528756 <a title="60-lda-18" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.34508929 <a title="60-lda-19" href="./jmlr-2006-On_Model_Selection_Consistency_of_Lasso.html">66 jmlr-2006-On Model Selection Consistency of Lasso</a></p>
<p>20 0.34487695 <a title="60-lda-20" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
