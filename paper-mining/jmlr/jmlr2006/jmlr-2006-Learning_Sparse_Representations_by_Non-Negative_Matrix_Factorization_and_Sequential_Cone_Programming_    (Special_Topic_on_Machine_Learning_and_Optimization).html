<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-51" href="#">jmlr2006-51</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-51-pdf" href="http://jmlr.org/papers/volume7/heiler06a/heiler06a.pdf">pdf</a></p><p>Author: Matthias Heiler, Christoph Schnörr</p><p>Abstract: We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. We show that for ordinary NMF, our approach performs as well as existing stateof-the-art algorithms, while for sparsity-constrained NMF, as recently proposed by P. O. Hoyer in JMLR 5 (2004), it outperforms previous methods. In addition, we show how to extend NMF learning within the same optimization framework in order to make use of class membership information in supervised learning problems. Keywords: non-negative matrix factorization, second-order cone programming, sequential convex optimization, reverse-convex programming, sparsity</p><p>Reference: <a title="jmlr-2006-51-reference" href="../jmlr2006_reference/jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Journal of Machine Learning Research 7 (2006) 1385–1407  Submitted 9/05; Revised 1/06; Published 7/06  Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming Matthias Heiler Christoph Schn¨ rr o  HEILER @ UNI - MANNHEIM . [sent-1, score-0.118]
</p><p>2 Bennett and Emilio Parrado-Hern´ ndez a  Abstract We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. [sent-4, score-0.298]
</p><p>3 Keywords: non-negative matrix factorization, second-order cone programming, sequential convex optimization, reverse-convex programming, sparsity  1. [sent-9, score-0.318]
</p><p>4 For example, Hoyer (2004) recently proposed NMF subject to additional constraints that allow particularly accurate control over sparseness and, indirectly, over the localization of features—see Figure 1 for an illustration. [sent-19, score-0.128]
</p><p>5 So far, learning NMF codes relied on variations of the gradient descent scheme which tend to be less efﬁcient in the presence of additional sparsity constraints. [sent-21, score-0.171]
</p><p>6 Therefore, in this work, we exploit both the biconvex nature of the Euclidean NMF optimization criterion and the reverseconvex structure of the sparsity constraints to derive efﬁcient optimization schemes using convex quadratic and second order cone programming (Lobo et al. [sent-22, score-0.492]
</p><p>7 Five basis functions (columns) with sparseness constraints ranging from 0. [sent-26, score-0.162]
</p><p>8 The representation of the sparsity constraints and the corresponding optimality conditions are described in Section 4. [sent-36, score-0.201]
</p><p>9 Using convex optimization problems as basic components, we suggest algorithms for solving the general NMF problem in Section 5. [sent-37, score-0.093]
</p><p>10 1 Unconstrained NMF The original NMF problem reads with a non-negative matrix of n data samples V ∈ Rm×n , a matrix + of basis functions W ∈ Rm×r , and corresponding loadings H ∈ Rr×n : + + 2 F  V −W H  min W,H  (1)  s. [sent-63, score-0.107]
</p><p>11 As a result, we will conﬁne ourselves to efﬁciently compute a local optimum by solving a sequence of convex programs (Section 3. [sent-70, score-0.088]
</p><p>12 2 Sparsity-Constrained NMF Although NMF codes tend to be sparse (Lee and Seung, 1999), it has been suggested to control sparsity by more direct means. [sent-73, score-0.181]
</p><p>13 In addition, we found that (2) can conveniently be represented in terms of second order cones (Section 4), allowing efﬁcient numerical solvers to be applied. [sent-78, score-0.109]
</p><p>14 0 ≤ W, H  (5)  sp(W ) = sw sp(H ⊤ ) = sh ,  where sw , sh are user parameters. [sent-82, score-0.102]
</p><p>15 The sparsity constraints control (i) to what extent basis functions are sparse, and (ii) how much each basis function contributes to the reconstruction of only a subset 1387  ¨ H EILER AND S CHN ORR  of the data V . [sent-83, score-0.269]
</p><p>16 In a pattern recognition application the sparsity constraints effectively weight the desired generality over the speciﬁcity of the basis functions. [sent-84, score-0.235]
</p><p>17 Instead of using equality constraints we will slightly generalize the constraints in this work to intervals smin ≤ sp(W ) ≤ smax and smin ≤ sp(H ⊤ ) ≤ smax . [sent-85, score-1.628]
</p><p>18 In particular, it allows for smax = smax = 1, which may often be useful. [sent-87, score-0.66]
</p><p>19 smin ≤ sp(W ) ≤ smax w w  (6)  smin ≤ sp(H ⊤ ) ≤ smax , h h  where smin , smax , smin , smax are user parameters. [sent-90, score-2.94]
</p><p>20 The basic idea is to restrict, for each class i and for each of its vectors j, the coefﬁcients H j • to a cone around the class center µi which is implicitly computed in the optimization process: min W,H  V −W H  2 F  (7)  s. [sent-98, score-0.198]
</p><p>21 4, these additional constraints are no more difﬁcult from the viewpoint of optimization than are the previously introduced constraints in (6). [sent-102, score-0.22]
</p><p>22 Of course, if the application suggests, supervised NMF (7) can be conducted with the additional sparsity constraints from (6). [sent-104, score-0.248]
</p><p>23 The min-sparsity constraints in (6) are essential in the sense that each global optimum of the problem with min-sparsity constraints removed violates at least one such constraint on W and H. [sent-111, score-0.239]
</p><p>24 Unlike previous work, where variations of the gradient descent scheme were applied (Paatero, 1997; Hoyer, 2004), our algorithms’ basic building blocks are convex programs for which fast and robust solvers exist. [sent-119, score-0.152]
</p><p>25 1 Convex Quadratic Programs (QP) Convex quadratic programs (QP) are optimization problems involving convex quadratic objectives functions and linear constraints. [sent-125, score-0.167]
</p><p>26 We note that from the viewpoint of optimization problem (6) is considerably more involved than (1) because the lower sparsity bound imposed in terms of smin , smin destroys convexity. [sent-176, score-0.994]
</p><p>27 1 Second Order Cone Programms (SOCP) and Sparsity The second order cone L n+1 ⊂ Rn+1 is the convex set (Lobo et al. [sent-178, score-0.196]
</p><p>28 Furthermore, additional linear constraints and, in particular, the condition x ∈ Rn are admissible, as they are special cases of constraints of the form (14). [sent-189, score-0.158]
</p><p>29 Our approach + to sparsity-constrained NMF, to be developed below, is based on this class of convex optimization problems. [sent-190, score-0.093]
</p><p>30 Motivated by the sparseness measure (2) and our goal to compute non-negative representations, we consider the family of convex sets parametrized by a sparsity parameter s: x  C (s) := x ∈ Rn  1 ⊤ cn,s e x  ∈ L n+1  ,  cn,s :=  √  √ n − ( n − 1)s . [sent-191, score-0.227]
</p><p>31 (24d)  Note that Q represents the convex constraints of problem (23) while Gw (W ) and Gh (H) are nonnegative exactly when sparsity is at least smin and smin . [sent-231, score-1.067]
</p><p>32 Both algorithms can also be applied to the supervised setting just by adding additional convex constraints—see Sections 2. [sent-244, score-0.103]
</p><p>33 For the rows of H that violate the min-sparsity constraint we compute tangent planes to the min-sparsity cone and solve the SOCP again with additional tangent-plane constraints in place. [sent-273, score-0.338]
</p><p>34 During iteration we repeatedly solve this SOCP where the tangent planes are permanently updated to follow their corresponding entries in H: This ensures that they constrain the feasible set no more than necessary. [sent-280, score-0.097]
</p><p>35 This process of updating the tangent planes and computing new estimates for H is repeated until the objective function no longer improves. [sent-281, score-0.125]
</p><p>36 The algorithm starts by setting smin = 0 in (30), and by computing the global h ˜ optimum of the convex problem: min f (H), H ∈ C h (smax ), denoted by H 0 . [sent-283, score-0.514]
</p><p>37 , we once more solve (32) with additional linear constraints enforcing feasibility of each H k• , j ∈ J k : j vec(V ⊤ ) − (W ⊗ I)vec(H ⊤ ) ∈ L r×n+1 z  min z , H ∈ Rr×n ∩ C h (smax ) , + h H,z  t k , H j • − π(H k• ) ≥ 0 , j j  ∀ j ∈ Jk . [sent-300, score-0.1]
</p><p>38 Instead, we permanently re-adjust the corre˜j sponding tangent plane constraints t k , setting t k ← ∇C (smin )(π(H k• )), ∀ j ∈ J k . [sent-305, score-0.125]
</p><p>39 This ensures j j h ˜j that the constraints are not active at termination unless a component H k• is actually on the boundary of the min-sparsity cone. [sent-306, score-0.11]
</p><p>40 Remark 3 The projection operator π mapping a point x ∈ Rm onto the boundary of the min-sparsity + cone can be implemented using either the method of Hoyer (2004) or a fast approximation. [sent-315, score-0.14]
</p><p>41 Remark 4 Problems (32) and (33) are formulated in terms of the rows of H, complying with the sparsity constraints (22). [sent-318, score-0.201]
</p><p>42 Except for solvers for linear programs, QP solvers are usually among the most efﬁcient mathematical programming codes available. [sent-324, score-0.108]
</p><p>43 When this happens, we must introduce some damping term or simply switch to the convergent sparsity maximization algorithm described in Section 5. [sent-384, score-0.122]
</p><p>44 (36b)  Since t k = ∇sp(π(H ∗k )⊤ ) constraint (36b) ensures that the min-sparsity constraint is enforced at H ∗ j j• when necessary (c. [sent-395, score-0.098]
</p><p>45 This scheme is a global optimization algorithm in the sense that it ﬁnds a true global optimum. [sent-408, score-0.101]
</p><p>46 The general idea of our algorithm is as follows: After an initialization step, it alternates between two convex optimization problems. [sent-412, score-0.117]
</p><p>47 One maximizes sparsity subject to the constraint that the objective value must not increase. [sent-413, score-0.196]
</p><p>48 Note that this step maximizes sparsity in the sense that sp(H k ) ≤ sp(H sp ), due to (38c) and the convexity of sp(·). [sent-433, score-0.38]
</p><p>49 While the intermediate solution H sp satisﬁes the min-sparsity constraint, it may not be an optimal local solution to the overall problem. [sent-435, score-0.258]
</p><p>50 H ∈ Rr×n ∩ C h (smax ) + h sp Hj• − Hj• 2  ≤  (40a)  min  q∈C (smin ) h  sp q − Hj• 2  ,  j = 1, . [sent-438, score-0.537]
</p><p>51 vec(V ⊤ ) − (W ⊗ I)vec(H ⊤ ) ∈ L rn+1 t  (41)  sp  Hj• − Hj• sp minq∈C (smin ) q − H j • h  2  ∈ L n+1 ,  ∀j  H ∈ Rr×n ∩ C h (smax ). [sent-443, score-0.516]
</p><p>52 + h Here, the objective function f is minimized subject to the constraint that the solution must not be too distant from H sp . [sent-444, score-0.332]
</p><p>53 To this end, the non-convex min-sparsity constraint is replaced by a convex max-distance constraint (40b), in effect deﬁning a spherical trust region. [sent-445, score-0.154]
</p><p>54 The radius sp minq∈C (smin ) q − H j • 2 of the trust region is computed by a small SOCP. [sent-446, score-0.258]
</p><p>55 If smax = h smin , the approximate approach in (38) breaks down, and each iteration just yields H k = H sp = h H k+1 . [sent-453, score-0.993]
</p><p>56 1: H 0 ← solution of (32) projected on ∂C h (smin ), k ← 0 h 2: repeat 3: H sp ← solution of (38) 4: H k+1 ← solution of (40) 5: k ← k+1 6: until | f (H k ) − f (H k−1 )| ≤ ε Proof Under the assumptions stated in Section 2. [sent-468, score-0.258]
</p><p>57 2, alternately applied to the optimization of W and H, respectively, computes a sequence of feasible points {W k , H k } that steadily decreases the objective function value. [sent-472, score-0.118]
</p><p>58 Moreover, t = smin h because after convergence of iterating (38) and (40), the min-sparsity constraint will be active for some of the indices j ∈ {1, . [sent-477, score-0.454]
</p><p>59 Therefore, using the notation (24), the solution to problem (38) satisﬁes max t ∗ = smin , f (H k ) − f (H ∗ ) ≥ 0 , Gh (H ∗ ) ∈ Rr . [sent-481, score-0.405]
</p><p>60 4 Solving Supervised NMF The supervised NMF problem (7) is solved either by the tangent-plane constraint or the sparsitymaximization algorithm presented above. [sent-489, score-0.096]
</p><p>61 We merely add constraints ensuring that the coefﬁcients r×n belonging to class i, abbreviated H(i) ∈ R+ i below, stay in a cone centered at mean µi = 1/ni H(i) e. [sent-490, score-0.219]
</p><p>62 Then, the supervised constraint in (7) translates to 1/ni H(i) e − H j∗ λ/ni e⊤ H(i) e  ∈ L n+1 ,  ∀i, ∀ j ∈ class(i) . [sent-491, score-0.122]
</p><p>63 (44)  It is an important advantage that the algorithms above can easily be augmented by various convex constraints (e. [sent-492, score-0.135]
</p><p>64 We also provide evidence that the local sparsity maximization seems not prone to end in bad local optima. [sent-564, score-0.122]
</p><p>65 Finally, we show that the supervised constraints from eqn. [sent-565, score-0.126]
</p><p>66 w  5  0  10  20  30  40  (d) Recovered bases using smin = 0. [sent-592, score-0.442]
</p><p>67 The results for different values of the min-sparsity constraint are shown in Figure 2(c) and 2(d): Only an active constraint allows to correctly recover W and H. [sent-598, score-0.098]
</p><p>68 2 we report the results for 10 repeated runs of the tangent-plane constraints and the sparsity-maximization algorithm using different choices of the min-sparsity constraint. [sent-606, score-0.135]
</p><p>69 The most important ﬁgure is the number of correct recoveries of the basis functions. [sent-607, score-0.104]
</p><p>70 First note that, 1401  ¨ H EILER AND S CHN ORR  smin w TPC TPC TPC SMA SMA SMA  # correct results med run time (sec. [sent-609, score-0.43]
</p><p>71 Statistics of the Paatero experiment collected over 10 runs for the tangent-plane constraints (TPC) and the sparsity-maximization algorithm (SMA). [sent-639, score-0.105]
</p><p>72 The number of correct reconstructions (see text), the median run time, and the best objective value obtained are reported for different choices of the sparsity constraint. [sent-640, score-0.172]
</p><p>73 nine out of ten trials for a sufﬁciently strong sparsity constraint: smin = 0. [sent-642, score-0.527]
</p><p>74 Only for the extremely sparse case with smin = 0. [sent-647, score-0.438]
</p><p>75 However, not shown in the table, for the interesting case smin = 0. [sent-649, score-0.405]
</p><p>76 6 the objective values of the correct recoveries were all below 0. [sent-650, score-0.095]
</p><p>77 Finally, we point out that for the correct value of the sparsity constraint the number of correct recoveries is essentially a function of the stopping parameters. [sent-654, score-0.293]
</p><p>78 3 Global Approaches A potential source of difﬁculties with the sparsity-maximization algorithm is that the lower bound on sparsity is optimized only locally in (38). [sent-660, score-0.155]
</p><p>79 Through the proximity constraint in (40) the amount of sparsity obtained in effect limits the step size of the algorithm. [sent-661, score-0.171]
</p><p>80 Insufﬁcient sparsity optimization may, in the worst case, lead to convergence to a bad local optimum. [sent-662, score-0.159]
</p><p>81 To see if this worst-case scenario is relevant in practice, we discretized the problem by sampling the sparsity cones using rotated and scaled version of the current estimate H k and then evaluated f (W, H) using samples from each individual sparsity cone. [sent-663, score-0.312]
</p><p>82 Then we picked one sample from each cone and computed (38) replacing the starting point H k by the sampled coordinates. [sent-664, score-0.14]
</p><p>83 This data set is suitable since it is not overly large and sparsity control is crucial for its successful factorization. [sent-667, score-0.122]
</p><p>84 In the sparsity-maximization algorithm we ﬁrst sampled the four sparsity cones corresponding to each basis function of the data for sw ≥ 0. [sent-668, score-0.253]
</p><p>85 then combined the samples on each cone in each possible way and evaluated g for all corresponding starting points. [sent-740, score-0.14]
</p><p>86 In a second experiment we placed 1000 points on each sparsity cone, and randomly selected 104 combinations as starting points. [sent-741, score-0.122]
</p><p>87 The best results obtained over four runs and 80 iterations with our local linearization method used in SMA and the sparse enumeration (ﬁrst) and the sampling (second) strategy, are reported below: Algorithm SMA sparse enumeration sampling  min-sparsity 0. [sent-742, score-0.092]
</p><p>88 26  We see that the local sparsity maximization in SMA yields results comparable to the sampling strategies. [sent-748, score-0.122]
</p><p>89 This is most likely caused by severe under-sampling of the sparsity cones. [sent-750, score-0.122]
</p><p>90 For different values of the sparsity constraints we derived NMF bases (Figure 1) and examined reconstruction error g and training time. [sent-755, score-0.238]
</p><p>91 In this experiment we used the tangent-plane constraints method and smin = smax = w w smin = smax . [sent-756, score-1.549]
</p><p>92 We show the median CPU time over three repeated runs for this experiment in Table 4: While the stopping criterion has only minor inﬂuence on the run time the number of basis functions is critical. [sent-798, score-0.117]
</p><p>93 It is evident that by strengthening the supervised label constraint we reduce the classiﬁcation error signiﬁcantly, increasing recognition accuracy by a factor of two. [sent-820, score-0.096]
</p><p>94 Conclusion We have shown that Euclidean NMF with and without sparsity constraints ﬁts nicely within the framework of sequential quadratic and second order cone programming. [sent-822, score-0.362]
</p><p>95 As λ decreases the supervised label constraint is strengthened, reducing the classiﬁcation error by a factor of two. [sent-839, score-0.096]
</p><p>96 able in supervised classiﬁcation settings leads to additional convex constraints that do not further complicate the optimization problem in a noteworthy way: no new algorithms need to be derived, no suitable, typically more stringent, learning rates need to be determined. [sent-840, score-0.219]
</p><p>97 Application of non-negative and local non negative matrix factorization to facial expression recognition. [sent-854, score-0.089]
</p><p>98 Generalized differentiability, duality and optimization for problems dealing with differences of convex functions. [sent-921, score-0.093]
</p><p>99 Minimization of continuous convex functionals on complements of convex subsets of locally convex spaces. [sent-1053, score-0.201]
</p><p>100 02, a Matlab toolbox for optimization over symmetric cones (updated version 1. [sent-1073, score-0.105]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nmf', 0.531), ('smin', 0.405), ('smax', 0.33), ('sp', 0.258), ('tpc', 0.174), ('cone', 0.14), ('chn', 0.125), ('eiler', 0.125), ('rogramming', 0.125), ('sparsity', 0.122), ('rr', 0.118), ('qp', 0.105), ('hoyer', 0.104), ('gh', 0.102), ('paatero', 0.102), ('sma', 0.096), ('socp', 0.095), ('vec', 0.095), ('equential', 0.095), ('orr', 0.087), ('parse', 0.081), ('tuy', 0.08), ('constraints', 0.079), ('pgd', 0.068), ('cones', 0.068), ('factorization', 0.063), ('mu', 0.062), ('mosek', 0.06), ('convex', 0.056), ('constraint', 0.049), ('sparseness', 0.049), ('unconstrained', 0.049), ('heiler', 0.048), ('gw', 0.048), ('supervised', 0.047), ('tangent', 0.046), ('mannheim', 0.045), ('nqh', 0.045), ('recoveries', 0.045), ('hj', 0.043), ('cbcl', 0.043), ('solvers', 0.041), ('qps', 0.04), ('horst', 0.039), ('qh', 0.039), ('optimization', 0.037), ('bases', 0.037), ('seung', 0.037), ('rn', 0.036), ('schn', 0.035), ('vision', 0.034), ('rcp', 0.034), ('basis', 0.034), ('rm', 0.033), ('locally', 0.033), ('sparse', 0.033), ('programs', 0.032), ('global', 0.032), ('tr', 0.031), ('termination', 0.031), ('repeated', 0.03), ('reads', 0.029), ('sw', 0.029), ('alternately', 0.029), ('lobo', 0.029), ('feasible', 0.027), ('stopping', 0.027), ('runs', 0.026), ('codes', 0.026), ('facial', 0.026), ('rockafellar', 0.026), ('translates', 0.026), ('correct', 0.025), ('lee', 0.025), ('viewpoint', 0.025), ('cplex', 0.025), ('objective', 0.025), ('planes', 0.024), ('recovered', 0.024), ('initialization', 0.024), ('descent', 0.023), ('buciu', 0.023), ('chichocki', 0.023), ('floudas', 0.023), ('graham', 0.023), ('hr', 0.023), ('isra', 0.023), ('loadings', 0.023), ('lyons', 0.023), ('minq', 0.023), ('optdigits', 0.023), ('rpcs', 0.023), ('smaragdis', 0.023), ('tilburg', 0.023), ('wang', 0.023), ('sh', 0.022), ('hh', 0.022), ('min', 0.021), ('quadratic', 0.021), ('remark', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="51-tfidf-1" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Matthias Heiler, Christoph Schnörr</p><p>Abstract: We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. We show that for ordinary NMF, our approach performs as well as existing stateof-the-art algorithms, while for sparsity-constrained NMF, as recently proposed by P. O. Hoyer in JMLR 5 (2004), it outperforms previous methods. In addition, we show how to extend NMF learning within the same optimization framework in order to make use of class membership information in supervised learning problems. Keywords: non-negative matrix factorization, second-order cone programming, sequential convex optimization, reverse-convex programming, sparsity</p><p>2 0.21031834 <a title="51-tfidf-2" href="./jmlr-2006-Learning_Image_Components_for_Object_Recognition.html">47 jmlr-2006-Learning Image Components for Object Recognition</a></p>
<p>Author: Michael W. Spratling</p><p>Abstract: In order to perform object recognition it is necessary to learn representations of the underlying components of images. Such components correspond to objects, object-parts, or features. Nonnegative matrix factorisation is a generative model that has been speciﬁcally proposed for ﬁnding such meaningful representations of image data, through the use of non-negativity constraints on the factors. This article reports on an empirical investigation of the performance of non-negative matrix factorisation algorithms. It is found that such algorithms need to impose additional constraints on the sparseness of the factors in order to successfully deal with occlusion. However, these constraints can themselves result in these algorithms failing to identify image components under certain conditions. In contrast, a recognition model (a competitive learning neural network algorithm) reliably and accurately learns representations of elementary image features without such constraints. Keywords: non-negative matrix factorisation, competitive learning, dendritic inhibition, object recognition</p><p>3 0.13780206 <a title="51-tfidf-3" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>4 0.0661688 <a title="51-tfidf-4" href="./jmlr-2006-A_Linear_Non-Gaussian_Acyclic_Model_for_Causal_Discovery.html">4 jmlr-2006-A Linear Non-Gaussian Acyclic Model for Causal Discovery</a></p>
<p>Author: Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvärinen, Antti Kerminen</p><p>Abstract: In recent years, several methods have been proposed for the discovery of causal structure from non-experimental data. Such methods make various assumptions on the data generating process to facilitate its identiﬁcation from purely observational data. Continuing this line of research, we show how to discover the complete causal structure of continuous-valued data, under the assumptions that (a) the data generating process is linear, (b) there are no unobserved confounders, and (c) disturbance variables have non-Gaussian distributions of non-zero variances. The solution relies on the use of the statistical method known as independent component analysis, and does not require any pre-speciﬁed time-ordering of the variables. We provide a complete Matlab package for performing this LiNGAM analysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the effectiveness of the method using artiﬁcially generated data and real-world data. Keywords: independent component analysis, non-Gaussianity, causal discovery, directed acyclic graph, non-experimental data</p><p>5 0.065745883 <a title="51-tfidf-5" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pannagadatta K. Shivaswamy, Chiranjib Bhattacharyya, Alexander J. Smola</p><p>Abstract: We propose a novel second order cone programming formulation for designing robust classiﬁers which can handle uncertainty in observations. Similar formulations are also derived for designing regression functions which are robust to uncertainties in the regression setting. The proposed formulations are independent of the underlying distribution, requiring only the existence of second order moments. These formulations are then specialized to the case of missing values in observations for both classiﬁcation and regression problems. Experiments show that the proposed formulations outperform imputation.</p><p>6 0.060134344 <a title="51-tfidf-6" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<p>7 0.048863459 <a title="51-tfidf-7" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>8 0.043499202 <a title="51-tfidf-8" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.036761321 <a title="51-tfidf-9" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.03643905 <a title="51-tfidf-10" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>11 0.032113407 <a title="51-tfidf-11" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.030154744 <a title="51-tfidf-12" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.028822109 <a title="51-tfidf-13" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.028577924 <a title="51-tfidf-14" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.02640936 <a title="51-tfidf-15" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.026353646 <a title="51-tfidf-16" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>17 0.026124243 <a title="51-tfidf-17" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.025659636 <a title="51-tfidf-18" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.025518205 <a title="51-tfidf-19" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.025155645 <a title="51-tfidf-20" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.166), (1, -0.128), (2, -0.007), (3, 0.168), (4, 0.001), (5, -0.074), (6, -0.094), (7, 0.168), (8, 0.297), (9, 0.162), (10, 0.081), (11, 0.23), (12, 0.012), (13, 0.112), (14, -0.233), (15, -0.167), (16, 0.331), (17, -0.002), (18, -0.071), (19, 0.153), (20, 0.063), (21, 0.114), (22, 0.041), (23, -0.139), (24, -0.053), (25, -0.007), (26, 0.025), (27, -0.002), (28, -0.075), (29, -0.04), (30, -0.013), (31, -0.067), (32, -0.048), (33, 0.01), (34, -0.032), (35, -0.033), (36, 0.018), (37, -0.047), (38, -0.07), (39, 0.025), (40, -0.011), (41, -0.002), (42, 0.049), (43, 0.016), (44, -0.013), (45, 0.049), (46, -0.0), (47, 0.052), (48, -0.009), (49, -0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9460932 <a title="51-lsi-1" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Matthias Heiler, Christoph Schnörr</p><p>Abstract: We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. We show that for ordinary NMF, our approach performs as well as existing stateof-the-art algorithms, while for sparsity-constrained NMF, as recently proposed by P. O. Hoyer in JMLR 5 (2004), it outperforms previous methods. In addition, we show how to extend NMF learning within the same optimization framework in order to make use of class membership information in supervised learning problems. Keywords: non-negative matrix factorization, second-order cone programming, sequential convex optimization, reverse-convex programming, sparsity</p><p>2 0.8532474 <a title="51-lsi-2" href="./jmlr-2006-Learning_Image_Components_for_Object_Recognition.html">47 jmlr-2006-Learning Image Components for Object Recognition</a></p>
<p>Author: Michael W. Spratling</p><p>Abstract: In order to perform object recognition it is necessary to learn representations of the underlying components of images. Such components correspond to objects, object-parts, or features. Nonnegative matrix factorisation is a generative model that has been speciﬁcally proposed for ﬁnding such meaningful representations of image data, through the use of non-negativity constraints on the factors. This article reports on an empirical investigation of the performance of non-negative matrix factorisation algorithms. It is found that such algorithms need to impose additional constraints on the sparseness of the factors in order to successfully deal with occlusion. However, these constraints can themselves result in these algorithms failing to identify image components under certain conditions. In contrast, a recognition model (a competitive learning neural network algorithm) reliably and accurately learns representations of elementary image features without such constraints. Keywords: non-negative matrix factorisation, competitive learning, dendritic inhibition, object recognition</p><p>3 0.34397352 <a title="51-lsi-3" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>4 0.29535589 <a title="51-lsi-4" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<p>Author: David A. Ross, Richard S. Zemel</p><p>Abstract: Many perceptual models and theories hinge on treating objects as a collection of constituent parts. When applying these approaches to data, a fundamental problem arises: how can we determine what are the parts? We attack this problem using learning, proposing a form of generative latent factor model, in which each data dimension is allowed to select a different factor or part as its explanation. This approach permits a range of variations that posit different models for the appearance of a part. Here we provide the details for two such models: a discrete and a continuous one. Further, we show that this latent factor model can be extended hierarchically to account for correlations between the appearances of different parts. This permits modeling of data consisting of multiple categories, and learning these categories simultaneously with the parts when they are unobserved. Experiments demonstrate the ability to learn parts-based representations, and categories, of facial images and user-preference data. Keywords: parts, unsupervised learning, latent factor models, collaborative ﬁltering, hierarchical learning</p><p>5 0.26325127 <a title="51-lsi-5" href="./jmlr-2006-A_Linear_Non-Gaussian_Acyclic_Model_for_Causal_Discovery.html">4 jmlr-2006-A Linear Non-Gaussian Acyclic Model for Causal Discovery</a></p>
<p>Author: Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvärinen, Antti Kerminen</p><p>Abstract: In recent years, several methods have been proposed for the discovery of causal structure from non-experimental data. Such methods make various assumptions on the data generating process to facilitate its identiﬁcation from purely observational data. Continuing this line of research, we show how to discover the complete causal structure of continuous-valued data, under the assumptions that (a) the data generating process is linear, (b) there are no unobserved confounders, and (c) disturbance variables have non-Gaussian distributions of non-zero variances. The solution relies on the use of the statistical method known as independent component analysis, and does not require any pre-speciﬁed time-ordering of the variables. We provide a complete Matlab package for performing this LiNGAM analysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the effectiveness of the method using artiﬁcially generated data and real-world data. Keywords: independent component analysis, non-Gaussianity, causal discovery, directed acyclic graph, non-experimental data</p><p>6 0.21703492 <a title="51-lsi-6" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.19619873 <a title="51-lsi-7" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.1812683 <a title="51-lsi-8" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>9 0.17046887 <a title="51-lsi-9" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.16506688 <a title="51-lsi-10" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.14921656 <a title="51-lsi-11" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.13456531 <a title="51-lsi-12" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>13 0.13315335 <a title="51-lsi-13" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.12524913 <a title="51-lsi-14" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>15 0.12454271 <a title="51-lsi-15" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.12232342 <a title="51-lsi-16" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.12178003 <a title="51-lsi-17" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.12046923 <a title="51-lsi-18" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.11654757 <a title="51-lsi-19" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>20 0.1121584 <a title="51-lsi-20" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.031), (29, 0.341), (35, 0.023), (36, 0.049), (45, 0.02), (50, 0.04), (61, 0.016), (63, 0.041), (76, 0.03), (78, 0.046), (81, 0.023), (84, 0.045), (90, 0.028), (91, 0.043), (96, 0.111)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74754369 <a title="51-lda-1" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Matthias Heiler, Christoph Schnörr</p><p>Abstract: We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. We show that for ordinary NMF, our approach performs as well as existing stateof-the-art algorithms, while for sparsity-constrained NMF, as recently proposed by P. O. Hoyer in JMLR 5 (2004), it outperforms previous methods. In addition, we show how to extend NMF learning within the same optimization framework in order to make use of class membership information in supervised learning problems. Keywords: non-negative matrix factorization, second-order cone programming, sequential convex optimization, reverse-convex programming, sparsity</p><p>2 0.39476013 <a title="51-lda-2" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>3 0.393778 <a title="51-lda-3" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>Author: Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou</p><p>Abstract: We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the ﬁrst time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction. html. Keywords: transduction, transductive SVMs, semi-supervised learning, CCCP</p><p>4 0.38460466 <a title="51-lda-4" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram. Keywords: spectral clustering, blind source separation, computational auditory scene analysis</p><p>5 0.38255942 <a title="51-lda-5" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>Author: Mingrui Wu, Bernhard Schölkopf, Gökhan Bakır</p><p>Abstract: Many kernel learning algorithms, including support vector machines, result in a kernel machine, such as a kernel classiﬁer, whose key component is a weight vector in a feature space implicitly introduced by a positive deﬁnite kernel function. This weight vector is usually obtained by solving a convex optimization problem. Based on this fact we present a direct method to build sparse kernel learning algorithms by adding one more constraint to the original convex optimization problem, such that the sparseness of the resulting kernel machine is explicitly controlled while at the same time performance is kept as high as possible. A gradient based approach is provided to solve this modiﬁed optimization problem. Applying this method to the support vectom machine results in a concrete algorithm for building sparse large margin classiﬁers. These classiﬁers essentially ﬁnd a discriminating subspace that can be spanned by a small number of vectors, and in this subspace, the diﬀerent classes of data are linearly well separated. Experimental results over several classiﬁcation benchmarks demonstrate the eﬀectiveness of our approach. Keywords: sparse learning, sparse large margin classiﬁers, kernel learning algorithms, support vector machine, kernel Fisher discriminant</p><p>6 0.38021576 <a title="51-lda-6" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.37773716 <a title="51-lda-7" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.37757027 <a title="51-lda-8" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.37716967 <a title="51-lda-9" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.37654123 <a title="51-lda-10" href="./jmlr-2006-Adaptive_Prototype_Learning_Algorithms%3A_Theoretical_and_Experimental_Studies.html">13 jmlr-2006-Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies</a></p>
<p>11 0.37645298 <a title="51-lda-11" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>12 0.37615514 <a title="51-lda-12" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.37359086 <a title="51-lda-13" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>14 0.37104008 <a title="51-lda-14" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.37046257 <a title="51-lda-15" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>16 0.37024117 <a title="51-lda-16" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.36767331 <a title="51-lda-17" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>18 0.36593872 <a title="51-lda-18" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>19 0.36543807 <a title="51-lda-19" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.36267361 <a title="51-lda-20" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
