<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-9" href="#">jmlr2006-9</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</h1>
<br/><p>Source: <a title="jmlr-2006-9-pdf" href="http://jmlr.org/papers/volume7/braun06a/braun06a.pdf">pdf</a></p><p>Author: Mikio L. Braun</p><p>Abstract: The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to inﬁnity. We derive probabilistic ﬁnite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reﬂecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a signiﬁcant improvement over existing non-scaling bounds. Keywords: kernel matrix, eigenvalues, relative perturbation bounds</p><p>Reference: <a title="jmlr-2006-9-reference" href="../jmlr2006_reference/jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 7 e 12489 Berlin, Germany  Editor: John Shawe-Taylor  Abstract The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. [sent-7, score-0.936]
</p><p>2 It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to inﬁnity. [sent-8, score-0.637]
</p><p>3 Such scaling bounds have so far only been known for tail sums of eigenvalues. [sent-10, score-0.614]
</p><p>4 Keywords: kernel matrix, eigenvalues, relative perturbation bounds  1. [sent-13, score-0.54]
</p><p>5 In order to obtain accurate bounds on the approximation error of eigenvalues, it has proven to be of prime importance to derive bounds which scale with the eigenvalue under consideration. [sent-16, score-0.703]
</p><p>6 The reason is that the approximation error scales with the eigenvalue such that the error is typically much smaller for small eigenvalues. [sent-17, score-0.375]
</p><p>7 Therefore, non-scaling bounds tend to overestimate the error for small eigenvalues as they are dominated by the largest occurring errors. [sent-18, score-0.762]
</p><p>8 Now, since smooth kernels usually display rapidly decaying eigenvalues, and such kernels are typically used in machine learning, obtaining accurate bounds in particular for small eigenvalues is highly relevant. [sent-19, score-0.897]
</p><p>9 In an asymptotic setting, the effect that the approximation errors scale with the corresponding eigenvalues is well understood. [sent-20, score-0.662]
</p><p>10 B RAUN  0  0  10  10  −2  10 Approximation error  Estimated eigenvalue  −2  10  −4  10  −6  10  −8  10  −6  10  −8  10  −10  10  −10  10  −4  10  0  −12  5  10 Eigenvalue index i  15  10  20  (a) Approximate eigenvalues (box plots) and the true eigenvalues (dotted line). [sent-25, score-1.101]
</p><p>11 For orientation, the true eigenvalues (dotted line) have also be included in the plot. [sent-28, score-0.461]
</p><p>12 Figure 1: Approximated eigenvalues for kernel matrices with rapidly decaying eigenvalues have an approximation error which scales with the true eigenvalue. [sent-31, score-1.322]
</p><p>13 leading to a much smaller approximation error for small eigenvalues than for large eigenvalues (neglecting the variance of ψ2 for the moment). [sent-32, score-1.119]
</p><p>14 The following numerical example illustrates this effect: In Figure 1 we have plotted the approximate eigenvalues and the approximation errors for a kernel function with exponentially decaying eigenvalues constructed from Legendre polynomials (see Section 7. [sent-34, score-1.304]
</p><p>15 On the other hand, our bounds (solid lines) scale with the eigenvalues resulting in a bound which matches the true approximation error signiﬁcantly better. [sent-37, score-0.936]
</p><p>16 Such scaling bounds have recently been derived for tail sums of eigenvalues by Blanchard et al. [sent-38, score-1.075]
</p><p>17 There, the square root of the considered tail sum occurs in the bound, leading to bounds which correctly predict that the error for tail sums of small eigenvalues is smaller than that for tail sums starting with larger eigenvalues. [sent-40, score-1.799]
</p><p>18 However, scaling bounds for the approximation error between individual eigenvalues, as are derived in this work, were not known so far. [sent-41, score-0.368]
</p><p>19 Note that these two settings are not interchangeable: although bounds on tail sums can be combined (more concretely, subtracted) to obtain bounds for single eigenvalues, the scaling still depends on tail sums, not single eigenvalues. [sent-42, score-1.05]
</p><p>20 However, we have adopted a more theoretical approach in this work with the goal to understand 2304  E RROR B OUNDS FOR THE E IGENVALUES OF THE K ERNEL M ATRIX  the underlying principles which permit the derivation of scaling bounds for individual eigenvalues ﬁrst. [sent-45, score-0.69]
</p><p>21 In a second step, one could then use these results to construct statistical tests to estimate, for example, the overall decay rate of the eigenvalues based on these bounds. [sent-46, score-0.673]
</p><p>22 The basic perturbation bound deals with the approximation error based on the norms of certain error matrices. [sent-58, score-0.535]
</p><p>23 The norms of these error matrices are estimated for kernels with uniformly bounded eigenfunctions, and for kernels with bounded diagonal. [sent-59, score-0.439]
</p><p>24 The λi are the eigenvalues and the ψi the eigenfunctions of the integral operator Tk which R maps f to the function x → X k(x, y) f (y)µ(dy). [sent-64, score-0.666]
</p><p>25 Slightly abbreviating the true relationships, we will call λi the eigenvalues and ψi the eigenfunctions of k. [sent-65, score-0.666]
</p><p>26 n Denote the (random) eigenvalues of Kn by l1 ≥ . [sent-74, score-0.461]
</p><p>27 These eigenvalues of Kn converge to their asymptotic counterparts (λi )i∈N (see, for example, the papers by Koltchinskii and Gin´ , 2000, e and Dauxois et al. [sent-78, score-0.556]
</p><p>28 2305  B RAUN  For kernels with an inﬁnite number of non-zero eigenvalues, k can be decomposed into a degenerate kernel k[r] and an error function er given a truncation point r: r  k[r] (x, y) := ∑ λi ψi (x)ψi (y), i=1  (1)  [r]  r  e (x, y) := k(x, y) − k (x, y). [sent-82, score-0.509]
</p><p>29 1≤r≤n  Note that the optimal choice of r can not be easily computed in general since the choice depends on the true eigenvalues and, as we will see below, the form of the bounds on C and E might not allow to write down the minimizer in closed form. [sent-114, score-0.657]
</p><p>30 3 Estimates I: Bounded Eigenfunctions The ﬁrst class of kernel functions which we consider are Mercer kernels whose eigenfunctions ψ i are uniformly bounded. [sent-126, score-0.451]
</p><p>31 Since the eigenfunctions are uniformly bounded, the estimation errors involved in C(r, n) can be bounded conveniently using the Hoeffding inequality uniformly over all r(r + 1)/2 entries of C r . [sent-132, score-0.412]
</p><p>32 From these considerations we see that although the bound does not vanish as n → ∞ for this choice of r, the bound scales with the true eigenvalue at a rate which is only slightly slower. [sent-142, score-0.419]
</p><p>33 In Table 2, the optimal rates for r(n) and the resulting rates for the bound are shown for the cases of polynomial and exponential decay of the true eigenvalues (see Section 6 for the proofs). [sent-146, score-0.841]
</p><p>34 4 Estimates II: Bounded Kernel Function Since the restriction of uniformly bounded eigenfunctions is rather severe, we next consider the case where the kernel function is bounded. [sent-149, score-0.449]
</p><p>35 These terms appear because the eigenfuncr tions of bounded kernels may have values as large as K/λr , leading to large second moments of the eigenfunctions and large error terms in Cr . [sent-165, score-0.435]
</p><p>36 The derivation of the basic perturbation result relies on several results from the perturbation theory of symmetric matrices which are collected in the Appendix, while the estimates of the norm of the error matrices in Section 4 and 5 rely on standard large deviation bounds. [sent-172, score-0.52]
</p><p>37 The Basic Perturbation Bound In this section, we prove the basic perturbation bound (Theorem 1) which derives a bound on the perturbation in terms of the norms of certain error matrices. [sent-177, score-0.676]
</p><p>38 Recall that the kernel function k is decomposed into a degenerate kernel k [r] obtained by trunca[r] tion, and the error term er (see Equation (1)). [sent-179, score-0.528]
</p><p>39 The effect on n n individual eigenvalues of such perturbations is addressed by Weyl’s theorem (Theorem A. [sent-182, score-0.51]
</p><p>40 n [r]  For the degenerate kernel matrix Kn , we will derive a multiplicative bound on the approximation error of the eigenvalues. [sent-186, score-0.44]
</p><p>41 The main step is to realize that the kernel matrix of the truncated kernel can be written as the multiplicative perturbation of the diagonal matrix containing the true √ eigenvalues: Recall that [Ψr ]i = ψ (Xi )/ n (see Section 2. [sent-187, score-0.465]
</p><p>42 2 and its Corollary) leads to a multiplicative bound [r] for the eigenvalues of Kn : Lemma 6 For 1 ≤ i ≤ r ≤ n,  [r]  |λi (Kn ) − λi | ≤ λi Cr . [sent-194, score-0.572]
</p><p>43 From this condition, one can derive upper bounds on individual eigenfunctions ψ i and the error function er . [sent-223, score-0.591]
</p><p>44 First of all, note that by the deﬁnition of er , the error function is itself a Mercer kernel such that Er is positive-semideﬁnite for all sample n realizations. [sent-254, score-0.376]
</p><p>45 In this section, we will ﬁrst compute E(er (X, X)) in terms of the eigenvalues of k, and then derive a probabilistic bound on Er . [sent-259, score-0.572]
</p><p>46 1 Using the alternative bounds from Corollary 12 and 16, one obtains the bounds from Equation (2). [sent-278, score-0.392]
</p><p>47 2 for rates concerning the tail sums Λ>r ): 1  rn− 2 + r1−α = o(1). [sent-288, score-0.429]
</p><p>48 The box plots show the distributions of the observed approximation errors for kernel matrices built from n = 1000 sample points over 100 re-samples. [sent-318, score-0.409]
</p><p>49 1 Examples for Kernels with Bounded Eigenfunctions For the class of Mercer kernels whose eigenfunctions are uniformly bounded, we have been able to derive rather accurate ﬁnite sample size bounds. [sent-325, score-0.379]
</p><p>50 The relative error term C(r, n) scales rather moderately as √ r log r with r, and E(r, n) decays quickly, depending on the rate of decay of the eigenvalues, for both the case of polynomial and exponential decay. [sent-327, score-0.479]
</p><p>51 This plot illustrates the fact that it is essential for obtaining accurate estimates that the error bounds scale with the considered eigenvalue. [sent-338, score-0.403]
</p><p>52 A non-scaling bound will overestimate the error of smaller eigenvalues signiﬁcantly. [sent-339, score-0.705]
</p><p>53 The plot might suggest that our bound is actually worse for the ﬁrst few large eigenvalues, but note that the dashed line is only a lower bound to any non-scaling error bound, and actual error bounds will typically be much larger. [sent-340, score-0.642]
</p><p>54 Since we cannot write down the resulting kernel given some choice of eigenvalues in closed form, we truncate the expansion to the ﬁrst 1000 terms, resulting in a negligible difference to the true kernel function. [sent-345, score-0.788]
</p><p>55 The resulting kernel function for different choices of eigenvalues are plotted in Figure 3(a). [sent-346, score-0.647]
</p><p>56 We plot the bound for different choices of r and see that, with increasing r, the absolute error term becomes smaller such that the bound for small eigenvalues also becomes smaller while, at the same time, the bound for larger eigenvalues becomes larger. [sent-348, score-1.443]
</p><p>57 These rounding errors effectively lead to an additive perturbation of the kernel matrix, which in turn results in an additive perturbation of the eigenvalues of the same magnitude. [sent-351, score-0.987]
</p><p>58 An interesting observation is that although our bounds fail to be purely relative in the general case, numerically computed eigenvalues will always display a stagnation of small eigenvalues at a certain level due to round-off errors as well. [sent-352, score-1.244]
</p><p>59 In this case, the eigenfunctions can in principle grow unboundedly as the eigenvalues become smaller, leading to considerably larger error estimates. [sent-357, score-0.77]
</p><p>60 The most important difference to the previous case is that the relative error term depends on the √ eigenvalues themselves and scales with the factor 1/ λr . [sent-358, score-0.591]
</p><p>61 Therefore, having smaller eigenvalues can lead to a much larger relative error term (which will nevertheless ultimately decay to zero). [sent-359, score-0.739]
</p><p>62 −10  −i  i  20  10  0  0  10 Approximation error  10 Approximation error  100  Bound on the approximation error for λ = e  i  10  −10  10  −20  10  −30  0  −20  10  −40  10  10  80  (b) For quadratic decay, the bounds are only slightly better than the best possible non-scaling bound. [sent-372, score-0.483]
</p><p>63 )  Bound on the approximation error for λ = i 10  40 60 Eigenvalues  −60  20  40 60 Eigenvalues  80  10  100  (c) For faster polynomial decay, the bounds are much more accurate than the best possible nonscaling bound for small eigenvalues. [sent-375, score-0.516]
</p><p>64 The reason is that λi is so small that not a single point has hit Ai in the sample of n = 1000 points; the kernel is effectively degenerate and the approximate eigenvalues beyond l 20 are equal to zero. [sent-395, score-0.689]
</p><p>65 Over all, compared with the examples for bounded kernel functions, the bounds are considerably less tight, but they still correctly predict the scaling of the approximation error with regard to the true eigenvalue. [sent-398, score-0.578]
</p><p>66 To illustrate that the bounds can nevertheless be much more accurate even for moderately small sample sizes, we will compare our bounds against a Hoeffding-type bound which does not scale with the eigenvalue under consideration. [sent-402, score-0.76]
</p><p>67 Such bounds exist, but only for tail sums of eigenvalues (see the papers by Shawe-Taylor et al. [sent-404, score-1.042]
</p><p>68 (2005), there is a bound on the concentration of single eigenvalues around their mean: with probability larger than 1 − δ, 1 2 log . [sent-409, score-0.63]
</p><p>69 Note that although the bound becomes very large for large r, the minimum over all bounds is the ﬁnal bound on the approximation error. [sent-412, score-0.483]
</p><p>70 We consider eigenvalues λi = ei/3 /Z, where Z is the normalization constant. [sent-415, score-0.461]
</p><p>71 The solid lines are the bounds for r ∈ {10, 50, 200, 500}, while the dashed line shows the best possible non-scaling error bound. [sent-418, score-0.379]
</p><p>72 2322  E RROR B OUNDS FOR THE E IGENVALUES OF THE K ERNEL M ATRIX  This bound has the required asymptotic decay rate of O(n −1/2 ). [sent-419, score-0.39]
</p><p>73 Terms similar to this bound also occur in the more complex bounds on tail sums of eigenvalues, albeit with larger constants. [sent-420, score-0.692]
</p><p>74 For these plots, since the eigenvalues are known, the optimal r has been computed by numerically minimizing the bound. [sent-425, score-0.493]
</p><p>75 For polynomial decay of rate λ i = i−2 , the bounds are clearly inferior to the Hoeffding-type bounds and one can also clearly see that the overall rate is sub-stochastic. [sent-427, score-0.699]
</p><p>76 However, for faster decay rates, the bounds for smaller eigenvalues are clearly superior, also demonstrating that the number of samples necessary to yield a comparably small bound using the Hoeffding-type bound is fairly large. [sent-428, score-1.056]
</p><p>77 As we will discuss in Section 8, there exist bounds which directly deal with tail sums of eigenvalues and are more accurate in this case. [sent-431, score-1.08]
</p><p>78 However, it is instructive to derive a rough estimate for tail sums based on our bounds. [sent-432, score-0.385]
</p><p>79 We start with bounding the difference between tail sums by summing the individual bounds: ∞  n  ∑  i=r+1  li −  ∑  i=r+1  λi ≤  n  ∑  i=r+1  |li − λi | + Λ>n+1 ≤  n  ∑  (λiC(r, n) + E(r, n)) + Λ>n+1 . [sent-433, score-0.492]
</p><p>80 i=r+1  i=r+1  Let us consider these tail sums for i → ∞. [sent-438, score-0.385]
</p><p>81 Thus, for large i, the bound on the tail sums actually becomes small, giving more accurate bounds than those obtainable by a non-scaling bound. [sent-440, score-0.73]
</p><p>82 In Figure 6, we again compare the bound derived in this work against a Hoeffding-type bound for tail sums of eigenvalues. [sent-441, score-0.607]
</p><p>83 Related Work The bounds presented in this work are the ﬁrst ﬁnite sample size bounds for single eigenvalues which scale with the eigenvalue under consideration. [sent-445, score-1.025]
</p><p>84 The bounds are plotted for eigenvalues λ1 , λ5 , λ10 , λ50 , λ100 , and for the cases of polynomial decay, also for λ500 . [sent-448, score-0.727]
</p><p>85 The tail sums are plotted for r ∈ {10, 20, 30, 40, 50}. [sent-453, score-0.423]
</p><p>86 (1982) and Koltchinskii and Gin´ (2000) where central limit type results for the limit distributions of the eigenvalues are derive. [sent-457, score-0.461]
</p><p>87 (2005) discusses several aspects of the relation between the approximate eigenvalues and their asymptotic counterparts. [sent-462, score-0.528]
</p><p>88 These also include concentration inequalities relating the approximate eigenvalues to their expectations (similar inequalities can also be found in the Ph. [sent-463, score-0.461]
</p><p>89 Finally, Theorem 1 and 2 of that paper provide ﬁnite sample size bounds on the approximation error for tail sums of the eigenvalues. [sent-466, score-0.758]
</p><p>90 However, these bounds do not scale with the size of the eigenvalues, leading to the already discussed overestimation of the true approximation error in particular for small eigenvalues. [sent-467, score-0.394]
</p><p>91 In particular, non-scaling bounds are derived exhibiting fast convergence rates, as well as scaling bounds for tail sums of eigenvalues. [sent-470, score-0.81]
</p><p>92 Compared to the results presented in this work, all of these results are either dealing with the asymptotic setting or, in the case of ﬁnite sample size bounds, are non-scaling or only deal with tail sums of eigenvalues. [sent-472, score-0.49]
</p><p>93 However, one could suspect that the absolute terms occurring in our bounds are an artifact of the more elementary approach (in particular 2325  B RAUN  since these terms are a side-effect of the truncation of the kernel matrix). [sent-480, score-0.435]
</p><p>94 Conclusion and Outlook We have derived ﬁnite sample size bounds on the error between single eigenvalues of the kernel matrix and their asymptotic limits. [sent-485, score-0.984]
</p><p>95 These bounds scale with the eigenvalue under consideration leading to signiﬁcantly more accurate bounds for single eigenvalues than previously known bounds in particular for small eigenvalues and small sample sizes. [sent-486, score-1.75]
</p><p>96 Also for fairly large sample sizes, the bounds can still be superior to existing bounds since the number of samples necessary to make existing non-scaling bounds competitive can be unrealistically large. [sent-487, score-0.657]
</p><p>97 (i) If additional information on the kernel, or the probability distribution is available, the bounds on the norms of the error matrices could be improved leading to more accurate bounds. [sent-489, score-0.417]
</p><p>98 Then, the bounds presented in this work predict that the estimated eigenvalues decay at the same rate giving conﬁdence bounds which scale at the correct rate. [sent-495, score-1.094]
</p><p>99 1 Perturbation of Hermitian Matrices We use two classical results on the perturbation of eigenvalues for Hermitian matrices. [sent-506, score-0.63]
</p><p>100 2 Asymptotics of Inﬁnite Sums For convenience, we collect two elementary computations to estimate the asymptotic rates of tail sums of sequences with polynomial and exponential decay. [sent-527, score-0.528]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('eigenvalues', 0.461), ('cr', 0.258), ('tail', 0.24), ('eigenfunctions', 0.205), ('raun', 0.201), ('kn', 0.198), ('bounds', 0.196), ('ernel', 0.186), ('perturbation', 0.169), ('atrix', 0.158), ('igenvalues', 0.158), ('rror', 0.158), ('decay', 0.149), ('kernel', 0.148), ('sums', 0.145), ('ounds', 0.13), ('er', 0.116), ('bound', 0.111), ('li', 0.107), ('eigenvalue', 0.105), ('bernstein', 0.094), ('ostrowski', 0.093), ('blanchard', 0.086), ('chebychev', 0.082), ('error', 0.074), ('tr', 0.071), ('asymptotic', 0.067), ('mercer', 0.066), ('approximation', 0.065), ('truncation', 0.065), ('kernels', 0.064), ('rate', 0.063), ('mikio', 0.062), ('weyl', 0.062), ('bounded', 0.062), ('var', 0.061), ('log', 0.058), ('hermitian', 0.053), ('theorem', 0.049), ('decaying', 0.047), ('moderately', 0.047), ('gin', 0.047), ('ansatz', 0.046), ('dauxois', 0.046), ('ktr', 0.046), ('polynomials', 0.044), ('rates', 0.044), ('dashed', 0.044), ('plots', 0.043), ('horn', 0.043), ('lemma', 0.043), ('dx', 0.042), ('norms', 0.042), ('degenerate', 0.042), ('errors', 0.04), ('dence', 0.04), ('legendre', 0.039), ('supplementary', 0.039), ('plugging', 0.038), ('box', 0.038), ('accurate', 0.038), ('plotted', 0.038), ('sample', 0.038), ('koltchinskii', 0.038), ('inequality', 0.037), ('matrices', 0.037), ('braun', 0.035), ('solid', 0.035), ('estimates', 0.034), ('uniformly', 0.034), ('orthogonal', 0.033), ('scaling', 0.033), ('numerically', 0.032), ('plot', 0.032), ('polynomial', 0.032), ('principal', 0.031), ('abramowitz', 0.031), ('eer', 0.031), ('interchangeable', 0.031), ('joachim', 0.031), ('overestimate', 0.031), ('quartile', 0.031), ('sas', 0.031), ('stegun', 0.031), ('truncate', 0.031), ('unrealistically', 0.031), ('lines', 0.03), ('leading', 0.03), ('xi', 0.03), ('scale', 0.029), ('scales', 0.029), ('converge', 0.028), ('omitting', 0.028), ('smaller', 0.028), ('display', 0.027), ('johnson', 0.027), ('relative', 0.027), ('bonn', 0.026), ('eigenfunction', 0.026), ('absolute', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="9-tfidf-1" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>Author: Mikio L. Braun</p><p>Abstract: The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to inﬁnity. We derive probabilistic ﬁnite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reﬂecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a signiﬁcant improvement over existing non-scaling bounds. Keywords: kernel matrix, eigenvalues, relative perturbation bounds</p><p>2 0.10959414 <a title="9-tfidf-2" href="./jmlr-2006-Inductive_Synthesis_of_Functional_Programs%3A_An_Explanation_Based_Generalization_Approach_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">39 jmlr-2006-Inductive Synthesis of Functional Programs: An Explanation Based Generalization Approach     (Special Topic on Inductive Programming)</a></p>
<p>Author: Emanuel Kitzelmann, Ute Schmid</p><p>Abstract: We describe an approach to the inductive synthesis of recursive equations from input/outputexamples which is based on the classical two-step approach to induction of functional Lisp programs of Summers (1977). In a ﬁrst step, I/O-examples are rewritten to traces which explain the outputs given the respective inputs based on a datatype theory. These traces can be integrated into one conditional expression which represents a non-recursive program. In a second step, this initial program term is generalized into recursive equations by searching for syntactical regularities in the term. Our approach extends the classical work in several aspects. The most important extensions are that we are able to induce a set of recursive equations in one synthesizing step, the equations may contain more than one recursive call, and additionally needed parameters are automatically introduced. Keywords: inductive program synthesis, inductive functional programming, explanation based generalization, recursive program schemes</p><p>3 0.087918155 <a title="9-tfidf-3" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram. Keywords: spectral clustering, blind source separation, computational auditory scene analysis</p><p>4 0.076662555 <a title="9-tfidf-4" href="./jmlr-2006-Universal_Kernels.html">93 jmlr-2006-Universal Kernels</a></p>
<p>Author: Charles A. Micchelli, Yuesheng Xu, Haizhang Zhang</p><p>Abstract: In this paper we investigate conditions on the features of a continuous kernel so that it may approximate an arbitrary continuous target function uniformly on any compact subset of the input space. A number of concrete examples are given of kernels with this universal approximating property. Keywords: density, translation invariant kernels, radial kernels</p><p>5 0.059368908 <a title="9-tfidf-5" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>Author: Sayan Mukherjee, Ding-Xuan Zhou</p><p>Abstract: We introduce an algorithm that learns gradients from samples in the supervised learning framework. An error analysis is given for the convergence of the gradient estimated by the algorithm to the true gradient. The utility of the algorithm for the problem of variable selection as well as determining variable covariance is illustrated on simulated data as well as two gene expression data sets. For square loss we provide a very efﬁcient implementation with respect to both memory and time. Keywords: Tikhnonov regularization, variable selection, reproducing kernel Hilbert space, generalization bounds</p><p>6 0.056336291 <a title="9-tfidf-6" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>7 0.054459859 <a title="9-tfidf-7" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.051957563 <a title="9-tfidf-8" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>9 0.049523585 <a title="9-tfidf-9" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>10 0.048669662 <a title="9-tfidf-10" href="./jmlr-2006-Adaptive_Prototype_Learning_Algorithms%3A_Theoretical_and_Experimental_Studies.html">13 jmlr-2006-Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies</a></p>
<p>11 0.048185337 <a title="9-tfidf-11" href="./jmlr-2006-Lower_Bounds_and_Aggregation_in_Density_Estimation.html">58 jmlr-2006-Lower Bounds and Aggregation in Density Estimation</a></p>
<p>12 0.04799984 <a title="9-tfidf-12" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.047901548 <a title="9-tfidf-13" href="./jmlr-2006-Stability_Properties_of_Empirical_Risk_Minimization_over_Donsker_Classes.html">84 jmlr-2006-Stability Properties of Empirical Risk Minimization over Donsker Classes</a></p>
<p>14 0.044249937 <a title="9-tfidf-14" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>15 0.042911548 <a title="9-tfidf-15" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>16 0.041969683 <a title="9-tfidf-16" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>17 0.04080211 <a title="9-tfidf-17" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.040354934 <a title="9-tfidf-18" href="./jmlr-2006-A_Simulation-Based_Algorithm_for_Ergodic_Control_of_Markov_Chains_Conditioned_on_Rare_Events.html">7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</a></p>
<p>19 0.040261589 <a title="9-tfidf-19" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>20 0.039334495 <a title="9-tfidf-20" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.215), (1, -0.078), (2, 0.001), (3, -0.112), (4, -0.023), (5, 0.075), (6, -0.069), (7, -0.042), (8, -0.099), (9, 0.043), (10, 0.156), (11, -0.229), (12, 0.007), (13, 0.177), (14, 0.018), (15, 0.048), (16, 0.007), (17, -0.067), (18, -0.198), (19, 0.129), (20, 0.049), (21, -0.037), (22, 0.062), (23, -0.119), (24, -0.278), (25, 0.01), (26, -0.079), (27, -0.095), (28, 0.085), (29, 0.066), (30, -0.039), (31, 0.155), (32, 0.055), (33, -0.106), (34, 0.124), (35, -0.22), (36, -0.001), (37, 0.021), (38, 0.038), (39, -0.109), (40, 0.106), (41, 0.074), (42, 0.093), (43, -0.189), (44, 0.063), (45, 0.098), (46, 0.026), (47, 0.054), (48, 0.072), (49, -0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96687645 <a title="9-lsi-1" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>Author: Mikio L. Braun</p><p>Abstract: The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to inﬁnity. We derive probabilistic ﬁnite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reﬂecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a signiﬁcant improvement over existing non-scaling bounds. Keywords: kernel matrix, eigenvalues, relative perturbation bounds</p><p>2 0.64643884 <a title="9-lsi-2" href="./jmlr-2006-Inductive_Synthesis_of_Functional_Programs%3A_An_Explanation_Based_Generalization_Approach_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">39 jmlr-2006-Inductive Synthesis of Functional Programs: An Explanation Based Generalization Approach     (Special Topic on Inductive Programming)</a></p>
<p>Author: Emanuel Kitzelmann, Ute Schmid</p><p>Abstract: We describe an approach to the inductive synthesis of recursive equations from input/outputexamples which is based on the classical two-step approach to induction of functional Lisp programs of Summers (1977). In a ﬁrst step, I/O-examples are rewritten to traces which explain the outputs given the respective inputs based on a datatype theory. These traces can be integrated into one conditional expression which represents a non-recursive program. In a second step, this initial program term is generalized into recursive equations by searching for syntactical regularities in the term. Our approach extends the classical work in several aspects. The most important extensions are that we are able to induce a set of recursive equations in one synthesizing step, the equations may contain more than one recursive call, and additionally needed parameters are automatically introduced. Keywords: inductive program synthesis, inductive functional programming, explanation based generalization, recursive program schemes</p><p>3 0.34030053 <a title="9-lsi-3" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram. Keywords: spectral clustering, blind source separation, computational auditory scene analysis</p><p>4 0.31443888 <a title="9-lsi-4" href="./jmlr-2006-Universal_Kernels.html">93 jmlr-2006-Universal Kernels</a></p>
<p>Author: Charles A. Micchelli, Yuesheng Xu, Haizhang Zhang</p><p>Abstract: In this paper we investigate conditions on the features of a continuous kernel so that it may approximate an arbitrary continuous target function uniformly on any compact subset of the input space. A number of concrete examples are given of kernels with this universal approximating property. Keywords: density, translation invariant kernels, radial kernels</p><p>5 0.28423572 <a title="9-lsi-5" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>Author: Magnus Ekdahl, Timo Koski</p><p>Abstract: In many pattern recognition/classiﬁcation problem the true class conditional model and class probabilities are approximated for reasons of reducing complexity and/or of statistical estimation. The approximated classiﬁer is expected to have worse performance, here measured by the probability of correct classiﬁcation. We present an analysis valid in general, and easily computable formulas for estimating the degradation in probability of correct classiﬁcation when compared to the optimal classiﬁer. An example of an approximation is the Na¨ve Bayes classiﬁer. We show that the perforı mance of the Na¨ve Bayes depends on the degree of functional dependence between the features ı and labels. We provide a sufﬁcient condition for zero loss of performance, too. Keywords: Bayesian networks, na¨ve Bayes, plug-in classiﬁer, Kolmogorov distance of variation, ı variational learning</p><p>6 0.28403363 <a title="9-lsi-6" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.28012863 <a title="9-lsi-7" href="./jmlr-2006-Adaptive_Prototype_Learning_Algorithms%3A_Theoretical_and_Experimental_Studies.html">13 jmlr-2006-Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies</a></p>
<p>8 0.26158404 <a title="9-lsi-8" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>9 0.26140022 <a title="9-lsi-9" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>10 0.25992876 <a title="9-lsi-10" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.2590037 <a title="9-lsi-11" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>12 0.25529492 <a title="9-lsi-12" href="./jmlr-2006-Lower_Bounds_and_Aggregation_in_Density_Estimation.html">58 jmlr-2006-Lower Bounds and Aggregation in Density Estimation</a></p>
<p>13 0.25083363 <a title="9-lsi-13" href="./jmlr-2006-Action_Elimination_and_Stopping_Conditions_for_the_Multi-Armed_Bandit_and_Reinforcement_Learning_Problems.html">10 jmlr-2006-Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems</a></p>
<p>14 0.25075939 <a title="9-lsi-14" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>15 0.24888638 <a title="9-lsi-15" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>16 0.247329 <a title="9-lsi-16" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>17 0.24659875 <a title="9-lsi-17" href="./jmlr-2006-Bounds_for_Linear_Multi-Task_Learning.html">16 jmlr-2006-Bounds for Linear Multi-Task Learning</a></p>
<p>18 0.24369311 <a title="9-lsi-18" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>19 0.2325086 <a title="9-lsi-19" href="./jmlr-2006-Stability_Properties_of_Empirical_Risk_Minimization_over_Donsker_Classes.html">84 jmlr-2006-Stability Properties of Empirical Risk Minimization over Donsker Classes</a></p>
<p>20 0.22809 <a title="9-lsi-20" href="./jmlr-2006-On_Model_Selection_Consistency_of_Lasso.html">66 jmlr-2006-On Model Selection Consistency of Lasso</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.049), (30, 0.056), (36, 0.056), (45, 0.054), (50, 0.087), (52, 0.189), (63, 0.04), (68, 0.013), (76, 0.047), (78, 0.022), (79, 0.025), (81, 0.058), (84, 0.029), (90, 0.074), (91, 0.036), (94, 0.01), (96, 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81371683 <a title="9-lda-1" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>Author: Mikio L. Braun</p><p>Abstract: The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to inﬁnity. We derive probabilistic ﬁnite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reﬂecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a signiﬁcant improvement over existing non-scaling bounds. Keywords: kernel matrix, eigenvalues, relative perturbation bounds</p><p>2 0.60464823 <a title="9-lda-2" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>3 0.60045344 <a title="9-lda-3" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>Author: Sayan Mukherjee, Qiang Wu</p><p>Abstract: We introduce an algorithm that simultaneously estimates a classiﬁcation function as well as its gradient in the supervised learning framework. The motivation for the algorithm is to ﬁnd salient variables and estimate how they covary. An efﬁcient implementation with respect to both memory and time is given. The utility of the algorithm is illustrated on simulated data as well as a gene expression data set. An error analysis is given for the convergence of the estimate of the classiﬁcation function and its gradient to the true classiﬁcation function and true gradient. Keywords: Tikhnonov regularization, variable selection, reproducing kernel Hilbert space, generalization bounds, classiﬁcation</p><p>4 0.6000967 <a title="9-lda-4" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>Author: Dmitry M. Malioutov, Jason K. Johnson, Alan S. Willsky</p><p>Abstract: We present a new framework based on walks in a graph for analysis and inference in Gaussian graphical models. The key idea is to decompose the correlation between each pair of variables as a sum over all walks between those variables in the graph. The weight of each walk is given by a product of edgewise partial correlation coefﬁcients. This representation holds for a large class of Gaussian graphical models which we call walk-summable. We give a precise characterization of this class of models, and relate it to other classes including diagonally dominant, attractive, nonfrustrated, and pairwise-normalizable. We provide a walk-sum interpretation of Gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles. The walk-sum perspective leads to a better understanding of Gaussian belief propagation and to stronger results for its convergence in loopy graphs. Keywords: Gaussian graphical models, walk-sum analysis, convergence of loopy belief propagation</p><p>5 0.59894729 <a title="9-lda-5" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>Author: Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We present a family of margin based online learning algorithms for various prediction tasks. In particular we derive and analyze algorithms for binary and multiclass categorization, regression, uniclass prediction and sequence prediction. The update steps of our different algorithms are all based on analytical solutions to simple constrained optimization problems. This uniﬁed view allows us to prove worst-case loss bounds for the different algorithms and for the various decision problems based on a single lemma. Our bounds on the cumulative loss of the algorithms are relative to the smallest loss that can be attained by any ﬁxed hypothesis, and as such are applicable to both realizable and unrealizable settings. We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real data sets.</p><p>6 0.58980179 <a title="9-lda-6" href="./jmlr-2006-On_Model_Selection_Consistency_of_Lasso.html">66 jmlr-2006-On Model Selection Consistency of Lasso</a></p>
<p>7 0.58930659 <a title="9-lda-7" href="./jmlr-2006-Learning_Minimum_Volume_Sets.html">48 jmlr-2006-Learning Minimum Volume Sets</a></p>
<p>8 0.58839005 <a title="9-lda-8" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>9 0.58591998 <a title="9-lda-9" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>10 0.58577114 <a title="9-lda-10" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>11 0.5840922 <a title="9-lda-11" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>12 0.57849443 <a title="9-lda-12" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>13 0.57199091 <a title="9-lda-13" href="./jmlr-2006-Pattern_Recognition_for__Conditionally_Independent_Data.html">73 jmlr-2006-Pattern Recognition for  Conditionally Independent Data</a></p>
<p>14 0.57119793 <a title="9-lda-14" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>15 0.569956 <a title="9-lda-15" href="./jmlr-2006-In_Search_of_Non-Gaussian_Components_of_a_High-Dimensional_Distribution.html">36 jmlr-2006-In Search of Non-Gaussian Components of a High-Dimensional Distribution</a></p>
<p>16 0.56673086 <a title="9-lda-16" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>17 0.5607971 <a title="9-lda-17" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>18 0.55880225 <a title="9-lda-18" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>19 0.55743408 <a title="9-lda-19" href="./jmlr-2006-Geometric_Variance_Reduction_in_Markov_Chains%3A_Application_to_Value_Function_and_Gradient_Estimation.html">35 jmlr-2006-Geometric Variance Reduction in Markov Chains: Application to Value Function and Gradient Estimation</a></p>
<p>20 0.55382144 <a title="9-lda-20" href="./jmlr-2006-Superior_Guarantees_for_Sequential_Prediction_and_Lossless_Compression_via_Alphabet_Decomposition.html">90 jmlr-2006-Superior Guarantees for Sequential Prediction and Lossless Compression via Alphabet Decomposition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
