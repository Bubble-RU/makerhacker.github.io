<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-15" href="#">jmlr2006-15</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-15-pdf" href="http://jmlr.org/papers/volume7/niculescu06a/niculescu06a.pdf">pdf</a></p><p>Author: Radu Stefan Niculescu, Tom M. Mitchell, R. Bharat Rao</p><p>Abstract: The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. This paper considers a variety of types of domain knowledge for constraining parameter estimates when learning Bayesian networks. In particular, we consider domain knowledge that constrains the values or relationships among subsets of parameters in a Bayesian network with known structure. We incorporate a wide variety of parameter constraints into learning procedures for Bayesian networks, by formulating this task as a constrained optimization problem. The assumptions made in module networks, dynamic Bayes nets and context speciﬁc independence models can be viewed as particular cases of such parameter constraints. We present closed form solutions or fast iterative algorithms for estimating parameters subject to several speciﬁc classes of parameter constraints, including equalities and inequalities among parameters, constraints on individual parameters, and constraints on sums and ratios of parameters, for discrete and continuous variables. Our methods cover learning from both frequentist and Bayesian points of view, from both complete and incomplete data. We present formal guarantees for our estimators, as well as methods for automatically learning useful parameter constraints from data. To validate our approach, we apply it to the domain of fMRI brain image analysis. Here we demonstrate the ability of our system to ﬁrst learn useful relationships among parameters, and then to use them to constrain the training of the Bayesian network, resulting in improved cross-validated accuracy of the learned model. Experiments on synthetic data are also presented. Keywords: Bayesian networks, constrained optimization, domain knowledge c 2006 Radu Stefan Niculescu, Tom Mitchell and Bharat Rao. N ICULESCU , M ITCHELL AND R AO</p><p>Reference: <a title="jmlr-2006-15-reference" href="../jmlr2006_reference/jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We incorporate a wide variety of parameter constraints into learning procedures for Bayesian networks, by formulating this task as a constrained optimization problem. [sent-13, score-0.325]
</p><p>2 For example, a domain expert might provide prior knowledge specifying conditional independencies among variables, constraining or even fully specifying the network structure of the Bayesian network. [sent-33, score-0.331]
</p><p>3 In addition to helping specify the network structure, the domain expert might also provide prior knowledge about the values of certain parameters in the conditional probability tables (CPTs) of the network, or knowledge in the form of prior distributions over these parameters. [sent-34, score-0.458]
</p><p>4 Applying these efﬁcient methods allows us to take advantage of parameter constraints provided by experts or learned by the program, to perform more accurate learning of very large Bayesian networks (thousands of variables) based on very few (tens) examples, as we will see later in the paper. [sent-40, score-0.341]
</p><p>5 We show how widely used models including hidden Markov models, dynamic Bayesian networks, module networks and context speciﬁc independence are special cases of one of our constraint types, described in subsection 4. [sent-42, score-0.345]
</p><p>6 Our framework is able to represent parameter sharing assumptions at the level of granularity of individual parameters. [sent-44, score-0.401]
</p><p>7 Moreover, we provide closed form maximum likelihood estimators when constraints come in the form of several types of inequality constraints. [sent-46, score-0.458]
</p><p>8 Finally, we present a method for automatically learning parameter constraints, which we illustrate on a complex task of modelling the fMRI brain image signal during a cognitive task. [sent-48, score-0.347]
</p><p>9 Section 3 presents the problem and describes our prior work on a framework for incorporating parameter constraints to perform estimation of parameters of Bayesian networks. [sent-50, score-0.358]
</p><p>10 There we show how learning in current models that use parameter sharing assumptions can be viewed as a special case of our approach. [sent-52, score-0.436]
</p><p>11 Related Work The main methods to represent relationships among parameters fall into two main categories: Dirichlet priors and their variants (including smoothing techniques) and parameter sharing of several kinds. [sent-57, score-0.587]
</p><p>12 Extensions of Dirichlet priors include Dirichlet tree priors (Minka, 1999) and dependent Dirichlet priors (Hooper, 2004). [sent-62, score-0.396]
</p><p>13 A widely used form of parameter constraints employed by Bayesian networks is parameter sharing. [sent-67, score-0.39]
</p><p>14 Models that use different types of parameter sharing include: dynamic Bayesian networks 1359  N ICULESCU , M ITCHELL AND R AO  (Murphy, 2002) and their special case hidden Markov models (Rabiner, 1989), module networks (Segal et al. [sent-68, score-0.724]
</p><p>15 Parameter sharing methods constrain parameters to share the same value, but do not capture more complicated constraints among parameters such as inequality constraints or constraints on sums of parameter values. [sent-73, score-1.037]
</p><p>16 None of the prior models allow sharing at the level of granularity of individual parameters. [sent-75, score-0.381]
</p><p>17 While the optimization methods we use are not new, applying them to our task allows us to take advantage of expert parameter constraints to perform more accurate learning of very large Bayesian networks (thousands of variables) based on very few (tens) examples, as we will see in subsection 5. [sent-84, score-0.481]
</p><p>18 The equality constraints are of the form gi (θ) = 0 for 1 ≤ i ≤ m and the inequality constraints are of the form h j (θ) ≤ 0 for 1 ≤ j ≤ k, where θ represents the set of parameters of the Bayesian network. [sent-91, score-0.384]
</p><p>19 When computing parameter estimators in the discrete case, we additionally assume that all observed counts corresponding to parameters in the Bayesian network are strictly positive. [sent-101, score-0.402]
</p><p>20 1 Parameter Sharing within One Distribution This class of parameter constraints allows asserting that speciﬁc user-selected parameters within a single conditional probability distribution must be shared. [sent-144, score-0.351]
</p><p>21 Assume the parameter constraint asserts that several parameters are equal by asserting that the parameter θi appears in ki different positions in the conditional distribution. [sent-148, score-0.4]
</p><p>22 At ﬁrst it may appear that we can develop maximum likelihood estimates for θi and the other network parameters using standard methods, by introducing new variables that capture the groups of shared parameters. [sent-154, score-0.344]
</p><p>23 Therefore, if Y takes only one value, the task of ﬁnding maximum likelihood estimators with parameter sharing is reduced to the one of ﬁnding standard maximum likelihood estimators for X12 |Y = 0. [sent-161, score-0.809]
</p><p>24 Below we present closed form solutions for the maximum likelihood estimators from complete data and for the normalization constant for the corresponding constrained Dirichlet priors used to perform maximum aposteriori estimation. [sent-169, score-0.517]
</p><p>25 The normalization constant for the constrained Dirichlet prior can be computed over the scope of a certain constraint and then all such constants are multiplied to obtain the normalization constant for the prior over the whole set of parameters of the Bayesian network. [sent-171, score-0.333]
</p><p>26 In the case of no parameter sharing (that is ki = 1 for all i), all these normalization constants are equal and we obtain the standard Dirichlet prior. [sent-199, score-0.551]
</p><p>27 The HPM model is inspired by our interest in modelling hidden cognitive processes in the brain, given a time series of observed fMRI images of brain activation. [sent-209, score-0.429]
</p><p>28 Figure 1 shows an example of a hidden process model for the fMRI activity in a voxel in the brain during a cognitive task involving reading a sentence and looking at a picture. [sent-225, score-0.754]
</p><p>29 We assume the times at which the hidden processes occur are known, that the types of the processes are known, and that two instances of the same types of process may not be active simultaneously. [sent-229, score-0.406]
</p><p>30 The natural constraints of this domain lead to an opportunity to specify prior knowledge in the form of parameter constraints, as follows: an external stimulus will typically inﬂuence the activity in multiple voxels of the brain during one cognitive task. [sent-237, score-0.989]
</p><p>31 For example, looking at a picture may activate 1365  N ICULESCU , M ITCHELL AND R AO  Figure 1: A hidden process model to model a human subject who is asked to read a sentence and to look at a picture. [sent-238, score-0.373]
</p><p>32 The activity in a given voxel X in the brain is modelled as a hidden process model with two processes: ”Sentence” (P1 ) and ”Picture” (P2 ). [sent-241, score-0.512]
</p><p>33 1366  BAYESIAN N ETWORK L EARNING WITH PARAMETER C ONSTRAINTS  many voxels in the visual cortex. [sent-246, score-0.351]
</p><p>34 However, certain groups of voxels that are close together often have similarly shaped time series, but with different amplitude. [sent-249, score-0.351]
</p><p>35 In this case, we believe it is reasonable to assume that the underlying hidden processes corresponding to these voxels are proportional to one another. [sent-250, score-0.53]
</p><p>36 , X V share their corresponding hidden process models if there exist base processes P1 , . [sent-256, score-0.326]
</p><p>37 With these considerations, we are now ready to present an algorithm ˆ ˆ ˆ to compute maximum likelihood estimators (P, C, σ) of the parameters in the shared hidden process model: ¯ Algorithm 2 (Maximum likelihood estimators in a shared hidden process model) Let X be the v . [sent-275, score-1.082]
</p><p>38 In Section 5 we will experimentally prove the beneﬁts of this algorithm over methods that do not take advantage of parameter sharing assumptions. [sent-290, score-0.401]
</p><p>39 We brieﬂy describe these types of parameter constraints below, and provide real-world examples of prior knowledge that can be expressed by each form of constraint. [sent-302, score-0.372]
</p><p>40 Example: Several neighboring voxels in the brain exhibit similar activation patterns, but with different amplitudes when a subject is presented with a given stimulus. [sent-338, score-0.464]
</p><p>41 Brieﬂy, this general parameter sharing allows for a group of conditional probability distributions to share some parameters across all distributions in the group, but not share the remaining parameters. [sent-340, score-0.586]
</p><p>42 It is also important to note that different types of parameter constraints can be mixed together when learning the parameters of a Bayesian network as long as the scopes of these constraints do not overlap. [sent-343, score-0.579]
</p><p>43 Our experiments demonstrate that Bayesian network models that take advantage of prior knowledge in the form of parameter constraints outperform similar models which choose to ignore this kind of knowledge. [sent-346, score-0.466]
</p><p>44 1 Synthetic Data - Estimating Parameters of a Discrete Variable This section describes experiments involving one of the simplest forms of parameter constraint: parameter sharing within one distribution, presented in subsection 4. [sent-348, score-0.544]
</p><p>45 One is a standard Bayesian network (STBN) that is learned using standard Bayesian networks maximum likelihood estimators with no parameter sharing. [sent-372, score-0.426]
</p><p>46 1 assuming the correct parameter sharing was speciﬁed by an oracle. [sent-374, score-0.401]
</p><p>47 It can be seen that our model (PDKBN) that takes advantage of parameter constraints consistently outperforms the standard Bayesian network model which does not employ such constraints. [sent-384, score-0.323]
</p><p>48 To get a better idea of how beneﬁcial the prior knowledge in these parameter constraints can be in this case, let us examine “how far STBN is behind PDKBN”. [sent-401, score-0.335]
</p><p>49 We next provide experimental results on a very complex task involving several thousand random variables and prior knowledge in the form of parameter constraints across many conditional probability distributions. [sent-409, score-0.43]
</p><p>50 Typically, there are ten to ﬁfteen thousand voxels (three dimensional pixels) in each image, where each voxel covers a few tens of millimeters of brain tissue. [sent-412, score-0.586]
</p><p>51 This section presents a generative model of the activity in the brain while a human subject performs a cognitive task, based on the hidden process model and parameter sharing approach discussed in section 4. [sent-415, score-0.868]
</p><p>52 In this experiment involving real fMRI data and a complex cognitive task, domain experts were unable to provide parameter sharing assumptions in advance. [sent-417, score-0.528]
</p><p>53 Therefore, we 1372  BAYESIAN N ETWORK L EARNING WITH PARAMETER C ONSTRAINTS  have developed an algorithm to automatically discover clusters of voxels that can be more accurately learned with shared parameters. [sent-418, score-0.552]
</p><p>54 This section describes the algorithm for discovering these parameter sharing constraints, and shows that training under these parameter constraints leads to hidden process models that far outperform the baseline hidden process models learned in the absence of such parameter constraints. [sent-419, score-1.246]
</p><p>55 In this data set, the voxels were grouped into 24 anatomically deﬁned spatial regions of interest (ROIs), each voxel having a resolution of 3 by 3 by 5 millimeters. [sent-429, score-0.443]
</p><p>56 We model the activity in each voxel by a hidden process model with two processes, corresponding to the cognitive processes of comprehending a Sentence or a Picture. [sent-434, score-0.52]
</p><p>57 We further assume that the activity in different voxels is independent given the hidden processes corresponding to these voxels. [sent-438, score-0.593]
</p><p>58 The second model ShHPM is a hidden process model, shared for all the voxels within an ROI. [sent-444, score-0.646]
</p><p>59 In other words, all voxels in a speciﬁc ROI share the same shape hidden processes, but with different amplitudes (see Section 4. [sent-445, score-0.489]
</p><p>60 Start with a partition of all voxels in the brain by their ROIs and mark all subsets as Not Final. [sent-457, score-0.464]
</p><p>61 Given the partition computed by STEPS 3 and 4, based on the 38 data points in F \ Fk , learn a hidden process model that is shared for all voxels inside each subset of the partition. [sent-463, score-0.646]
</p><p>62 With an increase in the number of examples, the performance of ShHPM starts to degrade because it makes the biased assumption that all voxels in CALC can be described by a single shared hidden process model. [sent-476, score-0.646]
</p><p>63 The last column of Table 2 displays the number of clusters of voxels in which HieHPM partitioned CALC. [sent-487, score-0.363]
</p><p>64 This number of shared voxel sets tends to stabilize around 60 clusters once the number of examples reaches 30, which yields an av1375  N ICULESCU , M ITCHELL AND R AO  erage of more than 5 voxels per cluster given that CALC is made of 318 voxels. [sent-490, score-0.626]
</p><p>65 For a training set of 40 examples, the largest cluster has 41 voxels while many clusters consist of only one voxel. [sent-491, score-0.363]
</p><p>66 While we have seen that ShHPM was biased in CALC, we see here that there are several ROIs where it makes sense to characterize all of its voxels by a single shared hidden process model. [sent-494, score-0.646]
</p><p>67 The explanation is that the hierarchical approach can get stuck in a local maximum of the data log-likelihood over the search space if it cannot improve by splitting at 1376  BAYESIAN N ETWORK L EARNING WITH PARAMETER C ONSTRAINTS  Figure 3: Parameter sharing found using model HieHPM. [sent-498, score-0.342]
</p><p>68 As mentioned above, HieHPM automatically learns clusters of voxels that can be represented using a shared hidden process model. [sent-506, score-0.688]
</p><p>69 This may be because of the fact that it makes sense to represent an entire ROI using a single shared hidden process model if the cognitive process does not activate voxels in this ROI. [sent-510, score-0.772]
</p><p>70 In Figure 4 we can see the learned Sentence hidden process for the voxels in the visual cortex (CALC). [sent-512, score-0.638]
</p><p>71 2 for more details about shared 1377  N ICULESCU , M ITCHELL AND R AO  hidden process models). [sent-515, score-0.325]
</p><p>72 We demonstrated experimentally that parameter sharing for hidden Process models (as deﬁned in Section 4. [sent-518, score-0.571]
</p><p>73 2) can greatly beneﬁt learning, and that it is possible to automatically discover useful parameter sharing constraints in this domain using our hierarchical partitioning algorithm. [sent-519, score-0.654]
</p><p>74 Formal Guarantees Taking advantage of parameter constraints can be beneﬁcial to learning because, intuitively, it has the effect of lowering the variance in parameter estimators by shrinking the degrees of freedom of the model. [sent-521, score-0.49]
</p><p>75 In order for our proof to work, we make the assumption that the true distribution factors according to the given Bayesian network structure and that it obeys the parameter constraints provided by the expert. [sent-523, score-0.323]
</p><p>76 While we only investigate this issue for one type of constraint, parameter sharing within one distribution (introduced in subsection 4. [sent-525, score-0.447]
</p><p>77 1 Variance Reduction by Using Parameter Constraints Assume we want to learn a Bayesian network in the case when a domain expert provides parameter constraints specifying that certain parameters appear multiple times (are shared) within a conditional probability distribution. [sent-528, score-0.574]
</p><p>78 Also, the case when all parameters are distinct within one such distribution may be seen as a particular case of parameter sharing within one distribution, where each parameter is shared exactly once. [sent-530, score-0.693]
</p><p>79 First, one may choose to ignore the constraints given by the expert and compute standard maximum likelihood estimators. [sent-532, score-0.335]
</p><p>80 One would intuitively expect that taking advantage of the constraints provided by the expert would reduce the variance in parameter estimates when compared to the ﬁrst approach. [sent-535, score-0.374]
</p><p>81 2 Performance with Potentially Inaccurate Constraints Sometimes it may happen that the parameter constraints provided by an expert are not completely accurate. [sent-539, score-0.374]
</p><p>82 In all our methods so far, we assumed that the parameter constraints are correct and therefore errors in domain knowledge can prove detrimental to the performance of our learned models. [sent-540, score-0.391]
</p><p>83 Assume an expert provides a set of potentially incorrect parameter sharing assumptions as described in subsection 4. [sent-543, score-0.559]
</p><p>84 In other words, for each conditional probability distribution c in the Bayesian network, the expert is stating that parameter θic is shared in kic given positions. [sent-545, score-0.385]
</p><p>85 If, for example, the expert states that θic is the shared parameter that describes the set {P(X = x1 |PA(X) = pa), . [sent-551, score-0.35]
</p><p>86 Theorem 3 P∗ is the closest distribution to P (in terms of KL(P, ·)) that factorizes according to the given structure and obeys the expert’s parameter sharing assumptions. [sent-556, score-0.401]
</p><p>87 Corollary 5 If the true distribution P factorizes according to the given structure and if the paramˆ eter sharing provided by the expert is completely accurate, then the distribution P given by the estimators computed in Theorem 1 converges to P with probability 1. [sent-567, score-0.547]
</p><p>88 In this paper we have demonstrated both theoretically and experimentally that the standard methods for parameter estimation in Bayesian networks can be naturally extended to accommodate parameter constraints capable of expressing a wide variety of prior domain knowledge. [sent-572, score-0.482]
</p><p>89 We mentioned our previous work on methods for incorporating general parameter constraints into estimators for the parameters of a Bayesian network, by framing this task as a constrained optimization problem. [sent-573, score-0.51]
</p><p>90 We have presented parameter estimators for several types of constraints, including constraints that force various types of parameter sharing, and constraints on sums and other relationships among groups of parameters. [sent-576, score-0.759]
</p><p>91 While for most of these types of parameter constraints we can derive closed form maximum likelihood estimators, we developed a very efﬁcient iterative algorithm to perform the same task for shared hidden process models. [sent-580, score-0.779]
</p><p>92 The general parameter sharing domain knowledge type (Constraint Type 6 deﬁned in subsection 4. [sent-582, score-0.528]
</p><p>93 It is also important to note that one can combine different types of parameter constraints when learning the parameters of a Bayesian network as long as the scopes of these constraints do not overlap. [sent-584, score-0.579]
</p><p>94 Experimental results using an fMRI brain imaging application demonstrate that taking advantage of parameter constraints can be very beneﬁcial for learning in this high-dimensional, sparsedata domain. [sent-585, score-0.405]
</p><p>95 In the context of this application we developed methods to automatically discover parameter sharing constraints. [sent-586, score-0.401]
</p><p>96 Using these methods our program discovered clusters of voxels whose parameters can be shared. [sent-587, score-0.417]
</p><p>97 A basic theoretical result is that the estimators taking advantage of a simple form of parameter sharing achieve variance lower than that achieved by estimators that ignore such constraints. [sent-590, score-0.663]
</p><p>98 In this paper we have considered only how to take advantage of deterministic parameter constraints when the structure of the Bayesian network is known in advance. [sent-594, score-0.323]
</p><p>99 Dependent dirichlet priors and optimal linear estimators for belief net parameters. [sent-656, score-0.39]
</p><p>100 Exploiting parameter domain knowledge for learning in bayesian networks. [sent-711, score-0.368]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('voxels', 0.321), ('sharing', 0.304), ('hiehpm', 0.266), ('fmri', 0.194), ('bayesian', 0.19), ('sthpm', 0.188), ('constraints', 0.165), ('ao', 0.144), ('iculescu', 0.144), ('itchell', 0.144), ('brain', 0.143), ('niculescu', 0.141), ('shared', 0.141), ('hidden', 0.135), ('sentence', 0.135), ('shhpm', 0.133), ('stbn', 0.133), ('priors', 0.132), ('estimators', 0.131), ('dirichlet', 0.127), ('etwork', 0.122), ('onstraints', 0.122), ('calc', 0.122), ('voxel', 0.122), ('expert', 0.112), ('pdkbn', 0.111), ('parameter', 0.097), ('rois', 0.078), ('cognitive', 0.077), ('processes', 0.074), ('ki', 0.073), ('closed', 0.067), ('hutchinson', 0.066), ('activity', 0.063), ('network', 0.061), ('counts', 0.059), ('likelihood', 0.058), ('aposteriori', 0.055), ('cortex', 0.055), ('roi', 0.055), ('tnk', 0.055), ('parameters', 0.054), ('kl', 0.054), ('module', 0.054), ('picture', 0.054), ('heart', 0.051), ('cv', 0.05), ('domain', 0.05), ('process', 0.049), ('learned', 0.048), ('pk', 0.047), ('disease', 0.047), ('siemens', 0.047), ('xnt', 0.047), ('mitchell', 0.046), ('subsection', 0.046), ('attack', 0.045), ('ntv', 0.044), ('pkt', 0.044), ('constraint', 0.044), ('earning', 0.043), ('clusters', 0.042), ('prior', 0.042), ('normalization', 0.041), ('pa', 0.041), ('ni', 0.04), ('bilinear', 0.039), ('hierarchical', 0.038), ('types', 0.037), ('constants', 0.036), ('patient', 0.036), ('style', 0.036), ('models', 0.035), ('conditional', 0.035), ('lm', 0.034), ('tenenbaum', 0.034), ('hpm', 0.033), ('hpms', 0.033), ('intl', 0.033), ('msft', 0.033), ('multinetworks', 0.033), ('nouns', 0.033), ('pcic', 0.033), ('radu', 0.033), ('constrained', 0.033), ('aggregate', 0.033), ('share', 0.033), ('synthetic', 0.033), ('fk', 0.032), ('networks', 0.031), ('knowledge', 0.031), ('ic', 0.031), ('stefan', 0.031), ('maker', 0.031), ('across', 0.03), ('folds', 0.03), ('task', 0.03), ('visual', 0.03), ('groups', 0.03), ('languages', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="15-tfidf-1" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Radu Stefan Niculescu, Tom M. Mitchell, R. Bharat Rao</p><p>Abstract: The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. This paper considers a variety of types of domain knowledge for constraining parameter estimates when learning Bayesian networks. In particular, we consider domain knowledge that constrains the values or relationships among subsets of parameters in a Bayesian network with known structure. We incorporate a wide variety of parameter constraints into learning procedures for Bayesian networks, by formulating this task as a constrained optimization problem. The assumptions made in module networks, dynamic Bayes nets and context speciﬁc independence models can be viewed as particular cases of such parameter constraints. We present closed form solutions or fast iterative algorithms for estimating parameters subject to several speciﬁc classes of parameter constraints, including equalities and inequalities among parameters, constraints on individual parameters, and constraints on sums and ratios of parameters, for discrete and continuous variables. Our methods cover learning from both frequentist and Bayesian points of view, from both complete and incomplete data. We present formal guarantees for our estimators, as well as methods for automatically learning useful parameter constraints from data. To validate our approach, we apply it to the domain of fMRI brain image analysis. Here we demonstrate the ability of our system to ﬁrst learn useful relationships among parameters, and then to use them to constrain the training of the Bayesian network, resulting in improved cross-validated accuracy of the learned model. Experiments on synthetic data are also presented. Keywords: Bayesian networks, constrained optimization, domain knowledge c 2006 Radu Stefan Niculescu, Tom Mitchell and Bharat Rao. N ICULESCU , M ITCHELL AND R AO</p><p>2 0.085369647 <a title="15-tfidf-2" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>Author: Kazuho Watanabe, Sumio Watanabe</p><p>Abstract: Bayesian learning has been widely used and proved to be effective in many data modeling problems. However, computations involved in it require huge costs and generally cannot be performed exactly. The variational Bayesian approach, proposed as an approximation of Bayesian learning, has provided computational tractability and good generalization performance in many applications. The properties and capabilities of variational Bayesian learning itself have not been clariﬁed yet. It is still unknown how good approximation the variational Bayesian approach can achieve. In this paper, we discuss variational Bayesian learning of Gaussian mixture models and derive upper and lower bounds of variational stochastic complexities. The variational stochastic complexity, which corresponds to the minimum variational free energy and a lower bound of the Bayesian evidence, not only becomes important in addressing the model selection problem, but also enables us to discuss the accuracy of the variational Bayesian approach as an approximation of true Bayesian learning. Keywords: Gaussian mixture model, variational Bayesian learning, stochastic complexity</p><p>3 0.080379687 <a title="15-tfidf-3" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>4 0.066447377 <a title="15-tfidf-4" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>Author: Luis M. de Campos</p><p>Abstract: We propose a new scoring function for learning Bayesian networks from data using score+search algorithms. This is based on the concept of mutual information and exploits some well-known properties of this measure in a novel way. Essentially, a statistical independence test based on the chi-square distribution, associated with the mutual information measure, together with a property of additive decomposition of this measure, are combined in order to measure the degree of interaction between each variable and its parent variables in the network. The result is a non-Bayesian scoring function called MIT (mutual information tests) which belongs to the family of scores based on information theory. The MIT score also represents a penalization of the Kullback-Leibler divergence between the joint probability distributions associated with a candidate network and with the available data set. Detailed results of a complete experimental evaluation of the proposed scoring function and its comparison with the well-known K2, BDeu and BIC/MDL scores are also presented. Keywords: Bayesian networks, scoring functions, learning, mutual information, conditional independence tests</p><p>5 0.054451175 <a title="15-tfidf-5" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Ben Taskar, Simon Lacoste-Julien, Michael I. Jordan</p><p>Abstract: We present a simple and scalable algorithm for maximum-margin estimation of structured output models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). The projection step can be solved using dynamic programming or combinatorial algorithms for min-cost convex ﬂow, depending on the structure of the problem. We show that this approach provides a memoryefﬁcient alternative to formulations based on reductions to a quadratic program (QP). We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.1 Keywords: Markov networks, large-margin methods, structured prediction, extragradient, Bregman projections</p><p>6 0.053862408 <a title="15-tfidf-6" href="./jmlr-2006-Noisy-OR_Component_Analysis_and_its_Application_to_Link_Analysis.html">64 jmlr-2006-Noisy-OR Component Analysis and its Application to Link Analysis</a></p>
<p>7 0.047897961 <a title="15-tfidf-7" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>8 0.042370327 <a title="15-tfidf-8" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<p>9 0.04220248 <a title="15-tfidf-9" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>10 0.042191599 <a title="15-tfidf-10" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>11 0.042002801 <a title="15-tfidf-11" href="./jmlr-2006-Linear_State-Space_Models_for_Blind_Source_Separation.html">57 jmlr-2006-Linear State-Space Models for Blind Source Separation</a></p>
<p>12 0.038151342 <a title="15-tfidf-12" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.037414167 <a title="15-tfidf-13" href="./jmlr-2006-MinReg%3A_A_Scalable_Algorithm_for_Learning_Parsimonious_Regulatory_Networks_in_Yeast_and_Mammals.html">62 jmlr-2006-MinReg: A Scalable Algorithm for Learning Parsimonious Regulatory Networks in Yeast and Mammals</a></p>
<p>14 0.037331309 <a title="15-tfidf-14" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>15 0.03672621 <a title="15-tfidf-15" href="./jmlr-2006-Learning_the_Structure_of_Linear_Latent_Variable_Models.html">54 jmlr-2006-Learning the Structure of Linear Latent Variable Models</a></p>
<p>16 0.036302645 <a title="15-tfidf-16" href="./jmlr-2006-Superior_Guarantees_for_Sequential_Prediction_and_Lossless_Compression_via_Alphabet_Decomposition.html">90 jmlr-2006-Superior Guarantees for Sequential Prediction and Lossless Compression via Alphabet Decomposition</a></p>
<p>17 0.03600087 <a title="15-tfidf-17" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.035639293 <a title="15-tfidf-18" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>19 0.035253301 <a title="15-tfidf-19" href="./jmlr-2006-Learning_Image_Components_for_Object_Recognition.html">47 jmlr-2006-Learning Image Components for Object Recognition</a></p>
<p>20 0.034943447 <a title="15-tfidf-20" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.2), (1, -0.036), (2, -0.095), (3, 0.081), (4, -0.136), (5, -0.036), (6, -0.081), (7, -0.028), (8, 0.13), (9, -0.019), (10, 0.03), (11, 0.086), (12, -0.046), (13, 0.04), (14, -0.011), (15, 0.085), (16, -0.216), (17, -0.089), (18, -0.018), (19, -0.031), (20, -0.005), (21, -0.034), (22, 0.159), (23, -0.11), (24, 0.148), (25, -0.045), (26, -0.014), (27, -0.025), (28, 0.132), (29, 0.057), (30, -0.083), (31, -0.092), (32, -0.123), (33, -0.159), (34, 0.075), (35, 0.074), (36, -0.011), (37, -0.085), (38, -0.021), (39, 0.052), (40, -0.024), (41, -0.046), (42, 0.083), (43, -0.094), (44, 0.063), (45, -0.094), (46, -0.212), (47, 0.115), (48, -0.11), (49, -0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95808136 <a title="15-lsi-1" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Radu Stefan Niculescu, Tom M. Mitchell, R. Bharat Rao</p><p>Abstract: The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. This paper considers a variety of types of domain knowledge for constraining parameter estimates when learning Bayesian networks. In particular, we consider domain knowledge that constrains the values or relationships among subsets of parameters in a Bayesian network with known structure. We incorporate a wide variety of parameter constraints into learning procedures for Bayesian networks, by formulating this task as a constrained optimization problem. The assumptions made in module networks, dynamic Bayes nets and context speciﬁc independence models can be viewed as particular cases of such parameter constraints. We present closed form solutions or fast iterative algorithms for estimating parameters subject to several speciﬁc classes of parameter constraints, including equalities and inequalities among parameters, constraints on individual parameters, and constraints on sums and ratios of parameters, for discrete and continuous variables. Our methods cover learning from both frequentist and Bayesian points of view, from both complete and incomplete data. We present formal guarantees for our estimators, as well as methods for automatically learning useful parameter constraints from data. To validate our approach, we apply it to the domain of fMRI brain image analysis. Here we demonstrate the ability of our system to ﬁrst learn useful relationships among parameters, and then to use them to constrain the training of the Bayesian network, resulting in improved cross-validated accuracy of the learned model. Experiments on synthetic data are also presented. Keywords: Bayesian networks, constrained optimization, domain knowledge c 2006 Radu Stefan Niculescu, Tom Mitchell and Bharat Rao. N ICULESCU , M ITCHELL AND R AO</p><p>2 0.52254319 <a title="15-lsi-2" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>Author: Kazuho Watanabe, Sumio Watanabe</p><p>Abstract: Bayesian learning has been widely used and proved to be effective in many data modeling problems. However, computations involved in it require huge costs and generally cannot be performed exactly. The variational Bayesian approach, proposed as an approximation of Bayesian learning, has provided computational tractability and good generalization performance in many applications. The properties and capabilities of variational Bayesian learning itself have not been clariﬁed yet. It is still unknown how good approximation the variational Bayesian approach can achieve. In this paper, we discuss variational Bayesian learning of Gaussian mixture models and derive upper and lower bounds of variational stochastic complexities. The variational stochastic complexity, which corresponds to the minimum variational free energy and a lower bound of the Bayesian evidence, not only becomes important in addressing the model selection problem, but also enables us to discuss the accuracy of the variational Bayesian approach as an approximation of true Bayesian learning. Keywords: Gaussian mixture model, variational Bayesian learning, stochastic complexity</p><p>3 0.46317482 <a title="15-lsi-3" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>Author: Luis M. de Campos</p><p>Abstract: We propose a new scoring function for learning Bayesian networks from data using score+search algorithms. This is based on the concept of mutual information and exploits some well-known properties of this measure in a novel way. Essentially, a statistical independence test based on the chi-square distribution, associated with the mutual information measure, together with a property of additive decomposition of this measure, are combined in order to measure the degree of interaction between each variable and its parent variables in the network. The result is a non-Bayesian scoring function called MIT (mutual information tests) which belongs to the family of scores based on information theory. The MIT score also represents a penalization of the Kullback-Leibler divergence between the joint probability distributions associated with a candidate network and with the available data set. Detailed results of a complete experimental evaluation of the proposed scoring function and its comparison with the well-known K2, BDeu and BIC/MDL scores are also presented. Keywords: Bayesian networks, scoring functions, learning, mutual information, conditional independence tests</p><p>4 0.45015031 <a title="15-lsi-4" href="./jmlr-2006-MinReg%3A_A_Scalable_Algorithm_for_Learning_Parsimonious_Regulatory_Networks_in_Yeast_and_Mammals.html">62 jmlr-2006-MinReg: A Scalable Algorithm for Learning Parsimonious Regulatory Networks in Yeast and Mammals</a></p>
<p>Author: Dana Pe'er, Amos Tanay, Aviv Regev</p><p>Abstract: In recent years, there has been a growing interest in applying Bayesian networks and their extensions to reconstruct regulatory networks from gene expression data. Since the gene expression domain involves a large number of variables and a limited number of samples, it poses both computational and statistical challenges to Bayesian network learning algorithms. Here we deﬁne a constrained family of Bayesian network structures suitable for this domain and devise an efﬁcient search algorithm that utilizes these structural constraints to ﬁnd high scoring networks from data. Interestingly, under reasonable assumptions on the underlying probability distribution, we can provide performance guarantees on our algorithm. Evaluation on real data from yeast and mouse, demonstrates that our method cannot only reconstruct a high quality model of the yeast regulatory network, but is also the ﬁrst method to scale to the complexity of mammalian networks and successfully reconstructs a reasonable model over thousands of variables. Keywords: Bayesian networks, structure learning, gene networks, gene expression, approximation algorithms</p><p>5 0.36311528 <a title="15-lsi-5" href="./jmlr-2006-Noisy-OR_Component_Analysis_and_its_Application_to_Link_Analysis.html">64 jmlr-2006-Noisy-OR Component Analysis and its Application to Link Analysis</a></p>
<p>Author: Tomáš Šingliar, Miloš Hauskrecht</p><p>Abstract: We develop a new component analysis framework, the Noisy-Or Component Analyzer (NOCA), that targets high-dimensional binary data. NOCA is a probabilistic latent variable model that assumes the expression of observed high-dimensional binary data is driven by a small number of hidden binary sources combined via noisy-or units. The component analysis procedure is equivalent to learning of NOCA parameters. Since the classical EM formulation of the NOCA learning problem is intractable, we develop its variational approximation. We test the NOCA framework on two problems: (1) a synthetic image-decomposition problem and (2) a co-citation data analysis problem for thousands of CiteSeer documents. We demonstrate good performance of the new model on both problems. In addition, we contrast the model to two mixture-based latent-factor models: the probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA). Differing assumptions underlying these models cause them to discover different types of structure in co-citation data, thus illustrating the beneﬁt of NOCA in building our understanding of highdimensional data sets. Keywords: component analysis, vector quantization, variational learning, link analysis</p><p>6 0.35606661 <a title="15-lsi-6" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>7 0.30692339 <a title="15-lsi-7" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.30619341 <a title="15-lsi-8" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.30097038 <a title="15-lsi-9" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.29528785 <a title="15-lsi-10" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>11 0.28067541 <a title="15-lsi-11" href="./jmlr-2006-Stability_Properties_of_Empirical_Risk_Minimization_over_Donsker_Classes.html">84 jmlr-2006-Stability Properties of Empirical Risk Minimization over Donsker Classes</a></p>
<p>12 0.27250814 <a title="15-lsi-12" href="./jmlr-2006-Segmental_Hidden_Markov_Models_with_Random_Effects_for_Waveform_Modeling.html">80 jmlr-2006-Segmental Hidden Markov Models with Random Effects for Waveform Modeling</a></p>
<p>13 0.26763156 <a title="15-lsi-13" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>14 0.25702599 <a title="15-lsi-14" href="./jmlr-2006-Evolutionary_Function_Approximation_for_Reinforcement_Learning.html">30 jmlr-2006-Evolutionary Function Approximation for Reinforcement Learning</a></p>
<p>15 0.2470642 <a title="15-lsi-15" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>16 0.2425984 <a title="15-lsi-16" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.23927394 <a title="15-lsi-17" href="./jmlr-2006-A_Simulation-Based_Algorithm_for_Ergodic_Control_of_Markov_Chains_Conditioned_on_Rare_Events.html">7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</a></p>
<p>18 0.23667336 <a title="15-lsi-18" href="./jmlr-2006-Superior_Guarantees_for_Sequential_Prediction_and_Lossless_Compression_via_Alphabet_Decomposition.html">90 jmlr-2006-Superior Guarantees for Sequential Prediction and Lossless Compression via Alphabet Decomposition</a></p>
<p>19 0.23293488 <a title="15-lsi-19" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.22484131 <a title="15-lsi-20" href="./jmlr-2006-Linear_State-Space_Models_for_Blind_Source_Separation.html">57 jmlr-2006-Linear State-Space Models for Blind Source Separation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.023), (36, 0.06), (45, 0.018), (50, 0.064), (63, 0.031), (68, 0.51), (76, 0.011), (78, 0.014), (79, 0.012), (81, 0.035), (84, 0.014), (90, 0.016), (91, 0.025), (96, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88047588 <a title="15-lda-1" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Radu Stefan Niculescu, Tom M. Mitchell, R. Bharat Rao</p><p>Abstract: The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. This paper considers a variety of types of domain knowledge for constraining parameter estimates when learning Bayesian networks. In particular, we consider domain knowledge that constrains the values or relationships among subsets of parameters in a Bayesian network with known structure. We incorporate a wide variety of parameter constraints into learning procedures for Bayesian networks, by formulating this task as a constrained optimization problem. The assumptions made in module networks, dynamic Bayes nets and context speciﬁc independence models can be viewed as particular cases of such parameter constraints. We present closed form solutions or fast iterative algorithms for estimating parameters subject to several speciﬁc classes of parameter constraints, including equalities and inequalities among parameters, constraints on individual parameters, and constraints on sums and ratios of parameters, for discrete and continuous variables. Our methods cover learning from both frequentist and Bayesian points of view, from both complete and incomplete data. We present formal guarantees for our estimators, as well as methods for automatically learning useful parameter constraints from data. To validate our approach, we apply it to the domain of fMRI brain image analysis. Here we demonstrate the ability of our system to ﬁrst learn useful relationships among parameters, and then to use them to constrain the training of the Bayesian network, resulting in improved cross-validated accuracy of the learned model. Experiments on synthetic data are also presented. Keywords: Bayesian networks, constrained optimization, domain knowledge c 2006 Radu Stefan Niculescu, Tom Mitchell and Bharat Rao. N ICULESCU , M ITCHELL AND R AO</p><p>2 0.35052195 <a title="15-lda-2" href="./jmlr-2006-Noisy-OR_Component_Analysis_and_its_Application_to_Link_Analysis.html">64 jmlr-2006-Noisy-OR Component Analysis and its Application to Link Analysis</a></p>
<p>Author: Tomáš Šingliar, Miloš Hauskrecht</p><p>Abstract: We develop a new component analysis framework, the Noisy-Or Component Analyzer (NOCA), that targets high-dimensional binary data. NOCA is a probabilistic latent variable model that assumes the expression of observed high-dimensional binary data is driven by a small number of hidden binary sources combined via noisy-or units. The component analysis procedure is equivalent to learning of NOCA parameters. Since the classical EM formulation of the NOCA learning problem is intractable, we develop its variational approximation. We test the NOCA framework on two problems: (1) a synthetic image-decomposition problem and (2) a co-citation data analysis problem for thousands of CiteSeer documents. We demonstrate good performance of the new model on both problems. In addition, we contrast the model to two mixture-based latent-factor models: the probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA). Differing assumptions underlying these models cause them to discover different types of structure in co-citation data, thus illustrating the beneﬁt of NOCA in building our understanding of highdimensional data sets. Keywords: component analysis, vector quantization, variational learning, link analysis</p><p>3 0.3451269 <a title="15-lda-3" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>Author: Pieter Abbeel, Daphne Koller, Andrew Y. Ng</p><p>Abstract: We study the computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Importantly, unlike standard maximum likelihood estimation algorithms, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. In addition to our main result, we show that the sample complexity of parameter learning in graphical models has an O(1) dependence on the number of variables in the model when using the KL-divergence normalized by the number of variables as the performance criterion.1 Keywords: probabilistic graphical models, parameter and structure learning, factor graphs, Markov networks, Bayesian networks</p><p>4 0.31946892 <a title="15-lda-4" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>Author: Kazuho Watanabe, Sumio Watanabe</p><p>Abstract: Bayesian learning has been widely used and proved to be effective in many data modeling problems. However, computations involved in it require huge costs and generally cannot be performed exactly. The variational Bayesian approach, proposed as an approximation of Bayesian learning, has provided computational tractability and good generalization performance in many applications. The properties and capabilities of variational Bayesian learning itself have not been clariﬁed yet. It is still unknown how good approximation the variational Bayesian approach can achieve. In this paper, we discuss variational Bayesian learning of Gaussian mixture models and derive upper and lower bounds of variational stochastic complexities. The variational stochastic complexity, which corresponds to the minimum variational free energy and a lower bound of the Bayesian evidence, not only becomes important in addressing the model selection problem, but also enables us to discuss the accuracy of the variational Bayesian approach as an approximation of true Bayesian learning. Keywords: Gaussian mixture model, variational Bayesian learning, stochastic complexity</p><p>5 0.31150103 <a title="15-lda-5" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>Author: Martin J. Wainwright</p><p>Abstract: Consider the problem of joint parameter estimation and prediction in a Markov random ﬁeld: that is, the model parameters are estimated on the basis of an initial set of data, and then the ﬁtted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working under the restriction of limited computation, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for ﬁtting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the “wrong” model even in the inﬁnite data limit) is provably beneﬁcial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of convex variational methods. This stability result provides additional incentive, apart from the obvious beneﬁt of unique global optima, for using message-passing methods based on convex variational relaxations. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. Keywords: graphical model, Markov random ﬁeld, belief propagation, variational method, parameter estimation, prediction error, algorithmic stability</p><p>6 0.30015421 <a title="15-lda-6" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>7 0.29384351 <a title="15-lda-7" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>8 0.29326665 <a title="15-lda-8" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.28892338 <a title="15-lda-9" href="./jmlr-2006-Linear_State-Space_Models_for_Blind_Source_Separation.html">57 jmlr-2006-Linear State-Space Models for Blind Source Separation</a></p>
<p>10 0.28790021 <a title="15-lda-10" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<p>11 0.28758994 <a title="15-lda-11" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.28398961 <a title="15-lda-12" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>13 0.28234598 <a title="15-lda-13" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>14 0.28233045 <a title="15-lda-14" href="./jmlr-2006-MinReg%3A_A_Scalable_Algorithm_for_Learning_Parsimonious_Regulatory_Networks_in_Yeast_and_Mammals.html">62 jmlr-2006-MinReg: A Scalable Algorithm for Learning Parsimonious Regulatory Networks in Yeast and Mammals</a></p>
<p>15 0.27928531 <a title="15-lda-15" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>16 0.27732727 <a title="15-lda-16" href="./jmlr-2006-Point-Based_Value_Iteration_for_Continuous_POMDPs.html">74 jmlr-2006-Point-Based Value Iteration for Continuous POMDPs</a></p>
<p>17 0.27538985 <a title="15-lda-17" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.27494228 <a title="15-lda-18" href="./jmlr-2006-Rearrangement_Clustering%3A_Pitfalls%2C_Remedies%2C_and_Applications.html">78 jmlr-2006-Rearrangement Clustering: Pitfalls, Remedies, and Applications</a></p>
<p>19 0.27489227 <a title="15-lda-19" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>20 0.27462912 <a title="15-lda-20" href="./jmlr-2006-Worst-Case_Analysis_of_Selective_Sampling_for_Linear_Classification.html">96 jmlr-2006-Worst-Case Analysis of Selective Sampling for Linear Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
