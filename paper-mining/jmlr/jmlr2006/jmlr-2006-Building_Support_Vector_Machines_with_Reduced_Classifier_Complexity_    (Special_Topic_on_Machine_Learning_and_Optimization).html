<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-18" href="#">jmlr2006-18</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-18-pdf" href="http://jmlr.org/papers/volume7/keerthi06a/keerthi06a.pdf">pdf</a></p><p>Author: S. Sathiya Keerthi, Olivier Chapelle, Dennis DeCoste</p><p>Abstract: Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classiﬁcation speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily ﬁnds a set of kernel basis functions of a speciﬁed maximum size (dmax ) to approximate the SVM primal cost function well; (3) it is 2 efﬁcient and roughly scales as O(ndmax ) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors. Keywords: SVMs, classiﬁcation, sparse design</p><p>Reference: <a title="jmlr-2006-18-reference" href="../jmlr2006_reference/jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This is due to the fact that a large set of basis functions is usually needed to form the SVM classiﬁer, making it complex and expensive. [sent-14, score-0.31]
</p><p>2 Our method incrementally ﬁnds basis functions to maximize accuracy. [sent-16, score-0.332]
</p><p>3 The process of adding new basis functions can be stopped when the classiﬁer has reached some limiting level of complexity. [sent-17, score-0.31]
</p><p>4 In many cases, our method efﬁciently forms classiﬁers which have an order of magnitude smaller number of basis functions compared to the full SVM, while achieving nearly the same level of accuracy. [sent-18, score-0.332]
</p><p>5 Direct simpliﬁcation via basis functions and primal Instead of ﬁnding the SVM solution by maximizing the dual problem, one approach is to directly minimize the primal form after invoking the representer theorem to represent w as n  w = ∑ βi φ(xi ). [sent-42, score-0.524]
</p><p>6 For many classiﬁcation problems there exists a small subset of the basis functions1 suited to the complexity of the problem being solved, irrespective of the training size growth, that will yield pretty much the same accuracy as the SVM classiﬁer. [sent-45, score-0.313]
</p><p>7 These recent non-SVM works have laid the claim that they can match the accuracy of SVMs, while also bringing down considerably, the number of basis functions as well as the training cost. [sent-48, score-0.359]
</p><p>8 Each k(x, xi ) will be referred to as a basis function. [sent-50, score-0.264]
</p><p>9 We deliberately use the variable name, βi in (2) so as to interpret it as a basis weight as opposed to viewing it as yi αi where αi is the Lagrange multiplier associated with the i-th primal slack constraint. [sent-53, score-0.399]
</p><p>10 There is a lot of freedom when we simply think of the βi ’s as basis weights that yield a good suboptimal w for (1). [sent-55, score-0.264]
</p><p>11 Going even one more step further, we do not even have to restrict the basis functions to be a subset of the training set examples. [sent-58, score-0.359]
</p><p>12 2 Because basis functions are ˜ chosen randomly, this method requires many more basis functions than needed in order to achieve a level of accuracy close to the full SVM solution; see Section 3. [sent-66, score-0.642]
</p><p>13 Greedy methods of basis selection also exist in the boosting literature (Friedman, 2001; R¨ tsch, 2001). [sent-70, score-0.338]
</p><p>14 These a methods entail selection from a continuum of basis functions using either gradient descent or linear programming column generation. [sent-71, score-0.358]
</p><p>15 (2004) give modiﬁed ideas for kernel methods that employ a set of basis functions ﬁxed at the training points. [sent-74, score-0.466]
</p><p>16 GSVC is an efﬁcient method that is developed for SVMs and uses a heuristic criterion for greedy selection of basis functions. [sent-78, score-0.372]
</p><p>17 Our approach The main aim of this paper is to give an effective greedy method SVMs which uses a basis selection criterion that is directly related to the training cost function and is also very efﬁcient. [sent-79, score-0.486]
</p><p>18 It starts with an empty set of basis functions and greedily chooses new basis functions (from the training set) to improve the primal objective function. [sent-81, score-0.817]
</p><p>19 We develop efﬁcient schemes for both, the greedy selection of a new basis function, as well as the optimization of the βi for a given selection of basis functions. [sent-82, score-0.662]
</p><p>20 For choosing 2 upto dmax basis functions, the overall compuational cost of our method is O(ndmax ). [sent-83, score-0.526]
</p><p>21 As can be seen there, our method gives a competing generalization performance while reducing the number of basis functions very signiﬁcantly. [sent-180, score-0.332]
</p><p>22 The key issue of selecting basis functions is taken up in Section 3. [sent-184, score-0.31]
</p><p>23 , n} be a given index set of basis functions that form a subset of the training set. [sent-192, score-0.359]
</p><p>24 With w restricted to (3), the primal problem (1) becomes the d dimensional minimization problem of ﬁnding βJ that solves 1 n λ min f (βJ ) = β⊤ KJJ βJ + ∑ max(0, 1 − yi oi )2 2 J 2 i=1 βJ  (4)  where oi = Ki,J βJ . [sent-201, score-0.341]
</p><p>25 2 Updating the Hessian As already pointed out in Section 1, we will mainly need to solve (4) in an incremental mode:4 with the solution βJ of (4) already available, solve (4) again, but with one more basis function added, i. [sent-227, score-0.313]
</p><p>26 In our method basis functions are added one at a time. [sent-231, score-0.332]
</p><p>27 3 Computational Complexity It is useful to ask: what is the complexity of the incremental computations needed to solve (4) when its solution is available for some J, at which point one more basis element is included in it and we want to re-solve (4)? [sent-240, score-0.29]
</p><p>28 Adding up these costs till dmax basis functions are selected, we get a complexity 2 of O(ndmax ). [sent-244, score-0.505]
</p><p>29 Note that this is the basic cost given that we already know the sequence of dmax basis 2 functions that are to be used. [sent-245, score-0.55]
</p><p>30 Thus, O(ndmax ) is also the complexity of the method in which basis functions are chosen randomly. [sent-246, score-0.332]
</p><p>31 In the next section we discuss the problem of selecting the basis functions systematically and efﬁciently. [sent-247, score-0.31]
</p><p>32 If the difference between them is large we would like to continue on and include another basis function. [sent-251, score-0.264]
</p><p>33 We already analyzed in the earlier section that the cost of doing one basis element inclusion is O(nd). [sent-258, score-0.329]
</p><p>34 So, if we want to try all elements outside J, the cost is O(n2 d); the overall cost of such a method of selecting dmax basis functions is 2 2 O(n2 dmax ), which is much higher than the basic cost, O(ndmax ) mentioned in the previous section. [sent-259, score-0.812]
</p><p>35 Instead, if we work only with a random subset of size κ chosen from outside J, then the cost in one 2 basis selection step comes down to O(κnd), and the overall cost is limited to O(κndmax ). [sent-260, score-0.442]
</p><p>36 However, note that, even with this scheme, the cost of new basis selection (O(κnd)) 5. [sent-262, score-0.377]
</p><p>37 1498  B UILDING SVM S WITH R EDUCED C OMPLEXITY  is still disproportionately higher (by κ times) than the cost of actually including the newly selected basis function (O(nd)). [sent-265, score-0.356]
</p><p>38 If all j ∈ J are tried, then the complexity of selecting a new basis function is O(n2 ), which is disproportionately large compared to the cost of including the chosen basis function, which is O(nd). [sent-280, score-0.62]
</p><p>39 Like in Basis Selection Method 1, we can simply choose κ random basis functions to try. [sent-281, score-0.31]
</p><p>40 It is also possible to think of variations of the method in which full kernel rows corresponding to a large set (as much that can ﬁt into memory) of randomly chosen training basis is pre-computed and only these basis functions are considered for selection. [sent-289, score-0.72]
</p><p>41 4 Shrinking As basis functions get added, the SVM solution w and the margin planes start stabilizing. [sent-291, score-0.336]
</p><p>42 5 Experimental Evaluation We now evaluate the performance of basis selection methods 1 and 2 (we will call them as SpSVM-1, SpSVM-2) on some sizable benchmark data sets. [sent-299, score-0.312]
</p><p>43 To have a baseline, we also consider the method, Random in which the basis functions are chosen randomly. [sent-302, score-0.31]
</p><p>44 For another baseline we consider J the (more systematic) unsupervised learning method in which an incomplete Cholesky factorization with pivoting (Meijerink and van der Vorst, 1977; Bach and Jordan, 2005) is used to choose basis functions. [sent-304, score-0.32]
</p><p>45 This criterion is based on an upper bound on the improvement to the training cost function obtained by including the j-th basis function. [sent-310, score-0.378]
</p><p>46 It also makes sense intuitively as it selects basis functions that are both not well approximated by the others (large d j ) and for which the error incurred is large. [sent-311, score-0.341]
</p><p>47 9 Overall, SpSVM-1 and SpSVM-2 give the best performance in terms of achieving good reduction of test error rate with respect to the number of basis functions. [sent-315, score-0.402]
</p><p>48 Although SpSVM-2 slightly lags SpSVM-1 in terms of performance in the early stages, it does equally well as more basis functions are added. [sent-316, score-0.31]
</p><p>49 Note that when the set of basis functions is not restricted, the optimal β satisﬁes λβi yi = max(0, 1 − yi oi ). [sent-323, score-0.495]
</p><p>50 One plot gives test error rate as a function of the number of basis functions, to see how effective the compression is. [sent-326, score-0.402]
</p><p>51 4 1 10  3  10  IJCNN  3  10  6 Test error rate (%)  7  6 Test error rate (%)  2  10 CPU time (secs)  IJCNN  7  5 4 3 2 1  SpSVM−2 SpSVM−1 Cholesky Random GSVC BH  5 4 3 2  2  10 Num of basis functions  1 2 10  3  10  Shuttle  3  4  10 CPU time (secs)  10  Shuttle 0. [sent-336, score-0.48]
</p><p>52 04 1 10  2  10 Num of basis functions  2  10 CPU time (secs)  3  10  Figure 1: Comparison of basis selection methods on Adult, IJCNN & Shuttle. [sent-356, score-0.622]
</p><p>53 1501  K EERTHI , C HAPELLE AND D E C OSTE  M3V8  M3V8 SpSVM−2 SpSVM−1 Cholesky Random GSVC BH  6 Test error rate (%)  7  6 Test error rate (%)  7  5 4 3 2  5 4 3 2  1  2  10 Num of basis functions  1  3  2  10  3  10  M3VOthers 3. [sent-358, score-0.48]
</p><p>54 5 2  10 Num of basis functions  3  2  10  3  10  10 CPU time (secs)  Vehicle  Vehicle  15. [sent-366, score-0.31]
</p><p>55 5  2  10  3  10 CPU time (secs)  Figure 2: Comparison of basis selection methods on M3V8, M3VOthers & Vehicle. [sent-376, score-0.312]
</p><p>56 As we mentioned in Section 1, there also exist other greedy methods of kernel basis selection that are motivated by ideas from boosting. [sent-380, score-0.457]
</p><p>57 These methods are usually given in a setting different from that we consider: a set of (kernel) basis functions is given and a regularizer (such as β 1 ) is directly speciﬁed on the multiplier vector β. [sent-381, score-0.379]
</p><p>58 (2004) uses column generation ideas from linear and quadratic programming to select new basis functions and so it requires the solution of, both, the primal and dual problems. [sent-387, score-0.462]
</p><p>59 10 Thus, the basis selection process is based on the sensitivity of the primal objective function to an incoming basis function. [sent-388, score-0.691]
</p><p>60 On the other hand, our SpSVM methods are based on computing an estimate of the decrease in the primal objective function due to an incoming basis function; also, the dual solution is not needed. [sent-389, score-0.405]
</p><p>61 Cross validation (CV) can also be used to choose d, the number of basis functions. [sent-393, score-0.264]
</p><p>62 For a given choice of hyperparameters, the basis selection method (say, SpSVM-2) is then applied on each training set formed from the k-fold partitions till dmax basis functions are chosen. [sent-399, score-0.888]
</p><p>63 We choose d to be the number of basis functions that gives the lowest k-fold CV error. [sent-401, score-0.31]
</p><p>64 Recall that, at stage d, our basis selection methods choose the (d + 1)-th basis function from a set of κ random basis functions. [sent-403, score-0.84]
</p><p>65 Thus, at stage d, the basis selection methods will choose the same set of κ random basis functions for all hyperparameter values. [sent-405, score-0.666]
</p><p>66 We applied the above ideas on 11 benchmark data sets from (R¨ tsch) using SpSVM-2 as the a basis selection method. [sent-406, score-0.344]
</p><p>67 Clearly, our method achieves an impressive reduction in the number of basis functions, while yielding test error rates comparable to the full SVM. [sent-417, score-0.37]
</p><p>68 Comparison with Kernel Matching Pursuit Kernel matching pursuit (KMP) (Vincent and Bengio, 2002) was mainly given as a method of greedily selecting basis functions for the non-regularized kernel least squares problem. [sent-419, score-0.608]
</p><p>69 As we already explained in Section 3, our basis selection methods can be viewed as extensions of the basic ideas of KMP to the SVM case. [sent-420, score-0.344]
</p><p>70 In the initial stages of basis addition we are in the underﬁtting zone and so they perform as well (in fact, a shade better) than their respective regularized counterparts. [sent-434, score-0.291]
</p><p>71 But, as expected, they start overﬁtting when many basis functions are added. [sent-435, score-0.31]
</p><p>72 The number of basis functions at which overﬁtting sets-in is smaller for SpSVM-NR than that of KMP-NR. [sent-438, score-0.31]
</p><p>73 In this method, a new basis function (say, the one corresponding to the j-th training example) is evaluated by looking at the magnitude (larger the better) of the gradient of the primal objective function with respect to β j evaluated at the current βJ . [sent-442, score-0.428]
</p><p>74 The basis selection idea used in MARK is also given in the earlier papers, Mallat and Zhang (1993) and Adler et al. [sent-448, score-0.312]
</p><p>75 4 1 10  3  10  2  IJCNN  SpSVM−R SpSVM−NR KMP−R KMP−NR  7 Test error rate (%)  8  7 Test error rate (%)  10  IJCNN  8  6 5 4  6 5 4  3  3  2  2  1  2  1  3  10 Num of basis functions  2  10  10  Shuttle  3  4  10 CPU time (secs)  10  Shuttle  0. [sent-460, score-0.48]
</p><p>76 05 0  10  1  2  1  10 10 Num of basis functions  10  2  10 CPU time (secs)  Figure 3: KMP vs SpSVM (with/without regularization) on Adult, IJCNN & Shuttle. [sent-472, score-0.31]
</p><p>77 1505  3  10  K EERTHI , C HAPELLE AND D E C OSTE  M3V8  M3V8  6  6 SpSVM−R SpSVM−NR KMP−R KMP−NR  5 Test error rate (%)  Test error rate (%)  5  4  3  2  4  3  2  1  2  1  3  10 Num of basis functions  10  2  10  M3VOthers 4 SpSVM−R SpSVM−NR KMP−R KMP−NR  3. [sent-473, score-0.48]
</p><p>78 5 1  10  2  10 Num of basis functions  3  2  10  10  Vehicle 14. [sent-481, score-0.31]
</p><p>79 Note that the overall complexity of the algorithm does not change since the cost of adding one basis function is still O(nd). [sent-500, score-0.329]
</p><p>80 For a given number of basis functions, the test error is usually not as good as if we always retrain. [sent-503, score-0.348]
</p><p>81 2 Inﬂuence of κ The parameter κ is the number of candidate basis functions that are being tried each time a new basis function should be added: we select a random set of κ examples and the best one (as explained in Section 3. [sent-508, score-0.615]
</p><p>82 The larger κ is, the better the test error for a given number of basis functions, but also the longer the training time. [sent-514, score-0.397]
</p><p>83 Indeed, the basic cost for one iteration is O(nd) and the number of kernel calculations is κnSV + n: the ﬁrst term corresponds to trying different basis function, while the second one correspond to the inclusion of the chosen basis function. [sent-519, score-0.668]
</p><p>84 The decay rate of the objective function as well as the variance of the scores produced by the basis selection scoring function would be two key quantities helpful to adjust them. [sent-522, score-0.387]
</p><p>85 The L2 version (quadratic penalization of the slacks) is the one relevant for our study since it is the same loss function as the one we used; note that, when the number of basis functions increases towards n, the SpSVM solution will converge to the L2 SVM solution. [sent-534, score-0.387]
</p><p>86 Finally, note that for simplicity we kept the same hyperparameters for the different sizes, but that both methods would certainly gain by additional hyerparameter tuning (for instance when the number of basis functions is smaller, the bandwith of the RBF kernel should be larger). [sent-536, score-0.445]
</p><p>87 In terms of compression (left columns), our method is usually able to reach the same accuracy as a standard SVM using less than one-tenth the number of basis functions (this conﬁrms the results of table 1). [sent-538, score-0.332]
</p><p>88 Finally, note that when the number of basis functions is extremely small compared to the number of training examples, SpSVM can be slower than a SVM trained on a small subset (left part of the right column plots). [sent-542, score-0.359]
</p><p>89 Conclusion In this paper we have given a fast primal algorithm that greedily chooses a subset of the training basis functions to approximate the SVM solution. [sent-546, score-0.486]
</p><p>90 The real power of the method lies in its ability to form very good approximations of the SVM classiﬁer with a clear control on the complexity of the classiﬁer (number of basis functions) as well as the training time. [sent-548, score-0.335]
</p><p>91 In most data sets, performance very close to that of the SVM is achieved using a set of basis functions whose size is a small fraction of the number of SVM support vectors. [sent-549, score-0.336]
</p><p>92 While there is no satisfactory way of doing early stopping with SVMs, our method enables the user to control the training time by choosing the number of basis functions to use. [sent-552, score-0.381]
</p><p>93 Improved methods of choosing the κ-subset of basis functions in each step can also make the method more ef12. [sent-555, score-0.332]
</p><p>94 144 2  3  4  10 10 Number of basis functions ijcnn  1  10  2  10  0. [sent-574, score-0.492]
</p><p>95 015 3  −3  x 10  2  10 Number of basis functions shuttle  3  10 −3  x 10  2. [sent-586, score-0.413]
</p><p>96 5 2  3  1  10 10 Number of basis functions  10  2  10 Time  3  10  Figure 7: Comparison of SpSVM with SVMLight on Adult, IJCNN, Shuttle. [sent-592, score-0.31]
</p><p>97 For SVMLight, “Num of basis functions” should be understood as number of support vectors. [sent-593, score-0.29]
</p><p>98 015 3  1  10 Number of basis functions vehicle  0. [sent-608, score-0.394]
</p><p>99 11 0  Number of basis functions  10  2  4  10  10 Time  Figure 8: Comparison of SpSVM with SVMLight on M3V8 and Vehicle. [sent-622, score-0.31]
</p><p>100 For SVMLight, “Num of basis functions” should be understood as number of support vectors. [sent-623, score-0.29]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spsvm', 0.51), ('kmp', 0.295), ('basis', 0.264), ('ijcnn', 0.182), ('dmax', 0.175), ('num', 0.168), ('svm', 0.167), ('educed', 0.148), ('eerthi', 0.148), ('gsvc', 0.148), ('hapelle', 0.148), ('oste', 0.148), ('secs', 0.14), ('nr', 0.133), ('omplexity', 0.125), ('uilding', 0.125), ('svmlight', 0.117), ('cholesky', 0.103), ('nsv', 0.103), ('oi', 0.103), ('shuttle', 0.103), ('vincent', 0.103), ('adult', 0.098), ('primal', 0.094), ('kjj', 0.094), ('ndmax', 0.094), ('cpu', 0.093), ('newton', 0.087), ('vehicle', 0.084), ('kernel', 0.075), ('bh', 0.071), ('retraining', 0.071), ('pursuit', 0.07), ('regularizer', 0.069), ('decoste', 0.068), ('cost', 0.065), ('keerthi', 0.062), ('rsvm', 0.061), ('rate', 0.054), ('kji', 0.054), ('bengio', 0.053), ('test', 0.053), ('penalization', 0.051), ('training', 0.049), ('selection', 0.048), ('svms', 0.046), ('functions', 0.046), ('hyperparameter', 0.044), ('tried', 0.041), ('yi', 0.041), ('adler', 0.04), ('testerate', 0.04), ('matching', 0.04), ('greedy', 0.038), ('seeger', 0.037), ('mark', 0.037), ('squares', 0.035), ('yahoo', 0.035), ('factorization', 0.034), ('greedily', 0.033), ('ideas', 0.032), ('tuning', 0.032), ('error', 0.031), ('cv', 0.031), ('sparse', 0.03), ('tsch', 0.028), ('hyperparameters', 0.028), ('disproportionately', 0.027), ('downs', 0.027), ('meijerink', 0.027), ('thies', 0.027), ('tnn', 0.027), ('chapelle', 0.027), ('stages', 0.027), ('cache', 0.027), ('solution', 0.026), ('support', 0.026), ('boosting', 0.026), ('nips', 0.026), ('classi', 0.025), ('tting', 0.023), ('mainly', 0.023), ('differentiating', 0.023), ('kij', 0.023), ('svs', 0.023), ('burbank', 0.023), ('undergo', 0.023), ('bach', 0.022), ('method', 0.022), ('objective', 0.021), ('sch', 0.021), ('lin', 0.02), ('six', 0.02), ('solver', 0.02), ('cheaper', 0.02), ('till', 0.02), ('dennis', 0.02), ('olivier', 0.02), ('empire', 0.02), ('inc', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="18-tfidf-1" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: S. Sathiya Keerthi, Olivier Chapelle, Dennis DeCoste</p><p>Abstract: Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classiﬁcation speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily ﬁnds a set of kernel basis functions of a speciﬁed maximum size (dmax ) to approximate the SVM primal cost function well; (3) it is 2 efﬁcient and roughly scales as O(ndmax ) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors. Keywords: SVMs, classiﬁcation, sparse design</p><p>2 0.08813379 <a title="18-tfidf-2" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>3 0.083138362 <a title="18-tfidf-3" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Katya Scheinberg</p><p>Abstract: We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efﬁcient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims’ SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difﬁcult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight , the generalization properties of these two approaches are identical and we do not discuss them in the paper. Keywords: active set methods, support vector machines, quadratic programming</p><p>4 0.077938624 <a title="18-tfidf-4" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>5 0.071867242 <a title="18-tfidf-5" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Luca Zanni, Thomas Serafini, Gaetano Zanghirati</p><p>Abstract: Parallel software for solving the quadratic program arising in training support vector machines for classiﬁcation problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-ofthe-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes. Keywords: support vector machines, large scale quadratic programs, decomposition techniques, gradient projection methods, parallel computation</p><p>6 0.069576733 <a title="18-tfidf-6" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.067420706 <a title="18-tfidf-7" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>8 0.066162772 <a title="18-tfidf-8" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>9 0.064161845 <a title="18-tfidf-9" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>10 0.059696298 <a title="18-tfidf-10" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.058800567 <a title="18-tfidf-11" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.051909175 <a title="18-tfidf-12" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>13 0.047116078 <a title="18-tfidf-13" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.046407182 <a title="18-tfidf-14" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>15 0.043884929 <a title="18-tfidf-15" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>16 0.040895365 <a title="18-tfidf-16" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>17 0.040460985 <a title="18-tfidf-17" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>18 0.04026027 <a title="18-tfidf-18" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>19 0.038321815 <a title="18-tfidf-19" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>20 0.036796618 <a title="18-tfidf-20" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.201), (1, -0.147), (2, 0.135), (3, 0.081), (4, 0.134), (5, 0.021), (6, -0.104), (7, -0.007), (8, -0.035), (9, -0.1), (10, 0.096), (11, 0.033), (12, 0.093), (13, -0.088), (14, 0.068), (15, 0.065), (16, 0.007), (17, 0.072), (18, 0.025), (19, 0.07), (20, 0.139), (21, -0.03), (22, -0.072), (23, 0.052), (24, 0.007), (25, 0.037), (26, -0.004), (27, -0.079), (28, 0.01), (29, 0.147), (30, 0.007), (31, -0.046), (32, -0.036), (33, -0.058), (34, -0.09), (35, -0.218), (36, 0.067), (37, -0.14), (38, -0.023), (39, 0.109), (40, 0.018), (41, 0.113), (42, -0.201), (43, 0.007), (44, -0.113), (45, -0.173), (46, -0.044), (47, -0.073), (48, -0.107), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92435098 <a title="18-lsi-1" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: S. Sathiya Keerthi, Olivier Chapelle, Dennis DeCoste</p><p>Abstract: Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classiﬁcation speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily ﬁnds a set of kernel basis functions of a speciﬁed maximum size (dmax ) to approximate the SVM primal cost function well; (3) it is 2 efﬁcient and roughly scales as O(ndmax ) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors. Keywords: SVMs, classiﬁcation, sparse design</p><p>2 0.64810091 <a title="18-lsi-2" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Katya Scheinberg</p><p>Abstract: We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efﬁcient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims’ SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difﬁcult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight , the generalization properties of these two approaches are identical and we do not discuss them in the paper. Keywords: active set methods, support vector machines, quadratic programming</p><p>3 0.57151401 <a title="18-lsi-3" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>4 0.56129932 <a title="18-lsi-4" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>Author: Mingrui Wu, Bernhard Schölkopf, Gökhan Bakır</p><p>Abstract: Many kernel learning algorithms, including support vector machines, result in a kernel machine, such as a kernel classiﬁer, whose key component is a weight vector in a feature space implicitly introduced by a positive deﬁnite kernel function. This weight vector is usually obtained by solving a convex optimization problem. Based on this fact we present a direct method to build sparse kernel learning algorithms by adding one more constraint to the original convex optimization problem, such that the sparseness of the resulting kernel machine is explicitly controlled while at the same time performance is kept as high as possible. A gradient based approach is provided to solve this modiﬁed optimization problem. Applying this method to the support vectom machine results in a concrete algorithm for building sparse large margin classiﬁers. These classiﬁers essentially ﬁnd a discriminating subspace that can be spanned by a small number of vectors, and in this subspace, the diﬀerent classes of data are linearly well separated. Experimental results over several classiﬁcation benchmarks demonstrate the eﬀectiveness of our approach. Keywords: sparse learning, sparse large margin classiﬁers, kernel learning algorithms, support vector machine, kernel Fisher discriminant</p><p>5 0.45954442 <a title="18-lsi-5" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Olvi L. Mangasarian</p><p>Abstract: Support vector machines utilizing the 1-norm, typically set up as linear programs (Mangasarian, 2000; Bradley and Mangasarian, 1998), are formulated here as a completely unconstrained minimization of a convex differentiable piecewise-quadratic objective function in the dual space. The objective function, which has a Lipschitz continuous gradient and contains only one additional ﬁnite parameter, can be minimized by a generalized Newton method and leads to an exact solution of the support vector machine problem. The approach here is based on a formulation of a very general linear program as an unconstrained minimization problem and its application to support vector machine classiﬁcation problems. The present approach which generalizes both (Mangasarian, 2004) and (Fung and Mangasarian, 2004) is also applied to nonlinear approximation where a minimal number of nonlinear kernel functions are utilized to approximate a function from a given number of function values.</p><p>6 0.41959155 <a title="18-lsi-6" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.40564057 <a title="18-lsi-7" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.35832369 <a title="18-lsi-8" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>9 0.34657371 <a title="18-lsi-9" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>10 0.33013466 <a title="18-lsi-10" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>11 0.31305543 <a title="18-lsi-11" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>12 0.30512801 <a title="18-lsi-12" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>13 0.2832711 <a title="18-lsi-13" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.27635458 <a title="18-lsi-14" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.25745553 <a title="18-lsi-15" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>16 0.24770804 <a title="18-lsi-16" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>17 0.24665311 <a title="18-lsi-17" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>18 0.23560877 <a title="18-lsi-18" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>19 0.23246528 <a title="18-lsi-19" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>20 0.21569821 <a title="18-lsi-20" href="./jmlr-2006-Universal_Kernels.html">93 jmlr-2006-Universal Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.021), (35, 0.014), (36, 0.056), (50, 0.036), (63, 0.05), (76, 0.018), (78, 0.507), (81, 0.024), (84, 0.024), (90, 0.028), (91, 0.042), (96, 0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95963025 <a title="18-lda-1" href="./jmlr-2006-A_Linear_Non-Gaussian_Acyclic_Model_for_Causal_Discovery.html">4 jmlr-2006-A Linear Non-Gaussian Acyclic Model for Causal Discovery</a></p>
<p>Author: Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvärinen, Antti Kerminen</p><p>Abstract: In recent years, several methods have been proposed for the discovery of causal structure from non-experimental data. Such methods make various assumptions on the data generating process to facilitate its identiﬁcation from purely observational data. Continuing this line of research, we show how to discover the complete causal structure of continuous-valued data, under the assumptions that (a) the data generating process is linear, (b) there are no unobserved confounders, and (c) disturbance variables have non-Gaussian distributions of non-zero variances. The solution relies on the use of the statistical method known as independent component analysis, and does not require any pre-speciﬁed time-ordering of the variables. We provide a complete Matlab package for performing this LiNGAM analysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the effectiveness of the method using artiﬁcially generated data and real-world data. Keywords: independent component analysis, non-Gaussianity, causal discovery, directed acyclic graph, non-experimental data</p><p>same-paper 2 0.84592247 <a title="18-lda-2" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: S. Sathiya Keerthi, Olivier Chapelle, Dennis DeCoste</p><p>Abstract: Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classiﬁcation speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily ﬁnds a set of kernel basis functions of a speciﬁed maximum size (dmax ) to approximate the SVM primal cost function well; (3) it is 2 efﬁcient and roughly scales as O(ndmax ) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors. Keywords: SVMs, classiﬁcation, sparse design</p><p>3 0.41308078 <a title="18-lda-3" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>Author: Mingrui Wu, Bernhard Schölkopf, Gökhan Bakır</p><p>Abstract: Many kernel learning algorithms, including support vector machines, result in a kernel machine, such as a kernel classiﬁer, whose key component is a weight vector in a feature space implicitly introduced by a positive deﬁnite kernel function. This weight vector is usually obtained by solving a convex optimization problem. Based on this fact we present a direct method to build sparse kernel learning algorithms by adding one more constraint to the original convex optimization problem, such that the sparseness of the resulting kernel machine is explicitly controlled while at the same time performance is kept as high as possible. A gradient based approach is provided to solve this modiﬁed optimization problem. Applying this method to the support vectom machine results in a concrete algorithm for building sparse large margin classiﬁers. These classiﬁers essentially ﬁnd a discriminating subspace that can be spanned by a small number of vectors, and in this subspace, the diﬀerent classes of data are linearly well separated. Experimental results over several classiﬁcation benchmarks demonstrate the eﬀectiveness of our approach. Keywords: sparse learning, sparse large margin classiﬁers, kernel learning algorithms, support vector machine, kernel Fisher discriminant</p><p>4 0.37740088 <a title="18-lda-4" href="./jmlr-2006-Linear_State-Space_Models_for_Blind_Source_Separation.html">57 jmlr-2006-Linear State-Space Models for Blind Source Separation</a></p>
<p>Author: Rasmus Kongsgaard Olsson, Lars Kai Hansen</p><p>Abstract: We apply a type of generative modelling to the problem of blind source separation in which prior knowledge about the latent source signals, such as time-varying auto-correlation and quasiperiodicity, are incorporated into a linear state-space model. In simulations, we show that in terms of signal-to-error ratio, the sources are inferred more accurately as a result of the inclusion of strong prior knowledge. We explore different schemes of maximum-likelihood optimization for the purpose of learning the model parameters. The Expectation Maximization algorithm, which is often considered the standard optimization method in this context, results in slow convergence when the noise variance is small. In such scenarios, quasi-Newton optimization yields substantial improvements in a range of signal to noise ratios. We analyze the performance of the methods on convolutive mixtures of speech signals. Keywords: blind source separation, state-space model, independent component analysis, convolutive model, EM, speech modelling</p><p>5 0.37727788 <a title="18-lda-5" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Katya Scheinberg</p><p>Abstract: We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efﬁcient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims’ SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difﬁcult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight , the generalization properties of these two approaches are identical and we do not discuss them in the paper. Keywords: active set methods, support vector machines, quadratic programming</p><p>6 0.37089485 <a title="18-lda-6" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.36654449 <a title="18-lda-7" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>8 0.35850811 <a title="18-lda-8" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>9 0.35693979 <a title="18-lda-9" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>10 0.3478446 <a title="18-lda-10" href="./jmlr-2006-A_Very_Fast_Learning_Method_for_Neural_Networks_Based_on_Sensitivity_Analysis.html">8 jmlr-2006-A Very Fast Learning Method for Neural Networks Based on Sensitivity Analysis</a></p>
<p>11 0.3437193 <a title="18-lda-11" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.34009176 <a title="18-lda-12" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.339008 <a title="18-lda-13" href="./jmlr-2006-Causal_Graph_Based_Decomposition_of_Factored_MDPs.html">19 jmlr-2006-Causal Graph Based Decomposition of Factored MDPs</a></p>
<p>14 0.33873594 <a title="18-lda-14" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.33010691 <a title="18-lda-15" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>16 0.32682484 <a title="18-lda-16" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<p>17 0.32469612 <a title="18-lda-17" href="./jmlr-2006-Noisy-OR_Component_Analysis_and_its_Application_to_Link_Analysis.html">64 jmlr-2006-Noisy-OR Component Analysis and its Application to Link Analysis</a></p>
<p>18 0.32415074 <a title="18-lda-18" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>19 0.32382554 <a title="18-lda-19" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.32245317 <a title="18-lda-20" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
