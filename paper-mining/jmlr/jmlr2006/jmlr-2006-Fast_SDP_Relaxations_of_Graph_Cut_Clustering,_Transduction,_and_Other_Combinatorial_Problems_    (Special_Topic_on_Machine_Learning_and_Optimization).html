<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-33" href="#">jmlr2006-33</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-33-pdf" href="http://jmlr.org/papers/volume7/debie06a/debie06a.pdf">pdf</a></p><p>Author: Tijl De Bie, Nello Cristianini</p><p>Abstract: The rise of convex programming has changed the face of many research ﬁelds in recent years, machine learning being one of the ones that beneﬁtted the most. A very recent developement, the relaxation of combinatorial problems to semi-deﬁnite programs (SDP), has gained considerable attention over the last decade (Helmberg, 2000; De Bie and Cristianini, 2004a). Although SDP problems can be solved in polynomial time, for many relaxations the exponent in the polynomial complexity bounds is too high for scaling to large problem sizes. This has hampered their uptake as a powerful new tool in machine learning. In this paper, we present a new and fast SDP relaxation of the normalized graph cut problem, and investigate its usefulness in unsupervised and semi-supervised learning. In particular, this provides a convex algorithm for transduction, as well as approaches to clustering. We further propose a whole cascade of fast relaxations that all hold the middle between older spectral relaxations and the new SDP relaxation, allowing one to trade oﬀ computational cost versus relaxation accuracy. Finally, we discuss how the methodology developed in this paper can be applied to other combinatorial problems in machine learning, and we treat the max-cut problem as an example. Keywords: convex transduction, normalized graph cut, semi-deﬁnite programming, semisupervised learning, relaxation, combinatorial optimization, max-cut</p><p>Reference: <a title="jmlr-2006-33-reference" href="../jmlr2006_reference/jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A very recent developement, the relaxation of combinatorial problems to semi-deﬁnite programs (SDP), has gained considerable attention over the last decade (Helmberg, 2000; De Bie and Cristianini, 2004a). [sent-8, score-0.409]
</p><p>2 In this paper, we present a new and fast SDP relaxation of the normalized graph cut problem, and investigate its usefulness in unsupervised and semi-supervised learning. [sent-11, score-0.763]
</p><p>3 We further propose a whole cascade of fast relaxations that all hold the middle between older spectral relaxations and the new SDP relaxation, allowing one to trade oﬀ computational cost versus relaxation accuracy. [sent-13, score-1.299]
</p><p>4 De Bie and Cristianini  Graph cut clustering Informally speaking, in this paper we are seeking to divide these data points into two coherent sets, denoted by P and N , such that P N = S and P N = ∅. [sent-23, score-0.385]
</p><p>5 A number of approaches to bipartitioning sets of data, known as graph cut clustering approaches, make use of an edge-weighted graph, where the nodes in the graph represent the data points and the edges between them are weighted with the aﬃnities between the data points. [sent-25, score-0.523]
</p><p>6 1 we recall a few graph cut cost functions that have been proposed in literature. [sent-29, score-0.387]
</p><p>7 Graph cut transduction Besides this clustering scenario, we also consider the transduction scenario, where part of the class labels is speciﬁed. [sent-30, score-0.575]
</p><p>8 In graph cut approaches, the problem of transduction can naturally be approached by restricting the search for a low cost graph cut to graph cuts that do not violate the label information. [sent-32, score-0.977]
</p><p>9 1 Cut, Average Cut and Normalized Cut Cost Functions Several graph cut cost functions have been proposed in literature in the context of clustering, among which the cut cost, the average cut cost (ACut) and the normalized cut cost (NCut) (Shi and Malik, 2000). [sent-36, score-1.361]
</p><p>10 To get around this, spectral relaxations of the ACut and NCut optimization problems have been proposed in a clustering (Shi and Malik, 2000; Ng et al. [sent-40, score-0.555]
</p><p>11 2 Paper Outline We should emphasize that we are not as much interested in making claims concerning the usefulness of the normalized graph cut for (constrained) clustering problems. [sent-47, score-0.44]
</p><p>12 Instead, we mainly focus on the algorithmic problem involved in the optimization of the normalized graph cut as an interesting object of study on itself, because of its direct applicability to machine learning algorithms design. [sent-49, score-0.388]
</p><p>13 • In Section 2 we recapitulate the well known spectral relaxation of the NCut problem to an eigenvalue problem. [sent-52, score-0.58]
</p><p>14 Subsequently, a ﬁrst main result of this paper is presented, which is an eﬃciently solvable SDP relaxation of the NCut optimization problem. [sent-53, score-0.4]
</p><p>15 Lastly, this section contains a methodology to construct a cascade of SDP relaxations, all tighter than the spectral relaxation and looser than the SDP relaxation, and with a computational cost in between the cost of both extremes. [sent-54, score-0.864]
</p><p>16 • Lastly, in Section 4 we illustrate how the relaxation cascade and the subspace trick can be applied to speed up relaxations of other combinatorial problems as well, by applying it to the max-cut problem. [sent-61, score-0.97]
</p><p>17 1 A Spectral Relaxation We now provide a short derivation of the spectral relaxation of the NCut optimization problem as ﬁrst given in Shi and Malik (2000). [sent-89, score-0.588]
</p><p>18 The spectral relaxation is obtained by doing so, and subsequently dropping the combinatorial constraint on y. [sent-102, score-0.634]
</p><p>19 Then we can write the Lagrangian as: L(Γ, q, Ξ, λ, µ) = =  D−A dd − Γ, Ξ − λ diag(Γ) − q1 − µ (q − 1) − Γ, 2 s s dd D−A Γ, − Ξ − diag(λ) + µ 2 + q(1 λ − µ) + µ, s s Γ,  and the primal optimization problem is equivalent with: optprimal = min b Γ,q  max L(Γ, q, Ξ, λ, µ) . [sent-127, score-0.394]
</p><p>20 Ξ 0,λ,µ  Indeed, either the primal constraints are fulﬁlled and then the inner maximization reduces to the primal objective, or the maximum over the dual constraints is unbounded: max L(Γ, q, Ξ, λ, µ) = Ξ 0,λ,µ  Γ, D−A if the primal constraints are fulﬁlled, and s ∞ otherwise. [sent-128, score-0.495]
</p><p>21 b Ξ 0,λ,µ Γ,q A very useful relation between the primal and dual optima (on its own already warranting the study of the dual problem) is known as weak duality, and says that the dual maximum is a lower bound for the primal minimum (see e. [sent-131, score-0.365]
</p><p>22 s2 Importantly, this relaxation contains only n + 1 dual variables. [sent-151, score-0.431]
</p><p>23 It is thanks to this feature that this relaxation leads to a much more eﬃcient algorithm than the one presented in Xing and Jordan (2003). [sent-152, score-0.368]
</p><p>24 3 A Cascade of Relaxations Tighter Than Spectral and Looser Than SDP Still, in many cases the SDP relaxation is too complex, while the spectral relaxation is computationally feasible but too loose. [sent-156, score-0.924]
</p><p>25 Whereas numerous eﬀorts have been made in literature to further tighten SDP relaxations of (other) combinatorial problems by adding in 1416  Fast SDP Relaxations of Graph Cut Clustering  additional constraints and using so-called lifting techniques (see e. [sent-157, score-0.383]
</p><p>26 Here we present a set of such relaxations, and we will show that they hold the middle between the SDP relaxation and the spectral relaxation, both in terms of computational complexity and in terms of accuracy. [sent-160, score-0.556]
</p><p>27 s2 The attractive feature of the relaxation cascade is the fact that the number of dual parameters is only m + 1, as opposed to n + 1 for the basic SDP relaxation. [sent-169, score-0.589]
</p><p>28 In general, it is clear that a relaxation is tighter than another if the column space of the matrix W used in the ﬁrst one contains the full column space of W of the second. [sent-171, score-0.461]
</p><p>29 In particular, for d = n the original SDP relaxation is obtained. [sent-172, score-0.368]
</p><p>30 Theorem 2 The SDP relaxation from the cascade with m = 1 and W = d is (essentially) equivalent to the spectral relaxation. [sent-175, score-0.714]
</p><p>31 Proof Let us write Γ = VMV with M ∈ n×n a symmetric matrix and with the eigenvectors v of the spectral relaxation (D − A)v = σDv as the columns of V, in order of increasing eigenvalue σ, and normalized such that V DV = I. [sent-176, score-0.662]
</p><p>32 , the ﬁrst column of 1 V(:, 1) = √s , and the second column V(:, 2) = y is the relaxed label vector obtained using the spectral relaxation. [sent-179, score-0.365]
</p><p>33 This result shows that, while the actual choice of how to choose the matrix W in the relaxation cascade is basically free, for interpretability it is reasonable that d is within its column space (as only then all relaxations in the cascade are tighter than the spectral relaxation). [sent-196, score-1.196]
</p><p>34 4 Discussion So far we have introduced a cascade of relaxations of the normalized cut problem, the loosest of which is equivalent to the spectral relaxation. [sent-199, score-0.901]
</p><p>35 For each SDP relaxation we have derived a dual version, the optimum of which is a lower bound for the primal optimum (weak duality). [sent-200, score-0.591]
</p><p>36 Additionally, this allows us to get a better insight in the relation between the spectral relaxation and the cascade of SDP relaxations. [sent-203, score-0.714]
</p><p>37 That the primal constraints in our SDP relaxations are strictly feasible d can be seen by construction: choose Γ = qI 0 and q = 1/(1 − d 2 ). [sent-213, score-0.395]
</p><p>38 A more in-depth study of the relation between the spectral and SDP relaxations makes things clear. [sent-230, score-0.439]
</p><p>39 We have already shown in Theorem 2 that the SDP relaxation from the cascade with W = d is equivalent to the spectral relaxation. [sent-234, score-0.714]
</p><p>40 Theorem 6 Also the solution of the basic SDP relaxation Pclust is essentially equivalent SDP to the spectral relaxation. [sent-238, score-0.556]
</p><p>41 This result is essentially equivalent with the result from the spectral relaxation, if we ignore the inﬁnitely large constant matrix, and the two rank 2 matrix m 1 + 1 m that merely makes the diagonal of the label matrix equal to a constant. [sent-240, score-0.355]
</p><p>42 A very similar theorem holds for the relaxations from the cascade Pclust . [sent-241, score-0.409]
</p><p>43 m-SDP So, does this mean that none of the SDP relaxations is tighter than the spectral relaxation? [sent-242, score-0.465]
</p><p>44 Certainly not: the constraint set is clearly much tighter, as is obvious by looking at the relaxation cascade where constraints on the diagonal of Γ can explicitly be added 1419  De Bie and Cristianini  or omitted. [sent-243, score-0.663]
</p><p>45 However, all constraints except for the ones that are also present in the spectral relaxation are inactive. [sent-244, score-0.63]
</p><p>46 If additional constraints are imposed on the problem, some of the inactive constraints may become active, such that the tightness of the SDP relaxations starts paying oﬀ. [sent-245, score-0.417]
</p><p>47 Instead of upper bounding q, one can pick two data points and specify their classes to be diﬀerent from each other and subsequently solve the transductive NCut SDP relaxation from Section 3. [sent-258, score-0.46]
</p><p>48 5 ), hence the complexity of our basic SDP relaxation with an additional upper bound on q (for dual strict feasibility). [sent-269, score-0.431]
</p><p>49 1420  Fast SDP Relaxations of Graph Cut Clustering  parameter trading oﬀ the tightness of the relaxation with the computational complexity, and can be adapted according to the available computing resources. [sent-274, score-0.386]
</p><p>50 5 Estimating the Label Vector In the context of the max-cut problem, several techniques have been proposed in literature to construct a good binary label vector based on a label matrix as found by the max-cut SDP relaxation (see e. [sent-277, score-0.569]
</p><p>51 (2004) to derive a spectral relaxation of label-constrained normalized cut cost problems. [sent-297, score-0.922]
</p><p>52 Then the label constraints can be imposed by observing that any valid Γ must satisfy: Γ = LML =  M(1, 1)yt yt yt M(2 : ntest , 1) M(2 : ntest , 1)yt M(2 : ntest , 2 : ntest )  . [sent-308, score-0.445]
</p><p>53 Using this parameterization we can easily derive the transductive NCut relaxation whose solution will by construction respect the constraints on the training labels:   D−A  maxλ,µ µ,  minM,q M, L s L     s. [sent-310, score-0.506]
</p><p>54 It is clear that the transduction scenario is a special case of the scenario where equivalence and inequivalence constraints are given. [sent-324, score-0.352]
</p><p>55 2 Approximating the SDP Relaxation for Speed-Up Besides for imposing label constraints, the subspace trick can also be used to achieve a further speed-up of the SDP relaxations developed in the previous section. [sent-367, score-0.49]
</p><p>56 1 Using a Coarse Pre-clustering The semi-supervised learning methodology lends itself to speed up the SDP relaxation itself. [sent-372, score-0.368]
</p><p>57 The equivalence constraints found by the pre-clustering can then be used as constraints in the constrained SDP relaxation of the NCut problem. [sent-374, score-0.543]
</p><p>58 The resulting approximated relaxation then becomes:  Σ(1:d,1:d) ,  minM,q M,  s  s. [sent-383, score-0.368]
</p><p>59 For W = 1 (for m = 1), the well-known spectral relaxation of max-cut is obtained: 1 (D − A)v = σv, 4 where the dominant eigenvector is an approximation for the maximal cut. [sent-406, score-0.588]
</p><p>60 Also the subspace trick can readily be applied here, to give rise to label-constrained maxcut relaxations, or to approximations of the max-cut relaxation to control the computational burden. [sent-407, score-0.52]
</p><p>61 Then, the approximated max-cut relaxation becomes:  1  maxM 4 Γ, D − A s. [sent-409, score-0.368]
</p><p>62 In order to make these constraints as strong as possible, we use the heuristic to put points with a large value in the result of the spectral relaxation in the same subset of the partition. [sent-421, score-0.658]
</p><p>63 More speciﬁcally, we sort the entries of the relaxed label vector from the spectral relaxation, and construct the partition such that the m subsets are (roughly) equally large and such that data points in the same subset occur consecutively in this sorted ordering. [sent-422, score-0.353]
</p><p>64 Empirical Results In this section we empirically evaluate the basic SDP relaxation of the NCut problem and its use for transduction. [sent-425, score-0.368]
</p><p>65 Next, we investigate the cascade of relaxations for the max-cut problem, and the subspace trick to speed up the calculations. [sent-426, score-0.561]
</p><p>66 1 NCut Clustering and Transduction In all experiments for NCut clustering and transduction, we use the randomized rounding technique (with 100 random projections) to derive a crisp label vector from the label matrix Γ, and K-means on the relaxed label vector y obtained from the spectral relaxation. [sent-428, score-0.642]
</p><p>67 1 A Few Toy Problems The results obtained by using the basic SDP relaxation for a few 2-dimensional clustering problems are summarized in Figure 1. [sent-432, score-0.452]
</p><p>68 2 Clustering and Transduction on Text We use the data from De Bie and Cristianini (2004b) to evaluate the clustering and transduction performance of the basic SDP relaxation of the NCut optimization problem. [sent-437, score-0.593]
</p><p>69 5  −1 −1  1  Figure 1: The labeling obtained by the SDP relaxation on 4 toy problems. [sent-481, score-0.368]
</p><p>70 Figures 2 contain the relaxed minimal cost for the transductive spectral relaxation (De Bie et al. [sent-492, score-0.732]
</p><p>71 , 2004) and for the transductive SDP relaxation developed in this paper, as well as the costs corresponding to the label vectors derived from them, as a function of the fraction of data points labeled. [sent-493, score-0.565]
</p><p>72 Both graphs conﬁrm that the lower bound on the true (unrelaxed) minimum, provided by the SDP relaxation minimum, is consistently (and signiﬁcantly) tighter than the one provided by the spectral relaxation. [sent-495, score-0.582]
</p><p>73 Furthermore, the cost of the label vector derived from the spectral relaxation is consistently and signiﬁcantly larger than the cost of the SDP derived solution. [sent-496, score-0.767]
</p><p>74 The leftmost points in the ﬁgures correspond to the unsupervised case (for the SDP relaxation we used the second approach explained in Section 2. [sent-497, score-0.414]
</p><p>75 Interesting to note is that for the easier problem (left ﬁgure), the spectral relaxation and the SDP relaxation perform exactly equally, while the SDP problem responds signiﬁcantly better to label information than the spectral relaxation for the harder problem (right ﬁgure). [sent-509, score-1.567]
</p><p>76 2 Max-Cut We use the max-cut problem to conduct an in-depth analysis of the computational consequences of the relaxation cascade and of the subspace trick. [sent-512, score-0.599]
</p><p>77 8  1  Figure 2: The costs for the best solution over 100 random roundings based on the SDP solution Γ (full bold line), and by performing K-means on the generalized eigenvector y of the spectral relaxation (full faint line). [sent-540, score-0.632]
</p><p>78 8  1  Figure 3: The test set accuracies for the best solution over 100 random roundings based on the SDP solution (bold), and by performing K-means on the generalized eigenvector of the spectral relaxation (faint). [sent-566, score-0.607]
</p><p>79 For these graphs, Figure 4, shows the results of a computational analysis of the relaxation cascade and of the subspace trick as outlined in Section 4. [sent-580, score-0.678]
</p><p>80 In all experiments, a crisp label vector is derived from a relaxed vector (obtained using the spectral relaxation) by simple thresholding around 0, and from a relaxed label matrix by using the randomized rounding technique explained in Section 2. [sent-582, score-0.521]
</p><p>81 For the relaxation cascade, there is only one parameter to study the eﬀect of: the number of constraints m on the trace of the label matrix. [sent-586, score-0.529]
</p><p>82 We varied this parameter over all values 1, 2, 4, 8, 16, 32, 64, 128, 256 and n, where for m = 1, the algorithm reduces to the spectral relaxation, and for m = n the well-known SDP relaxation is obtained. [sent-587, score-0.556]
</p><p>83 The small dots in the ﬁgures give an idea of the eﬀect of the subspace trick, for subspace dimensionality d equal to d = 2, 4, 8, 16, 32, in combination with the values for m (except for 1 and n) used as above in the relaxation cascade. [sent-592, score-0.514]
</p><p>84 Clearly the subspace trick allows one to achieve a generally higher cut value at a signiﬁcantly reduced computational cost. [sent-596, score-0.425]
</p><p>85 Using the subspace approximation, it is also possible to ﬁnd a better cut than the one found using the spectral relaxation for the two most challenging problems below in the ﬁgure. [sent-597, score-0.902]
</p><p>86 Even though the cascade of relaxations empirically appears less eﬃcient in obtaining good approximations to the relaxed optimum, a major disadvantage of the subspace trick is 1429  De Bie and Cristianini  4  1. [sent-598, score-0.611]
</p><p>87 The crosses correspond to the relaxation cascade, with m = 1, 2, 4, . [sent-625, score-0.368]
</p><p>88 For the last two graphs G67 abd G81, the relaxation cascade requires too much memory to solve on a Pentium 2GHz with 1Gb Ram and is therefore omitted (except for d = 1, the spectral relaxation). [sent-630, score-0.714]
</p><p>89 9 Cut upper bound, value of the cut found  Cut upper bound, value of the cut found  1. [sent-647, score-0.546]
</p><p>90 4 So let us now investigate to what extent the cascade of relaxations is helpful in obtaining a bound on the unrelaxed optimum in those cases where computing the full SDP relaxation is too time consuming. [sent-652, score-0.872]
</p><p>91 Figure 5 contains the value of the max-cut relaxations as a function of the number of constraints m on the diagonal of the label matrix, as well as the actual value of the cut found. [sent-653, score-0.711]
</p><p>92 At the same time, for larger m the objective value (the cut cost) for the found label vector increases. [sent-655, score-0.36]
</p><p>93 Conclusions We proposed a new cascade of SDP relaxations of the NP-complete normalized graph cut optimization problem. [sent-660, score-0.797]
</p><p>94 On both extremes of the cascade are the well-know spectral relaxation and a newly proposed SDP relaxation. [sent-661, score-0.714]
</p><p>95 We reported encouraging empirical results for the use of the NCut cost function and more in particular of its newly proposed SDP relaxation for clustering and for semi-supervised learning. [sent-668, score-0.514]
</p><p>96 Furthermore, we illustrated the use of the cascade of relaxations and of the subspace trick on the max-cut problem. [sent-669, score-0.561]
</p><p>97 An interesting research direction opened in this paper is the question which are good and and eﬃciently computable choices for the matrix W, both for the relaxation cascade and for the subspace approximation that is based on it. [sent-670, score-0.626]
</p><p>98 An answer to this question may have broad implications in the ﬁeld of combinatorial optimization and relaxation theory. [sent-671, score-0.441]
</p><p>99 Therefrom we can see that λ = σ2 d and hence Ξ = Σ(2 : n, 2 : n) − σ2 I at the optimum (where σ2 is used to denote Σ(2, 2), the smallest generalized eigenvalue of the spectral relaxation that is diﬀerent from 0). [sent-707, score-0.644]
</p><p>100 On semideﬁnite relaxation for normalized k-cut and connections to spectral clustering. [sent-864, score-0.587]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sdp', 0.531), ('relaxation', 0.368), ('ncut', 0.344), ('cut', 0.273), ('relaxations', 0.251), ('spectral', 0.188), ('cascade', 0.158), ('diag', 0.154), ('bie', 0.152), ('dd', 0.129), ('assoc', 0.117), ('pclust', 0.109), ('transduction', 0.109), ('inequivalence', 0.092), ('label', 0.087), ('clustering', 0.084), ('trick', 0.079), ('constraints', 0.074), ('subspace', 0.073), ('primal', 0.07), ('acut', 0.067), ('transductive', 0.064), ('dual', 0.063), ('cost', 0.062), ('cristianini', 0.061), ('helmberg', 0.059), ('ntest', 0.059), ('vmv', 0.059), ('graph', 0.052), ('psd', 0.05), ('unrelaxed', 0.05), ('relaxed', 0.05), ('optimum', 0.045), ('shi', 0.043), ('combinatorial', 0.041), ('nity', 0.04), ('burer', 0.038), ('constraint', 0.037), ('erent', 0.036), ('optima', 0.036), ('yy', 0.035), ('bipartitioning', 0.034), ('gset', 0.034), ('optdual', 0.034), ('optprimal', 0.034), ('sdplr', 0.034), ('eigenvector', 0.032), ('rounding', 0.032), ('dv', 0.032), ('optimization', 0.032), ('duality', 0.031), ('normalized', 0.031), ('qw', 0.029), ('malik', 0.029), ('minm', 0.028), ('points', 0.028), ('equivalence', 0.027), ('matrix', 0.027), ('diagonal', 0.026), ('tighter', 0.026), ('articles', 0.026), ('mv', 0.026), ('dclust', 0.025), ('faint', 0.025), ('lml', 0.025), ('monteiro', 0.025), ('symmetricity', 0.025), ('syy', 0.025), ('tijl', 0.025), ('scenario', 0.025), ('eigenvalue', 0.024), ('labeled', 0.024), ('yt', 0.024), ('eigenvectors', 0.024), ('semide', 0.023), ('relax', 0.023), ('di', 0.023), ('clusters', 0.022), ('nello', 0.022), ('kamvar', 0.021), ('shental', 0.021), ('slater', 0.021), ('fast', 0.021), ('dy', 0.02), ('column', 0.02), ('generalized', 0.019), ('su', 0.019), ('language', 0.019), ('sedumi', 0.019), ('fraction', 0.018), ('unsupervised', 0.018), ('ful', 0.018), ('bag', 0.018), ('tightness', 0.018), ('cuts', 0.017), ('anjos', 0.017), ('appr', 0.017), ('bristol', 0.017), ('goemans', 0.017), ('lifting', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="33-tfidf-1" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tijl De Bie, Nello Cristianini</p><p>Abstract: The rise of convex programming has changed the face of many research ﬁelds in recent years, machine learning being one of the ones that beneﬁtted the most. A very recent developement, the relaxation of combinatorial problems to semi-deﬁnite programs (SDP), has gained considerable attention over the last decade (Helmberg, 2000; De Bie and Cristianini, 2004a). Although SDP problems can be solved in polynomial time, for many relaxations the exponent in the polynomial complexity bounds is too high for scaling to large problem sizes. This has hampered their uptake as a powerful new tool in machine learning. In this paper, we present a new and fast SDP relaxation of the normalized graph cut problem, and investigate its usefulness in unsupervised and semi-supervised learning. In particular, this provides a convex algorithm for transduction, as well as approaches to clustering. We further propose a whole cascade of fast relaxations that all hold the middle between older spectral relaxations and the new SDP relaxation, allowing one to trade oﬀ computational cost versus relaxation accuracy. Finally, we discuss how the methodology developed in this paper can be applied to other combinatorial problems in machine learning, and we treat the max-cut problem as an example. Keywords: convex transduction, normalized graph cut, semi-deﬁnite programming, semisupervised learning, relaxation, combinatorial optimization, max-cut</p><p>2 0.17429011 <a title="33-tfidf-2" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>3 0.17113365 <a title="33-tfidf-3" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram. Keywords: spectral clustering, blind source separation, computational auditory scene analysis</p><p>4 0.070121445 <a title="33-tfidf-4" href="./jmlr-2006-A_Simulation-Based_Algorithm_for_Ergodic_Control_of_Markov_Chains_Conditioned_on_Rare_Events.html">7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</a></p>
<p>Author: Shalabh Bhatnagar, Vivek S. Borkar, Madhukar Akarapu</p><p>Abstract: We study the problem of long-run average cost control of Markov chains conditioned on a rare event. In a related recent work, a simulation based algorithm for estimating performance measures associated with a Markov chain conditioned on a rare event has been developed. We extend ideas from this work and develop an adaptive algorithm for obtaining, online, optimal control policies conditioned on a rare event. Our algorithm uses three timescales or step-size schedules. On the slowest timescale, a gradient search algorithm for policy updates that is based on one-simulation simultaneous perturbation stochastic approximation (SPSA) type estimates is used. Deterministic perturbation sequences obtained from appropriate normalized Hadamard matrices are used here. The fast timescale recursions compute the conditional transition probabilities of an associated chain by obtaining solutions to the multiplicative Poisson equation (for a given policy estimate). Further, the risk parameter associated with the value function for a given policy estimate is updated on a timescale that lies in between the two scales above. We brieﬂy sketch the convergence analysis of our algorithm and present a numerical application in the setting of routing multiple ﬂows in communication networks. Keywords: Markov decision processes, optimal control conditioned on a rare event, simulation based algorithms, SPSA with deterministic perturbations, reinforcement learning</p><p>5 0.069618337 <a title="33-tfidf-5" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><p>6 0.053917024 <a title="33-tfidf-6" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.048647564 <a title="33-tfidf-7" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>8 0.046171106 <a title="33-tfidf-8" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>9 0.043323498 <a title="33-tfidf-9" href="./jmlr-2006-Ensemble_Pruning_Via_Semi-definite_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">27 jmlr-2006-Ensemble Pruning Via Semi-definite Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.038775526 <a title="33-tfidf-10" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.038351856 <a title="33-tfidf-11" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>12 0.038005654 <a title="33-tfidf-12" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>13 0.033801623 <a title="33-tfidf-13" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.031973381 <a title="33-tfidf-14" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.030710006 <a title="33-tfidf-15" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.02752886 <a title="33-tfidf-16" href="./jmlr-2006-Some_Discriminant-Based_PAC_Algorithms.html">81 jmlr-2006-Some Discriminant-Based PAC Algorithms</a></p>
<p>17 0.027374711 <a title="33-tfidf-17" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>18 0.027320776 <a title="33-tfidf-18" href="./jmlr-2006-Rearrangement_Clustering%3A_Pitfalls%2C_Remedies%2C_and_Applications.html">78 jmlr-2006-Rearrangement Clustering: Pitfalls, Remedies, and Applications</a></p>
<p>19 0.027193855 <a title="33-tfidf-19" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>20 0.02713611 <a title="33-tfidf-20" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.181), (1, -0.119), (2, 0.014), (3, 0.166), (4, 0.033), (5, -0.057), (6, 0.054), (7, 0.275), (8, -0.084), (9, 0.243), (10, -0.173), (11, -0.129), (12, -0.204), (13, 0.038), (14, -0.003), (15, 0.076), (16, -0.131), (17, 0.112), (18, -0.096), (19, 0.112), (20, -0.044), (21, 0.013), (22, 0.059), (23, 0.02), (24, 0.037), (25, 0.113), (26, -0.01), (27, -0.049), (28, 0.089), (29, 0.144), (30, 0.213), (31, 0.202), (32, -0.105), (33, 0.097), (34, -0.016), (35, 0.05), (36, -0.003), (37, 0.101), (38, -0.007), (39, -0.124), (40, -0.021), (41, -0.109), (42, -0.011), (43, 0.071), (44, 0.086), (45, -0.119), (46, -0.122), (47, 0.064), (48, 0.025), (49, 0.094)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97401261 <a title="33-lsi-1" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tijl De Bie, Nello Cristianini</p><p>Abstract: The rise of convex programming has changed the face of many research ﬁelds in recent years, machine learning being one of the ones that beneﬁtted the most. A very recent developement, the relaxation of combinatorial problems to semi-deﬁnite programs (SDP), has gained considerable attention over the last decade (Helmberg, 2000; De Bie and Cristianini, 2004a). Although SDP problems can be solved in polynomial time, for many relaxations the exponent in the polynomial complexity bounds is too high for scaling to large problem sizes. This has hampered their uptake as a powerful new tool in machine learning. In this paper, we present a new and fast SDP relaxation of the normalized graph cut problem, and investigate its usefulness in unsupervised and semi-supervised learning. In particular, this provides a convex algorithm for transduction, as well as approaches to clustering. We further propose a whole cascade of fast relaxations that all hold the middle between older spectral relaxations and the new SDP relaxation, allowing one to trade oﬀ computational cost versus relaxation accuracy. Finally, we discuss how the methodology developed in this paper can be applied to other combinatorial problems in machine learning, and we treat the max-cut problem as an example. Keywords: convex transduction, normalized graph cut, semi-deﬁnite programming, semisupervised learning, relaxation, combinatorial optimization, max-cut</p><p>2 0.61643344 <a title="33-lsi-2" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram. Keywords: spectral clustering, blind source separation, computational auditory scene analysis</p><p>3 0.4442898 <a title="33-lsi-3" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>4 0.28828084 <a title="33-lsi-4" href="./jmlr-2006-A_Simulation-Based_Algorithm_for_Ergodic_Control_of_Markov_Chains_Conditioned_on_Rare_Events.html">7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</a></p>
<p>Author: Shalabh Bhatnagar, Vivek S. Borkar, Madhukar Akarapu</p><p>Abstract: We study the problem of long-run average cost control of Markov chains conditioned on a rare event. In a related recent work, a simulation based algorithm for estimating performance measures associated with a Markov chain conditioned on a rare event has been developed. We extend ideas from this work and develop an adaptive algorithm for obtaining, online, optimal control policies conditioned on a rare event. Our algorithm uses three timescales or step-size schedules. On the slowest timescale, a gradient search algorithm for policy updates that is based on one-simulation simultaneous perturbation stochastic approximation (SPSA) type estimates is used. Deterministic perturbation sequences obtained from appropriate normalized Hadamard matrices are used here. The fast timescale recursions compute the conditional transition probabilities of an associated chain by obtaining solutions to the multiplicative Poisson equation (for a given policy estimate). Further, the risk parameter associated with the value function for a given policy estimate is updated on a timescale that lies in between the two scales above. We brieﬂy sketch the convergence analysis of our algorithm and present a numerical application in the setting of routing multiple ﬂows in communication networks. Keywords: Markov decision processes, optimal control conditioned on a rare event, simulation based algorithms, SPSA with deterministic perturbations, reinforcement learning</p><p>5 0.27569014 <a title="33-lsi-5" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Chen Yanover, Talya Meltzer, Yair Weiss</p><p>Abstract: The problem of ﬁnding the most probable (MAP) conﬁguration in graphical models comes up in a wide range of applications. In a general graphical model this problem is NP hard, but various approximate algorithms have been developed. Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for ﬁnding the most probable assignment in small graphical models. However, applying this powerful method to real-world problems is extremely challenging due to the large numbers of variables and constraints in the linear program. Tree-Reweighted Belief Propagation is a promising recent algorithm for solving LP relaxations, but little is known about its running time on large problems. In this paper we compare tree-reweighted belief propagation (TRBP) and powerful generalpurpose LP solvers (CPLEX) on relaxations of real-world graphical models from the ﬁelds of computer vision and computational biology. We ﬁnd that TRBP almost always ﬁnds the solution signiﬁcantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. Using TRBP we can ﬁnd the MAP conﬁgurations in a matter of minutes for a large range of real world problems.</p><p>6 0.27195007 <a title="33-lsi-6" href="./jmlr-2006-Ensemble_Pruning_Via_Semi-definite_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">27 jmlr-2006-Ensemble Pruning Via Semi-definite Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.21303356 <a title="33-lsi-7" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.20528211 <a title="33-lsi-8" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>9 0.16996652 <a title="33-lsi-9" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>10 0.15605825 <a title="33-lsi-10" href="./jmlr-2006-A_Graphical_Representation_of_Equivalence_Classes_of_AMP_Chain_Graphs.html">2 jmlr-2006-A Graphical Representation of Equivalence Classes of AMP Chain Graphs</a></p>
<p>11 0.1516266 <a title="33-lsi-11" href="./jmlr-2006-Rearrangement_Clustering%3A_Pitfalls%2C_Remedies%2C_and_Applications.html">78 jmlr-2006-Rearrangement Clustering: Pitfalls, Remedies, and Applications</a></p>
<p>12 0.14512993 <a title="33-lsi-12" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.14479841 <a title="33-lsi-13" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.13646817 <a title="33-lsi-14" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>15 0.1258277 <a title="33-lsi-15" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.12518296 <a title="33-lsi-16" href="./jmlr-2006-Computational_and_Theoretical_Analysis_of__Null_Space__and_Orthogonal_Linear_Discriminant_Analysis.html">21 jmlr-2006-Computational and Theoretical Analysis of  Null Space  and Orthogonal Linear Discriminant Analysis</a></p>
<p>17 0.12220231 <a title="33-lsi-17" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>18 0.1197954 <a title="33-lsi-18" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.11736807 <a title="33-lsi-19" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.11679153 <a title="33-lsi-20" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.03), (35, 0.015), (36, 0.059), (38, 0.424), (45, 0.012), (50, 0.041), (63, 0.034), (68, 0.012), (71, 0.011), (76, 0.025), (78, 0.023), (81, 0.029), (84, 0.026), (90, 0.018), (91, 0.054), (96, 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76790857 <a title="33-lda-1" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tijl De Bie, Nello Cristianini</p><p>Abstract: The rise of convex programming has changed the face of many research ﬁelds in recent years, machine learning being one of the ones that beneﬁtted the most. A very recent developement, the relaxation of combinatorial problems to semi-deﬁnite programs (SDP), has gained considerable attention over the last decade (Helmberg, 2000; De Bie and Cristianini, 2004a). Although SDP problems can be solved in polynomial time, for many relaxations the exponent in the polynomial complexity bounds is too high for scaling to large problem sizes. This has hampered their uptake as a powerful new tool in machine learning. In this paper, we present a new and fast SDP relaxation of the normalized graph cut problem, and investigate its usefulness in unsupervised and semi-supervised learning. In particular, this provides a convex algorithm for transduction, as well as approaches to clustering. We further propose a whole cascade of fast relaxations that all hold the middle between older spectral relaxations and the new SDP relaxation, allowing one to trade oﬀ computational cost versus relaxation accuracy. Finally, we discuss how the methodology developed in this paper can be applied to other combinatorial problems in machine learning, and we treat the max-cut problem as an example. Keywords: convex transduction, normalized graph cut, semi-deﬁnite programming, semisupervised learning, relaxation, combinatorial optimization, max-cut</p><p>2 0.29850751 <a title="33-lda-2" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>3 0.29420769 <a title="33-lda-3" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer, Bernhard Schölkopf</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun. Keywords: multiple kernel learning, string kernels, large scale optimization, support vector machines, support vector regression, column generation, semi-inﬁnite linear programming</p><p>4 0.29293263 <a title="33-lda-4" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>Author: Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou</p><p>Abstract: We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the ﬁrst time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction. html. Keywords: transduction, transductive SVMs, semi-supervised learning, CCCP</p><p>5 0.2897813 <a title="33-lda-5" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Matthias Heiler, Christoph Schnörr</p><p>Abstract: We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. We show that for ordinary NMF, our approach performs as well as existing stateof-the-art algorithms, while for sparsity-constrained NMF, as recently proposed by P. O. Hoyer in JMLR 5 (2004), it outperforms previous methods. In addition, we show how to extend NMF learning within the same optimization framework in order to make use of class membership information in supervised learning problems. Keywords: non-negative matrix factorization, second-order cone programming, sequential convex optimization, reverse-convex programming, sparsity</p><p>6 0.28753772 <a title="33-lda-6" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>7 0.28320292 <a title="33-lda-7" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>8 0.28312179 <a title="33-lda-8" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.28269094 <a title="33-lda-9" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.28188911 <a title="33-lda-10" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>11 0.28086382 <a title="33-lda-11" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>12 0.28012908 <a title="33-lda-12" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>13 0.27908477 <a title="33-lda-13" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>14 0.2790415 <a title="33-lda-14" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.27650538 <a title="33-lda-15" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>16 0.27606618 <a title="33-lda-16" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>17 0.27606291 <a title="33-lda-17" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.2759783 <a title="33-lda-18" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.27189904 <a title="33-lda-19" href="./jmlr-2006-Ensemble_Pruning_Via_Semi-definite_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">27 jmlr-2006-Ensemble Pruning Via Semi-definite Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.27166101 <a title="33-lda-20" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
