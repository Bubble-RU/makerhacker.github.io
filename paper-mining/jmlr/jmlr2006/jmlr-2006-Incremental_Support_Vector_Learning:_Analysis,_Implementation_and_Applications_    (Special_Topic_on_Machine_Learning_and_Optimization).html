<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-38" href="#">jmlr2006-38</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-38-pdf" href="http://jmlr.org/papers/volume7/laskov06a/laskov06a.pdf">pdf</a></p><p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>Reference: <a title="jmlr-2006-38-reference" href="../jmlr2006_reference/jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. [sent-13, score-0.338]
</p><p>2 A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. [sent-14, score-0.304]
</p><p>3 Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. [sent-15, score-0.585]
</p><p>4 Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. [sent-17, score-0.24]
</p><p>5 Keywords: incremental SVM, online learning, drug discovery, intrusion detection  1. [sent-18, score-0.555]
</p><p>6 Their incremental algorithm (hereinafter referred to as a C&P; algorithm) updates an optimal solution of an SVM training problem after one training example is added (or removed). [sent-56, score-0.341]
</p><p>7 At a ﬁrst glance, a limited interest to incremental SVM learning may seem to result the absence of well-accepted implementations, such as its counterparts SVMlight (Joachims, 1999), SMO (Platt, 1999) and LIBSVM (Chang and Lin, 2000) for batch SVM learning. [sent-62, score-0.344]
</p><p>8 There are, however, deeper reasons why incremental SVM may not be so easy to implement. [sent-64, score-0.273]
</p><p>9 To understand them – and to build a foundation for an efﬁcient design and implementation of the algorithm, a detailed analysis of the incremental SVM technique is carried out in this paper. [sent-65, score-0.42]
</p><p>10 The actual runtime depends on the balance of memory access and arithmetic operations in a minor iteration. [sent-69, score-0.563]
</p><p>11 The main incremental step of the algorithm is guaranteed to bring progress in the objective function if a kernel matrix is positive semi-deﬁnite. [sent-71, score-0.466]
</p><p>12 Based on the results of our analysis, we propose a new storage design and organization of computation for a minor iteration of the algorithm. [sent-72, score-0.227]
</p><p>13 The second building block of our design is gaxpy-type matrix-vector multiplication, which allows to further minimize selection operations which cannot be eliminated by storage 1. [sent-74, score-0.316]
</p><p>14 To demonstrate applicability of incremental SVM to practical applications, two learning scenarios are presented. [sent-80, score-0.273]
</p><p>15 Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. [sent-83, score-0.24]
</p><p>16 An extension of the basic incremental SVM to one-class classiﬁcation is presented in Section 3. [sent-85, score-0.273]
</p><p>17 Analysis of computational complexity, design of efﬁcient storage and organization of operations are presented and evaluated in Section 5. [sent-87, score-0.278]
</p><p>18 Finally, potential applications of incremental SVM for learning with limited resources and active learning are illustrated in Section 6. [sent-88, score-0.474]
</p><p>19 Incremental SVM Algorithm In this section we present the basic incremental SVM algorithm. [sent-90, score-0.273]
</p><p>20 The matrix K is obtained from the kernel matrix by incorporating the labels: K = K 0 ⊙ (yyT ). [sent-99, score-0.254]
</p><p>21 2 Derivation of the Basic Incremental SVM Algorithm The main building block of the incremental SVM is a procedure for adding one example to an existing optimal solution. [sent-105, score-0.273]
</p><p>22 The goal of the weight update in the incremental SVM algorithm is to ﬁnd a weight assignment such that the Kuhn-Tucker conditions are satisﬁed for the enlarged data set. [sent-114, score-0.335]
</p><p>23 1912  (7)  I NCREMENTAL S UPPORT V ECTOR L EARNING  One can further substitute (6) into lines 1 and 3 of the system (4): ∆gc = γ∆αc , ∆gr  (8)  yc Kcs K β + cc T yr Krs Kcr  (9)  where γ=  is the gradient of the manifold of gradients gr at an optimal solution parameterized by αc . [sent-124, score-0.359]
</p><p>24 The upshot of these derivations is that the update is controlled by very simple sensitivity relations (6) and (8), where β is sensitivity of ∆s with respect to ∆αc and γ is sensitivity of ∆gc,r with respect to ∆αc . [sent-125, score-0.281]
</p><p>25 It can be shown that the step ∆αc is always positive in the incremental case. [sent-144, score-0.273]
</p><p>26 The high-level summary of incremental SVM algorithm is given in Algorithm 1. [sent-163, score-0.273]
</p><p>27 When the example xk 3 is added to the set S, the matrix to be inverted is partitioned as follows:   0 yT yk s Q−1 ηk T , (15) R := ys Kss Kks  = ηT Kkk k yk Kks Kkk where  ηk =  yk T . [sent-169, score-0.286]
</p><p>28 As we shall see, incremental learning can be naturally extended to both of these approaches. [sent-207, score-0.273]
</p><p>29 I NCREMENTAL S UPPORT V ECTOR L EARNING  Algorithm 2 Initialization of incremental one-class SVM. [sent-214, score-0.273]
</p><p>30 4: Compute µ so as to ensure non-positive gradients in E: µ = −max gi i∈E  5:  Enter the main loop of the incremental algorithm. [sent-219, score-0.358]
</p><p>31 In particular we present a technique for handling the special case of the empty set S and show that immediate cycling is impossible if a kernel matrix is positive semi-deﬁnite. [sent-233, score-0.265]
</p><p>32 With ∆α = 0, ∆αc = 0 and no examples in the set S, the equilibrium condition (4) reduces to ∆gc = yc ∆µ ∆gr = yr ∆µ. [sent-241, score-0.264]
</p><p>33 yc  Since ∆gc must be non-negative (gradient of the current example is negative and should be brought yr to zero if possible), the direction of ∆gr is given by − yc . [sent-250, score-0.363]
</p><p>34 Let us compute β after an addition of example k to set S: Q 0 ˜ ˜ β = −Qη = − 0 0  1 βk βT yc k − Kcs κ βT k  βk 1  yc . [sent-261, score-0.286]
</p><p>35 Kcs  Computing the last line in the above matrix products we obtain: 1 T yc ˜ + Kck . [sent-262, score-0.248]
</p><p>36 7) as Q=  −1 δ 1 −1 Kss ys δ  1 T −1 δ ys Kss −1 − 1 K −1 y yT K −1 Kss δ ss s s ss  1919  ,  ¨ ¨ L ASKOV, G EHL , K R UGER AND M ULLER  −1 where δ = yT Kss ys . [sent-286, score-0.57]
</p><p>37 Then 1 1 T −1 −1 −1 T −1 zT ys + yk = − yk yT Kss ys − Ksk Kss ys + Ksk Kss ys yT Kss ys + yk = 0 s s \0 δ δ δ  δ  and the result follows by Lemma 2. [sent-288, score-0.925]
</p><p>38 5 Asymptotically, the complexity of a minor iteration is quadratic in a number of examples learned so far: re-computation of the gradient, β and γ involve matrix-vector multiplications, which have quadratic complexity, and the recursive update of an inverse matrix has also been shown (cf. [sent-291, score-0.232]
</p><p>39 The focus of our analysis lies on the complexity of the main steps of a minor iteration in terms of arithmetic and memory access operations. [sent-300, score-0.254]
</p><p>40 Therefore, a key to efﬁciency of the incremental SVM algorithm lies in identifying performance bottlenecks associated with memory access operations and trying to eliminate them in a clever design. [sent-303, score-0.522]
</p><p>41 net/  1920  I NCREMENTAL S UPPORT V ECTOR L EARNING  It follows from our analysis and experiments that the main difﬁculty in efﬁcient implementation of incremental SVM indeed lies in selection of non-contiguous elements of matrices, e. [sent-318, score-0.362]
</p><p>42 Furthermore, it must be realized, as our experience showed us, that merely re-implementing incremental SVM without addressing the tradeoff between selection and arithmetic operations, for example using C++ with one-dimensional storage, does not solve the problem. [sent-323, score-0.411]
</p><p>43 3, which allows to completely eliminate expensive selection operations at a cost of minor increase of arithmetic operations, is based on a mixture of row- and column-major storage and on the gaxpy-type (e. [sent-325, score-0.418]
</p><p>44 1 Computational Complexity of Incremental SVM On the basis of the pseudo-code of Algorithm 1 we will now discuss the key issues that will later be used for a more efﬁcient implementation of the incremental SVM. [sent-331, score-0.324]
</p><p>45 (9) is especially costly since a two-dimensional selection  has to be performed to obtain the matrix Krs (O(sr) memory access operations), followed by a matrix-vector multiplication (O(sr) arithmetic operations). [sent-339, score-0.343]
</p><p>46 Lines 4-8: These lines have minor runtime relevance because only vector-scalar calculations and  selections are to be performed. [sent-343, score-0.258]
</p><p>47 If a kernel row of  the example k is not present (in case of xk entering the S from O) then it has to be re-computed for the update of the inverse matrix in line 17. [sent-345, score-0.381]
</p><p>48 To summarize, the main performance bottlenecks of the algorithm are lines 1, 3, and 17, in which memory access operations take place. [sent-350, score-0.249]
</p><p>49 As a test-bed the MNIST handwritten digits data set9 is used to proﬁle the training of an incremental SVM. [sent-354, score-0.307]
</p><p>50 Every test run was performed for a linear kernel, a polynomial kernel of degree 2 and an RBF kernel with σ = 30. [sent-356, score-0.264]
</p><p>51 Eight operations were identiﬁed where a Matlab implementation of Algorithm 1 spends the bulk of its runtime (varying from 75% to 95% depending on a digit). [sent-358, score-0.36]
</p><p>52 Another relatively expensive operation is augmentation of a kernel matrix with a kernel row. [sent-361, score-0.374]
</p><p>53 Figure 1 shows proportions of runtime spent in the most expensive operations for the digit 8. [sent-362, score-0.342]
</p><p>54 (16) : update of matrix Q Line 13 : kernel matrix augmentation Line 3, Eq. [sent-368, score-0.365]
</p><p>55 (9) : matrix creation from kernel matrix Line 1/3 : kernel calculation Line 1/3 : matrix creation from input space  Figure 1: Proﬁling results for digit 8 (MNIST). [sent-371, score-0.664]
</p><p>56 The analysis clearly shows that memory access operations dominate the runtime shown in Figure 1. [sent-372, score-0.396]
</p><p>57 3 Organization of Matrix Storage and Arithmetic Computations Having found the weak spots in the Matlab implementation of the incremental SVM, we will now consider the possibilities for the efﬁciency improvement. [sent-379, score-0.324]
</p><p>58 Furthermore, by storing a kernel matrix in a row-major format (a) additions of kernel rows can be carried out without memory relocation, and (b) selections in the matrix Krs are somewhat optimized since r ≫ s. [sent-391, score-0.471]
</p><p>59 To summarize our design, by using the row-major storage of (transposed) matrix K and the gaxpy-type matrix-vector multiplication selection can be avoided by at a cost of extra O(ss) arithmetic operations and performing a selection on a resulting vector. [sent-419, score-0.507]
</p><p>60 1924  I NCREMENTAL S UPPORT V ECTOR L EARNING  portion of runtime  linear kernel 1  0. [sent-423, score-0.378]
</p><p>61 (16) : update of matrix Q Line 13 : kernel matrix augmentation Line 3, Eq. [sent-428, score-0.365]
</p><p>62 (9) : gamma calculation by gaxpy loop Line 1/3 : kernel calculation without matrix creation  Figure 3: C++ runtime proportion digit 8 (MNIST) 5. [sent-430, score-0.63]
</p><p>63 4 Experimental Evaluation of the New Design The main goal of the experiments to be presented in this section is to evaluate the impact of the new design of storage and arithmetic operations on the overall performance of incremental SVM learning. [sent-431, score-0.651]
</p><p>64 In particular, the following issues are to be investigated: • How is the runtime proﬁle of the main operations affected by the new design? [sent-432, score-0.309]
</p><p>65 • How does the overall runtime of the new design scale with an increasing size of a training set? [sent-433, score-0.29]
</p><p>66 The selection was necessary in a Matlab implementation due to one-dimensional storage – a temporary matrix had to be created in order to represent a sub-matrix of the data. [sent-441, score-0.245]
</p><p>67 However, the relative cost of the gaxpy computation in the new design remains as high as the relative cost of the combined matrix creation / matrix-vector multiplication operations in the Matlab implementation. [sent-445, score-0.41]
</p><p>68 Matrix augmentation in Line 13 takes place at virtually no computational cost – due to rowmajor storage of the kernel matrix. [sent-448, score-0.276]
</p><p>69 The overall cost distribution of main operations remains largely the same in the new design, kernel computation having the largest weight for small training sets and gamma computation – for the large training sets. [sent-451, score-0.362]
</p><p>70 Four implementations of incremental SVM are compared: the original Matlab implementation of C&P; (with leave-one-out error estimation turned off), the Matlab implementation of Algorithm 1, the C++ implementation of Algorithms 1 & 3 and the C++ implementation of Algorithms 1 & 4. [sent-455, score-0.477]
</p><p>71 The results for the polynomial kernel are similar to the linear kernel and are not shown. [sent-459, score-0.264]
</p><p>72 The RBF kernel is more difﬁcult for training than the linear kernel for the MNIST data set, which is reﬂected by a larger proportion of support vectors (on average 15% for the RBF kernel compared to 5% with the linear kernel, at 10000 training examples). [sent-462, score-0.464]
</p><p>73 The Matlab implementation of Algorithm 1 was able to crank about 15000 examples with the RBF kernel, whereas the C++ implementation succeeded to learn 28000 examples, before running out of memory for storing the kernel matrix and the auxiliary data structures (at about 3GB). [sent-464, score-0.349]
</p><p>74 Applications As it was mentioned in the introduction, various applications of incremental SVM learning can be foreseen. [sent-468, score-0.273]
</p><p>75 2 Active Learning Another promising application of incremental SVM is active learning. [sent-500, score-0.389]
</p><p>76 Obviously, a better way to proceed is by applying incremental learning as presented in this paper. [sent-516, score-0.273]
</p><p>77 In the remaining part of this section experiments will be presented that prove the usefulness of active learning in the intrusion detection context. [sent-517, score-0.234]
</p><p>78 Since the observed data can contain thousands and even millions of examples it is clear that the problem can be addressed only using incremental learning. [sent-518, score-0.273]
</p><p>79 The incremental SVM was run with the linear kernel and the RBF kernel with σ = 30. [sent-522, score-0.537]
</p><p>80 Assume that we can run an anomaly detection tool over our data set which ranks all the points according to 12. [sent-528, score-0.235]
</p><p>81 html  1930  I NCREMENTAL S UPPORT V ECTOR L EARNING  ROC curves, training set, linear kernel  ROC curves, training set, RBF kernel  1  1  0. [sent-532, score-0.332]
</p><p>82 7  near boundary largest positive anomaly score  0. [sent-537, score-0.24]
</p><p>83 It is now the goal to use active learning to see if a ROC curve of anomaly detection can be improved. [sent-588, score-0.313]
</p><p>84 The ROC curves for active learning with the two heuristics and for anomaly detection are shown in Figure 9. [sent-596, score-0.347]
</p><p>85 The “largest positive” rule attains the highest true positive rate during the active learning phase, but does not perform signiﬁcantly better than anomaly detection during the classiﬁcation phase (i > 60). [sent-598, score-0.356]
</p><p>86 On the contrary, the “near boundary” rule is close or worse than anomaly detection during the learning phase but exhibit a sharp increase of the true positive rate after moving to classiﬁcation mode. [sent-599, score-0.24]
</p><p>87 Similar behavior 1931  ¨ ¨ L ASKOV, G EHL , K R UGER AND M ULLER  ROC curves, test set, linear kernel  ROC curves, test set, RBF kernel  0. [sent-603, score-0.264]
</p><p>88 2 SVM all SVM reduced near boundary largest positive anomaly score  0. [sent-618, score-0.24]
</p><p>89 8  SVM all SVM reduced near boundary largest positive anomaly score  0. [sent-624, score-0.24]
</p><p>90 The ROC curves of active learning, supervised learning and anomaly detection on test data are shown in Figure 10. [sent-641, score-0.347]
</p><p>91 (1998); Saad (1998)) have a well established online learning toolbox for optimization, incremental learning techniques for Support Vector Machines have been only recently developed (Cauwenberghs and Poggio, 2001; Tax and Laskov, 2003; Martin, 2002; Ma et al. [sent-658, score-0.367]
</p><p>92 The current paper contributes two-fold to the ﬁeld of incremental SVM learning. [sent-660, score-0.273]
</p><p>93 The convergence analysis of the algorithm has been performed showing that immediate cycling of the algorithm is impossible provided a kernel matrix is positive semi-deﬁnite. [sent-661, score-0.265]
</p><p>94 Furthermore, we propose a better scheme for organization of memory and arithmetic operations in exact incremental SVM using the gaxpy-type updates of the sensitivity vector. [sent-662, score-0.618]
</p><p>95 The achieved performance gains open wide possibilities for application of incremental SVM to various practical problems. [sent-664, score-0.273]
</p><p>96 Potential applications of incremental SVM learning include, among others, drug discovery, intrusion detection, network surveillance, monitoring of non-stationary time series etc. [sent-666, score-0.423]
</p><p>97 It is interesting to compare exact incremental learning to recently proposed alternative approaches to online learning. [sent-670, score-0.367]
</p><p>98 (2005) presents an online algorithm for L1 SVM, in which a very close approximation of the exact solution is built online before the last gap is bridged in the REPROCESS phase in an ofﬂine fashion. [sent-672, score-0.231]
</p><p>99 The major limilation of the exact incremental learning is its memory requirement, since the set of support vectors must be retained in memory during the entire learning. [sent-682, score-0.381]
</p><p>100 Future work will include further investigation of properties of incremental SVM such as numerical stability and their utility for tracking the values of generalization bounds. [sent-685, score-0.273]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kss', 0.289), ('incremental', 0.273), ('svm', 0.21), ('askov', 0.202), ('ehl', 0.202), ('uger', 0.202), ('runtime', 0.191), ('ncremental', 0.159), ('uller', 0.154), ('improvment', 0.144), ('ector', 0.143), ('upport', 0.143), ('yc', 0.143), ('ys', 0.14), ('kernel', 0.132), ('gc', 0.131), ('kcs', 0.13), ('anomaly', 0.126), ('operations', 0.118), ('active', 0.116), ('murata', 0.115), ('rbf', 0.104), ('arithmetic', 0.1), ('storage', 0.095), ('matlab', 0.094), ('online', 0.094), ('gr', 0.091), ('kcr', 0.087), ('ksk', 0.087), ('kkk', 0.086), ('yt', 0.082), ('laskov', 0.08), ('roc', 0.079), ('yr', 0.077), ('ss', 0.075), ('yk', 0.075), ('sensitivity', 0.073), ('cycling', 0.072), ('krs', 0.072), ('detection', 0.071), ('drug', 0.07), ('minor', 0.067), ('creation', 0.066), ('tax', 0.066), ('design', 0.065), ('update', 0.062), ('matrix', 0.061), ('multiplication', 0.057), ('pro', 0.057), ('earning', 0.056), ('portion', 0.055), ('memory', 0.054), ('calculation', 0.052), ('implementation', 0.051), ('augmentation', 0.049), ('gradients', 0.048), ('ller', 0.048), ('intrusion', 0.047), ('ling', 0.047), ('xc', 0.047), ('resources', 0.045), ('equilibrium', 0.044), ('bottlenecks', 0.044), ('fhg', 0.044), ('line', 0.044), ('largest', 0.044), ('gaxpy', 0.043), ('kks', 0.043), ('ksr', 0.043), ('surveillance', 0.043), ('thrown', 0.043), ('phase', 0.043), ('inverse', 0.042), ('orr', 0.04), ('entering', 0.04), ('limited', 0.04), ('lecun', 0.039), ('selection', 0.038), ('points', 0.038), ('boundary', 0.038), ('bordes', 0.037), ('cauwenberghs', 0.037), ('christian', 0.037), ('gi', 0.037), ('zt', 0.036), ('warmuth', 0.036), ('security', 0.035), ('training', 0.034), ('curves', 0.034), ('window', 0.033), ('monitoring', 0.033), ('likewise', 0.033), ('digit', 0.033), ('access', 0.033), ('near', 0.032), ('inclusion', 0.032), ('sec', 0.031), ('classi', 0.031), ('batch', 0.031), ('carried', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="38-tfidf-1" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>2 0.10768165 <a title="38-tfidf-2" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>3 0.10611251 <a title="38-tfidf-3" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Katya Scheinberg</p><p>Abstract: We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efﬁcient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims’ SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difﬁcult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight , the generalization properties of these two approaches are identical and we do not discuss them in the paper. Keywords: active set methods, support vector machines, quadratic programming</p><p>4 0.10283192 <a title="38-tfidf-4" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>Author: Hichem Sahbi, Donald Geman</p><p>Abstract: We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly ﬁner subsets. The hierarchy is traversed coarse-to-ﬁne and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This “free network” is then perturbed, cell by cell, into another network, which is “graded” in two ways: ﬁrst, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-speciﬁc SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network. Keywords: statistical learning, hierarchy of classiﬁers, coarse-to-ﬁne computation, support vector machines, face detection</p><p>5 0.10182737 <a title="38-tfidf-5" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>Author: S. V. N. Vishwanathan, Nicol N. Schraudolph, Alex J. Smola</p><p>Abstract: This paper presents an online support vector machine (SVM) that uses the stochastic meta-descent (SMD) algorithm to adapt its step size automatically. We formulate the online learning problem as a stochastic gradient descent in reproducing kernel Hilbert space (RKHS) and translate SMD to the nonparametric setting, where its gradient trace parameter is no longer a coefﬁcient vector but an element of the RKHS. We derive efﬁcient updates that allow us to perform the step size adaptation in linear time. We apply the online SVM framework to a variety of loss functions, and in particular show how to handle structured output spaces and achieve efﬁcient online multiclass classiﬁcation. Experiments show that our algorithm outperforms more primitive methods for setting the gradient step size. Keywords: online SVM, stochastic meta-descent, structured output spaces</p><p>6 0.088874541 <a title="38-tfidf-6" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.088731147 <a title="38-tfidf-7" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.08813379 <a title="38-tfidf-8" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.083831705 <a title="38-tfidf-9" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>10 0.079654977 <a title="38-tfidf-10" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>11 0.07476826 <a title="38-tfidf-11" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>12 0.072080113 <a title="38-tfidf-12" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>13 0.066561028 <a title="38-tfidf-13" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.066167466 <a title="38-tfidf-14" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>15 0.065566763 <a title="38-tfidf-15" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.057623323 <a title="38-tfidf-16" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>17 0.055689551 <a title="38-tfidf-17" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>18 0.055098962 <a title="38-tfidf-18" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>19 0.053680643 <a title="38-tfidf-19" href="./jmlr-2006-Considering_Cost_Asymmetry_in_Learning_Classifiers.html">22 jmlr-2006-Considering Cost Asymmetry in Learning Classifiers</a></p>
<p>20 0.052817989 <a title="38-tfidf-20" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.291), (1, -0.046), (2, 0.218), (3, 0.134), (4, 0.085), (5, 0.019), (6, -0.108), (7, -0.125), (8, -0.028), (9, -0.056), (10, 0.066), (11, -0.028), (12, 0.123), (13, -0.078), (14, 0.06), (15, 0.062), (16, 0.069), (17, 0.052), (18, 0.025), (19, 0.075), (20, 0.076), (21, -0.17), (22, 0.065), (23, 0.21), (24, -0.026), (25, 0.096), (26, 0.045), (27, 0.058), (28, 0.082), (29, 0.149), (30, -0.017), (31, -0.042), (32, 0.09), (33, 0.045), (34, 0.015), (35, -0.027), (36, -0.053), (37, -0.01), (38, -0.036), (39, 0.028), (40, -0.0), (41, 0.029), (42, 0.038), (43, 0.006), (44, 0.024), (45, 0.036), (46, 0.032), (47, 0.017), (48, -0.028), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93826705 <a title="38-lsi-1" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>2 0.73782122 <a title="38-lsi-2" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Katya Scheinberg</p><p>Abstract: We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efﬁcient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims’ SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difﬁcult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight , the generalization properties of these two approaches are identical and we do not discuss them in the paper. Keywords: active set methods, support vector machines, quadratic programming</p><p>3 0.63322598 <a title="38-lsi-3" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: S. Sathiya Keerthi, Olivier Chapelle, Dennis DeCoste</p><p>Abstract: Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classiﬁcation speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily ﬁnds a set of kernel basis functions of a speciﬁed maximum size (dmax ) to approximate the SVM primal cost function well; (3) it is 2 efﬁcient and roughly scales as O(ndmax ) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors. Keywords: SVMs, classiﬁcation, sparse design</p><p>4 0.59040254 <a title="38-lsi-4" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>Author: Hichem Sahbi, Donald Geman</p><p>Abstract: We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly ﬁner subsets. The hierarchy is traversed coarse-to-ﬁne and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This “free network” is then perturbed, cell by cell, into another network, which is “graded” in two ways: ﬁrst, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-speciﬁc SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network. Keywords: statistical learning, hierarchy of classiﬁers, coarse-to-ﬁne computation, support vector machines, face detection</p><p>5 0.56018084 <a title="38-lsi-5" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>Author: S. V. N. Vishwanathan, Nicol N. Schraudolph, Alex J. Smola</p><p>Abstract: This paper presents an online support vector machine (SVM) that uses the stochastic meta-descent (SMD) algorithm to adapt its step size automatically. We formulate the online learning problem as a stochastic gradient descent in reproducing kernel Hilbert space (RKHS) and translate SMD to the nonparametric setting, where its gradient trace parameter is no longer a coefﬁcient vector but an element of the RKHS. We derive efﬁcient updates that allow us to perform the step size adaptation in linear time. We apply the online SVM framework to a variety of loss functions, and in particular show how to handle structured output spaces and achieve efﬁcient online multiclass classiﬁcation. Experiments show that our algorithm outperforms more primitive methods for setting the gradient step size. Keywords: online SVM, stochastic meta-descent, structured output spaces</p><p>6 0.501324 <a title="38-lsi-6" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.47583872 <a title="38-lsi-7" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.42664972 <a title="38-lsi-8" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.42012671 <a title="38-lsi-9" href="./jmlr-2006-One-Class_Novelty_Detection_for_Seizure_Analysis_from_Intracranial_EEG.html">69 jmlr-2006-One-Class Novelty Detection for Seizure Analysis from Intracranial EEG</a></p>
<p>10 0.41295463 <a title="38-lsi-10" href="./jmlr-2006-Considering_Cost_Asymmetry_in_Learning_Classifiers.html">22 jmlr-2006-Considering Cost Asymmetry in Learning Classifiers</a></p>
<p>11 0.40934649 <a title="38-lsi-11" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.39086944 <a title="38-lsi-12" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>13 0.38375556 <a title="38-lsi-13" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.38296872 <a title="38-lsi-14" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>15 0.35494062 <a title="38-lsi-15" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>16 0.34636003 <a title="38-lsi-16" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>17 0.32526883 <a title="38-lsi-17" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>18 0.32157713 <a title="38-lsi-18" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>19 0.30735803 <a title="38-lsi-19" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>20 0.29466802 <a title="38-lsi-20" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.012), (36, 0.048), (50, 0.031), (63, 0.595), (76, 0.019), (78, 0.016), (81, 0.025), (84, 0.011), (90, 0.04), (91, 0.026), (96, 0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95534301 <a title="38-lda-1" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>2 0.9232325 <a title="38-lda-2" href="./jmlr-2006-On_the_Complexity_of_Learning_Lexicographic_Strategies.html">68 jmlr-2006-On the Complexity of Learning Lexicographic Strategies</a></p>
<p>Author: Michael Schmitt, Laura Martignon</p><p>Abstract: Fast and frugal heuristics are well studied models of bounded rationality. Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources. Take-the-best searches for a sufﬁciently good ordering of cues (or features) in a task where objects are to be compared lexicographically. We investigate the computational complexity of ﬁnding optimal cue permutations for lexicographic strategies and prove that the problem is NP-complete. It follows that no efﬁcient (that is, polynomial-time) algorithm computes optimal solutions, unless P = NP. We further analyze the complexity of approximating optimal cue permutations for lexicographic strategies. We show that there is no efﬁcient algorithm that approximates the optimum to within any constant factor, unless P = NP. The results have implications for the complexity of learning lexicographic strategies from examples. They show that learning them in polynomial time within the model of agnostic probably approximately correct (PAC) learning is impossible, unless RP = NP. We further consider greedy approaches for building lexicographic strategies and determine upper and lower bounds for the performance ratio of simple algorithms. Moreover, we present a greedy algorithm that performs provably better than take-the-best. Tight bounds on the sample complexity for learning lexicographic strategies are also given in this article. Keywords: bounded rationality, fast and frugal heuristic, PAC learning, NP-completeness, hardness of approximation, greedy method</p><p>3 0.78573972 <a title="38-lda-3" href="./jmlr-2006-Linear_State-Space_Models_for_Blind_Source_Separation.html">57 jmlr-2006-Linear State-Space Models for Blind Source Separation</a></p>
<p>Author: Rasmus Kongsgaard Olsson, Lars Kai Hansen</p><p>Abstract: We apply a type of generative modelling to the problem of blind source separation in which prior knowledge about the latent source signals, such as time-varying auto-correlation and quasiperiodicity, are incorporated into a linear state-space model. In simulations, we show that in terms of signal-to-error ratio, the sources are inferred more accurately as a result of the inclusion of strong prior knowledge. We explore different schemes of maximum-likelihood optimization for the purpose of learning the model parameters. The Expectation Maximization algorithm, which is often considered the standard optimization method in this context, results in slow convergence when the noise variance is small. In such scenarios, quasi-Newton optimization yields substantial improvements in a range of signal to noise ratios. We analyze the performance of the methods on convolutive mixtures of speech signals. Keywords: blind source separation, state-space model, independent component analysis, convolutive model, EM, speech modelling</p><p>4 0.60899311 <a title="38-lda-4" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tobias Glasmachers, Christian Igel</p><p>Abstract: Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is signiﬁcantly faster. Keywords: working set selection, sequential minimal optimization, quadratic programming, support vector machines, large scale optimization</p><p>5 0.52914381 <a title="38-lda-5" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Katya Scheinberg</p><p>Abstract: We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efﬁcient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims’ SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difﬁcult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight , the generalization properties of these two approaches are identical and we do not discuss them in the paper. Keywords: active set methods, support vector machines, quadratic programming</p><p>6 0.52618551 <a title="38-lda-6" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>7 0.48150808 <a title="38-lda-7" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>8 0.46456286 <a title="38-lda-8" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.45950553 <a title="38-lda-9" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>10 0.45553163 <a title="38-lda-10" href="./jmlr-2006-Computational_and_Theoretical_Analysis_of__Null_Space__and_Orthogonal_Linear_Discriminant_Analysis.html">21 jmlr-2006-Computational and Theoretical Analysis of  Null Space  and Orthogonal Linear Discriminant Analysis</a></p>
<p>11 0.45006496 <a title="38-lda-11" href="./jmlr-2006-Noisy-OR_Component_Analysis_and_its_Application_to_Link_Analysis.html">64 jmlr-2006-Noisy-OR Component Analysis and its Application to Link Analysis</a></p>
<p>12 0.44756073 <a title="38-lda-12" href="./jmlr-2006-Learning_Image_Components_for_Object_Recognition.html">47 jmlr-2006-Learning Image Components for Object Recognition</a></p>
<p>13 0.43709654 <a title="38-lda-13" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>14 0.43631241 <a title="38-lda-14" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.43623346 <a title="38-lda-15" href="./jmlr-2006-Toward_Attribute_Efficient_Learning_of_Decision_Lists_and_Parities.html">92 jmlr-2006-Toward Attribute Efficient Learning of Decision Lists and Parities</a></p>
<p>16 0.43527678 <a title="38-lda-16" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.43299511 <a title="38-lda-17" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>18 0.43231818 <a title="38-lda-18" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.42033568 <a title="38-lda-19" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>20 0.41511098 <a title="38-lda-20" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
