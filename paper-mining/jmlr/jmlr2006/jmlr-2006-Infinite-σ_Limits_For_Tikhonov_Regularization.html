<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-40" href="#">jmlr2006-40</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</h1>
<br/><p>Source: <a title="jmlr-2006-40-pdf" href="http://jmlr.org/papers/volume7/lippert06a/lippert06a.pdf">pdf</a></p><p>Author: Ross A. Lippert, Ryan M. Rifkin</p><p>Abstract: We consider the problem of Tikhonov regularization with a general convex loss function: this formalism includes support vector machines and regularized least squares. For a family of kernels that includes the Gaussian, parameterized by a “bandwidth” parameter σ, we characterize the limiting ˜ solution as σ → ∞. In particular, we show that if we set the regularization parameter λ = λσ−2p , the regularization term of the Tikhonov problem tends to an indicator function on polynomials of degree ⌊p⌋ (with residual regularization in the case where p ∈ Z). The proof rests on two key ideas: epi-convergence, a notion of functional convergence under which limits of minimizers converge to minimizers of limits, and a value-based formulation of learning, where we work with regularization on the function output values (y) as opposed to the function expansion coefﬁcients in the RKHS. Our result generalizes and uniﬁes previous results in this area. Keywords: Tikhonov regularization, Gaussian kernel, theory, kernel machines</p><p>Reference: <a title="jmlr-2006-40-reference" href="../jmlr2006_reference/jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 145 Tremont Street Boston, MA 02111, USA  Editor: Gabor Lugosi  Abstract We consider the problem of Tikhonov regularization with a general convex loss function: this formalism includes support vector machines and regularized least squares. [sent-7, score-0.329]
</p><p>2 For a family of kernels that includes the Gaussian, parameterized by a “bandwidth” parameter σ, we characterize the limiting ˜ solution as σ → ∞. [sent-8, score-0.144]
</p><p>3 In particular, we show that if we set the regularization parameter λ = λσ−2p , the regularization term of the Tikhonov problem tends to an indicator function on polynomials of degree ⌊p⌋ (with residual regularization in the case where p ∈ Z). [sent-9, score-0.626]
</p><p>4 One popular family of techniques is Tikhonov regularization in a Reproducing Kernel Hilbert Space (RKHS) (Evgeniou et al. [sent-18, score-0.154]
</p><p>5 , 2000): n  inf  f ∈H  ˆ nλ|| f ||2 + ∑ v( f (xi ), yi ) . [sent-19, score-0.22]
</p><p>6 κ i=1  Here, v : R × R → R is a loss function indicating the price we pay when we see xi , predict f (xi ), and the true value is yi . [sent-20, score-0.164]
</p><p>7 The regularization constant, λ > 0, controls the trade-off between ﬁtting the training set accurately (minimizing the penalties) and forcing f to be smooth in H . [sent-22, score-0.154]
</p><p>8 L IPPERT AND R IFKIN  the Tikhonov regularization can be written in the form n  f (x) = ∑ ci κ(xi , x). [sent-28, score-0.201]
</p><p>9 i=1  In practice, solving a Tikhonov regularization problem is equivalent to ﬁnding the expansion coefﬁcients ci . [sent-29, score-0.268]
</p><p>10 ′ 2  − ||x−x ||  One popular choice for κ is the Gaussian kernel κσ (x, x′ ) = e 2σ2 , where σ is the bandwidth of the Gaussian. [sent-30, score-0.133]
</p><p>11 Common choices for v include the square loss, v(y, y) = (y − y)2 , and the hinge ˆ ˆ loss, v(y, y) = max{0, 1 − yy}, which lead to regularized least squares and support vector machines, ˆ ˆ respectively. [sent-31, score-0.131]
</p><p>12 Our work was originally motivated by the empirical observation that on a range of tasks, regularized least squares achieved very good performance with very large σ. [sent-32, score-0.131]
</p><p>13 Regularized least squares (RLS) is an especially simple Tikhonov regularization algorithm: “training” RLS simply involves solving a system of linear equations. [sent-37, score-0.196]
</p><p>14 ˆ In Lippert and Rifkin (2006), we studied the limit of this expression as σ → ∞, showing that ˜ if we set λ = λσ−2p−1 for p a positive integer, the inﬁnite-σ limit converges (pointwise) to the degree p polynomial with minimal empirical risk on the training set. [sent-40, score-0.251]
</p><p>15 The asymptotic predictions are equivalent to those we would get if we simply ﬁt an (unregularized) degree p polynomial to our training data. [sent-41, score-0.135]
</p><p>16 In the current work, we unify and generalize these results, showing that the occurrence of these polynomial approximation limits is a general phenomenon, which holds across all convex loss functions and a wide variety of kernels taking the form κσ (x, x′ ) = κ(x/σ, x′ /σ). [sent-44, score-0.307]
</p><p>17 Our main result is that ˜ for a convex loss function and a valid kernel, if we take σ → ∞ and λ = λσ−2p , the regularization term of the Tikhonov problem tends to an indicator function on polynomials of degree ⌊p⌋. [sent-45, score-0.363]
</p><p>18 In the case where p ∈ Z, there is residual regularization on the degree-p coefﬁcients of the limiting polynomial. [sent-46, score-0.289]
</p><p>19 The ﬁrst is the notion of epi-convergence, a functional convergence under which limits of minimizers converge to minimizers of limits. [sent-48, score-0.378]
</p><p>20 This notion allows us to characterize the limiting Tikhonov regularization problem in a mathematically precise way. [sent-49, score-0.248]
</p><p>21 The idea is that instead of working with the expansion coefﬁcients in the RKHS (ci ), we can write the regularization problem directly in terms of the predicted values (yi ). [sent-51, score-0.221]
</p><p>22 Tikhonov regularization is given by n  inf  f ∈H  ˆ nλ|| f ||2 + ∑ v( f (xi ), yi ) . [sent-64, score-0.374]
</p><p>23 κ  (1)  i=1  Tikhonov regularization can be used for both classiﬁcation and regression tasks, but we refer to the function f as the regularized solution in all cases. [sent-65, score-0.243]
</p><p>24 We call the left-hand portion the regularization term, and the right-hand portion the loss term. [sent-66, score-0.195]
</p><p>25 Aside from convexity, we will be unconcerned with the form of the loss function and often take the loss term in the optimization in (1) to be some convex function V : Rn → R which is minimized by the vector y of yi ’s. [sent-69, score-0.207]
</p><p>26 Additionally, our main result requires not only genericity of the data, but also that n > d p where p is the degree of the asymptotic regularized solution. [sent-102, score-0.147]
</p><p>27 We say that a kernel is valid if every ﬁnite upper-left submatrix of M is symmetric and positive deﬁnite; in this case, we also say that the inﬁnite matrix M is symmetric positive deﬁnite. [sent-111, score-0.155]
</p><p>28 Lemma 1 If κ(x, x′ ) = ∑c≥0 (x·x′ )c gc (x)gc (x′ ) for some analytic functions gc (x) such that gc (0) = 0, then κ is a valid kernel. [sent-117, score-0.351]
</p><p>29 1 We note that κ(x, x′ ) = exp(− 2 ||x − x′ ||2 ) can be written in the form of Lemma 1: 1 1 κ(x, x′ ) = exp − ||x||2 exp(x · x′ ) exp − ||x′ ||2 2 2 ∞ ′ )c 1 1 (x · x exp − ||x||2 exp − ||x′ ||2 = ∑ c! [sent-122, score-0.132]
</p><p>30 2 2 c=0 ∞  =  ∑ (x · x′ )c gc (x)gc (x′ ),  c=0  1 where gc (x) = √c! [sent-123, score-0.234]
</p><p>31 2 We will consider kernel functions κσ which are parametrized by a bandwidth parameter σ (or 1 s = σ ). [sent-125, score-0.133]
</p><p>32 Using the representer theorem and basic facts about RKHS, the standard Tikhonov regularization problem (1) can be written in terms of the expansion coefﬁcients c and the kernel matrix K: n  ˆ infn nλct Kc + ∑ v([Kc]i , yi ) . [sent-130, score-0.38]
</p><p>33 If the kernel matrix K is invertible (which is the case for a Gaussian kernel and a generic data set), then c = K −1 y, and we can rewrite the minimization as inf nλyt K −1 y +V (y) . [sent-132, score-0.332]
</p><p>34 ˆ i=1 While problem 2 is explicit in the coefﬁcients of the expansion of the regularized solution, problem 3 is explicit in the predicted values yi . [sent-134, score-0.236]
</p><p>35 The purpose behind our choice of formulation is to avoid the unnecessary complexities which result from replacing κ with κσ and taking limits as both ci and κσ (x, xi ) change separately with σ: note that in problem 3, only the regularization term is varying with σ. [sent-135, score-0.338]
</p><p>36 In this section, we will show how our formulation achieves this, by allowing us to state a single optimization problem which simultaneously solves the Tikhonov regularization problem on the training data and evaluates the resulting function on the test data. [sent-136, score-0.154]
</p><p>37 For any V : Rn → R, if y minimizes ˙  Theorem 2 Let y =  K00 K10  K01 K11  ∈  yt K −1 y +V (y1 )  (4)  −1 yt1 K11 y1 +V (y1 )  (5)  inf {yt K −1 y +V (y1 )} = inf{inf{yt K −1 y} +V (y1 )}. [sent-138, score-0.232]
</p><p>38 Thus, ˙ ¯ Let K −1 = K =  −1 ¯ ¯ ¯ −1 ¯ inf yt K −1 y = yt1 (K11 − K10 K00 K01 )y1 = yt1 K11 y1 y0  by (19) of Lemma 15. [sent-143, score-0.232]
</p><p>39 When taking limits, we are going to work directly with the yi , and we are going to avoid dealing with the (divergent) limits of the ci . [sent-155, score-0.221]
</p><p>40 Since the component of our objective which depends on the limiting parameter is a quadratic form, we will eventually specialize the results to quadratic forms. [sent-160, score-0.182]
</p><p>41 Deﬁnition 4 (epigraphs) Given a function f : Rn → (−∞, ∞], its epigraph, epi f is the subset of R × Rn given by epi f = {(z, x) : z ≥ f (x)}. [sent-161, score-0.336]
</p><p>42 We call f closed, convex, or proper if those statements are true of epi f (proper referring to epi f / being neither 0 nor Rn+1 ). [sent-162, score-0.336]
</p><p>43 Additionally, since we will be studying parameterized functions, fs , for 0 < s as s → 0, we say that such a family of functions is eventually convex (or closed, or proper) when there exists some s0 > 0 such that fs is convex (or closed, or proper) for all 0 < s < s0 . [sent-165, score-1.014]
</p><p>44 We review the deﬁnition of lim inf and lim sup for functions of a single variable. [sent-166, score-0.505]
</p><p>45 Deﬁnition 5 For h : (0, ∞) → (−∞, ∞], lim inf h(s) = sup s→0  s>0  lim sup h(s) = inf  s>0  s→0  inf {h(s′ )}  s′ ∈(0,s)  sup {h(s′ )} . [sent-168, score-0.863]
</p><p>46 A useful alternate characterization, which is immediate from the deﬁnition, is lim infs→0 h(s) = h0 iff ∀ε > 0, ∃s0 , ∀s ∈ (0, s0 ) : h(s) ≥ h0 − ε, and lim sups→0 h(s) = h0 iff ∀ε > 0, ∃s0 , ∀s ∈ (0, s0 ) : h(s) ≤ h0 + ε, where either inequality can be strict if h0 < ∞. [sent-170, score-0.326]
</p><p>47 Deﬁnition 6 (epi-limits) We say lims→0 fs = f if for all x0 ∈ Rn , both the following properties hold: Property 1: ∀x : [0, ∞) → Rn continuous at x(0) = x0 satisﬁes lim inf fs (x(s)) ≥ f (x0 ) s→0  (7)  Property 2: ∃x : [0, ∞) → Rn continuous at x(0) = x0 satisfying lim sup fs (x(s)) ≤ f (x0 ). [sent-171, score-1.891]
</p><p>48 s→0  861  (8)  L IPPERT AND R IFKIN  exists doesn’t exist  Figure 1: (property 1) of Deﬁnition 6 says that paths of points within epi fs cannot end up below epi f , while (property 2) says that at least one such path hits every point of epi f . [sent-172, score-0.966]
</p><p>49 This notion of functional limit is called an epigraphical limit (or epi-limit). [sent-173, score-0.153]
</p><p>50 Less formally, (property 1) is the condition that paths of the form (x(s), fs (x(s))) are, asymptotically, inside epi f , while (property 2) asserts the existence of a path which hits the boundary of epi f , as depicted in ﬁgure 1. [sent-174, score-0.798]
</p><p>51 Considering (property 1) with the function x(s) = x0 , it is clear that the epigraphical limit minorizes the pointwise limit (assuming both exist), but the two need not coincide. [sent-175, score-0.204]
</p><p>52 An example of this distinction is given by the family of functions 2 fs (x) = x(x − s) + 1, s illustrated by Figure 2. [sent-176, score-0.462]
</p><p>53 ) The pointwise and epi-limits of quadratic forms agree when the limiting quadratic form is ﬁnite, but the example in the ﬁgure is not of that sort. [sent-181, score-0.233]
</p><p>54 Theorem 7 Let fs : Rn → (−∞, ∞] be eventually ccp, with lims→0 fs = f . [sent-185, score-0.924]
</p><p>55 If fs , f have unique minimizers x(s), x then ˙ ˙ lim x(s) = x and ˙ ˙  s→0  lim fs (x(s)) = inf f (x). [sent-186, score-1.532]
</p><p>56 Let s0 > 0 be such that ˆ ˆ ˙ ∀s ∈ (0, s0 ) : fs (x(s)) < f (x) + δ and x(s) ∈ Bδ . [sent-191, score-0.462]
</p><p>57 Thus, ∀s ∈ (0, s0 ) : infx∈Bδ fs (x) < f (x) + δ. [sent-192, score-0.462]
</p><p>58 015625  Figure 2: The function above, fs (x) = 2 x(x − s) + 1, has different pointwise and epi-limits, having s values 1 and 0, respectively, at x = 0 and ∞ for all other x. [sent-196, score-0.513]
</p><p>59 863  L IPPERT AND R IFKIN  By property 1 of deﬁnition 6, ∀x ∈ Rn , lim infs→0 fs (x) ≥ f (x), in particular, ∀x ∈ ∂Bδ , ∃s1 ∈ (0, s0 ), ∀s ∈ (0, s1 ) : fs (x) ≥ f (x) − δ = f (x) + δ. [sent-197, score-1.087]
</p><p>60 Since ∂Bδ is compact, we can choose s1 ∈ ˙ (0, s0 ), ∀x ∈ ∂Bδ , s ∈ (0, s1 ) : fs (x) ≥ f (x) + δ. [sent-198, score-0.462]
</p><p>61 ˙ Thus ∀x ∈ ∂Bδ , s < s1 : fs (x) ≥ f (x) + δ > infx∈Bδ fs (x), and therefore x(s) ∈ Bδ by the convexity ˙ ˙ of fs . [sent-199, score-1.386]
</p><p>62 In particular, ˙ ˙ lim sups→0 fs (x(s)) ≤ f (x) and f (x) ≤ lim infs→0 fs (x(s)). [sent-203, score-1.25]
</p><p>63 Since ∀s : fs (x(s)) ≤ fs (x(s)), we have ˆ ˙ ˙ ˙ ˙ ˆ lim sups→0 fs (x(s)) ≤ f (x) and hence f (x) ≤ lim infs→0 fs (x(s)) ≤ lim sups→0 fs (x(s)) ≤ f (x). [sent-204, score-2.799]
</p><p>64 ˙ ˙ ˙ ˙ ˙ ˙ We now apply this theorem to characterize limits of quadratic forms (which are becoming inﬁnite in the limit). [sent-205, score-0.138]
</p><p>65 If fs (x, y) =  x Z(s)−1 y  t  A(s) B(s)t B(s) C(s)  x Z(s)−1 y  then lims→0 fs = f , where f (x, y) =  ∞ xt (A(0) − B(0)t C(0)−1 B(0))x  y=0 . [sent-211, score-0.924]
</p><p>66 y=0  Proof Completing the square, 2 fs (x, y) = ||x||2 + ||Z(s)−1 y +C(s)−1 B(s)x||C(s) ˜ A(s) 2 ˜ ˜ where ||v||W = vt W v and A(s) = A(s) − B(s)t C(s)−1 B(s). [sent-212, score-0.462]
</p><p>67 2 c 6z(s)  Thus, for all s < s1 : fs (x(s), y(s)) > c ||y(0)|| , and hence lim infs→0 fs (x(s), y(s)) = ∞, which im4z(s) plies property 1 of deﬁnition 6 (and property 2, since lim inf ≤ lim sup). [sent-220, score-1.553]
</p><p>68 Otherwise (y(0) = 0), fs (x(s), y(s)) ≥ ||x(s)||2 and thus ˜ A(s) ≤ lim inf f (x(s), y(s)) lim ||x(s)||2 ˜ A(s) s→0  s→0  ||x(0)||2 ≤ lim inf f (x(s), y(s)) ˜ A(0) s→0  (property 1). [sent-221, score-1.231]
</p><p>69 fs (x(s), y(s)) = ||x(s)||2 when y(s) = −Z(s)C(s)−1 B(s)x(s) (which is continuous ˜ A(s) and vanishing at s = 0), and thus lim sup f (x(s), y(s)) = lim ||x(s)||2 = ||x(0)||2 ˜ ˜ A(s) A(0) s→0  s→0  (property 2). [sent-222, score-0.827]
</p><p>70 If t  Z1 (s)qa A(s) B(s)t   B(s) D(s) qb fs (qa , qb , qc ) =  −1 q Z2 (s) c C(s) E(s)   then lims→0 fs = f , where  f (qa , qb , qc ) =    C(s)t Z1 (s)qa  E(s)t   qb −1 q F(s) Z2 (s) c  ∞ qtb (D(0) − E(0)t F(0)−1 E(0))qb  865  qc = 0 . [sent-226, score-1.687]
</p><p>71 z=0  L IPPERT AND R IFKIN  Proof We apply Lemma 9 to the quadratic form given by  t  t t t qa Z1 AZ1 Z1 Bt Z1Ct  qb   BZ1 D Et −1 (CZ1 E ) F Z2 qc     qa   qb  −1 Z2 qc  (s dependence suppressed). [sent-227, score-0.668]
</p><p>72 We will have occasion to apply Corollary 10 when some of qa , qb and qc are empty. [sent-228, score-0.312]
</p><p>73 Kernel Expansions and Regularization Limits In this section, we present our key result, characterizing the asymptotic behavior of the regularization term of Tikhonov regularization. [sent-231, score-0.154]
</p><p>74 We deﬁne a family of quadratic forms on the polynomials in x; these forms will turn out to be the limits of the quadratic Tikhonov regularizer. [sent-232, score-0.247]
</p><p>75 For any p > 0, deﬁne RK : f → [0, ∞] by p 0 f (x) = ∑0≤i≤d⌊p⌋ qi xIi , if p ∈ Z / ∞ else  t   qd p−1 +1  qd p−1 +1  . [sent-234, score-0.164]
</p><p>76 Rp( f ) =   qd p qd p  ∞ else Rκ ( f ) = p  −1 where, for p ∈ Z, C = (Mbb − Mba Maa Mab )−1 where Maa and  Maa Mba  Mab Mbb  if p ∈ Z  are the d p−1 × d p−1  and d p × d p upper-left submatrices of K. [sent-240, score-0.199]
</p><p>77 Because our data set is generic, vα (X) is non-singular, and the interpolating polynomial through the points (xi , yi ) over the monomials {xIi : i < n} is given by f (x) = vα (x)vα (X)−1 y. [sent-245, score-0.206]
</p><p>78 We now state and prove our key result, showing the convergence of the regularization term of Tikhonov regularization to Rκ . [sent-246, score-0.308]
</p><p>79 Then lim fs = f ,  s→0  ˜ ˜ ˜ where f (y) = Rκ (q), and q(x) = vα (x)q = ∑0≤i < n, 0 ≤ j < ∞, the i, jth entry of χ(s) is s|I j+n |−|Ii | χi j , and |I j+n | − |Ii | ≥ 0. [sent-250, score-0.625]
</p><p>80 Summarizing, ˜ Therefore, lims→0 M(s) = ( I  ˜ χ(0) ) M  fs (y) = s2p yt κ(sX, sX)−1 y ˜ = (vα (X)−1 y)t (s p Σα (1/s))M(s)−1 (s p Σα (1/s))(vα (X)−1 y) t p −1 p ˜ = q (s Σα (1/s))M(s) (s Σα (1/s))q, ˜ ˜ where q ≡ vα (X)−1 y. [sent-255, score-0.554]
</p><p>81 If (hi = 0 or) qhi = 0, the limit is: ˜ ˜ −1 qtmi (Mmi,mi − Mmi,lo Mlo,lo Mlo,mi )−1 qmi , ˜ ˜  hence fs (y) → Rκ (q) for p ∈ Z. [sent-275, score-0.632]
</p><p>82 In other words, we can get polynomial behavior of degree ⌊p⌋ for any p, but we must have at least d⌊p⌋ = O(d ⌊p⌋ ) generic data points in order to do so. [sent-285, score-0.169]
</p><p>83 The Asymptotic Regularized Solution By Theorem 12, the regularization term (under certain conditions) becomes a penalty on degree 1 > p behavior of the regularized solution. [sent-301, score-0.301]
</p><p>84 Since the loss function is ﬁxed as σ, λ → ∞, the objective function in (1) approaches a limiting constrained optimization problem. [sent-302, score-0.135]
</p><p>85 Let f˙σ , f˙∞ ∈ H be the unique minimizers of n  ˆ nλ(σ)|| f ||2 σ + ∑ v( f (xi ), yi ) κ  (10)  i=1  and n  ˜ ˆ nλRκ ( f ) + ∑ v( f (xi ), yi ) p  (11)  i=1  respectively. [sent-306, score-0.302]
</p><p>86 Then ∀x0 ∈ Rd such that X0 =  x0 X  is generic,  lim f˙σ (x0 ) = f˙∞ (x0 ). [sent-307, score-0.163]
</p><p>87 ˙ ˙ ˙ ˙ ˙ ˜ ˆ ˙ Let g∞ (q) = nλRκ (q) + ∑n v(q(xi ), yi ) with minimizer q∞ . [sent-311, score-0.158]
</p><p>88 For κ strictly convex loss functions, such as the square loss used in regularized least squares, problem 11 will have a unique minimizer as well. [sent-315, score-0.294]
</p><p>89 In these cases, Theorem 12 still determines the value of the limiting solution, but Theorem 14 does not completely determine the limiting minimizer. [sent-317, score-0.188]
</p><p>90 33 of Rockafellar and Wets (2004) provides a generalization of Theorem 14 which applies when the minimizers are non-unique (and even when the objective functions are non-convex, as long as certain local convexity conditions hold). [sent-319, score-0.142]
</p><p>91 It can be shown that the minimizer of problem 10 will converge to one of the minimizers of problem 11, though not knowing which one, we cannot predict the limiting regularized solution. [sent-320, score-0.403]
</p><p>92 Additionally, we note that our work has focused on “standard” Tikhonov regularization problems, in which the function f is “completely” regularized. [sent-322, score-0.154]
</p><p>93 In this case, n  inf  b∈R, f ∈H  ˆ nλ|| f ||κσ + ∑ (1 − ( f (xi ) + b)yi )+ i=1 n  =  ˆ inf inf nλ|| f ||κσ + ∑ (1 − ( f (xi ) + b)yi )+ b  f  i=1 n  ˜ ˆ → inf inf nλRκ ( f ) + ∑ (1 − ( f (xi ) + b)yi )+ p b  f  ,  i=1  with our results applying to the inner optimization problem (where b is ﬁxed). [sent-325, score-0.7]
</p><p>94 Finally, we note that the limiting problem is one where all polynomials of degree < p are free, and hence, the bias term is “absorbed” into what is already free in the limiting problem. [sent-328, score-0.311]
</p><p>95 The proof is based on an expansion of the kernel function (Equation 2. [sent-332, score-0.146]
</p><p>96 In this case, for any degree p, an asymptotic regime was identiﬁed in which the regularized solution approached the least squares degree-p polynomial. [sent-338, score-0.189]
</p><p>97 The result hinges upon the simultaneous cancellation effects between the coefﬁcients c(σ, λ) and the kernel function κσ in the kernel expansion of f (x), with f (x) and c(σ, λ) given by f (x) =  ∑ ci (σ, λ)κσ (x, xi ) i  c(σ, λ) = (κσ (X, X) + nλI)−1 y when κσ (x, x′ ) = exp(−||x − x′ ||2 /σ2 ). [sent-339, score-0.315]
</p><p>98 Note that in our previous work, we did not work with the value-based formulation of learning, and we were forced to take the limit of an expression combining training and testing kernel products, exploiting the explicit nature of the regularized least squares equations. [sent-342, score-0.268]
</p><p>99 This in turn allowed us to avoid discussing the limits of the ci , which we do not know how to characterize. [sent-374, score-0.141]
</p><p>100 We are not suggesting that practicioners wishing to do polynomial approximation use Gaussian kernels with extreme σ, λ values; there is no difﬁculty in using standard polynomial kernels directly, and using extreme σ and λ values invites numerical difﬁculties. [sent-375, score-0.254]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fs', 0.462), ('ifkin', 0.206), ('ippert', 0.206), ('tikhonov', 0.204), ('epi', 0.168), ('ikhonov', 0.168), ('imits', 0.168), ('lim', 0.163), ('regularization', 0.154), ('lims', 0.15), ('lippert', 0.15), ('nfinite', 0.143), ('egularization', 0.143), ('minimizers', 0.142), ('inf', 0.14), ('bt', 0.132), ('qb', 0.127), ('gc', 0.117), ('infs', 0.112), ('rn', 0.1), ('qa', 0.1), ('limits', 0.094), ('limiting', 0.094), ('sups', 0.094), ('yt', 0.092), ('regularized', 0.089), ('qc', 0.085), ('yi', 0.08), ('kernel', 0.079), ('minimizer', 0.078), ('polynomial', 0.077), ('jjt', 0.075), ('qhi', 0.075), ('submatrices', 0.071), ('expansion', 0.067), ('rls', 0.065), ('polynomials', 0.065), ('keerthi', 0.065), ('xii', 0.065), ('qd', 0.064), ('wets', 0.064), ('degree', 0.058), ('limit', 0.058), ('rockafellar', 0.057), ('ba', 0.057), ('ccp', 0.056), ('maa', 0.056), ('ryan', 0.056), ('ux', 0.056), ('rkhs', 0.056), ('bandwidth', 0.054), ('rifkin', 0.054), ('cients', 0.053), ('sx', 0.052), ('lemma', 0.052), ('pointwise', 0.051), ('kernels', 0.05), ('monomials', 0.049), ('poly', 0.049), ('ci', 0.047), ('convex', 0.045), ('quadratic', 0.044), ('xi', 0.043), ('squares', 0.042), ('residual', 0.041), ('loss', 0.041), ('mi', 0.04), ('mercer', 0.04), ('coef', 0.039), ('lo', 0.039), ('sup', 0.039), ('gaussian', 0.039), ('symmetric', 0.038), ('hi', 0.038), ('epigraphical', 0.037), ('gci', 0.037), ('ggt', 0.037), ('grace', 0.037), ('infx', 0.037), ('llt', 0.037), ('mba', 0.037), ('mbb', 0.037), ('qmi', 0.037), ('tomaso', 0.037), ('phenomenon', 0.037), ('dc', 0.037), ('ii', 0.037), ('unregularized', 0.036), ('qi', 0.036), ('rd', 0.036), ('ct', 0.034), ('lin', 0.034), ('expansions', 0.034), ('ross', 0.034), ('generic', 0.034), ('exp', 0.033), ('valued', 0.033), ('uy', 0.032), ('honda', 0.032), ('kc', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="40-tfidf-1" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>Author: Ross A. Lippert, Ryan M. Rifkin</p><p>Abstract: We consider the problem of Tikhonov regularization with a general convex loss function: this formalism includes support vector machines and regularized least squares. For a family of kernels that includes the Gaussian, parameterized by a “bandwidth” parameter σ, we characterize the limiting ˜ solution as σ → ∞. In particular, we show that if we set the regularization parameter λ = λσ−2p , the regularization term of the Tikhonov problem tends to an indicator function on polynomials of degree ⌊p⌋ (with residual regularization in the case where p ∈ Z). The proof rests on two key ideas: epi-convergence, a notion of functional convergence under which limits of minimizers converge to minimizers of limits, and a value-based formulation of learning, where we work with regularization on the function output values (y) as opposed to the function expansion coefﬁcients in the RKHS. Our result generalizes and uniﬁes previous results in this area. Keywords: Tikhonov regularization, Gaussian kernel, theory, kernel machines</p><p>2 0.12094339 <a title="40-tfidf-2" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>Author: Mikhail Belkin, Partha Niyogi, Vikas Sindhwani</p><p>Abstract: We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework. Keywords: semi-supervised learning, graph transduction, regularization, kernel methods, manifold learning, spectral graph theory, unlabeled data, support vector machines</p><p>3 0.10766488 <a title="40-tfidf-3" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>Author: Régis Vert, Jean-Philippe Vert</p><p>Abstract: We determine the asymptotic behaviour of the function computed by support vector machines (SVM) and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel Hilbert space of the Gaussian RBF kernel, in the situation where the number of examples tends to inﬁnity, the bandwidth of the Gaussian kernel tends to 0, and the regularization parameter is held ﬁxed. Non-asymptotic convergence bounds to this limit in the L2 sense are provided, together with upper bounds on the classiﬁcation error that is shown to converge to the Bayes risk, therefore proving the Bayes-consistency of a variety of methods although the regularization term does not vanish. These results are particularly relevant to the one-class SVM, for which the regularization can not vanish by construction, and which is shown for the ﬁrst time to be a consistent density level set estimator. Keywords: regularization, Gaussian kernel RKHS, one-class SVM, convex loss functions, kernel density estimation</p><p>4 0.10403949 <a title="40-tfidf-4" href="./jmlr-2006-Consistency_of_Multiclass_Empirical_Risk_Minimization_Methods_Based_on_Convex_Loss.html">24 jmlr-2006-Consistency of Multiclass Empirical Risk Minimization Methods Based on Convex Loss</a></p>
<p>Author: Di-Rong Chen, Tao Sun</p><p>Abstract: The consistency of classiﬁcation algorithm plays a central role in statistical learning theory. A consistent algorithm guarantees us that taking more samples essentially sufﬁces to roughly reconstruct the unknown distribution. We consider the consistency of ERM scheme over classes of combinations of very simple rules (base classiﬁers) in multiclass classiﬁcation. Our approach is, under some mild conditions, to establish a quantitative relationship between classiﬁcation errors and convex risks. In comparison with the related previous work, the feature of our result is that the conditions are mainly expressed in terms of the differences between some values of the convex function. Keywords: multiclass classiﬁcation, classiﬁer, consistency, empirical risk minimization, constrained comparison method, Tsybakov noise condition</p><p>5 0.079906218 <a title="40-tfidf-5" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>Author: Sayan Mukherjee, Ding-Xuan Zhou</p><p>Abstract: We introduce an algorithm that learns gradients from samples in the supervised learning framework. An error analysis is given for the convergence of the gradient estimated by the algorithm to the true gradient. The utility of the algorithm for the problem of variable selection as well as determining variable covariance is illustrated on simulated data as well as two gene expression data sets. For square loss we provide a very efﬁcient implementation with respect to both memory and time. Keywords: Tikhnonov regularization, variable selection, reproducing kernel Hilbert space, generalization bounds</p><p>6 0.07672976 <a title="40-tfidf-6" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.076013036 <a title="40-tfidf-7" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>8 0.071194254 <a title="40-tfidf-8" href="./jmlr-2006-Worst-Case_Analysis_of_Selective_Sampling_for_Linear_Classification.html">96 jmlr-2006-Worst-Case Analysis of Selective Sampling for Linear Classification</a></p>
<p>9 0.067099437 <a title="40-tfidf-9" href="./jmlr-2006-Universal_Kernels.html">93 jmlr-2006-Universal Kernels</a></p>
<p>10 0.066193104 <a title="40-tfidf-10" href="./jmlr-2006-Stability_Properties_of_Empirical_Risk_Minimization_over_Donsker_Classes.html">84 jmlr-2006-Stability Properties of Empirical Risk Minimization over Donsker Classes</a></p>
<p>11 0.066167466 <a title="40-tfidf-11" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.061470687 <a title="40-tfidf-12" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>13 0.057312079 <a title="40-tfidf-13" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>14 0.056795269 <a title="40-tfidf-14" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>15 0.051957563 <a title="40-tfidf-15" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>16 0.050479032 <a title="40-tfidf-16" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.049634553 <a title="40-tfidf-17" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.048994463 <a title="40-tfidf-18" href="./jmlr-2006-Generalized_Bradley-Terry_Models_and_Multi-Class_Probability_Estimates.html">34 jmlr-2006-Generalized Bradley-Terry Models and Multi-Class Probability Estimates</a></p>
<p>19 0.04841391 <a title="40-tfidf-19" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>20 0.047345527 <a title="40-tfidf-20" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.258), (1, -0.026), (2, 0.155), (3, -0.175), (4, 0.056), (5, 0.117), (6, 0.031), (7, 0.033), (8, 0.011), (9, -0.076), (10, 0.117), (11, -0.07), (12, -0.142), (13, -0.115), (14, -0.051), (15, -0.055), (16, 0.08), (17, 0.027), (18, -0.169), (19, -0.089), (20, -0.086), (21, -0.03), (22, -0.011), (23, -0.001), (24, 0.079), (25, -0.162), (26, 0.166), (27, -0.095), (28, -0.044), (29, 0.078), (30, 0.075), (31, 0.002), (32, 0.007), (33, 0.154), (34, 0.139), (35, 0.015), (36, -0.117), (37, 0.003), (38, -0.004), (39, -0.013), (40, 0.055), (41, -0.015), (42, 0.038), (43, -0.039), (44, -0.019), (45, 0.056), (46, -0.033), (47, -0.1), (48, -0.223), (49, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93559885 <a title="40-lsi-1" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>Author: Ross A. Lippert, Ryan M. Rifkin</p><p>Abstract: We consider the problem of Tikhonov regularization with a general convex loss function: this formalism includes support vector machines and regularized least squares. For a family of kernels that includes the Gaussian, parameterized by a “bandwidth” parameter σ, we characterize the limiting ˜ solution as σ → ∞. In particular, we show that if we set the regularization parameter λ = λσ−2p , the regularization term of the Tikhonov problem tends to an indicator function on polynomials of degree ⌊p⌋ (with residual regularization in the case where p ∈ Z). The proof rests on two key ideas: epi-convergence, a notion of functional convergence under which limits of minimizers converge to minimizers of limits, and a value-based formulation of learning, where we work with regularization on the function output values (y) as opposed to the function expansion coefﬁcients in the RKHS. Our result generalizes and uniﬁes previous results in this area. Keywords: Tikhonov regularization, Gaussian kernel, theory, kernel machines</p><p>2 0.56582838 <a title="40-lsi-2" href="./jmlr-2006-Consistency_of_Multiclass_Empirical_Risk_Minimization_Methods_Based_on_Convex_Loss.html">24 jmlr-2006-Consistency of Multiclass Empirical Risk Minimization Methods Based on Convex Loss</a></p>
<p>Author: Di-Rong Chen, Tao Sun</p><p>Abstract: The consistency of classiﬁcation algorithm plays a central role in statistical learning theory. A consistent algorithm guarantees us that taking more samples essentially sufﬁces to roughly reconstruct the unknown distribution. We consider the consistency of ERM scheme over classes of combinations of very simple rules (base classiﬁers) in multiclass classiﬁcation. Our approach is, under some mild conditions, to establish a quantitative relationship between classiﬁcation errors and convex risks. In comparison with the related previous work, the feature of our result is that the conditions are mainly expressed in terms of the differences between some values of the convex function. Keywords: multiclass classiﬁcation, classiﬁer, consistency, empirical risk minimization, constrained comparison method, Tsybakov noise condition</p><p>3 0.51960725 <a title="40-lsi-3" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>Author: Régis Vert, Jean-Philippe Vert</p><p>Abstract: We determine the asymptotic behaviour of the function computed by support vector machines (SVM) and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel Hilbert space of the Gaussian RBF kernel, in the situation where the number of examples tends to inﬁnity, the bandwidth of the Gaussian kernel tends to 0, and the regularization parameter is held ﬁxed. Non-asymptotic convergence bounds to this limit in the L2 sense are provided, together with upper bounds on the classiﬁcation error that is shown to converge to the Bayes risk, therefore proving the Bayes-consistency of a variety of methods although the regularization term does not vanish. These results are particularly relevant to the one-class SVM, for which the regularization can not vanish by construction, and which is shown for the ﬁrst time to be a consistent density level set estimator. Keywords: regularization, Gaussian kernel RKHS, one-class SVM, convex loss functions, kernel density estimation</p><p>4 0.41851559 <a title="40-lsi-4" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><p>5 0.41420799 <a title="40-lsi-5" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>Author: Mikhail Belkin, Partha Niyogi, Vikas Sindhwani</p><p>Abstract: We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework. Keywords: semi-supervised learning, graph transduction, regularization, kernel methods, manifold learning, spectral graph theory, unlabeled data, support vector machines</p><p>6 0.39645371 <a title="40-lsi-6" href="./jmlr-2006-Universal_Kernels.html">93 jmlr-2006-Universal Kernels</a></p>
<p>7 0.39322159 <a title="40-lsi-7" href="./jmlr-2006-Stability_Properties_of_Empirical_Risk_Minimization_over_Donsker_Classes.html">84 jmlr-2006-Stability Properties of Empirical Risk Minimization over Donsker Classes</a></p>
<p>8 0.32096964 <a title="40-lsi-8" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>9 0.29794282 <a title="40-lsi-9" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.29302597 <a title="40-lsi-10" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>11 0.28776991 <a title="40-lsi-11" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.28479409 <a title="40-lsi-12" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>13 0.26918858 <a title="40-lsi-13" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>14 0.25883174 <a title="40-lsi-14" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>15 0.25260067 <a title="40-lsi-15" href="./jmlr-2006-Generalized_Bradley-Terry_Models_and_Multi-Class_Probability_Estimates.html">34 jmlr-2006-Generalized Bradley-Terry Models and Multi-Class Probability Estimates</a></p>
<p>16 0.25213069 <a title="40-lsi-16" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>17 0.24811716 <a title="40-lsi-17" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>18 0.23623724 <a title="40-lsi-18" href="./jmlr-2006-On_Representing_and_Generating_Kernels_by_Fuzzy_Equivalence_Relations.html">67 jmlr-2006-On Representing and Generating Kernels by Fuzzy Equivalence Relations</a></p>
<p>19 0.23572108 <a title="40-lsi-19" href="./jmlr-2006-Worst-Case_Analysis_of_Selective_Sampling_for_Linear_Classification.html">96 jmlr-2006-Worst-Case Analysis of Selective Sampling for Linear Classification</a></p>
<p>20 0.23380607 <a title="40-lsi-20" href="./jmlr-2006-Causal_Graph_Based_Decomposition_of_Factored_MDPs.html">19 jmlr-2006-Causal Graph Based Decomposition of Factored MDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.021), (17, 0.045), (34, 0.013), (36, 0.046), (45, 0.021), (50, 0.095), (63, 0.036), (76, 0.01), (78, 0.024), (81, 0.038), (84, 0.386), (90, 0.061), (91, 0.026), (96, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91706687 <a title="40-lda-1" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pannagadatta K. Shivaswamy, Chiranjib Bhattacharyya, Alexander J. Smola</p><p>Abstract: We propose a novel second order cone programming formulation for designing robust classiﬁers which can handle uncertainty in observations. Similar formulations are also derived for designing regression functions which are robust to uncertainties in the regression setting. The proposed formulations are independent of the underlying distribution, requiring only the existence of second order moments. These formulations are then specialized to the case of missing values in observations for both classiﬁcation and regression problems. Experiments show that the proposed formulations outperform imputation.</p><p>same-paper 2 0.87151003 <a title="40-lda-2" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>Author: Ross A. Lippert, Ryan M. Rifkin</p><p>Abstract: We consider the problem of Tikhonov regularization with a general convex loss function: this formalism includes support vector machines and regularized least squares. For a family of kernels that includes the Gaussian, parameterized by a “bandwidth” parameter σ, we characterize the limiting ˜ solution as σ → ∞. In particular, we show that if we set the regularization parameter λ = λσ−2p , the regularization term of the Tikhonov problem tends to an indicator function on polynomials of degree ⌊p⌋ (with residual regularization in the case where p ∈ Z). The proof rests on two key ideas: epi-convergence, a notion of functional convergence under which limits of minimizers converge to minimizers of limits, and a value-based formulation of learning, where we work with regularization on the function output values (y) as opposed to the function expansion coefﬁcients in the RKHS. Our result generalizes and uniﬁes previous results in this area. Keywords: Tikhonov regularization, Gaussian kernel, theory, kernel machines</p><p>3 0.50351071 <a title="40-lda-3" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>Author: Mikio L. Braun</p><p>Abstract: The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to inﬁnity. We derive probabilistic ﬁnite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reﬂecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a signiﬁcant improvement over existing non-scaling bounds. Keywords: kernel matrix, eigenvalues, relative perturbation bounds</p><p>4 0.48491931 <a title="40-lda-4" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>5 0.47583142 <a title="40-lda-5" href="./jmlr-2006-Quantile_Regression_Forests.html">77 jmlr-2006-Quantile Regression Forests</a></p>
<p>Author: Nicolai Meinshausen</p><p>Abstract: Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classiﬁcation. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power. Keywords: quantile regression, random forests, adaptive neighborhood regression</p><p>6 0.47437292 <a title="40-lda-6" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>7 0.46744847 <a title="40-lda-7" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>8 0.45892656 <a title="40-lda-8" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>9 0.45768315 <a title="40-lda-9" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>10 0.45429021 <a title="40-lda-10" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>11 0.45290381 <a title="40-lda-11" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>12 0.45283657 <a title="40-lda-12" href="./jmlr-2006-Nonparametric_Quantile_Estimation.html">65 jmlr-2006-Nonparametric Quantile Estimation</a></p>
<p>13 0.44669342 <a title="40-lda-13" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.44024655 <a title="40-lda-14" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.43725148 <a title="40-lda-15" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>16 0.43222976 <a title="40-lda-16" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>17 0.42696372 <a title="40-lda-17" href="./jmlr-2006-Some_Theory_for_Generalized_Boosting_Algorithms.html">82 jmlr-2006-Some Theory for Generalized Boosting Algorithms</a></p>
<p>18 0.42528623 <a title="40-lda-18" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>19 0.4251883 <a title="40-lda-19" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>20 0.42036942 <a title="40-lda-20" href="./jmlr-2006-Pattern_Recognition_for__Conditionally_Independent_Data.html">73 jmlr-2006-Pattern Recognition for  Conditionally Independent Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
