<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-94" href="#">jmlr2006-94</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</h1>
<br/><p>Source: <a title="jmlr-2006-94-pdf" href="http://jmlr.org/papers/volume7/hamerly06a/hamerly06a.pdf">pdf</a></p><p>Author: Greg Hamerly, Erez Perelman, Jeremy Lau, Brad Calder, Timothy Sherwood</p><p>Abstract: An essential step in designing a new computer architecture is the careful examination of different design options. It is critical that computer architects have efﬁcient means by which they may estimate the impact of various design options on the overall machine. This task is complicated by the fact that different programs, and even different parts of the same program, may have distinct behaviors that interact with the hardware in different ways. Researchers use very detailed simulators to estimate processor performance, which models every cycle of an executing program. Unfortunately, simulating every cycle of a real program can take weeks or months. To address this problem we have created a tool called SimPoint that uses data clustering algorithms from machine learning to automatically ﬁnd repetitive patterns in a program’s execution. By simulating one representative of each repetitive behavior pattern, simulation time can be reduced to minutes instead of weeks for standard benchmark programs, with very little cost in terms of accuracy. We describe this important problem, the data representation and preprocessing methods used by SimPoint, the clustering algorithm at the core of SimPoint, and we evaluate different options for tuning SimPoint. Keywords: k-means, random projection, Bayesian information criterion, simulation, SimPoint</p><p>Reference: <a title="jmlr-2006-94-reference" href="../jmlr2006_reference/jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('simpoint', 0.577), ('cpi', 0.388), ('instruct', 0.28), ('clust', 0.257), ('execut', 0.219), ('architect', 0.193), ('phas', 0.18), ('bic', 0.148), ('interv', 0.147), ('sim', 0.145), ('sherwood', 0.111), ('erelm', 0.1), ('herwood', 0.1), ('lau', 0.094), ('rchitecture', 0.094), ('program', 0.091), ('ald', 0.085), ('bil', 0.084), ('perelm', 0.083), ('mil', 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="94-tfidf-1" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>Author: Greg Hamerly, Erez Perelman, Jeremy Lau, Brad Calder, Timothy Sherwood</p><p>Abstract: An essential step in designing a new computer architecture is the careful examination of different design options. It is critical that computer architects have efﬁcient means by which they may estimate the impact of various design options on the overall machine. This task is complicated by the fact that different programs, and even different parts of the same program, may have distinct behaviors that interact with the hardware in different ways. Researchers use very detailed simulators to estimate processor performance, which models every cycle of an executing program. Unfortunately, simulating every cycle of a real program can take weeks or months. To address this problem we have created a tool called SimPoint that uses data clustering algorithms from machine learning to automatically ﬁnd repetitive patterns in a program’s execution. By simulating one representative of each repetitive behavior pattern, simulation time can be reduced to minutes instead of weeks for standard benchmark programs, with very little cost in terms of accuracy. We describe this important problem, the data representation and preprocessing methods used by SimPoint, the clustering algorithm at the core of SimPoint, and we evaluate different options for tuning SimPoint. Keywords: k-means, random projection, Bayesian information criterion, simulation, SimPoint</p><p>2 0.10202426 <a title="94-tfidf-2" href="./jmlr-2006-Rearrangement_Clustering%3A_Pitfalls%2C_Remedies%2C_and_Applications.html">78 jmlr-2006-Rearrangement Clustering: Pitfalls, Remedies, and Applications</a></p>
<p>Author: Sharlee Climer, Weixiong Zhang</p><p>Abstract: Given a matrix of values in which the rows correspond to objects and the columns correspond to features of the objects, rearrangement clustering is the problem of rearranging the rows of the matrix such that the sum of the similarities between adjacent rows is maximized. Referred to by various names and reinvented several times, this clustering technique has been extensively used in many ﬁelds over the last three decades. In this paper, we point out two critical pitfalls that have been previously overlooked. The ﬁrst pitfall is deleterious when rearrangement clustering is applied to objects that form natural clusters. The second concerns a similarity metric that is commonly used. We present an algorithm that overcomes these pitfalls. This algorithm is based on a variation of the Traveling Salesman Problem. It offers an extra beneﬁt as it automatically determines cluster boundaries. Using this algorithm, we optimally solve four benchmark problems and a 2,467-gene expression data clustering problem. As expected, our new algorithm identiﬁes better clusters than those found by previous approaches in all ﬁve cases. Overall, our results demonstrate the beneﬁts of rectifying the pitfalls and exemplify the usefulness of this clustering technique. Our code is available at our websites. Keywords: clustering, visualization of patterns in data, bond energy algorithm, traveling salesman problem, asymmetric clustering</p><p>3 0.084753558 <a title="94-tfidf-3" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram. Keywords: spectral clustering, blind source separation, computational auditory scene analysis</p><p>4 0.06123203 <a title="94-tfidf-4" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>5 0.0587552 <a title="94-tfidf-5" href="./jmlr-2006-Learning_Recursive_Control_Programs_from_Problem_Solving_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">50 jmlr-2006-Learning Recursive Control Programs from Problem Solving     (Special Topic on Inductive Programming)</a></p>
<p>Author: Pat Langley, Dongkyu Choi</p><p>Abstract: In this paper, we propose a new representation for physical control – teleoreactive logic programs – along with an interpreter that uses them to achieve goals. In addition, we present a new learning method that acquires recursive forms of these structures from traces of successful problem solving. We report experiments in three different domains that demonstrate the generality of this approach. In closing, we review related work on learning complex skills and discuss directions for future research on this topic. Keywords: teleoreactive control, logic programs, problem solving, skill learning</p><p>6 0.041887522 <a title="94-tfidf-6" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>7 0.040968172 <a title="94-tfidf-7" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>8 0.040843692 <a title="94-tfidf-8" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>9 0.035282657 <a title="94-tfidf-9" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.034502242 <a title="94-tfidf-10" href="./jmlr-2006-Causal_Graph_Based_Decomposition_of_Factored_MDPs.html">19 jmlr-2006-Causal Graph Based Decomposition of Factored MDPs</a></p>
<p>11 0.033973161 <a title="94-tfidf-11" href="./jmlr-2006-Noisy-OR_Component_Analysis_and_its_Application_to_Link_Analysis.html">64 jmlr-2006-Noisy-OR Component Analysis and its Application to Link Analysis</a></p>
<p>12 0.03257288 <a title="94-tfidf-12" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.0324311 <a title="94-tfidf-13" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>14 0.031179041 <a title="94-tfidf-14" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.030071381 <a title="94-tfidf-15" href="./jmlr-2006-In_Search_of_Non-Gaussian_Components_of_a_High-Dimensional_Distribution.html">36 jmlr-2006-In Search of Non-Gaussian Components of a High-Dimensional Distribution</a></p>
<p>16 0.029275317 <a title="94-tfidf-16" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>17 0.029166508 <a title="94-tfidf-17" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.02909773 <a title="94-tfidf-18" href="./jmlr-2006-Learning_the_Structure_of_Linear_Latent_Variable_Models.html">54 jmlr-2006-Learning the Structure of Linear Latent Variable Models</a></p>
<p>19 0.028529113 <a title="94-tfidf-19" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>20 0.026211308 <a title="94-tfidf-20" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.141), (1, 0.075), (2, 0.109), (3, -0.07), (4, -0.02), (5, -0.082), (6, 0.09), (7, 0.077), (8, 0.163), (9, -0.025), (10, -0.016), (11, 0.045), (12, 0.018), (13, 0.066), (14, 0.222), (15, -0.02), (16, -0.116), (17, -0.158), (18, 0.141), (19, -0.014), (20, -0.001), (21, -0.227), (22, 0.018), (23, 0.205), (24, -0.183), (25, -0.12), (26, -0.143), (27, 0.079), (28, -0.083), (29, -0.108), (30, -0.194), (31, -0.101), (32, 0.011), (33, 0.036), (34, 0.068), (35, -0.011), (36, -0.101), (37, 0.128), (38, -0.075), (39, -0.128), (40, -0.027), (41, 0.041), (42, 0.081), (43, -0.004), (44, -0.041), (45, 0.037), (46, -0.061), (47, 0.085), (48, 0.069), (49, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95236999 <a title="94-lsi-1" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>Author: Greg Hamerly, Erez Perelman, Jeremy Lau, Brad Calder, Timothy Sherwood</p><p>Abstract: An essential step in designing a new computer architecture is the careful examination of different design options. It is critical that computer architects have efﬁcient means by which they may estimate the impact of various design options on the overall machine. This task is complicated by the fact that different programs, and even different parts of the same program, may have distinct behaviors that interact with the hardware in different ways. Researchers use very detailed simulators to estimate processor performance, which models every cycle of an executing program. Unfortunately, simulating every cycle of a real program can take weeks or months. To address this problem we have created a tool called SimPoint that uses data clustering algorithms from machine learning to automatically ﬁnd repetitive patterns in a program’s execution. By simulating one representative of each repetitive behavior pattern, simulation time can be reduced to minutes instead of weeks for standard benchmark programs, with very little cost in terms of accuracy. We describe this important problem, the data representation and preprocessing methods used by SimPoint, the clustering algorithm at the core of SimPoint, and we evaluate different options for tuning SimPoint. Keywords: k-means, random projection, Bayesian information criterion, simulation, SimPoint</p><p>2 0.79802907 <a title="94-lsi-2" href="./jmlr-2006-Rearrangement_Clustering%3A_Pitfalls%2C_Remedies%2C_and_Applications.html">78 jmlr-2006-Rearrangement Clustering: Pitfalls, Remedies, and Applications</a></p>
<p>Author: Sharlee Climer, Weixiong Zhang</p><p>Abstract: Given a matrix of values in which the rows correspond to objects and the columns correspond to features of the objects, rearrangement clustering is the problem of rearranging the rows of the matrix such that the sum of the similarities between adjacent rows is maximized. Referred to by various names and reinvented several times, this clustering technique has been extensively used in many ﬁelds over the last three decades. In this paper, we point out two critical pitfalls that have been previously overlooked. The ﬁrst pitfall is deleterious when rearrangement clustering is applied to objects that form natural clusters. The second concerns a similarity metric that is commonly used. We present an algorithm that overcomes these pitfalls. This algorithm is based on a variation of the Traveling Salesman Problem. It offers an extra beneﬁt as it automatically determines cluster boundaries. Using this algorithm, we optimally solve four benchmark problems and a 2,467-gene expression data clustering problem. As expected, our new algorithm identiﬁes better clusters than those found by previous approaches in all ﬁve cases. Overall, our results demonstrate the beneﬁts of rectifying the pitfalls and exemplify the usefulness of this clustering technique. Our code is available at our websites. Keywords: clustering, visualization of patterns in data, bond energy algorithm, traveling salesman problem, asymmetric clustering</p><p>3 0.45486936 <a title="94-lsi-3" href="./jmlr-2006-Learning_Recursive_Control_Programs_from_Problem_Solving_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">50 jmlr-2006-Learning Recursive Control Programs from Problem Solving     (Special Topic on Inductive Programming)</a></p>
<p>Author: Pat Langley, Dongkyu Choi</p><p>Abstract: In this paper, we propose a new representation for physical control – teleoreactive logic programs – along with an interpreter that uses them to achieve goals. In addition, we present a new learning method that acquires recursive forms of these structures from traces of successful problem solving. We report experiments in three different domains that demonstrate the generality of this approach. In closing, we review related work on learning complex skills and discuss directions for future research on this topic. Keywords: teleoreactive control, logic programs, problem solving, skill learning</p><p>4 0.35370409 <a title="94-lsi-4" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram. Keywords: spectral clustering, blind source separation, computational auditory scene analysis</p><p>5 0.26458687 <a title="94-lsi-5" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>Author: Jing Zhou, Dean P. Foster, Robert A. Stine, Lyle H. Ungar</p><p>Abstract: In streamwise feature selection, new features are sequentially considered for addition to a predictive model. When the space of potential features is large, streamwise feature selection offers many advantages over traditional feature selection methods, which assume that all features are known in advance. Features can be generated dynamically, focusing the search for new features on promising subspaces, and overﬁtting can be controlled by dynamically adjusting the threshold for adding features to the model. In contrast to traditional forward feature selection algorithms such as stepwise regression in which at each step all possible features are evaluated and the best one is selected, streamwise feature selection only evaluates each feature once when it is generated. We describe information-investing and α-investing, two adaptive complexity penalty methods for streamwise feature selection which dynamically adjust the threshold on the error reduction required for adding a new feature. These two methods give false discovery rate style guarantees against overﬁtting. They differ from standard penalty methods such as AIC, BIC and RIC, which always drastically over- or under-ﬁt in the limit of inﬁnite numbers of non-predictive features. Empirical results show that streamwise regression is competitive with (on small data sets) and superior to (on large data sets) much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with millions of potential features. Keywords: classiﬁcation, stepwise regression, multiple regression, feature selection, false discovery rate</p><p>6 0.22514237 <a title="94-lsi-6" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.21890089 <a title="94-lsi-7" href="./jmlr-2006-Causal_Graph_Based_Decomposition_of_Factored_MDPs.html">19 jmlr-2006-Causal Graph Based Decomposition of Factored MDPs</a></p>
<p>8 0.20948036 <a title="94-lsi-8" href="./jmlr-2006-Adaptive_Prototype_Learning_Algorithms%3A_Theoretical_and_Experimental_Studies.html">13 jmlr-2006-Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies</a></p>
<p>9 0.202354 <a title="94-lsi-9" href="./jmlr-2006-In_Search_of_Non-Gaussian_Components_of_a_High-Dimensional_Distribution.html">36 jmlr-2006-In Search of Non-Gaussian Components of a High-Dimensional Distribution</a></p>
<p>10 0.19557247 <a title="94-lsi-10" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.17667837 <a title="94-lsi-11" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.16978768 <a title="94-lsi-12" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>13 0.16267802 <a title="94-lsi-13" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.16003145 <a title="94-lsi-14" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.15816069 <a title="94-lsi-15" href="./jmlr-2006-Action_Elimination_and_Stopping_Conditions_for_the_Multi-Armed_Bandit_and_Reinforcement_Learning_Problems.html">10 jmlr-2006-Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems</a></p>
<p>16 0.15703003 <a title="94-lsi-16" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.15634225 <a title="94-lsi-17" href="./jmlr-2006-Noisy-OR_Component_Analysis_and_its_Application_to_Link_Analysis.html">64 jmlr-2006-Noisy-OR Component Analysis and its Application to Link Analysis</a></p>
<p>18 0.15403992 <a title="94-lsi-18" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>19 0.15271369 <a title="94-lsi-19" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>20 0.14845787 <a title="94-lsi-20" href="./jmlr-2006-Point-Based_Value_Iteration_for_Continuous_POMDPs.html">74 jmlr-2006-Point-Based Value Iteration for Continuous POMDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(31, 0.022), (33, 0.036), (51, 0.021), (56, 0.018), (63, 0.06), (72, 0.041), (73, 0.081), (77, 0.093), (78, 0.056), (88, 0.019), (98, 0.428), (99, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82447886 <a title="94-lda-1" href="./jmlr-2006-A_Linear_Non-Gaussian_Acyclic_Model_for_Causal_Discovery.html">4 jmlr-2006-A Linear Non-Gaussian Acyclic Model for Causal Discovery</a></p>
<p>Author: Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvärinen, Antti Kerminen</p><p>Abstract: In recent years, several methods have been proposed for the discovery of causal structure from non-experimental data. Such methods make various assumptions on the data generating process to facilitate its identiﬁcation from purely observational data. Continuing this line of research, we show how to discover the complete causal structure of continuous-valued data, under the assumptions that (a) the data generating process is linear, (b) there are no unobserved confounders, and (c) disturbance variables have non-Gaussian distributions of non-zero variances. The solution relies on the use of the statistical method known as independent component analysis, and does not require any pre-speciﬁed time-ordering of the variables. We provide a complete Matlab package for performing this LiNGAM analysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the effectiveness of the method using artiﬁcially generated data and real-world data. Keywords: independent component analysis, non-Gaussianity, causal discovery, directed acyclic graph, non-experimental data</p><p>same-paper 2 0.65270621 <a title="94-lda-2" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>Author: Greg Hamerly, Erez Perelman, Jeremy Lau, Brad Calder, Timothy Sherwood</p><p>Abstract: An essential step in designing a new computer architecture is the careful examination of different design options. It is critical that computer architects have efﬁcient means by which they may estimate the impact of various design options on the overall machine. This task is complicated by the fact that different programs, and even different parts of the same program, may have distinct behaviors that interact with the hardware in different ways. Researchers use very detailed simulators to estimate processor performance, which models every cycle of an executing program. Unfortunately, simulating every cycle of a real program can take weeks or months. To address this problem we have created a tool called SimPoint that uses data clustering algorithms from machine learning to automatically ﬁnd repetitive patterns in a program’s execution. By simulating one representative of each repetitive behavior pattern, simulation time can be reduced to minutes instead of weeks for standard benchmark programs, with very little cost in terms of accuracy. We describe this important problem, the data representation and preprocessing methods used by SimPoint, the clustering algorithm at the core of SimPoint, and we evaluate different options for tuning SimPoint. Keywords: k-means, random projection, Bayesian information criterion, simulation, SimPoint</p><p>3 0.33649728 <a title="94-lda-3" href="./jmlr-2006-Considering_Cost_Asymmetry_in_Learning_Classifiers.html">22 jmlr-2006-Considering Cost Asymmetry in Learning Classifiers</a></p>
<p>Author: Francis R. Bach, David Heckerman, Eric Horvitz</p><p>Abstract: Receiver Operating Characteristic (ROC) curves are a standard way to display the performance of a set of binary classiﬁers for all feasible ratios of the costs associated with false positives and false negatives. For linear classiﬁers, the set of classiﬁers is typically obtained by training once, holding constant the estimated slope and then varying the intercept to obtain a parameterized set of classiﬁers whose performances can be plotted in the ROC plane. We consider the alternative of varying the asymmetry of the cost function used for training. We show that the ROC curve obtained by varying both the intercept and the asymmetry, and hence the slope, always outperforms the ROC curve obtained by varying only the intercept. In addition, we present a path-following algorithm for the support vector machine (SVM) that can compute efﬁciently the entire ROC curve, and that has the same computational complexity as training a single classiﬁer. Finally, we provide a theoretical analysis of the relationship between the asymmetric cost model assumed when training a classiﬁer and the cost model assumed in applying the classiﬁer. In particular, we show that the mismatch between the step function used for testing and its convex upper bounds, usually used for training, leads to a provable and quantiﬁable difference around extreme asymmetries. Keywords: support vector machines, receiver operating characteristic (ROC) analysis, linear classiﬁcation</p><p>4 0.33467376 <a title="94-lda-4" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>Author: S. V. N. Vishwanathan, Nicol N. Schraudolph, Alex J. Smola</p><p>Abstract: This paper presents an online support vector machine (SVM) that uses the stochastic meta-descent (SMD) algorithm to adapt its step size automatically. We formulate the online learning problem as a stochastic gradient descent in reproducing kernel Hilbert space (RKHS) and translate SMD to the nonparametric setting, where its gradient trace parameter is no longer a coefﬁcient vector but an element of the RKHS. We derive efﬁcient updates that allow us to perform the step size adaptation in linear time. We apply the online SVM framework to a variety of loss functions, and in particular show how to handle structured output spaces and achieve efﬁcient online multiclass classiﬁcation. Experiments show that our algorithm outperforms more primitive methods for setting the gradient step size. Keywords: online SVM, stochastic meta-descent, structured output spaces</p><p>5 0.33391705 <a title="94-lda-5" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<p>Author: Jelle R. Kok, Nikos Vlassis</p><p>Abstract: In this article we describe a set of scalable techniques for learning the behavior of a group of agents in a collaborative multiagent setting. As a basis we use the framework of coordination graphs of Guestrin, Koller, and Parr (2002a) which exploits the dependencies between agents to decompose the global payoff function into a sum of local terms. First, we deal with the single-state case and describe a payoff propagation algorithm that computes the individual actions that approximately maximize the global payoff function. The method can be viewed as the decision-making analogue of belief propagation in Bayesian networks. Second, we focus on learning the behavior of the agents in sequential decision-making tasks. We introduce different model-free reinforcementlearning techniques, unitedly called Sparse Cooperative Q-learning, which approximate the global action-value function based on the topology of a coordination graph, and perform updates using the contribution of the individual agents to the maximal global action value. The combined use of an edge-based decomposition of the action-value function and the payoff propagation algorithm for efﬁcient action selection, result in an approach that scales only linearly in the problem size. We provide experimental evidence that our method outperforms related multiagent reinforcement-learning methods based on temporal differences. Keywords: collaborative multiagent system, coordination graph, reinforcement learning, Qlearning, belief propagation</p><p>6 0.33281526 <a title="94-lda-6" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>7 0.3320882 <a title="94-lda-7" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>8 0.33189806 <a title="94-lda-8" href="./jmlr-2006-In_Search_of_Non-Gaussian_Components_of_a_High-Dimensional_Distribution.html">36 jmlr-2006-In Search of Non-Gaussian Components of a High-Dimensional Distribution</a></p>
<p>9 0.32981905 <a title="94-lda-9" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.32902324 <a title="94-lda-10" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>11 0.32896304 <a title="94-lda-11" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.32768658 <a title="94-lda-12" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<p>13 0.32641551 <a title="94-lda-13" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>14 0.32578564 <a title="94-lda-14" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.32547641 <a title="94-lda-15" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>16 0.32499254 <a title="94-lda-16" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>17 0.32494867 <a title="94-lda-17" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>18 0.32492644 <a title="94-lda-18" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.32486111 <a title="94-lda-19" href="./jmlr-2006-Learning_Minimum_Volume_Sets.html">48 jmlr-2006-Learning Minimum Volume Sets</a></p>
<p>20 0.32462427 <a title="94-lda-20" href="./jmlr-2006-Point-Based_Value_Iteration_for_Continuous_POMDPs.html">74 jmlr-2006-Point-Based Value Iteration for Continuous POMDPs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
