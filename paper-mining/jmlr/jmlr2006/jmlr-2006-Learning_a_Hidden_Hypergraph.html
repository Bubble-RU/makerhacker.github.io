<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 jmlr-2006-Learning a Hidden Hypergraph</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-53" href="#">jmlr2006-53</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>53 jmlr-2006-Learning a Hidden Hypergraph</h1>
<br/><p>Source: <a title="jmlr-2006-53-pdf" href="http://jmlr.org/papers/volume7/angluin06a/angluin06a.pdf">pdf</a></p><p>Author: Dana Angluin, Jiang Chen</p><p>Abstract: We consider the problem of learning a hypergraph using edge-detecting queries. In this model, the learner may query whether a set of vertices induces an edge of the hidden hypergraph or not. We show that an r-uniform hypergraph with m edges and n vertices is learnable with O(2 4r m · poly(r, log n)) queries with high probability. The queries can be made in O(min(2 r (log m + r)2 , (log m + r)3 )) rounds. We also give an algorithm that learns an almost uniform hypergraph of ∆ ∆ dimension r using O(2O((1+ 2 )r) · m1+ 2 · poly(log n)) queries with high probability, where ∆ is the difference between the maximum and the minimum edge sizes. This upper bound matches our ∆ lower bound of Ω(( m∆ )1+ 2 ) for this class of hypergraphs in terms of dependence on m. The 1+ 2 queries can also be made in O((1 + ∆) · min(2r (log m + r)2 , (log m + r)3 )) rounds. Keywords: query learning, hypergraph, multiple round algorithm, sampling, chemical reaction network</p><p>Reference: <a title="jmlr-2006-53-reference" href="../jmlr2006_reference/jmlr-2006-Learning_a_Hidden_Hypergraph_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Center for Computational Learning Systems Columbia University 475 Riverside Drive 850 Interchurch MC 7717 New York, NY 10115, USA  Editor: Manfred Warmuth  Abstract We consider the problem of learning a hypergraph using edge-detecting queries. [sent-7, score-0.512]
</p><p>2 In this model, the learner may query whether a set of vertices induces an edge of the hidden hypergraph or not. [sent-8, score-0.897]
</p><p>3 We show that an r-uniform hypergraph with m edges and n vertices is learnable with O(2 4r m · poly(r, log n)) queries with high probability. [sent-9, score-1.159]
</p><p>4 We also give an algorithm that learns an almost uniform hypergraph of ∆ ∆ dimension r using O(2O((1+ 2 )r) · m1+ 2 · poly(log n)) queries with high probability, where ∆ is the difference between the maximum and the minimum edge sizes. [sent-11, score-1.059]
</p><p>5 Introduction A hypergraph H = (V, E) is given by a set of vertices V and a set of edges E, which is a subset of the power set of V (E ⊆ 2V ). [sent-15, score-0.763]
</p><p>6 The dimension of a hypergraph H is the cardinality of the largest set in E. [sent-16, score-0.512]
</p><p>7 In this paper, we are interested in learning a hidden hypergraph using edge-detecting queries of the following form QH (S) : does S include at least one edge of H? [sent-18, score-0.967]
</p><p>8 The query QH (S) is answered 1 or 0, indicating whether S contains all vertices of at least one edge of H or not. [sent-20, score-0.491]
</p><p>9 Grebinski and Kucherov (2000) also study a somewhat different and interesting query model, which they call the additive model, where instead of giving a 1 or 0 answer, a query tells you the total number of edges contained in a certain vertex set. [sent-35, score-0.631]
</p><p>10 It is shown in Angluin and Chen (2004) that Ω((2m/r) r/2 ) edge-detecting queries are required to learn a general hypergraph of dimension r with m edges. [sent-42, score-0.808]
</p><p>11 However, this lower bound does not preclude efﬁcient algorithms for classes of hypergraphs whose edges sizes are close. [sent-45, score-0.392]
</p><p>12 In particular, the question whether there is a learning algorithm for uniform hypergraphs using a number of queries that is linear in the number of edges is still left open, which is the main subject of this paper. [sent-46, score-0.727]
</p><p>13 We show that an r-uniform hypergraph is learnable with O(24r m · poly(r, log n, log 1 )) queries with probability at least 1 − δ. [sent-49, score-1.087]
</p><p>14 Formally speaking, Deﬁnition 1 A hypergraph is (r, ∆)-uniform, where ∆ < r, if its dimension is r and the difference between its maximum and minimum edge sizes is ∆, or equivalently, the maximum and the minimum edge sizes are r and r − ∆ respectively. [sent-51, score-0.866]
</p><p>15 On 2 the other hand, we extend the algorithm that learns uniform hypergraphs to learning the class of 2216  L EARNING A H IDDEN H YPERGRAPH  ∆  ∆  (r, ∆)-uniform hypergraphs with O(2O((1+ 2 )r) · m1+ 2 · poly(log n, log 1 )) queries with probability at δ least 1 − δ. [sent-55, score-1.012]
</p><p>16 In this paper, we show that in our algorithm for r-uniform hypergraphs, queries can be made in O(min(2r (log m + r)2 , (log m + r)3 )) rounds, and in our algorithm for (r, ∆)-uniform hypergraphs, queries can be made in O((1 + ∆) · min(2r (log m + r)2 , (log m + r)3 )) rounds. [sent-64, score-0.628]
</p><p>17 Basically, an independent covering family of a hypergraph is a collection of independent sets that cover all non-edges. [sent-66, score-0.755]
</p><p>18 An interesting observation is that the set of negative queries of any algorithm that learns a hypergraph drawn from a class of hypergraphs that is closed under the operation of adding an edge is an independent covering family of that hypergraph. [sent-67, score-1.499]
</p><p>19 Note both the class of r-uniform hypergraphs and the class of (r, ∆)-uniform hypergraphs are closed under the operation of adding an edge. [sent-68, score-0.482]
</p><p>20 This implies that the query complexity of learning such a hypergraph is bounded below by the minimum size of its independent covering families. [sent-69, score-0.85]
</p><p>21 In this paper, we give a randomized construction of an independent covering family of size O(r2 2r m log n) for r-uniform hypergraphs with m edges. [sent-72, score-0.625]
</p><p>22 This yields a learning algorithm using a number of queries that is quadratic in m, which is further improved to give an algorithm using a number of queries that is linear in m. [sent-73, score-0.65]
</p><p>23 As mentioned in Angluin and Chen (2004) and some other papers, the hypergraph learning problem may also be viewed as the problem of learning a monotone disjunctive normal form (DNF) boolean formula using membership queries only. [sent-74, score-0.832]
</p><p>24 A membership query assigns 1 or 0 to each variable, and is answered 1 if the assignment satisﬁes at least one term, and 0 otherwise, that is, if the set of vertices corresponding to the variables assigned 1 contains all vertices of at least one edge of H. [sent-76, score-0.572]
</p><p>25 An (r, ∆)-uniform hypergraph corresponds to a monotone DNF whose terms are of sizes in the range of [r − ∆, r]. [sent-78, score-0.536]
</p><p>26 In Section 3, we formally deﬁne the concept of an independent covering family and give a randomized construction of independent covering families for general r-uniform hypergraphs. [sent-81, score-0.46]
</p><p>27 In Section 4, we show how to efﬁciently ﬁnd an arbitrary edge in a hypergraph and give a simple learning algorithm using a number of queries that is quadratic in the number of edges. [sent-82, score-1.007]
</p><p>28 In Section 5, we give an algorithm that learns r-uniform hypergraphs using a number of queries that is linear in the number of edges. [sent-83, score-0.59]
</p><p>29 In this paper, we assume that edges do not contain each other, as there is no way to detect the existence of edges that contain other edges using edge-detecting queries. [sent-88, score-0.505]
</p><p>30 We use the term non-edge to denote any set that is a candidate edge in some class of hypergraphs but is not an edge in the target hypergraph. [sent-90, score-0.559]
</p><p>31 The degree of a set χ ⊆ V in a hypergraph H denoted as d H (χ) is the number of / edges of H that contain χ. [sent-93, score-0.689]
</p><p>32 An Independent Covering Family Deﬁnition 2 An independent covering family of a hypergraph H is a collection of independent sets of H such that every non-edge not containing an edge is contained in one of these independent sets. [sent-97, score-0.982]
</p><p>33 First, we observe that if the target hypergraph is drawn from a class of hypergraphs that is closed under the operation of adding an edge (e. [sent-106, score-0.93]
</p><p>34 , the class of all r-uniform hypergraphs), the set of negative queries of any algorithm that learns it is an independent covering family of this hypergraph. [sent-108, score-0.569]
</p><p>35 This is because if there is a non-edge not contained in any of the sets on which these negative queries are made, we will not be able to distinguish between the target hypergraph and the hypergraph with this non-edge being an extra edge. [sent-109, score-1.365]
</p><p>36 Second, although the task of constructing an independent covering family seems substantially easier than that of learning, since the hypergraph is known in the construction task, we show that efﬁcient construction of small-sized independent covering families yields an efﬁcient learning algorithm. [sent-113, score-0.979]
</p><p>37 In Section 4, we will show how to ﬁnd an arbitrary edge out of a hypergraph of dimension r using O(r log n) queries. [sent-114, score-0.79]
</p><p>38 Imagine a simple algorithm in which at each iteration we maintain a sub-hypergraph of the target hypergraph which contains edges that we have found, and construct an independent covering family for it and ask queries on all the sets in the family. [sent-115, score-1.253]
</p><p>39 If there is a set whose query is answered positively, we can ﬁnd at least one edge out of this set. [sent-116, score-0.383]
</p><p>40 We repeat this process until we have collected all the edges in the target hypergraph, in which case the independent covering family we construct is a proof of this fact. [sent-118, score-0.371]
</p><p>41 Suppose that we can construct an independent covering family of size at most f (m) for any hypergraph with at most m edges drawn from certain class of hypergraphs. [sent-119, score-0.901]
</p><p>42 The above algorithm learns this class of hypergraphs using only O( f (m) · m · r log n) queries. [sent-120, score-0.413]
</p><p>43 In the rest of this section, we give a randomized construction of a linear-sized (linear in the number of edges) independent covering family of an r-uniform hypergraph which succeeds with probability at least 1/2. [sent-121, score-0.87]
</p><p>44 By the standard probabilistic argument, the construction proves the existence of an independent covering family of size linear in the number of edges for any uniform hypergraph. [sent-122, score-0.418]
</p><p>45 Theorem 3 Any r-uniform hypergraph with m edges has an independent covering family of size O(r22r m log n). [sent-125, score-1.002]
</p><p>46 We call a vertex set χ ⊆ V relevant if it is contained in at least one edge in the hypergraph. [sent-127, score-0.421]
</p><p>47 Similarly, a vertex is relevant if it is contained in at least one edge in the hypergraph. [sent-128, score-0.391]
</p><p>48 2219  A NGLUIN AND C HEN  Algorithm 1 Construction of an independent covering family 1: FH ← a set containing 4(ln 2 + r ln n) · 2r dH (χ) (χ, pH (χ))-samples drawn independently for every relevant set χ. [sent-144, score-0.395]
</p><p>49 A Simple Quadratic Algorithm In this section, we ﬁrst give an algorithm that ﬁnds an arbitrary edge in a hypergraph of dimension r using only r log n edge-detecting queries. [sent-156, score-0.808]
</p><p>50 Using the high-probability version of the construction, we obtain an algorithm using a number of queries that is quadratic in m that learns an r-uniform hypergraph with m edges with high probability. [sent-159, score-1.034]
</p><p>51 Lemma 6 FIND-ONE-VERTEX ﬁnds one relevant vertex in a non-empty hypergraph with n vertices using at most log n edge-detecting queries. [sent-171, score-0.899]
</p><p>52 Since we assume that the hypergraph is non-empty, the above equalities clearly hold for our initial assignment of S and A. [sent-175, score-0.558]
</p><p>53 The algorithm takes at most log n edge-detecting queries in total, as it makes one query in each iteration. [sent-182, score-0.578]
</p><p>54 8: Call FIND-ONE-EDGE on the hypergraph induced on S with the vertex v removed. [sent-192, score-0.627]
</p><p>55 In fact, each time, we make edge-detecting queries on the union of a subset of S and the set of vertices already found. [sent-198, score-0.396]
</p><p>56 Lemma 7 FIND-ONE-EDGE ﬁnds one edge in a non-empty hypergraph of dimension r with n vertices using r log n edge-detecting queries. [sent-202, score-0.871]
</p><p>57 It is evident that if FIND-ONE-EDGE uses (r − 1) log n queries for a hypergraph with dimension r − 1. [sent-204, score-0.927]
</p><p>58 then it only uses (r − 1) log n + log n = r log n queries for a hypergraph with dimension r. [sent-205, score-1.165]
</p><p>59 Algorithm 4 learns a uniform hypergraph with probability at least 1 − δ. [sent-210, score-0.609]
</p><p>60 Algorithm 4 ﬁnds one new edge at each iteration because F H is an independent covering family of the already found sub-hypergraph H. [sent-221, score-0.408]
</p><p>61 The query complexity will be O(22r m2 · r poly(r, log n) · log 1 ), which is quadratic in m. [sent-225, score-0.405]
</p><p>62 3 An Improved FIND-ONE-EDGE Despite the simplicity of FIND-ONE-EDGE, its queries have to be made in r log n rounds. [sent-227, score-0.415]
</p><p>63 When irrelevant vertices abound, that is, when n is much larger than m, we would like to arrange queries in a smaller number of rounds. [sent-228, score-0.377]
</p><p>64 In the following, we use a technique developed in Damaschke (1998) (for learning monotone boolean functions) to ﬁnd one edge from a non-empty hypergraph with high probability using only O(log m + r) rounds and O((log m + r) log n) queries. [sent-229, score-0.878]
</p><p>65 We say that we split a set A according to its ith (i ∈ [1, log n]) bit, we will divide A into two sets, one containing vertices whose ith bits are 0 and the other containing vertices whose ith bits are 1. [sent-241, score-0.447]
</p><p>66 One of the key ideas in Damaschke (1998) is that because the splits are pre-determined, and the queries are monotone in terms of subset relation, we can make queries on pre-determined splits to make predictions. [sent-247, score-0.635]
</p><p>67 15: if all queries are answered 1 then ∗ ∗ 16: A ← Ai , S ← Si (i∗ is the largest index in I in this case). [sent-262, score-0.375]
</p><p>68 We ﬁrst make queries on (S\A) ∪ A| bi =0 (= S\A|bi =1 ) and (S\A) ∪ A|bi =1 (= S\A|bi =0 ) for every i. [sent-266, score-0.416]
</p><p>69 case 1: If there exists i such that Ri = (0, 0), that is, both queries are answered 0, all edges contained in S are split between A|bi =0 and A|bi =1 , that is, the intersections of each edge with these two sets are a partition of the edge. [sent-268, score-0.752]
</p><p>70 case 2: If there exists i such that Ri = (1, 1), that is, both queries are answered 1, we can set S to be either of the two sets (S\A) ∪ A|bi =0 and (S\A) ∪ A|bi =1 as they both contain edges, and set A to be A|bi =0 or A|bi =1 respectively. [sent-274, score-0.401]
</p><p>71 Because they do not share a common edge as their intersection S\A does not contain an edge, the sum of the numbers of edges contained in these two sets is at most the number of edges contained in S. [sent-277, score-0.597]
</p><p>72 case 3: If neither of the two events happens, we need to deal with the third case where ∀i ∈ I, one of the queries is answered 0 and the other is answered 1. [sent-281, score-0.472]
</p><p>73 If all queries are answered 1, i∗ is the largest index in I and Ai is a singleton set containing a relevant vertex. [sent-293, score-0.447]
</p><p>74 At each iteration, we use at most 3 log n queries which are made in at most 2 rounds. [sent-301, score-0.415]
</p><p>75 Therefore, Lemma 8 In expectation, PARA-FIND-ONE-VERTEX ﬁnds one relevant vertex using O((log m + r) log n) queries, and the queries can be made in 2(log m + r) rounds. [sent-302, score-0.602]
</p><p>76 PARA-FIND-ONE-VERTEX can work with FIND-ONE-EDGE to ﬁnd an edge using expected O(r(log m + r) log n) queries in expected 2r(log m + r) rounds. [sent-303, score-0.574]
</p><p>77 In the new vertex ﬁnding process, 2225  A NGLUIN AND C HEN  instead of starting with A = V = S (recall in a recursive call of FIND-ONE-EDGE, we look for a relevant edge contained in the S. [sent-310, score-0.421]
</p><p>78 Lemma 9 There is an algorithm that ﬁnds an edge in a non-empty hypergraph using expected O((log m+r) log n) edge-detecting queries. [sent-315, score-0.808]
</p><p>79 Corollary 10 With probability at least 1−δ, PARA-FIND-ONE-EDGE ﬁnds an edge using O((log m+ r) log n log 1 ) edge-detecting queries, and the queries can be made in 4(log m + r) rounds. [sent-322, score-0.734]
</p><p>80 A Linear-Query Algorithm Reconstructing an independent covering family at the discovery of every new edge is indeed wasteful. [sent-324, score-0.443]
</p><p>81 The difference is that discovery probabilities reﬂect degrees in the hypergraph we have found, while threshold probabilities reﬂect degrees in the target hypergraph. [sent-339, score-0.675]
</p><p>82 Intuitively, a high-degree relevant set in the target hypergraph (not necessarily a high-degree relevant set in H), or a relevant set with small threshold probability is important, because an edge or a non-edge may not be found if its important relevant subsets are not found. [sent-371, score-1.035]
</p><p>83 2 The Algorithm Let H = (V, E) be the hypergraph the algorithm has found so far. [sent-377, score-0.53]
</p><p>84 The queries can be made in O(log m + r) rounds, as queries of each call to PARA-FIND-ONE-EDGE can be made in O(log m + r) rounds. [sent-382, score-0.622]
</p><p>85 Since ∑χ dH (χ) ≤ 2r m and ∑χ c(χ) ≤ (2r +1)m (note that c(χ) is one more than the 1 number of new edges that χ-samples in FH produce), the number of queries made at each iteration 1 is at most O(24r m · poly(r, log n, log δ )). [sent-401, score-0.714]
</p><p>86 Therefore, the total number of queries will be linear in the number of edges with high probability, as desired. [sent-402, score-0.447]
</p><p>87 Let H be the hypergraph the algorithm has found at the beginning of the iteration. [sent-405, score-0.53]
</p><p>88 Let H be the hypergraph the algorithm 1 has found before this group is processed. [sent-412, score-0.553]
</p><p>89 Assertion 17 If χ is inactive, then at the end of this iteration, either e has been found or a subset of e whose threshold probability is at most 1 pχ has been found (a relevant subset is found when an 2 edge that contains it is found). [sent-414, score-0.372]
</p><p>90 Let H be the hypergraph that has been found at the end of the iteration. [sent-434, score-0.512]
</p><p>91 Theorem 22 Ω((2m/r)r/2 ) edge-detecting queries are required to identify a hypergraph drawn from the class of all (r, r − 2)-uniform hypergraphs with n vertices and m edges. [sent-461, score-1.148]
</p><p>92 ∆  Theorem 23 Ω((2m/(∆ + 2))1+ 2 ) edge-detecting queries are required to identify a hypergraph drawn from the class of all (r, ∆)-uniform hypergraphs with n vertices and m edges. [sent-463, score-1.148]
</p><p>93 Proof Given a (∆ + 2, ∆)-uniform hypergraph H = (V, E), let H = (V ∪V , E ) be an (r, ∆)-uniform hypergraph, where |V | = r − ∆ − 2, V ∩V = φ and E = {e ∪V |e ∈ E}. [sent-464, score-0.512]
</p><p>94 Theorem 24 There is a randomized algorithm that learns an (r, ∆)-uniform hypergraph with m ∆ ∆ edges and n vertices with probability at least 1 − δ, using O(2 O((1+ 2 )r) · m1+ 2 · poly(log n, log 1 ))) δ queries. [sent-470, score-0.976]
</p><p>95 (We remark that this improvement is available for the uniform hypergraph problem in the case when |χ| = r − 1, but is not as important. [sent-487, score-0.533]
</p><p>96 / Deﬁnition 25 A (χ, ν, p)-sample (χ ∩ ν = 0) is a random set of vertices that contains χ and does not contain any vertex in ν and contains each other vertex independently with probability p. [sent-490, score-0.432]
</p><p>97 Algorithm 7 Learning an (r, ∆)-uniform hypergraph All PARA-FIND-ONE-EDGE’s are called with parameter δ . [sent-491, score-0.512]
</p><p>98 Let H be the hypergraph the algorithm has found before the group is processed. [sent-545, score-0.553]
</p><p>99 If no assertion is violated, the round complexity of Algorithm 7 is O((1 + ∆) · min(2r (log m + r)2 , (log m + r)3 )) 1 We can choose δ so that the algorithm succeeds with probability 1 − δ and log δ ≤ poly(r, log n) · log 1 . [sent-597, score-0.564]
</p><p>100 Therefore, the total number of queries the algorithm makes is bounded by ∆ ∆ 1 O(2O((1+ 2 )r) · m1+ 2 · poly(log n, log )). [sent-604, score-0.433]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hypergraph', 0.512), ('dh', 0.371), ('fh', 0.339), ('queries', 0.296), ('hypergraphs', 0.241), ('ph', 0.197), ('edge', 0.159), ('covering', 0.152), ('edges', 0.151), ('query', 0.145), ('bi', 0.12), ('log', 0.119), ('vertex', 0.115), ('ngluin', 0.102), ('ypergraph', 0.093), ('hen', 0.087), ('ln', 0.085), ('angluin', 0.085), ('vertices', 0.081), ('answered', 0.079), ('poly', 0.072), ('relevant', 0.072), ('idden', 0.065), ('discovery', 0.064), ('assertion', 0.058), ('alon', 0.056), ('assertions', 0.056), ('reaction', 0.056), ('succeeds', 0.052), ('phase', 0.051), ('chemicals', 0.047), ('grebinski', 0.046), ('equalities', 0.046), ('chen', 0.045), ('contained', 0.045), ('family', 0.045), ('probability', 0.041), ('beigel', 0.039), ('round', 0.038), ('newly', 0.035), ('draw', 0.035), ('learns', 0.035), ('threshold', 0.035), ('probabilities', 0.032), ('jiang', 0.032), ('divide', 0.031), ('call', 0.03), ('phases', 0.03), ('inactive', 0.03), ('ri', 0.03), ('iteration', 0.029), ('terminates', 0.029), ('kucherov', 0.028), ('noga', 0.028), ('calls', 0.028), ('earning', 0.028), ('contains', 0.027), ('contain', 0.026), ('construction', 0.026), ('iterations', 0.025), ('samples', 0.025), ('dna', 0.025), ('bits', 0.025), ('monotone', 0.024), ('react', 0.024), ('qh', 0.024), ('dnf', 0.024), ('halves', 0.024), ('independent', 0.023), ('ends', 0.023), ('group', 0.023), ('nds', 0.023), ('rounds', 0.023), ('ch', 0.023), ('pr', 0.022), ('lemma', 0.022), ('split', 0.022), ('quadratic', 0.022), ('dana', 0.021), ('nder', 0.021), ('hamiltonian', 0.021), ('ith', 0.021), ('uniform', 0.021), ('intersection', 0.02), ('families', 0.02), ('incident', 0.019), ('abbreviate', 0.019), ('excluding', 0.019), ('event', 0.019), ('subset', 0.019), ('randomized', 0.019), ('active', 0.019), ('asodi', 0.019), ('doubles', 0.019), ('algorithm', 0.018), ('drawn', 0.018), ('succeed', 0.018), ('shrink', 0.018), ('minimum', 0.018), ('events', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="53-tfidf-1" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>Author: Dana Angluin, Jiang Chen</p><p>Abstract: We consider the problem of learning a hypergraph using edge-detecting queries. In this model, the learner may query whether a set of vertices induces an edge of the hidden hypergraph or not. We show that an r-uniform hypergraph with m edges and n vertices is learnable with O(2 4r m · poly(r, log n)) queries with high probability. The queries can be made in O(min(2 r (log m + r)2 , (log m + r)3 )) rounds. We also give an algorithm that learns an almost uniform hypergraph of ∆ ∆ dimension r using O(2O((1+ 2 )r) · m1+ 2 · poly(log n)) queries with high probability, where ∆ is the difference between the maximum and the minimum edge sizes. This upper bound matches our ∆ lower bound of Ω(( m∆ )1+ 2 ) for this class of hypergraphs in terms of dependence on m. The 1+ 2 queries can also be made in O((1 + ∆) · min(2r (log m + r)2 , (log m + r)3 )) rounds. Keywords: query learning, hypergraph, multiple round algorithm, sampling, chemical reaction network</p><p>2 0.11793937 <a title="53-tfidf-2" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>Author: Thomas Kämpke</p><p>Abstract: Similarity of edge labeled graphs is considered in the sense of minimum squared distance between corresponding values. Vertex correspondences are established by isomorphisms if both graphs are of equal size and by subisomorphisms if one graph has fewer vertices than the other. Best ﬁt isomorphisms and subisomorphisms amount to solutions of quadratic assignment problems and are computed exactly as well as approximately by minimum cost ﬂow, linear assignment relaxations and related graph algorithms. Keywords: assignment problem, best approximation, branch and bound, inexact graph matching, model data base</p><p>3 0.092470042 <a title="53-tfidf-3" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>Author: Robert Castelo, Alberto Roverato</p><p>Abstract: Learning of large-scale networks of interactions from microarray data is an important and challenging problem in bioinformatics. A widely used approach is to assume that the available data constitute a random sample from a multivariate distribution belonging to a Gaussian graphical model. As a consequence, the prime objects of inference are full-order partial correlations which are partial correlations between two variables given the remaining ones. In the context of microarray data the number of variables exceed the sample size and this precludes the application of traditional structure learning procedures because a sampling version of full-order partial correlations does not exist. In this paper we consider limited-order partial correlations, these are partial correlations computed on marginal distributions of manageable size, and provide a set of rules that allow one to assess the usefulness of these quantities to derive the independence structure of the underlying Gaussian graphical model. Furthermore, we introduce a novel structure learning procedure based on a quantity, obtained from limited-order partial correlations, that we call the non-rejection rate. The applicability and usefulness of the procedure are demonstrated by both simulated and real data. Keywords: Gaussian distribution, gene network, graphical model, microarray data, non-rejection rate, partial correlation, small-sample inference</p><p>4 0.057595219 <a title="53-tfidf-4" href="./jmlr-2006-Toward_Attribute_Efficient_Learning_of_Decision_Lists_and_Parities.html">92 jmlr-2006-Toward Attribute Efficient Learning of Decision Lists and Parities</a></p>
<p>Author: Adam R. Klivans, Rocco A. Servedio</p><p>Abstract: We consider two well-studied problems regarding attribute efﬁcient learning: learning decision lists and learning parity functions. First, we give an algorithm for learning decision lists of length ˜ 1/3 ˜ 1/3 k over n variables using 2O(k ) log n examples and time nO(k ) . This is the ﬁrst algorithm for learning decision lists that has both subexponential sample complexity and subexponential running time in the relevant parameters. Our approach establishes a relationship between attribute efﬁcient learning and polynomial threshold functions and is based on a new construction of low degree, low weight polynomial threshold functions for decision lists. For a wide range of parameters our construction matches a lower bound due to Beigel for decision lists and gives an essentially optimal tradeoff between polynomial threshold function degree and weight. Second, we give an algorithm for learning an unknown parity function on k out of n variables using O(n1−1/k ) examples in poly(n) time. For k = o(log n) this yields a polynomial time algorithm with sample complexity o(n); this is the ﬁrst polynomial time algorithm for learning parity on a superconstant number of variables with sublinear sample complexity. We also give a simple algorithm for learning an unknown length-k parity using O(k log n) examples in nk/2 time, which improves on the naive nk time bound of exhaustive search. Keywords: PAC learning, attribute efﬁciency, learning parity, decision lists, Winnow</p><p>5 0.049184851 <a title="53-tfidf-5" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>Author: Ting Liu, Andrew W. Moore, Alexander Gray</p><p>Abstract: This paper is about non-approximate acceleration of high-dimensional nonparametric operations such as k nearest neighbor classiﬁers. We attempt to exploit the fact that even if we want exact answers to nonparametric queries, we usually do not need to explicitly ﬁnd the data points close to the query, but merely need to answer questions about the properties of that set of data points. This offers a small amount of computational leeway, and we investigate how much that leeway can be exploited. This is applicable to many algorithms in nonparametric statistics, memory-based learning and kernel-based learning. But for clarity, this paper concentrates on pure k-NN classiﬁcation. We introduce new ball-tree algorithms that on real-world data sets give accelerations from 2-fold to 100-fold compared against highly optimized traditional ball-tree-based k-NN . These results include data sets with up to 106 dimensions and 105 records, and demonstrate non-trivial speed-ups while giving exact answers. keywords: ball-tree, k-NN classiﬁcation</p><p>6 0.043577258 <a title="53-tfidf-6" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.038646042 <a title="53-tfidf-7" href="./jmlr-2006-Learning_the_Structure_of_Linear_Latent_Variable_Models.html">54 jmlr-2006-Learning the Structure of Linear Latent Variable Models</a></p>
<p>8 0.037037376 <a title="53-tfidf-8" href="./jmlr-2006-Some_Discriminant-Based_PAC_Algorithms.html">81 jmlr-2006-Some Discriminant-Based PAC Algorithms</a></p>
<p>9 0.034887653 <a title="53-tfidf-9" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>10 0.034839805 <a title="53-tfidf-10" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>11 0.034738347 <a title="53-tfidf-11" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.032964237 <a title="53-tfidf-12" href="./jmlr-2006-Action_Elimination_and_Stopping_Conditions_for_the_Multi-Armed_Bandit_and_Reinforcement_Learning_Problems.html">10 jmlr-2006-Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems</a></p>
<p>13 0.028301733 <a title="53-tfidf-13" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>14 0.027842367 <a title="53-tfidf-14" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>15 0.026774829 <a title="53-tfidf-15" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.026538305 <a title="53-tfidf-16" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.026208609 <a title="53-tfidf-17" href="./jmlr-2006-Generalized_Bradley-Terry_Models_and_Multi-Class_Probability_Estimates.html">34 jmlr-2006-Generalized Bradley-Terry Models and Multi-Class Probability Estimates</a></p>
<p>18 0.02498123 <a title="53-tfidf-18" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<p>19 0.024979368 <a title="53-tfidf-19" href="./jmlr-2006-Stability_Properties_of_Empirical_Risk_Minimization_over_Donsker_Classes.html">84 jmlr-2006-Stability Properties of Empirical Risk Minimization over Donsker Classes</a></p>
<p>20 0.023242969 <a title="53-tfidf-20" href="./jmlr-2006-Superior_Guarantees_for_Sequential_Prediction_and_Lossless_Compression_via_Alphabet_Decomposition.html">90 jmlr-2006-Superior Guarantees for Sequential Prediction and Lossless Compression via Alphabet Decomposition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.139), (1, -0.033), (2, -0.117), (3, 0.021), (4, -0.089), (5, 0.1), (6, 0.295), (7, -0.096), (8, -0.061), (9, 0.051), (10, -0.022), (11, 0.062), (12, 0.133), (13, -0.012), (14, -0.017), (15, 0.012), (16, 0.059), (17, 0.113), (18, -0.06), (19, -0.009), (20, -0.062), (21, -0.036), (22, 0.045), (23, 0.162), (24, -0.1), (25, -0.024), (26, -0.006), (27, -0.142), (28, 0.094), (29, -0.169), (30, -0.035), (31, 0.019), (32, -0.106), (33, 0.069), (34, -0.066), (35, 0.075), (36, 0.187), (37, -0.289), (38, -0.054), (39, 0.095), (40, 0.073), (41, 0.101), (42, -0.031), (43, 0.003), (44, 0.129), (45, 0.12), (46, -0.293), (47, 0.041), (48, 0.024), (49, 0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95706004 <a title="53-lsi-1" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>Author: Dana Angluin, Jiang Chen</p><p>Abstract: We consider the problem of learning a hypergraph using edge-detecting queries. In this model, the learner may query whether a set of vertices induces an edge of the hidden hypergraph or not. We show that an r-uniform hypergraph with m edges and n vertices is learnable with O(2 4r m · poly(r, log n)) queries with high probability. The queries can be made in O(min(2 r (log m + r)2 , (log m + r)3 )) rounds. We also give an algorithm that learns an almost uniform hypergraph of ∆ ∆ dimension r using O(2O((1+ 2 )r) · m1+ 2 · poly(log n)) queries with high probability, where ∆ is the difference between the maximum and the minimum edge sizes. This upper bound matches our ∆ lower bound of Ω(( m∆ )1+ 2 ) for this class of hypergraphs in terms of dependence on m. The 1+ 2 queries can also be made in O((1 + ∆) · min(2r (log m + r)2 , (log m + r)3 )) rounds. Keywords: query learning, hypergraph, multiple round algorithm, sampling, chemical reaction network</p><p>2 0.63870907 <a title="53-lsi-2" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>Author: Thomas Kämpke</p><p>Abstract: Similarity of edge labeled graphs is considered in the sense of minimum squared distance between corresponding values. Vertex correspondences are established by isomorphisms if both graphs are of equal size and by subisomorphisms if one graph has fewer vertices than the other. Best ﬁt isomorphisms and subisomorphisms amount to solutions of quadratic assignment problems and are computed exactly as well as approximately by minimum cost ﬂow, linear assignment relaxations and related graph algorithms. Keywords: assignment problem, best approximation, branch and bound, inexact graph matching, model data base</p><p>3 0.40203187 <a title="53-lsi-3" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>Author: Robert Castelo, Alberto Roverato</p><p>Abstract: Learning of large-scale networks of interactions from microarray data is an important and challenging problem in bioinformatics. A widely used approach is to assume that the available data constitute a random sample from a multivariate distribution belonging to a Gaussian graphical model. As a consequence, the prime objects of inference are full-order partial correlations which are partial correlations between two variables given the remaining ones. In the context of microarray data the number of variables exceed the sample size and this precludes the application of traditional structure learning procedures because a sampling version of full-order partial correlations does not exist. In this paper we consider limited-order partial correlations, these are partial correlations computed on marginal distributions of manageable size, and provide a set of rules that allow one to assess the usefulness of these quantities to derive the independence structure of the underlying Gaussian graphical model. Furthermore, we introduce a novel structure learning procedure based on a quantity, obtained from limited-order partial correlations, that we call the non-rejection rate. The applicability and usefulness of the procedure are demonstrated by both simulated and real data. Keywords: Gaussian distribution, gene network, graphical model, microarray data, non-rejection rate, partial correlation, small-sample inference</p><p>4 0.33044228 <a title="53-lsi-4" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>Author: Ting Liu, Andrew W. Moore, Alexander Gray</p><p>Abstract: This paper is about non-approximate acceleration of high-dimensional nonparametric operations such as k nearest neighbor classiﬁers. We attempt to exploit the fact that even if we want exact answers to nonparametric queries, we usually do not need to explicitly ﬁnd the data points close to the query, but merely need to answer questions about the properties of that set of data points. This offers a small amount of computational leeway, and we investigate how much that leeway can be exploited. This is applicable to many algorithms in nonparametric statistics, memory-based learning and kernel-based learning. But for clarity, this paper concentrates on pure k-NN classiﬁcation. We introduce new ball-tree algorithms that on real-world data sets give accelerations from 2-fold to 100-fold compared against highly optimized traditional ball-tree-based k-NN . These results include data sets with up to 106 dimensions and 105 records, and demonstrate non-trivial speed-ups while giving exact answers. keywords: ball-tree, k-NN classiﬁcation</p><p>5 0.26148883 <a title="53-lsi-5" href="./jmlr-2006-Action_Elimination_and_Stopping_Conditions_for_the_Multi-Armed_Bandit_and_Reinforcement_Learning_Problems.html">10 jmlr-2006-Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems</a></p>
<p>Author: Eyal Even-Dar, Shie Mannor, Yishay Mansour</p><p>Abstract: We incorporate statistical conﬁdence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it sufﬁces to pull the arms a total of O (n/ε2 ) log(1/δ) times to ﬁnd an ε-optimal arm with probability of at least 1 − δ. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the conﬁdence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over ε-greedy Q-learning.</p><p>6 0.22221984 <a title="53-lsi-6" href="./jmlr-2006-Toward_Attribute_Efficient_Learning_of_Decision_Lists_and_Parities.html">92 jmlr-2006-Toward Attribute Efficient Learning of Decision Lists and Parities</a></p>
<p>7 0.22180194 <a title="53-lsi-7" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>8 0.20731325 <a title="53-lsi-8" href="./jmlr-2006-Learning_the_Structure_of_Linear_Latent_Variable_Models.html">54 jmlr-2006-Learning the Structure of Linear Latent Variable Models</a></p>
<p>9 0.16983813 <a title="53-lsi-9" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.16227919 <a title="53-lsi-10" href="./jmlr-2006-Superior_Guarantees_for_Sequential_Prediction_and_Lossless_Compression_via_Alphabet_Decomposition.html">90 jmlr-2006-Superior Guarantees for Sequential Prediction and Lossless Compression via Alphabet Decomposition</a></p>
<p>11 0.15124142 <a title="53-lsi-11" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>12 0.15077667 <a title="53-lsi-12" href="./jmlr-2006-Lower_Bounds_and_Aggregation_in_Density_Estimation.html">58 jmlr-2006-Lower Bounds and Aggregation in Density Estimation</a></p>
<p>13 0.14927876 <a title="53-lsi-13" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.14291665 <a title="53-lsi-14" href="./jmlr-2006-On_the_Complexity_of_Learning_Lexicographic_Strategies.html">68 jmlr-2006-On the Complexity of Learning Lexicographic Strategies</a></p>
<p>15 0.13503303 <a title="53-lsi-15" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>16 0.13307279 <a title="53-lsi-16" href="./jmlr-2006-In_Search_of_Non-Gaussian_Components_of_a_High-Dimensional_Distribution.html">36 jmlr-2006-In Search of Non-Gaussian Components of a High-Dimensional Distribution</a></p>
<p>17 0.12469439 <a title="53-lsi-17" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>18 0.12334343 <a title="53-lsi-18" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>19 0.12205734 <a title="53-lsi-19" href="./jmlr-2006-Generalized_Bradley-Terry_Models_and_Multi-Class_Probability_Estimates.html">34 jmlr-2006-Generalized Bradley-Terry Models and Multi-Class Probability Estimates</a></p>
<p>20 0.1193097 <a title="53-lsi-20" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.016), (36, 0.068), (37, 0.373), (45, 0.019), (50, 0.057), (61, 0.045), (63, 0.027), (66, 0.013), (68, 0.014), (76, 0.045), (78, 0.017), (79, 0.022), (81, 0.029), (84, 0.017), (90, 0.02), (91, 0.048), (96, 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.6942997 <a title="53-lda-1" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>Author: Dana Angluin, Jiang Chen</p><p>Abstract: We consider the problem of learning a hypergraph using edge-detecting queries. In this model, the learner may query whether a set of vertices induces an edge of the hidden hypergraph or not. We show that an r-uniform hypergraph with m edges and n vertices is learnable with O(2 4r m · poly(r, log n)) queries with high probability. The queries can be made in O(min(2 r (log m + r)2 , (log m + r)3 )) rounds. We also give an algorithm that learns an almost uniform hypergraph of ∆ ∆ dimension r using O(2O((1+ 2 )r) · m1+ 2 · poly(log n)) queries with high probability, where ∆ is the difference between the maximum and the minimum edge sizes. This upper bound matches our ∆ lower bound of Ω(( m∆ )1+ 2 ) for this class of hypergraphs in terms of dependence on m. The 1+ 2 queries can also be made in O((1 + ∆) · min(2r (log m + r)2 , (log m + r)3 )) rounds. Keywords: query learning, hypergraph, multiple round algorithm, sampling, chemical reaction network</p><p>2 0.59430838 <a title="53-lda-2" href="./jmlr-2006-Linear_State-Space_Models_for_Blind_Source_Separation.html">57 jmlr-2006-Linear State-Space Models for Blind Source Separation</a></p>
<p>Author: Rasmus Kongsgaard Olsson, Lars Kai Hansen</p><p>Abstract: We apply a type of generative modelling to the problem of blind source separation in which prior knowledge about the latent source signals, such as time-varying auto-correlation and quasiperiodicity, are incorporated into a linear state-space model. In simulations, we show that in terms of signal-to-error ratio, the sources are inferred more accurately as a result of the inclusion of strong prior knowledge. We explore different schemes of maximum-likelihood optimization for the purpose of learning the model parameters. The Expectation Maximization algorithm, which is often considered the standard optimization method in this context, results in slow convergence when the noise variance is small. In such scenarios, quasi-Newton optimization yields substantial improvements in a range of signal to noise ratios. We analyze the performance of the methods on convolutive mixtures of speech signals. Keywords: blind source separation, state-space model, independent component analysis, convolutive model, EM, speech modelling</p><p>3 0.34427673 <a title="53-lda-3" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>Author: Thomas Kämpke</p><p>Abstract: Similarity of edge labeled graphs is considered in the sense of minimum squared distance between corresponding values. Vertex correspondences are established by isomorphisms if both graphs are of equal size and by subisomorphisms if one graph has fewer vertices than the other. Best ﬁt isomorphisms and subisomorphisms amount to solutions of quadratic assignment problems and are computed exactly as well as approximately by minimum cost ﬂow, linear assignment relaxations and related graph algorithms. Keywords: assignment problem, best approximation, branch and bound, inexact graph matching, model data base</p><p>4 0.33164769 <a title="53-lda-4" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>5 0.33016515 <a title="53-lda-5" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>Author: Mikio L. Braun</p><p>Abstract: The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to inﬁnity. We derive probabilistic ﬁnite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reﬂecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a signiﬁcant improvement over existing non-scaling bounds. Keywords: kernel matrix, eigenvalues, relative perturbation bounds</p><p>6 0.32369843 <a title="53-lda-6" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>7 0.32313013 <a title="53-lda-7" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>8 0.32123363 <a title="53-lda-8" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>9 0.32002884 <a title="53-lda-9" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>10 0.31770393 <a title="53-lda-10" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.31690615 <a title="53-lda-11" href="./jmlr-2006-Toward_Attribute_Efficient_Learning_of_Decision_Lists_and_Parities.html">92 jmlr-2006-Toward Attribute Efficient Learning of Decision Lists and Parities</a></p>
<p>12 0.31622189 <a title="53-lda-12" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.31509572 <a title="53-lda-13" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>14 0.31486759 <a title="53-lda-14" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<p>15 0.31472778 <a title="53-lda-15" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>16 0.31425261 <a title="53-lda-16" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>17 0.31347904 <a title="53-lda-17" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.31339344 <a title="53-lda-18" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>19 0.31235325 <a title="53-lda-19" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>20 0.31234631 <a title="53-lda-20" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
