<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-11" href="#">jmlr2006-11</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</h1>
<br/><p>Source: <a title="jmlr-2006-11-pdf" href="http://jmlr.org/papers/volume7/sugiyama06a/sugiyama06a.pdf">pdf</a></p><p>Author: Masashi Sugiyama</p><p>Abstract: The goal of active learning is to determine the locations of training input points so that the generalization error is minimized. We discuss the problem of active learning in linear regression scenarios. Traditional active learning methods using least-squares learning often assume that the model used for learning is correctly speciﬁed. In many practical situations, however, this assumption may not be fulﬁlled. Recently, active learning methods using “importance”-weighted least-squares learning have been proposed, which are shown to be robust against misspeciﬁcation of models. In this paper, we propose a new active learning method also using the weighted least-squares learning, which we call ALICE (Active Learning using the Importance-weighted least-squares learning based on Conditional Expectation of the generalization error). An important difference from existing methods is that we predict the conditional expectation of the generalization error given training input points, while existing methods predict the full expectation of the generalization error. Due to this difference, the training input design can be ﬁne-tuned depending on the realization of training input points. Theoretically, we prove that the proposed active learning criterion is a more accurate predictor of the single-trial generalization error than the existing criterion. Numerical studies with toy and benchmark data sets show that the proposed method compares favorably to existing methods. Keywords: Active Learning, Conditional Expectation of Generalization Error, Misspeciﬁcation of Models, Importance-Weighted Least-Squares Learning, Covariate Shift.</p><p>Reference: <a title="jmlr-2006-11-reference" href="../jmlr2006_reference/jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 JP  Department of Computer Science Tokyo Institute of Technology 2-12-1, O-okayama, Meguro-ku, Tokyo, 152-8552, Japan  Editor: Greg Ridgeway  Abstract The goal of active learning is to determine the locations of training input points so that the generalization error is minimized. [sent-4, score-0.588]
</p><p>2 Traditional active learning methods using least-squares learning often assume that the model used for learning is correctly speciﬁed. [sent-6, score-0.228]
</p><p>3 In this paper, we propose a new active learning method also using the weighted least-squares learning, which we call ALICE (Active Learning using the Importance-weighted least-squares learning based on Conditional Expectation of the generalization error). [sent-9, score-0.304]
</p><p>4 An important difference from existing methods is that we predict the conditional expectation of the generalization error given training input points, while existing methods predict the full expectation of the generalization error. [sent-10, score-0.571]
</p><p>5 Due to this difference, the training input design can be ﬁne-tuned depending on the realization of training input points. [sent-11, score-0.431]
</p><p>6 Theoretically, we prove that the proposed active learning criterion is a more accurate predictor of the single-trial generalization error than the existing criterion. [sent-12, score-0.416]
</p><p>7 Introduction In a standard setting of supervised learning, the training input points are provided from the environment (Vapnik, 1998). [sent-16, score-0.25]
</p><p>8 On the other hand, there are cases where the location of the training input points can be designed by users (Fedorov, 1972; Pukelsheim, 1993). [sent-17, score-0.299]
</p><p>9 In such situations, it is expected that the accuracy of learned results can be improved by appropriately choosing the location of the training input points, e. [sent-18, score-0.263]
</p><p>10 , by densely allocating the training input points in the regions with high uncertainty. [sent-20, score-0.25]
</p><p>11 , 1996; Fukumizu, 2000)—also referred to as experimental design in statistics (Kiefer, 1959; Fedorov, 1972; Pukelsheim, 1993)—is the problem of optimizing location of training input points so that the generalization error is minimized. [sent-22, score-0.438]
</p><p>12 In active learning research, it is often assumed that the model used for learning is correctly speciﬁed (Fedorov, 1972; Cohn et al. [sent-24, score-0.228]
</p><p>13 S UGIYAMA  with OLS tries to determine the location of the training input points so that the variance term is minimized (Fedorov, 1972). [sent-30, score-0.321]
</p><p>14 Active learning is a situation under the covariate shift (Shimodaira, 2000), where the training input distribution is different from the test input distribution. [sent-32, score-0.384]
</p><p>15 The key idea of this WLS is the use of the ratio of density functions of test and training input points: the goodness-of-ﬁt of the training input points is adjusted to that of the test input points by the density ratio, which is similar to importance sampling. [sent-36, score-0.806]
</p><p>16 In this paper, we propose a variance-only active learning method using WLS, which can be regarded as an extension of the traditional variance-only active learning method using OLS. [sent-37, score-0.398]
</p><p>17 Conditional Expectation of Generalization Error: A variance-only active learning method using WLS has also been proposed by Wiens (2000), which can also be theoretically justiﬁed for approximately correct models. [sent-39, score-0.247]
</p><p>18 The important difference is how the generalization error is predicted: we predict the conditional expectation of the generalization error given training input points, while in Wiens (2000), the full expectation of the generalization error is predicted. [sent-40, score-0.69]
</p><p>19 In order to explain this difference in more detail, we ﬁrst note that the generalization error of the WLS estimator depends on the training input density since WLS explicitly uses it. [sent-41, score-0.429]
</p><p>20 Therefore, when WLS is used in active learning, the generalization error is predicted as a function of the training input density, and the training input density is optimized so that the predicted generalization error is minimized. [sent-42, score-0.958]
</p><p>21 The parameters in the model are learned using the training examples, which consist of training input points drawn from the user-designed distribution and corresponding noisy output values. [sent-43, score-0.389]
</p><p>22 This means that the generalization error is a random variable which depends on the location of the training input points and noise contained in the training output values. [sent-44, score-0.576]
</p><p>23 , the generalization error for a single realization of the training examples at hand. [sent-47, score-0.271]
</p><p>24 From this viewpoint, we do not want to average out the random variables, but we want to plug the realization of the random variables into the generalization error and evaluate the realized value of the generalization error. [sent-48, score-0.293]
</p><p>25 However, we may not be able to avoid taking the expectation over the training output noise since the training output noise is inaccessible. [sent-49, score-0.317]
</p><p>26 In contrast, the location of the training input points are accessible by nature. [sent-50, score-0.33]
</p><p>27 Motivated by this fact, in this paper, we predict the generalization error without taking the expectation over the training input points. [sent-51, score-0.371]
</p><p>28 That is, we predict the conditional expectation of the generalization error given training input points. [sent-52, score-0.371]
</p><p>29 On the other hand, in Wiens (2000), the generalization error is predicted in terms of the expectation over both the training input points and the training output noise. [sent-53, score-0.546]
</p><p>30 The solid curves in the left graph (Figure 1-(a)) depict G pa (ε|x), the generalization error for a training input density pa as a function of the training output noise ε given a training input point x. [sent-56, score-0.862]
</p><p>31 The three solid curves correspond to the cases where the realizations of the training input point x are a1 , a2 , 142  ACTIVE L EARNING IN A PPROXIMATELY L INEAR R EGRESSION  (a)  (b)  Figure 1: Schematic illustration of conditional expectation and full expectation of the generalization error. [sent-57, score-0.43]
</p><p>32 The value of the generalization error for the density pa in the full-expectation approach is depicted by the dash-dotted line, where the generalization error is expected over both the training output noise ε and the training input points x (i. [sent-60, score-0.791]
</p><p>33 The values of the generalization error in the conditional-expectation approach are depicted by the dotted lines, where the generalization errors are expected only over the training output noise ε, given x = a1 , a2 , a3 , respectively (i. [sent-63, score-0.382]
</p><p>34 The right graph (Figure 1-(b)) depicts the generalization errors for the training input density pb in the same manner. [sent-66, score-0.448]
</p><p>35 In the full-expectation framework, the density pa is judged to be better than pb regardless of the realization of the training input point since the dash-dotted line in the left graph is lower than that in the right graph (see Figure 1 again). [sent-67, score-0.447]
</p><p>36 On the other hand, in the conditional-expectation framework, the goodness of the density is adaptively judged depending on the realizations of the training input point x. [sent-69, score-0.319]
</p><p>37 That is, the conditional-expectation framework may yield a better choice of the training input density (and the training input points) than the full-expectation framework. [sent-71, score-0.481]
</p><p>38 Theoretically, we prove that the proposed active learning criterion derived in the conditional-expectation framework is a better predictor of the single-trial generalization error than the full-expectation active learning criterion proposed by Wiens (2000). [sent-73, score-0.609]
</p><p>39 This method is not variance-only, but it takes both the bias and the variance into account by gathering training input points in two stages. [sent-77, score-0.301]
</p><p>40 In the ﬁrst stage, a certain number of training examples are randomly gathered from the environment, and the generalization error (i. [sent-78, score-0.269]
</p><p>41 Then in the second stage, the training input density for the remaining training examples is optimized based on the generalization error prediction. [sent-81, score-0.512]
</p><p>42 Theoretically, the two-stage method is shown to asymptotically give the optimal training input density not only for approximately correct models, but also for totally misspeciﬁed models. [sent-82, score-0.382]
</p><p>43 A drawback of this method is that it requires some randomly collected training examples in the ﬁrst stage, so we are not allowed to optimally design all the training input locations by ourselves. [sent-84, score-0.274]
</p><p>44 An alternative approach is the batch approach, where all training input points are gathered in the beginning. [sent-87, score-0.331]
</p><p>45 In this scenario, the sequential approach would be natural: estimating the unknown learning target function and optimizing location of the training input points are carried out alternately. [sent-98, score-0.334]
</p><p>46 This makes it possible to take the batch approach of determining all the training input points at once in advance. [sent-100, score-0.284]
</p><p>47 On the other hand, the sequential approach is computationally efﬁcient since only one or a few training input points are optimized in each iteration (Cohn et al. [sent-102, score-0.25]
</p><p>48 In this paper, we avoid the computational difﬁculty of the batch approach not by resorting to the sequential approach, but by optimizing the training input distribution, rather than directly optimizing the training input points themselves. [sent-104, score-0.475]
</p><p>49 seems to be a popular approach in batch active learning research (Wiens, 2000; Kanamori and Shimodaira, 2003). [sent-106, score-0.233]
</p><p>50 Derivation of New Active Learning Method In this section, we formulate the active learning problem in regression scenarios, and derive a new active learning method. [sent-112, score-0.398]
</p><p>51 We suppose that the training i=1 input points {xi }n are independently drawn from a user-deﬁned distribution with density p(x). [sent-119, score-0.349]
</p><p>52 We evaluate i=1 the goodness of the learned function f (x) by the expected squared test error over test input points, to which refer as the generalization error. [sent-121, score-0.27]
</p><p>53 When the test input points are drawn independently from a distribution with density q(x), the generalization error G is expressed as G =  Z  f (x) − f (x)  2  q(x)dx. [sent-122, score-0.405]
</p><p>54 In the following, we discuss the problem of optimizing the training input density p(x) so that the generalization error is minimized. [sent-132, score-0.429]
</p><p>55 3 Bias/Variance Decomposition of Generalization Error As described in Section 1, we evaluate the generalization error in terms of the expectation over only the training output noise {εi }n , not over the training input points {xi }n . [sent-160, score-0.568]
</p><p>56 When the training input points {xi }n are drawn from q(x), OLS is asymptotically unbiased even for i=1 misspeciﬁed models. [sent-177, score-0.324]
</p><p>57 However, the current situation is under the covariate shift (Shimodaira, 2000), where the training input density p(x) is generally different from the test input density q(x). [sent-178, score-0.582]
</p><p>58 For each p ∈ P (p) Create training input points {xi }n following p(x). [sent-192, score-0.25]
</p><p>59 (11), we propose determining the training input density p(x) as follows: For a set P of strictly positive probability densities, p∗ = argmin J(p), p∈P  where J = tr(ULW LW ). [sent-207, score-0.332]
</p><p>60 A pseudo code of the proposed active learning algorithm is described in Figure 4, which we call ALICE (Active Learning using the Importance-weighted leastsquares learning based on Conditional Expectation of the generalization error). [sent-209, score-0.304]
</p><p>61 Note that the value (p) of J depends not only on p(x), but also on the realization of the training input points {xi }n . [sent-210, score-0.299]
</p><p>62 {εi }  149  S UGIYAMA  Based on the above expression, the training input density p(x) is determined3 as follows (Fedorov, 1972; Cohn et al. [sent-219, score-0.29]
</p><p>63 Therefore, we do not have to optimize the training input density p(x), but we can directly optimize training input points {xi }n . [sent-234, score-0.54]
</p><p>64 Based on this approximation, the training input density p(x) is determined as follows. [sent-243, score-0.29]
</p><p>65 n  (17)  ∗ Comparison with J: A notable feature of JW is that the optimal training input density pW (x) can be obtained analytically (Wiens, 2000):  ∗ pW (x) = R  h(x) h(x)dx  ,  (18)  where b  h(x) = q(x)  −1 ∑ Ui, j ϕi (x)ϕ j (x)  1 2  . [sent-245, score-0.323]
</p><p>66 In the active learning context, we are interested in accurately predicting the single-trial generalization error GW , which depends on the realization of the training examples. [sent-263, score-0.47]
</p><p>67 (14) is a two-stage sampling scheme proposed5 by Kanamori and Shimodaira (2003): the training examples sampled in the ﬁrst stage are used for estimating H and in the second stage, the distribution of the remaining training input points is optimized based on the estimated H. [sent-273, score-0.359]
</p><p>68 First, (≤ n) training input points {xi }i=1 are created independently following the test input distribution with density q(x), and corresponding training output values {yi }i=1 are observed. [sent-275, score-0.573]
</p><p>69 Based on the above approximations, the training input density p(x) is determined as follows: p∗ = argmin JOW (p), OW p∈P  where  1 −1 JOW = tr(U H). [sent-289, score-0.332]
</p><p>70 n− After determining the optimal density p∗ , the remaining n − training input points {xi }i=1 OW ∗ (x), and corresponding training output values {y }n− are are created independently following pOW i i=1 observed. [sent-291, score-0.465]
</p><p>71 First, training input points should be gathered following q(x) in the ﬁrst stage, which implies that users are only allowed to optimize the location of n − remaining training input points. [sent-301, score-0.537]
</p><p>72 Let the test input density q(x) be the Gaussian density with mean 0. [sent-320, score-0.306]
</p><p>73 As a set of training input densities, P , we use the Gaussian densities with mean 0. [sent-336, score-0.229]
</p><p>74 5  2  Figure 5: Learning target function and input density functions. [sent-371, score-0.242]
</p><p>75 (OW): First, training input points are created following the test input density q(x), and corresponding training output values are observed. [sent-380, score-0.573]
</p><p>76 Then n − remaining training input points are created following the determined input density. [sent-383, score-0.358]
</p><p>77 (Passive): Following the test input density q(x), training input points {xi }n are created. [sent-391, score-0.457]
</p><p>78 The dashed curves show the means of the generalization error that corresponding active learning criteria are trying to predict. [sent-592, score-0.363]
</p><p>79 The dashed curves show the means of the generalization error that corresponding active learning criteria are trying to predict. [sent-599, score-0.363]
</p><p>80 Note that JW does not depend on the training input points {xi }n so it does not ﬂuctuate i=1 over 1000 runs. [sent-610, score-0.25]
</p><p>81 On the other hand, the generalization error, of course, depends on the learning target function even if the constant C is not included, since the training output values depend on it. [sent-628, score-0.256]
</p><p>82 Actually, in this case, training input densities that approximately minimize GW , GO , and GOW were successfully found by (ALICE), (W), (OW), and (O). [sent-644, score-0.251]
</p><p>83 This implies that the difference in the error is caused not by the quality of the active learning criteria, but by the difference between WLS and OLS: WLS generally has 157  S UGIYAMA  δ=0 2. [sent-645, score-0.233]
</p><p>84 From this pool of unlabeled samples, we choose n = 300 input points {xi }n for training and observe the corresponding output values {yi }n . [sent-752, score-0.328]
</p><p>85 160  ACTIVE L EARNING IN A PPROXIMATELY L INEAR R EGRESSION  We select the training input density p(x) from the set of uncorrelated multi-dimensional Gaussian densities with mean µMLE and standard deviation cγMLE , where c = 0. [sent-849, score-0.328]
</p><p>86 In this simulation, we can not create the training input points in an arbitrary location because we only have 8192 samples in the pool. [sent-861, score-0.299]
</p><p>87 Here, we ﬁrst create provisional input points following the determined training input density, and then choose the input points from the pool of unlabeled samples that are closest to the provisional input points. [sent-862, score-0.72]
</p><p>88 In this simulation, the expectation over the test input density q(x) in the matrix U is calculated by the empirical average over all 8192 unlabeled samples since the true test error is also calculated as such. [sent-863, score-0.327]
</p><p>89 The numerical study showed that the proposed method works well overall and compares favorably to existing WLS-based methods and the passive learning scheme. [sent-883, score-0.24]
</p><p>90 However, an important difference is that we predict the conditional expectation of the generalization error given training input points, while in Wiens (2000), the full expectation of the generalization error is predicted. [sent-889, score-0.551]
</p><p>91 As described in Section 1, the conditional-expectation approach conceptually gives a ﬁner choice of the training input density 161  S UGIYAMA  than the full-expectation approach. [sent-890, score-0.29]
</p><p>92 An advantage of Wiens’s criterion is that the optimal training input density can be obtained analytically, while we do not yet have such an analytic solution for the proposed criterion. [sent-893, score-0.311]
</p><p>93 In practice, using a set of input densities which consist of the optimal density analytically found by Wiens’s criterion and its variants would be a reasonable choice. [sent-896, score-0.299]
</p><p>94 In experiments with benchmark data sets, the test input density is indeed unknown and is approximated by a Gaussian density. [sent-900, score-0.229]
</p><p>95 Although the simulation results showed that the proposed method consistently outperforms the passive learning scheme (given unlabeled samples), a more detailed analysis should be carried out to see how approximating the test input density affects the performance. [sent-901, score-0.445]
</p><p>96 However, when the model is totally misspeciﬁed, even learning with the optimal training input points may not work well because of the model error. [sent-904, score-0.286]
</p><p>97 In most of the active learning research—including the current paper, the location of the training input points are designed for a single model at hand. [sent-906, score-0.498]
</p><p>98 However, in practice, we may want to select the model as well as the location of the training input points. [sent-908, score-0.24]
</p><p>99 Devising a method for simultaneously optimizing the model and the location of the training input points would therefore be a more important and promising future direction. [sent-909, score-0.299]
</p><p>100 In Sugiyama and Ogawa (2003), a method of active learning with model selection has been proposed for the trigonometric polynomial models. [sent-910, score-0.234]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('jw', 0.46), ('alice', 0.275), ('jo', 0.251), ('misspeci', 0.251), ('jow', 0.24), ('gw', 0.226), ('wls', 0.22), ('active', 0.199), ('wiens', 0.167), ('passive', 0.167), ('shimodaira', 0.157), ('ols', 0.151), ('lw', 0.125), ('pproximately', 0.125), ('ow', 0.119), ('ugiyama', 0.115), ('gow', 0.115), ('kanamori', 0.115), ('input', 0.108), ('egression', 0.106), ('generalization', 0.105), ('density', 0.099), ('dx', 0.091), ('training', 0.083), ('fedorov', 0.073), ('inear', 0.072), ('tr', 0.072), ('pw', 0.066), ('mle', 0.062), ('sugiyama', 0.062), ('points', 0.059), ('lo', 0.058), ('pb', 0.053), ('covariate', 0.051), ('location', 0.049), ('xk', 0.049), ('realization', 0.049), ('zr', 0.048), ('gathered', 0.047), ('unlabeled', 0.045), ('fukumizu', 0.044), ('kin', 0.042), ('ulw', 0.042), ('vo', 0.042), ('argmin', 0.042), ('expectation', 0.041), ('unbiased', 0.04), ('densities', 0.038), ('go', 0.037), ('earning', 0.037), ('totally', 0.036), ('zg', 0.035), ('bo', 0.035), ('trigonometric', 0.035), ('target', 0.035), ('shift', 0.034), ('asymptotically', 0.034), ('error', 0.034), ('batch', 0.034), ('toy', 0.033), ('output', 0.033), ('xi', 0.033), ('pro', 0.033), ('analytically', 0.033), ('bw', 0.032), ('accessible', 0.031), ('ogawa', 0.031), ('ulo', 0.031), ('predictor', 0.03), ('correctly', 0.029), ('bank', 0.029), ('judged', 0.029), ('bias', 0.029), ('cohn', 0.029), ('solid', 0.027), ('existing', 0.027), ('delve', 0.027), ('vw', 0.027), ('simulation', 0.026), ('stage', 0.026), ('correctness', 0.026), ('theoretically', 0.026), ('pa', 0.026), ('curves', 0.025), ('yi', 0.024), ('works', 0.024), ('outperformed', 0.023), ('learned', 0.023), ('favorably', 0.022), ('approximately', 0.022), ('benchmark', 0.022), ('noise', 0.022), ('variance', 0.022), ('experimentally', 0.021), ('criterion', 0.021), ('le', 0.021), ('dzr', 0.021), ('prepare', 0.021), ('provisional', 0.021), ('pukelsheim', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="11-tfidf-1" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>Author: Masashi Sugiyama</p><p>Abstract: The goal of active learning is to determine the locations of training input points so that the generalization error is minimized. We discuss the problem of active learning in linear regression scenarios. Traditional active learning methods using least-squares learning often assume that the model used for learning is correctly speciﬁed. In many practical situations, however, this assumption may not be fulﬁlled. Recently, active learning methods using “importance”-weighted least-squares learning have been proposed, which are shown to be robust against misspeciﬁcation of models. In this paper, we propose a new active learning method also using the weighted least-squares learning, which we call ALICE (Active Learning using the Importance-weighted least-squares learning based on Conditional Expectation of the generalization error). An important difference from existing methods is that we predict the conditional expectation of the generalization error given training input points, while existing methods predict the full expectation of the generalization error. Due to this difference, the training input design can be ﬁne-tuned depending on the realization of training input points. Theoretically, we prove that the proposed active learning criterion is a more accurate predictor of the single-trial generalization error than the existing criterion. Numerical studies with toy and benchmark data sets show that the proposed method compares favorably to existing methods. Keywords: Active Learning, Conditional Expectation of Generalization Error, Misspeciﬁcation of Models, Importance-Weighted Least-Squares Learning, Covariate Shift.</p><p>2 0.074884325 <a title="11-tfidf-2" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>Author: Hema Raghavan, Omid Madani, Rosie Jones</p><p>Abstract: We extend the traditional active learning framework to include feedback on features in addition to labeling instances, and we execute a careful study of the effects of feature selection and human feedback on features in the setting of text categorization. Our experiments on a variety of categorization tasks indicate that there is signiﬁcant potential in improving classiﬁer performance by feature re-weighting, beyond that achieved via membership queries alone (traditional active learning) if we have access to an oracle that can point to the important (most predictive) features. Our experiments on human subjects indicate that human feedback on feature relevance can identify a sufﬁcient proportion of the most relevant features (over 50% in our experiments). We ﬁnd that on average, labeling a feature takes much less time than labeling a document. We devise an algorithm that interleaves labeling features and documents which signiﬁcantly accelerates standard active learning in our simulation experiments. Feature feedback can complement traditional active learning in applications such as news ﬁltering, e-mail classiﬁcation, and personalization, where the human teacher can have signiﬁcant knowledge on the relevance of features. Keywords: active learning, feature selection, relevance feedback, term feedback, text classiﬁcation</p><p>3 0.060988903 <a title="11-tfidf-3" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>4 0.048592992 <a title="11-tfidf-4" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>5 0.04299318 <a title="11-tfidf-5" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Katya Scheinberg</p><p>Abstract: We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efﬁcient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims’ SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difﬁcult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight , the generalization properties of these two approaches are identical and we do not discuss them in the paper. Keywords: active set methods, support vector machines, quadratic programming</p><p>6 0.040688943 <a title="11-tfidf-6" href="./jmlr-2006-In_Search_of_Non-Gaussian_Components_of_a_High-Dimensional_Distribution.html">36 jmlr-2006-In Search of Non-Gaussian Components of a High-Dimensional Distribution</a></p>
<p>7 0.040378217 <a title="11-tfidf-7" href="./jmlr-2006-Lower_Bounds_and_Aggregation_in_Density_Estimation.html">58 jmlr-2006-Lower Bounds and Aggregation in Density Estimation</a></p>
<p>8 0.03665524 <a title="11-tfidf-8" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>9 0.033874456 <a title="11-tfidf-9" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>10 0.033804514 <a title="11-tfidf-10" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.030627282 <a title="11-tfidf-11" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>12 0.027486878 <a title="11-tfidf-12" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>13 0.026877407 <a title="11-tfidf-13" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.026217146 <a title="11-tfidf-14" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.025155645 <a title="11-tfidf-15" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.025069116 <a title="11-tfidf-16" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>17 0.024982048 <a title="11-tfidf-17" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.024689328 <a title="11-tfidf-18" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>19 0.02391069 <a title="11-tfidf-19" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>20 0.022903966 <a title="11-tfidf-20" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.141), (1, -0.067), (2, 0.02), (3, 0.016), (4, -0.008), (5, 0.034), (6, -0.068), (7, -0.022), (8, 0.003), (9, 0.053), (10, -0.015), (11, 0.115), (12, -0.019), (13, -0.018), (14, -0.027), (15, 0.019), (16, -0.033), (17, 0.037), (18, 0.058), (19, -0.052), (20, 0.062), (21, -0.218), (22, 0.183), (23, 0.074), (24, -0.395), (25, -0.055), (26, 0.21), (27, 0.012), (28, 0.16), (29, -0.131), (30, -0.053), (31, -0.177), (32, -0.018), (33, -0.003), (34, 0.053), (35, 0.058), (36, -0.121), (37, 0.054), (38, 0.069), (39, -0.052), (40, -0.234), (41, -0.234), (42, -0.007), (43, 0.093), (44, -0.079), (45, 0.126), (46, 0.207), (47, -0.143), (48, 0.013), (49, -0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95051819 <a title="11-lsi-1" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>Author: Masashi Sugiyama</p><p>Abstract: The goal of active learning is to determine the locations of training input points so that the generalization error is minimized. We discuss the problem of active learning in linear regression scenarios. Traditional active learning methods using least-squares learning often assume that the model used for learning is correctly speciﬁed. In many practical situations, however, this assumption may not be fulﬁlled. Recently, active learning methods using “importance”-weighted least-squares learning have been proposed, which are shown to be robust against misspeciﬁcation of models. In this paper, we propose a new active learning method also using the weighted least-squares learning, which we call ALICE (Active Learning using the Importance-weighted least-squares learning based on Conditional Expectation of the generalization error). An important difference from existing methods is that we predict the conditional expectation of the generalization error given training input points, while existing methods predict the full expectation of the generalization error. Due to this difference, the training input design can be ﬁne-tuned depending on the realization of training input points. Theoretically, we prove that the proposed active learning criterion is a more accurate predictor of the single-trial generalization error than the existing criterion. Numerical studies with toy and benchmark data sets show that the proposed method compares favorably to existing methods. Keywords: Active Learning, Conditional Expectation of Generalization Error, Misspeciﬁcation of Models, Importance-Weighted Least-Squares Learning, Covariate Shift.</p><p>2 0.45718193 <a title="11-lsi-2" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>Author: Hema Raghavan, Omid Madani, Rosie Jones</p><p>Abstract: We extend the traditional active learning framework to include feedback on features in addition to labeling instances, and we execute a careful study of the effects of feature selection and human feedback on features in the setting of text categorization. Our experiments on a variety of categorization tasks indicate that there is signiﬁcant potential in improving classiﬁer performance by feature re-weighting, beyond that achieved via membership queries alone (traditional active learning) if we have access to an oracle that can point to the important (most predictive) features. Our experiments on human subjects indicate that human feedback on feature relevance can identify a sufﬁcient proportion of the most relevant features (over 50% in our experiments). We ﬁnd that on average, labeling a feature takes much less time than labeling a document. We devise an algorithm that interleaves labeling features and documents which signiﬁcantly accelerates standard active learning in our simulation experiments. Feature feedback can complement traditional active learning in applications such as news ﬁltering, e-mail classiﬁcation, and personalization, where the human teacher can have signiﬁcant knowledge on the relevance of features. Keywords: active learning, feature selection, relevance feedback, term feedback, text classiﬁcation</p><p>3 0.26739281 <a title="11-lsi-3" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Katya Scheinberg</p><p>Abstract: We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efﬁcient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims’ SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difﬁcult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight , the generalization properties of these two approaches are identical and we do not discuss them in the paper. Keywords: active set methods, support vector machines, quadratic programming</p><p>4 0.25933373 <a title="11-lsi-4" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>5 0.22430605 <a title="11-lsi-5" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>6 0.19453447 <a title="11-lsi-6" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.1866927 <a title="11-lsi-7" href="./jmlr-2006-In_Search_of_Non-Gaussian_Components_of_a_High-Dimensional_Distribution.html">36 jmlr-2006-In Search of Non-Gaussian Components of a High-Dimensional Distribution</a></p>
<p>8 0.18317854 <a title="11-lsi-8" href="./jmlr-2006-Learning_Minimum_Volume_Sets.html">48 jmlr-2006-Learning Minimum Volume Sets</a></p>
<p>9 0.18262538 <a title="11-lsi-9" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>10 0.17066814 <a title="11-lsi-10" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.1692813 <a title="11-lsi-11" href="./jmlr-2006-Lower_Bounds_and_Aggregation_in_Density_Estimation.html">58 jmlr-2006-Lower Bounds and Aggregation in Density Estimation</a></p>
<p>12 0.16803817 <a title="11-lsi-12" href="./jmlr-2006-Segmental_Hidden_Markov_Models_with_Random_Effects_for_Waveform_Modeling.html">80 jmlr-2006-Segmental Hidden Markov Models with Random Effects for Waveform Modeling</a></p>
<p>13 0.16243438 <a title="11-lsi-13" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>14 0.1556107 <a title="11-lsi-14" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>15 0.15453246 <a title="11-lsi-15" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>16 0.15064462 <a title="11-lsi-16" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.140545 <a title="11-lsi-17" href="./jmlr-2006-Inductive_Synthesis_of_Functional_Programs%3A_An_Explanation_Based_Generalization_Approach_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">39 jmlr-2006-Inductive Synthesis of Functional Programs: An Explanation Based Generalization Approach     (Special Topic on Inductive Programming)</a></p>
<p>18 0.14021119 <a title="11-lsi-18" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>19 0.13973178 <a title="11-lsi-19" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>20 0.13763992 <a title="11-lsi-20" href="./jmlr-2006-Considering_Cost_Asymmetry_in_Learning_Classifiers.html">22 jmlr-2006-Considering Cost Asymmetry in Learning Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.424), (8, 0.028), (36, 0.08), (45, 0.023), (50, 0.062), (63, 0.037), (68, 0.013), (76, 0.019), (78, 0.026), (81, 0.039), (84, 0.023), (90, 0.018), (91, 0.034), (96, 0.087)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.67249835 <a title="11-lda-1" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>Author: Masashi Sugiyama</p><p>Abstract: The goal of active learning is to determine the locations of training input points so that the generalization error is minimized. We discuss the problem of active learning in linear regression scenarios. Traditional active learning methods using least-squares learning often assume that the model used for learning is correctly speciﬁed. In many practical situations, however, this assumption may not be fulﬁlled. Recently, active learning methods using “importance”-weighted least-squares learning have been proposed, which are shown to be robust against misspeciﬁcation of models. In this paper, we propose a new active learning method also using the weighted least-squares learning, which we call ALICE (Active Learning using the Importance-weighted least-squares learning based on Conditional Expectation of the generalization error). An important difference from existing methods is that we predict the conditional expectation of the generalization error given training input points, while existing methods predict the full expectation of the generalization error. Due to this difference, the training input design can be ﬁne-tuned depending on the realization of training input points. Theoretically, we prove that the proposed active learning criterion is a more accurate predictor of the single-trial generalization error than the existing criterion. Numerical studies with toy and benchmark data sets show that the proposed method compares favorably to existing methods. Keywords: Active Learning, Conditional Expectation of Generalization Error, Misspeciﬁcation of Models, Importance-Weighted Least-Squares Learning, Covariate Shift.</p><p>2 0.32711571 <a title="11-lda-2" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>3 0.3236101 <a title="11-lda-3" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram. Keywords: spectral clustering, blind source separation, computational auditory scene analysis</p><p>4 0.32091373 <a title="11-lda-4" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>Author: Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou</p><p>Abstract: We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the ﬁrst time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction. html. Keywords: transduction, transductive SVMs, semi-supervised learning, CCCP</p><p>5 0.31604344 <a title="11-lda-5" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>Author: Greg Hamerly, Erez Perelman, Jeremy Lau, Brad Calder, Timothy Sherwood</p><p>Abstract: An essential step in designing a new computer architecture is the careful examination of different design options. It is critical that computer architects have efﬁcient means by which they may estimate the impact of various design options on the overall machine. This task is complicated by the fact that different programs, and even different parts of the same program, may have distinct behaviors that interact with the hardware in different ways. Researchers use very detailed simulators to estimate processor performance, which models every cycle of an executing program. Unfortunately, simulating every cycle of a real program can take weeks or months. To address this problem we have created a tool called SimPoint that uses data clustering algorithms from machine learning to automatically ﬁnd repetitive patterns in a program’s execution. By simulating one representative of each repetitive behavior pattern, simulation time can be reduced to minutes instead of weeks for standard benchmark programs, with very little cost in terms of accuracy. We describe this important problem, the data representation and preprocessing methods used by SimPoint, the clustering algorithm at the core of SimPoint, and we evaluate different options for tuning SimPoint. Keywords: k-means, random projection, Bayesian information criterion, simulation, SimPoint</p><p>6 0.3160356 <a title="11-lda-6" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>7 0.31550762 <a title="11-lda-7" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>8 0.31514382 <a title="11-lda-8" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>9 0.31405172 <a title="11-lda-9" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>10 0.31303626 <a title="11-lda-10" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>11 0.31183812 <a title="11-lda-11" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.3115741 <a title="11-lda-12" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.31119913 <a title="11-lda-13" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>14 0.30948773 <a title="11-lda-14" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>15 0.30916265 <a title="11-lda-15" href="./jmlr-2006-Pattern_Recognition_for__Conditionally_Independent_Data.html">73 jmlr-2006-Pattern Recognition for  Conditionally Independent Data</a></p>
<p>16 0.30898714 <a title="11-lda-16" href="./jmlr-2006-On_Model_Selection_Consistency_of_Lasso.html">66 jmlr-2006-On Model Selection Consistency of Lasso</a></p>
<p>17 0.30801275 <a title="11-lda-17" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.30269721 <a title="11-lda-18" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>19 0.30224133 <a title="11-lda-19" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>20 0.30180177 <a title="11-lda-20" href="./jmlr-2006-Nonparametric_Quantile_Estimation.html">65 jmlr-2006-Nonparametric Quantile Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
