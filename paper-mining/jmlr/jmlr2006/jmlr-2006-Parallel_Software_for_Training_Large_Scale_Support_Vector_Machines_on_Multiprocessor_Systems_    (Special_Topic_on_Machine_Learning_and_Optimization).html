<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-72" href="#">jmlr2006-72</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-72-pdf" href="http://jmlr.org/papers/volume7/zanni06a/zanni06a.pdf">pdf</a></p><p>Author: Luca Zanni, Thomas Serafini, Gaetano Zanghirati</p><p>Abstract: Parallel software for solving the quadratic program arising in training support vector machines for classiﬁcation problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-ofthe-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes. Keywords: support vector machines, large scale quadratic programs, decomposition techniques, gradient projection methods, parallel computation</p><p>Reference: <a title="jmlr-2006-72-reference" href="../jmlr2006_reference/jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. [sent-9, score-0.337]
</p><p>2 Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. [sent-10, score-0.466]
</p><p>3 Keywords: support vector machines, large scale quadratic programs, decomposition techniques, gradient projection methods, parallel computation  1. [sent-14, score-0.328]
</p><p>4 The idea behind the decomposition techniques consists in splitting the problem into a sequence of smaller QP subproblems, sized nsp say, that can be stored in the available memory and efﬁciently solved (Boser et al. [sent-33, score-0.669]
</p><p>5 At each decomposition step, a subset of the variables, usually called working set, is optimized through the solution of the subproblem in order to obtain a progress towards the minimum of the objective function F (α). [sent-36, score-0.269]
</p><p>6 In case of working sets of minimal size, that is sized 2, a proper selection via the maximal-violating pair principle (or related criteria) is sufﬁcient to ensure asymptotic convergence of the decomposition scheme (Lin, 2002; Chen et al. [sent-40, score-0.36]
</p><p>7 The LIBSVM software is developed for working sets sized 2, hence it tends to minimize the computational cost per iteration. [sent-47, score-0.25]
</p><p>8 In fact, in this case the inner QP subproblem can be analytically solved without requiring a numerical QP solver and the updating of the objective gradient only involves the two Hessian columns corresponding to the updated variables. [sent-48, score-0.283]
</p><p>9 A generalized maximal-violating pair policy for the working set selection and a numerical solver for the inner QP subproblems are needed; furthermore, we must recall that the more variables are changed per iteration, the more expensive is the objective gradient updating. [sent-56, score-0.291]
</p><p>10 Even if SVMlight can run with any working set size, numerical experiences show that it effectively faces the above difﬁculties only in case of small sized working sets (nsp = O(10)), where it often exhibits comparable performance with LIBSVM. [sent-57, score-0.321]
</p><p>11 Zanghirati and Zanni (2003) obtained an efﬁcient subproblem solution by a gradient projection-type method: it exploits the simple structure of the constraints, exhibits good convergence rate and is well suited for a parallel implementation. [sent-62, score-0.291]
</p><p>12 The promising results given by this parallel scheme can be now further improved thanks to some recent studies on both the gradient projection QP solvers (Seraﬁni et al. [sent-63, score-0.285]
</p><p>13 , 2005; Dai and Fletcher, 2006) and the selection rules for large sized working sets (Seraﬁni and Zanni, 2005). [sent-64, score-0.241]
</p><p>14 On the basis of these studies a new parallel gradient projection-based decomposition technique (PGPDT) is developed and implemented in software available at http://www. [sent-65, score-0.33]
</p><p>15 Second, we show how, by exploiting the resources of common multiprocessor systems, PGPDT achieves good time speedup in comparison 1469  Z ANNI , S ERAFINI AND Z ANGHIRATI  with state-of-the-art serial packages and makes nonlinear SVMs tractable even on millions training samples. [sent-84, score-0.45]
</p><p>16 Furthermore, we denote by nsp the size of the working set (nsp = #B ) and by α∗ a solution of (1). [sent-102, score-0.567]
</p><p>17 Finally, suppose a distributed-memory multiprocessor system equipped with NP processors is available for solving the problem (1) and that each processor has a local copy of the training set. [sent-103, score-0.344]
</p><p>18 It must be observed that algorithm PDT essentially follows the SVMlight decomposition scheme proposed by Joachims (1998), but it allows to distribute among the available processors the subproblem solution in step A2 and the gradient updating in step A3. [sent-107, score-0.456]
</p><p>19 Set α(1) = 0 and let nsp and nc be two integer values such that n ≥ nsp ≥ nc > 0, nc even. [sent-114, score-1.383]
</p><p>20 Choose nsp indices for the working set B and set k = 1. [sent-115, score-0.634]
</p><p>21 to ∑i∈B yi αi = − ∑i∈N yi αi , min  0 ≤ αi ≤ C,  of  αB (2)  ∀i∈B , (k+1) T  where 1B is the nsp -vector of all one; set α(k+1) =  αB  (k) T  , αN  T  . [sent-119, score-0.453]
</p><p>22 , n} with #J ≤ nsp and λmin (GJ J ) denotes the smallest eigenvalue of GJ J . [sent-137, score-0.453]
</p><p>23 For the special case nsp = 2, where the selection rule (4) gives only the two indices corresponding to the maximal violation of the KKT conditions (the maximal-violating pair), Lin (2002) has shown that the assumption (5) is not necessary to ensure the convergence. [sent-141, score-0.554]
</p><p>24 For any working set size, Palagi and Sciandrone (2005) have shown that the condition (6) is ensured by solving at each iteration the following proximal point modiﬁcation of the subproblem (2): T 1 T τ (k) (k) min αB GB B αB + GB N αN − 1B αB + αB − αB 2 2 2 (k) (7) sub. [sent-142, score-0.212]
</p><p>25 Before to describe the PGPDT implementation in detail, it must be recalled that this software is designed to be effective in case of sufﬁciently large nsp , i. [sent-151, score-0.496]
</p><p>26 For this reason, in the sequel the reader may assume nsp to be of medium-tolarge size. [sent-154, score-0.453]
</p><p>27 (9)  We recall that the size nsp is such that A can ﬁt into the available memory. [sent-157, score-0.453]
</p><p>28 Since subproblem (8) appears at each decomposition iteration, an effective inner solver becomes a crucial tool for the performance of a decomposition technique. [sent-158, score-0.316]
</p><p>29 The standard library QP solvers can be successfully applied only in the small size case (nsp = O(10)), since their computational cost easily degrades the performance of a decomposition technique based on medium-to-large nsp . [sent-159, score-0.59]
</p><p>30 Furthermore, the single iteration core consists essentially in an nsp -dimensional matrix-vector product that is suited to be optimized (by exploiting the vector sparsity) and also to be parallelized. [sent-164, score-0.486]
</p><p>31 A general parallel gradient projection scheme sp for (8) is depicted in Algorithm PGPM. [sent-166, score-0.238]
</p><p>32 Concerning the convergence rate, that is the key element for the PGPM performance, the choices of both the steplength ρk and the linesearch parameter λk play a crucial role. [sent-170, score-0.281]
</p><p>33 It outperforms the monotone gradient projection scheme used by Zanghirati and Zanni (2003), 1473  Z ANNI , S ERAFINI AND Z ANGHIRATI  A LGORITHM PGPM Parallel gradient projection method for step A2 of Algorithm PDT. [sent-179, score-0.226]
</p><p>34 Initialize the parameters for the steplength selection rule and for the linesearch strategy. [sent-196, score-0.286]
</p><p>35 Figure 1: linesearch and steplength rule for the GVPM method. [sent-229, score-0.252]
</p><p>36 GVPM steplength selection and linesearch are described in Figure 1. [sent-233, score-0.286]
</p><p>37 If f (w(k+1) ) < fbest then set fbest = f (w(k+1) ) ,  fc = f (w(k+1) ) ,  h=0;  else set fc = max fc , f (w(k+1) ) , h = h + 1 ; If h = L then set fref = fc , fc = f (w(k+1) ) , h = 0 ; end. [sent-254, score-0.295]
</p><p>38 Figure 2: linesearch and steplength rule for the Dai-Fletcher method. [sent-256, score-0.252]
</p><p>39 ∞ For the computation of PΩ (·) in step B2, the default is the following: if nsp ≤ 20 the bisection-like method described by Pardalos and Kovoor (1990) is used, else the secant-based algorithm proposed by Dai and Fletcher (2006) is chosen, that usually is faster for large size. [sent-259, score-0.494]
</p><p>40 In the PGPDT, the tolerance required to the inner solver depends on the quality of the outer iterate α(k) : in the ﬁrst iterations the same tolerance as the decomposition scheme is used, while a progressively lower tolerance is imposed when α(k) nearly satisﬁes the outer stopping criterion. [sent-262, score-0.254]
</p><p>41 2 Parallel Gradient Updating The gradient updating in step A3 is usually the most expensive task of a decomposition iteration. [sent-265, score-0.237]
</p><p>42 Since the matrix G is assumed to be out of memory, in order to obtain ∇F (α(k+1) ) some entries of G need to be computed and, consequently, some kernel evaluations are involved that can be very expensive in case of large sized input space and not much sparse training examples. [sent-266, score-0.244]
</p><p>43 Thus, any strategy able to save kernel evaluations or to optimize their computation is crucial for minimizing the time consumption for updating the gradient. [sent-267, score-0.223]
</p><p>44 Further improvements in the number of kernel evaluations can be obtained by introducing a caching strategy, consisting in using an area of the available memory to store some elements of G to avoid their recomputation in subsequent iterations. [sent-269, score-0.33]
</p><p>45 This simple trick seems to well combine with the working set selection used in step A4, which forces some indices of the current B to remain in the new working set (see the next section for more details), and remarkable reduction of the kernel evaluations are often observed. [sent-271, score-0.444]
</p><p>46 Nevertheless, the improvements implied by a caching strategy are obviously dependent on the size of the caching area. [sent-272, score-0.319]
</p><p>47 One of the innovative features of PGPDT is to implement a parallel gradient updating where both the matrix-vector multiplication and the caching strategy are distributed among the processors. [sent-274, score-0.448]
</p><p>48 This is done by asking each processor to perform a part of the column combinations required in (3) and to make available its local memory for caching the columns of G. [sent-275, score-0.213]
</p><p>49 In this way, the gradient updating beneﬁts not only from a computations distribution, but also from a reduction of the kernel evaluations due to much larger caching areas. [sent-276, score-0.405]
</p><p>50 Concerning the reduction of the kernel evaluations, it is worth to recall that the entries of G stored in the caching area can be used also for GB B in step A2. [sent-279, score-0.229]
</p><p>51 The gradient updating overhead within each decomposition iteration can be further reduced by optimizing the kernel computation. [sent-281, score-0.317]
</p><p>52 Even if a caching strategy can limit the number of kernel evaluations, large problems often require millions of them and their optimization becomes a need. [sent-282, score-0.252]
</p><p>53 , NP , the caching area of the processor p and by Gi the i-th column of G. [sent-287, score-0.219]
</p><p>54 , NP , /  Distribute among the processors the sets B c and B n and denote by B c,p and B n,p the sets of indices assigned to processor p. [sent-292, score-0.258]
</p><p>55 Consequently, PGPDT faces linear SVMs without any caching strategy and performs the gradient updating by simply distributing the n tasks (11) among the processors. [sent-321, score-0.323]
</p><p>56 It consists in two phases: in the ﬁrst phase at most nc indices are chosen for the new working set by solving the problem (4), while in the second phase at least nsp − nc entries are selected from the current B to complete the new working set. [sent-324, score-1.066]
</p><p>57 (k+1) ¯ ii) Fill B up to nsp entries by adding the most recent indices† j ∈ B satisfying 0 < α j < C; (k+1)  =0  nsp 10  and  if these indices are not enough, then add the most recent indices j ∈ B such that α j  and, eventually, the most recent indices j ∈ B satisfying  (k+1) αj  = C. [sent-334, score-1.107]
</p><p>58 iii) Set nc = min{ nc , max{10, J, nnew } }, where J is the largest even integer such that J ≤ ¯ nnew is the largest even integer such that nnew ≤ #{ j, j ∈ B \ B } ; ¯ set B = B , k ← k + 1 and go to step A2. [sent-335, score-0.408]
</p><p>59 In particular, at each iteration the maximal-violating pair is included in the working set: this property is crucial for the asymptotic convergence of a decomposition technique. [sent-339, score-0.266]
</p><p>60 Nevertheless, for fast convergence, both nc and the updating phase in step A4. [sent-341, score-0.234]
</p><p>61 In fact, as it is experimentally shown by Seraﬁni and Zanni (2005), values of nc equal or close to nsp often yield a dangerous zigzagging phenomenon (i. [sent-343, score-0.612]
</p><p>62 , some variables enter and leave the working set many times), which can heavily degrade the convergence rate especially for large nsp . [sent-345, score-0.596]
</p><p>63 This drawback suggests to set nc sufﬁciently smaller than nsp and then it opens the problem of how to select the remaining indices to ﬁll up the new working set. [sent-346, score-0.793]
</p><p>64 Finally, Algorithm SP2 also introduces an adaptive reduction of the parameter nc , useful in case of large sized working sets. [sent-354, score-0.366]
</p><p>65 This trick allows the decomposition technique to start with nc close to nsp , in order to optimize many new variables in the very ﬁrst iterations, and avoids zigzagging through the progressive reduction of 1480  PARALLEL S OFTWARE FOR T RAINING L ARGE S CALE SVM S  nc . [sent-355, score-0.861]
</p><p>66 The reduction takes place only if nc is larger than an empirical threshold and it is controlled via the number of those new indices selected in step A4. [sent-356, score-0.226]
</p><p>67 To this end, it is also worth to show that the serial version of the proposed software (called GPDT) can train SVMs with effectiveness comparable to that of the state-of-the-art softwares LIBSVM (ver. [sent-360, score-0.246]
</p><p>68 Since there are no other parallel software currently available for comparison, the PGPDT will be evaluated in terms of scaling properties with respect to the serial packages. [sent-365, score-0.329]
</p><p>69 PGPDT has been tested also on different parallel architectures and, for completeness, we report the results obtained on a system where less memory than in the IBM SP5 is available for each CPU: the IBM CLX/1024 Linux Cluster, that owns 512 nodes equipped with two Intel Xeon processors at 3. [sent-371, score-0.312]
</p><p>70 The values we use for the working set parameters nsp and nc are also reported: as mentioned, the LIBSVM software works only with nsp = nc = 2, whilst both SVMlight and GPDT accept larger values. [sent-432, score-1.381]
</p><p>71 For these two softwares, meaningful ranges of parameters were explored: we report the results corresponding to the pairs that gave the best training time and to the default setting (nsp = nc = 10 for SVMlight , nsp = 400, nc = ⌊nsp /3⌋ = 132 for GPDT). [sent-433, score-0.848]
</p><p>72 SVMlight is run with several values of nsp in the range [2, 80] with both its inner solvers: the Hildreth-D’Esopo and the pr LOQO. [sent-434, score-0.482]
</p><p>73 The best training time is obtained by using the Hildreth-D’Esopo solver with nsp small and nc = nsp /2, generally observing a signiﬁcant performance decrease for nsp > 40. [sent-435, score-1.596]
</p><p>74 We run the codes assigning to the caching area 512MB for the MNIST test problems and 768MB in the other cases; the default threshold ε = 10−3 for the termination criterion is used, except for the two largest Cover Type and KDDCUP-99 test problems, where the stopping tolerance ε is set to 10−2 . [sent-436, score-0.31]
</p><p>75 Figure 3: scaling of the serial solvers on test problems from the Cover Type data set. [sent-572, score-0.236]
</p><p>76 In particular, we can see an essentially constant number of decomposition iterations (recall that only the computational burden within the decomposition iteration is distributed) and the same solution accuracy as the serial run (compare the numbers in SV, BSV and F opt columns). [sent-607, score-0.402]
</p><p>77 Moreover, remark the lower number of total kernel evaluations needed by the parallel version, due to the growing amount of global caching memory available, which our parallel caching strategy is able to exploit. [sent-608, score-0.717]
</p><p>78 )  Linear speedup 200  Relative speedup  Time (sec. [sent-612, score-0.242]
</p><p>79 )  300  4  16  Linear speedup  6000  8 4000  4  2000  2 0  1  2  4  8 Number of processors  Relative speedup  Time (sec) Rel. [sent-613, score-0.396]
</p><p>80 speedup  2 0  16  1  2  4  8 Number of processors  16  (b) KDDCUP-99 (n = 400000) test problem. [sent-614, score-0.303]
</p><p>81 )  Linear speedup  200  15000  8 10000  Relative speedup  Linear speedup  250  100 4 50 0  4  5000  2 1  2  4  8 Number of processors  2 0  16  (c) MNIST (poly) test problem. [sent-621, score-0.545]
</p><p>82 ﬁts due to the parallel caching strategy are not sufﬁcient to ensure optimal speedups. [sent-624, score-0.301]
</p><p>83 For instance, sometimes the nsp values that give satisfactory serial performance are not suited for good PGPDT scaling. [sent-625, score-0.586]
</p><p>84 This is the case of the KDDCUP-99 test problem (Figure 4b), where the small working sets sized nsp = 180 imply many decomposition iterations and consequently the ﬁxed costs of the non-distributed tasks (working set selection and stopping rule) become very heavy. [sent-626, score-0.812]
</p><p>85 Another example is provided by the MNIST (polynomial) test problem (Figure 4c): here the subproblem solution is a dominant task in comparison to the gradient updating and the suboptimal scaling of the PGPM solver on 16 processors leads to poor speedups. [sent-627, score-0.464]
</p><p>86 In particular, on multiprocessor systems where less memory than in the SP5 platform is available for each CPU, even better speedups can be expected due to the effectiveness of the parallel caching strategy. [sent-630, score-0.385]
</p><p>87 For instance, we report in Figure 5 what we get for the KDDCUP-99 test problem on the IBM CLX/1024 Linux Cluster, where only 400MB of caching area can be allocated on each CPU. [sent-631, score-0.21]
</p><p>88 Due to both the worse performance of this machine and the reduced caching area, larger training time is required, but an optimal PGPDT speedup is now observed up to 16 processors. [sent-632, score-0.3]
</p><p>89 )  24000  Linear speedup  18000  16  12000  Relative speedup  30000  8 6000 4 2 0  12  4  8  16 Number of processors  32  Figure 5: PGPDT scaling on the CLX/1024 system for the KDDCUP-99 (n = 4 · 105 ) test problem. [sent-636, score-0.452]
</p><p>90 In the two larger cases a different setting for the nsp , nc and caching area have been used. [sent-645, score-0.794]
</p><p>91 In particular, for the case n = 2 · 106 we used nsp = 150, nc = 40 and 600Mb of caching area; for the full case we used nsp = 90, nc = 30 and 250Mb of caching area. [sent-646, score-1.51]
</p><p>92 The reason for reducing the caching area is that every processor can allocate no more that 1. [sent-647, score-0.249]
</p><p>93 For the test problem sized n = 2 · 106 , the serial results concern only the GPDT because LIBSVM exceeded the time limit of 60 hours and SVMlight stopped without a valid solution after relaxing the KKT conditions. [sent-657, score-0.254]
</p><p>94 Due to the very large size of the problem, the amount of 600MB for the caching area seems not sufﬁcient to prevent a huge number of kernel evaluations in the serial run. [sent-658, score-0.43]
</p><p>95 In this case the overall remark is that, on the considered architecture, few processors allow to train the Gaussian SVM in less than one day while few tens of processors can be exploited to reduce the training time to about 10 hours. [sent-662, score-0.344]
</p><p>96 These results show that PGPDT is able to exploit the resources of today multiprocessor systems to overcome the limits of the serial SVM implementations in solving O(106 ) problems (see also the training time in Figure 6a). [sent-664, score-0.253]
</p><p>97 It implements an iterative decomposition technique based on a gradient projection solver for the inner subproblems. [sent-670, score-0.274]
</p><p>98 Furthermore, a parallel caching strategy allows to effectively exploit as much memory as available to avoid expensive kernel recomputations. [sent-674, score-0.381]
</p><p>99 Numerical comparisons with the state-of-the-art softwares LIBSVM and SVMlight on benchmark problems show the signiﬁcant speedup that the proposed parallel package can achieve in training large scale SVMs. [sent-675, score-0.352]
</p><p>100 On the working set selection in gradient projection-based decomposition techniques for support vector machines. [sent-832, score-0.31]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nsp', 0.453), ('pgpdt', 0.433), ('nc', 0.159), ('processors', 0.154), ('gpdt', 0.151), ('caching', 0.143), ('linesearch', 0.141), ('zanni', 0.137), ('serial', 0.133), ('anghirati', 0.131), ('anni', 0.131), ('erafini', 0.131), ('svmlight', 0.126), ('parallel', 0.125), ('speedup', 0.121), ('oftware', 0.121), ('working', 0.114), ('sera', 0.111), ('steplength', 0.111), ('arge', 0.107), ('sized', 0.093), ('raining', 0.092), ('cale', 0.092), ('decomposition', 0.09), ('mnist', 0.089), ('np', 0.086), ('gb', 0.084), ('multiprocessor', 0.084), ('libsvm', 0.081), ('pdt', 0.081), ('zanghirati', 0.081), ('updating', 0.075), ('svm', 0.072), ('gradient', 0.072), ('pgpm', 0.07), ('softwares', 0.07), ('luca', 0.069), ('evaluations', 0.068), ('indices', 0.067), ('subproblem', 0.065), ('gvpm', 0.06), ('ni', 0.06), ('qp', 0.057), ('opt', 0.056), ('bb', 0.05), ('bot', 0.05), ('ibm', 0.049), ('dai', 0.049), ('svms', 0.048), ('solvers', 0.047), ('packages', 0.047), ('kernel', 0.047), ('parallelization', 0.043), ('bsv', 0.043), ('wp', 0.043), ('software', 0.043), ('solver', 0.042), ('collobert', 0.042), ('default', 0.041), ('projection', 0.041), ('cover', 0.04), ('fref', 0.04), ('gaetano', 0.04), ('palagi', 0.04), ('rnsp', 0.04), ('ruggiero', 0.04), ('area', 0.039), ('aw', 0.039), ('fc', 0.039), ('poly', 0.039), ('sv', 0.038), ('fletcher', 0.038), ('processor', 0.037), ('training', 0.036), ('cascade', 0.035), ('lgorithm', 0.035), ('ad', 0.035), ('kkt', 0.034), ('gauss', 0.034), ('nonmonotone', 0.034), ('selection', 0.034), ('lin', 0.034), ('iteration', 0.033), ('memory', 0.033), ('strategy', 0.033), ('copy', 0.033), ('tolerance', 0.031), ('allocate', 0.03), ('fbest', 0.03), ('heaviest', 0.03), ('mkernel', 0.03), ('nnew', 0.03), ('sciandrone', 0.03), ('inner', 0.029), ('selections', 0.029), ('millions', 0.029), ('joachims', 0.029), ('convergence', 0.029), ('scaling', 0.028), ('test', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="72-tfidf-1" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Luca Zanni, Thomas Serafini, Gaetano Zanghirati</p><p>Abstract: Parallel software for solving the quadratic program arising in training support vector machines for classiﬁcation problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-ofthe-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes. Keywords: support vector machines, large scale quadratic programs, decomposition techniques, gradient projection methods, parallel computation</p><p>2 0.13387342 <a title="72-tfidf-2" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tobias Glasmachers, Christian Igel</p><p>Abstract: Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is signiﬁcantly faster. Keywords: working set selection, sequential minimal optimization, quadratic programming, support vector machines, large scale optimization</p><p>3 0.095349796 <a title="72-tfidf-3" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>4 0.090000391 <a title="72-tfidf-4" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer, Bernhard Schölkopf</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun. Keywords: multiple kernel learning, string kernels, large scale optimization, support vector machines, support vector regression, column generation, semi-inﬁnite linear programming</p><p>5 0.085909776 <a title="72-tfidf-5" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>Author: Don Hush, Patrick Kelly, Clint Scovel, Ingo Steinwart</p><p>Abstract: We describe polynomial–time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classiﬁers. These algorithms employ a two–stage process where the ﬁrst stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an √ √ approximate dual solution with accuracy (2 2Kn + 8 λ)−2 λε2 to an approximate primal solution p with accuracy ε p where n is the number of data samples, Kn is the maximum kernel value over the data and λ > 0 is the SVM regularization parameter. For the ﬁrst stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for τ–rate certifying decomposition algorithms we establish the optimality of τ = 1/(n − 1). In addition we extend the recent τ = 1/(n − 1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the τ = 1/(n − 1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the τ–rate certifying property of these algorithms to produce new stopping rules that are computationally efﬁcient and that guarantee a speciﬁed accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classiﬁcation problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n2 ) bound on the overall run time for the ﬁrst stage. Combining the ﬁrst and second stages gives an overall run time of O(n2 (c</p><p>6 0.071867242 <a title="72-tfidf-6" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.066561028 <a title="72-tfidf-7" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.061549719 <a title="72-tfidf-8" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>9 0.060544558 <a title="72-tfidf-9" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.043128625 <a title="72-tfidf-10" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.039495002 <a title="72-tfidf-11" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>12 0.036761321 <a title="72-tfidf-12" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.036694255 <a title="72-tfidf-13" href="./jmlr-2006-A_Simulation-Based_Algorithm_for_Ergodic_Control_of_Markov_Chains_Conditioned_on_Rare_Events.html">7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</a></p>
<p>14 0.03571824 <a title="72-tfidf-14" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>15 0.033658091 <a title="72-tfidf-15" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.033299658 <a title="72-tfidf-16" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>17 0.032641217 <a title="72-tfidf-17" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>18 0.03118651 <a title="72-tfidf-18" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>19 0.030227382 <a title="72-tfidf-19" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.029667018 <a title="72-tfidf-20" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, -0.115), (2, 0.095), (3, 0.121), (4, 0.162), (5, 0.073), (6, -0.085), (7, 0.034), (8, 0.022), (9, -0.08), (10, 0.116), (11, -0.04), (12, 0.259), (13, -0.109), (14, 0.064), (15, 0.139), (16, 0.014), (17, 0.118), (18, 0.04), (19, 0.046), (20, -0.034), (21, 0.267), (22, -0.019), (23, 0.058), (24, 0.028), (25, 0.038), (26, -0.045), (27, 0.11), (28, -0.089), (29, -0.08), (30, -0.012), (31, 0.003), (32, -0.114), (33, -0.074), (34, 0.038), (35, 0.122), (36, -0.018), (37, -0.006), (38, -0.014), (39, -0.031), (40, -0.035), (41, -0.071), (42, 0.171), (43, -0.033), (44, -0.04), (45, -0.045), (46, -0.175), (47, -0.086), (48, -0.019), (49, -0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93040448 <a title="72-lsi-1" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Luca Zanni, Thomas Serafini, Gaetano Zanghirati</p><p>Abstract: Parallel software for solving the quadratic program arising in training support vector machines for classiﬁcation problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-ofthe-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes. Keywords: support vector machines, large scale quadratic programs, decomposition techniques, gradient projection methods, parallel computation</p><p>2 0.75276661 <a title="72-lsi-2" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tobias Glasmachers, Christian Igel</p><p>Abstract: Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is signiﬁcantly faster. Keywords: working set selection, sequential minimal optimization, quadratic programming, support vector machines, large scale optimization</p><p>3 0.52569908 <a title="72-lsi-3" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>Author: Don Hush, Patrick Kelly, Clint Scovel, Ingo Steinwart</p><p>Abstract: We describe polynomial–time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classiﬁers. These algorithms employ a two–stage process where the ﬁrst stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an √ √ approximate dual solution with accuracy (2 2Kn + 8 λ)−2 λε2 to an approximate primal solution p with accuracy ε p where n is the number of data samples, Kn is the maximum kernel value over the data and λ > 0 is the SVM regularization parameter. For the ﬁrst stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for τ–rate certifying decomposition algorithms we establish the optimality of τ = 1/(n − 1). In addition we extend the recent τ = 1/(n − 1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the τ = 1/(n − 1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the τ–rate certifying property of these algorithms to produce new stopping rules that are computationally efﬁcient and that guarantee a speciﬁed accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classiﬁcation problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n2 ) bound on the overall run time for the ﬁrst stage. Combining the ﬁrst and second stages gives an overall run time of O(n2 (c</p><p>4 0.50241685 <a title="72-lsi-4" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer, Bernhard Schölkopf</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun. Keywords: multiple kernel learning, string kernels, large scale optimization, support vector machines, support vector regression, column generation, semi-inﬁnite linear programming</p><p>5 0.3879098 <a title="72-lsi-5" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>6 0.35423064 <a title="72-lsi-6" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.31804162 <a title="72-lsi-7" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.30167148 <a title="72-lsi-8" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.29107866 <a title="72-lsi-9" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>10 0.25828955 <a title="72-lsi-10" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>11 0.22149937 <a title="72-lsi-11" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.2078018 <a title="72-lsi-12" href="./jmlr-2006-A_Simulation-Based_Algorithm_for_Ergodic_Control_of_Markov_Chains_Conditioned_on_Rare_Events.html">7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</a></p>
<p>13 0.1956434 <a title="72-lsi-13" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>14 0.19533372 <a title="72-lsi-14" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>15 0.19189675 <a title="72-lsi-15" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>16 0.18908381 <a title="72-lsi-16" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.18843268 <a title="72-lsi-17" href="./jmlr-2006-Stability_Properties_of_Empirical_Risk_Minimization_over_Donsker_Classes.html">84 jmlr-2006-Stability Properties of Empirical Risk Minimization over Donsker Classes</a></p>
<p>18 0.18211353 <a title="72-lsi-18" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.17422536 <a title="72-lsi-19" href="./jmlr-2006-On_the_Complexity_of_Learning_Lexicographic_Strategies.html">68 jmlr-2006-On the Complexity of Learning Lexicographic Strategies</a></p>
<p>20 0.1680636 <a title="72-lsi-20" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.021), (35, 0.017), (36, 0.051), (45, 0.019), (50, 0.033), (61, 0.016), (63, 0.051), (76, 0.051), (78, 0.03), (81, 0.03), (84, 0.015), (90, 0.032), (91, 0.056), (96, 0.091), (99, 0.407)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72119433 <a title="72-lda-1" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Luca Zanni, Thomas Serafini, Gaetano Zanghirati</p><p>Abstract: Parallel software for solving the quadratic program arising in training support vector machines for classiﬁcation problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-ofthe-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes. Keywords: support vector machines, large scale quadratic programs, decomposition techniques, gradient projection methods, parallel computation</p><p>2 0.32400933 <a title="72-lda-2" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>3 0.31931284 <a title="72-lda-3" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Sören Sonnenburg, Gunnar Rätsch, Christin Schäfer, Bernhard Schölkopf</p><p>Abstract: While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classiﬁcation, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-inﬁnite linear program that can be efﬁciently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiﬁcation. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun. Keywords: multiple kernel learning, string kernels, large scale optimization, support vector machines, support vector regression, column generation, semi-inﬁnite linear programming</p><p>4 0.31842816 <a title="72-lda-4" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tobias Glasmachers, Christian Igel</p><p>Abstract: Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is signiﬁcantly faster. Keywords: working set selection, sequential minimal optimization, quadratic programming, support vector machines, large scale optimization</p><p>5 0.31586128 <a title="72-lda-5" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Matthias Heiler, Christoph Schnörr</p><p>Abstract: We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. We show that for ordinary NMF, our approach performs as well as existing stateof-the-art algorithms, while for sparsity-constrained NMF, as recently proposed by P. O. Hoyer in JMLR 5 (2004), it outperforms previous methods. In addition, we show how to extend NMF learning within the same optimization framework in order to make use of class membership information in supervised learning problems. Keywords: non-negative matrix factorization, second-order cone programming, sequential convex optimization, reverse-convex programming, sparsity</p><p>6 0.3136754 <a title="72-lda-6" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>7 0.31302106 <a title="72-lda-7" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>8 0.306205 <a title="72-lda-8" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>9 0.30596763 <a title="72-lda-9" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>10 0.30562979 <a title="72-lda-10" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.30403969 <a title="72-lda-11" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>12 0.30310869 <a title="72-lda-12" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>13 0.30255777 <a title="72-lda-13" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.30075592 <a title="72-lda-14" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>15 0.3006309 <a title="72-lda-15" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>16 0.30029798 <a title="72-lda-16" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.29709977 <a title="72-lda-17" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>18 0.29614305 <a title="72-lda-18" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>19 0.28994489 <a title="72-lda-19" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>20 0.2895655 <a title="72-lda-20" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
