<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-55" href="#">jmlr2006-55</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-55-pdf" href="http://jmlr.org/papers/volume7/yanover06a/yanover06a.pdf">pdf</a></p><p>Author: Chen Yanover, Talya Meltzer, Yair Weiss</p><p>Abstract: The problem of ﬁnding the most probable (MAP) conﬁguration in graphical models comes up in a wide range of applications. In a general graphical model this problem is NP hard, but various approximate algorithms have been developed. Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for ﬁnding the most probable assignment in small graphical models. However, applying this powerful method to real-world problems is extremely challenging due to the large numbers of variables and constraints in the linear program. Tree-Reweighted Belief Propagation is a promising recent algorithm for solving LP relaxations, but little is known about its running time on large problems. In this paper we compare tree-reweighted belief propagation (TRBP) and powerful generalpurpose LP solvers (CPLEX) on relaxations of real-world graphical models from the ﬁelds of computer vision and computational biology. We ﬁnd that TRBP almost always ﬁnds the solution signiﬁcantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. Using TRBP we can ﬁnd the MAP conﬁgurations in a matter of minutes for a large range of real world problems.</p><p>Reference: <a title="jmlr-2006-55-reference" href="../jmlr2006_reference/jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for ﬁnding the most probable assignment in small graphical models. [sent-13, score-0.214]
</p><p>2 In this paper we compare tree-reweighted belief propagation (TRBP) and powerful generalpurpose LP solvers (CPLEX) on relaxations of real-world graphical models from the ﬁelds of computer vision and computational biology. [sent-16, score-0.477]
</p><p>3 We ﬁnd that TRBP almost always ﬁnds the solution signiﬁcantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. [sent-17, score-0.306]
</p><p>4 However, linear programs that arise out of LP relaxations for graphical models have a common structure and are a small subset of all possible linear programs. [sent-36, score-0.18]
</p><p>5 Tree-reweighted belief propagation (TRBP) is a variant of belief propagation (BP) suggested by Wainwright and colleagues (Wainwright et al. [sent-38, score-0.274]
</p><p>6 In this paper we compare tree-reweighted BP and powerful commercial LP solvers (CPLEX) on relaxations of real-world graphical models from the ﬁelds of computer vision and computational biology. [sent-42, score-0.34]
</p><p>7 We ﬁnd that TRBP almost always ﬁnds the solution signiﬁcantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. [sent-43, score-0.306]
</p><p>8 We introduce indicator variables qi (xi ) for each individual variable and additional indicator variables qi j (xi , x j ) for all connected pairs of nodes in the graph. [sent-52, score-0.29]
</p><p>9 This leads to the following problem: The LP relaxation of MAP: minimize J({q}) =  ∑ ∑ qi j (xi , x j )Ei j (xi , x j ) + ∑ ∑ qi (xi )Ei (xi )    xi ,x j  xi  i  subject to qi j (xi , x j ) ∈ [0, 1],  (1)  ∑ qi j (xi , x j )  = 1,  (2)  ∑ qi j (xi , x j )  = q j (x j ). [sent-56, score-0.687]
</p><p>10 It can therefore be solved in polynomial time and we have the following guarantee: Lemma If the solutions {qi j (xi , x j ), qi (xi )} to the MAP LP relaxation are all integer, that is qi j (xi , x j ), qi (xi ) ∈ {0, 1}, then xi∗ = arg maxxi qi (xi ) is the MAP assignment. [sent-58, score-0.504]
</p><p>11 1 The Need for Special Purpose LP Solvers Given the tremendous amount of research devoted to LP solvers, it may seem that the best way to solve LP relaxations for graphical models, would be to simply use an industrial-strength, generalpurpose LP solver. [sent-60, score-0.157]
</p><p>12 Solving Linear Programs Using Tree-Reweighted Belief Propagation Tree-reweighted belief propagation (TRBP) is a variant of belief propagation introduced by Wainwright and colleagues (Wainwright et al. [sent-70, score-0.274]
</p><p>13 We start by brieﬂy reviewing ordinary max-product belief propagation [see e. [sent-72, score-0.154]
</p><p>14 After the messages have converged, each node can form an estimate of its local “belief” deﬁned as bi (xi ) ∝ Ψi (xi ) ∏ m ji (xi ). [sent-80, score-0.157]
</p><p>15 j∈Ni  It is easy to show that when the graph is singly-connected, choosing an assignment that maximizes the local belief will give the MAP estimate (Pearl, 1988). [sent-81, score-0.164]
</p><p>16 As in standard belief propagation, at each iteration a node i sends a message mi j (x j ) to its neighbor in the graph j. [sent-87, score-0.196]
</p><p>17 Given a set of TRBP beliefs, we deﬁne the sharpened beliefs as follows: qi (xi ) ∝ δ(bi (xi ) − max bi (xi )), xi  qi j (xi , x j ) ∝ δ(bi j (xi , x j ) − max bi j (xi , x j )), xi ,x j  where δ(·) is the Dirac delta function (δ(0) = 1 and δ(x) = 0 for all x = 0). [sent-94, score-0.692]
</p><p>18 Using these sharpened beliefs, the following properties hold: • At any iteration, and in particular in ﬁxed-point, the TRBP beliefs provide a lower bound on the solution of the LP (see appendix A). [sent-104, score-0.224]
</p><p>19 • If there exists a unique maximizing value for the pairwise beliefs bi j (xi , x j ) then the sharpened beliefs solve the LP. [sent-105, score-0.525]
</p><p>20 i i • If the sharpened beliefs at a ﬁxed-point of TRBP satisfy the LP constraints (equations 1-3), then the sharpened beliefs are a solution to the LP relaxation. [sent-110, score-0.47]
</p><p>21 If there exists bi , bi j that satisfy the LP constraints and ˜ i (xi ) = 0 if qi (xi ) = 0 and bi j (xi , x j ) = 0 if qi j (xi , x j ) = 0, then bi , bi j are a ˜ ˜ ˜ for all xi , x j , b solution to the LP relaxation. [sent-112, score-0.65]
</p><p>22 The pairwise potentials are 1 if the pair maximizes the ∗ pairwise belief and ε otherwise. [sent-122, score-0.251]
</p><p>23 Additionally, storing the messages in memory requires ∑  ki + k j (which is typically small relative to the memory required for the pairwise potentials). [sent-135, score-0.172]
</p><p>24 The Benchmark Problems We constructed benchmark problems from three domains: stereo vision, side-chain prediction and protein design. [sent-140, score-0.362]
</p><p>25 Given a stereo pair of images, Left(u, v) and Right(u, v), the problem is to ﬁnd the disparity of each pixel in a reference image. [sent-145, score-0.201]
</p><p>26 The best algorithms currently known for the stereo problem are those that minimize a global energy function (Scharstein and Szeliski, 2002): disp∗ = arg min ∑ dissim [Left(u, v), Right(u + disp(u, v), v)] + λ · smoothness(disp), disp u,v  1. [sent-151, score-0.311]
</p><p>27 The best results for this problem use energy minimization formulations which are equivalent to solving the MAP for a grid graphical model. [sent-155, score-0.179]
</p><p>28 We associate each disparity disp(u, v) with an assignment of a node xi in a two dimensional grid graph. [sent-157, score-0.184]
</p><p>29 If we deﬁne x to be the disparity ﬁeld, and P(x|y) ∝ exp(−E(x)) where E(x) is the energy function, minimizing the energy is equivalent to maximizing P(x). [sent-158, score-0.245]
</p><p>30 , 1893  YANOVER , M ELTZER AND W EISS  (a)  (b)  (c)  Figure 2: (a) Cow actin binding protein (PDB code 1pne). [sent-162, score-0.171]
</p><p>31 Given the protein backbone (black) and the amino acid sequence, side-chain prediction is the problem of predicting the native side-chain conformation (gray). [sent-164, score-0.316]
</p><p>32 The local cost is based on the Birchﬁeld-Tomasi matching cost (Birchﬁeld and Tomasi, 1998) and the pairwise energy penalizes for neighboring pixels having different disparities. [sent-169, score-0.185]
</p><p>33 Speciﬁcally, the pairwise energy penalty is deﬁned using 3 parameters – s, P and T – and set to P · s when the intensity difference between the two pixels is smaller than a threshold T , and s otherwise. [sent-172, score-0.185]
</p><p>34 We used four images from the standard Middlebury stereo benchmark set (Scharstein and Szeliski, 2003). [sent-173, score-0.166]
</p><p>35 By varying the parameters of the energy function, as in (Tappen and Freeman, 2003), we obtained 22 different graphical models. [sent-174, score-0.16]
</p><p>36 Amino acids are joined end to end during protein synthesis by the formation of peptide bonds. [sent-181, score-0.19]
</p><p>37 An amino acid unit in a protein is called a residue. [sent-182, score-0.241]
</p><p>38 Even when the energy function contains only pairwise interactions, the conﬁguration space grows exponentially and it can be shown that the prediction problem is NP-complete (Fraenkel, 1997; Pierce and Winfree, 2002). [sent-187, score-0.182]
</p><p>39 Since we have a discrete optimization problem and the energy function is a sum of pairwise interactions, we can transform the problem into a graphical model with pairwise potentials. [sent-192, score-0.28]
</p><p>40 Equation (6) requires multiplying Ψi j for all pairs of residues i, j but in all reasonable energy functions the pairwise interactions go to zero for atoms that are sufﬁciently far away. [sent-195, score-0.216]
</p><p>41 To deﬁne the topology of the undirected graph, we examine all pairs of residues i, j and check whether there exists an assignment ri , r j for which the energy is nonzero. [sent-197, score-0.189]
</p><p>42 A typical protein in the data set gives rise to a model with hundreds of loops of size 3. [sent-201, score-0.171]
</p><p>43 Each protein consisted of a single chain and up to 1,000 residues. [sent-203, score-0.171]
</p><p>44 1895  YANOVER , M ELTZER AND W EISS  The Rosetta energy function accounts for distant interactions and therefore gives rise to denser graphical models, compared to SCWRL’s. [sent-215, score-0.188]
</p><p>45 3 Protein Design The protein design problem is the inverse of the protein folding problem. [sent-225, score-0.397]
</p><p>46 Typically this is done by ﬁnding a set of (1) amino-acids and (2) rotamer conﬁgurations that minimizes an approximate energy [see (Street and Mayo, 1999) for a review of computational protein design]. [sent-227, score-0.309]
</p><p>47 We, again, used the Rosetta energy function to deﬁne the pairwise and local potentials (Kuhlman and Baker, 2000). [sent-231, score-0.195]
</p><p>48 The protein design data set is available from http://www. [sent-235, score-0.195]
</p><p>49 Experiments We compared TRBP to the LP solvers available from CPLEX (CPLEX v9. [sent-240, score-0.153]
</p><p>50 Note that loading a protein ﬁle in Matlab requires using the sparse cell package available from http://www. [sent-254, score-0.171]
</p><p>51 The largest image that could be solved using the CPLEX solvers is approximately 50 × 50 while TRBP can be run on full size images. [sent-260, score-0.173]
</p><p>52 The barrier algorithm is a primal-dual logarithmic barrier algorithm which generates a sequence of strictly positive primal and dual solutions. [sent-262, score-0.284]
</p><p>53 Convergence is declared when the beliefs change by no more than 10−8 between successive iterations and the same threshold is used to determine ties. [sent-277, score-0.169]
</p><p>54 The junction tree algorithm needed for the post-processing of the TRBP beliefs is performed in Matlab using Kevin Murphy’s BNT package (Murphy, 2001). [sent-279, score-0.194]
</p><p>55 For the side-chain prediction problem both TRBP and the CPLEX solvers could solve the LP relaxation for all proteins in the database. [sent-289, score-0.297]
</p><p>56 ]  500  TRBP  30 20 10  100 0  5  10  BP  15  non zeros  TRBP  20  0  25  5  BARRIER  10  15  non zeros TRBP  BP  20  30 20 10 0  25  5  SIFTING  BP  non zeros  TRBP  20  25  CONCURRENT  4 40  2 1 0  time [min. [sent-294, score-0.426]
</p><p>57 The barrier method is the fastest of the CPLEX solvers but it is still signiﬁcantly slower than TRBP for relatively large problems. [sent-298, score-0.285]
</p><p>58 Figure 3 shows the results for a standard stereo benchmark image (the “map” image from the Middlebury stereo benchmark set (Scharstein and Szeliski, 2003)). [sent-300, score-0.372]
</p><p>59 Out of the CPLEX solvers, the dual simplex algorithm could solve the largest subproblem (the barrier algorithm requires more memory) but it could not solve an image larger than approximately 50 × 50 pixels. [sent-302, score-0.172]
</p><p>60 Figure 4 shows the problem sizes for the side-chain prediction and the protein design problems. [sent-304, score-0.22]
</p><p>61 For the side-chain prediction problem all solvers could be applied to the full benchmark set. [sent-305, score-0.212]
</p><p>62 However for the protein design problem (in which the state space is much larger) the CPLEX solvers could solve only 2 out of the 96 problems in the database (this is indicated by the horizontal line in the plots in the right column) while TRBP could solve them all. [sent-306, score-0.348]
</p><p>63 In the second experiment we asked: how do the run-times of the solvers compare in settings where all solvers can be applied. [sent-307, score-0.306]
</p><p>64 Figure 5 compares the run-times on the sequence of subproblems constructed from the Middlebury stereo benchmark set. [sent-308, score-0.166]
</p><p>65 As can be seen, the barrier method is the fastest of the CPLEX solvers but it is still signiﬁcantly slower than TRBP on large problems. [sent-309, score-0.285]
</p><p>66 Figure 6 compares the run times of the different solvers on the side-chain prediction graphical models. [sent-310, score-0.241]
</p><p>67 Again, the barrier method is the fastest of the CPLEX solvers (with dual simplex and network solvers providing similar performance with less memory requirements) but is signiﬁcantly slower than TRBP for large problems. [sent-311, score-0.479]
</p><p>68 Figure 7 shows the run times of TRBP, BP, and the barrier CPLEX solver on the protein design problem. [sent-312, score-0.349]
</p><p>69 ]  400  2 1  100 0  5  10  non zeros  BP  TRBP  0  15 5 x 10  10  non zeros  BP  TRBP  0. [sent-315, score-0.284]
</p><p>70 ]  TRBP  BP  PRIMAL  5  10  non zeros  2 1 0  15 5 x 10  5  10  15 5 x 10  non zeros  Rosetta TRBP  BP  BP  PRIMAL  TRBP  BP  DUAL  TRBP  NET  15  time [min. [sent-326, score-0.284]
</p><p>71 ]  10  1000  8 6 4  10 5  2 0  5  10  non zeros  TRBP  BP  0  15 5 x 10  BARRIER  TRBP  BP  0  15 5 x 10  BP  SIFTING  2  5  10  non zeros  15 5 x 10  1500 1000 500 0  10  TRBP  15 5 x 10  CONCURRENT  10  time [min. [sent-329, score-0.284]
</p><p>72 ]  10  non zeros  6  0  5  8 6 4 2  5  10  non zeros  15 5 x 10  0  5  10  non zeros  15 5 x 10  Figure 6: A comparison of the run-times of the different solvers in CPLEX and TRBP on the sidechain prediction benchmark. [sent-332, score-0.645]
</p><p>73 Again the barrier method is the fastest of the CPLEX solvers (with dual simplex and the network solver providing similar performance with less memory requirements). [sent-333, score-0.348]
</p><p>74 ]  1200 1000 800 600 400 200 2  4  6  non zeros  8 7 x 10  Figure 7: A comparison of the run-times of the barrier method and TRBP on the protein design problem. [sent-336, score-0.469]
</p><p>75 We deﬁne a run of TRBP as “successful” if the TRBP beliefs allowed us to ﬁnd the MAP of the graphical model (i. [sent-339, score-0.232]
</p><p>76 In the stereo benchmark we could directly ﬁnd the MAP in 12 out of 22 cases, but by using additional algorithms on the TRBP output we could ﬁnd the MAP on 19 out of the 22 cases (Meltzer et al. [sent-342, score-0.166]
</p><p>77 The ﬁgures also show the fraction of times in which the TRBP beliefs allowed us to solve the linear program. [sent-347, score-0.169]
</p><p>78 The protein design problem is, apparently, a more difﬁcult problem and the success rate is therefore much lower – the MAP assignment could be found for 2 proteins only and TRBP beliefs allowed us to solve the LP relaxations for 6 proteins only. [sent-348, score-0.667]
</p><p>79 Note, however, that we could still use the TRBP beliefs to obtain a lower bound on the optimal solution. [sent-349, score-0.169]
</p><p>80 We found that in all cases in which the LP solution was nonfractional the TRBP beliefs had a unique maximum. [sent-351, score-0.169]
</p><p>81 Thus the success rate of the standard LP solvers was strictly less than that of TRBP (since TRBP also allows for obtaining a solution with partially tied beliefs). [sent-352, score-0.177]
</p><p>82 A run of the algorithm was considered a success if we could use the TRBP beliefs to ﬁnd the MAP of the graphical model. [sent-362, score-0.256]
</p><p>83 The ﬁgures also show the fraction of times in which the TRBP beliefs allowed us to solve the linear program. [sent-363, score-0.169]
</p><p>84 (a)  Constraints  qi  qij  (b)  Constraints  qi  qij  Figure 9: The sparsity pattern of a typical equality matrix A (a) and a random permutation of this matrix (b). [sent-364, score-0.212]
</p><p>85 The special structure arises from the fact that we only have a consistency constraint for a pairwise indicator qi j to the two singleton indicators, that involve nodes i and j. [sent-370, score-0.257]
</p><p>86 There is no interaction between the pairwise indicators qi j and any other pairwise indicator qkl nor is there an interaction with any other singleton 1902  LP R ELAXATIONS AND BP–A N E MPIRICAL S TUDY  BP  TRBP  BP  BARRIER  BARRIER  20  time [sec. [sent-371, score-0.293]
</p><p>87 In contrast, the CPLEX solvers explicitly represent A and this matrix implicitly represents the graph G (by ﬁnding the correct permutation of A that reveals the block structure, it is possible to reconstruct the graph G). [sent-379, score-0.221]
</p><p>88 As Figure 10(a) shows, for binary nodes the barrier solver was consistently faster than TRBP. [sent-383, score-0.178]
</p><p>89 However, as we increased k, and consequently – the size of the blocks in A, the barrier solver became much slower than TRBP (Figure 10(b)). [sent-384, score-0.154]
</p><p>90 In this paper we have experimented with the powerful solvers in CPLEX on LP relaxations of the MAP problem for graphical models from the ﬁelds of computer vision and computational biology. [sent-391, score-0.34]
</p><p>91 Despite 1903  YANOVER , M ELTZER AND W EISS  the many optimizations in CPLEX for exploiting sparsity, we found that many of the graphical models gave rise to linear programs that were beyond the capability of all the solvers in CPLEX. [sent-392, score-0.239]
</p><p>92 By running the junction tree algorithm on a reduced graphical model deﬁned by the nodes for which the TRBP beliefs had ties, we could ﬁnd the MAP solution for a large range of real-world problems. [sent-394, score-0.281]
</p><p>93 The LP solvers available in CPLEX are of course only a subset of the large number of LP algorithms suggested in the literature and it may very well be possible to design LP solvers that outperform TRBP on our benchmark set. [sent-395, score-0.364]
</p><p>94 Deriving Bounds for the LP Solution Using TRBP In this section, we give the formula for calculating a bound on the LP solution from TRBP ﬁxedpoint beliefs bi j , bi . [sent-406, score-0.313]
</p><p>95 We assume that the beliefs have been normalized so that maxxi ,x j bi j (xi , x j ) = 1 and maxxi bi (xi ) = 1. [sent-407, score-0.383]
</p><p>96 Note that this normalization does not change the nature of ﬁxed-points so in case we have any set of ﬁxed-point beliefs, we can just divide every pairwise belief by the maximal value in that belief and similarly divide every singleton belief by its maximal value. [sent-408, score-0.379]
</p><p>97 For any assignment x, the probability (or equivalently the energy) can be calculated from the original potentials or from the beliefs: Z Pr(x) =  ∏ Ψi j (xi , x j )Ψi (xi ) ij  ρ  = K(b) ∏ bi ji j (xi , x j ) ∏ bci (xi ) i ij  i  with ci = 1 − ∑ j ρi j and K(b) is a constant independent of x. [sent-412, score-0.171]
</p><p>98 Direct inspection shows that if qi j, qi are the sharpened beliefs then they achieve the bound (since they are nonzero only when bi j (xi , x j ) = 1 or bi (xi ) = 1). [sent-417, score-0.58]
</p><p>99 Native protein sequences are close to optimal for their structures. [sent-516, score-0.171]
</p><p>100 Globally optimal solutions for energy minimization in stereo vision using reweighted belief propagation. [sent-534, score-0.352]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('trbp', 0.686), ('lp', 0.347), ('bp', 0.188), ('protein', 0.171), ('beliefs', 0.169), ('cplex', 0.16), ('solvers', 0.153), ('barrier', 0.132), ('stereo', 0.132), ('qi', 0.106), ('yanover', 0.104), ('energy', 0.097), ('relaxations', 0.094), ('belief', 0.093), ('disp', 0.082), ('eiss', 0.082), ('elaxations', 0.082), ('eltzer', 0.082), ('tudy', 0.082), ('non', 0.076), ('proteins', 0.074), ('rosetta', 0.073), ('bi', 0.072), ('zeros', 0.066), ('ei', 0.063), ('graphical', 0.063), ('mpirical', 0.062), ('map', 0.062), ('pairwise', 0.06), ('aa', 0.058), ('xi', 0.056), ('sharpened', 0.055), ('amino', 0.052), ('disparity', 0.051), ('backbone', 0.05), ('scwrl', 0.049), ('relaxation', 0.045), ('propagation', 0.044), ('kolmogorov', 0.043), ('potts', 0.041), ('rotamer', 0.041), ('scharstein', 0.041), ('sidechain', 0.041), ('tappen', 0.041), ('messages', 0.04), ('wainwright', 0.04), ('singleton', 0.04), ('potentials', 0.038), ('assignment', 0.037), ('sifting', 0.035), ('maxxi', 0.035), ('meltzer', 0.035), ('benchmark', 0.034), ('graph', 0.034), ('disparities', 0.033), ('middlebury', 0.033), ('rotamers', 0.033), ('folding', 0.031), ('residues', 0.031), ('ki', 0.03), ('vision', 0.03), ('pixels', 0.028), ('interactions', 0.028), ('message', 0.027), ('indicator', 0.027), ('prediction', 0.025), ('junction', 0.025), ('concurrent', 0.025), ('dunbrack', 0.024), ('feldman', 0.024), ('felzenszwalb', 0.024), ('kuhlman', 0.024), ('pdb', 0.024), ('szeliski', 0.024), ('ji', 0.024), ('success', 0.024), ('nodes', 0.024), ('design', 0.024), ('ri', 0.024), ('programs', 0.023), ('ni', 0.022), ('constraints', 0.022), ('solver', 0.022), ('memory', 0.021), ('node', 0.021), ('sends', 0.021), ('birch', 0.021), ('dual', 0.02), ('probable', 0.02), ('image', 0.02), ('freeman', 0.019), ('grid', 0.019), ('bixby', 0.019), ('acids', 0.019), ('lps', 0.019), ('pixel', 0.018), ('acid', 0.018), ('spanning', 0.018), ('ordinary', 0.017), ('atom', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="55-tfidf-1" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Chen Yanover, Talya Meltzer, Yair Weiss</p><p>Abstract: The problem of ﬁnding the most probable (MAP) conﬁguration in graphical models comes up in a wide range of applications. In a general graphical model this problem is NP hard, but various approximate algorithms have been developed. Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for ﬁnding the most probable assignment in small graphical models. However, applying this powerful method to real-world problems is extremely challenging due to the large numbers of variables and constraints in the linear program. Tree-Reweighted Belief Propagation is a promising recent algorithm for solving LP relaxations, but little is known about its running time on large problems. In this paper we compare tree-reweighted belief propagation (TRBP) and powerful generalpurpose LP solvers (CPLEX) on relaxations of real-world graphical models from the ﬁelds of computer vision and computational biology. We ﬁnd that TRBP almost always ﬁnds the solution signiﬁcantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. Using TRBP we can ﬁnd the MAP conﬁgurations in a matter of minutes for a large range of real world problems.</p><p>2 0.18639438 <a title="55-tfidf-2" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>3 0.078067854 <a title="55-tfidf-3" href="./jmlr-2006-Generalized_Bradley-Terry_Models_and_Multi-Class_Probability_Estimates.html">34 jmlr-2006-Generalized Bradley-Terry Models and Multi-Class Probability Estimates</a></p>
<p>Author: Tzu-Kuo Huang, Ruby C. Weng, Chih-Jen Lin</p><p>Abstract: The Bradley-Terry model for obtaining individual skill from paired comparisons has been popular in many areas. In machine learning, this model is related to multi-class probability estimates by coupling all pairwise classiﬁcation results. Error correcting output codes (ECOC) are a general framework to decompose a multi-class problem to several binary problems. To obtain probability estimates under this framework, this paper introduces a generalized Bradley-Terry model in which paired individual comparisons are extended to paired team comparisons. We propose a simple algorithm with convergence proofs to solve the model and obtain individual skill. Experiments on synthetic and real data demonstrate that the algorithm is useful for obtaining multi-class probability estimates. Moreover, we discuss four extensions of the proposed model: 1) weighted individual skill, 2) home-ﬁeld advantage, 3) ties, and 4) comparisons with more than two teams. Keywords: Bradley-Terry model, probability estimates, error correcting output codes, support vector machines</p><p>4 0.067840464 <a title="55-tfidf-4" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Olvi L. Mangasarian</p><p>Abstract: Support vector machines utilizing the 1-norm, typically set up as linear programs (Mangasarian, 2000; Bradley and Mangasarian, 1998), are formulated here as a completely unconstrained minimization of a convex differentiable piecewise-quadratic objective function in the dual space. The objective function, which has a Lipschitz continuous gradient and contains only one additional ﬁnite parameter, can be minimized by a generalized Newton method and leads to an exact solution of the support vector machine problem. The approach here is based on a formulation of a very general linear program as an unconstrained minimization problem and its application to support vector machine classiﬁcation problems. The present approach which generalizes both (Mangasarian, 2004) and (Fung and Mangasarian, 2004) is also applied to nonlinear approximation where a minimal number of nonlinear kernel functions are utilized to approximate a function from a given number of function values.</p><p>5 0.067517847 <a title="55-tfidf-5" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>Author: Martin J. Wainwright</p><p>Abstract: Consider the problem of joint parameter estimation and prediction in a Markov random ﬁeld: that is, the model parameters are estimated on the basis of an initial set of data, and then the ﬁtted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working under the restriction of limited computation, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for ﬁtting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the “wrong” model even in the inﬁnite data limit) is provably beneﬁcial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of convex variational methods. This stability result provides additional incentive, apart from the obvious beneﬁt of unique global optima, for using message-passing methods based on convex variational relaxations. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. Keywords: graphical model, Markov random ﬁeld, belief propagation, variational method, parameter estimation, prediction error, algorithmic stability</p><p>6 0.061354592 <a title="55-tfidf-6" href="./jmlr-2006-Point-Based_Value_Iteration_for_Continuous_POMDPs.html">74 jmlr-2006-Point-Based Value Iteration for Continuous POMDPs</a></p>
<p>7 0.053917024 <a title="55-tfidf-7" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.051777437 <a title="55-tfidf-8" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>9 0.048372667 <a title="55-tfidf-9" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.046980213 <a title="55-tfidf-10" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>11 0.03876318 <a title="55-tfidf-11" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.037213616 <a title="55-tfidf-12" href="./jmlr-2006-Consistency_of_Multiclass_Empirical_Risk_Minimization_Methods_Based_on_Convex_Loss.html">24 jmlr-2006-Consistency of Multiclass Empirical Risk Minimization Methods Based on Convex Loss</a></p>
<p>13 0.030670125 <a title="55-tfidf-13" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.029911356 <a title="55-tfidf-14" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>15 0.02987623 <a title="55-tfidf-15" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>16 0.029514799 <a title="55-tfidf-16" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<p>17 0.028182512 <a title="55-tfidf-17" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.027342362 <a title="55-tfidf-18" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.026612366 <a title="55-tfidf-19" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>20 0.024978751 <a title="55-tfidf-20" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.164), (1, -0.081), (2, -0.087), (3, 0.169), (4, 0.052), (5, 0.039), (6, 0.124), (7, 0.265), (8, 0.156), (9, -0.049), (10, -0.115), (11, -0.145), (12, -0.127), (13, -0.074), (14, -0.159), (15, 0.015), (16, -0.076), (17, -0.176), (18, 0.05), (19, 0.016), (20, -0.035), (21, -0.059), (22, -0.026), (23, 0.19), (24, 0.039), (25, 0.064), (26, -0.143), (27, 0.102), (28, -0.025), (29, -0.188), (30, -0.033), (31, 0.056), (32, 0.189), (33, -0.025), (34, -0.043), (35, -0.127), (36, -0.039), (37, 0.064), (38, 0.146), (39, 0.035), (40, 0.054), (41, 0.029), (42, -0.017), (43, 0.149), (44, 0.009), (45, 0.024), (46, -0.043), (47, -0.039), (48, 0.074), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96198177 <a title="55-lsi-1" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Chen Yanover, Talya Meltzer, Yair Weiss</p><p>Abstract: The problem of ﬁnding the most probable (MAP) conﬁguration in graphical models comes up in a wide range of applications. In a general graphical model this problem is NP hard, but various approximate algorithms have been developed. Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for ﬁnding the most probable assignment in small graphical models. However, applying this powerful method to real-world problems is extremely challenging due to the large numbers of variables and constraints in the linear program. Tree-Reweighted Belief Propagation is a promising recent algorithm for solving LP relaxations, but little is known about its running time on large problems. In this paper we compare tree-reweighted belief propagation (TRBP) and powerful generalpurpose LP solvers (CPLEX) on relaxations of real-world graphical models from the ﬁelds of computer vision and computational biology. We ﬁnd that TRBP almost always ﬁnds the solution signiﬁcantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. Using TRBP we can ﬁnd the MAP conﬁgurations in a matter of minutes for a large range of real world problems.</p><p>2 0.52956367 <a title="55-lsi-2" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>3 0.43324202 <a title="55-lsi-3" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Olvi L. Mangasarian</p><p>Abstract: Support vector machines utilizing the 1-norm, typically set up as linear programs (Mangasarian, 2000; Bradley and Mangasarian, 1998), are formulated here as a completely unconstrained minimization of a convex differentiable piecewise-quadratic objective function in the dual space. The objective function, which has a Lipschitz continuous gradient and contains only one additional ﬁnite parameter, can be minimized by a generalized Newton method and leads to an exact solution of the support vector machine problem. The approach here is based on a formulation of a very general linear program as an unconstrained minimization problem and its application to support vector machine classiﬁcation problems. The present approach which generalizes both (Mangasarian, 2004) and (Fung and Mangasarian, 2004) is also applied to nonlinear approximation where a minimal number of nonlinear kernel functions are utilized to approximate a function from a given number of function values.</p><p>4 0.34513563 <a title="55-lsi-4" href="./jmlr-2006-Generalized_Bradley-Terry_Models_and_Multi-Class_Probability_Estimates.html">34 jmlr-2006-Generalized Bradley-Terry Models and Multi-Class Probability Estimates</a></p>
<p>Author: Tzu-Kuo Huang, Ruby C. Weng, Chih-Jen Lin</p><p>Abstract: The Bradley-Terry model for obtaining individual skill from paired comparisons has been popular in many areas. In machine learning, this model is related to multi-class probability estimates by coupling all pairwise classiﬁcation results. Error correcting output codes (ECOC) are a general framework to decompose a multi-class problem to several binary problems. To obtain probability estimates under this framework, this paper introduces a generalized Bradley-Terry model in which paired individual comparisons are extended to paired team comparisons. We propose a simple algorithm with convergence proofs to solve the model and obtain individual skill. Experiments on synthetic and real data demonstrate that the algorithm is useful for obtaining multi-class probability estimates. Moreover, we discuss four extensions of the proposed model: 1) weighted individual skill, 2) home-ﬁeld advantage, 3) ties, and 4) comparisons with more than two teams. Keywords: Bradley-Terry model, probability estimates, error correcting output codes, support vector machines</p><p>5 0.28314194 <a title="55-lsi-5" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>Author: Dmitry M. Malioutov, Jason K. Johnson, Alan S. Willsky</p><p>Abstract: We present a new framework based on walks in a graph for analysis and inference in Gaussian graphical models. The key idea is to decompose the correlation between each pair of variables as a sum over all walks between those variables in the graph. The weight of each walk is given by a product of edgewise partial correlation coefﬁcients. This representation holds for a large class of Gaussian graphical models which we call walk-summable. We give a precise characterization of this class of models, and relate it to other classes including diagonally dominant, attractive, nonfrustrated, and pairwise-normalizable. We provide a walk-sum interpretation of Gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles. The walk-sum perspective leads to a better understanding of Gaussian belief propagation and to stronger results for its convergence in loopy graphs. Keywords: Gaussian graphical models, walk-sum analysis, convergence of loopy belief propagation</p><p>6 0.25583264 <a title="55-lsi-6" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.24513519 <a title="55-lsi-7" href="./jmlr-2006-Point-Based_Value_Iteration_for_Continuous_POMDPs.html">74 jmlr-2006-Point-Based Value Iteration for Continuous POMDPs</a></p>
<p>8 0.22030756 <a title="55-lsi-8" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>9 0.19439301 <a title="55-lsi-9" href="./jmlr-2006-Consistency_of_Multiclass_Empirical_Risk_Minimization_Methods_Based_on_Convex_Loss.html">24 jmlr-2006-Consistency of Multiclass Empirical Risk Minimization Methods Based on Convex Loss</a></p>
<p>10 0.19398171 <a title="55-lsi-10" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.1837061 <a title="55-lsi-11" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>12 0.16150591 <a title="55-lsi-12" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<p>13 0.15864262 <a title="55-lsi-13" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>14 0.15722445 <a title="55-lsi-14" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.14785489 <a title="55-lsi-15" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.13570148 <a title="55-lsi-16" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.12900165 <a title="55-lsi-17" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>18 0.12833756 <a title="55-lsi-18" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.11505985 <a title="55-lsi-19" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.11299919 <a title="55-lsi-20" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.011), (35, 0.01), (36, 0.052), (44, 0.466), (45, 0.022), (50, 0.041), (61, 0.023), (63, 0.03), (70, 0.011), (76, 0.039), (78, 0.013), (79, 0.012), (81, 0.02), (84, 0.016), (89, 0.017), (90, 0.023), (91, 0.039), (96, 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76266235 <a title="55-lda-1" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Chen Yanover, Talya Meltzer, Yair Weiss</p><p>Abstract: The problem of ﬁnding the most probable (MAP) conﬁguration in graphical models comes up in a wide range of applications. In a general graphical model this problem is NP hard, but various approximate algorithms have been developed. Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for ﬁnding the most probable assignment in small graphical models. However, applying this powerful method to real-world problems is extremely challenging due to the large numbers of variables and constraints in the linear program. Tree-Reweighted Belief Propagation is a promising recent algorithm for solving LP relaxations, but little is known about its running time on large problems. In this paper we compare tree-reweighted belief propagation (TRBP) and powerful generalpurpose LP solvers (CPLEX) on relaxations of real-world graphical models from the ﬁelds of computer vision and computational biology. We ﬁnd that TRBP almost always ﬁnds the solution signiﬁcantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. Using TRBP we can ﬁnd the MAP conﬁgurations in a matter of minutes for a large range of real world problems.</p><p>2 0.22804098 <a title="55-lda-2" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>Author: Mikio L. Braun</p><p>Abstract: The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to inﬁnity. We derive probabilistic ﬁnite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reﬂecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a signiﬁcant improvement over existing non-scaling bounds. Keywords: kernel matrix, eigenvalues, relative perturbation bounds</p><p>3 0.22755364 <a title="55-lda-3" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>Author: Martin J. Wainwright</p><p>Abstract: Consider the problem of joint parameter estimation and prediction in a Markov random ﬁeld: that is, the model parameters are estimated on the basis of an initial set of data, and then the ﬁtted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working under the restriction of limited computation, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for ﬁtting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the “wrong” model even in the inﬁnite data limit) is provably beneﬁcial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of convex variational methods. This stability result provides additional incentive, apart from the obvious beneﬁt of unique global optima, for using message-passing methods based on convex variational relaxations. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. Keywords: graphical model, Markov random ﬁeld, belief propagation, variational method, parameter estimation, prediction error, algorithmic stability</p><p>4 0.22660516 <a title="55-lda-4" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>5 0.22326767 <a title="55-lda-5" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>Author: Thomas Kämpke</p><p>Abstract: Similarity of edge labeled graphs is considered in the sense of minimum squared distance between corresponding values. Vertex correspondences are established by isomorphisms if both graphs are of equal size and by subisomorphisms if one graph has fewer vertices than the other. Best ﬁt isomorphisms and subisomorphisms amount to solutions of quadratic assignment problems and are computed exactly as well as approximately by minimum cost ﬂow, linear assignment relaxations and related graph algorithms. Keywords: assignment problem, best approximation, branch and bound, inexact graph matching, model data base</p><p>6 0.22288817 <a title="55-lda-6" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>7 0.22148588 <a title="55-lda-7" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>8 0.22141406 <a title="55-lda-8" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>9 0.22013651 <a title="55-lda-9" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.2199035 <a title="55-lda-10" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>11 0.21968961 <a title="55-lda-11" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.21902382 <a title="55-lda-12" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>13 0.21872728 <a title="55-lda-13" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>14 0.21867706 <a title="55-lda-14" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.21722439 <a title="55-lda-15" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.21661253 <a title="55-lda-16" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>17 0.2147693 <a title="55-lda-17" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>18 0.21441966 <a title="55-lda-18" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>19 0.21380872 <a title="55-lda-19" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.2129899 <a title="55-lda-20" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
