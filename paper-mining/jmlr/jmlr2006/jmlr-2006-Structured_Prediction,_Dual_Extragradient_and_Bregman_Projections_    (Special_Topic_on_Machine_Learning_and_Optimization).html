<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-89" href="#">jmlr2006-89</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-89-pdf" href="http://jmlr.org/papers/volume7/taskar06a/taskar06a.pdf">pdf</a></p><p>Author: Ben Taskar, Simon Lacoste-Julien, Michael I. Jordan</p><p>Abstract: We present a simple and scalable algorithm for maximum-margin estimation of structured output models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). The projection step can be solved using dynamic programming or combinatorial algorithms for min-cost convex ﬂow, depending on the structure of the problem. We show that this approach provides a memoryefﬁcient alternative to formulations based on reductions to a quadratic program (QP). We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.1 Keywords: Markov networks, large-margin methods, structured prediction, extragradient, Bregman projections</p><p>Reference: <a title="jmlr-2006-89-reference" href="../jmlr2006_reference/jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). [sent-10, score-0.476]
</p><p>2 We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm. [sent-13, score-0.331]
</p><p>3 Real-life examples of such problems include bipartite matchings in alignment of 2D shapes (Belongie et al. [sent-20, score-0.261]
</p><p>4 , 2002) and word alignment of sentences from a source language to a target language in machine translation (Matusov et al. [sent-21, score-0.415]
</p><p>5 In the case of structured prediction based on graphical models, which encompasses most work to date on structured prediction, two major approaches to discriminative learning have been explored: (1) maximum conditional likelihood (Lafferty et al. [sent-34, score-0.24]
</p><p>6 In Section 4 we discuss the dual extragradient method for solving saddle-point problems and show how it specializes to our setting. [sent-75, score-0.425]
</p><p>7 In Section 6 we illustrate the effectiveness of our approach on two very different largescale structured prediction tasks: 3D image segmentation and word alignment in natural language translation. [sent-77, score-0.464]
</p><p>8 × D N via P(y) ∝  ∏ φ j (y j ) ∏ φ jk (y j , yk ),  j∈V  jk∈E  where (V = {1, . [sent-93, score-0.676]
</p><p>9 , N}, E ⊂ { jk : j < k, j ∈ V , k ∈ V }) is an undirected graph, and where {φ j (y j ), j ∈ V } are the node potentials and {φ jk (y j , yk ), jk ∈ E } are the edge potentials. [sent-96, score-2.102]
</p><p>10 Similarly, we introduce variables z jkαβ to denote indicators ½(y j = α, yk = β) for all edges jk ∈ E and the values of their nodes, α ∈ D j , β ∈ D k . [sent-100, score-0.728]
</p><p>11 z jkαβ = zkβ , ∀ jk ∈ E , β ∈ D k ;  ∑  α∈D j α∈D j  z jkαβ log φ jk (α, β) z jkαβ = 1, ∀ jk ∈ E ;  (2)  z jkαβ = z jα , ∀ jk ∈ E , α ∈ D j ,  (3)  α∈D j ,β∈D k  ∑  β∈D k  (1)  where (2) expresses normalization constraints and (3) captures marginalization constraints. [sent-103, score-2.508]
</p><p>12 , submodular): log φ jk (0, 0) + log φ jk (1, 1) ≥ log φ jk (1, 0) + log φ jk (0, 1),  ∀ jk ∈ E . [sent-116, score-3.1]
</p><p>13 Assuming that such correlations tend to be positive (connected nodes tend to have the same label) leads us to consider simpliﬁed edge potentials of the form φ jk (y j , yk ) = exp{−s jk ½(y j = yk )}, where s jk is a nonnegative penalty for assigning y j and yk different labels. [sent-129, score-2.174]
</p><p>14 Note that such potentials are regular if s jk ≥ 0. [sent-130, score-0.717]
</p><p>15 Expressing node potentials as φ j (y j ) = exp{s j y j }, we have P(y) ∝ exp ∑ j∈V s j y j − ∑ jk∈E s jk ½(y j = yk ) . [sent-131, score-0.813]
</p><p>16 ∑ s jz j − ∑  j∈V  s jk z jk  jk∈E  z j − zk ≤ z jk , zk − z j ≤ z jk , ∀ jk ∈ E , 1630  (5)  S TRUCTURED P REDICTION , D UAL E XTRAGRADIENT AND B REGMAN P ROJECTIONS  En vertu de les nouvelles propositions , quel est le coût prévu de perception de les droits ? [sent-134, score-3.288]
</p><p>17 where the continuous variables z j correspond to a relaxation of the binary variables y j , and the constraints encode z jk = ½(z j = zk ). [sent-137, score-0.742]
</p><p>18 To see this, note that the constraints can be equivalently expressed as |z j − zk | ≤ z jk . [sent-138, score-0.742]
</p><p>19 Because s jk is positive, z jk = |zk − z j | at the maximum, which is equivalent to ½(z j = zk ) if the z j , zk variables are binary. [sent-139, score-1.428]
</p><p>20 We can parameterize the node and edge potentials in terms of user-provided features x j and x jk associated with the nodes and edges. [sent-141, score-0.806]
</p><p>21 In particular, in 3D range data, x j might involve spin-image features or spatial occupancy histograms of a point j, while x jk might include the distance between points j and k, the dot-product of their normals, etc. [sent-142, score-0.62]
</p><p>22 The simplest model of dependence is a linear combination of features: s j = w⊤ fn (x j ) and s jk = w⊤ fe (x jk ), where wn and we are node and edge e n parameters, and fn and fe are node and edge feature mappings, of dimension dn and de , respectively. [sent-143, score-1.472]
</p><p>23 To ensure non-negativity of s jk , we assume that the edge features fe are nonnegative and we impose the restriction we ≥ 0. [sent-144, score-0.696]
</p><p>24 We abbreviate the score assigned to a labeling y for an input x as w⊤ f(x, y) = ∑ j y j w⊤ fn (x j ) − ∑ jk∈E y jk w⊤ fe (x jk ), where y jk = ½(y j = yk ). [sent-147, score-1.943]
</p><p>25 3 Matchings Consider modeling the task of word alignment of parallel bilingual sentences (Fig. [sent-149, score-0.355]
</p><p>26 1b) as a maximum weight bipartite matching problem in a graph, where the nodes V = V s ∪ V t correspond to the words in the “source” sentence (V s ) and the “target” sentence (V t ) and the edges E = { jk : j ∈ V s , k ∈ V t } correspond to possible alignments between the words. [sent-150, score-0.987]
</p><p>27 The edge weight s jk represents the degree to which word j in one sentence can translate into the word k in the other sentence. [sent-152, score-0.977]
</p><p>28 We represent a matching using a set of 1631  TASKAR , L ACOSTE -J ULIEN AND J ORDAN  binary variables y jk that are set to 1 if word j is assigned to word k in the other sentence, and 0 otherwise. [sent-154, score-0.883]
</p><p>29 The score of an assignment is the sum of edge scores: s(y) = ∑ jk∈E s jk y jk . [sent-155, score-1.289]
</p><p>30 ∑  s jk z jk  ∑  z jk ≤ 1, ∀k ∈ V t ;  (6)  jk∈E  j∈V  s  ∑  z jk ≤ 1, ∀ j ∈ V s . [sent-158, score-2.48]
</p><p>31 k∈V t  where again the continuous variables z jk correspond to the relaxation of the binary variables y jk . [sent-159, score-1.24]
</p><p>32 For word alignment, the scores s jk can be deﬁned in terms of the word pair jk and input features associated with x jk . [sent-161, score-2.08]
</p><p>33 We let s jk = w⊤ f(x jk ) for a user-provided feature mapping f and abbreviate w⊤ f(x, y) = ∑ jk y jk w⊤ f(x jk ). [sent-163, score-3.1]
</p><p>34 In our word alignment example, the output space is deﬁned by the length of the two sentences. [sent-170, score-0.266]
</p><p>35 Large Margin Estimation We assume a set of training instances S = {(xi , yi )}m , where each instance consists of a structured i=1 object xi (such as a graph) and a target solution yi (such as a matching). [sent-180, score-0.239]
</p><p>36 i To obtain a convex formulation, we upper bound the loss ℓ(yi , hw (xi )) using the hinge function: maxy′i ∈Y i [w⊤ fi (y′ ) + ℓi (y′ ) − w⊤ fi (yi )], where ℓi (y′ ) = ℓ(yi , y′ ), and fi (y′ ) = f(xi , y′ ). [sent-191, score-0.444]
</p><p>37 Minimizing i i i i i i this upper bound will force the true structure yi to be optimal with respect to w for each instance i: min  w∈W  ∑ max[w⊤ fi (y′i ) + ℓi (y′i )] − w⊤ fi (yi ), y ∈Y i  ′ i  (8)  i  where W is the set of allowed parameters w. [sent-192, score-0.325]
</p><p>38 For general submodular potentials, we can parameterize the log of the edge potential using four sets of edge parameters, we00 , we01 , we10 , we11 , as follows: log φ jk (α, β) = w⊤ f(x jk ). [sent-205, score-1.376]
</p><p>39 i i ′  (9)  yi ∈Y i  This optimization problem has precisely the same form as the prediction problem whose parameters we are trying to learn—maxy′i ∈Y i w⊤ fi (y′ )—but with an additional term corresponding to the loss i function. [sent-209, score-0.244]
</p><p>40 (6) (without the constant term ∑ jk c- yi, jk ): max  0≤zi ≤1  s. [sent-215, score-1.24]
</p><p>41 ∑  zi, jk [w⊤ f(xi, jk ) + c+ − (c- + c+ )yi, jk ]  jk∈E i  ∑  ∑  zi, jk ≤ 1, ∀k ∈ V it ;  j∈V is  zi, jk ≤ 1, ∀ j ∈ V is ,  k∈V it  where f(xi, jk ) is the vector of features of the edge jk in example i and V is and V it are the nodes in example i. [sent-217, score-4.389]
</p><p>42 As before, the continuous variables zi, jk correspond to the binary values y′ jk . [sent-218, score-1.24]
</p><p>43 i, Generally, suppose we can express the prediction problem as an LP: max w⊤ fi (y′ ) = max w⊤ Fi zi , i ′ zi ∈Z i  yi ∈Y i  where  Z i = {zi : Ai zi ≤ bi , 0 ≤ zi ≤ 1},  (11)  for appropriately deﬁned Fi , Ai and bi . [sent-219, score-0.784]
</p><p>44 Then we have a similar LP for the loss-augmented inference for each example i: (12) max w⊤ fi (y′ ) + ℓi (y′ ) = di + max(F⊤ w + ci )⊤ zi , i i i ′ zi ∈Z i  yi ∈Y i  for appropriately deﬁned di and ci . [sent-220, score-0.53]
</p><p>45 For the matching case, di = ∑ jk c- yi, jk is the constant term, Fi is a matrix that has a column of features f(xi, jk ) for each edge jk in example i, and ci is the vector of the loss terms c+ − (c- + c+ )yi, jk . [sent-221, score-3.223]
</p><p>46 With these deﬁnitions, we have the following saddle-point problem: min max  w∈W z∈Z  ∑  w⊤ Fi zi + c⊤ zi − w⊤ fi (yi ) . [sent-229, score-0.397]
</p><p>47 Denote the objective of the saddle-point problem in (13) by: L (w, z) ≡ ∑ w⊤ Fi zi + c⊤ zi − w⊤ fi (yi ). [sent-264, score-0.397]
</p><p>48 We pursue a different approach that is based on the dual extragradient method of Nesterov (2003). [sent-271, score-0.425]
</p><p>49 , 2006), we used a related method, the extragradient method due to Korpelevich (1976). [sent-273, score-0.294]
</p><p>50 The dual extragradient is, however, a more ﬂexible and general method, in terms of the types of projections and feasible sets that can be used, allowing a broader range of structured problems and parameter regularization schemes. [sent-274, score-0.627]
</p><p>51 1 Dual Extragradient We ﬁrst present the dual extragradient algorithm of Nesterov (2003) using the Euclidean geometry induced by the standard 2-norm, and consider a non-Euclidean setup in Sec. [sent-292, score-0.425]
</p><p>52 2, the dual extragradient algorithm proceeds using very simple gradient and projection calculations. [sent-296, score-0.509]
</p><p>53 For non-optimal points (w, z), the gap G (w, z) is positive and serves as a useful merit function, a measure of accuracy of a solution found by the extragradient algorithm. [sent-316, score-0.294]
</p><p>54 The original edges jk have a quadratic cost 1 (z′jk − 2 z jk )2 and capacity 1. [sent-332, score-1.292]
</p><p>55 The complexity of these projections is the key issue determining the viability of the extragradient approach for our class of problems. [sent-342, score-0.353]
</p><p>56 In fact, for both alignment and matchings these projections turn out to reduce to classical network ﬂow problems for which efﬁcient solutions exist. [sent-343, score-0.277]
</p><p>57 1 ′ (z − zi, jk )2 2 i, jk jk∈E i  ∑  ∑  zi, jk ≤ 1, ∀ j ∈ V it ;  j∈V is  ∑  zi, jk ≤ 1, ∀k ∈ V is . [sent-347, score-2.48]
</p><p>58 The original 1 edges jk have a quadratic cost 2 (z′ jk − zi, jk )2 and capacity 1. [sent-350, score-1.912]
</p><p>59 Iteration t, 0 ≤ t ≤ τ: ˆ zi vzi = Tη (uzi , st−1 ), ∀i;  ˆ w vw = Tη (uw , st−1 ); utw = Tη (vw , −  ∑ Fi vz − fi (yi ) i  );  utzi = Tη (vzi , F⊤ vw + ci ), ∀i; i  i  stw = st−1 − w  ∑ Fi utz − fi (yi ) i  stzi = st−1 + F⊤ utw + ci , ∀i. [sent-366, score-0.832]
</p><p>60 ||u − u′ || u,u ∈U  L ≡ max ′  The dual extragradient algorithm adjusts to the geometry induced by the norm by making use of Bregman divergences. [sent-383, score-0.462]
</p><p>61 Let us use a more suggestive notation for the components of z: z j (α) = z jα and z jk (α, β) = z jkαβ . [sent-410, score-0.62]
</p><p>62 We can construct a natural joint probability distribution via Pz (y) =  ∏ z jk (y j , yk ) ∏ (z j (y j ))1−q , j  jk∈E  j∈V  where q j is the number of neighbors of node j. [sent-411, score-0.716]
</p><p>63 We deﬁne h(z) as the negative entropy of the distribution represented by z: h(z) =  ∑  ∑  z jk (α, β) log z jk (α, β) + (1 − q j )  ∑ ∑  z j (α) log z j (α). [sent-414, score-1.24]
</p><p>64 ˜ Hence, to obtain the projection z, we compute the node and edge marginals of the distribution Pz (y) ˜ via the standard sum-product dynamic programming algorithm using the node and edge potentials deﬁned above. [sent-417, score-0.326]
</p><p>65 For example, for word alignment, we need O (|V is | log |V it |) bits to encode a matching yi by using roughly log V it bits per node in V is to identify its match. [sent-427, score-0.264]
</p><p>66 The situation is worse in context-free parsing, where a parse tree yi requires space linear in the sentence length and logarithmic in grammar size, while |Z i | is the product of the grammar size and the cube of the sentence length. [sent-429, score-0.247]
</p><p>67 In particular, instead ¯ ¯ ¯ of maintaining the entire vector ut and reconstructing st from ut , we can instead store only utw and t between iterations, since sw stzi = (t + 1)(F⊤ utw + ci ). [sent-433, score-0.251]
</p><p>68 Experiments In this section we describe experiments focusing on two of the structured models we described earlier: bipartite matchings for word alignments and restricted potential Markov nets for 3D segmentation. [sent-446, score-0.391]
</p><p>69 3 We compared three algorithms: the dual extragradient (dual-ex), the averaged projected gradient (proj-grad) deﬁned in Eq. [sent-447, score-0.491]
</p><p>70 Each example is processed one by one and the intermediate results are accumulated as rw = rw − Fi vzi + fi (yi ) and qw = qw − Fi uzi + fi (yi ). [sent-457, score-0.504]
</p><p>71 Example i, 1 ≤ i ≤ m: ˆ vzi = Tη (uzi ,t(F⊤ uw + ci )); i ¯ uzi = ¯ t u +T (v ,rw )  η ¯ uw = w t+1 w ¯ Output w = uw . [sent-462, score-0.416]
</p><p>72 Tη (vzi , F⊤ vw + ci ); i  rw = rw − Fi vzi + fi (yi ); sw = sw − Fi uzi + fi (yi ). [sent-463, score-0.68]
</p><p>73 i We compared the performance of the dual extragradient algorithm along its unregularized path to solutions of the regularized problems for different settings of the norm. [sent-493, score-0.526]
</p><p>74 The unregularized dual extragradient seems to explore the range of models (in terms on their norm) on the regularization path more thoroughly than the averaged perceptron and eventually asymptotes to the unregularized solution, while proj-grad quickly achieves very large norm. [sent-506, score-0.702]
</p><p>75 The stepsize for the dual extragradient algorithm was chosen to be 1/||F||2 . [sent-558, score-0.425]
</p><p>76 8(e) compare the hinge loss of the regularization path with the evolution of the objective for the unregularized dual extragradient, averaged projected gradient and averaged perceptron algorithms when trained on the Gold data set, 500 sentences and 1000 sentences of the GIZA++ output respectively. [sent-570, score-0.71]
</p><p>77 5 The dual extragradient path appears to follow the regularization path closely for ||w|| ≤ 2 and ||w|| ≥ 12. [sent-571, score-0.581]
</p><p>78 8(b) compares the AER on the test set along the dual extragradient path trained on the Gold dataset versus the regularization path AER. [sent-573, score-0.581]
</p><p>79 Interestingly, the unregularized dual extragradient path seems to give better performance on the test set than that obtained by optimizing along the regularization path. [sent-576, score-0.572]
</p><p>80 The dominance of the dual extragradient path over the regularization path is more salient in ﬁgure 8(f) for the case where both are trained on 1000 sentences from the GIZA++ output. [sent-577, score-0.696]
</p><p>81 We conjecture that the dual extragradient method provides additional statistical regularization (compensating for the noisier labels of the GIZA++ output) by enforcing local smoothness of the path in parameter space. [sent-578, score-0.526]
</p><p>82 The averaged projected gradient performed much better for this task than segmentation, getting somewhat close to the dual extragradient path as is shown in Fig. [sent-579, score-0.546]
</p><p>83 1 0  5  10  15 ||w||  20  25  30  2  (e)  (f)  Figure 8: Word alignment results: (a) Training hinge loss for the three different algorithms and the regularization path on the Gold dataset. [sent-630, score-0.257]
</p><p>84 The running time for 500 iterations of dual extragradient on a 3. [sent-641, score-0.425]
</p><p>85 Using a saddle-point formulation of the problem, we exploit the dual extragradient algorithm, a simple gradient-based algorithm for saddle-point problems (Nesterov, 2003). [sent-649, score-0.425]
</p><p>86 Key to our approach is the recognition that the projection step in the extragradient algorithm can be solved by network ﬂow algorithms for matchings and min-cuts (and dynamic programming for decomposable models). [sent-651, score-0.433]
</p><p>87 Min-Cut Polytope Projections Consider projection for a single example i: 1 1 ′ (z j − z j )2 + ∑ (z′jk − z jk )2 2 jk∈E 2 j∈V 0 ≤ z j ≤ 1, ∀ j ∈ V ; z j − zk ≤ z jk , zk − z j ≤ z jk , ∀ jk ∈ E . [sent-660, score-2.719]
</p><p>88 We introduce non-negative Lagrangian variables j λ jk , λk j for the two constraints for each edge jk and λ j0 for the constraint z j ≤ 1 each node j. [sent-664, score-1.357]
</p><p>89 j∈V  jk∈E  jk∈E  j∈V  So the Lagrangian becomes: L(z, λ) =  ∑  h+ (z j ) + z j λ0 j − λ j0 + j  j∈V  1 ′ (z − z jk )2 − z jk (λ jk + λk j ) . [sent-666, score-1.86]
</p><p>90 2 jk jk∈E  ∑  Now, minimizing L(z, λ) with respect to z, we have min L(z, λ) = z  ∑  q jk (λ jk + λk j ) +  jk∈E  ∑ [q0 j (λ0 j ) − λ j0 ],  j∈V  where q jk (λ jk + λk j ) = minz jk 1 (z′jk − z jk )2 − z jk (λ jk + λk j ) and q0 j (λ0 j ) = minz j [h+ (z j ) + z j λ0 j ]. [sent-667, score-5.58]
</p><p>91 j 2 The minimizing values of z are: z∗ = arg min h+ (z j ) + z j λ0 j = j j zj  z∗ = arg min jk z jk  0 z′j − λ0 j  λ0 j ≥ z′j ; λ0 j ≤ z′j ;  1 ′ (z − z jk )2 − z jk (λ jk + λk j ) = z′jk + λ jk + λk j . [sent-668, score-3.72]
</p><p>92 2 jk  Hence, we have: 1 q jk (λ jk + λk j ) = −z′jk (λ jk + λk j ) − (λ jk + λk j )2 2 1 ′2 λ0 j ≥ z′j ; 2zj q0 j (λ0 j ) = 1 λ0 j ≤ z′j . [sent-669, score-3.1]
</p><p>93 z′j λ0 j − 2 λ2 j 0 1649  TASKAR , L ACOSTE -J ULIEN AND J ORDAN  The dual of the projection problem is thus: 1 −z′jk (λ jk + λk j ) − (λ jk + λk j )2 2 jk∈E  ∑ [q0 j (λ0 j ) − λ j0 ] + ∑  max λ  j∈V  λ j0 − λ0 j +  s. [sent-670, score-1.422]
</p><p>94 ∑ (λ jk − λk j ) = 0,  (22)  ∀j ∈ V ;  jk∈E  λ jk , λk j ≥ 0, ∀ jk ∈ E ; λ j0 ≥ 0, ∀ j ∈ V . [sent-672, score-1.86]
</p><p>95 Interpreting λ jk as ﬂow from node j to node k, and λk j as ﬂow from k to j and λ j0 , λ0 j as ﬂow from and to a special node 0, we can identify the constraints of Eq. [sent-673, score-0.768]
</p><p>96 The last transformation we need is to address the presence of cross-terms λ jk λk j in the objective. [sent-675, score-0.62]
</p><p>97 Note that in the ﬂow conservation constraints, λ jk , λk j always appear together as λ jk − λk j . [sent-676, score-1.24]
</p><p>98 Since we are minimizing (λ jk + λk j )2 subject to constraints on λ jk − λk j , at least one of λ jk , λk j will be zero at the optimum and the cross-terms can be ignored. [sent-677, score-1.888]
</p><p>99 1 1 z′jk λ jk + λ2 + ∑ z′jk λk j + λ2 j 2 jk 2 k jk∈E jk∈E  ∑ [−q0 j (λ0 j ) + λ j0 ] + ∑  j∈V  λ j0 − λ0 j +  ∑ (λ jk − λk j ) = 0,  (23)  ∀j ∈ V ;  jk∈E  λ jk , λk j ≥ 0, ∀ jk ∈ E ; λ j0 ≥ 0, ∀ j ∈ V . [sent-682, score-3.1]
</p><p>100 The extragradient method for ﬁnding saddle points and other problems. [sent-760, score-0.294]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('jk', 0.62), ('extragradient', 0.294), ('taskar', 0.201), ('zi', 0.135), ('dual', 0.131), ('alignment', 0.13), ('fi', 0.127), ('acoste', 0.115), ('regman', 0.115), ('ulien', 0.115), ('xtragradient', 0.115), ('sentences', 0.115), ('word', 0.11), ('aer', 0.107), ('rojections', 0.105), ('ual', 0.098), ('ordan', 0.098), ('tructured', 0.098), ('potentials', 0.097), ('structured', 0.097), ('zk', 0.094), ('sentence', 0.088), ('matchings', 0.088), ('rediction', 0.088), ('ow', 0.087), ('reg', 0.08), ('uzi', 0.08), ('vzi', 0.08), ('vw', 0.075), ('uw', 0.075), ('ben', 0.074), ('yi', 0.071), ('pz', 0.071), ('giza', 0.062), ('grad', 0.062), ('proj', 0.062), ('utw', 0.062), ('nesterov', 0.061), ('projections', 0.059), ('ex', 0.058), ('gold', 0.057), ('lp', 0.056), ('yk', 0.056), ('bregman', 0.056), ('path', 0.055), ('chekuri', 0.053), ('ave', 0.053), ('perc', 0.053), ('alignments', 0.053), ('edges', 0.052), ('segmentation', 0.051), ('perceptron', 0.051), ('projection', 0.051), ('validation', 0.05), ('edge', 0.049), ('prediction', 0.046), ('regularization', 0.046), ('unregularized', 0.046), ('rw', 0.045), ('matching', 0.043), ('bipartite', 0.043), ('node', 0.04), ('scoring', 0.038), ('submodular', 0.038), ('vd', 0.037), ('convex', 0.037), ('norm', 0.037), ('divergence', 0.036), ('anguelov', 0.036), ('markov', 0.035), ('euclidean', 0.035), ('sw', 0.035), ('st', 0.034), ('kolmogorov', 0.034), ('dw', 0.034), ('averaged', 0.033), ('gradient', 0.033), ('maxy', 0.032), ('qp', 0.031), ('ci', 0.031), ('simplex', 0.031), ('language', 0.03), ('uz', 0.03), ('daphne', 0.03), ('zm', 0.03), ('dz', 0.03), ('collins', 0.029), ('constraints', 0.028), ('combinatorial', 0.028), ('polytope', 0.027), ('favorable', 0.027), ('uv', 0.027), ('fe', 0.027), ('esd', 0.027), ('guerriero', 0.027), ('och', 0.027), ('stzi', 0.027), ('vassil', 0.027), ('hinge', 0.026), ('output', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999845 <a title="89-tfidf-1" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Ben Taskar, Simon Lacoste-Julien, Michael I. Jordan</p><p>Abstract: We present a simple and scalable algorithm for maximum-margin estimation of structured output models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). The projection step can be solved using dynamic programming or combinatorial algorithms for min-cost convex ﬂow, depending on the structure of the problem. We show that this approach provides a memoryefﬁcient alternative to formulations based on reductions to a quadratic program (QP). We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.1 Keywords: Markov networks, large-margin methods, structured prediction, extragradient, Bregman projections</p><p>2 0.13015626 <a title="89-tfidf-2" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><p>3 0.12827808 <a title="89-tfidf-3" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>4 0.093002088 <a title="89-tfidf-4" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>5 0.06750384 <a title="89-tfidf-5" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>Author: Martin J. Wainwright</p><p>Abstract: Consider the problem of joint parameter estimation and prediction in a Markov random ﬁeld: that is, the model parameters are estimated on the basis of an initial set of data, and then the ﬁtted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working under the restriction of limited computation, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for ﬁtting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the “wrong” model even in the inﬁnite data limit) is provably beneﬁcial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of convex variational methods. This stability result provides additional incentive, apart from the obvious beneﬁt of unique global optima, for using message-passing methods based on convex variational relaxations. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. Keywords: graphical model, Markov random ﬁeld, belief propagation, variational method, parameter estimation, prediction error, algorithmic stability</p><p>6 0.060963303 <a title="89-tfidf-6" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>7 0.057430509 <a title="89-tfidf-7" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>8 0.055984639 <a title="89-tfidf-8" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>9 0.054451175 <a title="89-tfidf-9" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.052020136 <a title="89-tfidf-10" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>11 0.048372667 <a title="89-tfidf-11" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.047866311 <a title="89-tfidf-12" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.044891812 <a title="89-tfidf-13" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>14 0.044688907 <a title="89-tfidf-14" href="./jmlr-2006-Universal_Kernels.html">93 jmlr-2006-Universal Kernels</a></p>
<p>15 0.041047283 <a title="89-tfidf-15" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>16 0.039166391 <a title="89-tfidf-16" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>17 0.036983024 <a title="89-tfidf-17" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>18 0.03485373 <a title="89-tfidf-18" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>19 0.034317169 <a title="89-tfidf-19" href="./jmlr-2006-Worst-Case_Analysis_of_Selective_Sampling_for_Linear_Classification.html">96 jmlr-2006-Worst-Case Analysis of Selective Sampling for Linear Classification</a></p>
<p>20 0.034257066 <a title="89-tfidf-20" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.234), (1, -0.065), (2, 0.008), (3, 0.116), (4, -0.033), (5, 0.008), (6, 0.134), (7, 0.187), (8, 0.014), (9, -0.017), (10, -0.138), (11, 0.052), (12, -0.005), (13, 0.061), (14, 0.046), (15, -0.002), (16, -0.019), (17, -0.153), (18, -0.014), (19, -0.271), (20, -0.005), (21, 0.022), (22, 0.004), (23, -0.198), (24, 0.121), (25, -0.032), (26, 0.176), (27, -0.116), (28, 0.115), (29, 0.1), (30, -0.172), (31, -0.055), (32, 0.067), (33, 0.01), (34, 0.108), (35, 0.137), (36, 0.036), (37, 0.038), (38, -0.027), (39, 0.022), (40, 0.029), (41, 0.032), (42, -0.009), (43, 0.003), (44, -0.183), (45, 0.06), (46, -0.064), (47, -0.12), (48, 0.22), (49, -0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97443503 <a title="89-lsi-1" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Ben Taskar, Simon Lacoste-Julien, Michael I. Jordan</p><p>Abstract: We present a simple and scalable algorithm for maximum-margin estimation of structured output models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). The projection step can be solved using dynamic programming or combinatorial algorithms for min-cost convex ﬂow, depending on the structure of the problem. We show that this approach provides a memoryefﬁcient alternative to formulations based on reductions to a quadratic program (QP). We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.1 Keywords: Markov networks, large-margin methods, structured prediction, extragradient, Bregman projections</p><p>2 0.72820187 <a title="89-lsi-2" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by deﬁning a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a signiﬁcantly smaller size and prove that it attains the same minimum. We then describe an efﬁcient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron deﬁned by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show signiﬁcant improvements in run time over a state of the art interior-point algorithm.</p><p>3 0.37827668 <a title="89-lsi-3" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>4 0.34882706 <a title="89-lsi-4" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>5 0.33923483 <a title="89-lsi-5" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>Author: Luis M. de Campos</p><p>Abstract: We propose a new scoring function for learning Bayesian networks from data using score+search algorithms. This is based on the concept of mutual information and exploits some well-known properties of this measure in a novel way. Essentially, a statistical independence test based on the chi-square distribution, associated with the mutual information measure, together with a property of additive decomposition of this measure, are combined in order to measure the degree of interaction between each variable and its parent variables in the network. The result is a non-Bayesian scoring function called MIT (mutual information tests) which belongs to the family of scores based on information theory. The MIT score also represents a penalization of the Kullback-Leibler divergence between the joint probability distributions associated with a candidate network and with the available data set. Detailed results of a complete experimental evaluation of the proposed scoring function and its comparison with the well-known K2, BDeu and BIC/MDL scores are also presented. Keywords: Bayesian networks, scoring functions, learning, mutual information, conditional independence tests</p><p>6 0.29406717 <a title="89-lsi-6" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>7 0.29277897 <a title="89-lsi-7" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.26555353 <a title="89-lsi-8" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>9 0.22084016 <a title="89-lsi-9" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.21839306 <a title="89-lsi-10" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>11 0.19981577 <a title="89-lsi-11" href="./jmlr-2006-Universal_Kernels.html">93 jmlr-2006-Universal Kernels</a></p>
<p>12 0.19761787 <a title="89-lsi-12" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.19163327 <a title="89-lsi-13" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.18820414 <a title="89-lsi-14" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.18316929 <a title="89-lsi-15" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>16 0.17219575 <a title="89-lsi-16" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.16744632 <a title="89-lsi-17" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>18 0.16344769 <a title="89-lsi-18" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>19 0.15599789 <a title="89-lsi-19" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>20 0.14970893 <a title="89-lsi-20" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.012), (8, 0.021), (35, 0.01), (36, 0.043), (45, 0.02), (46, 0.337), (50, 0.04), (61, 0.017), (63, 0.036), (68, 0.023), (76, 0.026), (78, 0.021), (79, 0.014), (81, 0.034), (84, 0.028), (90, 0.037), (91, 0.064), (96, 0.113)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82203865 <a title="89-lda-1" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>Author: Sayan Mukherjee, Ding-Xuan Zhou</p><p>Abstract: We introduce an algorithm that learns gradients from samples in the supervised learning framework. An error analysis is given for the convergence of the gradient estimated by the algorithm to the true gradient. The utility of the algorithm for the problem of variable selection as well as determining variable covariance is illustrated on simulated data as well as two gene expression data sets. For square loss we provide a very efﬁcient implementation with respect to both memory and time. Keywords: Tikhnonov regularization, variable selection, reproducing kernel Hilbert space, generalization bounds</p><p>same-paper 2 0.78600907 <a title="89-lda-2" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Ben Taskar, Simon Lacoste-Julien, Michael I. Jordan</p><p>Abstract: We present a simple and scalable algorithm for maximum-margin estimation of structured output models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). The projection step can be solved using dynamic programming or combinatorial algorithms for min-cost convex ﬂow, depending on the structure of the problem. We show that this approach provides a memoryefﬁcient alternative to formulations based on reductions to a quadratic program (QP). We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.1 Keywords: Markov networks, large-margin methods, structured prediction, extragradient, Bregman projections</p><p>3 0.59972215 <a title="89-lda-3" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>Author: Sayan Mukherjee, Qiang Wu</p><p>Abstract: We introduce an algorithm that simultaneously estimates a classiﬁcation function as well as its gradient in the supervised learning framework. The motivation for the algorithm is to ﬁnd salient variables and estimate how they covary. An efﬁcient implementation with respect to both memory and time is given. The utility of the algorithm is illustrated on simulated data as well as a gene expression data set. An error analysis is given for the convergence of the estimate of the classiﬁcation function and its gradient to the true classiﬁcation function and true gradient. Keywords: Tikhnonov regularization, variable selection, reproducing kernel Hilbert space, generalization bounds, classiﬁcation</p><p>4 0.42520475 <a title="89-lda-4" href="./jmlr-2006-MinReg%3A_A_Scalable_Algorithm_for_Learning_Parsimonious_Regulatory_Networks_in_Yeast_and_Mammals.html">62 jmlr-2006-MinReg: A Scalable Algorithm for Learning Parsimonious Regulatory Networks in Yeast and Mammals</a></p>
<p>Author: Dana Pe'er, Amos Tanay, Aviv Regev</p><p>Abstract: In recent years, there has been a growing interest in applying Bayesian networks and their extensions to reconstruct regulatory networks from gene expression data. Since the gene expression domain involves a large number of variables and a limited number of samples, it poses both computational and statistical challenges to Bayesian network learning algorithms. Here we deﬁne a constrained family of Bayesian network structures suitable for this domain and devise an efﬁcient search algorithm that utilizes these structural constraints to ﬁnd high scoring networks from data. Interestingly, under reasonable assumptions on the underlying probability distribution, we can provide performance guarantees on our algorithm. Evaluation on real data from yeast and mouse, demonstrates that our method cannot only reconstruct a high quality model of the yeast regulatory network, but is also the ﬁrst method to scale to the complexity of mammalian networks and successfully reconstructs a reasonable model over thousands of variables. Keywords: Bayesian networks, structure learning, gene networks, gene expression, approximation algorithms</p><p>5 0.41033649 <a title="89-lda-5" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>6 0.4007206 <a title="89-lda-6" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.39253306 <a title="89-lda-7" href="./jmlr-2006-Rearrangement_Clustering%3A_Pitfalls%2C_Remedies%2C_and_Applications.html">78 jmlr-2006-Rearrangement Clustering: Pitfalls, Remedies, and Applications</a></p>
<p>8 0.38997471 <a title="89-lda-8" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.38899136 <a title="89-lda-9" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>10 0.38497987 <a title="89-lda-10" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>11 0.38172048 <a title="89-lda-11" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>12 0.38158411 <a title="89-lda-12" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>13 0.38002598 <a title="89-lda-13" href="./jmlr-2006-Adaptive_Prototype_Learning_Algorithms%3A_Theoretical_and_Experimental_Studies.html">13 jmlr-2006-Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies</a></p>
<p>14 0.37764075 <a title="89-lda-14" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.37602121 <a title="89-lda-15" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.37595785 <a title="89-lda-16" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>17 0.3737542 <a title="89-lda-17" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.37325487 <a title="89-lda-18" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.37152424 <a title="89-lda-19" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.36945319 <a title="89-lda-20" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
