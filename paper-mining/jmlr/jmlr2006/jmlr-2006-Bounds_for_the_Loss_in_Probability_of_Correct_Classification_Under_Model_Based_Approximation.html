<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-17" href="#">jmlr2006-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</h1>
<br/><p>Source: <a title="jmlr-2006-17-pdf" href="http://jmlr.org/papers/volume7/ekdahl06a/ekdahl06a.pdf">pdf</a></p><p>Author: Magnus Ekdahl, Timo Koski</p><p>Abstract: In many pattern recognition/classiﬁcation problem the true class conditional model and class probabilities are approximated for reasons of reducing complexity and/or of statistical estimation. The approximated classiﬁer is expected to have worse performance, here measured by the probability of correct classiﬁcation. We present an analysis valid in general, and easily computable formulas for estimating the degradation in probability of correct classiﬁcation when compared to the optimal classiﬁer. An example of an approximation is the Na¨ve Bayes classiﬁer. We show that the perforı mance of the Na¨ve Bayes depends on the degree of functional dependence between the features ı and labels. We provide a sufﬁcient condition for zero loss of performance, too. Keywords: Bayesian networks, na¨ve Bayes, plug-in classiﬁer, Kolmogorov distance of variation, ı variational learning</p><p>Reference: <a title="jmlr-2006-17-reference" href="../jmlr2006_reference/jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The approximated classiﬁer is expected to have worse performance, here measured by the probability of correct classiﬁcation. [sent-6, score-0.108]
</p><p>2 We present an analysis valid in general, and easily computable formulas for estimating the degradation in probability of correct classiﬁcation when compared to the optimal classiﬁer. [sent-7, score-0.152]
</p><p>3 Introduction Classiﬁcation procedures based on probability models are widely used in data mining and machine learning (Hand et al. [sent-12, score-0.069]
</p><p>4 We consider classiﬁcation procedures based on class conditional probabilities that belong to a model family that does not necessarily contain the true probability distribution, and analyze how the probability of correct classiﬁcation is affected. [sent-17, score-0.214]
</p><p>5 By this we refer to the formal operation performed by the optimal classiﬁer based on Bayes’ formula of posterior probabilities of classes, but now plugging in the modeling or approximate class conditional densities as well as approximated class probabilities. [sent-19, score-0.173]
</p><p>6 As the network topologies increase in size and complexity, the run-time complexity of probabilistic inference and classiﬁcation procedures becomes prohibitive. [sent-30, score-0.058]
</p><p>7 One way of approximating or simplifying the model is to enforce additional conditional independencies or by removing edges in the graph, see van Engelen (1997) and the references therein. [sent-32, score-0.085]
</p><p>8 Here we analyze a simpliﬁcation of Bayesian networks by a strategy of approximating factors of the joint probability, and give a bound for the probability of correct classiﬁcation under the ensuing plug-in function. [sent-33, score-0.135]
</p><p>9 This corresponds to some degree to the general heuristics in the work by Lewis (1959); Brown (1959); Chow and Liu (1968); Ku and Kullback (1969), who developed the idea of approximating multivariate discrete probability distributions by a product of lower order marginal distributions. [sent-34, score-0.215]
</p><p>10 The set of marginal distributions applied needs not be the full set of margins of some order, the requirements are that the product is an extension of the lower order distributions which are compatible. [sent-35, score-0.086]
</p><p>11 Section 4 provides rationales and examples of approximating models and plug-in classiﬁers. [sent-38, score-0.054]
</p><p>12 Section 5 introduces results about the degradation of classiﬁer performance with respect to the optimal probability of correct classiﬁcation. [sent-40, score-0.112]
</p><p>13 The results are phrased in terms of a distance between probabilities known as the Kolmogorov variation distance. [sent-41, score-0.108]
</p><p>14 There are several well known bounds for the Kolmogorov variation distance by other distances between probability measures, quoted in Section 5, which in many examples yield explicit and computable bounds for the plug-in classiﬁer performance. [sent-42, score-0.21]
</p><p>15 This assumption is needed to justify the use of results such as the Fubini theorem and the existence of conditional densities. [sent-54, score-0.074]
</p><p>16 In P(X ∈ A|C = c) =  Z  f (x|c)dµ(x)  A  we call f (x|c) the conditional density of a sample x given that the random variable C equals the label c with respect to the σ-ﬁnite measure µ. [sent-71, score-0.075]
</p><p>17 We assume in other words that µ dominates, see Schervish (1995), the probability measure P(·|C = c) for every c, that is, the same measure µ can be used for all P(X · |C = c) to deﬁne the corresponding class conditional density f (x|c). [sent-72, score-0.118]
</p><p>18 P(c) is the short notation for the marginal probability P(C = c). [sent-74, score-0.075]
</p><p>19 For example, the procedure known as proportional prediction chooses the label for x by drawing c from the probability mass function P(c|x) (Goodman and Kruskal, 1954). [sent-77, score-0.115]
</p><p>20 We evaluate the performance of a classiﬁer by the probability of correct classiﬁcation and assess ˆ the effect of approximating f (x|c) by fˆ(x|c) and P(c) by P(c). [sent-85, score-0.135]
</p><p>21 Deﬁnition 3 For a classiﬁer c(X) the probability of correct classiﬁcation is P(c(X) = C). [sent-86, score-0.081]
</p><p>22 If we set exists a literature for bounding the optimal probability of error, Pe ˆB Pe = 1 − P(cB (X) = C), then ˆˆ ∗ P(cB (X) = C) − P(cB (X) = C) = Pe − Pe . [sent-94, score-0.091]
</p><p>23 Examples of Plug-In Approximations of the Bayes Classiﬁer As outlined in the introduction, there are several reasons for approximating f (x|c) in classiﬁcation. [sent-98, score-0.054]
</p><p>24 In block transmission the target density and f systems a tractable density is found for fast computation of the signal classiﬁer (detector) (Kaleh, 1995). [sent-101, score-0.088]
</p><p>25 For the binary hypercube we need in general 2d − 1 parameters to specify each class conditional probability mass function. [sent-104, score-0.199]
</p><p>26 Let f (x|c) be a probability mass function on X such that f (x|c) > 0 for all x ∈ X . [sent-110, score-0.115]
</p><p>27 Let fic =  ∑  x∈X , xi =1  f (x|c),  yic = yic (x) =  2452  xi − fic . [sent-111, score-0.988]
</p><p>28 fic (1 − fic )  (3)  B OUNDS IN M ODEL BASED C LASSIFICATION  Let w = (w1 , w2 , . [sent-112, score-0.43]
</p><p>29 i=1  We set  d  d  i=1  i=1  x f1 (x|c) = ∏ f (xi |c) = ∏ fici (1 − fic )1−xi . [sent-120, score-0.215]
</p><p>30 (4)  Hence f1 (x|c) is another probability mass function, which is positive on {0, 1} d . [sent-121, score-0.115]
</p><p>31 Its marginal distributions coincide with those of f (x|c). [sent-122, score-0.059]
</p><p>32 In fact  ∑  (Uw,c ,Uw∗ ,c ) =  Uw,c (x)Uw∗ ,c (x) f1 (x|c) =  x∈{0,1}d d  =  ∑ ∏ yic (x)w yic (x)w i  ∗ i  f1 (x|c). [sent-127, score-0.378]
</p><p>33 In this case there is at least one i such that wi = w∗ , and for this i we get i ∗  E f1 yic (X)wi yic (X)wi = E f1 [yic (X)] =  E f1 [Xi ] − fic  fic (1 − fic )  = 0,  since by the deﬁnitions above E f1 [Xi ] = 1 · P1 (Xi = 1) = fic . [sent-130, score-1.278]
</p><p>34 If w = w∗ , then we get from (6) that (Uw,c ,Uw,c ) =  ∏  E f1 yic (X)2 . [sent-132, score-0.189]
</p><p>35 i=1:wi =1  Here E f1 yic (X)2 =  1 E f (Xi − fic )2 fic (1 − fic ) 1 2453  E KDAHL AND KOSKI  But since Xi is a binary random variable (or, a Bernoulli random variable) with respect to f 1 , we have 2 E f1 (Xi − fic )2 = fic − fic = fic (1 − fic ) . [sent-133, score-1.909]
</p><p>36 f1 (x|c) w∈{0,1}d This gives us the the (Bahadur-Lazarsfeld) representation of any positive probability mass function f (x|c) on {0, 1}d as f (x|c) = f1 (x|c) fc,interactions (x), (9) where we have written fc,interactions (x) =  ∑  βw,cUw,c (x) . [sent-144, score-0.115]
</p><p>37 The probability mass function f 1 (x|c) in (4) is known as the ﬁrst order term. [sent-147, score-0.115]
</p><p>38 One can deﬁne a family of probability mass functions called kth order Bahadur distributions as the set of all probabilities on the binary hypercube in d dimensions such that β w = 0 for R(w) > k. [sent-150, score-0.228]
</p><p>39 ε, when the conditional distributions of X|C are in the class of kth order Bahadur  If we expand log canonical form  f (x|c) f1 (x|c)  with respect to the basis {Uw,c (x)}w∈{0,1}d we obtain the following f (x|c) = f1 (x|c)e∑w∈{0,1}d  αw,cUw,c (x)  ,  (11)  f (X|c) ·Uw,c (X) . [sent-153, score-0.058]
</p><p>40 f1 (X|c)  (12)  where it follows similarly as above that αw,c = E f1 log  The two canonical forms (10) and (11) above are of interest in the sequel for deﬁning structures of approximations and for evaluating the effect of a plug-in classiﬁer on probability of correct classiﬁcation. [sent-154, score-0.113]
</p><p>41 Here the trade-off is between the additional complexity and the more accurate statistical description, and, as it will turn out in the sequel, higher probability of correct classiﬁcation with the plug-in classiﬁer. [sent-160, score-0.081]
</p><p>42 In some of the contributions referred to in the above the approximating structure is not necessarily a probability, since an arbitrary truncation of a representation of a probability mass function with respect to a basis is not always a probability mass function. [sent-163, score-0.284]
</p><p>43 A suboptimal detector may be introduced, for example, for the purpose of reducing run time complexity, see Barbosa (1989), by a d × N matrix M of the same structure as H, but with a shorter memory and the plug-in classiﬁer cB (x) = arg min Σ−1 (x − Mb) ˆˆ b∈C  2  . [sent-212, score-0.064]
</p><p>44 The difﬁculty with these expressions is to ﬁnd the set {x|cB (x) = cB (x)} and to compute the integrals above. [sent-219, score-0.056]
</p><p>45 ˆ approximating distribution f The result above is the starting point of our development of approximations of probability to ﬁnd plug-in classiﬁers for Bayesian networks and in particular to evaluate Na¨ve Bayes. [sent-235, score-0.129]
</p><p>46 w=0  Example 4 The Kolmogorov distance of variation is very effectively evaluated and bounded for the class of two dimensional densities having an expansion with respect to an orthonormal system of polynomials. [sent-243, score-0.157]
</p><p>47 Let fˆ(x) = f1 (x1 ) f2 (x2 ) be the product of the two Gaussian marginal densities for X1 and X2 . [sent-248, score-0.114]
</p><p>48 The bound on the difference between the probabilities of correct classiﬁcation is seen to be a power series in the absolute value of the coefﬁcient of correlation. [sent-257, score-0.071]
</p><p>49 There R computational routines for the Hermite polynoare | xi − mi |k fi (xi )dxi involved here are explicitly mials, and in addition integrals of the form computable. [sent-258, score-0.119]
</p><p>50 There is an extension of the Mehler expansion for n-variate densities (Slepian, 1972), which could be used in some of the examples below, but we will not expand on this due to the extensive notational machinery thereby required. [sent-259, score-0.082]
</p><p>51 ¡  There are certain well known inequalities between the Kolmogorov distance of variation and other distances or divergences between probability measures. [sent-260, score-0.144]
</p><p>52 These distances are often readily computable in an explicit form, a compendium is recapitulated in Kailath (1967). [sent-261, score-0.065]
</p><p>53 For two probability densities f and fˆ we have the inequality due to Ch. [sent-265, score-0.125]
</p><p>54 f (x) · fˆ(x)dµ(x)  Brown (1959) and Ku and Kullback (1969) developed a convergent iteration that ﬁnds fˆ minimizing D( fˆ, f ) in the class of all densities on discrete X that have some given set of lower order marginals. [sent-278, score-0.114]
</p><p>55 Example 7 In Example 2 above the true and plug-in densities correspond to the distributions N (Hb, Σ) and N (Mb, Σ), respectively. [sent-288, score-0.109]
</p><p>56 Thus we introduce more easily computable bounds in Corollary 8 and Theorem 11. [sent-300, score-0.066]
</p><p>57 If the factor of P(c|x) that is not approximated is not functionally dependent of the factor that is approximated, we can develop sharper bounds on the degradation of the approximated classiﬁer performance. [sent-302, score-0.111]
</p><p>58 f (x|c) denotes a generic target probability mass function and G is a directed acyclic graph. [sent-310, score-0.115]
</p><p>59 Then fˆ(x|c) is an approximating probability obtained by taking fˆ(xi |πi , c, G) = f (xi |πi , c), (26) and multiplying fˆ(xi |πi , c, G) according to (25). [sent-312, score-0.097]
</p><p>60 The best approximating G (in some family of directed acyclic graphs) is found by maximizing  ∑  x∈X  d  f (x|c) ∑ log i=1  f (xi , πi |c) , f (xi |c) f (πi |c)  which is shown to minimize D f , fˆ . [sent-313, score-0.054]
</p><p>61 (27)  i=2  In other words this is a joint probability factorized along a rooted tree. [sent-322, score-0.069]
</p><p>62 i=1 i=1 We designate by f (si |G) the class conditional density of all s. [sent-334, score-0.075]
</p><p>63 Given a Bayesian network (G, P) and P, the partition S is deﬁned for a class conditional density as follows: • Xi ∈ S1 if f (xi |πi , c) = fˆ(xi |πi , c). [sent-348, score-0.107]
</p><p>64 • Xi ∈ S2 if for all xi , πi we have f (xi |πi , c) = fˆ(xi |πi , c) and for all j = i such that X j ∈ S1 we have Xi ∈ π j . [sent-349, score-0.09]
</p><p>65 • Xi ∈ S3 if for all xi , πi , we have f (xi |πi , c) = fˆ(xi |πi , c), there exists j = i such that X j ∈ S1 and Xi ∈ π j . [sent-350, score-0.09]
</p><p>66 We approximate the joint density in (27) by the product of marginal densities. [sent-354, score-0.076]
</p><p>67 Next we extend the result in Theorem 5, (15) by specifying the difference in probability of correct classiﬁcation in terms of the partial structure speciﬁc difference through combining Theorem 7 and Theorem 5. [sent-382, score-0.081]
</p><p>68 ˆ  S3  The following examples demonstrate computable expressions for this bound. [sent-387, score-0.067]
</p><p>69 We have from (27) d  f (s1 |c, G) = ∏ f (xi |xπ(i) , c, G), i=2  which is a probability mass function on S1 and d  fˆ (s1 |c, G) = ∏ f (xi |c, G), i=2  which is a probability mass function on S1 . [sent-392, score-0.23]
</p><p>70 Then in view of (19) and (20) we compute D f (s1 |c, G), fˆ (s1 |c, G) = d  =∑  ∑  i=2 xi ,xπ(i)  ∑  s1 ∈S1  f xi , xπ(i) |c, G log  f (s1 |c, G) log  f (s1 |c, G) fˆ (s1 |c, G)  f xi , xπ(i) |c, G  f (xi |c, G) · f xπ(i) |c, G  . [sent-393, score-0.27]
</p><p>71 Here we recognize, see Cover and Thomas (1991), the mutual informations Ic,G xi , xπ(i) between xi and xπ(i) so that the expression in the right hand side of the preceding equation equals d  = ∑ Ic,G xi , xπ(i) . [sent-394, score-0.295]
</p><p>72 Chow and Liu (1968) developed an algorithm for ﬁnding the tree from data that maximizes sum of the mutual informations between a variable and its parents shown above. [sent-396, score-0.064]
</p><p>73 Example 13 The conditionally Gaussian regressions are useful probability models for Bayesian networks with both continuous and discrete variables, see Lauritzen (1990). [sent-397, score-0.075]
</p><p>74 In order not to overburden the notation we omit here the dependence on c in the expressions below. [sent-399, score-0.102]
</p><p>75 We illustrate, however, the simplest of the upper bounds with the Kullback-Leibler divergence by taking the plug-in distribution ˆ Nr A + B(π )πγ , Σ(π ) . [sent-403, score-0.059]
</p><p>76 i=1  In order not to overburden the notation we avoid symbols like f X i (xi |c) for marginal densities. [sent-424, score-0.057]
</p><p>77 In spite of this we are not restricted to the case where all marginal densities are identical. [sent-425, score-0.114]
</p><p>78 Then the Kolmogorov variation distance is f (X|C) 1 −1 , E fˆ,P ˆ 2 fˆ(X|C) where now fˆ(X|C) is as in Deﬁnition 12. [sent-446, score-0.075]
</p><p>79 The Kolmogorov variation distance measuring the degree of association between X and C is in Vilmansen (1971, 1973) deﬁned as K (X,C) =  1 k ∑ ∑ | f (x, c) − f (x)P(c)|. [sent-451, score-0.075]
</p><p>80 The maximum of K (X,C) in (32) is obtained, as soon as there is a functional dependence between X and C in the sense that the supports of the class-conditional densities are disjoint subsets of X . [sent-454, score-0.132]
</p><p>81 Hence, in the last mentioned case the probability of correct classiﬁcation is one. [sent-455, score-0.081]
</p><p>82 This extreme case is approached when the dependence between X and C is “very close“ to being functional, in the sense that each f (x|c) is concentrated around its mode, say m c , so that observation of mc is an almost noise-free message, as it were, from the source c. [sent-456, score-0.25]
</p><p>83 ı The following theorem shows in a more precise fashion how the modes of the densities f (x|c) control the difference between the pertinent probabilities of the correct decision. [sent-460, score-0.196]
</p><p>84 In words the result in Theorem 13 below tells that, if the class conditional probability densities are predominantly well concentrated, the Kolmogorov variation distance with respect to Na¨ve Bayes is small. [sent-462, score-0.231]
</p><p>85 By (34), f (m j ) f (m) we obtain f (x j )  ∑  xi =m j  f (xi ) = 1 − f (m j )  1 − f (m). [sent-485, score-0.09]
</p><p>86 It is possible to increase f (m) dividing for example multimodal class conditional densities into many unimodal densities (Vilata and Rish, 2003), but a theoretical investigation of identiﬁcation and interpretation of such a partition scheme is lacking. [sent-495, score-0.195]
</p><p>87 From the three to ﬁve dimensional cases in Figures 5 and 6 we see that the inequality (36) is often sharp enough, if the probability density is concentrated, that is f (m) is close to 1. [sent-523, score-0.087]
</p><p>88 in (36) simulated max difference maxx∈X f (x) − ∏d f (xi ) i=1  1  PSfrag replacements  ) (d = 3) from above 0. [sent-529, score-0.073]
</p><p>89 5  PSfrag replacements  3  2  PSfrag replacements  2  1. [sent-546, score-0.092]
</p><p>90 Which features have this dependence is, however, unknown and detection is complicated since 2473  E KDAHL AND KOSKI  Xi  Xi+1  Xi+2 Figure 7: Graphical representation of dependence P(there exists at least one j = i such that(X j , . [sent-585, score-0.1]
</p><p>91 Here we present sufﬁcient conditions for ˜ ˜ this pointwise separation between classes so that the probability of correct classiﬁcation does not ˆ ˆ decrease by plugging in fˆ(x|c)P(c). [sent-595, score-0.081]
</p><p>92 The question is, how close must fˆ(x|c)P(c) be to f (x|c)P(c), so that there should be no decrease in the probability of correct classiﬁcation. [sent-596, score-0.081]
</p><p>93 Lemma 17 used to state sufﬁcient conditions such that f (x|c) can be approximated without affecting the probability of correct classiﬁcation. [sent-611, score-0.108]
</p><p>94 Summary We have presented exact and easily computable bounds for the degradation of probability of correct classiﬁcation when Bayes classiﬁers are used with respect to partial plug-in conditional densities in a Bayesian network model (Theorem 7, Corollary 8 and Theorem 11). [sent-622, score-0.323]
</p><p>95 Estimation of discrete multivariate densities for computer-aided differential diagnosis. [sent-692, score-0.141]
</p><p>96 The computational complexity of probabilistic inference using bayesian Belief networks. [sent-708, score-0.085]
</p><p>97 On the optimality of the simple bayesian classiﬁer under zero-one loss. [sent-741, score-0.085]
</p><p>98 The divergence and Bhattacharyya distance measures in signal selection. [sent-827, score-0.064]
</p><p>99 Gr obner bases and factorisation in discrete probability and Bayes. [sent-897, score-0.075]
</p><p>100 Bayes risk consistency of classiﬁcation procedures using density estimation. [sent-918, score-0.07]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cb', 0.613), ('na', 0.256), ('fic', 0.215), ('maxx', 0.214), ('koski', 0.203), ('kdahl', 0.202), ('yic', 0.189), ('mc', 0.172), ('bayes', 0.16), ('ounds', 0.132), ('ve', 0.115), ('odel', 0.109), ('kolmogorov', 0.105), ('bahadur', 0.101), ('lassification', 0.098), ('classi', 0.093), ('xi', 0.09), ('bayesian', 0.085), ('densities', 0.082), ('er', 0.072), ('mass', 0.072), ('rish', 0.064), ('ekdahl', 0.063), ('hb', 0.058), ('approximating', 0.054), ('chow', 0.053), ('hypercube', 0.053), ('vilmansen', 0.05), ('dependence', 0.05), ('psfrag', 0.049), ('hn', 0.049), ('bounding', 0.048), ('replacements', 0.046), ('density', 0.044), ('variation', 0.044), ('theorem', 0.043), ('probability', 0.043), ('mb', 0.041), ('pe', 0.041), ('computable', 0.04), ('wi', 0.04), ('parents', 0.039), ('detector', 0.038), ('hl', 0.038), ('correct', 0.038), ('cerm', 0.038), ('glick', 0.038), ('hermite', 0.038), ('maxdi', 0.038), ('mehler', 0.038), ('ott', 0.038), ('liu', 0.036), ('atoms', 0.035), ('probabilities', 0.033), ('hoeffding', 0.033), ('corollary', 0.033), ('divergence', 0.033), ('approximations', 0.032), ('ku', 0.032), ('swedish', 0.032), ('network', 0.032), ('discrete', 0.032), ('marginal', 0.032), ('conditional', 0.031), ('distance', 0.031), ('bhattacharyya', 0.031), ('kullback', 0.031), ('degradation', 0.031), ('integrals', 0.029), ('nr', 0.029), ('concentrated', 0.028), ('wainwright', 0.028), ('multivariate', 0.027), ('simulated', 0.027), ('distributions', 0.027), ('expressions', 0.027), ('independence', 0.027), ('approximated', 0.027), ('channel', 0.026), ('divergences', 0.026), ('rooted', 0.026), ('procedures', 0.026), ('bounds', 0.026), ('suboptimal', 0.026), ('anoulova', 0.025), ('barbosa', 0.025), ('brunk', 0.025), ('csi', 0.025), ('dichotomous', 0.025), ('gyllenberg', 0.025), ('informations', 0.025), ('isi', 0.025), ('kaleh', 0.025), ('korb', 0.025), ('kronmal', 0.025), ('overburden', 0.025), ('pings', 0.025), ('pistone', 0.025), ('recapitulated', 0.025), ('ryzin', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="17-tfidf-1" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>Author: Magnus Ekdahl, Timo Koski</p><p>Abstract: In many pattern recognition/classiﬁcation problem the true class conditional model and class probabilities are approximated for reasons of reducing complexity and/or of statistical estimation. The approximated classiﬁer is expected to have worse performance, here measured by the probability of correct classiﬁcation. We present an analysis valid in general, and easily computable formulas for estimating the degradation in probability of correct classiﬁcation when compared to the optimal classiﬁer. An example of an approximation is the Na¨ve Bayes classiﬁer. We show that the perforı mance of the Na¨ve Bayes depends on the degree of functional dependence between the features ı and labels. We provide a sufﬁcient condition for zero loss of performance, too. Keywords: Bayesian networks, na¨ve Bayes, plug-in classiﬁer, Kolmogorov distance of variation, ı variational learning</p><p>2 0.066308796 <a title="17-tfidf-2" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>Author: Pieter Abbeel, Daphne Koller, Andrew Y. Ng</p><p>Abstract: We study the computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Importantly, unlike standard maximum likelihood estimation algorithms, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. In addition to our main result, we show that the sample complexity of parameter learning in graphical models has an O(1) dependence on the number of variables in the model when using the KL-divergence normalized by the number of variables as the performance criterion.1 Keywords: probabilistic graphical models, parameter and structure learning, factor graphs, Markov networks, Bayesian networks</p><p>3 0.065499634 <a title="17-tfidf-3" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>Author: Kazuho Watanabe, Sumio Watanabe</p><p>Abstract: Bayesian learning has been widely used and proved to be effective in many data modeling problems. However, computations involved in it require huge costs and generally cannot be performed exactly. The variational Bayesian approach, proposed as an approximation of Bayesian learning, has provided computational tractability and good generalization performance in many applications. The properties and capabilities of variational Bayesian learning itself have not been clariﬁed yet. It is still unknown how good approximation the variational Bayesian approach can achieve. In this paper, we discuss variational Bayesian learning of Gaussian mixture models and derive upper and lower bounds of variational stochastic complexities. The variational stochastic complexity, which corresponds to the minimum variational free energy and a lower bound of the Bayesian evidence, not only becomes important in addressing the model selection problem, but also enables us to discuss the accuracy of the variational Bayesian approach as an approximation of true Bayesian learning. Keywords: Gaussian mixture model, variational Bayesian learning, stochastic complexity</p><p>4 0.061690446 <a title="17-tfidf-4" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>Author: Luis M. de Campos</p><p>Abstract: We propose a new scoring function for learning Bayesian networks from data using score+search algorithms. This is based on the concept of mutual information and exploits some well-known properties of this measure in a novel way. Essentially, a statistical independence test based on the chi-square distribution, associated with the mutual information measure, together with a property of additive decomposition of this measure, are combined in order to measure the degree of interaction between each variable and its parent variables in the network. The result is a non-Bayesian scoring function called MIT (mutual information tests) which belongs to the family of scores based on information theory. The MIT score also represents a penalization of the Kullback-Leibler divergence between the joint probability distributions associated with a candidate network and with the available data set. Detailed results of a complete experimental evaluation of the proposed scoring function and its comparison with the well-known K2, BDeu and BIC/MDL scores are also presented. Keywords: Bayesian networks, scoring functions, learning, mutual information, conditional independence tests</p><p>5 0.056336291 <a title="17-tfidf-5" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>Author: Mikio L. Braun</p><p>Abstract: The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to inﬁnity. We derive probabilistic ﬁnite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reﬂecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a signiﬁcant improvement over existing non-scaling bounds. Keywords: kernel matrix, eigenvalues, relative perturbation bounds</p><p>6 0.056213778 <a title="17-tfidf-6" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>7 0.055832852 <a title="17-tfidf-7" href="./jmlr-2006-Lower_Bounds_and_Aggregation_in_Density_Estimation.html">58 jmlr-2006-Lower Bounds and Aggregation in Density Estimation</a></p>
<p>8 0.050950922 <a title="17-tfidf-8" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>9 0.0495001 <a title="17-tfidf-9" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>10 0.047897961 <a title="17-tfidf-10" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.046615273 <a title="17-tfidf-11" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<p>12 0.04516257 <a title="17-tfidf-12" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.042102054 <a title="17-tfidf-13" href="./jmlr-2006-Learning_Minimum_Volume_Sets.html">48 jmlr-2006-Learning Minimum Volume Sets</a></p>
<p>14 0.041443348 <a title="17-tfidf-14" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.039656915 <a title="17-tfidf-15" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>16 0.038808651 <a title="17-tfidf-16" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>17 0.038416799 <a title="17-tfidf-17" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>18 0.038389809 <a title="17-tfidf-18" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>19 0.036272958 <a title="17-tfidf-19" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.035862055 <a title="17-tfidf-20" href="./jmlr-2006-Nonparametric_Quantile_Estimation.html">65 jmlr-2006-Nonparametric Quantile Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.193), (1, -0.044), (2, -0.106), (3, -0.058), (4, -0.129), (5, 0.027), (6, -0.033), (7, -0.075), (8, 0.056), (9, -0.075), (10, 0.058), (11, 0.035), (12, 0.055), (13, 0.021), (14, 0.1), (15, 0.009), (16, -0.142), (17, -0.064), (18, -0.076), (19, 0.066), (20, -0.031), (21, -0.121), (22, 0.166), (23, -0.205), (24, -0.028), (25, 0.064), (26, -0.244), (27, -0.018), (28, -0.073), (29, -0.113), (30, 0.054), (31, -0.066), (32, 0.006), (33, 0.167), (34, 0.053), (35, -0.104), (36, -0.023), (37, 0.064), (38, 0.279), (39, 0.063), (40, -0.081), (41, 0.046), (42, 0.007), (43, 0.147), (44, 0.057), (45, -0.24), (46, 0.012), (47, -0.032), (48, 0.151), (49, -0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93701464 <a title="17-lsi-1" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>Author: Magnus Ekdahl, Timo Koski</p><p>Abstract: In many pattern recognition/classiﬁcation problem the true class conditional model and class probabilities are approximated for reasons of reducing complexity and/or of statistical estimation. The approximated classiﬁer is expected to have worse performance, here measured by the probability of correct classiﬁcation. We present an analysis valid in general, and easily computable formulas for estimating the degradation in probability of correct classiﬁcation when compared to the optimal classiﬁer. An example of an approximation is the Na¨ve Bayes classiﬁer. We show that the perforı mance of the Na¨ve Bayes depends on the degree of functional dependence between the features ı and labels. We provide a sufﬁcient condition for zero loss of performance, too. Keywords: Bayesian networks, na¨ve Bayes, plug-in classiﬁer, Kolmogorov distance of variation, ı variational learning</p><p>2 0.55583787 <a title="17-lsi-2" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>Author: Pieter Abbeel, Daphne Koller, Andrew Y. Ng</p><p>Abstract: We study the computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Importantly, unlike standard maximum likelihood estimation algorithms, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. In addition to our main result, we show that the sample complexity of parameter learning in graphical models has an O(1) dependence on the number of variables in the model when using the KL-divergence normalized by the number of variables as the performance criterion.1 Keywords: probabilistic graphical models, parameter and structure learning, factor graphs, Markov networks, Bayesian networks</p><p>3 0.42245144 <a title="17-lsi-3" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>Author: Luis M. de Campos</p><p>Abstract: We propose a new scoring function for learning Bayesian networks from data using score+search algorithms. This is based on the concept of mutual information and exploits some well-known properties of this measure in a novel way. Essentially, a statistical independence test based on the chi-square distribution, associated with the mutual information measure, together with a property of additive decomposition of this measure, are combined in order to measure the degree of interaction between each variable and its parent variables in the network. The result is a non-Bayesian scoring function called MIT (mutual information tests) which belongs to the family of scores based on information theory. The MIT score also represents a penalization of the Kullback-Leibler divergence between the joint probability distributions associated with a candidate network and with the available data set. Detailed results of a complete experimental evaluation of the proposed scoring function and its comparison with the well-known K2, BDeu and BIC/MDL scores are also presented. Keywords: Bayesian networks, scoring functions, learning, mutual information, conditional independence tests</p><p>4 0.3539947 <a title="17-lsi-4" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>Author: Janez Demšar</p><p>Abstract: While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classiﬁers: the Wilcoxon signed ranks test for comparison of two classiﬁers and the Friedman test with the corresponding post-hoc tests for comparison of more classiﬁers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams. Keywords: comparative studies, statistical methods, Wilcoxon signed ranks test, Friedman test, multiple comparisons tests</p><p>5 0.33422327 <a title="17-lsi-5" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pannagadatta K. Shivaswamy, Chiranjib Bhattacharyya, Alexander J. Smola</p><p>Abstract: We propose a novel second order cone programming formulation for designing robust classiﬁers which can handle uncertainty in observations. Similar formulations are also derived for designing regression functions which are robust to uncertainties in the regression setting. The proposed formulations are independent of the underlying distribution, requiring only the existence of second order moments. These formulations are then specialized to the case of missing values in observations for both classiﬁcation and regression problems. Experiments show that the proposed formulations outperform imputation.</p><p>6 0.30768147 <a title="17-lsi-6" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>7 0.29788792 <a title="17-lsi-7" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.27731499 <a title="17-lsi-8" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>9 0.27256399 <a title="17-lsi-9" href="./jmlr-2006-Learning_Minimum_Volume_Sets.html">48 jmlr-2006-Learning Minimum Volume Sets</a></p>
<p>10 0.27158678 <a title="17-lsi-10" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<p>11 0.26918563 <a title="17-lsi-11" href="./jmlr-2006-Lower_Bounds_and_Aggregation_in_Density_Estimation.html">58 jmlr-2006-Lower Bounds and Aggregation in Density Estimation</a></p>
<p>12 0.26683855 <a title="17-lsi-12" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.25136262 <a title="17-lsi-13" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>14 0.24823712 <a title="17-lsi-14" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>15 0.24653503 <a title="17-lsi-15" href="./jmlr-2006-Point-Based_Value_Iteration_for_Continuous_POMDPs.html">74 jmlr-2006-Point-Based Value Iteration for Continuous POMDPs</a></p>
<p>16 0.24352723 <a title="17-lsi-16" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>17 0.24351007 <a title="17-lsi-17" href="./jmlr-2006-Nonparametric_Quantile_Estimation.html">65 jmlr-2006-Nonparametric Quantile Estimation</a></p>
<p>18 0.23690151 <a title="17-lsi-18" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.22923107 <a title="17-lsi-19" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>20 0.2281 <a title="17-lsi-20" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.021), (36, 0.096), (45, 0.036), (50, 0.086), (63, 0.036), (68, 0.018), (76, 0.024), (78, 0.022), (79, 0.013), (81, 0.046), (84, 0.021), (90, 0.04), (91, 0.023), (94, 0.353), (96, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70144552 <a title="17-lda-1" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>Author: Magnus Ekdahl, Timo Koski</p><p>Abstract: In many pattern recognition/classiﬁcation problem the true class conditional model and class probabilities are approximated for reasons of reducing complexity and/or of statistical estimation. The approximated classiﬁer is expected to have worse performance, here measured by the probability of correct classiﬁcation. We present an analysis valid in general, and easily computable formulas for estimating the degradation in probability of correct classiﬁcation when compared to the optimal classiﬁer. An example of an approximation is the Na¨ve Bayes classiﬁer. We show that the perforı mance of the Na¨ve Bayes depends on the degree of functional dependence between the features ı and labels. We provide a sufﬁcient condition for zero loss of performance, too. Keywords: Bayesian networks, na¨ve Bayes, plug-in classiﬁer, Kolmogorov distance of variation, ı variational learning</p><p>2 0.40616947 <a title="17-lda-2" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>Author: Mikio L. Braun</p><p>Abstract: The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to inﬁnity. We derive probabilistic ﬁnite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reﬂecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a signiﬁcant improvement over existing non-scaling bounds. Keywords: kernel matrix, eigenvalues, relative perturbation bounds</p><p>3 0.40397352 <a title="17-lda-3" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>Author: Sayan Mukherjee, Qiang Wu</p><p>Abstract: We introduce an algorithm that simultaneously estimates a classiﬁcation function as well as its gradient in the supervised learning framework. The motivation for the algorithm is to ﬁnd salient variables and estimate how they covary. An efﬁcient implementation with respect to both memory and time is given. The utility of the algorithm is illustrated on simulated data as well as a gene expression data set. An error analysis is given for the convergence of the estimate of the classiﬁcation function and its gradient to the true classiﬁcation function and true gradient. Keywords: Tikhnonov regularization, variable selection, reproducing kernel Hilbert space, generalization bounds, classiﬁcation</p><p>4 0.39788947 <a title="17-lda-4" href="./jmlr-2006-On_Model_Selection_Consistency_of_Lasso.html">66 jmlr-2006-On Model Selection Consistency of Lasso</a></p>
<p>Author: Peng Zhao, Bin Yu</p><p>Abstract: Sparsity or parsimony of statistical models is crucial for their proper interpretations, as in sciences and social sciences. Model selection is a commonly used method to ﬁnd such models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani, 1996) is now being used as a computationally feasible alternative to model selection. Therefore it is important to study Lasso for model selection purposes. In this paper, we prove that a single condition, which we call the Irrepresentable Condition, is almost necessary and sufﬁcient for Lasso to select the true model both in the classical ﬁxed p setting and in the large p setting as the sample size n gets large. Based on these results, sufﬁcient conditions that are veriﬁable in practice are given to relate to previous works and help applications of Lasso for feature selection and sparse representation. This Irrepresentable Condition, which depends mainly on the covariance of the predictor variables, states that Lasso selects the true model consistently if and (almost) only if the predictors that are not in the true model are “irrepresentable” (in a sense to be clariﬁed) by predictors that are in the true model. Furthermore, simulations are carried out to provide insights and understanding of this result. Keywords: Lasso, regularization, sparsity, model selection, consistency</p><p>5 0.38932657 <a title="17-lda-5" href="./jmlr-2006-Pattern_Recognition_for__Conditionally_Independent_Data.html">73 jmlr-2006-Pattern Recognition for  Conditionally Independent Data</a></p>
<p>Author: Daniil Ryabko</p><p>Abstract: In this work we consider the task of relaxing the i.i.d. assumption in pattern recognition (or classiﬁcation), aiming to make existing learning algorithms applicable to a wider range of tasks. Pattern recognition is guessing a discrete label of some object based on a set of given examples (pairs of objects and labels). We consider the case of deterministically deﬁned labels. Traditionally, this task is studied under the assumption that examples are independent and identically distributed. However, it turns out that many results of pattern recognition theory carry over a weaker assumption. Namely, under the assumption of conditional independence and identical distribution of objects, while the only assumption on the distribution of labels is that the rate of occurrence of each label should be above some positive threshold. We ﬁnd a broad class of learning algorithms for which estimations of the probability of the classiﬁcation error achieved under the classical i.i.d. assumption can be generalized to the similar estimates for case of conditionally i.i.d. examples.</p><p>6 0.38697618 <a title="17-lda-6" href="./jmlr-2006-Learning_Minimum_Volume_Sets.html">48 jmlr-2006-Learning Minimum Volume Sets</a></p>
<p>7 0.38513619 <a title="17-lda-7" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>8 0.38499713 <a title="17-lda-8" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>9 0.38393795 <a title="17-lda-9" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>10 0.38372535 <a title="17-lda-10" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.38338438 <a title="17-lda-11" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>12 0.38301143 <a title="17-lda-12" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>13 0.37995043 <a title="17-lda-13" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>14 0.37923181 <a title="17-lda-14" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>15 0.3785423 <a title="17-lda-15" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>16 0.37801796 <a title="17-lda-16" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>17 0.3766346 <a title="17-lda-17" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>18 0.37559056 <a title="17-lda-18" href="./jmlr-2006-Some_Theory_for_Generalized_Boosting_Algorithms.html">82 jmlr-2006-Some Theory for Generalized Boosting Algorithms</a></p>
<p>19 0.37485346 <a title="17-lda-19" href="./jmlr-2006-Toward_Attribute_Efficient_Learning_of_Decision_Lists_and_Parities.html">92 jmlr-2006-Toward Attribute Efficient Learning of Decision Lists and Parities</a></p>
<p>20 0.37314212 <a title="17-lda-20" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
