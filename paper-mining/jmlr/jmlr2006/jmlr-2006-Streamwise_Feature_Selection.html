<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 jmlr-2006-Streamwise Feature Selection</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-88" href="#">jmlr2006-88</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>88 jmlr-2006-Streamwise Feature Selection</h1>
<br/><p>Source: <a title="jmlr-2006-88-pdf" href="http://jmlr.org/papers/volume7/zhou06a/zhou06a.pdf">pdf</a></p><p>Author: Jing Zhou, Dean P. Foster, Robert A. Stine, Lyle H. Ungar</p><p>Abstract: In streamwise feature selection, new features are sequentially considered for addition to a predictive model. When the space of potential features is large, streamwise feature selection offers many advantages over traditional feature selection methods, which assume that all features are known in advance. Features can be generated dynamically, focusing the search for new features on promising subspaces, and overﬁtting can be controlled by dynamically adjusting the threshold for adding features to the model. In contrast to traditional forward feature selection algorithms such as stepwise regression in which at each step all possible features are evaluated and the best one is selected, streamwise feature selection only evaluates each feature once when it is generated. We describe information-investing and α-investing, two adaptive complexity penalty methods for streamwise feature selection which dynamically adjust the threshold on the error reduction required for adding a new feature. These two methods give false discovery rate style guarantees against overﬁtting. They differ from standard penalty methods such as AIC, BIC and RIC, which always drastically over- or under-ﬁt in the limit of inﬁnite numbers of non-predictive features. Empirical results show that streamwise regression is competitive with (on small data sets) and superior to (on large data sets) much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with millions of potential features. Keywords: classiﬁcation, stepwise regression, multiple regression, feature selection, false discovery rate</p><p>Reference: <a title="jmlr-2006-88-reference" href="../jmlr2006_reference/jmlr-2006-Streamwise_Feature_Selection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Computer and Information Science University of Pennsylvania Philadelphia, PA 19104, USA  Editor: Isabelle Guyon  Abstract In streamwise feature selection, new features are sequentially considered for addition to a predictive model. [sent-12, score-1.063]
</p><p>2 When the space of potential features is large, streamwise feature selection offers many advantages over traditional feature selection methods, which assume that all features are known in advance. [sent-13, score-1.485]
</p><p>3 Features can be generated dynamically, focusing the search for new features on promising subspaces, and overﬁtting can be controlled by dynamically adjusting the threshold for adding features to the model. [sent-14, score-0.53]
</p><p>4 In contrast to traditional forward feature selection algorithms such as stepwise regression in which at each step all possible features are evaluated and the best one is selected, streamwise feature selection only evaluates each feature once when it is generated. [sent-15, score-1.659]
</p><p>5 We describe information-investing and α-investing, two adaptive complexity penalty methods for streamwise feature selection which dynamically adjust the threshold on the error reduction required for adding a new feature. [sent-16, score-0.998]
</p><p>6 Empirical results show that streamwise regression is competitive with (on small data sets) and superior to (on large data sets) much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with millions of potential features. [sent-19, score-1.36]
</p><p>7 Figure 1 gives the basic framework of streamwise feature selection. [sent-32, score-0.804]
</p><p>8 In stepwise regression, there is no order on the features; all features must be known in advance, since all features are evaluated at each iteration and the best feature is added to the model. [sent-36, score-0.751]
</p><p>9 ) In contrast, in streamwise regression, since potential features are tested one by one, they can be generated dynamically. [sent-39, score-0.888]
</p><p>10 By modeling the candidate feature set as a dynamically generated stream, we can handle candidate feature sets of unknown, or even inﬁnite size, since not all potential features need to be generated and tested. [sent-40, score-0.592]
</p><p>11 Traditional regularization and feature selection settings assume that all features are pre-computed and presented to a learner before any feature selection begins. [sent-48, score-0.597]
</p><p>12 The algorithms select features and add these features into regression models. [sent-52, score-0.514]
</p><p>13 {initialize} model = {} //initially no features in model i=1 // index of features while CPU time used < max CPU time do xi ← get next feature() {Is xi a “good” feature? [sent-57, score-0.472]
</p><p>14 } if ﬁt of(xi , model) > threshold then model ← model ∪ xi // add xi to the model decrease threshold else increase threshold end if i ← i+1 end while Figure 1: Algorithm: general framework of streamwise feature selection. [sent-58, score-0.993]
</p><p>15 As feature set size becomes larger, streamwise regression offers signiﬁcant computational savings and higher prediction accuracy. [sent-65, score-0.915]
</p><p>16 If features which are cheaper to collect are placed early in the feature stream, they will be preferentially selected over redundant expensive features later in the stream. [sent-73, score-0.563]
</p><p>17 1863  Z HOU , F OSTER , S TINE AND U NGAR  Alternatively, features can be sorted so as to place potentially higher signal content features earlier in the feature stream, making it easier to discover the useful features. [sent-75, score-0.544]
</p><p>18 A combination of domain knowledge and use of the different sizes of the feature sets can be used to provide a partial order on the features, and thus to take full advantage of streamwise feature selection. [sent-80, score-0.942]
</p><p>19 Many commercial statistical packages offer variants of a greedy method, stepwise feature selection, an iterative procedure in which at each step all features are tested at each iteration, and the single best feature is selected and added to the model. [sent-86, score-0.705]
</p><p>20 Stepwise selection is terminated when either all candidate features have been added, or none of the remaining features lead to increased expected beneﬁt according to some measure, such as a p-value threshold. [sent-88, score-0.491]
</p><p>21 Variants of stepwise selection abound, including forward (adding features deemed helpful), backward (removing features no longer deemed helpful), and mixed methods (alternating between forward and backward). [sent-90, score-0.633]
</p><p>22 We speak of the set of beneﬁcial features in a stream as those which would have improved the prediction accuracy of the model at the time they were considered for addition if all prior beneﬁcial features had been added. [sent-109, score-0.559]
</p><p>23 Using the fact that the log-likelihood of the data given a model gives the number of bits to code the model residual error leads to the criteria for feature selection: accept a new feature xi only if the change in log-likelihood from adding the feature is greater than the penalty F, that is, if 2. [sent-120, score-0.692]
</p><p>24 They thus are expected to perform poorly as m grows larger than n, a situation common in streamwise regression settings. [sent-131, score-0.756]
</p><p>25 RIC can be problematic for streamwise feature selection since RIC requires that we know m in advance, which is often not the case (see Section 3). [sent-139, score-0.863]
</p><p>26 Bonferroni methods are known to be overly stringent (Benjamini and Hochberg, 1995), a problem exacerbated in streamwise feature selection applications when m should technically be chosen to be the largest number of features that might be examined. [sent-144, score-1.083]
</p><p>27 Streamwise feature selection is closer in spirit to an alternate class of feature selection methods that control the false discovery rate (FDR), the fraction of the features that are added to the model that reduce predictive accuracy (Benjamini and Hochberg, 1995). [sent-146, score-0.743]
</p><p>28 Interleaving Feature Generation and Testing In streamwise feature selection, candidate features are sequentially presented to the modeling code for potential inclusion in the model. [sent-150, score-1.098]
</p><p>29 ) We refer to the interaction terms as generated features; they are examples of a more general class of features formed from transformations of the original features (square root, log, etc. [sent-161, score-0.475]
</p><p>30 ) Statistical relational learning (SRL) methods can easily generate millions of potentially predictive features as they “crawl” through a database or other relational structure and generate features by building increasingly complex compound relations or SQL queries (Popescul and Ungar, 2004). [sent-167, score-0.526]
</p><p>31 Both stepwise regression and standard shrinkage methods require knowing all features in advance, and are poorly suited for the feature sets generated by SRL. [sent-169, score-0.599]
</p><p>32 Since stepwise regression tests all features for inclusion at each iteration, it is computational infeasible on large data sets. [sent-170, score-0.477]
</p><p>33 Some other strategy such as streamwise feature selection is required. [sent-172, score-0.863]
</p><p>34 One cannot use the coefﬁcients of the features that were not added to the model, since streamwise regression does not include the cost of coding these coefﬁcients, and so this would lead to overﬁtting. [sent-174, score-1.105]
</p><p>35 The main feature stream used in streamwise regression is dynamically constructed by taking the next feature from the sub stream which has had the highest success in having features accepted. [sent-182, score-1.475]
</p><p>36 To assure that all streams are eventually tried, one can use a score for each stream deﬁned as (number of features selected + a)/(number of features tried + b). [sent-184, score-0.541]
</p><p>37 We ﬁrst present streamwise regression in an information-investing setting. [sent-189, score-0.756]
</p><p>38 Therefore, bits saved is the decrease in the bits required to code the model error minus the increase in the bits required to code the model. [sent-196, score-0.47]
</p><p>39 Streamwise regression with information-investing consists of three components: • Wealth Updating: a method for adaptively adjusting the bits available to code the features which will not be added to the model. [sent-205, score-0.492]
</p><p>40 {initialize} model = {} //initially no features in model i=1 // index of features // initial bits available for coding w1 = W0 while CPU time used < max CPU time do xi ← get next feature() // select bid amount εi ← wi /2i {see Section 4. [sent-212, score-0.786]
</p><p>41 3 for the calculation of bits saved} if bits saved(xi , εi , model) > WΔ then model ← model ∪ xi // add xi to the model wi+1 ← wi +WΔ // increase wealth else wi+1 ← wi − εi // reduce wealth end if i ← i+1 end while Figure 2: Algorithm: streamwise regression using information-investing. [sent-213, score-1.531]
</p><p>42 2 Bid Selection The selection of εi as wi /2i gives the slowest possible decrease in wealth such that all wealth is used; that is, so that as many features as possible are included in the model without systematically overﬁtting. [sent-224, score-0.702]
</p><p>43 Therefore, the probability of adding a feature is 1 − e−ε = 1 − (1 − ε + O(ε2 )) ≈ ε, and the cost in bits of coding the fact the feature is added is roughly − log(ε) bits. [sent-237, score-0.579]
</p><p>44 It can be generalized to add WΔ bits to the wealth each time a feature is added to the model. [sent-268, score-0.486]
</p><p>45 Streamwise Regression using Alpha-investing One can deﬁne an alternate form of streamwise regression, α-investing (Zhou et al. [sent-271, score-0.666]
</p><p>46 Of the three components of streamwise regression using information-investing, in α-investing, wealth updating is similar, bid selection is identical, and feature coding is not required. [sent-274, score-1.304]
</p><p>47 The two different streamwise regression algorithms are asymptotically identical (the wealth update of αΔ − αi approaches the update of WΔ as αi becomes small), but differ slightly when the initial features in the stream are considered. [sent-275, score-1.233]
</p><p>48 {initialize} model = {} //initially no features in model i=1 // index of features // initial prob. [sent-280, score-0.472]
</p><p>49 } if get p-value(xi , model) < αi then model ← model ∪ xi // add xi to the model wi+1 ← wi + αΔ − αi // increase wealth else wi+1 ← wi − αi // reduce wealth end if i ← i+1 end while Figure 4: Algorithm: streamwise regression with α-investing. [sent-282, score-1.299]
</p><p>50 The idea of α-investing is to adaptively control the threshold for adding features so that when new (probably predictive) features are added to the model, one “invests” α increasing the wealth, raising the threshold, and allowing a slightly higher future chance of incorrect inclusion of features. [sent-294, score-0.527]
</p><p>51 Experimental Evaluation We compared streamwise feature selection using α-investing against both streamwise and stepwise feature selection (see Section 2) using the AIC, BIC and RIC penalties on a battery of synthetic and real data sets. [sent-301, score-1.913]
</p><p>52 The artiﬁcially simple structure of the data (the features are uncorrelated and have relatively strong signal) allows us to easily see which feature selection methods are adding spurious features or failing to ﬁnd features that should be in the model. [sent-311, score-0.918]
</p><p>53 As one would also expect, if all of the beneﬁcial features in the model occur at the beginning of the stream, α-investing does better, giving the same error as RIC, while if all of the beneﬁcial features in the model are last, α-investing does (two times) worse than RIC. [sent-316, score-0.472]
</p><p>54 Stepwise regression gave noticeably better results than streamwise regression for this problem when the penalty is AIC or BIC. [sent-318, score-0.874]
</p><p>55 Stepwise regression with RIC gave the same error of its streamwise counterpart. [sent-320, score-0.756]
</p><p>56 However, using standard code from R, the stepwise regression was much slower than streamwise regression. [sent-321, score-0.951]
</p><p>57 One might hope that adding more spurious features to the end of a feature stream would not severely harm an algorithm’s performance. [sent-323, score-0.552]
</p><p>58 6 However, AIC and BIC, since their penalty is not a function of m, will add even more spurious features (if they haven’t already added a feature for every observation! [sent-324, score-0.497]
</p><p>59 1873  Z HOU , F OSTER , S TINE AND U NGAR  streamwise  AIC  BIC  RIC  α-invest. [sent-333, score-0.666]
</p><p>60 Table 6 shows that when the number of potential features goes up to 1,000,000, RIC puts in one less beneﬁcial feature, while streamwise regression puts the same four beneﬁcial features plus a half of a spurious feature. [sent-389, score-1.252]
</p><p>61 Thus, streamwise regression is able to ﬁnd the extra feature even when the feature is way out in the 1,000,000 features. [sent-390, score-1.032]
</p><p>62 Since the gene expression data sets have large feature sets, we shufﬂed their original features ﬁve times (in addition to the cross validations), applied streamwise regression on each feature 1874  S TREAMWISE F EATURE S ELECTION  m RIC RIC RIC α invest. [sent-401, score-1.271]
</p><p>63 features, m nominal features continuous features observations, n baseline accuracy  cleve 13 7 6 296 54%  internet 1558 1555 3 2359 84%  ionosphere 34 0 34 351 64%  spect 22 22 0 267 79%  wdbc 30 0 30 569 63%  wpbc 33 0 33 194 76%  Table 7: Description of the UCI data sets. [sent-434, score-0.487]
</p><p>64 The second interleaved feature selection and generation, initially testing PCA components and the original features, and then generating interaction terms between any of the features which had been selected and any of the original features. [sent-440, score-0.485]
</p><p>65 On UCI data sets (Figure 5)7 , when only the original feature set is used, paired two-sample t-tests show that α-investing has better performance than streamwise AIC and BIC only on two of the six UCI data sets: the internet and wpbc data sets. [sent-446, score-0.855]
</p><p>66 On the other data sets, which have relatively few features, the less stringent penalties do as well as or better than streamwise regression. [sent-447, score-0.702]
</p><p>67 When interaction terms and PCA components are included, α-investing gives better performance than streamwise AIC on ﬁve data sets, than streamwise BIC on three data sets, and than streamwise RIC on two data sets. [sent-448, score-2.064]
</p><p>68 On the UCI data sets (Figure 5), we also compared streamwise regression with α-investing8 with stepwise regression. [sent-451, score-0.924]
</p><p>69 We did not tune any parameters in streamwise regression to particular problems either. [sent-460, score-0.756]
</p><p>70 On the other data sets, streamwise regression doesn’t have 7. [sent-462, score-0.756]
</p><p>71 We were unable to compute the stepwise regression results on the internet data set using the software at hand when interaction terms and PCA components were included giving millions of potential features with thousands of observations. [sent-466, score-0.588]
</p><p>72 These tests shows that the performance of streamwise regression is at least comparable to those of SVM, NNET, and TREE. [sent-481, score-0.772]
</p><p>73 But when interaction terms and PCA components are included , RIC is often too conservative to select even only one feature, whereas α-investing has stable performance and the t-tests show that α-investing has signiﬁcant better prediction accuracies than streamwise RIC on 5 out of 7 data sets. [sent-483, score-0.801]
</p><p>74 Note that, regardless of whether or not interaction terms and PCA components are included, α-investing always has much higher accuracy than streamwise AIC and BIC. [sent-484, score-0.732]
</p><p>75 Streamwise RIC has similar SEs as α-investing has, but streamwise AIC and BIC usually have one percent higher SEs than α-investing and streamwise RIC. [sent-488, score-1.332]
</p><p>76 Also note that, for streamwise AIC, BIC, and RIC, adding interaction terms and PCA components often hurts. [sent-493, score-0.773]
</p><p>77 10 Streamwise and stepwise feature selection are one step less greedy, sequentially adding features by computing I(y; xi |Model). [sent-514, score-0.628]
</p><p>78 This richer feature set of transformed variables could, of course, be used within the streamwise feature selection setting, or streamwise regression could be used to ﬁnd an initial set of features to be provided to the Bayesian model. [sent-518, score-1.96]
</p><p>79 When SVM is used on the features selected by streamwise regression, the errors on arcene, gisette, and madelon are reduced further to 0. [sent-524, score-0.935]
</p><p>80 There is only one data set, madelon, where streamwise regression gives substantially higher error than the other methods. [sent-529, score-0.756]
</p><p>81 This may be partly due to the fact that madelon has substantially more observations than features, thus making streamwise regression (when not augmented with sophisticated feature generators) less competitive with more complex models. [sent-530, score-0.965]
</p><p>82 Discussion: Statistical Feature Selection Recent developments in statistical feature selection take into account the size of the feature space, but only allow for ﬁnite, ﬁxed feature spaces, and do not support streamwise feature selection. [sent-533, score-1.277]
</p><p>83 The black-dot and solid black bars are the average accuracies using streamwise regressions. [sent-553, score-0.695]
</p><p>84 0 Table 10: Comparison of streamwise regression and other methods on UCI Data. [sent-612, score-0.756]
</p><p>85 The number in parentheses is the average number of features used by the streamwise regression, and these features includes PCA components, raw features, and interaction terms (see Footnote 7 for the details of this kind of feature stream). [sent-615, score-1.295]
</p><p>86 features greatest-hits-one error greatest-hits-one features BayesNN-DFT error BayesNN-DFT features  arcene 0. [sent-621, score-0.636]
</p><p>87 The black-dot bars are the average accuracies using streamwise regressions on raw features. [sent-641, score-0.73]
</p><p>88 The solid black bars are the average accuracy using streamwise regressions with PCA components, raw features, and interaction terms (see Footnote 7 for the details of this kind of feature stream). [sent-642, score-0.889]
</p><p>89 The Benjamini-Hochberg method for controlling the FDR suggests the α-investing rule used in streamwise regression, which keeps the FDR below α: order the p-values of the independents tests of H1 , H2 , . [sent-658, score-0.682]
</p><p>90 The α-investing rule of streamwise regression controls a similar characteristic. [sent-671, score-0.756]
</p><p>91 As a streamwise feature selector, if one has added, say, 20 features to the model, then with high probability (tending to 1 as the number of accepted features grows) no more than 5% (that is, one feature in the case of 20 features) are false positives. [sent-674, score-1.385]
</p><p>92 Algorithms such as the streamwise regression presented in this paper, which are online in the features being used are much less common. [sent-677, score-0.959]
</p><p>93 In such cases, regularization or smoothing methods work well and streamwise feature selection does not make sense. [sent-679, score-0.863]
</p><p>94 For the problems in citation prediction and bankruptcy prediction that we have looked at, generating potential features (for example, by querying a database or by computing transformations or combinations of the raw features) takes far more time than the streamwise feature selection. [sent-683, score-1.122]
</p><p>95 Thus, the ﬂexibility that streamwise regression provides to dynamically decide which features to generate and add to the feature stream provides potentially large savings in computation. [sent-684, score-1.256]
</p><p>96 1882  S TREAMWISE F EATURE S ELECTION  Empirical tests show that for the smaller UCI data sets where stepwise regression can be done, streamwise regression gives comparable results to stepwise regression or techniques such as decision trees, neural networks, or SVMs. [sent-685, score-1.288]
</p><p>97 Fortunately, given any software which incrementally considers features for addition and calculates their p-value or entropy reduction, streamwise regression using information-investing or α-investing is extremely easy to implement. [sent-688, score-0.959]
</p><p>98 For linear and logistic regression, we have found that streamwise regression can easily handle millions of features. [sent-689, score-0.779]
</p><p>99 The results presented here show that streamwise feature selection is highly competitive even when there is no prior knowledge about the structure of the feature space. [sent-690, score-1.001]
</p><p>100 Our expectation is that in real problems where we do know more about the different kinds of features that can be generated, streamwise regression will provide even greater beneﬁt. [sent-691, score-0.959]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('streamwise', 0.666), ('ric', 0.406), ('features', 0.203), ('aic', 0.199), ('wealth', 0.175), ('stepwise', 0.168), ('bic', 0.16), ('feature', 0.138), ('bits', 0.116), ('coding', 0.107), ('stream', 0.099), ('foster', 0.096), ('fdr', 0.09), ('regression', 0.09), ('alpha', 0.088), ('ngar', 0.08), ('oster', 0.08), ('tine', 0.08), ('treamwise', 0.08), ('stine', 0.073), ('spurious', 0.071), ('eature', 0.068), ('nnet', 0.06), ('selection', 0.059), ('bid', 0.053), ('hou', 0.052), ('interaction', 0.05), ('election', 0.049), ('bene', 0.047), ('madelon', 0.047), ('ungar', 0.047), ('dynamically', 0.042), ('adding', 0.041), ('pca', 0.039), ('added', 0.039), ('wi', 0.038), ('false', 0.037), ('predictive', 0.037), ('gene', 0.036), ('saved', 0.035), ('raw', 0.035), ('bonferroni', 0.035), ('popescul', 0.033), ('model', 0.033), ('interactions', 0.031), ('mdl', 0.031), ('relational', 0.03), ('accuracies', 0.029), ('benjamini', 0.028), ('penalty', 0.028), ('code', 0.027), ('arcene', 0.027), ('hung', 0.027), ('ses', 0.027), ('wpbc', 0.027), ('candidate', 0.026), ('uci', 0.025), ('coef', 0.024), ('george', 0.024), ('paired', 0.024), ('zhou', 0.024), ('log', 0.024), ('observations', 0.024), ('threshold', 0.024), ('receiver', 0.023), ('millions', 0.023), ('nips', 0.023), ('sql', 0.023), ('dzeroski', 0.023), ('shuf', 0.023), ('cpu', 0.022), ('ha', 0.022), ('prediction', 0.021), ('upenn', 0.02), ('ation', 0.02), ('batches', 0.02), ('dexter', 0.02), ('gisette', 0.02), ('pcancer', 0.02), ('wdbc', 0.02), ('included', 0.019), ('potential', 0.019), ('selected', 0.019), ('penalties', 0.019), ('transformations', 0.019), ('sequentially', 0.019), ('add', 0.018), ('guyon', 0.018), ('adjusting', 0.017), ('streams', 0.017), ('stringent', 0.017), ('cleve', 0.017), ('diffusion', 0.017), ('invested', 0.017), ('spect', 0.017), ('chance', 0.017), ('generation', 0.017), ('advance', 0.016), ('components', 0.016), ('tests', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="88-tfidf-1" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>Author: Jing Zhou, Dean P. Foster, Robert A. Stine, Lyle H. Ungar</p><p>Abstract: In streamwise feature selection, new features are sequentially considered for addition to a predictive model. When the space of potential features is large, streamwise feature selection offers many advantages over traditional feature selection methods, which assume that all features are known in advance. Features can be generated dynamically, focusing the search for new features on promising subspaces, and overﬁtting can be controlled by dynamically adjusting the threshold for adding features to the model. In contrast to traditional forward feature selection algorithms such as stepwise regression in which at each step all possible features are evaluated and the best one is selected, streamwise feature selection only evaluates each feature once when it is generated. We describe information-investing and α-investing, two adaptive complexity penalty methods for streamwise feature selection which dynamically adjust the threshold on the error reduction required for adding a new feature. These two methods give false discovery rate style guarantees against overﬁtting. They differ from standard penalty methods such as AIC, BIC and RIC, which always drastically over- or under-ﬁt in the limit of inﬁnite numbers of non-predictive features. Empirical results show that streamwise regression is competitive with (on small data sets) and superior to (on large data sets) much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with millions of potential features. Keywords: classiﬁcation, stepwise regression, multiple regression, feature selection, false discovery rate</p><p>2 0.067318663 <a title="88-tfidf-2" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>Author: Hema Raghavan, Omid Madani, Rosie Jones</p><p>Abstract: We extend the traditional active learning framework to include feedback on features in addition to labeling instances, and we execute a careful study of the effects of feature selection and human feedback on features in the setting of text categorization. Our experiments on a variety of categorization tasks indicate that there is signiﬁcant potential in improving classiﬁer performance by feature re-weighting, beyond that achieved via membership queries alone (traditional active learning) if we have access to an oracle that can point to the important (most predictive) features. Our experiments on human subjects indicate that human feedback on feature relevance can identify a sufﬁcient proportion of the most relevant features (over 50% in our experiments). We ﬁnd that on average, labeling a feature takes much less time than labeling a document. We devise an algorithm that interleaves labeling features and documents which signiﬁcantly accelerates standard active learning in our simulation experiments. Feature feedback can complement traditional active learning in applications such as news ﬁltering, e-mail classiﬁcation, and personalization, where the human teacher can have signiﬁcant knowledge on the relevance of features. Keywords: active learning, feature selection, relevance feedback, term feedback, text classiﬁcation</p><p>3 0.041779716 <a title="88-tfidf-3" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>Author: Don Hush, Patrick Kelly, Clint Scovel, Ingo Steinwart</p><p>Abstract: We describe polynomial–time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classiﬁers. These algorithms employ a two–stage process where the ﬁrst stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an √ √ approximate dual solution with accuracy (2 2Kn + 8 λ)−2 λε2 to an approximate primal solution p with accuracy ε p where n is the number of data samples, Kn is the maximum kernel value over the data and λ > 0 is the SVM regularization parameter. For the ﬁrst stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for τ–rate certifying decomposition algorithms we establish the optimality of τ = 1/(n − 1). In addition we extend the recent τ = 1/(n − 1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the τ = 1/(n − 1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the τ–rate certifying property of these algorithms to produce new stopping rules that are computationally efﬁcient and that guarantee a speciﬁed accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classiﬁcation problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n2 ) bound on the overall run time for the ﬁrst stage. Combining the ﬁrst and second stages gives an overall run time of O(n2 (c</p><p>4 0.041754279 <a title="88-tfidf-4" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>Author: Peter Bühlmann, Bin Yu</p><p>Abstract: We propose Sparse Boosting (the SparseL2 Boost algorithm), a variant on boosting with the squared error loss. SparseL2 Boost yields sparser solutions than the previously proposed L2 Boosting by minimizing some penalized L2 -loss functions, the FPE model selection criteria, through smallstep gradient descent. Although boosting may give already relatively sparse solutions, for example corresponding to the soft-thresholding estimator in orthogonal linear models, there is sometimes a desire for more sparseness to increase prediction accuracy and ability for better variable selection: such goals can be achieved with SparseL2 Boost. We prove an equivalence of SparseL2 Boost to Breiman’s nonnegative garrote estimator for orthogonal linear models and demonstrate the generic nature of SparseL2 Boost for nonparametric interaction modeling. For an automatic selection of the tuning parameter in SparseL2 Boost we propose to employ the gMDL model selection criterion which can also be used for early stopping of L2 Boosting. Consequently, we can select between SparseL2 Boost and L2 Boosting by comparing their gMDL scores. Keywords: lasso, minimum description length (MDL), model selection, nonnegative garrote, regression</p><p>5 0.037991464 <a title="88-tfidf-5" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>Author: Janez Demšar</p><p>Abstract: While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classiﬁers: the Wilcoxon signed ranks test for comparison of two classiﬁers and the Friedman test with the corresponding post-hoc tests for comparison of more classiﬁers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams. Keywords: comparative studies, statistical methods, Wilcoxon signed ranks test, Friedman test, multiple comparisons tests</p><p>6 0.035670504 <a title="88-tfidf-6" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>7 0.035496816 <a title="88-tfidf-7" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>8 0.033192649 <a title="88-tfidf-8" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>9 0.030778987 <a title="88-tfidf-9" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>10 0.026965 <a title="88-tfidf-10" href="./jmlr-2006-Noisy-OR_Component_Analysis_and_its_Application_to_Link_Analysis.html">64 jmlr-2006-Noisy-OR Component Analysis and its Application to Link Analysis</a></p>
<p>11 0.026355071 <a title="88-tfidf-11" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>12 0.022932062 <a title="88-tfidf-12" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>13 0.022612935 <a title="88-tfidf-13" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.022337947 <a title="88-tfidf-14" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.021865088 <a title="88-tfidf-15" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.021791289 <a title="88-tfidf-16" href="./jmlr-2006-Toward_Attribute_Efficient_Learning_of_Decision_Lists_and_Parities.html">92 jmlr-2006-Toward Attribute Efficient Learning of Decision Lists and Parities</a></p>
<p>17 0.021113379 <a title="88-tfidf-17" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.020979173 <a title="88-tfidf-18" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>19 0.020974575 <a title="88-tfidf-19" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>20 0.020848922 <a title="88-tfidf-20" href="./jmlr-2006-Learning_Image_Components_for_Object_Recognition.html">47 jmlr-2006-Learning Image Components for Object Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.117), (1, -0.056), (2, -0.02), (3, 0.013), (4, -0.039), (5, -0.03), (6, -0.038), (7, -0.119), (8, 0.067), (9, -0.026), (10, -0.005), (11, 0.042), (12, 0.053), (13, 0.06), (14, 0.0), (15, 0.018), (16, -0.116), (17, 0.092), (18, 0.211), (19, -0.005), (20, 0.041), (21, -0.029), (22, 0.04), (23, -0.078), (24, 0.016), (25, -0.138), (26, 0.253), (27, -0.045), (28, -0.205), (29, -0.193), (30, 0.069), (31, 0.207), (32, 0.197), (33, 0.106), (34, -0.178), (35, -0.097), (36, 0.344), (37, 0.101), (38, 0.017), (39, -0.031), (40, 0.273), (41, -0.116), (42, 0.023), (43, -0.152), (44, 0.064), (45, -0.04), (46, -0.058), (47, 0.149), (48, 0.017), (49, -0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96668059 <a title="88-lsi-1" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>Author: Jing Zhou, Dean P. Foster, Robert A. Stine, Lyle H. Ungar</p><p>Abstract: In streamwise feature selection, new features are sequentially considered for addition to a predictive model. When the space of potential features is large, streamwise feature selection offers many advantages over traditional feature selection methods, which assume that all features are known in advance. Features can be generated dynamically, focusing the search for new features on promising subspaces, and overﬁtting can be controlled by dynamically adjusting the threshold for adding features to the model. In contrast to traditional forward feature selection algorithms such as stepwise regression in which at each step all possible features are evaluated and the best one is selected, streamwise feature selection only evaluates each feature once when it is generated. We describe information-investing and α-investing, two adaptive complexity penalty methods for streamwise feature selection which dynamically adjust the threshold on the error reduction required for adding a new feature. These two methods give false discovery rate style guarantees against overﬁtting. They differ from standard penalty methods such as AIC, BIC and RIC, which always drastically over- or under-ﬁt in the limit of inﬁnite numbers of non-predictive features. Empirical results show that streamwise regression is competitive with (on small data sets) and superior to (on large data sets) much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with millions of potential features. Keywords: classiﬁcation, stepwise regression, multiple regression, feature selection, false discovery rate</p><p>2 0.57717234 <a title="88-lsi-2" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>Author: Hema Raghavan, Omid Madani, Rosie Jones</p><p>Abstract: We extend the traditional active learning framework to include feedback on features in addition to labeling instances, and we execute a careful study of the effects of feature selection and human feedback on features in the setting of text categorization. Our experiments on a variety of categorization tasks indicate that there is signiﬁcant potential in improving classiﬁer performance by feature re-weighting, beyond that achieved via membership queries alone (traditional active learning) if we have access to an oracle that can point to the important (most predictive) features. Our experiments on human subjects indicate that human feedback on feature relevance can identify a sufﬁcient proportion of the most relevant features (over 50% in our experiments). We ﬁnd that on average, labeling a feature takes much less time than labeling a document. We devise an algorithm that interleaves labeling features and documents which signiﬁcantly accelerates standard active learning in our simulation experiments. Feature feedback can complement traditional active learning in applications such as news ﬁltering, e-mail classiﬁcation, and personalization, where the human teacher can have signiﬁcant knowledge on the relevance of features. Keywords: active learning, feature selection, relevance feedback, term feedback, text classiﬁcation</p><p>3 0.2510888 <a title="88-lsi-3" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>Author: Peter Bühlmann, Bin Yu</p><p>Abstract: We propose Sparse Boosting (the SparseL2 Boost algorithm), a variant on boosting with the squared error loss. SparseL2 Boost yields sparser solutions than the previously proposed L2 Boosting by minimizing some penalized L2 -loss functions, the FPE model selection criteria, through smallstep gradient descent. Although boosting may give already relatively sparse solutions, for example corresponding to the soft-thresholding estimator in orthogonal linear models, there is sometimes a desire for more sparseness to increase prediction accuracy and ability for better variable selection: such goals can be achieved with SparseL2 Boost. We prove an equivalence of SparseL2 Boost to Breiman’s nonnegative garrote estimator for orthogonal linear models and demonstrate the generic nature of SparseL2 Boost for nonparametric interaction modeling. For an automatic selection of the tuning parameter in SparseL2 Boost we propose to employ the gMDL model selection criterion which can also be used for early stopping of L2 Boosting. Consequently, we can select between SparseL2 Boost and L2 Boosting by comparing their gMDL scores. Keywords: lasso, minimum description length (MDL), model selection, nonnegative garrote, regression</p><p>4 0.23886181 <a title="88-lsi-4" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>Author: Don Hush, Patrick Kelly, Clint Scovel, Ingo Steinwart</p><p>Abstract: We describe polynomial–time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classiﬁers. These algorithms employ a two–stage process where the ﬁrst stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an √ √ approximate dual solution with accuracy (2 2Kn + 8 λ)−2 λε2 to an approximate primal solution p with accuracy ε p where n is the number of data samples, Kn is the maximum kernel value over the data and λ > 0 is the SVM regularization parameter. For the ﬁrst stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for τ–rate certifying decomposition algorithms we establish the optimality of τ = 1/(n − 1). In addition we extend the recent τ = 1/(n − 1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the τ = 1/(n − 1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the τ–rate certifying property of these algorithms to produce new stopping rules that are computationally efﬁcient and that guarantee a speciﬁed accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classiﬁcation problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n2 ) bound on the overall run time for the ﬁrst stage. Combining the ﬁrst and second stages gives an overall run time of O(n2 (c</p><p>5 0.21449511 <a title="88-lsi-5" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>Author: Janez Demšar</p><p>Abstract: While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classiﬁers: the Wilcoxon signed ranks test for comparison of two classiﬁers and the Friedman test with the corresponding post-hoc tests for comparison of more classiﬁers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams. Keywords: comparative studies, statistical methods, Wilcoxon signed ranks test, Friedman test, multiple comparisons tests</p><p>6 0.18522881 <a title="88-lsi-6" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>7 0.18058632 <a title="88-lsi-7" href="./jmlr-2006-Universal_Kernels.html">93 jmlr-2006-Universal Kernels</a></p>
<p>8 0.17036584 <a title="88-lsi-8" href="./jmlr-2006-Noisy-OR_Component_Analysis_and_its_Application_to_Link_Analysis.html">64 jmlr-2006-Noisy-OR Component Analysis and its Application to Link Analysis</a></p>
<p>9 0.16623256 <a title="88-lsi-9" href="./jmlr-2006-One-Class_Novelty_Detection_for_Seizure_Analysis_from_Intracranial_EEG.html">69 jmlr-2006-One-Class Novelty Detection for Seizure Analysis from Intracranial EEG</a></p>
<p>10 0.15975498 <a title="88-lsi-10" href="./jmlr-2006-Stochastic_Complexities_of_Gaussian_Mixtures_in_Variational_Bayesian_Approximation.html">87 jmlr-2006-Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation</a></p>
<p>11 0.15298806 <a title="88-lsi-11" href="./jmlr-2006-A_Scoring_Function_for_Learning_Bayesian_Networks_based_on_Mutual_Information_and_Conditional_Independence_Tests.html">6 jmlr-2006-A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests</a></p>
<p>12 0.14666075 <a title="88-lsi-12" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>13 0.14181441 <a title="88-lsi-13" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>14 0.13956112 <a title="88-lsi-14" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.12099294 <a title="88-lsi-15" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.11324456 <a title="88-lsi-16" href="./jmlr-2006-Learning_Coordinate_Covariances_via_Gradients.html">45 jmlr-2006-Learning Coordinate Covariances via Gradients</a></p>
<p>17 0.10969367 <a title="88-lsi-17" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>18 0.106187 <a title="88-lsi-18" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>19 0.10533679 <a title="88-lsi-19" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>20 0.10302535 <a title="88-lsi-20" href="./jmlr-2006-In_Search_of_Non-Gaussian_Components_of_a_High-Dimensional_Distribution.html">36 jmlr-2006-In Search of Non-Gaussian Components of a High-Dimensional Distribution</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.017), (35, 0.448), (36, 0.085), (45, 0.017), (46, 0.021), (50, 0.043), (61, 0.013), (63, 0.025), (68, 0.016), (70, 0.014), (76, 0.014), (78, 0.026), (81, 0.024), (84, 0.023), (90, 0.018), (91, 0.018), (96, 0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87947917 <a title="88-lda-1" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Olvi L. Mangasarian</p><p>Abstract: Support vector machines utilizing the 1-norm, typically set up as linear programs (Mangasarian, 2000; Bradley and Mangasarian, 1998), are formulated here as a completely unconstrained minimization of a convex differentiable piecewise-quadratic objective function in the dual space. The objective function, which has a Lipschitz continuous gradient and contains only one additional ﬁnite parameter, can be minimized by a generalized Newton method and leads to an exact solution of the support vector machine problem. The approach here is based on a formulation of a very general linear program as an unconstrained minimization problem and its application to support vector machine classiﬁcation problems. The present approach which generalizes both (Mangasarian, 2004) and (Fung and Mangasarian, 2004) is also applied to nonlinear approximation where a minimal number of nonlinear kernel functions are utilized to approximate a function from a given number of function values.</p><p>same-paper 2 0.74812037 <a title="88-lda-2" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>Author: Jing Zhou, Dean P. Foster, Robert A. Stine, Lyle H. Ungar</p><p>Abstract: In streamwise feature selection, new features are sequentially considered for addition to a predictive model. When the space of potential features is large, streamwise feature selection offers many advantages over traditional feature selection methods, which assume that all features are known in advance. Features can be generated dynamically, focusing the search for new features on promising subspaces, and overﬁtting can be controlled by dynamically adjusting the threshold for adding features to the model. In contrast to traditional forward feature selection algorithms such as stepwise regression in which at each step all possible features are evaluated and the best one is selected, streamwise feature selection only evaluates each feature once when it is generated. We describe information-investing and α-investing, two adaptive complexity penalty methods for streamwise feature selection which dynamically adjust the threshold on the error reduction required for adding a new feature. These two methods give false discovery rate style guarantees against overﬁtting. They differ from standard penalty methods such as AIC, BIC and RIC, which always drastically over- or under-ﬁt in the limit of inﬁnite numbers of non-predictive features. Empirical results show that streamwise regression is competitive with (on small data sets) and superior to (on large data sets) much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with millions of potential features. Keywords: classiﬁcation, stepwise regression, multiple regression, feature selection, false discovery rate</p><p>3 0.32117951 <a title="88-lda-3" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>Author: Greg Hamerly, Erez Perelman, Jeremy Lau, Brad Calder, Timothy Sherwood</p><p>Abstract: An essential step in designing a new computer architecture is the careful examination of different design options. It is critical that computer architects have efﬁcient means by which they may estimate the impact of various design options on the overall machine. This task is complicated by the fact that different programs, and even different parts of the same program, may have distinct behaviors that interact with the hardware in different ways. Researchers use very detailed simulators to estimate processor performance, which models every cycle of an executing program. Unfortunately, simulating every cycle of a real program can take weeks or months. To address this problem we have created a tool called SimPoint that uses data clustering algorithms from machine learning to automatically ﬁnd repetitive patterns in a program’s execution. By simulating one representative of each repetitive behavior pattern, simulation time can be reduced to minutes instead of weeks for standard benchmark programs, with very little cost in terms of accuracy. We describe this important problem, the data representation and preprocessing methods used by SimPoint, the clustering algorithm at the core of SimPoint, and we evaluate different options for tuning SimPoint. Keywords: k-means, random projection, Bayesian information criterion, simulation, SimPoint</p><p>4 0.31964657 <a title="88-lda-4" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Matthias Heiler, Christoph Schnörr</p><p>Abstract: We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. We show that for ordinary NMF, our approach performs as well as existing stateof-the-art algorithms, while for sparsity-constrained NMF, as recently proposed by P. O. Hoyer in JMLR 5 (2004), it outperforms previous methods. In addition, we show how to extend NMF learning within the same optimization framework in order to make use of class membership information in supervised learning problems. Keywords: non-negative matrix factorization, second-order cone programming, sequential convex optimization, reverse-convex programming, sparsity</p><p>5 0.31392112 <a title="88-lda-5" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>Author: Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou</p><p>Abstract: We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the ﬁrst time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction. html. Keywords: transduction, transductive SVMs, semi-supervised learning, CCCP</p><p>6 0.30475095 <a title="88-lda-6" href="./jmlr-2006-Generalized_Bradley-Terry_Models_and_Multi-Class_Probability_Estimates.html">34 jmlr-2006-Generalized Bradley-Terry Models and Multi-Class Probability Estimates</a></p>
<p>7 0.3027159 <a title="88-lda-7" href="./jmlr-2006-MinReg%3A_A_Scalable_Algorithm_for_Learning_Parsimonious_Regulatory_Networks_in_Yeast_and_Mammals.html">62 jmlr-2006-MinReg: A Scalable Algorithm for Learning Parsimonious Regulatory Networks in Yeast and Mammals</a></p>
<p>8 0.30144191 <a title="88-lda-8" href="./jmlr-2006-Sparse_Boosting.html">83 jmlr-2006-Sparse Boosting</a></p>
<p>9 0.30086827 <a title="88-lda-9" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.2905049 <a title="88-lda-10" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.28427416 <a title="88-lda-11" href="./jmlr-2006-Fast_SDP_Relaxations_of_Graph_Cut_Clustering%2C_Transduction%2C_and_Other_Combinatorial_Problems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">33 jmlr-2006-Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.28214729 <a title="88-lda-12" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.28180724 <a title="88-lda-13" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>14 0.28102273 <a title="88-lda-14" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<p>15 0.27884105 <a title="88-lda-15" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.27768341 <a title="88-lda-16" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.27681243 <a title="88-lda-17" href="./jmlr-2006-Geometric_Variance_Reduction_in_Markov_Chains%3A_Application_to_Value_Function_and_Gradient_Estimation.html">35 jmlr-2006-Geometric Variance Reduction in Markov Chains: Application to Value Function and Gradient Estimation</a></p>
<p>18 0.27581424 <a title="88-lda-18" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>19 0.27488014 <a title="88-lda-19" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>20 0.2746723 <a title="88-lda-20" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
