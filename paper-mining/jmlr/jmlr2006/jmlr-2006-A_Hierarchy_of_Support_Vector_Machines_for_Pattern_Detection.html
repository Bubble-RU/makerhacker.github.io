<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-3" href="#">jmlr2006-3</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</h1>
<br/><p>Source: <a title="jmlr-2006-3-pdf" href="http://jmlr.org/papers/volume7/sahbi06a/sahbi06a.pdf">pdf</a></p><p>Author: Hichem Sahbi, Donald Geman</p><p>Abstract: We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly ﬁner subsets. The hierarchy is traversed coarse-to-ﬁne and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This “free network” is then perturbed, cell by cell, into another network, which is “graded” in two ways: ﬁrst, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-speciﬁc SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network. Keywords: statistical learning, hierarchy of classiﬁers, coarse-to-ﬁne computation, support vector machines, face detection</p><p>Reference: <a title="jmlr-2006-3-reference" href="../jmlr2006_reference/jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. [sent-11, score-0.451]
</p><p>2 When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-speciﬁc SVM many times. [sent-12, score-0.409]
</p><p>3 Keywords: statistical learning, hierarchy of classiﬁers, coarse-to-ﬁne computation, support vector machines, face detection  1. [sent-14, score-0.573]
</p><p>4 Our optimization framework is motivated by natural trade-offs among invariance, selectivity (background rejection rate) and the cost of processing the data in order to determine all detected patterns. [sent-22, score-0.32]
</p><p>5 This is illustrated for face detection in Fig 1; a graded network of SVMs achieves approximately the same accuracy as a pattern-speciﬁc SVM but with order 100 to 1000 times fewer kernel evaluations, resulting from the network architecture as well as the reduced number of support vectors. [sent-28, score-0.535]
</p><p>6 ) Moreover, the decision boundaries are shifted to preserve all positive responses from the original set of training data; consequently, the false negative (missed detection) rate of g t is at most that of ft and any pattern detected by the f-network is also detected by the g-network. [sent-46, score-0.52]
</p><p>7 We minimize the mean computation necessary to collect all detections subject to a constraint on the rate of false detections. [sent-49, score-0.391]
</p><p>8 (In the application to face detection, a false detection refers to ﬁnding a face amidst clutter. [sent-50, score-0.777]
</p><p>9 2088  A H IERARCHY OF S UPPORT V ECTOR M ACHINES FOR PATTERN D ETECTION  Figure 1: Comparison between a single SVM (top row) dedicated to a nearly ﬁxed pose and our designed network (bottom row) which investigates many poses simultaneously. [sent-59, score-0.31]
</p><p>10 Consider an SVM f in the f-network with N support vectors and dedicated to a particular hypothesis cell; this network is slow, but has high selectivity and few false negatives. [sent-63, score-0.567]
</p><p>11 87 19 20 20  Table 1: Comparisons among i) a single SVM dedicated to a small set of hypotheses (in this case a constrained pose domain), ii) the f-network and iii) our designed g-network, for the images in Fig 1. [sent-96, score-0.368]
</p><p>12 02  300 200 100  0  0  50 Level 1  Level 2  Level 3  Level 4  Level 5  Level 6  100  150 200 250 300 Cardinality of the reduced set  350  400  Figure 2: Left: The average number of support vectors for each level in an f-network built for face detection. [sent-105, score-0.336]
</p><p>13 Right: False alarm rate as a function of the number of support vectors using two SVM classiﬁers in the g-network with different pose constraints. [sent-107, score-0.341]
</p><p>14 Of course, we cannot explore all possible designs so a model-based approach is necessary: The false alarm rate of each SVM is assumed to vary with complexity and invariance in a certain way. [sent-116, score-0.407]
</p><p>15 This seems to the case for example with frontal face detection as long as large training sets are available, at least thousands of faces and sometimes billions of negative examples, for learning long, powerful cascades. [sent-126, score-0.587]
</p><p>16 In Section 3, we discuss hierarchical representation and search in general terms; decomposing the pose space provides a running example of the ideas and sets the stage for our main application - face detection. [sent-132, score-0.387]
</p><p>17 These ideas are illustrated for a pose hierarchy in Section 7, including a speciﬁc instance of the model for chain probabilities and the corresponding minimization of cost subject to a constraint on false alarms. [sent-136, score-0.59]
</p><p>18 In the context of ﬁnding faces in cluttered scenes, Fleuret and Geman (2001) developed a fast, coarse-to-ﬁne detector based on simple edge conﬁgurations and a hierarchical decomposition of the space of poses (location, scale and tilt). [sent-151, score-0.396]
</p><p>19 ) One constructs a family of classiﬁers, one for each cell in a recursive partitioning of the pose space and trained on a sub-population of faces meeting the pose constraints. [sent-154, score-0.519]
</p><p>20 A face is declared with pose in a leaf cell if all the classiﬁers along the chain from root to leaf respond positively. [sent-155, score-0.515]
</p><p>21 For example, Viola and Jones (2001) developed an accurate, real-time face detection algorithm in the form of a cascade of boosted classiﬁers and computationally efﬁcient feature detection. [sent-174, score-0.407]
</p><p>22 In comparing the two strategies, especially our work with cascades of SVMs for face detection as in Kienzle et al. [sent-186, score-0.455]
</p><p>23 Under this cost model, and equalizing the selectivity, the subadditivity of Γ would render the test dedicated to |A| B cheaper than doing the test dedicated to A approximately |B| times, even ignoring the inevitable reduction in selectivity due to repeated tests. [sent-198, score-0.38]
</p><p>24 More importantly, perhaps, it is not clear that cascades will scale to more ambitious problems involving many classes and instantiations since repeatedly testing a coarse set of hypotheses will lack selectivity and repeatedly testing a narrow one will require a great many implementations. [sent-199, score-0.318]
</p><p>25 The tests are constructed to be very conservative in the sense that each false negative error rate is very small, that is, given that Y ∈ A, we are very unlikely to declare background if A ⊂ Λ is the subset of hypotheses tested at a given stage. [sent-208, score-0.495]
</p><p>26 The price for this small false negative error is of course a non-negligible false positive error, particularly for testing “large” subsets A. [sent-209, score-0.38]
</p><p>27 one object class – faces – the family of hypotheses of interest is a set of poses Λ. [sent-245, score-0.416]
</p><p>28 Thus, we regard Λ as a “reference set” of poses in the sense of possible instantiations of a single face within a given 64 × 64 image assuming that the position is restricted to a subwindow (e. [sent-248, score-0.353]
</p><p>29 ) / Notice that D = 0 if and only if there is a “null covering” of the hierarchy in the sense of a collection of negative responses whose corresponding cells cover all hypotheses in Λ. [sent-274, score-0.315]
</p><p>30 If D(x) = 0, the estimated pose of the face detected in ω is obtained by averaging over the “pose prototypes” of each leaf cell represented in D, where the pose prototype of Λt is the midpoint (cf. [sent-283, score-0.617]
</p><p>31 For each scale, the base face detector visits each non-overlapping 16 × 16 block, and searches the surrounding image data for all faces with position in the block, scale anywhere in the range [10, 20] and in-plane orientation in the range [−20 o , +20o ]. [sent-287, score-0.497]
</p><p>32 (In our application to face detection, we train ft based on face images ω with pose in Λt . [sent-307, score-0.743]
</p><p>33 We assume the false negative rate of f t is very small for each t; in other words, ft (ω) > 0 for nearly all patterns ω for which Y (ω) ∈ Λt . [sent-311, score-0.36]
</p><p>34 As a result, for nodes t near the root of T the false positive rate of gt will be higher than that of the corresponding ft since low cost comes at the expense of a weakened background ﬁlter. [sent-320, score-0.671]
</p><p>35 Instead of imposing an absolute constraint on the false negative error, we impose one relative to the f-network, referred to as the conservation hypothesis: For each t ∈ T and ω ∈ Ω: ft (ω) > 0 ⇒ gt (ω) > 0. [sent-323, score-0.373]
</p><p>36 With the same number of support vectors, gt will generally produce more false alarms than gs since more invariance is expected of gt (cf. [sent-326, score-0.694]
</p><p>37 In constructing the g-network, all classiﬁers at the same level will have the same number of support vectors and are then expected to have approximately the same false alarm rate (cf. [sent-328, score-0.447]
</p><p>38 In the following sections, we will introduce a model which accounts for both the overall mean cost and the false alarm rate. [sent-330, score-0.397]
</p><p>39 The proposed analysis is performed under the assumption that there exists a convex function which models the false alarm rate as a function of the number of support vectors and the degree of “pose invariance”. [sent-332, score-0.415]
</p><p>40 01 0 0  20 40 60 80 100 120 140 Cardinality of the reduced set  Figure 8: For the root cell, a particular cell in the ﬁfth level and three particular pose cells in the sixth level of the g-network, we built SVMs with varying numbers of (virtual) support vectors. [sent-345, score-0.379]
</p><p>41 All curves show false alarm rates with respect to the number of (virtual) support vectors. [sent-346, score-0.332]
</p><p>42 For the sixth level, and in the regime of fewer than 10 (virtual) support vectors, the false alarm rates show considerable variation, but have the same order of magnitude. [sent-347, score-0.332]
</p><p>43 These experiments were run on background patterns taken from 200 images including highly textured areas (ﬂowers, houses, trees, etc. [sent-348, score-0.339]
</p><p>44 Relative to the problem of deciding Y = 0 vs Y = 0, that is, deciding between “background” / and “object” (some hypothesis in Λ), the two error rates for the f-network are P0 (D f = 0), the false / / positive rate, and P1 (D f = 0), the false negative rate. [sent-358, score-0.409]
</p><p>45 Consider the event that a T background pattern traverses the hierarchy up to node t, namely the event s∈At {gs > 0}, where At denotes the set of ancestors of t – the nodes from the parent of t to the root, inclusive. [sent-366, score-0.391]
</p><p>46 In order to achieve efﬁcient computation (at the expense of extra false alarms relative to the f-network), we choose n = (n1 , . [sent-387, score-0.345]
</p><p>47 , nL ) = E0 (Cost) L  =  νl  ∑ ∑ P0 ({gl,k is performed}) cl,k  l=1 k=1 L νl  =  ∑ ∑ δ(l − 1; n) cl,k  l=1 k=1 L  =  ∑  νl δ(l − 1; n) cl  l=1 L  = a ∑ νl δ(l − 1; n) nl + b l=1  L  ∑  νl δ(l − 1; n). [sent-412, score-0.317]
</p><p>48 In the application to face detection we shall assume the preprocessing cost – the computation of Haar wavelet coefﬁcients for a given subimage – is small compared with kernel evaluations, and set a = 1 and b = 0. [sent-414, score-0.545]
</p><p>49 Finally, since we are going to use the SVMs in the f-network to build those in the g-network, the number of support vectors nl for each SVM in the g-network at level l is bounded by the corresponding number, Nl , for the f-network. [sent-426, score-0.428]
</p><p>50 ) Summarizing, our constrained optimization problem (3) becomes L  min n1 + ∑ νl nl δ(l − 1; n) s. [sent-428, score-0.317]
</p><p>51 Right: Visual appearance of some face and “non-face” virtual support vectors. [sent-477, score-0.317]
</p><p>52 Hence we prefer to severely limit the number of missed detections at the expense of additional false positives; hopefully these background patterns will be ﬁltered out before reaching the leaves. [sent-495, score-0.603]
</p><p>53 Application to Face Detection We apply the general construction of the previous sections to a particular two-class problem – face detection – which has been widely investigated, especially in the last ten years. [sent-502, score-0.362]
</p><p>54 The motivation for this functional form is that the conditional false alarm rate δ(l | 1, . [sent-548, score-0.336]
</p><p>55 Indeed, when g s > 0 for all nodes s upstream of node t, and when these SVMs have a large number of support vectors and hence are very selective, the background patterns reaching node t resemble faces very closely and are likely to be accepted by the test at t. [sent-555, score-0.532]
</p><p>56 Of course, ﬁxing the numbers of support vectors upstream, the conditional selectivity (that is, one minus the false positive error rate) at level l grows with n l . [sent-556, score-0.459]
</p><p>57 + βl nl 2106  A H IERARCHY OF S UPPORT V ECTOR M ACHINES FOR PATTERN D ETECTION  −1  l  =  ∑ β jn j  . [sent-567, score-0.317]
</p><p>58 This problem is solved in two steps:  n 1 +        νL  l=2  −1  l−1 i=1  ∑ ∑ βi n i L  −1  ∑ βi n i    νl n l   (8)  ≤ µ  i=1  0 < nl ≤ Nl . [sent-575, score-0.317]
</p><p>59 Here Ψ1 represents the degree of pose invariance at the root cell and Ψ2 is the rate of the decrease of this invariance. [sent-587, score-0.356]
</p><p>60 The empirical conditional false alarms were estimated on background patterns taken from 200 images including highly textured areas (ﬂowers, houses, trees, etc. [sent-639, score-0.65]
</p><p>61 4 Features and Parameters Many factors intervene in ﬁtting our cost/error model to real observations (the conditional false alarms), including the size of the training sets and the choice of features, kernels and other parameters, such as the bound on the expected number of false alarms. [sent-647, score-0.38]
</p><p>62 Accordingly, in principle, the g-network could be designed with few (virtual) support vectors while satisfying the false alarm bound in (3). [sent-864, score-0.377]
</p><p>63 Experiments All the training images of faces are based on the Olivetti database of 400 gray level pictures – ten frontal images for each of forty individuals. [sent-869, score-0.469]
</p><p>64 Nonetheless, this criterion is rarely taken into account in the literature on face detection (and more generally in machine learning). [sent-873, score-0.362]
</p><p>65 In order to sample the pose variation within Λt , for each face image in the original Olivetti database, we synthesize 20 images of 64 × 64 pixels with randomly chosen poses in Λt . [sent-874, score-0.575]
</p><p>66 Thus, a set of 8, 000 faces is synthesized for each pose cell in the hierarchy. [sent-875, score-0.403]
</p><p>67 Each subimage, either a face or background, is encoded using the 16 × 16 low frequency coefﬁcients of the Haar wavelet transform computed efﬁciently using the integral image (Sahbi, 2003; Viola and Jones, 2001). [sent-881, score-0.315]
</p><p>68 The set of face and background patterns belonging to Λ t are used to train the underlying SVM ft in the f-network (using a Gaussian kernel). [sent-883, score-0.482]
</p><p>69 1 Clustering Detections Generally, a face will be detected at several poses; similarly, false positives will often be found in small clusters. [sent-885, score-0.481]
</p><p>70 In fact, every method faces the problem of clustering detections in order to provide a reasonable estimate of the “false alarm rate,” rendering comparisons somewhat difﬁcult. [sent-886, score-0.464]
</p><p>71 It results in a set of detections D g for each non-overlapping 16 × 16 block in the original image and each such block in each of three downsampled images (to detect larger faces). [sent-889, score-0.314]
</p><p>72 In this way, the false negative rate does 2110  A H IERARCHY OF S UPPORT V ECTOR M ACHINES FOR PATTERN D ETECTION  not increase due to pruning and yet some false positives are removed. [sent-897, score-0.418]
</p><p>73 Our main intention is to illustrate the performance of the g-network on a real pattern recognition problem rather than to provide a detailed study of face detection or to optimize our error rates. [sent-902, score-0.414]
</p><p>74 , a threshold) in order to investigate the trade-off between false positives and false negatives. [sent-908, score-0.38]
</p><p>75 8 % with 245 false alarms and examples are shown in the top of Fig 10. [sent-915, score-0.311]
</p><p>76 The false alarm rate is the total number of false detections divided by the total number of hierarchies traversed, that is, the total number of 16 × 16 blocks visited in processing the entire database. [sent-924, score-0.689]
</p><p>77 2 ARF DATABASE  AND  S ENSITIVITY A NALYSIS  The full ARF database contains 4000 images on ten DVDs; eight of these DVDs – 3, 200 images with faces of 100 individuals against uniform backgrounds – are publicly available at (http://rvl1. [sent-927, score-0.405]
</p><p>78 Among the 10 face images for a given person, two images show the person with sunglasses, three with scarves and ﬁve with some variation in the facial expression and/or strong lighting effects. [sent-934, score-0.437]
</p><p>79 3 CMU+MIT DATASET The CMU subset contains frontal (upright and in-plane rotated) faces whereas the MIT subset contains lower quality face images. [sent-943, score-0.45]
</p><p>80 The false alarm rate is given as the number of background pattern declared as faces over the total number of background patterns. [sent-956, score-0.831]
</p><p>81 # of images # of faces False alarms Detection rate Time (384 × 288)  Sahbi & Geman 164 556 112 89. [sent-959, score-0.458]
</p><p>82 (1998); Viola and Jones (2001): for 95 false alarms, the detection rate in Viola and Jones (2001) was 90. [sent-971, score-0.365]
</p><p>83 Put another way, we have an equivalent number of false alarms with a larger test set but a slightly smaller detection rate; see Table 4. [sent-975, score-0.448]
</p><p>84 94 %  # False alarms 312 112 096 004  False alarms rate 1/2,157 1/6,011 1/7,013 1/168,315  Table 5: Evaluation of our face detector on the CMU+MIT databases. [sent-1032, score-0.539]
</p><p>85 Summary We presented a general method for exploring a space of hypotheses based on a coarse-to-ﬁne hierarchy of SVM classiﬁers and applied it to the special case of detecting faces in cluttered images. [sent-1040, score-0.477]
</p><p>86 This is in fact the case at deep levels of the hierarchy, at which point the conditional selectivity of the classiﬁers should ideally be calculated with respect to both object and background probabilities. [sent-1047, score-0.394]
</p><p>87 Extensive experiments on face detection demonstrate the huge gain in efﬁciency relative to either a dedicated SVM or an unrestricted hierarchy of SVMs, while at the same time maintaining precision. [sent-1049, score-0.618]
</p><p>88 Clearly, the unconstrained problem is degenerate, minimized by choosing nl ≡ 0; indeed this minimizes cost. [sent-1063, score-0.317]
</p><p>89 We start by minimizing cost for a ﬁxed value of nL and for real-valued nl , l = 1, . [sent-1064, score-0.381]
</p><p>90 , L − 1, are given by:   1/L −1  L−1    L(L−1)  2 2  nL  if l = 1 ∏ βi    i=1 (11) nl =  − l(l−1) l−1 −1 l−1 l−1  2 2  ∏ βi βl n1 (βl n1 − 2 ) l ∈ {2, . [sent-1074, score-0.317]
</p><p>91 The proof is straightforward:        L  l−1  ∂C  β1 2 n l  = 1 − ∑  l−1  ∂n1  l=2  ( ∑ βi n i )2 i=1  ∂C = ∂n j     β j 2l−1 nl    , j ∈ {2, L − 1}. [sent-1078, score-0.317]
</p><p>92 − ∑  l−1  j−1  l= j+1  ( ∑ βi n i )2 ( ∑ βi n i ) L  2 j−1  i=1  i=1  We have:  ∂C =0 ⇒ ∂n j+1  ∂C =0 ∂n j  (12)    ⇒  L       l−1   2 nl  1 2j  = ∑  l−1  j β j+1  l= j+2  ( ∑ βi n i )2 ( ∑ βi n i ) i=1 i=1  2 j n j+1    L   l−1   2 nl    = 0. [sent-1079, score-0.634]
</p><p>93 j  (13)  β j+1 ( ∑ βi ni )  2  i=1  i=1  2j  −βj  i=1  Suppose n1 is known; we show by a recursion that the general term nl is given by (11): ∂C ∂C = 0 and = 0, we obtain: Combining ∂n1 ∂n2 1−  β1 2 n 2 2 β1 = 0 − 2 n2 β2 β1 n 1 β1 1  ⇒  n2 =  1 β1 n1 (β2 n1 − 2). [sent-1081, score-0.317]
</p><p>94 (14)  i=1  We now demonstrate that: nl+1 = 2−  (l+1)l 2  l  ∏ βi  β−1 nl (βl+1 n1 − 2l ). [sent-1083, score-0.317]
</p><p>95 Then, for each value of this parameter, we check the consistency of the candidate solution, that is, whether the ﬁrst constraint (on expected false alarms) is satisﬁed and whether each nl is bounded Nl . [sent-1102, score-0.507]
</p><p>96 For large values of n L , the bounds on {nl } might not be satisﬁed and the average cost increases, although the mean false alarm constraint is typically satisﬁed. [sent-1110, score-0.362]
</p><p>97 Convolutional face ﬁnder: A neural architecture for fast and robust face detection. [sent-1234, score-0.478]
</p><p>98 Feature reduction and hierarchy of classiﬁers for fast object detection in video images. [sent-1260, score-0.387]
</p><p>99 A hierarchical multiscale and multiangle system for human face detection in complex background using gravity center template. [sent-1317, score-0.533]
</p><p>100 A statistical method for 3d object detection applied to faces and cars. [sent-1362, score-0.403]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nl', 0.317), ('fig', 0.251), ('face', 0.225), ('faces', 0.193), ('false', 0.19), ('geman', 0.182), ('hierarchy', 0.177), ('ahbi', 0.168), ('eman', 0.168), ('ierarchy', 0.168), ('sahbi', 0.168), ('detections', 0.163), ('selectivity', 0.158), ('achines', 0.142), ('etection', 0.142), ('detection', 0.137), ('viola', 0.13), ('ector', 0.127), ('upport', 0.127), ('background', 0.125), ('alarms', 0.121), ('pose', 0.116), ('gt', 0.112), ('alarm', 0.108), ('images', 0.106), ('dg', 0.099), ('fleuret', 0.095), ('cell', 0.094), ('cascades', 0.093), ('rowley', 0.093), ('svm', 0.088), ('poses', 0.083), ('dedicated', 0.079), ('subimage', 0.074), ('object', 0.073), ('invariance', 0.071), ('ft', 0.071), ('svms', 0.07), ('hypotheses', 0.067), ('detected', 0.066), ('cost', 0.064), ('bg', 0.063), ('jones', 0.063), ('patterns', 0.061), ('virtual', 0.058), ('gangaputra', 0.056), ('tilt', 0.056), ('subimages', 0.055), ('wg', 0.055), ('gs', 0.054), ('cmu', 0.052), ('pattern', 0.052), ('graded', 0.047), ('tests', 0.047), ('vision', 0.047), ('kanade', 0.047), ('romdhani', 0.047), ('schneiderman', 0.047), ('textured', 0.047), ('hierarchical', 0.046), ('blanchard', 0.045), ('cascade', 0.045), ('wavelet', 0.045), ('vectors', 0.045), ('image', 0.045), ('ers', 0.044), ('chain', 0.043), ('scenes', 0.042), ('amit', 0.04), ('cluttered', 0.04), ('levels', 0.038), ('rate', 0.038), ('responses', 0.037), ('node', 0.037), ('root', 0.037), ('feret', 0.037), ('kienzle', 0.037), ('socolinsky', 0.037), ('classi', 0.036), ('accounts', 0.035), ('cells', 0.034), ('detector', 0.034), ('expense', 0.034), ('support', 0.034), ('rejection', 0.032), ('targeted', 0.032), ('network', 0.032), ('level', 0.032), ('frontal', 0.032), ('missed', 0.03), ('scene', 0.029), ('hypothesis', 0.029), ('architecture', 0.028), ('color', 0.028), ('osuna', 0.028), ('declare', 0.028), ('eyes', 0.028), ('arf', 0.028), ('groot', 0.028), ('houses', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="3-tfidf-1" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>Author: Hichem Sahbi, Donald Geman</p><p>Abstract: We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly ﬁner subsets. The hierarchy is traversed coarse-to-ﬁne and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This “free network” is then perturbed, cell by cell, into another network, which is “graded” in two ways: ﬁrst, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-speciﬁc SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network. Keywords: statistical learning, hierarchy of classiﬁers, coarse-to-ﬁne computation, support vector machines, face detection</p><p>2 0.10283192 <a title="3-tfidf-2" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>3 0.081567362 <a title="3-tfidf-3" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>4 0.07754752 <a title="3-tfidf-4" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>Author: S. V. N. Vishwanathan, Nicol N. Schraudolph, Alex J. Smola</p><p>Abstract: This paper presents an online support vector machine (SVM) that uses the stochastic meta-descent (SMD) algorithm to adapt its step size automatically. We formulate the online learning problem as a stochastic gradient descent in reproducing kernel Hilbert space (RKHS) and translate SMD to the nonparametric setting, where its gradient trace parameter is no longer a coefﬁcient vector but an element of the RKHS. We derive efﬁcient updates that allow us to perform the step size adaptation in linear time. We apply the online SVM framework to a variety of loss functions, and in particular show how to handle structured output spaces and achieve efﬁcient online multiclass classiﬁcation. Experiments show that our algorithm outperforms more primitive methods for setting the gradient step size. Keywords: online SVM, stochastic meta-descent, structured output spaces</p><p>5 0.074632525 <a title="3-tfidf-5" href="./jmlr-2006-One-Class_Novelty_Detection_for_Seizure_Analysis_from_Intracranial_EEG.html">69 jmlr-2006-One-Class Novelty Detection for Seizure Analysis from Intracranial EEG</a></p>
<p>Author: Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos, Brian Litt</p><p>Abstract: This paper describes an application of one-class support vector machine (SVM) novelty detection for detecting seizures in humans. Our technique maps intracranial electroencephalogram (EEG) time series into corresponding novelty sequences by classifying short-time, energy-based statistics computed from one-second windows of data. We train a classifier on epochs of interictal (normal) EEG. During ictal (seizure) epochs of EEG, seizure activity induces distributional changes in feature space that increase the empirical outlier fraction. A hypothesis test determines when the parameter change differs significantly from its nominal value, signaling a seizure detection event. Outputs are gated in a “one-shot” manner using persistence to reduce the false alarm rate of the system. The detector was validated using leave-one-out cross-validation (LOO-CV) on a sample of 41 interictal and 29 ictal epochs, and achieved 97.1% sensitivity, a mean detection latency of ©2005 Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos and Brian Litt GARDNER, KRIEGER, VACHTSEVANOS AND LITT -7.58 seconds, and an asymptotic false positive rate (FPR) of 1.56 false positives per hour (Fp/hr). These results are better than those obtained from a novelty detection technique based on Mahalanobis distance outlier detection, and comparable to the performance of a supervised learning technique used in experimental implantable devices (Echauz et al., 2001). The novelty detection paradigm overcomes three significant limitations of competing methods: the need to collect seizure data, precisely mark seizure onset and offset times, and perform patient-specific parameter tuning for detector training. Keywords: seizure detection, novelty detection, one-class SVM, epilepsy, unsupervised learning 1</p><p>6 0.072170816 <a title="3-tfidf-6" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<p>7 0.068851486 <a title="3-tfidf-7" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>8 0.060650211 <a title="3-tfidf-8" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>9 0.057171915 <a title="3-tfidf-9" href="./jmlr-2006-Learning_Image_Components_for_Object_Recognition.html">47 jmlr-2006-Learning Image Components for Object Recognition</a></p>
<p>10 0.049806722 <a title="3-tfidf-10" href="./jmlr-2006-A_Simulation-Based_Algorithm_for_Ergodic_Control_of_Markov_Chains_Conditioned_on_Rare_Events.html">7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</a></p>
<p>11 0.049320791 <a title="3-tfidf-11" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.046407182 <a title="3-tfidf-12" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.044724915 <a title="3-tfidf-13" href="./jmlr-2006-%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">97 jmlr-2006- (Special Topic on Machine Learning for Computer Security)</a></p>
<p>14 0.043109126 <a title="3-tfidf-14" href="./jmlr-2006-Machine_Learning_for_Computer_Security%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">59 jmlr-2006-Machine Learning for Computer Security    (Special Topic on Machine Learning for Computer Security)</a></p>
<p>15 0.041672781 <a title="3-tfidf-15" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>16 0.039752975 <a title="3-tfidf-16" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>17 0.039495002 <a title="3-tfidf-17" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.039169472 <a title="3-tfidf-18" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>19 0.038013272 <a title="3-tfidf-19" href="./jmlr-2006-Considering_Cost_Asymmetry_in_Learning_Classifiers.html">22 jmlr-2006-Considering Cost Asymmetry in Learning Classifiers</a></p>
<p>20 0.037343685 <a title="3-tfidf-20" href="./jmlr-2006-Learning_Minimum_Volume_Sets.html">48 jmlr-2006-Learning Minimum Volume Sets</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.217), (1, -0.043), (2, 0.057), (3, 0.131), (4, -0.022), (5, -0.016), (6, -0.126), (7, -0.226), (8, 0.002), (9, 0.015), (10, -0.02), (11, -0.013), (12, 0.066), (13, 0.006), (14, 0.096), (15, -0.056), (16, 0.155), (17, -0.076), (18, 0.11), (19, 0.247), (20, -0.113), (21, -0.169), (22, -0.096), (23, 0.005), (24, 0.212), (25, 0.039), (26, -0.087), (27, -0.001), (28, 0.178), (29, 0.087), (30, 0.107), (31, 0.123), (32, 0.168), (33, 0.035), (34, 0.013), (35, 0.1), (36, -0.046), (37, 0.054), (38, -0.062), (39, 0.03), (40, -0.003), (41, -0.01), (42, -0.084), (43, -0.034), (44, 0.003), (45, 0.016), (46, 0.021), (47, -0.051), (48, -0.042), (49, 0.133)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94903511 <a title="3-lsi-1" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>Author: Hichem Sahbi, Donald Geman</p><p>Abstract: We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly ﬁner subsets. The hierarchy is traversed coarse-to-ﬁne and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This “free network” is then perturbed, cell by cell, into another network, which is “graded” in two ways: ﬁrst, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-speciﬁc SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network. Keywords: statistical learning, hierarchy of classiﬁers, coarse-to-ﬁne computation, support vector machines, face detection</p><p>2 0.5760169 <a title="3-lsi-2" href="./jmlr-2006-One-Class_Novelty_Detection_for_Seizure_Analysis_from_Intracranial_EEG.html">69 jmlr-2006-One-Class Novelty Detection for Seizure Analysis from Intracranial EEG</a></p>
<p>Author: Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos, Brian Litt</p><p>Abstract: This paper describes an application of one-class support vector machine (SVM) novelty detection for detecting seizures in humans. Our technique maps intracranial electroencephalogram (EEG) time series into corresponding novelty sequences by classifying short-time, energy-based statistics computed from one-second windows of data. We train a classifier on epochs of interictal (normal) EEG. During ictal (seizure) epochs of EEG, seizure activity induces distributional changes in feature space that increase the empirical outlier fraction. A hypothesis test determines when the parameter change differs significantly from its nominal value, signaling a seizure detection event. Outputs are gated in a “one-shot” manner using persistence to reduce the false alarm rate of the system. The detector was validated using leave-one-out cross-validation (LOO-CV) on a sample of 41 interictal and 29 ictal epochs, and achieved 97.1% sensitivity, a mean detection latency of ©2005 Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos and Brian Litt GARDNER, KRIEGER, VACHTSEVANOS AND LITT -7.58 seconds, and an asymptotic false positive rate (FPR) of 1.56 false positives per hour (Fp/hr). These results are better than those obtained from a novelty detection technique based on Mahalanobis distance outlier detection, and comparable to the performance of a supervised learning technique used in experimental implantable devices (Echauz et al., 2001). The novelty detection paradigm overcomes three significant limitations of competing methods: the need to collect seizure data, precisely mark seizure onset and offset times, and perform patient-specific parameter tuning for detector training. Keywords: seizure detection, novelty detection, one-class SVM, epilepsy, unsupervised learning 1</p><p>3 0.48833928 <a title="3-lsi-3" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>4 0.44234467 <a title="3-lsi-4" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<p>Author: David A. Ross, Richard S. Zemel</p><p>Abstract: Many perceptual models and theories hinge on treating objects as a collection of constituent parts. When applying these approaches to data, a fundamental problem arises: how can we determine what are the parts? We attack this problem using learning, proposing a form of generative latent factor model, in which each data dimension is allowed to select a different factor or part as its explanation. This approach permits a range of variations that posit different models for the appearance of a part. Here we provide the details for two such models: a discrete and a continuous one. Further, we show that this latent factor model can be extended hierarchically to account for correlations between the appearances of different parts. This permits modeling of data consisting of multiple categories, and learning these categories simultaneously with the parts when they are unobserved. Experiments demonstrate the ability to learn parts-based representations, and categories, of facial images and user-preference data. Keywords: parts, unsupervised learning, latent factor models, collaborative ﬁltering, hierarchical learning</p><p>5 0.41610041 <a title="3-lsi-5" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>Author: S. V. N. Vishwanathan, Nicol N. Schraudolph, Alex J. Smola</p><p>Abstract: This paper presents an online support vector machine (SVM) that uses the stochastic meta-descent (SMD) algorithm to adapt its step size automatically. We formulate the online learning problem as a stochastic gradient descent in reproducing kernel Hilbert space (RKHS) and translate SMD to the nonparametric setting, where its gradient trace parameter is no longer a coefﬁcient vector but an element of the RKHS. We derive efﬁcient updates that allow us to perform the step size adaptation in linear time. We apply the online SVM framework to a variety of loss functions, and in particular show how to handle structured output spaces and achieve efﬁcient online multiclass classiﬁcation. Experiments show that our algorithm outperforms more primitive methods for setting the gradient step size. Keywords: online SVM, stochastic meta-descent, structured output spaces</p><p>6 0.36436886 <a title="3-lsi-6" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>7 0.33243906 <a title="3-lsi-7" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.32684332 <a title="3-lsi-8" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>9 0.31388867 <a title="3-lsi-9" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.30861703 <a title="3-lsi-10" href="./jmlr-2006-Learning_Image_Components_for_Object_Recognition.html">47 jmlr-2006-Learning Image Components for Object Recognition</a></p>
<p>11 0.28774685 <a title="3-lsi-11" href="./jmlr-2006-A_Simulation-Based_Algorithm_for_Ergodic_Control_of_Markov_Chains_Conditioned_on_Rare_Events.html">7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</a></p>
<p>12 0.26905885 <a title="3-lsi-12" href="./jmlr-2006-Considering_Cost_Asymmetry_in_Learning_Classifiers.html">22 jmlr-2006-Considering Cost Asymmetry in Learning Classifiers</a></p>
<p>13 0.24621123 <a title="3-lsi-13" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.23317815 <a title="3-lsi-14" href="./jmlr-2006-%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">97 jmlr-2006- (Special Topic on Machine Learning for Computer Security)</a></p>
<p>15 0.23260249 <a title="3-lsi-15" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>16 0.21224116 <a title="3-lsi-16" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>17 0.20504183 <a title="3-lsi-17" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>18 0.20493215 <a title="3-lsi-18" href="./jmlr-2006-Computational_and_Theoretical_Analysis_of__Null_Space__and_Orthogonal_Linear_Discriminant_Analysis.html">21 jmlr-2006-Computational and Theoretical Analysis of  Null Space  and Orthogonal Linear Discriminant Analysis</a></p>
<p>19 0.19943725 <a title="3-lsi-19" href="./jmlr-2006-Learning_Minimum_Volume_Sets.html">48 jmlr-2006-Learning Minimum Volume Sets</a></p>
<p>20 0.19205062 <a title="3-lsi-20" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.013), (36, 0.068), (45, 0.023), (50, 0.037), (61, 0.484), (63, 0.056), (76, 0.014), (78, 0.013), (79, 0.016), (81, 0.044), (84, 0.022), (90, 0.032), (91, 0.018), (96, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84427822 <a title="3-lda-1" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>Author: Hichem Sahbi, Donald Geman</p><p>Abstract: We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly ﬁner subsets. The hierarchy is traversed coarse-to-ﬁne and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This “free network” is then perturbed, cell by cell, into another network, which is “graded” in two ways: ﬁrst, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-speciﬁc SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network. Keywords: statistical learning, hierarchy of classiﬁers, coarse-to-ﬁne computation, support vector machines, face detection</p><p>2 0.69145793 <a title="3-lda-2" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>Author: Thomas Kämpke</p><p>Abstract: Similarity of edge labeled graphs is considered in the sense of minimum squared distance between corresponding values. Vertex correspondences are established by isomorphisms if both graphs are of equal size and by subisomorphisms if one graph has fewer vertices than the other. Best ﬁt isomorphisms and subisomorphisms amount to solutions of quadratic assignment problems and are computed exactly as well as approximately by minimum cost ﬂow, linear assignment relaxations and related graph algorithms. Keywords: assignment problem, best approximation, branch and bound, inexact graph matching, model data base</p><p>3 0.35294393 <a title="3-lda-3" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>Author: Dana Angluin, Jiang Chen</p><p>Abstract: We consider the problem of learning a hypergraph using edge-detecting queries. In this model, the learner may query whether a set of vertices induces an edge of the hidden hypergraph or not. We show that an r-uniform hypergraph with m edges and n vertices is learnable with O(2 4r m · poly(r, log n)) queries with high probability. The queries can be made in O(min(2 r (log m + r)2 , (log m + r)3 )) rounds. We also give an algorithm that learns an almost uniform hypergraph of ∆ ∆ dimension r using O(2O((1+ 2 )r) · m1+ 2 · poly(log n)) queries with high probability, where ∆ is the difference between the maximum and the minimum edge sizes. This upper bound matches our ∆ lower bound of Ω(( m∆ )1+ 2 ) for this class of hypergraphs in terms of dependence on m. The 1+ 2 queries can also be made in O((1 + ∆) · min(2r (log m + r)2 , (log m + r)3 )) rounds. Keywords: query learning, hypergraph, multiple round algorithm, sampling, chemical reaction network</p><p>4 0.34279302 <a title="3-lda-4" href="./jmlr-2006-Learning_Image_Components_for_Object_Recognition.html">47 jmlr-2006-Learning Image Components for Object Recognition</a></p>
<p>Author: Michael W. Spratling</p><p>Abstract: In order to perform object recognition it is necessary to learn representations of the underlying components of images. Such components correspond to objects, object-parts, or features. Nonnegative matrix factorisation is a generative model that has been speciﬁcally proposed for ﬁnding such meaningful representations of image data, through the use of non-negativity constraints on the factors. This article reports on an empirical investigation of the performance of non-negative matrix factorisation algorithms. It is found that such algorithms need to impose additional constraints on the sparseness of the factors in order to successfully deal with occlusion. However, these constraints can themselves result in these algorithms failing to identify image components under certain conditions. In contrast, a recognition model (a competitive learning neural network algorithm) reliably and accurately learns representations of elementary image features without such constraints. Keywords: non-negative matrix factorisation, competitive learning, dendritic inhibition, object recognition</p><p>5 0.34201771 <a title="3-lda-5" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<p>Author: David A. Ross, Richard S. Zemel</p><p>Abstract: Many perceptual models and theories hinge on treating objects as a collection of constituent parts. When applying these approaches to data, a fundamental problem arises: how can we determine what are the parts? We attack this problem using learning, proposing a form of generative latent factor model, in which each data dimension is allowed to select a different factor or part as its explanation. This approach permits a range of variations that posit different models for the appearance of a part. Here we provide the details for two such models: a discrete and a continuous one. Further, we show that this latent factor model can be extended hierarchically to account for correlations between the appearances of different parts. This permits modeling of data consisting of multiple categories, and learning these categories simultaneously with the parts when they are unobserved. Experiments demonstrate the ability to learn parts-based representations, and categories, of facial images and user-preference data. Keywords: parts, unsupervised learning, latent factor models, collaborative ﬁltering, hierarchical learning</p><p>6 0.31090927 <a title="3-lda-6" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>7 0.3072862 <a title="3-lda-7" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.28978851 <a title="3-lda-8" href="./jmlr-2006-Pattern_Recognition_for__Conditionally_Independent_Data.html">73 jmlr-2006-Pattern Recognition for  Conditionally Independent Data</a></p>
<p>9 0.28946865 <a title="3-lda-9" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.28345993 <a title="3-lda-10" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>11 0.2829214 <a title="3-lda-11" href="./jmlr-2006-Incremental_Algorithms_for_Hierarchical_Classification.html">37 jmlr-2006-Incremental Algorithms for Hierarchical Classification</a></p>
<p>12 0.27898353 <a title="3-lda-12" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>13 0.2784507 <a title="3-lda-13" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<p>14 0.27797279 <a title="3-lda-14" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>15 0.27484432 <a title="3-lda-15" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>16 0.27406737 <a title="3-lda-16" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.27056411 <a title="3-lda-17" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.26991051 <a title="3-lda-18" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>19 0.26906088 <a title="3-lda-19" href="./jmlr-2006-Rearrangement_Clustering%3A_Pitfalls%2C_Remedies%2C_and_Applications.html">78 jmlr-2006-Rearrangement Clustering: Pitfalls, Remedies, and Applications</a></p>
<p>20 0.26561037 <a title="3-lda-20" href="./jmlr-2006-Superior_Guarantees_for_Sequential_Prediction_and_Lossless_Compression_via_Alphabet_Decomposition.html">90 jmlr-2006-Superior Guarantees for Sequential Prediction and Lossless Compression via Alphabet Decomposition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
