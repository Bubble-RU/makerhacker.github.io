<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-61" href="#">jmlr2006-61</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</h1>
<br/><p>Source: <a title="jmlr-2006-61-pdf" href="http://jmlr.org/papers/volume7/glasmachers06a/glasmachers06a.pdf">pdf</a></p><p>Author: Tobias Glasmachers, Christian Igel</p><p>Abstract: Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is signiﬁcantly faster. Keywords: working set selection, sequential minimal optimization, quadratic programming, support vector machines, large scale optimization</p><p>Reference: <a title="jmlr-2006-61-reference" href="../jmlr2006_reference/jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. [sent-9, score-0.499]
</p><p>2 We propose the maximum-gain working set selection algorithm for large scale quadratic programming. [sent-11, score-0.502]
</p><p>3 We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. [sent-14, score-0.465]
</p><p>4 For large data sets, this is typically done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. [sent-20, score-0.499]
</p><p>5 Recently, a very efﬁcient SMO-like (sequential minimal optimization using working sets of size 2, see Platt, 1999) decomposition algorithm was presented by Fan et al. [sent-22, score-0.499]
</p><p>6 Independent from this approach, we have developed a working set selection strategy sharing this basic idea but with a different focus, namely to minimize the number of kernel evaluations per iteration. [sent-25, score-0.593]
</p><p>7 2 Decomposition Algorithms Making SVM classiﬁcation applicable in case of large training data sets requires an algorithm for the solution of P that does not presuppose the ℓ(ℓ + 1)/2 independent entries of the symmetric matrix Q to ﬁt into working memory. [sent-64, score-0.485]
</p><p>8 In each iteration an active set or working set B(t) ⊂ {1, . [sent-68, score-0.446]
</p><p>9 The working set must always be larger than working set B the number of equality constraints to allow for a useful, feasible step. [sent-78, score-0.899]
</p><p>10 3 Working Set Selection Step 3 is crucial as the convergence of the decomposition algorithm depends strongly on the working set selection procedure. [sent-86, score-0.599]
</p><p>11 As the selection of the working set of a given size q that gives the largest improvement in a single iteration requires the knowledge of the full matrix Q, well working heuristics for choosing the variables using less information are needed. [sent-87, score-0.978]
</p><p>12 (t−1) +1 ∧ αi  (t−1) −1 ∧ αi  Now the MVP algorithm selects the working set B(t) = {b1 , b2 } using the rule b1 := argmax i∈I  b2 := argmin i∈J  ∂f (α) ∂αi ∂f yi (α) ∂αi yi  . [sent-108, score-0.458]
</p><p>13 SVMlight uses essentially the same working set selection method with the important difference that it is not restricted to working sets of size 2. [sent-117, score-0.915]
</p><p>14 (2005) propose a working set selection procedure which uses second order information. [sent-121, score-0.483]
</p><p>15 Thus, the selection schemes presented above have a freedom of choice for the selection of the ﬁrst working set. [sent-140, score-0.554]
</p><p>16 Both methods can proﬁt from our working set selection algorithm presented below, as the Core Vector Machine uses an inner SMO loop and LASVM is basically an online version of SMO. [sent-153, score-0.502]
</p><p>17 Maximum-Gain Working Set Selection Before we describe our new working set selection method, we recall how the quadratic problem restricted to a working set can be solved (cf. [sent-161, score-0.915]
</p><p>18 Picking the variable pair maximizing the functional gain while minimizing kernel evaluations—by reducing cache misses when looking up rows of Q—leads to the new working set selection strategy. [sent-164, score-0.864]
</p><p>19 1 Solving the Problem Restricted to the Working Set In every iteration of the decomposition algorithm all variables indexed by the inactive set N are ﬁxed and the problem P is restricted to the variables indexed by the working set B = {b1 , . [sent-166, score-0.611]
</p><p>20 3 Maximum-Gain Working Set Selection Now, a straightforward working set selection strategy is to look at all ℓ(ℓ − 1)/2 possible variable pairs, to evaluate the gain (3) for every one of them, and to select the best, that is, the one with maximum gain. [sent-234, score-0.617]
</p><p>21 As Q is in general too big to ﬁt into the working memory, expensive kernel function evaluations become necessary. [sent-237, score-0.485]
</p><p>22 Fortunately, modern SVM implementations use a considerable amount of working memory as a cache for the rows of Q computed in recent iterations. [sent-239, score-0.671]
</p><p>23 In all cases, this cache contains the two rows corresponding to the working set chosen in the most recent iteration, because they were needed for 1443  G LASMACHERS AND I GEL  the gradient update (2). [sent-240, score-0.686]
</p><p>24 This fact leads to the following maximum-gain working pair selection (MG) algorithm: Maximum-Gain Working Set Selection in step t 1 2 3 4  if t = 1 then select arbitrary working set B(1) = {b1 , b2 }, yb1 = yb2 else select pair B(t) ←  gB (α)  argmax B={b1 ,b2 } | b1 ∈B(t−1) ,b2 ∈{1,. [sent-241, score-0.922]
</p><p>25 In all following iterations, given the previous working set B(t−1) = {b1 , b2 }, the gain of all combinations {b1 , b} and {b2 , b} (b ∈ {1, . [sent-246, score-0.509]
</p><p>26 The complexity of the working set selection is linear in the number of training examples. [sent-250, score-0.509]
</p><p>27 These information are ignored by all existing working set selection strategies, albeit they are available for free, that is, without spending any additional computational effort. [sent-252, score-0.483]
</p><p>28 The maximum gain working pair selection can immediately be generalized to the class of maximum-gain working set selection algorithms (see Section 2. [sent-256, score-1.063]
</p><p>29 Under this term we want to subsume all working set selection strategies choosing variables according to a greedy policy with respect to the functional gain computed using cached matrix rows. [sent-258, score-0.777]
</p><p>30 In the following, we restrict ourselves to the selection of pairs of variables as working sets. [sent-259, score-0.483]
</p><p>31 In some SVM implementations, such as LIBSVM, the computation of the stopping condition is done using information provided during the working set selection. [sent-260, score-0.481]
</p><p>32 The only requirement for the algorithm to efﬁciently proﬁt from the kernel cache is that the cache always contains the two rows of the matrix Q that correspond to the previous working set. [sent-264, score-0.915]
</p><p>33 In order to inherit the convergence properties from MVP we introduce the hybrid maximum gain (HMG) working set selection algorithm. [sent-270, score-0.633]
</p><p>34 If in iteration t > 1 both variables indexed by the previous working set B(t−1) = {b1 , b2 } are no more than η · C, 0 < η ≪ 1, from the bounds, then the MVP ∇ algorithm is used. [sent-276, score-0.494]
</p><p>35 from the working set selection algorithm used in the current iteration. [sent-282, score-0.502]
</p><p>36 Then the working set selection algorithm becomes quite time consuming in comparison to the gradient update. [sent-300, score-0.537]
</p><p>37 As the number of iterations does not decrease accordingly, as we observed in real world applications, we recommend to use only the two rows of the matrix from the previous working set. [sent-301, score-0.571]
</p><p>38 A small change speeding up the working set selection is ﬁxing one element of the working set in every iteration. [sent-304, score-0.895]
</p><p>39 Note that this becomes necessary also for every gain computation during the working set selection. [sent-311, score-0.509]
</p><p>40 To keep the complexity of the working set selection linear in ℓ only one element new to the working set can be evaluated. [sent-312, score-0.895]
</p><p>41 The enlarged working set size may decrease the number of iterations required, but at the cost of the usage of an iterative solver. [sent-314, score-0.476]
</p><p>42 This should increase the speed of the SVM algorithm only on large problems with extremely complicated kernels, where the kernel matrix does not ﬁt into the cache and the kernel evaluations in every iteration take much longer than the working set selection. [sent-315, score-0.783]
</p><p>43 Convergence of the Algorithms In this section we discuss the convergence properties of the decomposition algorithm using MG and HMG working set selection. [sent-317, score-0.528]
</p><p>44 It will be the aim of this section to prove that α(∞) is the maximizer of f within the feasible region R (P ) if the decomposition algorithm is used with HMG working set selection. [sent-336, score-0.598]
</p><p>45 By analytically solving the problem restricted to a working set B we can compute the gain gB (α). [sent-342, score-0.529]
</p><p>46 If a feasible point α is not optimal then there exists a working set B on which it can be improved. [sent-352, score-0.487]
</p><p>47 This simply follows from the fact that there are working set selection policies based on which the decomposition algorithm is known to converge (Lin, 2001; Keerthi and Gilbert, 2002; Fan et al. [sent-353, score-0.594]
</p><p>48 From the fact that there are working set selection policies for which the decomposition algorithm converges to an optimum it follows that there exists a working set B on which α0 can be improved, which means gB (α0 ) > 0. [sent-380, score-1.03]
</p><p>49 The fat lines represent the feasible region for the 2-dimensional problem induced by the working set. [sent-385, score-0.531]
</p><p>50 2 Convergence of the Greedy Policy Before we look at the convergence of the MG algorithm, we use Theorem 4 by List and Simon (2004) to prove the convergence of the decomposition algorithm using the greedy policy with respect to the gain for the working set selection. [sent-388, score-0.719]
</p><p>51 Every 2-sparse witness of suboptimality induces a working set selection algorithm by B(t) := argmax CB (α(t−1) )  . [sent-391, score-0.632]
</p><p>52 Property (C3) follows from the fact that there exist working set selection strategies such that the decomposition method converges (Lin, 2001; Takahashi and Nishi, 2005; List and Simon, 2005). [sent-399, score-0.577]
</p><p>53 Let (α(t) )t∈N be a sequence of feasible points produced by the decomposition algorithm using the greedy working set selection policy. [sent-424, score-0.669]
</p><p>54 Proof For a feasible point α and a working set B the achievable gain is computed as gB (α). [sent-426, score-0.584]
</p><p>55 Thus, the decomposition method induced by the family (gB ) selects the working set resulting in the maximum gain, which is exactly the greedy policy. [sent-427, score-0.524]
</p><p>56 3 Convergence of the MG Algorithm Theorem 7 Given a feasible point α(t) for P and a previous working set B(t−1) of size 2 as a starting point. [sent-431, score-0.487]
</p><p>57 The MG algorithm may get stuck before reaching the optimum because it is restricted to reselect one element of the previous working set. [sent-434, score-0.525]
</p><p>58 Note that this is the result of the ﬁrst iteration starting from α(0) = (0, 0, 0, 0) greedily choosing the working set B(1) = {1, 3}. [sent-441, score-0.446]
</p><p>59 Thus, the maximum gain working pair algorithm cannot select B(2) = B and gets stuck although the point α(1) is not optimal. [sent-450, score-0.55]
</p><p>60 The following lemma deals with the speciﬁc property of the MG algorithm to reselect one element of the working set, that is, to select related working sets in consecutive iterations. [sent-457, score-0.898]
</p><p>61 Lemma 8 We consider P , a current non-optimal feasible point α and a previous working set B1 = {b1 , b2 }. [sent-459, score-0.487]
</p><p>62 If at least one of the variables αb1 and αb2 is free (not at the bounds 0 or C) then there exists a working set B2 related to B1 such that positive gain gB2 (α) > 0 can be achieved. [sent-460, score-0.509]
</p><p>63 In the point α(1) the gradient ∇ f (α(1) ) (which lies within the plane) has an angle of less than π/2 only with the horizontally drawn edge corresponding to the working set {2, 4}. [sent-471, score-0.447]
</p><p>64 A feasible point α is a counter example if it is not optimal and does not allow for positive gain on any working set related to B1 . [sent-476, score-0.663]
</p><p>65 (4)  Looking at the six possible working sets B we observe from Lemma 2 that we have to distinguish three cases for sub-problems induced by the working sets: • The current point α is at the bounds for a variable indexed by B and the points α + µ · wB lie within R (P ) only for µ ≤ 0. [sent-478, score-0.873]
</p><p>66 Lemma 10 We consider problem P , a sequence (α(t) )t∈N produced by the decomposition algorithm and the corresponding sequence of working sets (B(t) )t∈N . [sent-518, score-0.499]
</p><p>67 Then, no gain can be achieved in the limit point α(∞) using working sets B ∈ I. [sent-521, score-0.54]
</p><p>68 denote the set of working sets related to working sets in I. [sent-523, score-0.824]
</p><p>69 If the decomposition algorithm chooses MG working sets, no gain can be achieved in the limit point α(∞) using working sets B ∈ R. [sent-524, score-1.039]
</p><p>70 Let us assume that the limit point can be improved using a working set B ∈ R resulting in ε := gB (α(∞) ) > 0. [sent-542, score-0.443]
</p><p>71 The deﬁnition of I yields that there is a ˜ working set B ∈ I related to B which is chosen in an iteration t > max{t0 ,t1 }, t ∈ S. [sent-545, score-0.446]
</p><p>72 Then in iteration t + 1 ∈ S(+1) due to the MG policy the working set B (or another working set resulting in larger gain) is selected. [sent-546, score-0.899]
</p><p>73 We again distinguish two cases depending on the working set selection algorithm used just before the stopping condition is met. [sent-550, score-0.571]
</p><p>74 Now let us assume that the limit point can be improved using any other working set. [sent-559, score-0.443]
</p><p>75 Experiments The main purpose of our experiments is the comparison of different working set selection policies for large scale problems. [sent-565, score-0.507]
</p><p>76 Three SMO-like working set selection policies were compared, namely the LIBSVM-2. [sent-573, score-0.507]
</p><p>77 If not stated otherwise, the SVM was given 40 MB of working memory to store matrix rows. [sent-582, score-0.46]
</p><p>78 The comparison of the working set selection algorithms involves one major difﬁculty: The stopping criteria are different. [sent-595, score-0.552]
</p><p>79 Whether a certain ordering leads to fast or slow convergence is dependent on the working set selection method used. [sent-610, score-0.512]
</p><p>80 Besides the overall performance of the working set selection strategies we investigated the inﬂuence of a variety of conditions. [sent-615, score-0.509]
</p><p>81 It is a hopeless approach to adapt the cache size in order to ﬁt larger parts of the kernel matrix into working memory. [sent-648, score-0.657]
</p><p>82 The matrix Q requires about 81 MB of working memory. [sent-683, score-0.44]
</p><p>83 664  Table 2: Comparison of the number of iterations of the decomposition algorithm and training times for the different working set selection approaches. [sent-719, score-0.66]
</p><p>84 Please note that according to the numerous implementation differences these experiments do not provide a fair comparison between SMO-like methods and decomposition algorithms using larger working sets. [sent-728, score-0.48]
</p><p>85 643  Table 3: Iterations, runtime and objective function value of the SVMlight experiments with working set size q = 10. [sent-733, score-0.514]
</p><p>86 8 outperformed HMG have one important property in common, namely, that it is a bad idea to reselect an element of the previous working set. [sent-741, score-0.44]
</p><p>87 This is true when after most iterations both coordinates indexed by the working set are already optimal. [sent-742, score-0.505]
</p><p>88 The temporary reduction of the problem restricts working set selection algorithms to a subset of possible choices. [sent-756, score-0.483]
</p><p>89 We repeated our experiments with the LIBSVM shrinking heuristics turned off to reveal the relation between the working set selection algorithms and shrinking, see Table 4. [sent-758, score-0.61]
</p><p>90 The experiments show that the inﬂuence of the shrinking algorithm on the different working set selection policies is highly task dependent. [sent-759, score-0.608]
</p><p>91 8 766 s 734 s 649 s 547 s  HMG 656 s 633 s 583 s 555 s  Table 5: The training time for the connect-4 task for the different working set selection algorithms depending on the cache size. [sent-828, score-0.681]
</p><p>92 6 Number of Matrix Rows Considered In the deﬁnition of the HMG algorithm we restrict ourselves to computing the gain using the two cached matrix rows corresponding to the previous working set. [sent-830, score-0.701]
</p><p>93 We computed the gain using only one matrix row corresponding to one element of the working set. [sent-833, score-0.537]
</p><p>94 This policy reduces the time required for the working set selection by about 50%. [sent-835, score-0.524]
</p><p>95 This test gives us the minimum number of iterations the HMG working pair algorithm can achieve, as it is the strategy using all information available. [sent-862, score-0.532]
</p><p>96 It thus provides a bound on the performance of possible extensions of the algorithm using more than two cached matrix rows to determine the working pair. [sent-863, score-0.604]
</p><p>97 This is a considerable amount which was bought dearly using all cached matrix rows for the working set selection. [sent-880, score-0.585]
</p><p>98 Conclusion The time needed by a decomposition algorithm to solve the support vector machine (SVM) optimization problem up to a given accuracy depends highly on the working set selection. [sent-886, score-0.499]
</p><p>99 In our experiments with large data sets, that is, when training time really matters, our new hybrid maximum-gain working set selection (HMG) saved a lot of time compared to the latest second order selection algorithm. [sent-887, score-0.634]
</p><p>100 In particular, an elaborate cooperation between the kernel cache strategy and the working set selection algorithm is promising to increase the efﬁciency of future algorithms. [sent-902, score-0.756]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hmg', 0.488), ('working', 0.412), ('wb', 0.39), ('mg', 0.253), ('cache', 0.172), ('gb', 0.15), ('gel', 0.141), ('lasmachers', 0.141), ('mvp', 0.141), ('ain', 0.111), ('orking', 0.111), ('fb', 0.108), ('runtime', 0.102), ('gain', 0.097), ('aximum', 0.092), ('libsvm', 0.085), ('shrinking', 0.082), ('election', 0.08), ('qb', 0.08), ('counter', 0.079), ('cached', 0.078), ('feasible', 0.075), ('mb', 0.073), ('svm', 0.071), ('selection', 0.071), ('stopping', 0.069), ('decomposition', 0.068), ('rows', 0.067), ('iterations', 0.064), ('takahashi', 0.056), ('witness', 0.056), ('box', 0.054), ('simon', 0.051), ('cb', 0.049), ('lling', 0.049), ('bq', 0.048), ('nishi', 0.047), ('suboptimality', 0.047), ('svmlight', 0.046), ('vp', 0.046), ('kernel', 0.045), ('mnist', 0.044), ('keerthi', 0.043), ('policy', 0.041), ('smo', 0.04), ('ful', 0.04), ('strategy', 0.037), ('gilbert', 0.036), ('list', 0.035), ('gradient', 0.035), ('iteration', 0.034), ('face', 0.032), ('compactness', 0.032), ('minors', 0.032), ('convergent', 0.031), ('limit', 0.031), ('saved', 0.03), ('hyperplane', 0.03), ('indexed', 0.029), ('convergence', 0.029), ('yb', 0.029), ('neuroinformatik', 0.028), ('reselect', 0.028), ('tobias', 0.028), ('zb', 0.028), ('matrix', 0.028), ('stops', 0.028), ('evaluations', 0.028), ('lemma', 0.027), ('uence', 0.027), ('argmax', 0.027), ('strategies', 0.026), ('vb', 0.026), ('training', 0.026), ('hybrid', 0.024), ('fan', 0.024), ('policies', 0.024), ('comparability', 0.024), ('christian', 0.024), ('glasmachers', 0.024), ('turned', 0.024), ('optimum', 0.024), ('greedy', 0.024), ('region', 0.024), ('stuck', 0.022), ('violating', 0.022), ('precomputed', 0.021), ('player', 0.021), ('heuristics', 0.021), ('restricted', 0.02), ('caching', 0.02), ('induced', 0.02), ('memory', 0.02), ('optimality', 0.019), ('algorithm', 0.019), ('endangered', 0.019), ('octahedron', 0.019), ('qbq', 0.019), ('rub', 0.019), ('unshrinking', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="61-tfidf-1" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tobias Glasmachers, Christian Igel</p><p>Abstract: Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is signiﬁcantly faster. Keywords: working set selection, sequential minimal optimization, quadratic programming, support vector machines, large scale optimization</p><p>2 0.13387342 <a title="61-tfidf-2" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Luca Zanni, Thomas Serafini, Gaetano Zanghirati</p><p>Abstract: Parallel software for solving the quadratic program arising in training support vector machines for classiﬁcation problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-ofthe-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes. Keywords: support vector machines, large scale quadratic programs, decomposition techniques, gradient projection methods, parallel computation</p><p>3 0.12387437 <a title="61-tfidf-3" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>Author: Don Hush, Patrick Kelly, Clint Scovel, Ingo Steinwart</p><p>Abstract: We describe polynomial–time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classiﬁers. These algorithms employ a two–stage process where the ﬁrst stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an √ √ approximate dual solution with accuracy (2 2Kn + 8 λ)−2 λε2 to an approximate primal solution p with accuracy ε p where n is the number of data samples, Kn is the maximum kernel value over the data and λ > 0 is the SVM regularization parameter. For the ﬁrst stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for τ–rate certifying decomposition algorithms we establish the optimality of τ = 1/(n − 1). In addition we extend the recent τ = 1/(n − 1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the τ = 1/(n − 1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the τ–rate certifying property of these algorithms to produce new stopping rules that are computationally efﬁcient and that guarantee a speciﬁed accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classiﬁcation problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n2 ) bound on the overall run time for the ﬁrst stage. Combining the ﬁrst and second stages gives an overall run time of O(n2 (c</p><p>4 0.088731147 <a title="61-tfidf-4" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>5 0.064353652 <a title="61-tfidf-5" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Kristin P. Bennett, Emilio Parrado-Hernández</p><p>Abstract: The ﬁelds of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semideﬁnite, and semi-inﬁnite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for speciﬁc classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms. Keywords: machine learning, mathematical programming, convex optimization</p><p>6 0.059486132 <a title="61-tfidf-6" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>7 0.058800567 <a title="61-tfidf-7" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.054715231 <a title="61-tfidf-8" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.054375414 <a title="61-tfidf-9" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.047345527 <a title="61-tfidf-10" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>11 0.045373827 <a title="61-tfidf-11" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.04516257 <a title="61-tfidf-12" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>13 0.039281327 <a title="61-tfidf-13" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.039011478 <a title="61-tfidf-14" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.036711331 <a title="61-tfidf-15" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>16 0.035333026 <a title="61-tfidf-16" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>17 0.030966878 <a title="61-tfidf-17" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>18 0.030545512 <a title="61-tfidf-18" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>19 0.030058177 <a title="61-tfidf-19" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>20 0.029623959 <a title="61-tfidf-20" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.198), (1, -0.103), (2, 0.077), (3, 0.101), (4, 0.12), (5, 0.086), (6, -0.067), (7, 0.005), (8, 0.018), (9, -0.106), (10, 0.149), (11, -0.029), (12, 0.32), (13, -0.039), (14, 0.147), (15, 0.158), (16, 0.006), (17, 0.168), (18, -0.039), (19, -0.011), (20, -0.046), (21, 0.255), (22, 0.096), (23, -0.074), (24, -0.021), (25, -0.023), (26, -0.092), (27, 0.072), (28, -0.119), (29, -0.061), (30, 0.079), (31, 0.0), (32, 0.039), (33, 0.064), (34, 0.043), (35, 0.151), (36, -0.079), (37, 0.131), (38, 0.061), (39, -0.032), (40, -0.05), (41, -0.1), (42, -0.015), (43, 0.025), (44, 0.042), (45, -0.003), (46, 0.05), (47, 0.01), (48, -0.05), (49, 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94992441 <a title="61-lsi-1" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tobias Glasmachers, Christian Igel</p><p>Abstract: Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is signiﬁcantly faster. Keywords: working set selection, sequential minimal optimization, quadratic programming, support vector machines, large scale optimization</p><p>2 0.75690246 <a title="61-lsi-2" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Luca Zanni, Thomas Serafini, Gaetano Zanghirati</p><p>Abstract: Parallel software for solving the quadratic program arising in training support vector machines for classiﬁcation problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-ofthe-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes. Keywords: support vector machines, large scale quadratic programs, decomposition techniques, gradient projection methods, parallel computation</p><p>3 0.71421188 <a title="61-lsi-3" href="./jmlr-2006-QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines.html">76 jmlr-2006-QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines</a></p>
<p>Author: Don Hush, Patrick Kelly, Clint Scovel, Ingo Steinwart</p><p>Abstract: We describe polynomial–time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classiﬁers. These algorithms employ a two–stage process where the ﬁrst stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an √ √ approximate dual solution with accuracy (2 2Kn + 8 λ)−2 λε2 to an approximate primal solution p with accuracy ε p where n is the number of data samples, Kn is the maximum kernel value over the data and λ > 0 is the SVM regularization parameter. For the ﬁrst stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for τ–rate certifying decomposition algorithms we establish the optimality of τ = 1/(n − 1). In addition we extend the recent τ = 1/(n − 1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the τ = 1/(n − 1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the τ–rate certifying property of these algorithms to produce new stopping rules that are computationally efﬁcient and that guarantee a speciﬁed accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classiﬁcation problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n2 ) bound on the overall run time for the ﬁrst stage. Combining the ﬁrst and second stages gives an overall run time of O(n2 (c</p><p>4 0.36779243 <a title="61-lsi-4" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller</p><p>Abstract: Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efﬁcient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network trafﬁc, can be foreseen. Keywords: incremental SVM, online learning, drug discovery, intrusion detection</p><p>5 0.32923323 <a title="61-lsi-5" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>Author: Pieter Abbeel, Daphne Koller, Andrew Y. Ng</p><p>Abstract: We study the computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Importantly, unlike standard maximum likelihood estimation algorithms, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. In addition to our main result, we show that the sample complexity of parameter learning in graphical models has an O(1) dependence on the number of variables in the model when using the KL-divergence normalized by the number of variables as the performance criterion.1 Keywords: probabilistic graphical models, parameter and structure learning, factor graphs, Markov networks, Bayesian networks</p><p>6 0.28962517 <a title="61-lsi-6" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.27675474 <a title="61-lsi-7" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.26574421 <a title="61-lsi-8" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.23926592 <a title="61-lsi-9" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.23801702 <a title="61-lsi-10" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.23257431 <a title="61-lsi-11" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>12 0.23231687 <a title="61-lsi-12" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.21617813 <a title="61-lsi-13" href="./jmlr-2006-Infinite-%CF%83_Limits_For_Tikhonov_Regularization.html">40 jmlr-2006-Infinite-σ Limits For Tikhonov Regularization</a></p>
<p>14 0.20404431 <a title="61-lsi-14" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>15 0.19261339 <a title="61-lsi-15" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>16 0.18965353 <a title="61-lsi-16" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>17 0.18922195 <a title="61-lsi-17" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>18 0.18779773 <a title="61-lsi-18" href="./jmlr-2006-Action_Elimination_and_Stopping_Conditions_for_the_Multi-Armed_Bandit_and_Reinforcement_Learning_Problems.html">10 jmlr-2006-Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems</a></p>
<p>19 0.18280268 <a title="61-lsi-19" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>20 0.16697516 <a title="61-lsi-20" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.328), (8, 0.02), (36, 0.065), (45, 0.016), (50, 0.042), (61, 0.014), (63, 0.076), (76, 0.053), (78, 0.031), (81, 0.032), (84, 0.033), (90, 0.038), (91, 0.047), (96, 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72607476 <a title="61-lda-1" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Tobias Glasmachers, Christian Igel</p><p>Abstract: Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is signiﬁcantly faster. Keywords: working set selection, sequential minimal optimization, quadratic programming, support vector machines, large scale optimization</p><p>2 0.43416473 <a title="61-lda-2" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>3 0.42634094 <a title="61-lda-3" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram. Keywords: spectral clustering, blind source separation, computational auditory scene analysis</p><p>4 0.42537928 <a title="61-lda-4" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>Author: Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou</p><p>Abstract: We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the ﬁrst time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction. html. Keywords: transduction, transductive SVMs, semi-supervised learning, CCCP</p><p>5 0.41995317 <a title="61-lda-5" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Matthias Heiler, Christoph Schnörr</p><p>Abstract: We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. We show that for ordinary NMF, our approach performs as well as existing stateof-the-art algorithms, while for sparsity-constrained NMF, as recently proposed by P. O. Hoyer in JMLR 5 (2004), it outperforms previous methods. In addition, we show how to extend NMF learning within the same optimization framework in order to make use of class membership information in supervised learning problems. Keywords: non-negative matrix factorization, second-order cone programming, sequential convex optimization, reverse-convex programming, sparsity</p><p>6 0.41443157 <a title="61-lda-6" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>7 0.41426712 <a title="61-lda-7" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>8 0.41425419 <a title="61-lda-8" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.41410589 <a title="61-lda-9" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.41234356 <a title="61-lda-10" href="./jmlr-2006-Accurate_Error_Bounds_for_the_Eigenvalues_of_the_Kernel_Matrix.html">9 jmlr-2006-Accurate Error Bounds for the Eigenvalues of the Kernel Matrix</a></p>
<p>11 0.41190106 <a title="61-lda-11" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>12 0.40749726 <a title="61-lda-12" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.40730849 <a title="61-lda-13" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.40685317 <a title="61-lda-14" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>15 0.40486461 <a title="61-lda-15" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>16 0.40388101 <a title="61-lda-16" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>17 0.40085727 <a title="61-lda-17" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>18 0.39567289 <a title="61-lda-18" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>19 0.39349216 <a title="61-lda-19" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>20 0.39236462 <a title="61-lda-20" href="./jmlr-2006-Estimation_of_Gradients_and_Coordinate_Covariation_in_Classification.html">29 jmlr-2006-Estimation of Gradients and Coordinate Covariation in Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
