<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>97 jmlr-2006- (Special Topic on Machine Learning for Computer Security)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-97" href="#">jmlr2006-97</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>97 jmlr-2006- (Special Topic on Machine Learning for Computer Security)</h1>
<br/><p>Source: <a title="jmlr-2006-97-pdf" href="http://jmlr.org/papers/volume7/bratko06a/bratko06a.pdf">pdf</a></p><p>Author: Charles V. Wright, Fabian Monrose, Gerald M. Masson</p><p>Abstract: Several fundamental security mechanisms for restricting access to network resources rely on the ability of a reference monitor to inspect the contents of trafﬁc as it traverses the network. However, with the increasing popularity of cryptographic protocols, the traditional means of inspecting packet contents to enforce security policies is no longer a viable approach as message contents are concealed by encryption. In this paper, we investigate the extent to which common application protocols can be identiﬁed using only the features that remain intact after encryption—namely packet size, timing, and direction. We ﬁrst present what we believe to be the ﬁrst exploratory look at protocol identiﬁcation in encrypted tunnels which carry trafﬁc from many TCP connections simultaneously, using only post-encryption observable features. We then explore the problem of protocol identiﬁcation in individual encrypted TCP connections, using much less data than in other recent approaches. The results of our evaluation show that our classiﬁers achieve accuracy greater than 90% for several protocols in aggregate trafﬁc, and, for most protocols, greater than 80% when making ﬁne-grained classiﬁcations on single connections. Moreover, perhaps most surprisingly, we show that one can even estimate the number of live connections in certain classes of encrypted tunnels to within, on average, better than 20%. Keywords: trafﬁc classiﬁcation, hidden Markov models, network security</p><p>Reference: <a title="jmlr-2006-97-reference" href="../jmlr2006_reference/jmlr-2006-%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we investigate the extent to which common application protocols can be identiﬁed using only the features that remain intact after encryption—namely packet size, timing, and direction. [sent-8, score-0.505]
</p><p>2 We ﬁrst present what we believe to be the ﬁrst exploratory look at protocol identiﬁcation in encrypted tunnels which carry trafﬁc from many TCP connections simultaneously, using only post-encryption observable features. [sent-9, score-0.866]
</p><p>3 We then explore the problem of protocol identiﬁcation in individual encrypted TCP connections, using much less data than in other recent approaches. [sent-10, score-0.547]
</p><p>4 Moreover, perhaps most surprisingly, we show that one can even estimate the number of live connections in certain classes of encrypted tunnels to within, on average, better than 20%. [sent-12, score-0.49]
</p><p>5 Even more problematic for such trafﬁc characterization techniques is the fact that with the increased use of cryptographic protocols such as SSL (Rescorla, 2000) and SSH (Ylonen, 1996), fewer and fewer packets in legitimate trafﬁc become available for inspection. [sent-27, score-0.572]
</p><p>6 Here we investigate the extent to which common Internet application protocols remain distinguishable even when packet payloads and TCP headers have been stripped away, leaving only extremely lean data which includes nothing more than the packets’ timing, size, and direction. [sent-40, score-0.526]
</p><p>7 We begin our analysis in §3 by exploring protocol recognition techniques for trafﬁc aggregates where all ﬂows carry the same application protocol. [sent-41, score-0.466]
</p><p>8 Since we do not have access to packet payloads in these traces, we do not attempt to determine the “ground truth” of which connections truly belong to which protocols. [sent-54, score-0.481]
</p><p>9 We encode the packet’s direction in the sign bit of the packet’s size, so that packets sent from server to client have size less than zero and those from client to server have size greater than zero. [sent-58, score-0.58]
</p><p>10 Trafﬁc Classiﬁcation in Aggregate Encrypted Trafﬁc Here we investigate the problem of determining the application protocol in use in aggregate trafﬁc composed of several TCP connections which all employ the same application protocol. [sent-64, score-0.74]
</p><p>11 The techniques we develop here can be used to quickly and efﬁciently infer the nature of the application protocol used in aggregate trafﬁc without demultiplexing or reassembling the individual ﬂows from the aggregate. [sent-67, score-0.49]
</p><p>12 To evaluate the techniques developed in this section, we assemble trafﬁc aggregates for each protocol using several TCP connections extracted from the GMU data as described in §2. [sent-73, score-0.716]
</p><p>13 For each 10-minute trace and each protocol, we select all connections for the given protocol in the given trace, and interleave their packets into a single uniﬁed stream, sorted in order of arrival on the link. [sent-74, score-0.988]
</p><p>14 Currently, we group packets into four types; any packet is classiﬁed as either small (i. [sent-83, score-0.529]
</p><p>15 In general, when we consider M different packet types, this splitting and counting procedure yields a vector-valued count of packets nt = nt1 , nt2 , . [sent-88, score-0.605]
</p><p>16 We then assemble single-protocol aggregates from this day’s traces for each protocol in the study, yielding a list of vectors n1 , n2 , . [sent-100, score-0.528]
</p><p>17 Larger values of s mean that each epoch includes packets from a greater number of connections, so it is not surprising that, as s increases, the mix of packets observed in a given epoch approaches the mix of packets the protocol tends to produce overall. [sent-113, score-1.45]
</p><p>18 On the other hand, smaller values of s allow us to analyze shorter traces and should make it more difﬁcult for an adversary to successfully masquerade one protocol as another. [sent-114, score-0.48]
</p><p>19 Given a sequence of packets corresponding to a trafﬁc aggregate, we begin by preprocessing it into a sequence of vectors of packet counts and normalizing each vector just as we did for each of the aggregates in the training set. [sent-118, score-0.6]
</p><p>20 This classiﬁer is able to correctly recognize 100% of the aggregates for several of the protocols with many different values of k, leading us to believe that the vectors of packet counts observed for each of these protocols tend to cluster together into perhaps a few large groups. [sent-125, score-0.85]
</p><p>21 The results in this section show that, by using the Kullback-Leibler distance to construct a kNearest Neighbor classiﬁer for short slices of time, we can then build a classiﬁer for longer traces which performs quite well on aggregate trafﬁc where only a single application protocol is involved. [sent-127, score-0.587]
</p><p>22 However, we may not always be able to assume that all ﬂows in the aggregate carry the same appli2749  W RIGHT, M ONROSE AND M ASSON  protocol HTTP HTTPS AIM SMTP-in SMTP-out FTP SSH Telnet  1-NN TD FD 100. [sent-128, score-0.49]
</p><p>23 If we are concerned only with detecting instances of a given target protocol (or indeed, a set of target protocols), we simply label the vectors in the training set based on whether they contain an instance of the target protocol(s). [sent-198, score-0.442]
</p><p>24 We ﬂag the aggregate as an instance of the target protocol if and only if the percentage of the time slices for which the classiﬁer returns True is above some threshold. [sent-200, score-0.49]
</p><p>25 Figure 2 shows the detection rates for the k-Nearest Neighbor-based multi-ﬂow protocol detectors for AIM, HTTP, FTP, and SMTP-in, with k = 7. [sent-202, score-0.614]
</p><p>26 In each graph, the x-axis represents the threshold level, and the plots show the probability that the given detector, when set with a particular threshold, ﬂags instances of each protocol in the study. [sent-203, score-0.442]
</p><p>27 Overall, the multi-ﬂow protocol detectors seem to perform quite well detecting broad classes of protocol behavior. [sent-204, score-0.926]
</p><p>28 The FTP detector’s rates (d) show that, when observed in a multi-ﬂow aggregate, the more interactive protocols exhibit very similar on-the-wire behaviors; after FTP itself, the FTP detector is most likely to ﬂag instances of AIM, SSH, and Telnet. [sent-208, score-0.48]
</p><p>29 Even if the connections use SSL or TLS to encrypt their packets, the administrator could perform more in-depth analysis to determine the application protocol used in each individual TCP connection. [sent-216, score-0.719]
</p><p>30 Machine Learning Techniques for the Analysis of Single Flows We now relax the earlier assumption that all TCP connections in a given set carry the same application protocol, but retain the assumption that the individual TCP connections can be demultiplexed. [sent-219, score-0.5]
</p><p>31 We present an approach based on building statistical models for the sequence of packets produced by each protocol of interest, and then use these models to identify the protocol in use in new TCP connections. [sent-221, score-1.134]
</p><p>32 Identifying protocols in this setting is fairly difﬁcult due to the fact that certain application protocols exhibit more than one typical behavior pattern (e. [sent-224, score-0.548]
</p><p>33 , λn , that correspond to the protocols of interest (say AIM, SMTP, FTP), the goal is to pick the model that best describes the sequences of encrypted packets observed in the different connections. [sent-237, score-0.725]
</p><p>34 Given a set of connections for training, we begin by constructing an initial model (see Figure 4) such that the length of the chain of states in the 2752  O N I NFERRING A PPLICATION P ROTOCOL B EHAVIORS  model is equal to the average length (in packets) of the connections in the training set. [sent-247, score-0.5]
</p><p>35 To allow for variations between the observed sequences of packets in connections of the same protocol, the model has two additional states for each position in the chain. [sent-260, score-0.572]
</p><p>36 Transitions from the Delete state in each column to Insert state in the next column allow for a normal packet at the given position to be removed and replaced with a packet which does not ﬁt the proﬁle. [sent-263, score-0.462]
</p><p>37 In our case, the addition of a second match state per position was intended to allow the model to better represent the correlation between successive packets in TCP connections (Wright et al. [sent-273, score-0.548]
</p><p>38 Since TCP uses sliding windows and positive acknowledgments to achieve reliable data transfer, the direction of a packet is often closely correlated (either positively or negatively) to the direction of the previous packet in the connection. [sent-275, score-0.462]
</p><p>39 Therefore, the Server Match state matches only packets observed traveling from the server to the client, and the Client Match state matches packets traveling in the opposite direction. [sent-276, score-0.72]
</p><p>40 For example, a transition from a Client Match state to a Server Match state indicates that a typical packet (for the given protocol) was observed traveling from the client to the server, followed by a similarly typical packet on its way from the server to the client. [sent-277, score-0.624]
</p><p>41 In practice, the Insert states represent duplicate packets and retransmissions, while the Delete states account for packets lost in the network or dropped by the detector. [sent-278, score-0.632]
</p><p>42 Our ﬁrst such classiﬁer assigns protocol labels to sequences according to the principle of maximum likelihood. [sent-287, score-0.442]
</p><p>43 For a given experiment, we select one day for use as a 2754  O N I NFERRING A PPLICATION P ROTOCOL B EHAVIORS  protocol AIM SMTP-out SMTP-in HTTP HTTPS FTP SSH Telnet  micro-level TD FD 80. [sent-305, score-0.472]
</p><p>44 From this day’s traces, we randomly select approximately 400 connections 2 of each protocol and use these to build our proﬁle HMMs. [sent-338, score-0.703]
</p><p>45 Then, for each of the remaining 8 days, we randomly select approximately 400 connections for each protocol and use the model-based classiﬁer to assign class labels to each of them. [sent-339, score-0.668]
</p><p>46 As a result, we believe the detection rates presented here could be improved for a given network by including the relative frequencies of the protocols (i. [sent-342, score-0.44]
</p><p>47 With the exception of the connections for FTP and SSH, the Viterbi classiﬁer correctly identiﬁes the protocol more than 73% of the time. [sent-353, score-0.668]
</p><p>48 As such, we also report the true detection (TD) and false detection (FD) rates when we group protocols into the following equivalence classes: {[AIM], [HTTP, HTTPS], [SMTP − in, SMTP − out], [FTP], [SSH, Telnet]} where the latter class represents the grouping of the interactive protocols. [sent-358, score-0.521]
</p><p>49 We ﬁnd the Viterbi classiﬁer to be slightly more accurate than the Maximum Likelihood classiﬁer in almost every case,3 but the protocol whose recognition rates are most improved with the Viterbi method is SSH. [sent-359, score-0.464]
</p><p>50 We choose 400 because it is the largest size for which we can select the same number of instances of each protocol on every day in the data set. [sent-361, score-0.472]
</p><p>51 We therefore split the packets into two sets: those sent from the client to the server, and those sent from server to client. [sent-403, score-0.439]
</p><p>52 4 A Protocol Detector for Single Flows In this section, we evaluate the suitability of our proﬁle HMMs for a slightly different task: identifying the TCP connections that belong to a given protocol of interest. [sent-424, score-0.668]
</p><p>53 2, such a detector could be used, for example, by a network administrator to detect policy violations by a user running a prohibited application (such as instant messenger) or remotely accessing a rogue SMTP server over an encrypted connection. [sent-426, score-0.449]
</p><p>54 2, and have the system ﬂag a detection when a sequence is classiﬁed as belonging to the protocol of interest. [sent-428, score-0.502]
</p><p>55 While we believe this cost to be warranted when we are interested in determining which protocol generated what connections in the network, at other times one may simply be interested in determining whether or not connections belong to a target protocol. [sent-433, score-0.918]
</p><p>56 As in the previous sections, we build a proﬁle HMM λP for the target protocol P. [sent-437, score-0.453]
</p><p>57 Intuitively, this model is intended to represent the packets we expect to see in background trafﬁc, so we estimate its parameters using connections from all protocols in the study. [sent-440, score-0.822]
</p><p>58 In our simplest (and most efﬁcient) protocol detector, a test connection whose log odds score falls above the threshold is immediately ﬂagged as an instance of the given protocol. [sent-453, score-0.476]
</p><p>59 The goal, of course, is to build detectors which simultaneously achieve high detection rates for their target protocols and near-zero detection rates for the other, non-target protocols. [sent-454, score-0.635]
</p><p>60 To empirically evaluate the extent to which our protocol detectors are able to do so, we run each detector on a number of instances of each protocol. [sent-455, score-0.611]
</p><p>61 We then extract 400 connections from the training set for each of the protocols we want to detect, and use these connections to build one proﬁle HMM for each protocol and one unigram HMM for the noise model. [sent-458, score-1.227]
</p><p>62 Similarly, we randomly select 400 connections from the holdout set for each protocol, and use these to determine thresholds for a range of detection rates between 1% and 99% for each protocol detector. [sent-459, score-0.83]
</p><p>63 Finally, we randomly select 400 connections of each protocol from the test set, and run each protocol detector on all 3200 test connections. [sent-460, score-1.213]
</p><p>64 Similarly, detectors for HTTPS, SMTP and FTP built on this basic design are also able to distinguish their respective protocols from most of the other protocols in our test set, with reasonable accuracy. [sent-467, score-0.614]
</p><p>65 This is not surprising, since FTP and SMTP share a similar “numeric code and status message” format and generate sequences of packets that look very similar, even when examining packet payloads. [sent-470, score-0.553]
</p><p>66 Nevertheless, we are able to improve our initial false positive rates for these two protocols using a technique based on iteratively reﬁning of the set of protocols that we suspect a connection might belong to. [sent-472, score-0.628]
</p><p>67 To build an improved protocol detector, we construct proﬁle HMMs not only for the target protocol, but also for any other similarly-behaving protocols with which it is frequently confused. [sent-473, score-0.727]
</p><p>68 2) with the models for the frequently-confused protocols to identify the other (non-target) protocol most likely to have generated the sequence of packets in the connection. [sent-478, score-0.99]
</p><p>69 Only if the model for the target protocol produces a higher Viterbi path probability than this protocol’s model, do we ﬂag the connection as an instance of the target protocol. [sent-479, score-0.452]
</p><p>70 Tracking the Number of Live Connections in Encrypted Tunnels In §3, we showed that it is often possible to determine the application protocol used in aggregate trafﬁc without demultiplexing or reassembling the TCP connections in the aggregate. [sent-488, score-0.74]
</p><p>71 We now turn our attention to the case where we cannot demultiplex the ﬂows or determine which packets in the aggregate belong to which ﬂows, as is the case when aggregate trafﬁc is encrypted at the network layer using IPsec Encapsulating Security Payload (Kent and Atkinson, 1998) or SSH tunneling. [sent-490, score-0.639]
</p><p>72 Speciﬁcally, we develop a model-based technique which enables us to accurately track the number of connections in a network-layer tunnel which carries trafﬁc for only a single application protocol. [sent-491, score-0.433]
</p><p>73 As an example of this scenario, consider a proxy server which listens for clients’ requests on one edge network and forwards them through an encrypted tunnel across the Internet to a set of servers on another edge network. [sent-492, score-0.455]
</p><p>74 Despite our inability to demultiplex the ﬂows inside such a tunnel, the technique developed in §3 still enables us to correctly identify the application protocol much of the time. [sent-493, score-0.45]
</p><p>75 We now go on to show how we can, given the application protocol, derive an estimate for the number of connections in the tunnel at each point in time. [sent-494, score-0.433]
</p><p>76 Assumption 3 For each packet type m, each connection in the tunnel generates packets of type m according to a homogeneous Poisson process with constant rate γm , which is determined by the application protocol in use in the connection. [sent-501, score-1.164]
</p><p>77 Implications It follows from Assumption 1 and Assumption 2 that, in each timeslice, the number of connections in the tunnel will have a Gaussian distribution with mean equal to the number of connections in the tunnel during the previous timeslice. [sent-502, score-0.866]
</p><p>78 Accordingly, the set of packet rates {γm } provides a sufﬁciently descriptive 2760  O N I NFERRING A PPLICATION P ROTOCOL B EHAVIORS  model for the given application protocol (in this scenario). [sent-504, score-0.695]
</p><p>79 We use these observations in the following section to build models that enable us to extract information about the number of tunneled TCP connections from the observed sequence of packet arrivals. [sent-505, score-0.516]
</p><p>80 1 A Model for Multi-Flow Tunnels To track the number of connections in a multi-ﬂow tunnel, we build a statistical model which relates the stochastic process describing the number of live connections to the stochastic process of packet arrivals. [sent-507, score-0.808]
</p><p>81 Here, the hidden state transition process describes the changing number of connections Nt in the tunnel, and the symbol output process describes the arrival of packets on the link. [sent-509, score-0.57]
</p><p>82 States in the HMM therefore correspond to connection counts, and the event that the HMM is in state i at time t corresponds to the event that we see packets from i distinct connections during time slice t. [sent-510, score-0.582]
</p><p>83 These parameters are: ﬁrst, the standard deviation σ of the number of live connections in each epoch, and second, the set of base packet rates {γm : packet types m}. [sent-513, score-0.8]
</p><p>84 Under Assumption 2, the average number of live connections in a time slice follows a Normal distribution with mean equal to the average number of live connections in the previous interval and standard deviation σ. [sent-514, score-0.584]
</p><p>85 σ σ  Due to Assumption 3, that each live connection generates packets independently according to a Poisson process, we expect the total packet arrival rate to increase linearly with the number of live connections. [sent-521, score-0.669]
</p><p>86 Then, when there are j connections in the tunnel, the number of type-m packet arrivals in an interval of length s will follow a Poisson distribution with parameter equal to jγm s. [sent-522, score-0.481]
</p><p>87 Therefore, the probability of the joint event that we observe ntm packets of each type m during an interval of length s, when there are j connections in the tunnel, is given by ˆ b j (nt ) =  M  ∏ e− jγm s  m=1  ( jγm s)ntm . [sent-523, score-0.592]
</p><p>88 For each s-length interval t, we measure (1) the number of connections Nt in the tunnel during the interval, and (2) nt = nt1 , nt2 , . [sent-528, score-0.509]
</p><p>89 That is, γm gives us the rate at which the number of packets observed increases with the number of connections in the tunnel. [sent-533, score-0.548]
</p><p>90 2761  W RIGHT, M ONROSE AND M ASSON  We estimate σ as the sample conditional standard deviation of the number of connections in the tunnel Nt during an interval t, given the number Nt−1 in the tunnel in the preceding interval. [sent-534, score-0.616]
</p><p>91 The models for AIM and HTTPS are able to track the true number of connections in their tunnels especially well: on average, their predictions differ from true number of connections by only 22% and 19%, respectively. [sent-564, score-0.569]
</p><p>92 Between 40 and 65 ticks, the HTTP tunnel produces a sequence of packets where the relative frequencies of the different packet types are out of proportion to those on the training day. [sent-568, score-0.712]
</p><p>93 ” Zhang and Paxson (2000a) present one of the earliest studies of techniques for network protocol recognition without using port numbers, based on matching patterns in the packet payloads. [sent-574, score-0.717]
</p><p>94 (2003), where a decision tree classiﬁer that used n-grams of packets was proposed for distinguishing among ﬂows from HTTP, SMTP, FTP, SSH and Telnet servers based on average packet size, average inter-arrival time, and TCP ﬂags. [sent-578, score-0.554]
</p><p>95 (2006) build a classiﬁer based on k-means clustering of the sizes of the ﬁrst ﬁve packets in each connection to identify application protocols “on the ﬂy. [sent-581, score-0.641]
</p><p>96 (2001) show that the interarrival times of packets in SSH (version 1) connections can be used to infer information about the user’s keystrokes and thereby reduce the search space for cracking login passwords. [sent-616, score-0.548]
</p><p>97 We also show that, when it is possible to demultiplex the ﬂows, more in-depth analysis of the packets in each ﬂow can lead to even more robust and accurate classiﬁcation even when a mix of several protocols are included. [sent-622, score-0.627]
</p><p>98 Finally, and perhaps most surprisingly, we show that encrypted tunnels which carry only a single application protocol leak sufﬁcient information about the ﬂows in the tunnel to allow us to accurately track their number. [sent-623, score-0.799]
</p><p>99 We also intend to extend our techniques to more general types of encrypted tunnels, with the ultimate goal of being able to track the number of connections of each protocol inside a full IPsec VPN (Kent and Atkinson, 1998). [sent-628, score-0.797]
</p><p>100 Table 4 depicts shows the full confusion matrix for the Viterbi classiﬁer when analyzing TCP connections as sequences of packet sizes. [sent-634, score-0.505]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('protocol', 0.418), ('packets', 0.298), ('protocols', 0.274), ('traf', 0.257), ('connections', 0.25), ('ssh', 0.234), ('packet', 0.231), ('tcp', 0.204), ('tunnel', 0.183), ('ftp', 0.176), ('https', 0.152), ('encrypted', 0.129), ('detector', 0.127), ('viterbi', 0.11), ('telnet', 0.107), ('hmm', 0.101), ('security', 0.096), ('smtp', 0.095), ('ows', 0.085), ('detection', 0.084), ('server', 0.082), ('nt', 0.076), ('asson', 0.076), ('ehaviors', 0.076), ('nferring', 0.076), ('onrose', 0.076), ('rotocol', 0.076), ('aggregate', 0.072), ('tunnels', 0.069), ('detectors', 0.066), ('pplication', 0.064), ('traces', 0.062), ('td', 0.062), ('client', 0.059), ('timing', 0.057), ('day', 0.054), ('pro', 0.052), ('administrator', 0.051), ('karagiannis', 0.051), ('paxson', 0.051), ('aggregates', 0.048), ('epoch', 0.046), ('rates', 0.046), ('ntm', 0.044), ('fd', 0.044), ('percentile', 0.043), ('live', 0.042), ('aim', 0.041), ('hmms', 0.04), ('er', 0.04), ('le', 0.038), ('gmu', 0.038), ('hosts', 0.038), ('codebook', 0.038), ('network', 0.036), ('build', 0.035), ('classi', 0.034), ('connection', 0.034), ('interactive', 0.033), ('port', 0.032), ('demultiplex', 0.032), ('holdout', 0.032), ('ssl', 0.032), ('usenix', 0.032), ('internet', 0.029), ('http', 0.029), ('intrusion', 0.029), ('epochs', 0.027), ('wright', 0.027), ('fabian', 0.027), ('encryption', 0.027), ('moore', 0.026), ('faxon', 0.025), ('papagiannaki', 0.025), ('pviterbi', 0.025), ('quantize', 0.025), ('schliep', 0.025), ('servers', 0.025), ('vern', 0.025), ('quantization', 0.025), ('ow', 0.025), ('detect', 0.024), ('detecting', 0.024), ('august', 0.024), ('sequences', 0.024), ('eddy', 0.024), ('krogh', 0.024), ('stepping', 0.024), ('threshold', 0.024), ('mix', 0.023), ('counts', 0.023), ('arrival', 0.022), ('days', 0.022), ('ags', 0.021), ('gerald', 0.021), ('bulk', 0.021), ('monrose', 0.021), ('headers', 0.021), ('bytes', 0.021), ('traveling', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="97-tfidf-1" href="./jmlr-2006-%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">97 jmlr-2006- (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Charles V. Wright, Fabian Monrose, Gerald M. Masson</p><p>Abstract: Several fundamental security mechanisms for restricting access to network resources rely on the ability of a reference monitor to inspect the contents of trafﬁc as it traverses the network. However, with the increasing popularity of cryptographic protocols, the traditional means of inspecting packet contents to enforce security policies is no longer a viable approach as message contents are concealed by encryption. In this paper, we investigate the extent to which common application protocols can be identiﬁed using only the features that remain intact after encryption—namely packet size, timing, and direction. We ﬁrst present what we believe to be the ﬁrst exploratory look at protocol identiﬁcation in encrypted tunnels which carry trafﬁc from many TCP connections simultaneously, using only post-encryption observable features. We then explore the problem of protocol identiﬁcation in individual encrypted TCP connections, using much less data than in other recent approaches. The results of our evaluation show that our classiﬁers achieve accuracy greater than 90% for several protocols in aggregate trafﬁc, and, for most protocols, greater than 80% when making ﬁne-grained classiﬁcations on single connections. Moreover, perhaps most surprisingly, we show that one can even estimate the number of live connections in certain classes of encrypted tunnels to within, on average, better than 20%. Keywords: trafﬁc classiﬁcation, hidden Markov models, network security</p><p>2 0.24351022 <a title="97-tfidf-2" href="./jmlr-2006-Machine_Learning_for_Computer_Security%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">59 jmlr-2006-Machine Learning for Computer Security    (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Philip K. Chan, Richard P. Lippmann</p><p>Abstract: The prevalent use of computers and internet has enhanced the quality of life for many people, but it has also attracted undesired attempts to undermine these systems. This special topic contains several research studies on how machine learning algorithms can help improve the security of computer systems. Keywords: computer security, spam, images with embedded text, malicious executables, network protocols, encrypted trafﬁc</p><p>3 0.054993711 <a title="97-tfidf-3" href="./jmlr-2006-One-Class_Novelty_Detection_for_Seizure_Analysis_from_Intracranial_EEG.html">69 jmlr-2006-One-Class Novelty Detection for Seizure Analysis from Intracranial EEG</a></p>
<p>Author: Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos, Brian Litt</p><p>Abstract: This paper describes an application of one-class support vector machine (SVM) novelty detection for detecting seizures in humans. Our technique maps intracranial electroencephalogram (EEG) time series into corresponding novelty sequences by classifying short-time, energy-based statistics computed from one-second windows of data. We train a classifier on epochs of interictal (normal) EEG. During ictal (seizure) epochs of EEG, seizure activity induces distributional changes in feature space that increase the empirical outlier fraction. A hypothesis test determines when the parameter change differs significantly from its nominal value, signaling a seizure detection event. Outputs are gated in a “one-shot” manner using persistence to reduce the false alarm rate of the system. The detector was validated using leave-one-out cross-validation (LOO-CV) on a sample of 41 interictal and 29 ictal epochs, and achieved 97.1% sensitivity, a mean detection latency of ©2005 Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos and Brian Litt GARDNER, KRIEGER, VACHTSEVANOS AND LITT -7.58 seconds, and an asymptotic false positive rate (FPR) of 1.56 false positives per hour (Fp/hr). These results are better than those obtained from a novelty detection technique based on Mahalanobis distance outlier detection, and comparable to the performance of a supervised learning technique used in experimental implantable devices (Echauz et al., 2001). The novelty detection paradigm overcomes three significant limitations of competing methods: the need to collect seizure data, precisely mark seizure onset and offset times, and perform patient-specific parameter tuning for detector training. Keywords: seizure detection, novelty detection, one-class SVM, epilepsy, unsupervised learning 1</p><p>4 0.044724915 <a title="97-tfidf-4" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>Author: Hichem Sahbi, Donald Geman</p><p>Abstract: We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly ﬁner subsets. The hierarchy is traversed coarse-to-ﬁne and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This “free network” is then perturbed, cell by cell, into another network, which is “graded” in two ways: ﬁrst, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-speciﬁc SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network. Keywords: statistical learning, hierarchy of classiﬁers, coarse-to-ﬁne computation, support vector machines, face detection</p><p>5 0.041753851 <a title="97-tfidf-5" href="./jmlr-2006-Evolutionary_Function_Approximation_for_Reinforcement_Learning.html">30 jmlr-2006-Evolutionary Function Approximation for Reinforcement Learning</a></p>
<p>Author: Shimon Whiteson, Peter Stone</p><p>Abstract: Temporal difference methods are theoretically grounded and empirically effective methods for addressing reinforcement learning problems. In most real-world reinforcement learning tasks, TD methods require a function approximator to represent the value function. However, using function approximators requires manually making crucial representational decisions. This paper investigates evolutionary function approximation, a novel approach to automatically selecting function approximator representations that enable efﬁcient individual learning. This method evolves individuals that are better able to learn. We present a fully implemented instantiation of evolutionary function approximation which combines NEAT, a neuroevolutionary optimization technique, with Q-learning, a popular TD method. The resulting NEAT+Q algorithm automatically discovers effective representations for neural network function approximators. This paper also presents on-line evolutionary computation, which improves the on-line performance of evolutionary computation by borrowing selection mechanisms used in TD methods to choose individual actions and using them in evolutionary computation to select policies for evaluation. We evaluate these contributions with extended empirical studies in two domains: 1) the mountain car task, a standard reinforcement learning benchmark on which neural network function approximators have previously performed poorly and 2) server job scheduling, a large probabilistic domain drawn from the ﬁeld of autonomic computing. The results demonstrate that evolutionary function approximation can signiﬁcantly improve the performance of TD methods and on-line evolutionary computation can signiﬁcantly improve evolutionary methods. This paper also presents additional tests that offer insight into what factors can make neural network function approximation difﬁcult in practice. Keywords: reinforcement learning, temporal difference methods, evolutionary computation, neuroevolution, on-</p><p>6 0.039654057 <a title="97-tfidf-6" href="./jmlr-2006-Segmental_Hidden_Markov_Models_with_Random_Effects_for_Waveform_Modeling.html">80 jmlr-2006-Segmental Hidden Markov Models with Random Effects for Waveform Modeling</a></p>
<p>7 0.037262581 <a title="97-tfidf-7" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.032343019 <a title="97-tfidf-8" href="./jmlr-2006-A_Simulation-Based_Algorithm_for_Ergodic_Control_of_Markov_Chains_Conditioned_on_Rare_Events.html">7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</a></p>
<p>9 0.031864524 <a title="97-tfidf-9" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>10 0.027787173 <a title="97-tfidf-10" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.02701886 <a title="97-tfidf-11" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>12 0.026398202 <a title="97-tfidf-12" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>13 0.023597751 <a title="97-tfidf-13" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>14 0.023306957 <a title="97-tfidf-14" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>15 0.022097226 <a title="97-tfidf-15" href="./jmlr-2006-Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets.html">85 jmlr-2006-Statistical Comparisons of Classifiers over Multiple Data Sets</a></p>
<p>16 0.020044126 <a title="97-tfidf-16" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.019531542 <a title="97-tfidf-17" href="./jmlr-2006-Action_Elimination_and_Stopping_Conditions_for_the_Multi-Armed_Bandit_and_Reinforcement_Learning_Problems.html">10 jmlr-2006-Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems</a></p>
<p>18 0.019261181 <a title="97-tfidf-18" href="./jmlr-2006-Distance_Patterns_in_Structural_Similarity.html">25 jmlr-2006-Distance Patterns in Structural Similarity</a></p>
<p>19 0.018558076 <a title="97-tfidf-19" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<p>20 0.018527227 <a title="97-tfidf-20" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.117), (1, -0.036), (2, -0.058), (3, 0.11), (4, -0.005), (5, -0.015), (6, -0.176), (7, -0.415), (8, 0.064), (9, 0.008), (10, -0.45), (11, -0.164), (12, -0.026), (13, -0.123), (14, -0.055), (15, -0.004), (16, 0.093), (17, -0.063), (18, -0.229), (19, -0.115), (20, 0.035), (21, 0.156), (22, 0.057), (23, -0.032), (24, 0.009), (25, -0.026), (26, 0.024), (27, 0.023), (28, -0.013), (29, -0.028), (30, -0.011), (31, -0.026), (32, 0.008), (33, -0.027), (34, 0.007), (35, -0.051), (36, 0.02), (37, -0.02), (38, 0.058), (39, -0.023), (40, -0.006), (41, 0.066), (42, 0.011), (43, 0.041), (44, -0.047), (45, -0.009), (46, -0.01), (47, -0.013), (48, 0.014), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96504688 <a title="97-lsi-1" href="./jmlr-2006-%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">97 jmlr-2006- (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Charles V. Wright, Fabian Monrose, Gerald M. Masson</p><p>Abstract: Several fundamental security mechanisms for restricting access to network resources rely on the ability of a reference monitor to inspect the contents of trafﬁc as it traverses the network. However, with the increasing popularity of cryptographic protocols, the traditional means of inspecting packet contents to enforce security policies is no longer a viable approach as message contents are concealed by encryption. In this paper, we investigate the extent to which common application protocols can be identiﬁed using only the features that remain intact after encryption—namely packet size, timing, and direction. We ﬁrst present what we believe to be the ﬁrst exploratory look at protocol identiﬁcation in encrypted tunnels which carry trafﬁc from many TCP connections simultaneously, using only post-encryption observable features. We then explore the problem of protocol identiﬁcation in individual encrypted TCP connections, using much less data than in other recent approaches. The results of our evaluation show that our classiﬁers achieve accuracy greater than 90% for several protocols in aggregate trafﬁc, and, for most protocols, greater than 80% when making ﬁne-grained classiﬁcations on single connections. Moreover, perhaps most surprisingly, we show that one can even estimate the number of live connections in certain classes of encrypted tunnels to within, on average, better than 20%. Keywords: trafﬁc classiﬁcation, hidden Markov models, network security</p><p>2 0.93525153 <a title="97-lsi-2" href="./jmlr-2006-Machine_Learning_for_Computer_Security%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">59 jmlr-2006-Machine Learning for Computer Security    (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Philip K. Chan, Richard P. Lippmann</p><p>Abstract: The prevalent use of computers and internet has enhanced the quality of life for many people, but it has also attracted undesired attempts to undermine these systems. This special topic contains several research studies on how machine learning algorithms can help improve the security of computer systems. Keywords: computer security, spam, images with embedded text, malicious executables, network protocols, encrypted trafﬁc</p><p>3 0.22517265 <a title="97-lsi-3" href="./jmlr-2006-One-Class_Novelty_Detection_for_Seizure_Analysis_from_Intracranial_EEG.html">69 jmlr-2006-One-Class Novelty Detection for Seizure Analysis from Intracranial EEG</a></p>
<p>Author: Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos, Brian Litt</p><p>Abstract: This paper describes an application of one-class support vector machine (SVM) novelty detection for detecting seizures in humans. Our technique maps intracranial electroencephalogram (EEG) time series into corresponding novelty sequences by classifying short-time, energy-based statistics computed from one-second windows of data. We train a classifier on epochs of interictal (normal) EEG. During ictal (seizure) epochs of EEG, seizure activity induces distributional changes in feature space that increase the empirical outlier fraction. A hypothesis test determines when the parameter change differs significantly from its nominal value, signaling a seizure detection event. Outputs are gated in a “one-shot” manner using persistence to reduce the false alarm rate of the system. The detector was validated using leave-one-out cross-validation (LOO-CV) on a sample of 41 interictal and 29 ictal epochs, and achieved 97.1% sensitivity, a mean detection latency of ©2005 Andrew B. Gardner, Abba M. Krieger, George Vachtsevanos and Brian Litt GARDNER, KRIEGER, VACHTSEVANOS AND LITT -7.58 seconds, and an asymptotic false positive rate (FPR) of 1.56 false positives per hour (Fp/hr). These results are better than those obtained from a novelty detection technique based on Mahalanobis distance outlier detection, and comparable to the performance of a supervised learning technique used in experimental implantable devices (Echauz et al., 2001). The novelty detection paradigm overcomes three significant limitations of competing methods: the need to collect seizure data, precisely mark seizure onset and offset times, and perform patient-specific parameter tuning for detector training. Keywords: seizure detection, novelty detection, one-class SVM, epilepsy, unsupervised learning 1</p><p>4 0.16430795 <a title="97-lsi-4" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>Author: Hichem Sahbi, Donald Geman</p><p>Abstract: We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly ﬁner subsets. The hierarchy is traversed coarse-to-ﬁne and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This “free network” is then perturbed, cell by cell, into another network, which is “graded” in two ways: ﬁrst, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-speciﬁc SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network. Keywords: statistical learning, hierarchy of classiﬁers, coarse-to-ﬁne computation, support vector machines, face detection</p><p>5 0.15378693 <a title="97-lsi-5" href="./jmlr-2006-Evolutionary_Function_Approximation_for_Reinforcement_Learning.html">30 jmlr-2006-Evolutionary Function Approximation for Reinforcement Learning</a></p>
<p>Author: Shimon Whiteson, Peter Stone</p><p>Abstract: Temporal difference methods are theoretically grounded and empirically effective methods for addressing reinforcement learning problems. In most real-world reinforcement learning tasks, TD methods require a function approximator to represent the value function. However, using function approximators requires manually making crucial representational decisions. This paper investigates evolutionary function approximation, a novel approach to automatically selecting function approximator representations that enable efﬁcient individual learning. This method evolves individuals that are better able to learn. We present a fully implemented instantiation of evolutionary function approximation which combines NEAT, a neuroevolutionary optimization technique, with Q-learning, a popular TD method. The resulting NEAT+Q algorithm automatically discovers effective representations for neural network function approximators. This paper also presents on-line evolutionary computation, which improves the on-line performance of evolutionary computation by borrowing selection mechanisms used in TD methods to choose individual actions and using them in evolutionary computation to select policies for evaluation. We evaluate these contributions with extended empirical studies in two domains: 1) the mountain car task, a standard reinforcement learning benchmark on which neural network function approximators have previously performed poorly and 2) server job scheduling, a large probabilistic domain drawn from the ﬁeld of autonomic computing. The results demonstrate that evolutionary function approximation can signiﬁcantly improve the performance of TD methods and on-line evolutionary computation can signiﬁcantly improve evolutionary methods. This paper also presents additional tests that offer insight into what factors can make neural network function approximation difﬁcult in practice. Keywords: reinforcement learning, temporal difference methods, evolutionary computation, neuroevolution, on-</p><p>6 0.14153892 <a title="97-lsi-6" href="./jmlr-2006-Segmental_Hidden_Markov_Models_with_Random_Effects_for_Waveform_Modeling.html">80 jmlr-2006-Segmental Hidden Markov Models with Random Effects for Waveform Modeling</a></p>
<p>7 0.11271366 <a title="97-lsi-7" href="./jmlr-2006-Exact_1-Norm_Support_Vector_Machines_Via_Unconstrained_Convex_Differentiable_Minimization_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">31 jmlr-2006-Exact 1-Norm Support Vector Machines Via Unconstrained Convex Differentiable Minimization     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.11087344 <a title="97-lsi-8" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<p>9 0.10574377 <a title="97-lsi-9" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<p>10 0.10247242 <a title="97-lsi-10" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.10119373 <a title="97-lsi-11" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.09723074 <a title="97-lsi-12" href="./jmlr-2006-Learning_Factor_Graphs_in_Polynomial_Time_and_Sample_Complexity.html">46 jmlr-2006-Learning Factor Graphs in Polynomial Time and Sample Complexity</a></p>
<p>13 0.096720941 <a title="97-lsi-13" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>14 0.094795063 <a title="97-lsi-14" href="./jmlr-2006-Optimising_Kernel_Parameters_and_Regularisation_Coefficients_for_Non-linear_Discriminant_Analysis.html">71 jmlr-2006-Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis</a></p>
<p>15 0.094416589 <a title="97-lsi-15" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.090441473 <a title="97-lsi-16" href="./jmlr-2006-A_Simulation-Based_Algorithm_for_Ergodic_Control_of_Markov_Chains_Conditioned_on_Rare_Events.html">7 jmlr-2006-A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events</a></p>
<p>17 0.085640192 <a title="97-lsi-17" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>18 0.084021389 <a title="97-lsi-18" href="./jmlr-2006-Action_Elimination_and_Stopping_Conditions_for_the_Multi-Armed_Bandit_and_Reinforcement_Learning_Problems.html">10 jmlr-2006-Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems</a></p>
<p>19 0.082317516 <a title="97-lsi-19" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>20 0.08008039 <a title="97-lsi-20" href="./jmlr-2006-Rearrangement_Clustering%3A_Pitfalls%2C_Remedies%2C_and_Applications.html">78 jmlr-2006-Rearrangement Clustering: Pitfalls, Remedies, and Applications</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.012), (35, 0.011), (36, 0.057), (45, 0.03), (50, 0.022), (61, 0.01), (63, 0.035), (68, 0.014), (70, 0.048), (76, 0.014), (78, 0.016), (79, 0.015), (81, 0.029), (84, 0.012), (90, 0.012), (91, 0.025), (95, 0.489), (96, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79230934 <a title="97-lda-1" href="./jmlr-2006-%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">97 jmlr-2006- (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Charles V. Wright, Fabian Monrose, Gerald M. Masson</p><p>Abstract: Several fundamental security mechanisms for restricting access to network resources rely on the ability of a reference monitor to inspect the contents of trafﬁc as it traverses the network. However, with the increasing popularity of cryptographic protocols, the traditional means of inspecting packet contents to enforce security policies is no longer a viable approach as message contents are concealed by encryption. In this paper, we investigate the extent to which common application protocols can be identiﬁed using only the features that remain intact after encryption—namely packet size, timing, and direction. We ﬁrst present what we believe to be the ﬁrst exploratory look at protocol identiﬁcation in encrypted tunnels which carry trafﬁc from many TCP connections simultaneously, using only post-encryption observable features. We then explore the problem of protocol identiﬁcation in individual encrypted TCP connections, using much less data than in other recent approaches. The results of our evaluation show that our classiﬁers achieve accuracy greater than 90% for several protocols in aggregate trafﬁc, and, for most protocols, greater than 80% when making ﬁne-grained classiﬁcations on single connections. Moreover, perhaps most surprisingly, we show that one can even estimate the number of live connections in certain classes of encrypted tunnels to within, on average, better than 20%. Keywords: trafﬁc classiﬁcation, hidden Markov models, network security</p><p>2 0.26723665 <a title="97-lda-2" href="./jmlr-2006-Machine_Learning_for_Computer_Security%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">59 jmlr-2006-Machine Learning for Computer Security    (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Philip K. Chan, Richard P. Lippmann</p><p>Abstract: The prevalent use of computers and internet has enhanced the quality of life for many people, but it has also attracted undesired attempts to undermine these systems. This special topic contains several research studies on how machine learning algorithms can help improve the security of computer systems. Keywords: computer security, spam, images with embedded text, malicious executables, network protocols, encrypted trafﬁc</p><p>3 0.20966603 <a title="97-lda-3" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Juho Rousu, Craig Saunders, Sandor Szedmak, John Shawe-Taylor</p><p>Abstract: We present a kernel-based algorithm for hierarchical text classiﬁcation where the documents are allowed to belong to more than one category at a time. The classiﬁcation model is a variant of the Maximum Margin Markov Network framework, where the classiﬁcation hierarchy is represented as a Markov tree equipped with an exponential family deﬁned on the edges. We present an efﬁcient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classiﬁcation hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efﬁcient as training independent SVM-light classiﬁers for each node. The algorithm’s predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classiﬁcation learning algorithms. Keywords: kernel methods, hierarchical classiﬁcation, text categorization, convex optimization, structured outputs</p><p>4 0.20226349 <a title="97-lda-4" href="./jmlr-2006-Learning_Spectral_Clustering%2C_With_Application_To_Speech_Separation.html">52 jmlr-2006-Learning Spectral Clustering, With Application To Speech Separation</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram. Keywords: spectral clustering, blind source separation, computational auditory scene analysis</p><p>5 0.2021189 <a title="97-lda-5" href="./jmlr-2006-Large_Scale_Transductive_SVMs.html">44 jmlr-2006-Large Scale Transductive SVMs</a></p>
<p>Author: Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou</p><p>Abstract: We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the ﬁrst time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction. html. Keywords: transduction, transductive SVMs, semi-supervised learning, CCCP</p><p>6 0.20098916 <a title="97-lda-6" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>7 0.19733307 <a title="97-lda-7" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>8 0.19721857 <a title="97-lda-8" href="./jmlr-2006-Learning_Sparse_Representations_by_Non-Negative_Matrix_Factorization_and_Sequential_Cone_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">51 jmlr-2006-Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.19602397 <a title="97-lda-9" href="./jmlr-2006-Active_Learning_in_Approximately_Linear_Regression_Based_on_Conditional_Expectation_of_Generalization_Error.html">11 jmlr-2006-Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error</a></p>
<p>10 0.19520208 <a title="97-lda-10" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.19517246 <a title="97-lda-11" href="./jmlr-2006-Maximum-Gain_Working_Set_Selection_for_SVMs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">61 jmlr-2006-Maximum-Gain Working Set Selection for SVMs     (Special Topic on Machine Learning and Optimization)</a></p>
<p>12 0.19289081 <a title="97-lda-12" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.19272171 <a title="97-lda-13" href="./jmlr-2006-A_Direct_Method_for_Building_Sparse_Kernel_Learning_Algorithms.html">1 jmlr-2006-A Direct Method for Building Sparse Kernel Learning Algorithms</a></p>
<p>14 0.1925893 <a title="97-lda-14" href="./jmlr-2006-Manifold__Regularization%3A_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples.html">60 jmlr-2006-Manifold  Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a></p>
<p>15 0.19119391 <a title="97-lda-15" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.19114377 <a title="97-lda-16" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>17 0.19113022 <a title="97-lda-17" href="./jmlr-2006-Estimating_the_%22Wrong%22_Graphical_Model%3A_Benefits_in_the_Computation-Limited_Setting.html">28 jmlr-2006-Estimating the "Wrong" Graphical Model: Benefits in the Computation-Limited Setting</a></p>
<p>18 0.19093914 <a title="97-lda-18" href="./jmlr-2006-Noisy-OR_Component_Analysis_and_its_Application_to_Link_Analysis.html">64 jmlr-2006-Noisy-OR Component Analysis and its Application to Link Analysis</a></p>
<p>19 0.19068733 <a title="97-lda-19" href="./jmlr-2006-Structured_Prediction%2C_Dual_Extragradient_and_Bregman_Projections_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">89 jmlr-2006-Structured Prediction, Dual Extragradient and Bregman Projections     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.19002733 <a title="97-lda-20" href="./jmlr-2006-Learning_Minimum_Volume_Sets.html">48 jmlr-2006-Learning Minimum Volume Sets</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
