<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>59 jmlr-2006-Machine Learning for Computer Security    (Special Topic on Machine Learning for Computer Security)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2006" href="../home/jmlr2006_home.html">jmlr2006</a> <a title="jmlr-2006-59" href="#">jmlr2006-59</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>59 jmlr-2006-Machine Learning for Computer Security    (Special Topic on Machine Learning for Computer Security)</h1>
<br/><p>Source: <a title="jmlr-2006-59-pdf" href="http://jmlr.org/papers/volume7/MLSEC-intro06a/MLSEC-intro06a.pdf">pdf</a></p><p>Author: Philip K. Chan, Richard P. Lippmann</p><p>Abstract: The prevalent use of computers and internet has enhanced the quality of life for many people, but it has also attracted undesired attempts to undermine these systems. This special topic contains several research studies on how machine learning algorithms can help improve the security of computer systems. Keywords: computer security, spam, images with embedded text, malicious executables, network protocols, encrypted trafﬁc</p><p>Reference: <a title="jmlr-2006-59-reference" href="../jmlr2006_reference/jmlr-2006-Machine_Learning_for_Computer_Security%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spam', 0.536), ('email', 0.286), ('malicy', 0.238), ('protocol', 0.208), ('encrypt', 0.204), ('sec', 0.194), ('mess', 0.174), ('dmc', 0.16), ('chan', 0.146), ('advers', 0.131), ('traf', 0.122), ('fumer', 0.12), ('jajod', 0.12), ('maloof', 0.12), ('ocr', 0.112), ('embed', 0.107), ('florid', 0.102), ('philip', 0.102), ('ppm', 0.102), ('packet', 0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="59-tfidf-1" href="./jmlr-2006-Machine_Learning_for_Computer_Security%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">59 jmlr-2006-Machine Learning for Computer Security    (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Philip K. Chan, Richard P. Lippmann</p><p>Abstract: The prevalent use of computers and internet has enhanced the quality of life for many people, but it has also attracted undesired attempts to undermine these systems. This special topic contains several research studies on how machine learning algorithms can help improve the security of computer systems. Keywords: computer security, spam, images with embedded text, malicious executables, network protocols, encrypted trafﬁc</p><p>2 0.28826559 <a title="59-tfidf-2" href="./jmlr-2006-%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">97 jmlr-2006- (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Charles V. Wright, Fabian Monrose, Gerald M. Masson</p><p>Abstract: Several fundamental security mechanisms for restricting access to network resources rely on the ability of a reference monitor to inspect the contents of trafﬁc as it traverses the network. However, with the increasing popularity of cryptographic protocols, the traditional means of inspecting packet contents to enforce security policies is no longer a viable approach as message contents are concealed by encryption. In this paper, we investigate the extent to which common application protocols can be identiﬁed using only the features that remain intact after encryption—namely packet size, timing, and direction. We ﬁrst present what we believe to be the ﬁrst exploratory look at protocol identiﬁcation in encrypted tunnels which carry trafﬁc from many TCP connections simultaneously, using only post-encryption observable features. We then explore the problem of protocol identiﬁcation in individual encrypted TCP connections, using much less data than in other recent approaches. The results of our evaluation show that our classiﬁers achieve accuracy greater than 90% for several protocols in aggregate trafﬁc, and, for most protocols, greater than 80% when making ﬁne-grained classiﬁcations on single connections. Moreover, perhaps most surprisingly, we show that one can even estimate the number of live connections in certain classes of encrypted tunnels to within, on average, better than 20%. Keywords: trafﬁc classiﬁcation, hidden Markov models, network security</p><p>3 0.054510497 <a title="59-tfidf-3" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>Author: Hichem Sahbi, Donald Geman</p><p>Abstract: We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly ﬁner subsets. The hierarchy is traversed coarse-to-ﬁne and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This “free network” is then perturbed, cell by cell, into another network, which is “graded” in two ways: ﬁrst, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-speciﬁc SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network. Keywords: statistical learning, hierarchy of classiﬁers, coarse-to-ﬁne computation, support vector machines, face detection</p><p>4 0.048156384 <a title="59-tfidf-4" href="./jmlr-2006-Ensemble_Pruning_Via_Semi-definite_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">27 jmlr-2006-Ensemble Pruning Via Semi-definite Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Yi Zhang, Samuel Burer, W. Nick Street</p><p>Abstract: An ensemble is a group of learning models that jointly solve a problem. However, the ensembles generated by existing techniques are sometimes unnecessarily large, which can lead to extra memory usage, computational costs, and occasional decreases in eﬀectiveness. The purpose of ensemble pruning is to search for a good subset of ensemble members that performs as well as, or better than, the original ensemble. This subset selection problem is a combinatorial optimization problem and thus ﬁnding the exact optimal solution is computationally prohibitive. Various heuristic methods have been developed to obtain an approximate solution. However, most of the existing heuristics use simple greedy search as the optimization method, which lacks either theoretical or empirical quality guarantees. In this paper, the ensemble subset selection problem is formulated as a quadratic integer programming problem. By applying semi-deﬁnite programming (SDP) as a solution technique, we are able to get better approximate solutions. Computational experiments show that this SDP-based pruning algorithm outperforms other heuristics in the literature. Its application in a classiﬁer-sharing study also demonstrates the eﬀectiveness of the method. Keywords: ensemble pruning, semi-deﬁnite programming, heuristics, knowledge sharing</p><p>5 0.041859366 <a title="59-tfidf-5" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Katya Scheinberg</p><p>Abstract: We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efﬁcient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims’ SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difﬁcult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight , the generalization properties of these two approaches are identical and we do not discuss them in the paper. Keywords: active set methods, support vector machines, quadratic programming</p><p>6 0.037739839 <a title="59-tfidf-6" href="./jmlr-2006-Segmental_Hidden_Markov_Models_with_Random_Effects_for_Waveform_Modeling.html">80 jmlr-2006-Segmental Hidden Markov Models with Random Effects for Waveform Modeling</a></p>
<p>7 0.03514212 <a title="59-tfidf-7" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>8 0.032032184 <a title="59-tfidf-8" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>9 0.028654685 <a title="59-tfidf-9" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>10 0.028111881 <a title="59-tfidf-10" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.025938176 <a title="59-tfidf-11" href="./jmlr-2006-One-Class_Novelty_Detection_for_Seizure_Analysis_from_Intracranial_EEG.html">69 jmlr-2006-One-Class Novelty Detection for Seizure Analysis from Intracranial EEG</a></p>
<p>12 0.025195334 <a title="59-tfidf-12" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>13 0.023547165 <a title="59-tfidf-13" href="./jmlr-2006-The_Interplay_of_Optimization_and_Machine_Learning_Research_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">91 jmlr-2006-The Interplay of Optimization and Machine Learning Research     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.02264484 <a title="59-tfidf-14" href="./jmlr-2006-Superior_Guarantees_for_Sequential_Prediction_and_Lossless_Compression_via_Alphabet_Decomposition.html">90 jmlr-2006-Superior Guarantees for Sequential Prediction and Lossless Compression via Alphabet Decomposition</a></p>
<p>15 0.022099983 <a title="59-tfidf-15" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>16 0.021433851 <a title="59-tfidf-16" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<p>17 0.020311022 <a title="59-tfidf-17" href="./jmlr-2006-Linear_State-Space_Models_for_Blind_Source_Separation.html">57 jmlr-2006-Linear State-Space Models for Blind Source Separation</a></p>
<p>18 0.01929481 <a title="59-tfidf-18" href="./jmlr-2006-A_Robust_Procedure_For_Gaussian_Graphical_Model_Search_From_Microarray_Data_WithpLarger_Thann.html">5 jmlr-2006-A Robust Procedure For Gaussian Graphical Model Search From Microarray Data WithpLarger Thann</a></p>
<p>19 0.017914724 <a title="59-tfidf-19" href="./jmlr-2006-Second_Order_Cone_Programming_Approaches_for_Handling_Missing_and_Uncertain_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">79 jmlr-2006-Second Order Cone Programming Approaches for Handling Missing and Uncertain Data     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.017559126 <a title="59-tfidf-20" href="./jmlr-2006-Learning_Recursive_Control_Programs_from_Problem_Solving_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">50 jmlr-2006-Learning Recursive Control Programs from Problem Solving     (Special Topic on Inductive Programming)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2006_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.105), (1, 0.048), (2, 0.103), (3, -0.099), (4, -0.006), (5, -0.049), (6, 0.209), (7, 0.47), (8, -0.236), (9, -0.178), (10, 0.128), (11, 0.332), (12, -0.108), (13, -0.115), (14, -0.111), (15, 0.052), (16, 0.067), (17, 0.062), (18, -0.16), (19, 0.024), (20, -0.023), (21, 0.003), (22, 0.099), (23, 0.008), (24, -0.096), (25, -0.075), (26, -0.093), (27, 0.013), (28, 0.082), (29, -0.031), (30, 0.013), (31, -0.016), (32, 0.028), (33, -0.005), (34, -0.052), (35, -0.03), (36, -0.031), (37, 0.049), (38, -0.038), (39, 0.052), (40, -0.033), (41, 0.027), (42, -0.044), (43, -0.006), (44, -0.027), (45, -0.024), (46, -0.028), (47, -0.009), (48, -0.009), (49, -0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97231752 <a title="59-lsi-1" href="./jmlr-2006-Machine_Learning_for_Computer_Security%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">59 jmlr-2006-Machine Learning for Computer Security    (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Philip K. Chan, Richard P. Lippmann</p><p>Abstract: The prevalent use of computers and internet has enhanced the quality of life for many people, but it has also attracted undesired attempts to undermine these systems. This special topic contains several research studies on how machine learning algorithms can help improve the security of computer systems. Keywords: computer security, spam, images with embedded text, malicious executables, network protocols, encrypted trafﬁc</p><p>2 0.91574967 <a title="59-lsi-2" href="./jmlr-2006-%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">97 jmlr-2006- (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Charles V. Wright, Fabian Monrose, Gerald M. Masson</p><p>Abstract: Several fundamental security mechanisms for restricting access to network resources rely on the ability of a reference monitor to inspect the contents of trafﬁc as it traverses the network. However, with the increasing popularity of cryptographic protocols, the traditional means of inspecting packet contents to enforce security policies is no longer a viable approach as message contents are concealed by encryption. In this paper, we investigate the extent to which common application protocols can be identiﬁed using only the features that remain intact after encryption—namely packet size, timing, and direction. We ﬁrst present what we believe to be the ﬁrst exploratory look at protocol identiﬁcation in encrypted tunnels which carry trafﬁc from many TCP connections simultaneously, using only post-encryption observable features. We then explore the problem of protocol identiﬁcation in individual encrypted TCP connections, using much less data than in other recent approaches. The results of our evaluation show that our classiﬁers achieve accuracy greater than 90% for several protocols in aggregate trafﬁc, and, for most protocols, greater than 80% when making ﬁne-grained classiﬁcations on single connections. Moreover, perhaps most surprisingly, we show that one can even estimate the number of live connections in certain classes of encrypted tunnels to within, on average, better than 20%. Keywords: trafﬁc classiﬁcation, hidden Markov models, network security</p><p>3 0.17790093 <a title="59-lsi-3" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>Author: Hichem Sahbi, Donald Geman</p><p>Abstract: We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly ﬁner subsets. The hierarchy is traversed coarse-to-ﬁne and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This “free network” is then perturbed, cell by cell, into another network, which is “graded” in two ways: ﬁrst, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-speciﬁc SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network. Keywords: statistical learning, hierarchy of classiﬁers, coarse-to-ﬁne computation, support vector machines, face detection</p><p>4 0.17094189 <a title="59-lsi-4" href="./jmlr-2006-Ensemble_Pruning_Via_Semi-definite_Programming_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">27 jmlr-2006-Ensemble Pruning Via Semi-definite Programming     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Yi Zhang, Samuel Burer, W. Nick Street</p><p>Abstract: An ensemble is a group of learning models that jointly solve a problem. However, the ensembles generated by existing techniques are sometimes unnecessarily large, which can lead to extra memory usage, computational costs, and occasional decreases in eﬀectiveness. The purpose of ensemble pruning is to search for a good subset of ensemble members that performs as well as, or better than, the original ensemble. This subset selection problem is a combinatorial optimization problem and thus ﬁnding the exact optimal solution is computationally prohibitive. Various heuristic methods have been developed to obtain an approximate solution. However, most of the existing heuristics use simple greedy search as the optimization method, which lacks either theoretical or empirical quality guarantees. In this paper, the ensemble subset selection problem is formulated as a quadratic integer programming problem. By applying semi-deﬁnite programming (SDP) as a solution technique, we are able to get better approximate solutions. Computational experiments show that this SDP-based pruning algorithm outperforms other heuristics in the literature. Its application in a classiﬁer-sharing study also demonstrates the eﬀectiveness of the method. Keywords: ensemble pruning, semi-deﬁnite programming, heuristics, knowledge sharing</p><p>5 0.13249846 <a title="59-lsi-5" href="./jmlr-2006-An_Efficient_Implementation_of_an_Active_Set_Method_for_SVMs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">14 jmlr-2006-An Efficient Implementation of an Active Set Method for SVMs    (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Katya Scheinberg</p><p>Abstract: We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efﬁcient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims’ SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difﬁcult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight , the generalization properties of these two approaches are identical and we do not discuss them in the paper. Keywords: active set methods, support vector machines, quadratic programming</p><p>6 0.13012376 <a title="59-lsi-6" href="./jmlr-2006-Incremental_Support_Vector_Learning%3A_Analysis%2C_Implementation_and_Applications_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">38 jmlr-2006-Incremental Support Vector Learning: Analysis, Implementation and Applications     (Special Topic on Machine Learning and Optimization)</a></p>
<p>7 0.12271386 <a title="59-lsi-7" href="./jmlr-2006-One-Class_Novelty_Detection_for_Seizure_Analysis_from_Intracranial_EEG.html">69 jmlr-2006-One-Class Novelty Detection for Seizure Analysis from Intracranial EEG</a></p>
<p>8 0.11795481 <a title="59-lsi-8" href="./jmlr-2006-Active_Learning_with_Feedback_on_Features_and_Instances.html">12 jmlr-2006-Active Learning with Feedback on Features and Instances</a></p>
<p>9 0.11674309 <a title="59-lsi-9" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>10 0.10647952 <a title="59-lsi-10" href="./jmlr-2006-Building_Support_Vector_Machines_with_Reduced_Classifier_Complexity_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">18 jmlr-2006-Building Support Vector Machines with Reduced Classifier Complexity     (Special Topic on Machine Learning and Optimization)</a></p>
<p>11 0.10523771 <a title="59-lsi-11" href="./jmlr-2006-Segmental_Hidden_Markov_Models_with_Random_Effects_for_Waveform_Modeling.html">80 jmlr-2006-Segmental Hidden Markov Models with Random Effects for Waveform Modeling</a></p>
<p>12 0.10088052 <a title="59-lsi-12" href="./jmlr-2006-Using_Machine_Learning_to_Guide_Architecture_Simulation.html">94 jmlr-2006-Using Machine Learning to Guide Architecture Simulation</a></p>
<p>13 0.098474748 <a title="59-lsi-13" href="./jmlr-2006-Learning_Parts-Based_Representations_of_Data.html">49 jmlr-2006-Learning Parts-Based Representations of Data</a></p>
<p>14 0.085323907 <a title="59-lsi-14" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>15 0.080942839 <a title="59-lsi-15" href="./jmlr-2006-Noisy-OR_Component_Analysis_and_its_Application_to_Link_Analysis.html">64 jmlr-2006-Noisy-OR Component Analysis and its Application to Link Analysis</a></p>
<p>16 0.078076325 <a title="59-lsi-16" href="./jmlr-2006-Parallel_Software_for_Training_Large_Scale_Support_Vector_Machines_on_Multiprocessor_Systems_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">72 jmlr-2006-Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.077675052 <a title="59-lsi-17" href="./jmlr-2006-Learning_Image_Components_for_Object_Recognition.html">47 jmlr-2006-Learning Image Components for Object Recognition</a></p>
<p>18 0.076815471 <a title="59-lsi-18" href="./jmlr-2006-New_Algorithms_for_Efficient_High-Dimensional_Nonparametric_Classification.html">63 jmlr-2006-New Algorithms for Efficient High-Dimensional Nonparametric Classification</a></p>
<p>19 0.076710217 <a title="59-lsi-19" href="./jmlr-2006-Bayesian_Network_Learning_with_Parameter_Constraints_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">15 jmlr-2006-Bayesian Network Learning with Parameter Constraints     (Special Topic on Machine Learning and Optimization)</a></p>
<p>20 0.075446725 <a title="59-lsi-20" href="./jmlr-2006-Bounds_for_the_Loss_in_Probability_of_Correct_Classification_Under_Model_Based_Approximation.html">17 jmlr-2006-Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2006_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.052), (7, 0.531), (31, 0.035), (33, 0.027), (51, 0.014), (63, 0.018), (72, 0.041), (73, 0.045), (77, 0.029), (78, 0.04), (88, 0.025), (99, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81863552 <a title="59-lda-1" href="./jmlr-2006-Machine_Learning_for_Computer_Security%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">59 jmlr-2006-Machine Learning for Computer Security    (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Philip K. Chan, Richard P. Lippmann</p><p>Abstract: The prevalent use of computers and internet has enhanced the quality of life for many people, but it has also attracted undesired attempts to undermine these systems. This special topic contains several research studies on how machine learning algorithms can help improve the security of computer systems. Keywords: computer security, spam, images with embedded text, malicious executables, network protocols, encrypted trafﬁc</p><p>2 0.22906786 <a title="59-lda-2" href="./jmlr-2006-%C2%A0%28Special_Topic_on_Machine_Learning_for_Computer_Security%29.html">97 jmlr-2006- (Special Topic on Machine Learning for Computer Security)</a></p>
<p>Author: Charles V. Wright, Fabian Monrose, Gerald M. Masson</p><p>Abstract: Several fundamental security mechanisms for restricting access to network resources rely on the ability of a reference monitor to inspect the contents of trafﬁc as it traverses the network. However, with the increasing popularity of cryptographic protocols, the traditional means of inspecting packet contents to enforce security policies is no longer a viable approach as message contents are concealed by encryption. In this paper, we investigate the extent to which common application protocols can be identiﬁed using only the features that remain intact after encryption—namely packet size, timing, and direction. We ﬁrst present what we believe to be the ﬁrst exploratory look at protocol identiﬁcation in encrypted tunnels which carry trafﬁc from many TCP connections simultaneously, using only post-encryption observable features. We then explore the problem of protocol identiﬁcation in individual encrypted TCP connections, using much less data than in other recent approaches. The results of our evaluation show that our classiﬁers achieve accuracy greater than 90% for several protocols in aggregate trafﬁc, and, for most protocols, greater than 80% when making ﬁne-grained classiﬁcations on single connections. Moreover, perhaps most surprisingly, we show that one can even estimate the number of live connections in certain classes of encrypted tunnels to within, on average, better than 20%. Keywords: trafﬁc classiﬁcation, hidden Markov models, network security</p><p>3 0.19087905 <a title="59-lda-3" href="./jmlr-2006-Linear_Programming_Relaxations_and_Belief_Propagation_--_An_Empirical_Study_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">55 jmlr-2006-Linear Programming Relaxations and Belief Propagation -- An Empirical Study     (Special Topic on Machine Learning and Optimization)</a></p>
<p>Author: Chen Yanover, Talya Meltzer, Yair Weiss</p><p>Abstract: The problem of ﬁnding the most probable (MAP) conﬁguration in graphical models comes up in a wide range of applications. In a general graphical model this problem is NP hard, but various approximate algorithms have been developed. Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for ﬁnding the most probable assignment in small graphical models. However, applying this powerful method to real-world problems is extremely challenging due to the large numbers of variables and constraints in the linear program. Tree-Reweighted Belief Propagation is a promising recent algorithm for solving LP relaxations, but little is known about its running time on large problems. In this paper we compare tree-reweighted belief propagation (TRBP) and powerful generalpurpose LP solvers (CPLEX) on relaxations of real-world graphical models from the ﬁelds of computer vision and computational biology. We ﬁnd that TRBP almost always ﬁnds the solution signiﬁcantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. Using TRBP we can ﬁnd the MAP conﬁgurations in a matter of minutes for a large range of real world problems.</p><p>4 0.16506577 <a title="59-lda-4" href="./jmlr-2006-Step_Size_Adaptation_in_Reproducing_Kernel_Hilbert_Space.html">86 jmlr-2006-Step Size Adaptation in Reproducing Kernel Hilbert Space</a></p>
<p>Author: S. V. N. Vishwanathan, Nicol N. Schraudolph, Alex J. Smola</p><p>Abstract: This paper presents an online support vector machine (SVM) that uses the stochastic meta-descent (SMD) algorithm to adapt its step size automatically. We formulate the online learning problem as a stochastic gradient descent in reproducing kernel Hilbert space (RKHS) and translate SMD to the nonparametric setting, where its gradient trace parameter is no longer a coefﬁcient vector but an element of the RKHS. We derive efﬁcient updates that allow us to perform the step size adaptation in linear time. We apply the online SVM framework to a variety of loss functions, and in particular show how to handle structured output spaces and achieve efﬁcient online multiclass classiﬁcation. Experiments show that our algorithm outperforms more primitive methods for setting the gradient step size. Keywords: online SVM, stochastic meta-descent, structured output spaces</p><p>5 0.16469446 <a title="59-lda-5" href="./jmlr-2006-Collaborative_Multiagent_Reinforcement_Learning_by_Payoff_Propagation.html">20 jmlr-2006-Collaborative Multiagent Reinforcement Learning by Payoff Propagation</a></p>
<p>Author: Jelle R. Kok, Nikos Vlassis</p><p>Abstract: In this article we describe a set of scalable techniques for learning the behavior of a group of agents in a collaborative multiagent setting. As a basis we use the framework of coordination graphs of Guestrin, Koller, and Parr (2002a) which exploits the dependencies between agents to decompose the global payoff function into a sum of local terms. First, we deal with the single-state case and describe a payoff propagation algorithm that computes the individual actions that approximately maximize the global payoff function. The method can be viewed as the decision-making analogue of belief propagation in Bayesian networks. Second, we focus on learning the behavior of the agents in sequential decision-making tasks. We introduce different model-free reinforcementlearning techniques, unitedly called Sparse Cooperative Q-learning, which approximate the global action-value function based on the topology of a coordination graph, and perform updates using the contribution of the individual agents to the maximal global action value. The combined use of an edge-based decomposition of the action-value function and the payoff propagation algorithm for efﬁcient action selection, result in an approach that scales only linearly in the problem size. We provide experimental evidence that our method outperforms related multiagent reinforcement-learning methods based on temporal differences. Keywords: collaborative multiagent system, coordination graph, reinforcement learning, Qlearning, belief propagation</p><p>6 0.16185726 <a title="59-lda-6" href="./jmlr-2006-A_Graphical_Representation_of_Equivalence_Classes_of_AMP_Chain_Graphs.html">2 jmlr-2006-A Graphical Representation of Equivalence Classes of AMP Chain Graphs</a></p>
<p>7 0.1610048 <a title="59-lda-7" href="./jmlr-2006-Streamwise_Feature_Selection.html">88 jmlr-2006-Streamwise Feature Selection</a></p>
<p>8 0.16080573 <a title="59-lda-8" href="./jmlr-2006-Large_Scale_Multiple_Kernel_Learning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">43 jmlr-2006-Large Scale Multiple Kernel Learning     (Special Topic on Machine Learning and Optimization)</a></p>
<p>9 0.16056517 <a title="59-lda-9" href="./jmlr-2006-Learning_a_Hidden_Hypergraph.html">53 jmlr-2006-Learning a Hidden Hypergraph</a></p>
<p>10 0.16045868 <a title="59-lda-10" href="./jmlr-2006-Kernels_on_Prolog_Proof_Trees%3A_Statistical_Learning_in_the_ILP_Setting_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Inductive_Programming%29.html">42 jmlr-2006-Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting     (Special Topic on Inductive Programming)</a></p>
<p>11 0.15955049 <a title="59-lda-11" href="./jmlr-2006-Walk-Sums_and_Belief_Propagation_in_Gaussian_Graphical_Models.html">95 jmlr-2006-Walk-Sums and Belief Propagation in Gaussian Graphical Models</a></p>
<p>12 0.15927197 <a title="59-lda-12" href="./jmlr-2006-Linear_Programs_for_Hypotheses_Selection_in_Probabilistic_Inference_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">56 jmlr-2006-Linear Programs for Hypotheses Selection in Probabilistic Inference Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>13 0.15794122 <a title="59-lda-13" href="./jmlr-2006-Kernel-Based_Learning_of_Hierarchical_Multilabel_Classification_Models_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">41 jmlr-2006-Kernel-Based Learning of Hierarchical Multilabel Classification Models     (Special Topic on Machine Learning and Optimization)</a></p>
<p>14 0.15713644 <a title="59-lda-14" href="./jmlr-2006-Learning_Minimum_Volume_Sets.html">48 jmlr-2006-Learning Minimum Volume Sets</a></p>
<p>15 0.1569296 <a title="59-lda-15" href="./jmlr-2006-A_Hierarchy_of_Support_Vector_Machines_for_Pattern_Detection.html">3 jmlr-2006-A Hierarchy of Support Vector Machines for Pattern Detection</a></p>
<p>16 0.15656976 <a title="59-lda-16" href="./jmlr-2006-Efficient_Learning_of_Label_Ranking_by_Soft_Projections_onto_Polyhedra_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Machine_Learning_and_Optimization%29.html">26 jmlr-2006-Efficient Learning of Label Ranking by Soft Projections onto Polyhedra     (Special Topic on Machine Learning and Optimization)</a></p>
<p>17 0.15648535 <a title="59-lda-17" href="./jmlr-2006-Consistency_and_Convergence_Rates_of_One-Class_SVMs_and_Related_Algorithms.html">23 jmlr-2006-Consistency and Convergence Rates of One-Class SVMs and Related Algorithms</a></p>
<p>18 0.1549048 <a title="59-lda-18" href="./jmlr-2006-Online_Passive-Aggressive_Algorithms.html">70 jmlr-2006-Online Passive-Aggressive Algorithms</a></p>
<p>19 0.15435067 <a title="59-lda-19" href="./jmlr-2006-Toward_Attribute_Efficient_Learning_of_Decision_Lists_and_Parities.html">92 jmlr-2006-Toward Attribute Efficient Learning of Decision Lists and Parities</a></p>
<p>20 0.15410955 <a title="59-lda-20" href="./jmlr-2006-Pattern_Recognition_for__Conditionally_Independent_Data.html">73 jmlr-2006-Pattern Recognition for  Conditionally Independent Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
