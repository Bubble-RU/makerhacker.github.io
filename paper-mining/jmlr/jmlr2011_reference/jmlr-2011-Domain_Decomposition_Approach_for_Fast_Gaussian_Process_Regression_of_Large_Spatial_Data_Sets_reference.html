<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>27 jmlr-2011-Domain Decomposition Approach for Fast Gaussian Process Regression of Large Spatial Data Sets</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-27" href="../jmlr2011/jmlr-2011-Domain_Decomposition_Approach_for_Fast_Gaussian_Process_Regression_of_Large_Spatial_Data_Sets.html">jmlr2011-27</a> <a title="jmlr-2011-27-reference" href="#">jmlr2011-27-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>27 jmlr-2011-Domain Decomposition Approach for Fast Gaussian Process Regression of Large Spatial Data Sets</h1>
<br/><p>Source: <a title="jmlr-2011-27-pdf" href="http://jmlr.org/papers/volume12/park11a/park11a.pdf">pdf</a></p><p>Author: Chiwoo Park, Jianhua Z. Huang, Yu Ding</p><p>Abstract: Gaussian process regression is a ﬂexible and powerful tool for machine learning, but the high computational complexity hinders its broader applications. In this paper, we propose a new approach for fast computation of Gaussian process regression with a focus on large spatial data sets. The approach decomposes the domain of a regression function into small subdomains and infers a local piece of the regression function for each subdomain. We explicitly address the mismatch problem of the local pieces on the boundaries of neighboring subdomains by imposing continuity constraints. The new approach has comparable or better computation complexity as other competing methods, but it is easier to be parallelized for faster computation. Moreover, the method can be adaptive to non-stationary features because of its local nature and, in particular, its use of different hyperparameters of the covariance function for different local regions. We illustrate application of the method and demonstrate its advantages over existing methods using two synthetic data sets and two real spatial data sets. Keywords: domain decomposition, boundary value problem, Gaussian process regression, parallel computation, spatial prediction</p><br/>
<h2>reference text</h2><p>Roland Becker and Rolf Rannacher. An optimal control approach to a posteriori error estimation in ﬁnite element methods. Acta Numerica, 10:1–102, 2001. Tao Chen and Jianghong Ren. Bagging for Gaussian process regression. Neurocomputing, 72(7-9): 1605–1610, 2009. Alexandre Ern and Jean-Luc Guermond. Theory and Practice of Finite Elements. Springer, 2004. Reinhard Furrer, Marc G. Genton, and Douglas Nychka. Covariance tapering for interpolation of large spatial datasets. Journal of Computational and Graphical Statistics, 15(3):502–523, 2006. Tilmann Gneiting. Compactly supported correlation functions. Journal of Multivariate Analysis, 83(2):493–508, 2002. 1727  PARK , H UANG AND D ING  Robert B. Gramacy and Herbert K. H. Lee. Bayesian treed Gaussian process models with an application to computer modeling. Journal of the American Statistical Association, 103(483):1119– 1130, 2008. Alﬁo Quarteroni and Alberto Valli. Domain Decomposition Methods for Partial Differential Equations. Oxford University Press, 1999. Joaquin Qui˜ onero-Candela and Carl E. Rasmussen. A unifying view of sparse approximate n Gaussian process regression. Journal of Machine Learning Research, 6:1939–1959, 2005. Carl E. Rasmussen and Zoubin Ghahramani. Inﬁnite mixtures of Gaussian process experts. In Advances in Neural Information Processing Systems 14, pages 881–888. MIT Press, 2002. Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006. Anton Schwaighofer, Marian Grigoras, Volker Tresp, and Clemens Hoffmann. Transductive and inductive methods for approximate Gaussian process regression. In Advances in Neural Information Processing Systems 16, pages 977–984. MIT Press, 2003. Matthias Seeger, Christopher K. I. Williams, and Neil D. Lawrence. Fast forward selection to speed up sparse Gaussian process regression. In International Workshop on Artiﬁcial Intelligence and Statistics 9. Society for Artiﬁcial Intelligence and Statistics, 2003. Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems 18, pages 1257–1264. MIT Press, 2006. Edward Snelson and Zoubin Ghahramani. Local and global sparse Gaussian process approximations. In International Conference on Artiﬁcal Intelligence and Statistics 11, pages 524–531. Society for Artiﬁcial Intelligence and Statistics, 2007. Volker Tresp. A Bayesian committee machine. Neural Computation, 12(11):2719–2741, 2000. Raquel Urtasun and Trevor Darrell. Sparse probabilistic regression for activity-independent human pose inference. In IEEE Conference on Computer Vision and Pattern Recognition 2008, pages 1–8. IEEE, 2008. Christopher K. I. Williams and Matthias Seeger. Using the Nystr¨ m method to speed up kernel o machines. In Advances in Neural Information Processing Systems 12, pages 682–688. MIT Press, 2000.  1728</p>
<br/>
<br/><br/><br/></body>
</html>
