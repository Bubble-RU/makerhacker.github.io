<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 jmlr-2011-A Refined Margin Analysis for Boosting Algorithms via Equilibrium Margin</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-5" href="../jmlr2011/jmlr-2011-A_Refined_Margin_Analysis_for_Boosting_Algorithms_via_Equilibrium_Margin.html">jmlr2011-5</a> <a title="jmlr-2011-5-reference" href="#">jmlr2011-5-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 jmlr-2011-A Refined Margin Analysis for Boosting Algorithms via Equilibrium Margin</h1>
<br/><p>Source: <a title="jmlr-2011-5-pdf" href="http://jmlr.org/papers/volume12/wang11a/wang11a.pdf">pdf</a></p><p>Author: Liwei Wang, Masashi Sugiyama, Zhaoxiang Jing, Cheng Yang, Zhi-Hua Zhou, Jufu Feng</p><p>Abstract: Much attention has been paid to the theoretical explanation of the empirical success of AdaBoost. The most inﬂuential work is the margin theory, which is essentially an upper bound for the generalization error of any voting classiﬁer in terms of the margin distribution over the training data. However, important questions were raised about the margin explanation. Breiman (1999) proved a bound in terms of the minimum margin, which is sharper than the margin distribution bound. He argued that the minimum margin would be better in predicting the generalization error. Grove and Schuurmans (1998) developed an algorithm called LP-AdaBoost which maximizes the minimum margin while keeping all other factors the same as AdaBoost. In experiments however, LP-AdaBoost usually performs worse than AdaBoost, putting the margin explanation into serious doubt. In this paper, we make a reﬁned analysis of the margin theory. We prove a bound in terms of a new margin measure called the Equilibrium margin (Emargin). The Emargin bound is uniformly ©2011 Liwei Wang, Masashi Sugiyama, Zhaoxiang Jing, Cheng Yang, Zhi-Hua Zhou and Jufu Feng. WANG , S UGIYAMA , J ING , YANG , Z HOU AND F ENG sharper than Breiman’s minimum margin bound. Thus our result suggests that the minimum margin may be not crucial for the generalization error. We also show that a large Emargin and a small empirical error at Emargin imply a smaller bound of the generalization error. Experimental results on benchmark data sets demonstrate that AdaBoost usually has a larger Emargin and a smaller test error than LP-AdaBoost, which agrees well with our theory. Keywords: boosting, margin bounds, voting classiﬁer</p><br/>
<h2>reference text</h2><p>A. Asuncion and D. J. Newman. UCI machine learning repository, 2007. http://www.ics.uci.edu/∼mlearn/MLRepository.html.  URL  P. Bartlett, M. Jordan, and J.D. McAuliffe. Convexity, classiﬁcation, and risk bounds. Journal of the American Statistical Association, 101:138–156, 2006. E. Bauer and R. Kohavi. An empirical comparison of voting classiﬁcation algorithms: Bagging, boosting and variants. Machine Learning, 36:105–139, 1999. 1861  WANG , S UGIYAMA , J ING , YANG , Z HOU AND F ENG  L. Breiman. Arcing classiﬁers. The Annals of Statistics, 26:801–849, 1998. L. Breiman. Prediction games and arcing algorithms. Neural Computation, 11:1493–1517, 1999. L. Breiman. Population theory for boosting ensembles. Annals of Statistics, 32:1–11, 2004. R. Caruana and A. Niculescu-Mizil. An empirical comparison of supervised learning algorithms. In 23th International Conference on Machine Learning, 2006. L. Devroye. Bounds for the uniform deviation of empirical measures. Journal of Multivariate Analysis, 12:72–79, 1982. T. Dietterich. An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting and randomization. Machine Learning, 40:139–157, 2000. Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm. In International Conference on Machine Learning, 1996. Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55:119–139, 1997. J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: A statistical view of boosting. Annals of Statistics, 28:337–407, 2000. A. J. Grove and D. Schuurmans. Boosting in the limit: Maximizing the margin of learned ensembles. In National Conference on Artiﬁcial Intelligence, 1998. W. Hoeffding. Probability inequalities for sum of bounded random variables. Journal of American Statistical Society, 58:13–30, 1963. W. Jiang. Process consistency for adaboost. The Annals of Statistics, 32:13–29, 2004. V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error of combined classiﬁers. Annals of Statistics, 30:1–50, 2002. V. Koltchinskii and D. Panchenko. Complexities of convex combinations and bounding the generalization error in classiﬁcation. Annals of Statistics, 33:1455–1496, 2005. J. Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine Learning Research, 6:273–306, 2005. G. Lugosi and Nicolas Vayatis. On the bayes-risk consistency of regularized boosting methods. The Annals of Statistics, 32:30–55, 2004. D. Mease and A. Wyner. Evidence contrary to the statistical view of boosting. Journal of Machine Learning Research, 9:131–156, 2008. R. Meir and G. R¨ tsch. An introduction to boosting and leveraging. In Advanced Lectures on a Machine Learning, pages 118–183, 2003. J. R. Quinlan. Bagging, boosting, and c4.5. In 13th International Conference on Artiﬁcial Intelligence, 1996. 1862  A R EFINED M ARGIN A NALYSIS FOR B OOSTING A LGORITHMS VIA E QUILIBRIUM M ARGIN  G. R¨ tsch and M. Warmuth. Efﬁcient margin maximization with boosting. Journal of Machine a Learning Research, 6:2131–2152, 2005. L. Reyzin and R. E. Schapire. How boosting the margin can also boost classiﬁer complexity. In International Conference on Machine Learning, 2006. C. Rudin, I. Daubechies, and R. Schapire. The dynamics of AdaBoost: Cyclic behavior and convergence of margins. Journal of Machine Learning Research, 5:1557–1595, Dec 2004. C. Rudin, I. Daubechies, and R. Schapire. Analysis of boosting algorithms using the smooth margin function. Annals of Statistics, 35:2723–2768, 2007. N. Sauer. On the density of family of sets. Journal of Combinatorial Theory, Series A, 13:145–147, 1972. R. Schapire, Y. Freund, P. Bartlett, and W. Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. Annals of Statistics, 26:1651–1686, 1998. V. Vapnik. Statistical Learning Theory. John Wiley and Sons Inc., 1998. V. N. Vapnik and A. YA. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and Its Applications, 16:264–280, 1971. P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2001. L. Wang, C. Yang, and J. Feng. On learning with dissimilarity functions. In 24th International Conference on Machine Learning, 2007. L. Wang, M. Sugiyama, C. Yang, Z. Zhou, and J. Feng. On the margin explanation of boosting algorithms. In 21th Annual Conference on Learning Theory, 2008. T. Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. The Annals of Statistics, 32:56–85, 2004.  1863</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
