<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 jmlr-2011-A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-3" href="../jmlr2011/jmlr-2011-A_Cure_for_Variance_Inflation_in_High_Dimensional_Kernel_Principal_Component_Analysis.html">jmlr2011-3</a> <a title="jmlr-2011-3-reference" href="#">jmlr2011-3-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3 jmlr-2011-A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis</h1>
<br/><p>Source: <a title="jmlr-2011-3-pdf" href="http://jmlr.org/papers/volume12/abrahamsen11a/abrahamsen11a.pdf">pdf</a></p><p>Author: Trine Julie Abrahamsen, Lars Kai Hansen</p><p>Abstract: Small sample high-dimensional principal component analysis (PCA) suffers from variance inﬂation and lack of generalizability. It has earlier been pointed out that a simple leave-one-out variance renormalization scheme can cure the problem. In this paper we generalize the cure in two directions: First, we propose a computationally less intensive approximate leave-one-out estimator, secondly, we show that variance inﬂation is also present in kernel principal component analysis (kPCA) and we provide a non-parametric renormalization scheme which can quite efﬁciently restore generalizability in kPCA. As for PCA our analysis also suggests a simpliﬁed approximate expression. Keywords: PCA, kernel PCA, generalizability, variance renormalization</p><br/>
<h2>reference text</h2><p>Michael Biehl and Andreas Mietzner. Statistical mechanics of unsupervised structure recognition. Journal of Physics A-Mathematical and General, 27(6):1885–1897, 1994. Gilles Blanchard, Olivier Bousquet, and Laurent Zwald. Statistical properties of kernel principal component analysis. Machine Learning, 66(2-3):259–294, 2007. Mikio L. Braun, Joachim M. Buhmann, and Klaus-Robert M¨ ller. On relevant dimensions in kernel u feature spaces. Journal of Machine Learning Research, 9:1875–1908, 2008. Rafael C. Gonzalez and Paul Wintz. Digital Image Processing. 1977. ISBN 0-201-02596-5 (hardcover), 0-201-02597-3 (paperback). David C. Hoyle and Magnus Rattray. A statistical mechanics analysis of gram matrix eigenvalue spectra. In Lecture Notes in Computer Science, 17th Annual Conference on Learning Theory, volume 3120, pages 579–593. Springer Verlag, 2004a. David C. Hoyle and Magnus Rattray. Limiting form of the sample covariance eigenspectrum in pca and kernel pca. In Advances in Neural Information Processing Systems 16, pages 16–23. MIT Press, 2004b. 2043  A BRAHAMSEN AND H ANSEN  David C. Hoyle and Magnus Rattray. Principal-component-analysis eigenvalue spectra from data with symmetry-breaking structure. Physical Review E, 69(2):026124, 2004c. David C. Hoyle and Magnus Rattray. Statistical mechanics of learning multiple orthogonal signals: Asymptotic theory and ﬂuctuation effects. Physical Review E (Statistical, Nonlinear, and Soft Matter Physics), 75(1):016101, 2007. Jonathan J . Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(5):550–554, 1994. Robert Jenssen, Torbj¨ rn Eltoft, Deniz Erdogmus, and Jose C. Principe. Some equivalences between o kernel methods and information theoretic methods. Journal of VLSI Signal Processing, 45:49–65, 2006. Iain M. Johnstone. On the distribution of the largest eigenvalue in principal components analysis. Annals of Statistics, 29(2):295–327, 2001. Ulrik Kjems, Lars K. Hansen, and Stephen C. Strother. Generalizable singular value decomposition for ill-posed datasets. In Advances in Neural Information Processing Systems 13, pages 549–555. MIT Press, 2001. Rudy Moddemeijer. On estimation of entropy and mutual information of continuous distributions. Signal Processing, 16(3):233–246, 1989. Soﬁa Mosci, Lorenzo Rosasco, and Alessandro Verri. Dimensionality reduction and generalization. In Proceedings of the 24th International Conference on Machine Learning, pages 657–664, 2007. Peter Reimann, Chris Van den Broeck, and Geert J. Bex. A Gaussian scenario for unsupervised learning. Journal of Physics A - Mathematical and General, 29(13):3521–3535, 1996. Bernhard Sch¨ lkopf, Alex Smola, and Klaus-Robert M¨ ller. Nonlinear component analysis as a o u kernel eigenvalue problem. Neural Computation, 10(5):1299–1319, 1998. John Shawe-Taylor and Christopher K. I. Williams. The stability of kernel principal components analysis and its relation to the process eigenspectrum. In Advances in Neural Information Processing Systems 15, pages 367–374. MIT Press, 2003. Jack W. Silverstein and Patrick L. Combettes. Signal-detection via spectral theory of large dimensional random matrices. IEEE Transactions on Signal Processing, 40(8):2100–2105, 1992. Laurent Zwald and Gilles Blanchard. On the convergence of eigenspaces in kernel principal component analysis. In Advances in Neural Information Processing Systems 18, pages 1649–1656. MIT Press, 2006.  2044</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
