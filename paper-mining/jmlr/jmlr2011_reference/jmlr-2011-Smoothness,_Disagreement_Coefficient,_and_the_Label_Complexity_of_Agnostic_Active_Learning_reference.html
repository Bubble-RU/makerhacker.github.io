<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>85 jmlr-2011-Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-85" href="../jmlr2011/jmlr-2011-Smoothness%2C_Disagreement_Coefficient%2C_and_the_Label_Complexity_of_Agnostic_Active_Learning.html">jmlr2011-85</a> <a title="jmlr-2011-85-reference" href="#">jmlr2011-85-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>85 jmlr-2011-Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning</h1>
<br/><p>Source: <a title="jmlr-2011-85-pdf" href="http://jmlr.org/papers/volume12/wang11b/wang11b.pdf">pdf</a></p><p>Author: Liwei Wang</p><p>Abstract: We study pool-based active learning in the presence of noise, that is, the agnostic setting. It is known that the effectiveness of agnostic active learning depends on the learning problem and the hypothesis space. Although there are many cases on which active learning is very useful, it is also easy to construct examples that no active learning algorithm can have an advantage. Previous works have shown that the label complexity of active learning relies on the disagreement coefﬁcient which often characterizes the intrinsic difﬁculty of the learning problem. In this paper, we study the disagreement coefﬁcient of classiﬁcation problems for which the classiﬁcation boundary is smooth and the data distribution has a density that can be bounded by a smooth function. We prove upper and lower bounds for the disagreement coefﬁcients of both ﬁnitely and inﬁnitely smooth problems. Combining with existing results, it shows that active learning is superior to passive supervised learning for smooth problems. Keywords: active learning, disagreement coefﬁcient, label complexity, smooth function</p><br/>
<h2>reference text</h2><p>M.-F. Balcan, A.Beygelzimer, and J. Langford. Agnostic active learning. In 23th International Conference on Machine Learning, 2006. A. Beygelzimer, S. Dasgupta, and J. Langford. Importance weighted active learning. In 26th International Conference on Machine Learning, 2009. A. Beygelzimer, D. Hsu, J. Langford, and T. Zhang. Agnostic active learning without constraints. In Advances in Neural Information Processing Systems, 2010. M. Burnashev and K. Zigangirov. An interval estimation problem for controlled problem. Problems of Information Transmission, 10:223–231, 1974. R. Castro and R. Nowak. Minimax bounds for active learning. IEEE Transactions on Information Theory, 54:2339–2353, 2008. D. Cohn, L. Atlas, and R. Ladner. Improving generalization with active learning. Machine Learning, 15:201–221, 1994. S. Dasgupta. Coarse sample complexity bounds for active learning. In Advances in Neural Information Processing Systems, 2005. S. Dasgupta and J. Langford. A tutorial on active learning, 2009. S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In Advances in Neural Information Processing Systems, 2007. R.M. Dudley. Uniform Central Limit Theorems. Cambridge University Press, 1999. E. Friedman. Active learning for smooth problems. In 22th Annual Conference on Learning Theory, 2009. A. Gorny. Contribution a l’etude des fonctions d´ rivables d’une variable r´ elle. Acta Mathematica, e e 13:317–358, 1939. S. Hanneke. A bound on the label complexity of agnostic active learning. In 24th International Conference on Machine Learning, 2007. S. Hanneke. Adaptive rates of convergence in active learning. In 22th Annual Conference on Learning Theory, 2009. S. Hanneke. Rates of convergence in active learning. Annals of Statistics, 39:333–361, 2011. M. K¨ ari¨ inen. Active learning in the non-realizable case. In 17th International Conference on a¨ a Algorithmic Learning Theory, 2006. 2291  WANG  A. Kolmogorov. Une g´ n´ ralisation de l’in´ galite de J. Hadamard entre les bornes sup´ rieurs des e e e e d´ riv´ s successives d’une fonction. C. R. Acad´ mie des Sciences, Paris, 207:763–765, 1938. e e e V. Koltchinskii. Local rademacher complexities and oracle inequalities in risk minimization. The Annals of Statistics, 34:2593–2656, 2006. V. Koltchinskii. Rademacher complexities and bounding the excess risk in active learning. Journal of Machine Learning Research, 11:2457–2485, 2010. E. Landau. Einige ungleichungen f¨ r zweimal differentiierbare funktionen. Proceedings of the u London Mathematical Society, 13:43–49, 1913. D.S. Mitrinovi´ , J.E. Peˇ ari´ , and A.M. Fink. Inequalities Involving Functions and Their Integrals c c e and Derivatives. Kluwer Academic Publishers, 1991. I.J. Schoenberg. The elementary cases of landau’s problem of inequlities between derivatives. The American Mathematical Monthly, 80:121–158, 1973. A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals of Statistics, 32: 135–166, 2004. A. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes with Application to Statistics. Springer Verlag, 1996. V. Vapnik. Statistical Learning Theory. John Wiely and Sons, 1998. L. Wang. Sufﬁcient conditions for agnostic active learnable. In Advances in Neural Information Processing Systems, 2009.  2292</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
