<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 jmlr-2011-Minimum Description Length Penalization for Group and Multi-Task Sparse Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-64" href="../jmlr2011/jmlr-2011-Minimum_Description_Length_Penalization_for_Group_and_Multi-Task_Sparse_Learning.html">jmlr2011-64</a> <a title="jmlr-2011-64-reference" href="#">jmlr2011-64-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>64 jmlr-2011-Minimum Description Length Penalization for Group and Multi-Task Sparse Learning</h1>
<br/><p>Source: <a title="jmlr-2011-64-pdf" href="http://jmlr.org/papers/volume12/dhillon11a/dhillon11a.pdf">pdf</a></p><p>Author: Paramveer S. Dhillon, Dean Foster, Lyle H. Ungar</p><p>Abstract: We propose a framework MIC (Multiple Inclusion Criterion) for learning sparse models based on the information theoretic Minimum Description Length (MDL) principle. MIC provides an elegant way of incorporating arbitrary sparsity patterns in the feature space by using two-part MDL coding schemes. We present MIC based models for the problems of grouped feature selection (MICG ROUP) and multi-task feature selection (MIC-M ULTI). MIC-G ROUP assumes that the features are divided into groups and induces two level sparsity, selecting a subset of the feature groups, and also selecting features within each selected group. MIC-M ULTI applies when there are multiple related tasks that share the same set of potentially predictive features. It also induces two level sparsity, selecting a subset of the features, and then selecting which of the tasks each feature should be added to. Lastly, we propose a model, T RANS F EAT, that can be used to transfer knowledge from a set of previously learned tasks to a new task that is expected to share similar features. All three methods are designed for selecting a small set of predictive features from a large pool of candidate features. We demonstrate the effectiveness of our approach with experimental results on data from genomics and from word sense disambiguation problems.1 Keywords: feature selection, minimum description length principle, multi-task learning</p><br/>
<h2>reference text</h2><p>H. Akaike. Information theory and the extension of the maximum likelihood principle. In 2nd International Symposium on Information Theory, Budapest, pages 261–281, 1973. R. Ando. Applying alternating structure optimization to word sense disambiguation. In (CoNLL-X), 2006. R. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6:1817–1853, 2005. A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243–272, 2008. ISSN 0885-6125. F. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine Learning Research, 9:1179–1225, 2008. F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the smo algorithm. In ICML, 2004. A. R. Barron and T. M. Cover. Minimum complexity density estimation. IEEE Transactions on Information Theory, 37(4):1034–1054, 1991. A. R. Barron, J. Rissanen, and B. Yu. The minimum description length principle in coding and modeling. IEEE Transactions on Information Theory, 44(6):2743–, 1998. P. Bickel and K. Doksum. Mathematical Statistics. Prentice Hall, 2001. R. Caruana. Multitask learning. In Machine Learning, pages 41–75, 1997. C. C Chang and C.J. Lin. LIBSVM: a library for support vector machines, 2001. URL http: //www.csie.ntu.edu.tw/~cjlin/libsvm. J. Chen and M. Palmer. Towards robust high performance word sense disambiguation of english verbs using rich linguistic features. In IJCNLP, pages 933–944, 2005. 560  MDL P ENALIZATION FOR G ROUP AND M ULTI -TASK S PARSE L EARNING  J. Chen, A. I. Schein, L. H. Ungar, and M. Palmer. An empirical study of the behavior of active learning for word sense disambiguation. In HLT-NAACL, 2006. T. M. Cover and J. A. Thomas. Elements of information theory. Wiley-Interscience, New York, NY, USA, 2006. P. S. Dhillon and L. H. Ungar. Transfer Learning, Feature Selection and Word Sense Disambiguation. In Annual Meeting of the Association of Computational Linguistics, (ACL), August 2009. P. S. Dhillon, D. P. Foster, and L. H. Ungar. Efﬁcient Feature Selection in the Presence of Multiple Feature Classes. In International Conference on Data Mining (ICDM), pages 779–784, 2008. P. S. Dhillon, B. Tomasik, D. P. Foster, and L. Ungar. Multi-Task Feature Selection Using The Multiple Inclusion Criterion (MIC). In European Conference on Machine Learning (ECML)PKDD, Lecture Notes in Computer Science. Springer, September 2009. P. S. Dhillon, D. P. Foster, and L. Ungar. Feature selection using multiple streams. In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics, volume 13, 2010. B. Efron, T. Hastie, L. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32:407–499, 2004. R. Florian and D. Yarowsky. Modeling consensus: classiﬁer combination for word sense disambiguation. In EMNLP ’02, pages 25–32, 2002. D. P. Foster and E. I. George. The risk inﬂation criterion for multiple regression. The Annals of Statistics, 22(4):1947–1975, 1994. ISSN 00905364. P. D. Grünwald. A tutorial introduction to the minimum description length principle. In Advances in Minimum Description Length: Theory and Applications. MIT Press, 2005. P. D. Grünwald. The Minimum Description Length Principle (Adaptive Computation and Machine Learning). The MIT Press, 2007. ISBN 0262072815. E. H. Hovy, M. P. Marcus, M. Palmer, L. A. Ramshaw, and R. M. Weischedel. Ontonotes: The 90% solution. In HLT-NAACL, 2006. J. Huang, T. Zhang, and D. Metaxas. Learning with structured sparsity. In ICML ’09, 2009. L. Jacob, G. Obozinski, and J-P. Vert. Group lasso with overlap and graph lasso. In ICML ’09, 2009. T. Jebara. Multi-task feature and kernel selection for SVMs. In Proceedings of the Twenty-ﬁrst International Conference on Machine Learning. ACM New York, NY, USA, 2004. V. Kandylas, S. P. Upham, and L. H. Ungar. Finding cohesive clusters for analyzing knowledge communities. In ICDM, pages 203–212, 2007. K. Kipper, H. T. Dang, and M. Palmer. Class-based construction of a verb lexicon. In AAAI/IAAI, pages 691–696, 2000. 561  D HILLON , F OSTER AND U NGAR  S.-I. Lee, V. Chatalbashev, D. Vickrey, and D. Koller. Learning a meta-level prior for feature relevance from multiple related tasks. In ICML ’07, pages 489–496, 2007. ISBN 978-1-59593-793-3. B. Levin. English Verb Classes and Alternations. University of Chicago Press, 1993. D. Lin. Review of WordNet: an electronic lexical database by Christiane Fellbaum. The MIT Press 1998. Comput. Linguist., 25(2):292–296, 1999. ISSN 0891-2017. D. Lin, E. Pitler, D. P. Foster, and L. H. Ungar. In defense of ℓ0 . In Workshop on Feature Selection,(ICML 2008), 2008. H. Liu and J. Zhang. On the ℓ1 -ℓq regularized regression. Technical report, Carnegie Mellon University, 2008. K. Lounici. Sup-norm convergence rate and sign concentration property of lasso and dantzig estimators. Electronic Journal of Statistics, 2:90–102, 2008. L. Meier, S. van de Geer, and P. Bühlmann. The group lasso for logistic regression. Journal of the Royal Statistical Society. Series B, 70(1):53–71, 2008. N. Meinshausen and P. Bühlmann. High dimensional graphs and variable selection with the lasso. Annals of Statistics, 34:1436–1462, 2006. G. Miller. WordNet: An on-line lexical database. Special Issue: International Journal of Lexicography, 4(3), 1990. V. K. Mootha, C. M. Lindgren, K.-F. Eriksson, A. Subramanian, S. Sihag, J. Lehar, P. Puigserver, E. Carlsson, M. Ridderstrale, E. Laurila, N. Houstis, M. J. Daly, N. Patterson, J. P. Mesirov, T. R. Golub, P. Tamayo, B. Spiegelman, E. S. Lander, J. N. Hirschhorn, D. Altshuler, and L. C. Groop. PGC-1α-responsive genes involved in oxidative phosphorylation are coordinately downregulated in human diabetes. Nature Genetics, 34:267 – 73, 2003. Datasets available at: http://www. broad.mit.edu/gsea/datasets.jsp. Y. Nardi and A. Rinaldo. On the asymptotic properties of the group lasso estimator for linear models. Electronic Journal of Statistics, 2:605–633, 2008. B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing, 24: 227, 1995. G. Obozinski, B. Taskar, and M. I. Jordan. Joint covariate selection and joint subspace selection for multiple classiﬁcation problems. Statistics and Computing, 2009. E. O. Perlstein, D. M. Ruderfer, D. C. Roberts, S. L. Schreiber, and L. Kruglyak. Genetic basis of individual differences in the response to small-molecule drugs in yeast. Nat Genet, 39, 2007. R. Raina, A. Y. Ng, and D. Koller. Constructing informative priors using transfer learning. In ICML ’06, pages 713–720, New York, NY, USA, 2006. ACM. ISBN 1-59593-383-2. A. Rakhlin. Transfer learning toolkit, 2007. Software available at: http://multitask.cs. berkeley.edu. 562  MDL P ENALIZATION FOR G ROUP AND M ULTI -TASK S PARSE L EARNING  A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. SimpleMKL. JMLR, 9:2491–2521, 2008. J. Rissanen. Modeling by shortest data description. Automatica, 14:465–471, 1978. J. Rissanen. A universal prior for integers and estimation by minimum description length. Annals of Statistics, 11(2):416–431, 1983. J. Rissanen. Hypothesis selection and testing by the mdl principle. The Computer Journal, 42: 260–269, 1999. K. K. Schuler. Verbnet: A broad coverage, comprehensive verb lexicon. In Ph.D. Thesis, Computer and Information Sciences, University of Pennsylvania, June 2006. G.: Schwartz. Estimating the dimensions of a model. The Annals of Statistics, 6(2):461–464, 1978. R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, 58:267–288, 1996. J. A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans. Inform. Theory, 50:2231–2242, 2004. B.A. Turlach, W.N. Venables, and S.J. Wright. Simultaneous variable selection. Technometrics, 47 (3):349–363, 2005. L. J. van ’t Veer, H. Dai, M. J. van de Vijver, Y. D. He, A. A. Hart, M. Mao, H. L. Peterse, K. van der Kooy, M. J. Marton, A. T. Witteveen, G. J. Schreiber, R. M. Kerkhoven, C. Roberts, P. S. Linsley, R. Bernards, and S. H. Friend. Gene expression proﬁling predicts clinical outcome of breast cancer. Nature, 415(6871):530–536, January 2002. ISSN 0028-0836. M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using ℓ1 constrained quadratic programming (lasso). IEEE Trans. Inf. Theor., 55(5):2183–2202, 2009. ISSN 0018-9448. Z. Wu and H. H. Zhou. Model selection and sharp asymptotic minimaxity. Under Submission, -(-): –, 2010. ISSN -. M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49–67, February 2006. ISSN 1369-7412. T. Zhang. On the convergence of mdl density estimation. In COLT, pages 315–330, 2004. T. Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1921–1928. Curran Associates, Inc., 2009a. T. Zhang. On the consistency of feature selection using greedy least squares regression. Journal of Machine Learning Research (JMLR), 10:555–568, 2009b. ISSN 1532-4435. P. Zhao and B. Yu. On model selection consistency of lasso. Journal of Machine Learning Research (JMLR), 7:2541–2563, 2006. ISSN 1532-4435. 563  D HILLON , F OSTER AND U NGAR  P. Zhao, G. Rocha, and B. Yu. Grouped and hierarchical model selection through composite absolute penalties. Annals of Statistics, 2008. S. Zhou. Thresholding procedures for high dimensional variable selection and statistical estimation. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 2304–2312. 2009. H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal Of The Royal Statistical Society Series B, 67(2):301–320, 2005.  564</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
