<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>20 jmlr-2011-Convex and Network Flow Optimization for Structured Sparsity</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-20" href="../jmlr2011/jmlr-2011-Convex_and_Network_Flow_Optimization_for_Structured_Sparsity.html">jmlr2011-20</a> <a title="jmlr-2011-20-reference" href="#">jmlr2011-20-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>20 jmlr-2011-Convex and Network Flow Optimization for Structured Sparsity</h1>
<br/><p>Source: <a title="jmlr-2011-20-pdf" href="http://jmlr.org/papers/volume12/mairal11a/mairal11a.pdf">pdf</a></p><p>Author: Julien Mairal, Rodolphe Jenatton, Guillaume Obozinski, Francis Bach</p><p>Abstract: We consider a class of learning problems regularized by a structured sparsity-inducing norm deﬁned as the sum of ℓ2 - or ℓ∞ -norms over groups of variables. Whereas much effort has been put in developing fast optimization techniques when the groups are disjoint or embedded in a hierarchy, we address here the case of general overlapping groups. To this end, we present two different strategies: On the one hand, we show that the proximal operator associated with a sum of ℓ∞ norms can be computed exactly in polynomial time by solving a quadratic min-cost ﬂow problem, allowing the use of accelerated proximal gradient methods. On the other hand, we use proximal splitting techniques, and address an equivalent formulation with non-overlapping groups, but in higher dimension and with additional constraints. We propose efﬁcient and scalable algorithms exploiting these two strategies, which are signiﬁcantly faster than alternative approaches. We illustrate these methods with several problems such as CUR matrix factorization, multi-task learning of tree-structured dictionaries, background subtraction in video sequences, image denoising with wavelets, and topographic dictionary learning of natural image patches. Keywords: convex optimization, proximal methods, sparse coding, structured sparsity, matrix factorization, network ﬂow optimization, alternating direction method of multipliers</p><br/>
<h2>reference text</h2><p>R. K. Ahuja, T. L. Magnanti, and J. Orlin. Network Flows. Prentice Hall, 1993. A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243–272, 2008. M. Babenko and A.V. Goldberg. Experimental evaluation of a parametric ﬂow algorithm. Technical report, Microsoft Research, 2006. MSR-TR-2006-77. 2715  M AIRAL , J ENATTON , O BOZINSKI AND BACH  F. Bach. High-dimensional non-linear variable selection through hierarchical kernel learning. Technical report, arXiv:0909.0844, 2009. F. Bach. Structured sparsity-inducing norms through submodular functions. In Advances in Neural Information Processing Systems, 2010. F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Convex optimization with sparsity-inducing norms. In S. Sra, S. Nowozin, and S. J. Wright, editors, Optimization for Machine Learning. MIT Press, 2011. To appear. R. G. Baraniuk, V. Cevher, M. Duarte, and C. Hegde. Model-based compressive sensing. IEEE Transactions on Information Theory, 56(4):1982–2001, 2010. A. Barron, J. Rissanen, and B. Yu. The minimum description length principle in coding and modeling. IEEE Transactions on Information Theory, 44(6):2743–2760, 1998. A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009. D. P. Bertsekas. Network Optimization: Continuous and Discrete Models. Athena Scientiﬁc, 1998. D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc Belmont, 1999. D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. Prentice Hall Inc., 1989. P. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Annals of Statistics, 37(4):1705–1732, 2009. J. Bien, Y. Xu, and M. W. Mahoney. CUR from a sparse optimization viewpoint. In Advances in Neural Information Processing Systems, 2010. J. M. Borwein and A. S. Lewis. Convex Analysis and Nonlinear Optimization: Theory and Examples. Springer, 2006. S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1–122, 2011. S. P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004. Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max-ﬂow algorithms for energy minimization in vision. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(9):1124–1137, 2004. P. Brucker. An O(n) algorithm for quadratic knapsack problems. Operations Research Letters, 3: 163–166, 1984. T. T. Cai. Adaptive wavelet estimation: a block thresholding and oracle inequality approach. Annals of Statistics, pages 898–924, 1999. 2716  C ONVEX AND N ETWORK F LOW O PTIMIZATION FOR S TRUCTURED S PARSITY  V. Cehver, M. F. Duarte, C. Hedge, and R. G. Baraniuk. Sparse signal recovery using Markov random ﬁelds. In Advances in Neural Information Processing Systems, 2008. A. Chambolle and J. Darbon. On total variation minimization and surface evolution using parametric maximal ﬂows. International Journal of Computer Vision, 84(3), 2009. S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientiﬁc Computing, 20:33–61, 1999. X. Chen, Q. Lin, S. Kim, J. Pena, J. G. Carbonell, and E. P. Xing. An efﬁcient proximal-gradient method for single and multi-task regression with structured sparsity. Technical report, 2010. ArXiv:1005.4717v1. B. V. Cherkassky and A. V. Goldberg. On implementing the push-relabel method for the maximum ﬂow problem. Algorithmica, 19(4):390–410, 1997. P. L. Combettes and J.-C. Pesquet. A proximal decomposition method for solving convex variational inverse problems. Inverse Problems, 24(27), 2008. Art. 065014. P. L. Combettes and J.-C. Pesquet. Proximal splitting methods in signal processing. In Fixed-Point Algorithms for Inverse Problems in Science and Engineering. Springer, 2010. D. L. Donoho and I. M. Johnstone. Adapting to unknown smoothness via wavelet shrinkage. Journal of the American Statistical Association, 90(432):1200–1224, 1995. J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efﬁcient projections onto the ℓ1 -ball for learning in high dimensions. In Proceedings of the International Conference on Machine Learning (ICML), 2008. B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32 (2):407–499, 2004. M. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned dictionaries. IEEE Transactions on Image Processing, 54(12):3736–3745, December 2006. L. R. Ford and D. R. Fulkerson. Maximal ﬂow through a network. Canadian Journal of Mathematics, 8(3):399–404, 1956. J. Friedman, T. Hastie, and R. Tibshirani. A note on the group Lasso and a sparse group Lasso. Technical report, Preprint arXiv:1001.0736, 2010. S. Fujishige. Submodular Functions and Optimization. Elsevier, 2005. G. Gallo, M. E. Grigoriadis, and R. E. Tarjan. A fast parametric maximum ﬂow algorithm and applications. SIAM J. Comput., 18:30–55, 1989. P. Garrigues and B. Olshausen. Group sparse coding with a Laplacian scale mixture prior. In Advances in Neural Information Processing Systems, 2010. A. V. Goldberg and R. E. Tarjan. A new approach to the maximum ﬂow problem. In Proc. of ACM Symposium on Theory of Computing, pages 136–146, 1986. 2717  M AIRAL , J ENATTON , O BOZINSKI AND BACH  D. Goldfarg and S. Ma. Fast multiple splitting algorithms for convex optimization. Technical report, 2009. Preprint arXiv:0912.4570v1. H. Groenevelt. Two algorithms for maximizing a separable concave function over a polymatroid feasible region. Europeran Journal of Operations Research, pages 227–236, 1991. L. He and L. Carin. Exploiting structure in wavelet-based Bayesian compressive sensing. IEEE Transactions on Signal Processing, 57(9):3488–3497, 2009. D. S. Hochbaum and S. P. Hong. About strongly polynomial time algorithms for quadratic optimization over submodular constraints. Mathematical Programming, 69(1):269–309, 1995. H. Hoeﬂing. A path algorithm for the fused lasso signal approximator. Journal of Computational and Graphical Statistics, 19(4):984–1006, 2010. R. A. Horn and C. R. Johnson. Matrix analysis. Cambridge University Press, 1990. J. Huang and T. Zhang. The beneﬁt of group sparsity. Annals of Statistics, 38(4):1978–2004, 2010. J. Huang, Z. Zhang, and D. Metaxas. Learning with structured sparsity. In Proceedings of the International Conference on Machine Learning (ICML), 2009. A. Hyv¨ rinen, P. Hoyer, and M. Inki. Topographic independent component analysis. Neural Coma putation, 13(7):1527–1558, 2001. L. Jacob, G. Obozinski, and J.-P. Vert. Group Lasso with overlap and graph Lasso. In Proceedings of the International Conference on Machine Learning (ICML), 2009. R. Jenatton, J-Y. Audibert, and F. Bach. Structured variable selection with sparsity-inducing norms. Technical report, 2009. Preprint arXiv:0904.3523v3. R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical dictionary learning. In Proceedings of the International Conference on Machine Learning (ICML), 2010a. R. Jenatton, G. Obozinski, and F. Bach. Structured sparse principal component analysis. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2010b. R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for hierarchical sparse coding. Journal of Machine Learning Research, 12:2297–2334, 2011. K. Kavukcuoglu, M. Ranzato, R. Fergus, and Y. LeCun. Learning invariant features through topographic ﬁlter maps. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009. S. Kim and E. P. Xing. Tree-guided group Lasso for multi-task regression with structured sparsity. In Proceedings of the International Conference on Machine Learning (ICML), 2010. N. Maculan and J. R. G. Galdino de Paula. A linear-time median-ﬁnding algorithm for projecting a vector on the simplex of Rn . Operations Research Letters, 8(4):219–222, 1989. 2718  C ONVEX AND N ETWORK F LOW O PTIMIZATION FOR S TRUCTURED S PARSITY  M. W. Mahoney and P. Drineas. CUR matrix decompositions for improved data analysis. Proceedings of the National Academy of Sciences, 106(3):697, 2009. J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Non-local sparse models for image restoration. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2009. J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse coding. Journal of Machine Learning Research, 11:19–60, 2010a. J. Mairal, R. Jenatton, G. Obozinski, and F. Bach. Network ﬂow algorithms for structured sparsity. In Advances in Neural Information Processing Systems, 2010b. S. Mallat. A Wavelet Tour of Signal Processing, Second Edition. Academic Press, New York, September 1999. C. A. Micchelli, J. M. Morales, and M. Pontil. A family of penalty functions for structured sparsity. In Advances in Neural Information Processing Systems, 2010. J. J. Moreau. Fonctions convexes duales et points proximaux dans un espace Hilbertien. Compte Rendus de l’Acad´ mie des Sciencs, Paris, S´ rie A, Math´ matiques, 255:2897–2899, 1962. e e e D. Needell and J. A. Tropp. CoSaMP: Iterative signal recovery from incomplete and inaccurate samples. Applied and Computational Harmonic Analysis, 26(3):301–321, 2009. S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A uniﬁed framework for highdimensional analysis of M-estimators with decomposable regularizers. In Advances in Neural Information Processing Systems, 2009. Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103(1): 127–152, 2005. Y. Nesterov. Gradient methods for minimizing composite objective function. Technical report, Center for Operations Research and Econometrics (CORE), Catholic University of Louvain, 2007. G. Obozinski, B. Taskar, and M. I. Jordan. Joint covariate selection and joint subspace selection for multiple classiﬁcation problems. Statistics and Computing, 20(2):231–252, 2010. B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381:607–609, 1996. Z. Qin and D. Goldfarb. Structured sparsity via alternating directions methods. Technical report, 2011. preprint ArXiv:1105.0728. A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. SimpleMKL. Journal of Machine Learning Research, 9:2491–2521, 2008. V. Roth and B. Fischer. The group-Lasso for generalized linear models: uniqueness of solutions and efﬁcient algorithms. In Proceedings of the International Conference on Machine Learning (ICML), 2008. 2719  M AIRAL , J ENATTON , O BOZINSKI AND BACH  M. Schmidt and K. Murphy. Convex structure learning in log-linear models: Beyond pairwise potentials. In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2010. M. Schmidt, N. Le Roux, and F. Bach. Convergence rates of inexact proximal-gradient methods for convex optimization. In Advances in Neural Information Processing Systems, 2011. to appear, preprint ArXiv:1109.2415v1. J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004. P. Sprechmann, I. Ramirez, G. Sapiro, and Y. C. Eldar. Collaborative hierarchical sparse modeling. Technical report, 2010. Preprint arXiv:1003.0400v1. M. Stojnic, F. Parvaresh, and B. Hassibi. On the reconstruction of block-sparse signals with an optimal number of measurements. IEEE Transactions on Signal Processing, 57(8):3075–3085, 2009. R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society: Series B, 58(1):267–288, 1996. R. Tomioka, T. Suzuki, and M. Sugiyama. Augmented Lagrangian methods for learning, selecting and combining features. In S. Sra, S. Nowozin, and S. J. Wright, editors, Optimization for Machine Learning. MIT Press, 2011. To appear. P. Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization. Journal of optimization theory and applications, 109(3):475–494, 2001. B. A. Turlach, W. N. Venables, and S. J. Wright. Simultaneous variable selection. Technometrics, 47(3):349–363, 2005. M. J. Wainwright. Sharp thresholds for noisy and high-dimensional recovery of sparsity using ℓ1 constrained quadratic programming (Lasso). IEEE Transactions on Information Theory, 55(5): 2183–2202, May 2009. J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma. Robust face recognition via sparse representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(2):210– 227, 2009a. S. Wright, R. Nowak, and M. Figueiredo. Sparse reconstruction by separable approximation. IEEE Transactions on Signal Processing, 57(7):2479–2493, 2009b. M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B, 68:49–67, 2006. X. Zhang, M. Burger, and S. Osher. A uniﬁed primal-dual algorithm framework based on Bregman iteration. Journal of Scientiﬁc Computing, 46(1):20–46, 2011. P. Zhao, G. Rocha, and B. Yu. The composite absolute penalties family for grouped and hierarchical variable selection. Annals of Statistics, 37(6A):3468–3497, 2009. 2720</p>
<br/>
<br/><br/><br/></body>
</html>
