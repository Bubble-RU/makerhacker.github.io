<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>25 jmlr-2011-Discriminative Learning of Bayesian Networks via Factorized Conditional Log-Likelihood</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-25" href="../jmlr2011/jmlr-2011-Discriminative_Learning_of_Bayesian_Networks_via_Factorized_Conditional_Log-Likelihood.html">jmlr2011-25</a> <a title="jmlr-2011-25-reference" href="#">jmlr2011-25-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>25 jmlr-2011-Discriminative Learning of Bayesian Networks via Factorized Conditional Log-Likelihood</h1>
<br/><p>Source: <a title="jmlr-2011-25-pdf" href="http://jmlr.org/papers/volume12/carvalho11a/carvalho11a.pdf">pdf</a></p><p>Author: Alexandra M. Carvalho, Teemu Roos, Arlindo L. Oliveira, Petri Myllymäki</p><p>Abstract: We propose an efﬁcient and parameter-free scoring criterion, the factorized conditional log-likelihood (ˆ fCLL), for learning Bayesian network classiﬁers. The proposed score is an approximation of the conditional log-likelihood criterion. The approximation is devised in order to guarantee decomposability over the network structure, as well as efﬁcient estimation of the optimal parameters, achieving the same time and space complexity as the traditional log-likelihood scoring criterion. The resulting criterion has an information-theoretic interpretation based on interaction information, which exhibits its discriminative nature. To evaluate the performance of the proposed criterion, we present an empirical comparison with state-of-the-art classiﬁers. Results on a large suite of benchmark data sets from the UCI repository show that ˆ fCLL-trained classiﬁers achieve at least as good accuracy as the best compared classiﬁers, using signiﬁcantly less computational resources. Keywords: Bayesian networks, discriminative learning, conditional log-likelihood, scoring criterion, classiﬁcation, approximation c 2011 Alexandra M. Carvalho, Teemu Roos, Arlindo L. Oliveira and Petri Myllym¨ ki. a ¨ C ARVALHO , ROOS , O LIVEIRA AND M YLLYM AKI</p><br/>
<h2>reference text</h2><p>A. J. Bell. The co-information lattice. In Proc. ICA’03, pages 921–926, 2003. J. Bilmes. Dynamic Bayesian multinets. In Proc. UAI’00, pages 38–45. Morgan Kaufmann, 2000. A. M. Carvalho. Scoring function for learning Bayesian networks. Technical report, INESC-ID Tec. Rep. 54/2009, 2009. A. M. Carvalho, A. L. Oliveira, and M.-F. Sagot. Efﬁcient learning of Bayesian network classiﬁers: An extension to the TAN classiﬁer. In M. A. Orgun and J. Thornton, editors, Proc. IA’07, volume 4830 of LNCS, pages 16–25. Springer, 2007. D. M. Chickering. A transformational characterization of equivalent Bayesian network structures. In Proc. UAI’95, pages 87–98. Morgan Kaufmann, 1995. D. M. Chickering. Learning Bayesian networks is NP-complete. In D. Fisher and H.-J. Lenz, editors, Learning from Data: AI and Statistics V, pages 121–130. Springer, 1996. D. M. Chickering. Learning equivalence classes of Bayesian-network structures. Journal of Machine Learning Research, 2:445–498, 2002. D. M. Chickering, D. Heckerman, and C. Meek. Large-sample learning of Bayesian networks is NP-hard. Journal of Machine Learning Research, 5:1287–1330, 2004. C. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees. IEEE Transactions on Information Theory, 14(3):462–467, 1968. T. Cover and J. Thomas. Elements of Information Theory. John Wiley & Sons, 2006. S. Dasgupta. Learning polytrees. In Proc. UAI’99, pages 134–141. Morgan Kaufmann, 1999. L. M. de Campos. A scoring function for learning Bayesian networks based on mutual information and conditional independence tests. Journal of Machine Learning Research, 7:2149–2187, 2006. P. Domingos and M. J. Pazzani. On the optimality of the simple Bayesian classiﬁer under zero-one loss. Machine Learning, 29(2–3):103–130, 1997. J. Edmonds. Optimum branchings. Journal of Research of the National Bureau of Standards, 71B: 233–240, 1967. U. M. Fayyad and K. B. Irani. Multi-interval discretization of continuous-valued attributes for classiﬁcation learning. In Proc. IJCAI’93, pages 1022–1029. Morgan Kaufmann, 1993. N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian network classiﬁers. Machine Learning, 29 (2-3):131–163, 1997. R. Greiner and W. Zhou. Structural extension to logistic regression: Discriminative parameter learning of belief net classiﬁers. In Proc. AAAI/IAAI’02, pages 167–173. AAAI Press, 2002. R. Greiner, X. Su, B. Shen, and W. Zhou. Structural extension to logistic regression: Discriminative parameter learning of belief net classiﬁers. Machine Learning, 59(3):297–322, 2005. 2208  FACTORIZED C ONDITIONAL L OG -L IKELIHOOD  D. Grossman and P. Domingos. Learning Bayesian network classiﬁers by maximizing conditional likelihood. In Proc. ICML’04, pages 46–53. ACM Press, 2004. M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. The WEKA data mining software: An update. SIGKDD Explorations, 11(1):10–18, 2009. D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20(3):197–243, 1995. C.-W. Hsu, C.-C. Chang, and C.-J. Lin. A practical guide to support vector classiﬁcation. Technical report, Department of Computer Science, National Taiwan University, 2003. A. Jakulin. Machine Learning Based on Attribute Interactions. PhD thesis, University of Ljubljana, 2005. R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proc. IJCAI’95, pages 1137–1145. Morgan Kaufmann, 1995. R. Kohavi and G. H. John. Wrappers for feature subset selection. Artiﬁcial Intelligence, 97(1-2): 273–324, 1997. P. Kontkanen, P. Myllym¨ ki, T. Silander, and H. Tirri. BAYDA: Software for Bayesian classiﬁcation a and feature selection. In Proc. KDD’98, pages 254–258. AAAI Press, 1998. E. Lawler. Combinatorial Optimization: Networks and Matroids. Dover, 1976. W. J. McGill. Multivariate information transmission. Psychometrika, 19:97–116, 1954. C. Meek. Finding a path is harder than ﬁnding a tree. Journal of Artiﬁcial Intelligence Research, 15:383–389, 2001. D. J. Newman, S. Hettich, C. L. Blake, and C. J. Merz. UCI repository of machine learning databases, 1998. URL http://www.ics.uci.edu/˜mlearn/MLRepository.html. J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Francisco, CA, USA, 1988. S. V. Pemmaraju and S. S. Skiena. Computational Discrete Mathematics: Combinatorics and Graph Theory with Mathematica. Cambridge University Press, 2003. F. Pernkopf and J. A. Bilmes. Discriminative versus generative parameter and structure learning of Bayesian network classiﬁers. In Proc. ICML’05, pages 657–664. ACM Press, 2005. T. Roos, H. Wettig, P. Gr¨ nwald, P. Myllym¨ ki, and H. Tirri. On discriminative Bayesian network u a classiﬁers and logistic regression. Machine Learning, 59(3):267–296, 2005. T. Silander, T. Roos, and P. Myllym¨ ki. Learning locally minimax optimal Bayesian networks. a International Journal of Approximate Reasoning, 51(5):544–557, 2010. J. Su and H. Zhang. Full Bayesian network classiﬁers. In Proc. ICML’06, pages 897–904. ACM Press, 2006. 2209  ¨ C ARVALHO , ROOS , O LIVEIRA AND M YLLYM AKI  J. Su, H. Zhang, C. X. Ling, and S. Matwin. Discriminative parameter learning for Bayesian networks. In Proc ICML’08, pages 1016–1023. ACM Press, 2008. T. Verma and J. Pearl. Equivalence and synthesis of causal models. In Proc. UAI’90, pages 255–270. Elsevier, 1990. S. Yang and K.-C. Chang. Comparison of score metrics for Bayesian network learning. IEEE Transactions on Systems, Man, and Cybernetics, Part A, 32(3):419–428, 2002.  2210</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
