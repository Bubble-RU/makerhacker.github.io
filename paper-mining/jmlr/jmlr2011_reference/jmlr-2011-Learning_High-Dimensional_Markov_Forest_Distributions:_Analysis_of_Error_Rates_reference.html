<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-53" href="../jmlr2011/jmlr-2011-Learning_High-Dimensional_Markov_Forest_Distributions%3A_Analysis_of_Error_Rates.html">jmlr2011-53</a> <a title="jmlr-2011-53-reference" href="#">jmlr2011-53-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>53 jmlr-2011-Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates</h1>
<br/><p>Source: <a title="jmlr-2011-53-pdf" href="http://jmlr.org/papers/volume12/tan11a/tan11a.pdf">pdf</a></p><p>Author: Vincent Y.F. Tan, Animashree Anandkumar, Alan S. Willsky</p><p>Abstract: The problem of learning forest-structured discrete graphical models from i.i.d. samples is considered. An algorithm based on pruning of the Chow-Liu tree through adaptive thresholding is proposed. It is shown that this algorithm is both structurally consistent and risk consistent and the error probability of structure learning decays faster than any polynomial in the number of samples under ﬁxed model size. For the high-dimensional scenario where the size of the model d and the number of edges k scale with the number of samples n, sufﬁcient conditions on (n, d, k) are given for the algorithm to satisfy structural and risk consistencies. In addition, the extremal structures for learning are identiﬁed; we prove that the independent (resp., tree) model is the hardest (resp., easiest) to learn using the proposed algorithm in terms of error rates for structure learning. Keywords: graphical models, forest distributions, structural consistency, risk consistency, method of types</p><br/>
<h2>reference text</h2><p>P. Abbeel, D. Koller, and A. Y. Ng. Learning factor graphs in polynomial time and sample complexity. Journal of Machine Learning Research, Dec 2006. M. Aigner and G. M. Ziegler. Proofs From THE BOOK. Springer, 2009. F. Bach and M. I. Jordan. Beyond independent components: trees and clusters. Journal of Machine Learning Research, 4:1205–1233, 2003. D. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 1999. C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2008. S. Borade and L. Zheng. Euclidean information theory. In IEEE International Zurich Seminar on Communications, pages 14–17, 2008. G. Bresler, E. Mossel, and A. Sly. Reconstruction of Markov random ﬁelds from samples: Some observations and algorithms. In 11th International workshop APPROX 2008 and 12th International workshop RANDOM, pages 343–356., 2008. A. Chechetka and C. Guestrin. Efﬁcient principled learning of thin junction trees. In Advances of Neural Information Processing Systems (NIPS), 2007. C. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees. IEEE Transactions on Infomation Theory, 14(3):462–467, May 1968. C. K. Chow and T. Wagner. Consistency of an estimate of tree-dependent probability distributions . IEEE Transactions in Information Theory, 19(3):369 – 371, May 1973. T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley-Interscience, 2nd edition, 2006. I. Csisz´ r and F. Mat´ s. Information projections revisited. IEEE Transactions on Infomation Theory, a uˇ 49(6):1474–1490, 2003. I. Csisz´ r and P. Shields. The consistency of the BIC Markov order estimator. Ann. Statist., 28(6): a 1601–1619, 2000. 1651  TAN , A NANDKUMAR AND W ILLSKY  I. Csisz´ r and Z. Talata. Context tree estimation for not necessarily ﬁnite memory processes, via bic a and mdl. IEEE Transactions on Information Theory, 52(3):1007–16, 2006. A. Custovic, B. M. Simpson, C. S. Murray, L. Lowe, and A. Woodcock. The national asthma campaign Manchester asthma and allergy study. Pediatr Allergy Immunol, 13:32–37, 2002. A. Dembo and O. Zeitouni. Large Deviations Techniques and Applications. Springer, 2nd edition, 1998. F. Den Hollander. Large Deviations (Fields Institute Monographs, 14). American Mathematical Society, Feb 2000. M. Dudik, S. J. Phillips, and R. E. Schapire. Performance guarantees for regularized maximum entropy density estimation. In Conference on Learning Theory (COLT), 2004. R. M. Fano. Transmission of Information. New York: Wiley, 1961. L. Finesso, C. C. Liu, and P. Narayan. The optimal error exponent for Markov order estimation. IEEE Trans. on Info Th., 42(5):1488–1497, 1996. R. G. Gallager. Claude E. Shannon: A retrospective on his life, work and impact. IEEE Trans. on Info. Th., 47:2687–95, Nov 2001. E. Gassiat and S. Boucheron. Optimal error exponents in hidden Markov models order estimation. IEEE Transactions on Infomation Theory, 49(4):964–980, Apr 2003. J. Johnson, V. Chandrasekaran, and A. S. Willsky. Learning Markov structure by maximum entropy relaxation. In Artiﬁcial Intelligence and Statistics (AISTATS), 2007. J. B. Kruskal. On the shortest spanning subtree of a graph and the traveling salesman problem. Proceedings of the American Mathematical Society, 7(1), Feb 1956. S. Lauritzen. Graphical Models. Oxford University Press, USA, 1996. S. Lee, V. Ganapathi, and D. Koller. Efﬁcient structure learning of Markov networks using L1regularization. In Advances in Neural Information Processing Systems (NIPS), 2006. H. Liu, M. Xu, H. Gu, A. Gupta, J. Lafferty, and L. Wasserman. Forest density estimation. Journal of Machine Learning Research, 12:907–951, March 2011. M. Meil˘ and M. I. Jordan. Learning with mixtures of trees. Journal of Machine Learning Research, a 1:1–48, Oct 2000. N. Meinshausen and P. Buehlmann. High dimensional graphs and variable selection with the Lasso. Annals of Statistics, 34(3):1436–1462, 2006. N. Merhav. The estimation of the model order in exponential families. IEEE Transactions on Infomation Theory, 35(5):1109–1115, 1989. N. Merhav, M. Gutman., and J. Ziv. On the estimation of the order of a Markov chain and universal data compression. IEEE Transactions on Infomation Theory, 35:1014–1019, 1989. 1652  L EARNING H IGH -D IMENSIONAL M ARKOV F OREST D ISTRIBUTIONS  D. J. Newman, S. Hettich, C. L. Blake, and C. J. Merz. UCI Repository of Machine Learning Databases, University of California, Irvine, 1998. R. C. Prim. Shortest connection networks and some generalizations. Bell System Technical Journal, 36, 1957. N. Santhanam and M. J. Wainwright. Information-theoretic limits of selecting binary graphical models in high dimensions. In Proc. of IEEE Intl. Symp. on Info. Theory, Toronto, Canada, July 2008. R. J. Serﬂing. Approximation Theorems of Mathematical Statistics. Wiley-Interscience, Nov 1980. G. Simon. Additivity of information in exponential family probability laws. Amer. Statist. Assoc., 68(478–482), 1973. A. Simpson, V. Y. F. Tan, J. M. Winn, M. Svens´ n, C. M. Bishop, D. E. Heckerman, I. Buchan, e and A. Custovic. Beyond atopy: Multiple patterns of sensitization in relation to asthma in a birth cohort study. Am J Respir Crit Care Med, 2010. V. Y. F. Tan, A. Anandkumar, and A. S. Willsky. Learning Gaussian tree models: Analysis of error exponents and extremal structures. IEEE Transactions on Signal Processing, 58(5):2701 – 2714, May 2010a. V. Y. F. Tan, A. Anandkumar, and A. S. Willsky. Error exponents for composite hypothesis testing of Markov forest distributions. In Proc. of Intl. Symp. on Info. Th., June 2010b. V. Y. F. Tan, A. Anandkumar, L. Tong, and A. S. Willsky. A large-deviation analysis for the maximum-likelihood learning of Markov tree structures. IEEE Transactions on Infomation Theory, Mar 2011. L. Vandenberghe and S. Boyd. Semideﬁnite programming. SIAM Review, 38:49–95, Mar 1996. M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Technical report, University of California, Berkeley, 2003. M. J. Wainwright, P. Ravikumar, and J. D. Lafferty. High-dimensional graphical model selection using ℓ1 -regularized logistic regression. In Advances of Neural Information Processing Systems (NIPS), pages 1465–1472, 2006. O. Zuk, S. Margel, and E. Domany. On the number of samples needed to learn the correct structure of a Bayesian network. In Proc of Uncertainty in Artiﬁcial Intelligence (UAI), 2006.  1653</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
