<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 jmlr-2011-Laplacian Support Vector Machines  Trained in the Primal</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-51" href="../jmlr2011/jmlr-2011-Laplacian_Support_Vector_Machines__Trained_in_the_Primal.html">jmlr2011-51</a> <a title="jmlr-2011-51-reference" href="#">jmlr2011-51-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>51 jmlr-2011-Laplacian Support Vector Machines  Trained in the Primal</h1>
<br/><p>Source: <a title="jmlr-2011-51-pdf" href="http://jmlr.org/papers/volume12/melacci11a/melacci11a.pdf">pdf</a></p><p>Author: Stefano Melacci, Mikhail Belkin</p><p>Abstract: In the last few years, due to the growing ubiquity of unlabeled data, much effort has been spent by the machine learning community to develop better understanding and improve the quality of classiﬁers exploiting unlabeled data. Following the manifold regularization approach, Laplacian Support Vector Machines (LapSVMs) have shown the state of the art performance in semi-supervised classiﬁcation. In this paper we present two strategies to solve the primal LapSVM problem, in order to overcome some issues of the original dual formulation. In particular, training a LapSVM in the primal can be efﬁciently performed with preconditioned conjugate gradient. We speed up training by using an early stopping strategy based on the prediction on unlabeled data or, if available, on labeled validation examples. This allows the algorithm to quickly compute approximate solutions with roughly the same classiﬁcation accuracy as the optimal ones, considerably reducing the training time. The computational complexity of the training algorithm is reduced from O(n3 ) to O(kn2 ), where n is the combined number of labeled and unlabeled examples and k is empirically evaluated to be signiﬁcantly smaller than n. Due to its simplicity, training LapSVM in the primal can be the starting point for additional enhancements of the original LapSVM formulation, such as those for dealing with large data sets. We present an extensive experimental evaluation on real world data showing the beneﬁts of the proposed approach. Keywords: Laplacian support vector machines, manifold regularization, semi-supervised learning, classiﬁcation, optimization</p><br/>
<h2>reference text</h2><p>J. Abernethy, O. Chapelle, and C. Castillo. Witch: A new approach to web spam detection. Technical Report 2008-001, Yahoo! Research, 2008. M. Belkin and P. Niyogi. Using manifold stucture for partially labeled classiﬁcation. Advances in Neural Information Processing Systems, pages 953–960, 2003. M. Belkin and P. Niyogi. Towards a theoretical foundation for Laplacian-based manifold methods. Journal of Computer and System Sciences, 74(8):1289–1308, 2008. M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. The Journal of Machine Learning Research, 7: 2399–2434, 2006. S.P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge university press, 2004. O. Chapelle. Training a support vector machine in the primal. Neural Computation, 19(5):1155– 1178, 2007. O. Chapelle, J. Weston, and B. Sch¨ lkopf. Cluster kernels for semi-supervised learning. In Advances o in Neural Information Processing Systems, pages 585–592. Cambridge, MA, USA: MIT Press, 2003. O. Chapelle, B. Sch¨ lkopf, and A. Zien. Semi-supervised learning. MIT press, 2006. o O. Chapelle, V. Sindhwani, and S.S. Keerthi. Optimization techniques for semi-supervised support vector machines. The Journal of Machine Learning Research, 9:203–233, 2008. 1178  L APLACIAN SVM S T RAINED IN THE P RIMAL  Classiﬁer SVM RLSC LapRLSC LapSVM Dual (Original) LapSVM Primal (Newton)  σ 17.5 17.5 17.5 17.5 17.5  nn 50 50 50  p 5 5 5  γA 10−1 1 10−6 1 10−1  γI 10−2 10 10  COIL20(B)  SVM RLSC LapRLSC LapSVM Dual (Original) LapSVM Primal (Newton)  0.6 0.6 0.6 0.6 0.6  2 2 2  1 1 1  10−6 10−6 10−6 10−2 10−6  1 100 1  PCMAC  SVM RLSC LapRLSC LapSVM Dual (Original) LapSVM Primal (Newton)  2.7 2.7 2.7 2.7 2.7  50 50 50  5 5 5  10−6 10−6 10−6 10−6 10−6  10−2 10−4 1  USPST(B)  SVM RLSC LapRLSC LapSVM Dual (Original) LapSVM Primal (Newton)  9.4 9.4 9.4 9.4 9.4  10 10 10  2 2 2  10−6 10−1 10−4 10−6 10−6  10−1 10−2 10−2  COIL20  SVM RLSC LapRLSC LapSVM Dual (Original) LapSVM Primal (Newton)  0.6 0.6 0.6 0.6 0.6  2 2 2  1 1 1  10−6 10−6 10−6 10−6 10−6  1 10 1  USPST  SVM RLSC LapRLSC LapSVM Dual (Original) LapSVM Primal (Newton)  9.4 9.4 9.4 9.4 9.4  10 10 10  2 2 2  10−1 10−6 10−6 10−6 10−4  10−1 10−2 1  MNIST3VS8  SVM RLSC LapRLSC LapSVM Dual (Original) LapSVM Primal (Newton)  9 9 9 9 9  20 20 20  3 3 3  10−6 10−6 10−6 10−6 10−6  10−2 10−2 10−2  FACEMIT  SVM RLSC LapSVM Primal (PCG)  4.3 4.3 4.3  6  1  10−6 10−6 10−6  10−8  Data Set  G50C  Table 9: Parameters selected by cross-validation for supervised algorithms (SVM, RLSC) and semi-supervised ones based on manifold regularization, using different loss functions (LapRLSC, LapSVM trained in the dual formulation and in the primal one by means of Newton’s method). The parameter σ is the bandwidth of the Gaussian kernel or, in the MNIST3VS8, the degree of the polynomial one.  1179  M ELACCI AND B ELKIN  γA  γI  G50C  Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  10−1 10−1 10−1 10−1  10 10 10 10  COIL20(B)  Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  10−6 10−6 1 10−6  1 1 100 1  PCMAC  Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  10−6 10−4 10−4 10−6  1 1 1 10−1  USPST(B)  Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  10−6 10−6 10−6 10−6  10−2 1 1 1  COIL20  Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  10−6 10−6 10−6 10−6  1 1 1 1  USPST  Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  10−4 10−4 10−4 10−4  1 1 1 1  MNIST3VS8  Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  10−6 10−6 10−6 10−6  10−2 10−1 10−1 10−1  FACEMIT  PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  10−6 10−6 10−6  10−8 10−8 10−8  Data Set  Laplacian SVM  Table 10: A comparison of the parameters selected by cross-validation for Laplacian SVMs trained in the primal by means of Newton’s method (Newton) and preconditioned conjugate gradient (PCG) with the proposed early stopping conditions (in square brackets).  1180  L APLACIAN SVM S T RAINED IN THE P RIMAL  Data Set  Laplacian SVM Dual Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  Training Time 0.155 (0.004) 0.134 (0.006) 0.044 (0.006) 0.043 (0.006) 0.044 (0.006)  PCG Iters 20 (0) 20.83 (2.89) 20.83 (2.89)  LS Iters 1 (0) 1 (0) 1 (0)  COIL20(B)  Dual Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  0.311 (0.012) 0.367 (0.097) 0.198 (0.074) 0.095 (0.018) 0.206 (0.089)  74.67 (28.4) 36 (7.24) 78.67 (34.42)  2.41 (1.83) 3.26 (2.21) 2.38 (1.79)  PCMAC  Dual Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  14.8203 (0.104) 15.756 (0.285) 1.901 (0.022) 1.970 (0.265) 1.969 (0.268)  38.00 (0) 39.58 (5.48) 39.58 (5.48)  1.18 (0.45) 1.18 (0.44) 1.18 (0.44)  USPST(B)  Dual Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  1.196 (0.015) 1.4727 (0.2033) 0.496 (0.172) 0.279 (0.096) 0.567 (0.226)  95.00 (33.40) 52.25 (18.34) 107.67 (43.88)  6.56 (3.18) 6.83 (3.44) 6.49 (3.15)  COIL20  Dual Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  6.321 (0.441) 7.26 (1.921) 3.297 (1.471) 1.769 (0.299) 3.487 (1.734)  65.47 (30.35) 34.07 (6.12) 69.53 (35.86)  2.53 (1.90) 3.37 (2.22) 2.48 (1.87)  USPST  Dual Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  12.25 (0.2) 17.74 (2.44) 1.953 (0.403) 2.032 (0.434) 2.158 (0.535)  41.17 (8.65) 42.91 (9.38) 45.60 (11.66)  3.11 (1.73) 3.13 (1.73) 3.12 (1.72)  MNIST3VS8  Dual Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  2064.18 (3.1) 2824.174 (105.07) 188.775 (0.237) 207.986 (35.330) 207.915 (35.438)  165 (0) 183.33 (31.75) 183.33 (31.75)  6.78 (3.65) 6.65 (3.57) 6.65 (3.57)  FACEMIT  PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  35.728 (0.868) 35.728 (0.868) 35.728 (0.868)  3 (0) 3 (0) 3 (0)  1 (0) 1 (0) 1 (0)  G50C  Table 11: Training time comparison among the Laplacian SVMs trained in the dual (Dual), LapSVM trained in the primal by means of Newton’s method (Newton) and by means of preconditioned conjugate gradient (PCG) with the proposed early stopping conditions (in square brackets). Parameters of the classiﬁers were tuned using the Newton’s method. Average training times (in seconds) and their standard deviations, the number of PCG iterations, and of Line Search (LS) iterations (per each PCG one) are reported.  1181  M ELACCI AND B ELKIN  Data Set  Laplacian SVM  U  V  T  G50C  Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  6.16 (1.48) 6.13 (1.46) 6.16 (1.48) 6.16 (1.48)  6.17 (3.46) 6.17 (3.46) 6.17 (3.46) 6.17 (3.46)  7.27 (2.87) 7.27 (2.87) 7.27 (2.87) 7.27 (2.87)  COIL20(B)  Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  8.16 (2.04) 8.81 (2.23) 8.97 (2.32) 8.84 (2.28)  7.92 (3.96) 8.13 (3.71) 9.17 (3.74) 8.13 (3.71)  8.56 (1.9) 8.84 (1.93) 8.96 (1.64) 8.84 (1.96)  PCMAC  Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  9.68 (0.77) 9.65 (0.76) 9.65 (0.76) 9.65 (0.76)  7.83 (4.04) 7.83 (4.04) 7.83 (4.04) 7.83 (4.04)  9.37 (1.51) 9.42 (1.43) 9.40 (1.43) 9.40 (1.43)  USPST(B)  Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  8.72 (2.15) 11.07 (2.27) 12.02 (2.22) 10.81 (2.39)  9.33 (3.85) 13.33 (4.21) 14.67 (2.99) 12.83 (4.78)  9.42 (2.34) 11.49 (2.55) 12.01 (2.14) 11.31 (2.71)  COIL20  Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  10.54 (2.03) 12.42 (2.68) 13.07 (2.73) 12.43 (2.69)  9.79 (4.94) 10.63 (4.66) 12.08 (4.75) 10.42 (4.63)  11.32 (2.19) 12.92 (2.14) 13.52 (2.12) 12.87 (2.20)  USPST  Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  14.98 (2.88) 15.60 (3.45) 15.40 (3.38) 15.45 (3.53)  15 (3.57) 15.67 (3.60) 15.67 (3.98) 15.50 (3.92)  15.38 (3.55) 16.11 (3.95) 15.94 (4.04) 15.94 (4.08)  MNIST3VS8  Newton PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  2.2 (0.14) 3.16 (0.15) 2.89 (0.62) 2.89 (0.62)  1.67 (1.44) 2.5 (1.25) 2.50 (1.25) 2.5 (1.25)  2.02 (0.22) 2.4 (0.38) 2.37 (0.44) 2.37 (0.44)  FACEMIT  PCG [Stability Check] PCG [Validation Check] PCG [Mixed Check]  29.97 (2.51) 29.97 (2.51) 29.97 (2.51)  36 (3.46) 36 (3.46) 36 (3.46)  27.97 (5.38) 27.97 (5.38) 27.97 (5.38)  Table 12: Average classiﬁcation error (standard deviation is reported brackets) of Laplacian SVMs trained in the primal by means of Newton’s method and of preconditioned conjugate gradient (PCG) with the proposed early stopping conditions (in square brackets). Parameters of the classiﬁers were tuned using the Newton’s method. U is the set of unlabeled examples used to train the classiﬁers. V is the labeled set for cross-validating parameters whereas T is the out-of-sample test set. Results on the labeled training set L are omitted since all classiﬁers perfectly ﬁt such few labeled training points.  1182  L APLACIAN SVM S T RAINED IN THE P RIMAL  D. Decoste and B. Sch¨ lkopf. Training invariant support vector machines. Machine Learning, 46 o (1):161–190, 2002. A. Demiriz and K. Bennett. Optimization approaches to semi-supervised learning. Complementarity: Applications, Algorithms and Extensions, 50:1–19, 2000. R.E. Fan, P.H. Chen, and C.J. Lin. Working set selection using second order information for training support vector machines. The Journal of Machine Learning Research, 6:1889–1918, 2005. T. Joachims. Transductive inference for text classiﬁcation using support vector machines. In Proceedings of the International Conference on Machine Learning, pages 200–209, 1999. T. Joachims. Transductive learning via spectral graph partitioning. In Proceedings of the International Conference on Machine Learning, volume 20, pages 290–297, 2003. T. Joachims. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 217–226. ACM New York, NY, USA, 2006. S.S. Keerthi and D. DeCoste. A modiﬁed ﬁnite Newton method for fast solution of large scale linear SVMs. The Journal of Machine Learning Research, 6(1):341–361, 2005. S.S. Keerthi, O. Chapelle, and D. DeCoste. Building support vector machines with reduced classiﬁer complexity. The Journal of Machine Learning Research, 7:1493–1515, 2006. W. Li, K.-H. Lee, and K.-S. Leung. Large-scale RLSC learning without agony. In Proceedings of the 24th International Conference on Machine learning, pages 529–536, New York, NY, USA, 2007. ACM. C.-J. Lin, R. C. Weng, and S. S. Keerthi. Trust region Newton methods for large-scale logistic regression. The Journal of Machine Learning Research, 9:627–650, 2008. O. L. Mangasarian. A ﬁnite newton method for classiﬁcation. Optimization Methods and Software, 17(5):913–929, 2002. M. Seeger. Low rank updates for the Cholesky decomposition. Department of EECS, University of California at Berkeley, Technical Report, 2008. S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In Proceedings of the International Conference on Machine Learning, pages 807–814, 2007. J.R. Shewchuk. An introduction to the conjugate gradient method without the agonizing pain. School of Computer Science, Carnegie Mellon University, Techical Report, 1994. V. Sindhwani. On Semi-supervised Kernel Methods. PhD thesis, University of Chicago, 2007. V. Sindhwani and D.S. Rosenberg. An RKHS for multi-view learning and manifold coregularization. In Proceedings of the International Conference on Machine Learning, pages 976–983, 2008. 1183  M ELACCI AND B ELKIN  V. Sindhwani, P. Niyogi, and M. Belkin. Beyond the point cloud: From transductive to semisupervised learning. In Proceedings of the International Conference on Machine Learning, volume 22, pages 825–832, 2005. I.W. Tsang and J.T. Kwok. Large-scale sparsiﬁed manifold regularization. Advances in Neural Information Processing Systems, 19:1401–1408, 2006. V.N. Vapnik. The Nature of Statistical Learning Theory. Springer, 2000. X. Zhu and A.B. Goldberg. Introduction to Semi-Supervised Learning. Morgan and Claypool, 2009. X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In Proceedings of the International Conference on Machine Learning, 2003.  1184</p>
<br/>
<br/><br/><br/></body>
</html>
