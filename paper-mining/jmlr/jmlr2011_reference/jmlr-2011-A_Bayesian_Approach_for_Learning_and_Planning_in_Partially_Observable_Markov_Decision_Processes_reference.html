<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 jmlr-2011-A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-1" href="../jmlr2011/jmlr-2011-A_Bayesian_Approach_for_Learning_and_Planning_in_Partially_Observable_Markov_Decision_Processes.html">jmlr2011-1</a> <a title="jmlr-2011-1-reference" href="#">jmlr2011-1-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>1 jmlr-2011-A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes</h1>
<br/><p>Source: <a title="jmlr-2011-1-pdf" href="http://jmlr.org/papers/volume12/ross11a/ross11a.pdf">pdf</a></p><p>Author: Stéphane Ross, Joelle Pineau, Brahim Chaib-draa, Pierre Kreitmann</p><p>Abstract: Bayesian learning methods have recently been shown to provide an elegant solution to the explorationexploitation trade-off in reinforcement learning. However most investigations of Bayesian reinforcement learning to date focus on the standard Markov Decision Processes (MDPs). The primary focus of this paper is to extend these ideas to the case of partially observable domains, by introducing the Bayes-Adaptive Partially Observable Markov Decision Processes. This new framework can be used to simultaneously (1) learn a model of the POMDP domain through interaction with the environment, (2) track the state of the system under partial observability, and (3) plan (near-)optimal sequences of actions. An important contribution of this paper is to provide theoretical results showing how the model can be ﬁnitely approximated while preserving good learning performance. We present approximate algorithms for belief tracking and planning in this model, as well as empirical results that illustrate how the model estimate and agent’s return improve as a function of experience. Keywords: processes reinforcement learning, Bayesian inference, partially observable Markov decision</p><br/>
<h2>reference text</h2><p>J. Asmuth, L. Li, M. Littman, A. Nouri, and D. Wingate. A bayesian sampling approach to exploration in reinforcement learning. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2009. P. Auer and R. Ortner. Logarithmic online regret bounds for undiscounted reinforcement learning. In Neural Information Processing Systems (NIPS), volume 19, pages 49–56, 2006. P. Auer, T. Jaksch, and R. Ortner. Near-optimal regret bounds for reinforcement learning. In Neural Information Processing Systems (NIPS), volume 21, 2009. J. Baxter and P. L. Bartlett. Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial Intelligence Research (JAIR), 15:319–350, 2001. R. Bellman. Adaptive Control Processes: A Guided Tour. Princeton University Press, 1961. R. I. Brafman and M. Tennenholtz. R-max - a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research (JMLR), 3:213–231, 2003. 1767  ROSS , P INEAU , C HAIB - DRAA AND K REITMANN  G. Casella and R. Berger. Statistical Inference. Duxbury Resource Center, 2001. P. S. Castro and D. Precup. Using linear programming for bayesian exploration in markov decision processes. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), pages 2437–2442, 2007. R. Dearden, N. Friedman, and S. J. Russell. Bayesian Q-learning. In AAAI Conference on Artiﬁcial Intelligence, pages 761–768, 1998. R. Dearden, N. Friedman, and D. Andre. Model based bayesian exploration. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 150–159, 1999. E. Delage and S. Mannor. Percentile optimization in uncertain mdps with application to efﬁcient exploration. In International Conference on Machine Learning (ICML), 2007. F. Doshi, J. Pineau, and N. Roy. Reinforcement learning with limited reinforcement: Using Bayes risk for active learning in POMDPs. In International Conference on Machine Learning, pages 256–263. ACM, 2008. F. Doshi-Velez. The inﬁnite partially observable markov decision process. In Neural Information Processing Systems (NIPS), volume 22, 2010. A. Doucet, N. de Freitas, and N. Gordon. Sequential Monte Carlo Methods In Practice. Springer, 2001. M. Duff. Monte-Carlo algorithms for the improvement of ﬁnite-state stochastic controllers: Application to bayes-adaptive Markov decision processes. In International Workshop on Artiﬁcial Intelligence and Statistics (AISTATS), 2001. M. Duff. Optimal Learning: Computational Procedures for Bayes-Adaptive Markov Decision Processes. PhD thesis, University of Massachusetts Amherst, Amherst, MA, 2002. Y. Engel, S. Mannor, and R. Meir. Bayes meets Bellman: The gaussian process approach to temporal difference learning. In International Conference on Machine Learning (ICML), pages 154–161, 2003. Y. Engel, S. Mannor, and R. Meir. Reinforcement learning with gaussian processes. In International Conference on Machine learning (ICML), pages 201–208, 2005. A. A. Feldbaum. Dual control theory, parts i and ii. Automation and Remote Control, 21:874–880 and 1033–1039, 1961. N. M. Filatov and H. Unbehauen. Survey of adaptive dual control methods. In IEEE Control Theory and Applications, volume 147, pages 118–128, 2000. M. Ghavamzadeh and Y. Engel. Bayesian policy gradient algorithms. In Neural Information Processing Systems (NIPS), volume 19, pages 457–464, 2007a. M. Ghavamzadeh and Y. Engel. Bayesian actor-critic algorithms. In International Conference on Machine Learning (ICML), pages 297–304, 2007b. 1768  BAYES -A DAPTIVE POMDP S  A. Greenﬁeld and A. Brockwell. Adaptive control of nonlinear stochastic systems by particle ﬁltering. In International Conference on Control and Automation (ICCA), pages 887–890, 2003. D. Heckerman, D. Geiger, and D. M. Chickering. Learning bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20(3):197–243, 1995. M. Hutter. Universal Artiﬁcial Intelligence. Springer, 2005. R. Jaulmes, J. Pineau, and D. Precup. Active learning in partially observable markov decision processes. European Conference on Machine Learning, pages 601–608, 2005. E. T. Jaynes. Prior probabilities. IEEE Transactions on Systems Science and Cybernetics, 4:227– 241, 1968. H. Jeffreys. Theory of Probability. Oxford University Press, 1961. L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains. Artiﬁcial Intelligence, 101:99–134, 1998. M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. In International Conference on Machine Learning (ICML), pages 260–268, 1998. M. J. Kearns, Y. Mansour, and A. Y. Ng. A sparse sampling algorithm for near-optimal planning in large markov decision processes. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), pages 1324–1331, 1999. J. Zico Kolter and Andrew Y. Ng. Near-bayesian exploration in polynomial time. In International Conference on Machine Learning (ICML), 2009. M. L. Littman, R. S. Sutton, and S. Singh. Predictive representations of state. In Neural Information Processing Systems (NIPS), volume 14, pages 1555–1561, 2002. A. K. McCallum. Reinforcement Learning with Selective Perception and Hidden State. PhD thesis, University of Rochester, 1996. S. Paquet, L. Tobin, and B. Chaib-draa. An online POMDP algorithm for complex multiagent environments. In International Joint Conference on Autonomous Agents and Multi Agent Systems (AAMAS), pages 970–977, 2005. J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: an anytime algorithm for POMDPs. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), pages 1025–1032, 2003. P. Poupart and N. Vlassis. Model-based bayesian reinforcement learning in partially observable domains. In International Symposium on Artiﬁcial Intelligence and Mathematics (ISAIM), 2008. P. Poupart, N. Vlassis, J. Hoey, and K. Regan. An analytic solution to discrete bayesian reinforcement learning. In International Conference on Machine learning (ICML), pages 697–704, 2006. R. Ravikanth, S.P. Meyn, and L.J. Brown. Bayesian adaptive control of time varying systems. In IEEE Conference on Decision and Control, pages 705–709, 1992. 1769  ROSS , P INEAU , C HAIB - DRAA AND K REITMANN  S. Ross, B. Chaib-draa, and J. Pineau. Bayes-adaptive POMDPs. In Neural Information Processing Systems (NIPS), volume 20, pages 1225–1232, 2008a. S. Ross, B. Chaib-draa, and J. Pineau. Bayesian reinforcement learning in continuous POMDPs. In International Conference on Robotics and Automation (ICRA), 2008b. S. Ross, J. Pineau, S. Paquet, and B. Chaib-draa. Online POMDPs. Journal of Artiﬁcial Intelligence Research (JAIR), 32:663–704, 2008c. I. Rusnak. Optimal adaptive control of uncertain stochastic discrete linear systems. In IEEE International Conference on Systems, Man and Cybernetics, pages 4521–4526, 1995. D. Silver and J. Veness. Monte-Carlo planning in large POMDPs. In Neural Information Processing Systems (NIPS), 2010. R. D. Smallwood and E. J. Sondik. The optimal control of partially observable Markov processes over a ﬁnite horizon. Operations Research, 21(5):1071–1088, Sep/Oct 1973. T. Smith and R. Simmons. Heuristic search value iteration for POMDPs. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 520–527, 2004. E. J. Sondik. The Optimal Control of Partially Observable Markov Processes. PhD thesis, Stanford University, 1971. M. T. J. Spaan and N. Vlassis. Perseus: randomized point-based value iteration for POMDPs. Journal of Artiﬁcial Intelligence Research (JAIR), 24:195–220, 2005. A. L. Strehl and M. L. Littman. A theoretical analysis of model-based interval estimation. In International Conference on Machine learning (ICML), pages 856–863, 2005. M. Strens. A bayesian framework for reinforcement learning. In International Conference on Machine Learning (ICML), 2000. R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. The MIT Press, 1998. C. Szepesvari. Algorithms for Reinforcement Learning. Morgan & Claypool, 2010. A. Tewari and P. Bartlett. Optimistic linear programming gives logarithmic regret for irreducible MDPs. In Neural Information Processing Systems (NIPS), volume 20, pages 1505–1512, 2008. J. Veness, K. S. Ng, M. Hutter, W. Uther, and D. Silver. A monte-carlo aixi approximation. Journal of Artiﬁcial Intelligence Research (JAIR), 2011. T. Wang, D. Lizotte, M. Bowling, and D. Schuurmans. Bayesian sparse sampling for on-line reward optimization. In International Conference on Machine learning (ICML), pages 956–963, 2005. O. Zane. Discrete-time bayesian adaptive control problems with complete information. In IEEE Conference on Decision and Control, pages 2748–2749, 1992.  1770</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
