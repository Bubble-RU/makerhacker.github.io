<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 jmlr-2011-Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-8" href="../jmlr2011/jmlr-2011-Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization.html">jmlr2011-8</a> <a title="jmlr-2011-8-reference" href="#">jmlr2011-8-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>8 jmlr-2011-Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</h1>
<br/><p>Source: <a title="jmlr-2011-8-pdf" href="http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">pdf</a></p><p>Author: John Duchi, Elad Hazan, Yoram Singer</p><p>Abstract: We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to ﬁnd needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which signiﬁcantly simpliﬁes setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efﬁcient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms. Keywords: subgradient methods, adaptivity, online learning, stochastic convex optimization</p><br/>
<h2>reference text</h2><p>J. Abernethy, P. L. Bartlett, A. Rakhlin, and A. Tewari. Optimal strategies and minimax lower bounds for online convex games. In Proceedings of the Twenty First Annual Conference on Computational Learning Theory, 2008. T. Ando. Concavity of certain maps on positive deﬁnite matrices and applications to Hadamard products. Linear Algebra and its Applications, 26:203–241, 1979. A. Asuncion and D. J. Newman. UCI machine learning repository, 2007. URL http://www.ics. uci.edu/˜mlearn/MLRepository.html. P. Auer and C. Gentile. Adaptive and self-conﬁdent online learning algorithms. In Proceedings of the Thirteenth Annual Conference on Computational Learning Theory, 2000. P. L. Bartlett, E. Hazan, and A. Rakhlin. Adaptive online gradient descent. In Advances in Neural Information Processing Systems 20, 2007. A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31:167–175, 2003. J. V. Bondar. Comments on and complements to Inequalities: Theory of Majorization and Its Applications. Linear Algebra and its Applications, 199:115–129, 1994. A. Bordes, L. Bottou, and P. Gallinari. Sgd-qn: Careful quasi-newton stochastic gradient descent. Journal of Machine Learning Research, 10:1737–1754, 2009. 2156  A DAPTIVE S UBGRADIENT M ETHODS  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004. P. Brucker. An O(n) algorithm for quadratic knapsack problems. Operations Research Letters, 3 (3):163–166, 1984. N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 50(9):2050–2057, September 2004. N. Cesa-Bianchi, A. Conconi, , and C. Gentile. A second-order perceptron algorithm. SIAM Journal on Computing, 34(3):640–668, 2005. N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with expert advice. Machine Learning, 66:321–352, 2007. K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online passive aggressive algorithms. Journal of Machine Learning Research, 7:551–585, 2006. K. Crammer, M. Dredze, and F. Pereira. Exact convex conﬁdence-weighted learning. In Advances in Neural Information Processing Systems 22, 2008. K. Crammer, M. Dredze, and A. Kulesza. Adaptive regularization of weight vectors. In Advances in Neural Information Processing Systems 23, 2009. C. Davis. Notions generalizing convexity for functions deﬁned on spaces of matrices. In Proceedings of the Symposia in Pure Mathematics, volume 7, pages 187–201. American Mathematical Society, 1963. J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei. ImageNet: a large-scale hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2009. J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward backward splitting. Journal of Machine Learning Research, 10:2873–2908, 2009. J. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In Proceedings of the Twenty Third Annual Conference on Computational Learning Theory, 2010. R. Fletcher. A new approach to variable metric algorithms. Computer Journal, 13:317–322, 1970. D. Grangier and S. Bengio. A discriminative kernel-based model to rank images from text queries. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(8):1371–1384, 2008. E. Hazan and S. Kale. Extracting certainty from uncertainty: regret bounded by variation in costs. In Proceedings of the Twenty First Annual Conference on Computational Learning Theory, 2008. E. Hazan, A. Kalai, S. Kale, and A. Agarwal. Logarithmic regret algorithms for online convex optimization. In Proceedings of the Nineteenth Annual Conference on Computational Learning Theory, 2006. J. B. Hiriart-Urruty and C. Lemar´ chal. Convex Analysis and Minimization Algorithms II. Springere Verlag, 1996. 2157  D UCHI , H AZAN AND S INGER  R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 1985. A. Juditsky, A. Nemirovski, and C. Tauvel. Solving variational inequalities with the stochastic mirror-prox algorithm. http://arxiv.org/abs/0809.0815, 2008. A. Kalai and S. Vempala. Efﬁcient algorithms for online decision problems. Journal of Computer and System Sciences, 71(3):291–307, 2003. G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming Series A, 2010. Online ﬁrst; to appear. D. Lewis, Y. Yang, T. Rose, and F. Li. RCV1: A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361–397, 2004. H. B. McMahan and M. Streeter. Adaptive bound optimization for online convex optimization. In Proceedings of the Twenty Third Annual Conference on Computational Learning Theory, 2010. A. Nedi´ . Subgradient Methods for Convex Minimization. PhD thesis, Massachusetts Institute of c Technology, 2002. A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009. A. S. Nemirovski and D. B. Yudin. Problem Complexity and Efﬁciency in Optimization. John Wiley and Sons, 1983. Y. Nesterov. Smooth minimization of nonsmooth functions. Mathematical Programming, 103: 127–152, 2005. Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming, 120(1):221–259, 2009. G. Obozinski, B. Taskar, and M. Jordan. Joint covariate selection for grouped classiﬁcation. Technical Report 743, Dept. of Statistics, University of California Berkeley, 2007. P. M. Pardalos and J. B. Rosen. An algorithm for a singly constrained class of quadratic programs subject to upper and lower bounds. Mathematical Programming, 46:321–328, 1990. A. Rakhlin. Lecture notes on online learning. For the Statistical Machine Learning Course at University of California, Berkeley, 2009. G. Salton and C. Buckley. Term weighting approaches in automatic text retrieval. Information Processing and Management, 24(5), 1988. N. Z. Shor. Utilization of the operation of space dilation in the minimization of convex functions. Cybernetics and Systems Analysis, 6(1):7–15, 1972. Translated from Kibernetika. P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. Technical report, Department of Mathematics, University of Washington, 2008. L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Technical Report MSR-TR-2010-23, Microsoft Research, 2010. 2158  A DAPTIVE S UBGRADIENT M ETHODS  M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings of the Twentieth International Conference on Machine Learning, 2003.  2159</p>
<br/>
<br/><br/><br/></body>
</html>
