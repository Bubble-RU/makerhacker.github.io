<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>69 jmlr-2011-Neyman-Pearson Classification, Convexity and Stochastic Constraints</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-69" href="../jmlr2011/jmlr-2011-Neyman-Pearson_Classification%2C_Convexity_and_Stochastic_Constraints.html">jmlr2011-69</a> <a title="jmlr-2011-69-reference" href="#">jmlr2011-69-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>69 jmlr-2011-Neyman-Pearson Classification, Convexity and Stochastic Constraints</h1>
<br/><p>Source: <a title="jmlr-2011-69-pdf" href="http://jmlr.org/papers/volume12/rigollet11a/rigollet11a.pdf">pdf</a></p><p>Author: Philippe Rigollet, Xin Tong</p><p>Abstract: Motivated by problems of anomaly detection, this paper implements the Neyman-Pearson paradigm to deal with asymmetric errors in binary classiﬁcation with a convex loss ϕ. Given a ﬁnite collection of classiﬁers, we combine them and obtain a new classiﬁer that satisﬁes simultaneously the two following properties with high probability: (i) its ϕ-type I error is below a pre-speciﬁed level and (ii), it has ϕ-type II error close to the minimum possible. The proposed classiﬁer is obtained by minimizing an empirical convex objective with an empirical convex constraint. The novelty of the method is that the classiﬁer output by this computationally feasible program is shown to satisfy the original constraint on type I error. New techniques to handle such problems are developed and they have consequences on chance constrained programming. We also evaluate the price to pay in terms of type II error for being conservative on type I error. Keywords: binary classiﬁcation, Neyman-Pearson paradigm, anomaly detection, empirical constraint, empirical risk minimization, chance constrained optimization</p><br/>
<h2>reference text</h2><p>P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classiﬁcation, and risk bounds. J. Amer. Statist. Assoc., 101(473):138–156, 2006. A. Ben-Tal, L. El Ghaoui, and A. Nemirovski. Robust Optimization. Princeton Series in Applied Mathematics. Princeton University Press, Princeton, NJ, 2009. G. Blanchard, G. Lee, and C. Scott. Semi-supervised novelty detection. J. Mach. Learn. Res., 11: 2973–3009, 2010. S. Boucheron, O. Bousquet, and G. Lugosi. Theory of classiﬁcation: A survey of some recent advances. ESAIM Probab. Stat., 9:323–375, 2005. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, 2004. G. C. Calaﬁore and M. C. Campi. The scenario approach to robust control design. IEEE Trans. Automat. Control, 51(5):742–753, 2006. A. Cannon, J. Howse, D. Hush, and C. Scovel. Learning with the Neyman-Pearson and min-max criteria. Technical Report LA-UR-02-2951, 2002. A. Cannon, J. Howse, D. Hush, and C. Scovel. Simple classiﬁers. Technical Report LA-UR-030193, 2003. D. Casasent and X. Chen. Radial basis function neural networks for nonlinear ﬁsher discrimination and neyman-pearson classiﬁcation. Neural Networks, 16(5-6):529 – 535, 2003. L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition, Volume 31 of o Applications of Mathematics (New York). Springer-Verlag, New York, 1996. W. Feller. An Introduction to Probability Theory and Its Applications. Vol. II. Second edition. John Wiley & Sons Inc., New York, 1971. M. Han, D. Chen, and Z. Sun. Analysis to Neyman-Pearson classiﬁcation with convex loss function. Anal. Theory Appl., 24(1):18–28, 2008. V. Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems. ´ ´ e Ecole d’Et´ de Probabilit´ s de Saint-Flour XXXVIII-2008. Lecture Notes in Mathematics 2033. e Berlin: Springer. ix, 254 p. EUR 48.10 , 2011. 2854  N EYMAN -P EARSON C LASSIFICATION  C. M. Lagoa, X. Li, and M. Sznaier. Probabilistically constrained linear programs and risk-adjusted controller design. SIAM J. Optim., 15(3):938–951 (electronic), 2005. E. L. Lehmann and J. P. Romano. Testing Statistical Hypotheses. Springer Texts in Statistics. Springer, New York, third edition, 2005. A. Nemirovski and A. Shapiro. Convex approximations of chance constrained programs. SIAM J. Optim., 17(4):969–996, 2006. A. Pr´ kopa. Stochastic Programming, volume 324 of Mathematics and its Applications. Kluwer e Academic Publishers Group, Dordrecht, 1995. R. T. Rockafellar. Convex Analysis. Princeton Landmarks in Mathematics. Princeton University Press, Princeton, NJ, 1997. Reprint of the 1970 original, Princeton Paperbacks. R. Schapire. The strength of weak learnability. Machine learning, 5(2):197–227, 1990. C. Scott. Comparison and design of Neyman-Pearson classiﬁers. Unpublished, 2005. C. Scott. Performance measures for Neyman-Pearson classiﬁcation. IEEE Trans. Inform. Theory, 53(8):2852–2863, 2007. C. Scott and R. Nowak. A Neyman-Pearson approach to statistical learning. IEEE Transactions on Information Theory, 51(11):3806–3819, 2005. E. V. Slud. Distribution inequalities for the binomial law. Ann. Probability, 5(3):404–412, 1977.  2855</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
