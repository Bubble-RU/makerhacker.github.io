<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>89 jmlr-2011-Super-Linear Convergence of Dual Augmented Lagrangian Algorithm for Sparsity Regularized Estimation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-89" href="../jmlr2011/jmlr-2011-Super-Linear_Convergence_of_Dual_Augmented_Lagrangian_Algorithm_for_Sparsity_Regularized_Estimation.html">jmlr2011-89</a> <a title="jmlr-2011-89-reference" href="#">jmlr2011-89-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>89 jmlr-2011-Super-Linear Convergence of Dual Augmented Lagrangian Algorithm for Sparsity Regularized Estimation</h1>
<br/><p>Source: <a title="jmlr-2011-89-pdf" href="http://jmlr.org/papers/volume12/tomioka11a/tomioka11a.pdf">pdf</a></p><p>Author: Ryota Tomioka, Taiji Suzuki, Masashi Sugiyama</p><p>Abstract: We analyze the convergence behaviour of a recently proposed algorithm for regularized estimation called Dual Augmented Lagrangian (DAL). Our analysis is based on a new interpretation of DAL as a proximal minimization algorithm. We theoretically show under some conditions that DAL converges super-linearly in a non-asymptotic and global sense. Due to a special modelling of sparse estimation problems in the context of machine learning, the assumptions we make are milder and more natural than those made in conventional analysis of augmented Lagrangian algorithms. In addition, the new interpretation enables us to generalize DAL to wide varieties of sparse estimation problems. We experimentally conﬁrm our analysis in a large scale ℓ1 -regularized logistic regression problem and extensively compare the efﬁciency of DAL algorithm to previously proposed algorithms on both synthetic and benchmark data sets. Keywords: dual augmented Lagrangian, proximal minimization, global convergence, sparse estimation, convex optimization</p><br/>
<h2>reference text</h2><p>G. Andrew and J. Gao. Scalable training of L1-regularized log-linear models. In Proc. of the 24th international conference on Machine learning, pages 33–40, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-793-3. doi: http://doi.acm.org/10.1145/1273496.1273501. A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In B. Sch¨ lkopf, J. Platt, and o T. Hoffman, editors, Advances in NIPS 19, pages 41–48. MIT Press, Cambridge, MA, 2007. A. Argyriou, C. A. Micchelli, M. Pontil, and Y. Ying. A spectral regularization framework for multi-task structure learning. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 25–32. MIT Press, Cambridge, MA, 2008. S. E Baranzini, P. Mousavi, J. Rio, S. J. Caillier, A. Stillman, P. Villoslada, M. M. Wyatt, M. Comabella, L. D. Greller, R. Somogyi, X. Montalban, and J. R. Oksenberg. Transcription-based prediction of response to ifnβ using supervised computational methods. PLoS Biol., 3(1):e2, 2004. 1582  D UAL AUGMENTED -L AGRANGIAN C ONVERGES S UPER -L INEARLY  A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. Imaging Sciences, 2(1):183–202, 2009. D. P. Bertsekas. Constrained Optimization and Lagrange Multiplier Methods. Academic Press, 1982. D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 1999. 2nd edition. P.J. Bickel, Y. Ritov, and A.B. Tsybakov. Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705–1732, 2009. ISSN 0090-5364. J. M. Bioucas-Dias. Bayesian wavelet-based image deconvolution: A GEM algorithm exploiting a class of heavy-tailed priors. IEEE Trans. Image Process., 15:937–951, 2006. J. M. Bioucas-Dias and M. A. T. Figueiredo. A new twist: two-step iterative shrinkage/thresholding algorithms for image restoration. IEEE Trans. Image Process., 16(12):2992–3004, 2007. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004. S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers, 2011. Unﬁnished working draft. R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientiﬁc Computing, 16(5):1190–1208, 1995. ISSN 1064-8275. J.-F. Cai, E. J. Candes, and Z. Shen. A singular value thresholding algorithm for matrix completion. Technical report, arXiv:0810.3286, 2008. URL http://arxiv.org/abs/0810.3286. P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. Multiscale Modeling and Simulation, 4(4):1168–1200, 2005. I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Commun. Pur. Appl. Math., LVII:1413–1457, 2004. D. L. Donoho. De-noising by soft-thresholding. IEEE Trans. Inform. Theory, 41(3):613–627, 1995. J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward backward splitting. J. Mach. Learn. Res., 10:2899–2934, 2009. ISSN 1532-4435. B. Efron, T. Hastie, R. Tibshirani, and I. Johnstone. Least angle regression. Annals of Statistics, 32 (2):407–499, 2004. M. Fazel, H. Hindi, and S. P. Boyd. A rank minimization heuristic with application to minimum order system approximation. In Proc. of the American Control Conference, 2001. M. A. T. Figueiredo and R. Nowak. An EM algorithm for wavelet-based image restoration. IEEE Trans. Image Process., 12:906–916, 2003. M. A. T. Figueiredo, J. M. Bioucas-Dias, and R. D. Nowak. Majorization-minimization algorithm for wavelet-based image restoration. IEEE Trans. Image Process., 16(12), 2007a. 1583  T OMIOKA , S UZUKI AND S UGIYAMA  M. A. T. Figueiredo, R. D. Nowak, and S. J. Wright. Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems. IEEE Journal on selected topics in Signal Processing, 1(4):586–597, 2007b. M. Girolami. A variational method for learning sparse and overcomplete representations. Neural Computation, 13(11):2517–2532, 2001. T. Goldstein and S. Osher. The split Bregman method for L1 regularized problems. SIAM Journal on Imaging Sciences, 2(2):323–343, 2009. ISSN 1936-4954. I. F. Gorodnitsky and B. D. Rao. Sparse signal reconstruction from limited data using FOCUSS: A re-weighted minimum norm algorithm. IEEE Trans. Signal Process., 45(3), 1997. I. Guyon, S. Gunn, M. Nikravesh, and L. Zadeh, editors. Feature Extraction: Foundations and Applications. Springer Verlag, 2006. S. Haufe, R. Tomioka, G. Nolte, K.-R. M¨ ller, and M. Kawanabe. Modeling sparse connectivity u between underlying brain sources for EEG/MEG. IEEE Trans. Biomed. Eng., 57(8):1954–1963, 2010. ISSN 0018-9294. M. R. Hestenes. Multiplier and gradient methods. J. Optim. Theory Appl., 4:303–320, 1969. J.-B. Hiriart-Urruty and C. Lemar´ chal. Convex Analysis and Minimization Algorithms II: Advanced e Theory and Bundle Methods. Springer, 1993. T. S. Jaakkola. Variational methods for inference and estimation in graphical models. PhD thesis, Massachusetts Institute of Technology, 1997. S.-J. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinvesky. An interior-point method for large-scale ℓ1 -regularized least squares. IEEE journal of selected topics in signal processing, 1:606–617, 2007. K. Koh, S.-J. Kim, and S. Boyd. An interior-point method for large-scale ℓ1 -regularized logistic regression. Journal of Machine Learning Research, 8:1519–1555, 2007. B. W. Kort and D. P. Bertsekas. Combined primal–dual and penalty methods for convex programming. SIAM Journal on Control and Optimization, 14(2):268–294, 1976. G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 2010. Under revision. J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated gradient. J. Mach. Learn. Res., 10:777–801, 2009. ISSN 1532-4435. Z. Lin, M. Chen, L. Wu, and Y. Ma. The augmented lagrange multiplier method for exact recovery of a corrupted low-rank matrices. Mathematical Programming, 2009. submitted. P. L. Lions and B. Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM Journal on Numerical Analysis, 16(6):964–979, 1979. 1584  D UAL AUGMENTED -L AGRANGIAN C ONVERGES S UPER -L INEARLY  N. Meinshausen and P. B¨ hlmann. High-dimensional graphs and variable selection with the lasso. u The Annals of Statistics, 34(3):1436–1462, 2006. ISSN 0090-5364. C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine Learning Research, 6:1099–1125, 2005. J. J. Moreau. Proximit´ et dualit´ dans un espace hilbertien. Bulletin de la S. M. F., 93:273–299, e e 1965. Y. Nesterov. Gradient methods for minimizing composite objective function. Technical Report 2007/76, Center for Operations Research and Econometrics (CORE), Catholic University of Louvain, 2007. J. Nocedal and S. Wright. Numerical Optimization. Springer, 1999. J. Palmer, D. Wipf, K. Kreutz-Delgado, and B. Rao. Variational EM algorithms for non-gaussian latent variable models. In Y. Weiss, B. Sch¨ lkopf, and J. Platt, editors, Advances in Neural o Information Processing Systems 18, pages 1059–1066. MIT Press, Cambridge, MA, 2006. M. J. D. Powell. A method for nonlinear constraints in minimization problems. In R. Fletcher, editor, Optimization, pages 283–298. Academic Press, London, New York, 1969. A. Rakotomamonjy, F. R. Bach, S. Canu, and Y. Grandvalet. SimpleMKL. JMLR, 9:2491–2521, 2008. R. T. Rockafellar. Convex Analysis. Princeton University Press, 1970. R. T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal on Control and Optimization, 14:877–898, 1976a. R. T. Rockafellar. Augmented Lagrangians and applications of the proximal point algorithm in convex programming. Math. of Oper. Res., 1:97–116, 1976b. S. Shalev-Shwartz and N. Srebro. SVM optimization: inverse dependence on training set size. In Proc. of the 25th International Conference on Machine Learning, pages 928–935. ACM, 2008. S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In Proc. of the 24th International Conference on Machine Learning, pages 807–814. ACM, 2007. N. Srebro, J. D. M. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. In Advances in NIPS 17, pages 1329–1336. MIT Press, Cambridge, MA, 2005. R. Tibshirani. Regression shrinkage and selection via the lasso. J. Roy. Stat. Soc. B, 58(1):267–288, 1996. R. Tomioka and K.-R. M¨ ller. A regularized discriminative framework for EEG analysis with u application to brain-computer interface. Neuroimage, 49(1):415–432, 2010. R. Tomioka and M. Sugiyama. Dual augmented Lagrangian method for efﬁcient sparse reconstruction. IEEE Signal Processing Letters, 16(12):1067–1070, 2009. 1585  T OMIOKA , S UZUKI AND S UGIYAMA  R. Tomioka and T. Suzuki. Regularization strategies and empirical Bayesian learning for MKL. Technical report, arXiv:1011.3090v1, 2010. R. Tomioka, T. Suzuki, M. Sugiyama, and H. Kashima. A fast augmented Lagrangian algorithm for learning low-rank matrices. In Proc. of the 27th International Conference on Machine Learning. Omnipress, 2010. R. Tomioka, K. Hayashi, and H. Kashima. Estimation of low-rank tensors via convex optimization. SIAM J. Matrix Anal. A., 2011a. Submitted. R. Tomioka, T. Suzuki, and M. Sugiyama. Augmented Lagrangian methods for learning, selecting, and combining features. In Suvrit Sra, Sebastian Nowozin, and Stephen J. Wright, editors, Optimization for Machine Learning. MIT Press, 2011b. D. Wipf and S. Nagarajan. A new view of automatic relevance determination. In Advances in NIPS 20, pages 1625–1632. MIT Press, 2008. S. J. Wright, R. D. Nowak, and M. A. T. Figueiredo. Sparse reconstruction by separable approximation. IEEE Trans. Signal Process., 2009. J. Yang and Y. Zhang. Alternating direction algorithms for L1-problems in compressive sensing. Technical Report TR09-37, Dept. of Computational & Applied Mathematics, Rice University, 2009. W. Yin, S. Osher, D. Goldfarb, and J. Darbon. Bregman iterative algorithms for l1-minimization with applications to compressed sensing. SIAM J. Imaging Sciences, 1(1):143–168, 2008. J. Yu, S. V. N. Vishwanathan, S. G¨ nter, and N. N. Schraudolph. A quasi-newton approach to u nonsmooth convex optimization problems in machine learning. The Journal of Machine Learning Research, 11:1145–1200, 2010. M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J. Roy. Stat. Soc. B, 68(1):49–67, 2006. X. Zhang, M. Burger, and S. Osher. A uniﬁed primal-dual algorithm framework based on bregman iteration. Journal of Scientiﬁc Computing, 46(1):20–46, 2010. P. Zhao and B. Yu. On model selection consistency of lasso. J. Mach. Learn. Res., 7:2541–2563, 2006. ISSN 1532-4435. H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society Series B(Statistical Methodology), 67(2):301–320, 2005.  1586</p>
<br/>
<br/><br/><br/></body>
</html>
