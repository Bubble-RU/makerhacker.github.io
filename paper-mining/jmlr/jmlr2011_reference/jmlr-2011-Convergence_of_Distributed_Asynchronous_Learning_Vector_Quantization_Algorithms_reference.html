<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>19 jmlr-2011-Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2011" href="../home/jmlr2011_home.html">jmlr2011</a> <a title="jmlr-2011-19" href="../jmlr2011/jmlr-2011-Convergence_of_Distributed_Asynchronous_Learning_Vector_Quantization_Algorithms.html">jmlr2011-19</a> <a title="jmlr-2011-19-reference" href="#">jmlr2011-19-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>19 jmlr-2011-Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms</h1>
<br/><p>Source: <a title="jmlr-2011-19-pdf" href="http://jmlr.org/papers/volume12/patra11a/patra11a.pdf">pdf</a></p><p>Author: Benoît Patra</p><p>Abstract: Motivated by the problem of effectively executing clustering algorithms on very large data sets, we address a model for large scale distributed clustering methods. To this end, we brieﬂy recall some standards on the quantization problem and some results on the almost sure convergence of the competitive learning vector quantization (CLVQ) procedure. A general model for linear distributed asynchronous algorithms well adapted to several parallel computing architectures is also discussed. Our approach brings together this scalable model and the CLVQ algorithm, and we call the resulting technique the distributed asynchronous learning vector quantization algorithm (DALVQ). An indepth analysis of the almost sure convergence of the DALVQ algorithm is performed. A striking result is that we prove that the multiple versions of the quantizers distributed among the processors in the parallel architecture asymptotically reach a consensus almost surely. Furthermore, we also show that these versions converge almost surely towards the same nearly optimal value for the quantization criterion. Keywords: k-means, vector quantization, distributed, asynchronous, stochastic optimization, scalability, distributed consensus</p><br/>
<h2>reference text</h2><p>E. A. Abaya and G. L. Wise. Convergence of vector quantizers with applications to optimal quantization. SIAM Journal on Applied Mathematics, 44(1):183–189, 1984. A. Antos. Improved minimax bounds on the test and training distortion of empirically designed vector quantizers. IEEE Transactions on Information Theory, 51(11):4022–4032, 2005. A. Antos, L. Gy¨ rﬁ, and A. Gy¨ rgy. Individual convergence rates in empirical vector quantizer o o design. IEEE Transactions on Information Theory, 51(11):4013–4022, 2005. P. L. Bartlett, T. Linder, and G. Lugosi. The minimax distortion redundancy in empirical quantizer design. IEEE Transactions on Information Theory, 44(5):1802–1813, 1998. A. Benveniste, M. M´ tivier, and P. Priouret. Adaptive Algorithms and Stochastic Approximations. e Springer-Verlag, 1990. D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. Prentice-Hall, Inc., 1989. G. Biau, L. Devroye, and G. Lugosi. On the performance of clustering in hilbert spaces. IEEE Transactions on Information Theory, 54(2):781–790, 2008. V. D. Blondel, J. M. Hendrickx, A. Olshevsky, and J. N. Tsitsiklis. Convergence in multiagent coordination, consensus, and ﬂocking. In Decision and Control, 2005 and 2005 European Control Conference., 2005. L. Bottou. Stochastic gradient learning in neural networks. In Proceedings of Neuro-Nˆmes 91, ı 1991. L. Bottou. Online algorithms and stochastic approximations. In Online Learning and Neural Networks. Cambridge University Press, 1998. L. Bottou and Y. Bengio. Convergence properties of the k-means algorithm. In Advances in Neural Information Processing Systems. MIT Press, 1995. L. Bottou and Y. LeCun. Large scale online learning. In Advances in Neural Information Processing Systems 16. MIT Press, 2004. F. Bullo, J. Cort´ s, and S. Mart´nez. Distributed Control of Robotic Networks. Princeton University e ı Press, 2009. G. Choquet. Topology. Academic Press, 1966. P. A. Chou. The distortion of vector quantizers trained on n vectors decreases to the optimum at o p (1/n). In In Proceedings of IEEE International Symposium on Information Theory, 1994. J. W. Durham, R. Carli, P. Frasca, and F. Bullo. Discrete partitioning and coverage control with gossip communication. In ASME Conference Proceedings. ASME, 2009. R. Durrett. Probability: Theory and Examples. Duxbury Press, 1990. 3464  C ONVERGENCE OF D ISTRIBUTED A SYNCHRONOUS L EARNING V ECTOR Q UANTIZATION A LGORITHMS  J. C. Fort and G. Pag` s. On the a.s. convergence of the kohonen algorithm with a general neighbore hood function. The Annals of Applied Probability, 5(4):1177–1216, 1995. J. C. Fort and G. Pag` s. Convergence of stochastic algorithms: from the kushner-clark theorem to e the lyapounov functional method. Advances in Applied Probability, 28(4):1072–1094, 1996. P. Frasca, R. Carli, and F. Bullo. Multiagent coverage algorithms with gossip communication: Control systems on the space of partitions. In American Control Conference, 2009. A. Gersho and R. M. Gray. Vector Quantization and Signal Compression. Kluwer, 1992. S. Graf and H. Luschgy. Foundations of Quantization for Probability Distributions. SpringerVerlag, 2000. M. Inaba, N. Katoh, and H. Imai. Applications of weighted voronoi diagrams and randomization to variance-based k-clustering: (extended abstract). In SCG ’94: Proceedings of the Tenth Annual Symposium on Computational Geometry, 1994. D. Jungnickel. Graphs, Networks and Algorithms. Springer-Verlag, 1999. T. Kohonen. Analysis of a simple self-organizing process. Biological Cybernetics, 44(2):135–140, 1982. H. J. Kushner and D. S. Clark. Stochastic Approximation for Constrained and Unconstrained Systems. Springer-Verlag, 1978. T. Linder. On the training distortion of vector quantizers. IEEE Transactions on Information Theory, 46(4):1617–1623, 2000. T. Linder. Learning-theoretic methods in vector quantization. In Lecture Notes for the Advanced School on the Principles of Nonparametric Learning, 2001. T. Linder, G. Lugosi, and K. Zeger. Rates of convergence in the source coding theorem, in empirical quantizer design, and in universal lossy source coding. IEEE Transactions on Information Theory, 40(6):1728–1740, 1994. S. Lloyd. Least squares quantization in pcm. IEEE Transactions on Information Theory, 28(2): 129–137, 2003. J. B. MacQueen. Some methods of classiﬁcation and analysis of multivariate observations. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, 1967. R. McDonald, K. Hall, and G. Mann. Distributed training strategies for the structured perceptron. In HLT 2010: Human Language Technologies, 2010. N. Murata. A statistical study of on-line learning. In Online Learning and Neural Networks, 1998. G. Pag` s. A space vector quantization for numerical integration. Journal of Applied and Computae tional Mathematics, 89(1):1–38, 1997. D. Pollard. Stong consistency of k-means clustering. The Annals of Statistics, 9(1):135–140, 1981. 3465  PATRA  D. Pollard. Quantization and the method of k-means. IEEE Transactions on Information Theory, 28(2):199–205, 1982a. D. Pollard. A central limit theorem for k-means clustering. The Annals of Probability, 28(4):199– 205, 1982b. H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, 22(3):400–407, 1951. M. J. Sabin and R. M. Gray. Global convergence and empirical consistency of the generalized lloyd algorithm. IEEE Transactions on Information Theory, 32(2):148–155, 1986. J. Tsitsiklis. Problems in Decentralized Decision Making and Computation. PhD thesis, Department of EECS, MIT, Cambridge, USA, 1984. J. Tsitsiklis, D. Bertsekas, and M. Athans. Distributed asynchronous deterministic and stochastic gradient optimization algorithms. IEEE Transactions on Automatic Control, 31(9):803–812, 1986. M. Zinkevich, A. Smola, and J. Langford. Slow learners are fast. In Advances in Neural Information Processing Systems 22. Curran Associates, 2009.  3466</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
