<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-27" href="../jmlr2009/jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">jmlr2009-27</a> <a title="jmlr-2009-27-reference" href="#">jmlr2009-27-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</h1>
<br/><p>Source: <a title="jmlr-2009-27-pdf" href="http://jmlr.org/papers/volume10/duchi09a/duchi09a.pdf">pdf</a></p><p>Author: John Duchi, Yoram Singer</p><p>Abstract: We describe, analyze, and experiment with a framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we ﬁrst perform an unconstrained gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the ﬁrst phase. This view yields a simple yet effective algorithm that can be used for batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as ℓ1 . We derive concrete and very simple algorithms for minimization of loss functions with ℓ1 , ℓ2 , ℓ2 , and ℓ∞ regularization. We also show how to construct ef2 ﬁcient algorithms for mixed-norm ℓ1 /ℓq regularization. We further extend the algorithms and give efﬁcient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in a series of experiments with synthetic and natural data sets. Keywords: subgradient methods, group sparsity, online learning, convex optimization</p><br/>
<h2>reference text</h2><p>A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31:167–175, 2003. D.P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 1999. J. Blitzer, M. Dredze, and F. Pereira. Biographies, Bollywood, boom-boxes and blenders: domain adaptation for sentiment classiﬁcation. In Association for Computational Linguistics, 2007. 2932  E FFICIENT L EARNING USING F ORWARD BACKWARD S PLITTING  S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004. G. Chen and R. T. Rockafellar. Convergence rates in forward-backward splitting. SIAM Journal on Optimization, 7(2), 1997. P. Combettes and V. Wajs. Signal recovery by proximal forward-backward splitting. Multiscale Modeling and Simulation, 4(4):1168–1200, 2005. I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Communication on Pure and Applied Mathematics, 57(11):1413–1457, 2004. I. Daubechies, M. Fornasier, and I. Loris. Accelerated projected gradient method for linear inverse problems with sparsity constraints. Fourier Analysis and Applications, 14(5):764–792, 2008. D. L. Donoho. De-noising via soft thresholding. IEEE Transactions on Information Theory, 41(3):613–627, 1995. J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efﬁcient projections onto the ℓ1 -ball for learning in high dimensions. In Proceedings of the 25th International Conference on Machine Learning, 2008. D. Grangier and S. Bengio. A discriminative kernel-based model to rank images from text queries. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(8):1371–1384, 2008. E. Hale, W. Yin, and Y. Zhang. A ﬁxed-point continuation method for ℓ1 -regularized minimization with applications to compressed sensing. Technical Report TR07-07, Rice University Department of Computational and Applied Mathematics, July 2007. E. Hazan, A. Kalai, S. Kale, and A. Agarwal. Logarithmic regret algorithms for online convex optimization. In Proceedings of the Nineteenth Annual Conference on Computational Learning Theory, 2006. K. Koh, S.J. Kim, and S. Boyd. An interior-point method for large-scale ℓ1 -regularized logistic regression. Journal of Machine Learning Research, 8:1519–1555, 2007. J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated gradient. In Advances in Neural Information Processing Systems 22, 2008. P. L. Lions and B. Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM Journal on Numerical Analysis, 16:964–979, 1979. L. Meier, S. van de Geer, and P. B¨ hlmann. The group lasso for logistic regression. Journal of the Royal u Statistical Society Series B, 70(1):53–71, 2008. N. Meinshausen and P. B¨ hlmann. High dimensional graphs and variable selection with the Lasso. Annals of u Statistics, 34:1436–1462, 2006. S. Negahban and M. Wainwright. Phase transitions for high-dimensional joint support recovery. In Advances in Neural Information Processing Systems 22, 2008. Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publishers, 2004. Y. Nesterov. Gradient methods for minimizing composite objective function. Technical Report 76, Center for Operations Research and Econometrics (CORE), Catholic University of Louvain (UCL), 2007. G. Obozinski, B. Taskar, and M. Jordan. Joint covariate selection for grouped classiﬁcation. Technical Report 743, Dept. of Statistics, University of California Berkeley, 2007. G. Obozinski, M. Wainwright, and M. Jordan. High-dimensional union support recovery in multivariate regression. In Advances in Neural Information Processing Systems 22, 2008. 2933  D UCHI AND S INGER  Art Owen. A robust hybrid of lasso and ridge regression. Technical report, Stanford University, 2006. A. Quattoni, X. Carreras, M. Collins, and T. Darrell. An efﬁcient projection for L1,inﬁnity regularization. In Proceedings of the 26th International Conference on Machine Learning, 2009. R.T. Rockafellar. Convex Analysis. Princeton University Press, 1970. M. Schmidt, E. van den Berg, M. Friedlander, and K. Murphy. Optimizing costly functions with simple constraints: a limited-memory projected quasi-Newton method. In Proceedings of the Twelfth International Conference on Artiﬁcial Intelligence and Statistics, 2009. S. Shalev-Shwartz and Y. Singer. Logarithmic regret algorithms for strongly convex repeated games. Technical report, The Hebrew University, 2007. Available at http://www.cs.huji.ac.il/∼shais. S. Shalev-Shwartz and A. Tewari. Stochastic methods for ℓ1 -regularized loss minimization. In Proceedings of the 26th International Conference on Machine Learning, 2009. S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In Proceedings of the 24th International Conference on Machine Learning, 2007. D. Spiegelhalter and C. Taylor. Machine Learning, Neural and Statistical Classiﬁcation. Ellis Horwood, 1994. P. Tseng. A modiﬁed forward backward splitting method for maximal monotone mappings. SIAM Journal on Control and Optimization, 38:431–446, 2000. P. Tseng and S. Yun. A coordinate gradient descent method for nonsmooth separable minimization. Mathematical Programming Series B, 117:387–423, 2007. S. Wright, R. Nowak, and M. Figueiredo. Sparse reconstruction by separable approximation. IEEE Transactions on Signal Processing, 57(7):2479–2493, 2009. P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning Research, 7: 2541–2567, 2006. P. Zhao, G. Rocha, and B. Yu. Grouped and hierarchical model selection through composite absolute penalties. Technical Report 703, Statistics Department, University of California Berkeley, 2006. M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings of the Twentieth International Conference on Machine Learning, 2003.  2934</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
