<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-35" href="../jmlr2009/jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">jmlr2009-35</a> <a title="jmlr-2009-35-reference" href="#">jmlr2009-35-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</h1>
<br/><p>Source: <a title="jmlr-2009-35-pdf" href="http://jmlr.org/papers/volume10/tuv09a/tuv09a.pdf">pdf</a></p><p>Author: Eugene Tuv, Alexander Borisov, George Runger, Kari Torkkola</p><p>Abstract: Predictive models beneﬁt from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for ﬁlters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach. Keywords: trees, resampling, importance, masking, residuals</p><br/>
<h2>reference text</h2><p>H. Almuallin and T. G. Dietterich. Learning boolean concepts in the presence of many irrelevant features. Artiﬁcial Intelligence, 69(1-2):279–305, 1994. Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation, 9(7):1545–88, 1997. E. Bauer and R. Kohavi. An empirical comparison of voting classiﬁcation algorithms: Bagging, boosting and variants. Machine Learning, 36(1/2):525–536, 1999. A. Berrado and G.C. Runger. Using metarules to organize and group discovered association rules. Data Mining and Knowledge Discovery, 14(3):409–431, 2007. A. Borisov, V. Eruhimov, and E. Tuv. Tree-based ensembles with dynamic soft feature selection. In I. Guyon, S. Gunn, M. Nikravesh, and L. Zadeh, editors, Feature Extraction Foundations and Applications: Studies in Fuzziness and Soft Computing. Springer, 2006. B. Boser, I. Guyon, and V. Vapnik. A training algorithm for optimal margin classiﬁers. In D. Haussler, editor, 5th Annual ACM Workshop on COLT, Pittsburgh, PA, pages 144–152. ACM Press, 1992. 1363  T UV, B ORISOV, RUNGER AND T ORKKOLA  O. Bousquet and A. Elisseeff. Algorithmic stability and generalization performance. In Advances in Neural Information Processing Systems, volume 13, pages 196–202. MIT Press, 2001. L. Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996. L. Breiman. Arcing classiﬁers. The Annals of Statistics, 26(3):801–849, 1998. L. Breiman. Random forests. Machine Learning, 45(1):5–32, 2001. L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classiﬁcation and Regression Trees. Wadsworth, Belmont, MA, 1984. S. Cost and S. Salzberg. A weighted nearest neighbor algorithm for learning with symbolic features. Machine Learning, 10(1):57–78, 1993. P. Diaconis and B. Efron. Computer intensive methods in statistics. Scientiﬁc American, (248): 116–131, 1983. T. G. Dietterich. An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization. Machine Learning, 40(2):139–157, 2000a. T. G. Dietterich. Ensemble methods in machine learning. In First International Workshop on Multiple Classiﬁer Systems 2000, Cagliari, Italy, volume 1857 of Lecture Notes in Computer Science, pages 1–15. Springer, 2000b. B. Efron, T. Hastie, L. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32:407–499, 2004. Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm. In The 13th International Conference on Machine Learning, pages 148–156. Morgan Kaufman, 1996. J. Friedman. Greedy function approximation: a gradient boosting machine. Technical report, Dept. of Statistics, Stanford University, 1999. J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: A statistical view of boosting. Annals of Statistics, 28:832–844, 2000. J. H Friedman, M. Jacobson, and W. Stuetzle. Projection pursuit regression. Journal of the American Statistical Association, 76:817–823, 1981. I. Guyon and A. Elisseeff. An introduction to variable and feature selection. Journal of Machine Learning Research, 3:1157–1182, Mar 2003. I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for cancer classiﬁcation using support vector machines. Machine Learning, 46(1-3):389–422, 2002. M. A. Hall. Correlation-based feature selection for discrete and numeric class machine learning. In Proceedings of the 17th International Conference on Machine Learning, pages 359–366, 2000. L. K. Hansen and P. Salamon. Neural network ensembles. IEEE Trans. on Pattern Analysis and Machine Intelligence, 12(10):993–1001, 1990. 1364  F EATURE S ELECTION WITH E NSEMBLES  T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. on Pattern Analysis and Machine Intelligence, 20(8):832–844, 1998. K. Kira and L. A. Rendell. A practical approach to feature selection. In ML92: Proceedings of the ninth international workshop on Machine learning, pages 249–256, San Francisco, CA, USA, 1992. Morgan Kaufmann Publishers Inc. ISBN 1-5586-247-X. D. Koller and M. Sahami. Toward optimal feature selection. In Proceedings of ICML-96, 13th International Conference on Machine Learning, pages 284–292, Bari, Italy, 1996. URL citeseer.nj.nec.com/koller96toward.html. S. Kullback and R.A. Liebler. On information and sufﬁciency. Annals of Mathematical Statistics, 22:76–86, 1951. H. Liu and L. Yu. Toward integrating feature selection algorithms for classiﬁcation and clustering. IEEE Trans. Knowledge and Data Eng., 17(4):491–502, 2005. S. Mukherjee, P. Niyogi, T. Poggio, and R. Rifkin. Learning theory: Stability is sufﬁcient for generalization and necessary and sufﬁcient for consistency of empirical risk minimization. Advances in Computational Mathematics, 25:161–193, 2006. B. Parmanto, P. Munro, and H. Doyle. Improving committee diagnosis with resampling techniques. In D. S. Touretzky, M. C. Mozer, and M. Hesselmo, editors, Advances in Neural Information Processing Systems 8, pages 882–888. Cambridge, MA: MIT Press, 1996. T. Poggio, R. Rifkin, S. Mukherjee, and A. Rakhlin. Bagging regularizes. In CBCL Paper 214/AI Memo 2002-003. MIT, Cambridge, MA, 2002. T. Poggio, R. Rifkin, S. Mukherjee, and P. Niyogi. General conditions for predictivity in learning theory. Nature, 428:419–422, 2004. M. Robnik-Sikonja and I. Kononenko. Theoretical and empirical analysis of relief and relieff. Machine Learning, 53:23–69, 2003. C. Stanﬁll and D. Waltz. Toward memory-based reasoning. Communications of the ACM, 29: 1213–1228, December 1986. A. Statnikov and C.F. Aliferis. Tied: An artiﬁcially simulated dataset with multiple Markov boundaries. Journal of Machine Learning Research Workshop Conference & Proceedings, 2009. to appear. H. Stoppiglia, G. Dreyfus, R. Dubois, and Y. Oussar. Ranking a random feature for variable and feature selection. Journal of Machine Learning Research, 3:1399–1414, March 2003. E. Tuv. Ensemble learning and feature selection. In I. Guyon, S. Gunn, M. Nikravesh, and L. Zadeh, editors, Feature Extraction, Foundations and Applications. Springer, 2006. E. Tuv, A. Borisov, and K. Torkkola. Feature selection using ensemble based ranking against artiﬁcial contrasts. In Proceedings of the International Joint Conference on Neural Networks (IJCNN), 2006. 1365  T UV, B ORISOV, RUNGER AND T ORKKOLA  G. Valentini and T. Dietterich. Low bias bagged support vector machines. In ICML 2003, pages 752–759, 2003. G. Valentini and F. Masulli. Ensembles of learning machines. In M. Marinaro and R. Tagliaferri, editors, Neural Nets WIRN Vietri-02, Lecture Notes in Computer Science. Springer-Verlag, 2002. J.W. Wisnowski, J.R. Simpson, D.C. Montgomery, and G.C. Runger. Resampling methods for variable selection in robust regression. Computational Statistics and Data Analysis, 43(3):341– 355, 2003. L. Yu and H. Liu. Efﬁcient feature selection via analysis of relevance and redundancy. J. of Machine Learning Research, 5:1205–1224, 2004.  1366</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
