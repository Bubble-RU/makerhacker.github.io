<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-22" href="../jmlr2009/jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">jmlr2009-22</a> <a title="jmlr-2009-22-reference" href="#">jmlr2009-22-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</h1>
<br/><p>Source: <a title="jmlr-2009-22-pdf" href="http://jmlr.org/papers/volume10/rieger09a/rieger09a.pdf">pdf</a></p><p>Author: Christian Rieger, Barbara Zwicknagl</p><p>Abstract: We introduce a new technique for the analysis of kernel-based regression problems. The basic tools are sampling inequalities which apply to all machine learning problems involving penalty terms induced by kernels related to Sobolev spaces. They lead to explicit deterministic results concerning the worst case behaviour of ε- and ν-SVRs. Using these, we show how to adjust regularization parameters to get best possible approximation orders for regression. The results are illustrated by some numerical examples. Keywords: sampling inequality, radial basis functions, approximation theory, reproducing kernel Hilbert space, Sobolev space</p><br/>
<h2>reference text</h2><p>S.C. Brenner and L.R. Scott. The Mathematical Theory of Finite Element Methods, volume 15 of Texts in Applied Mathematics. Springer, New York, 1994. C-C. Chang and C-L. Lin. Training ν-support vector regression: Theory and algorithms. Neural Computation, 14(8):1959–1977, 2002. T. Evgeniou, M. Pontil, and T. Poggio. Regularization networks and support vector machines. Advances in Computational Mathematics, 13:1–50, 2000. F. Girosi. An equivalence between sparse approximation and support vector machines. Neural Computation, 10 (8):1455–1480, 1998. C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine Learning Research, 6:1099–1125, 2005. C. Rieger and B. Zwicknagl. Sampling inequalities for inﬁnitely smooth functions, with applications to interpolation and machine learning. To appear in Advances in Computational Mathematics, 2008. M. Riplinger. Lernen als inverses Problem und deterministische Fehlerabschätzung bei Support Vektor Regression. Diplomarbeit, Universität des Saarlandes, 2007. R. Schaback. The missing wendland functions. To appear in Advances in Computational Mathematics, 2009. B. Schölkopf and A.J. Smola. Learning with kernels - Support Vector Machines, Regularisation, and Beyond. MIT Press, Cambridge, Massachusetts, 2002. B. Schölkopf, C. Burges, and V.Vapnik. Extracting support data for a given task. In Proceedings, First International Conference on Knowledge Discovery and Data Mining. CA:AAAI Press., Menlo Park, 1995. B. Schölkopf, R.C. Wiliamson, and P.L. Bartlett. New support vector algorithms. Neural Computation, 12:1207–1245, 2000. C.J. Stone. Optimal global rates of convergence for nonparametric regression. The Annals of Statistics, 10:1040–1053, 1982. V. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, New York, 1995. H. Wendland. Scattered Data Approximation. Cambridge Monographs on Applied and Computational Mathematics. Cambridge University Press, Cambridge, 2005. H. Wendland and C. Rieger. Approximate interpolation. Numerische Mathematik, 101:643–662, 2005. J. Wloka. Partielle Differentialgleichungen: Sobolevräume und Randwertaufgaben. Mathematische Leitfäden. Teubner, Stuttgart, 1982. B. Zwicknagl. Power series kernels. Constructive Approximation, 29(1):61–84, 2009. 2132</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
