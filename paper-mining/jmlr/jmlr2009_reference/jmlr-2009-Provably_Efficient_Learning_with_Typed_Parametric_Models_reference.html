<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-75" href="../jmlr2009/jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">jmlr2009-75</a> <a title="jmlr-2009-75-reference" href="#">jmlr2009-75-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</h1>
<br/><p>Source: <a title="jmlr-2009-75-pdf" href="http://jmlr.org/papers/volume10/brunskill09a/brunskill09a.pdf">pdf</a></p><p>Author: Emma Brunskill, Bethany R. Leffler, Lihong Li, Michael L. Littman, Nicholas Roy</p><p>Abstract: To quickly achieve good performance, reinforcement-learning algorithms for acting in large continuous-valued domains must use a representation that is both sufﬁciently powerful to capture important domain characteristics, and yet simultaneously allows generalization, or sharing, among experiences. Our algorithm balances this tradeoff by using a stochastic, switching, parametric dynamics representation. We argue that this model characterizes a number of signiﬁcant, real-world domains, such as robot navigation across varying terrain. We prove that this representational assumption allows our algorithm to be probably approximately correct with a sample complexity that scales polynomially with all problem-speciﬁc quantities including the state-space dimension. We also explicitly incorporate the error introduced by approximate planning in our sample complexity bounds, in contrast to prior Probably Approximately Correct (PAC) Markov Decision Processes (MDP) approaches, which typically assume the estimated MDP can be solved exactly. Our experimental results on constructing plans for driving to work using real car trajectory data, as well as a small robot experiment on navigating varying terrain, demonstrate that our dynamics representation enables us to capture real-world dynamics in a sufﬁcient manner to produce good performance. Keywords: reinforcement learning, provably efﬁcient learning</p><br/>
<h2>reference text</h2><p>Pieter Abbeel and Andrew Y. Ng. Exploration and apprenticeship learning in reinforcement learning. In Proceedings of the 22nd International Conference on Machine Learning (ICML), pages 1–8, 2005. Justin Boyan and Andrew Moore. Generalization in reinforcement learning: Safely approximating the value function. In Advances in Neural Information Processing Systems (NIPS) 7, pages 369– 376, 1995. Ronen I. Brafman and Moshe Tennenholtz. R-MAX—a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 3:213–231, 2002. Emma Brunskill, Bethany R. Lefﬂer, Lihong Li, Michael L. Littman, and Nicholas Roy. CORL: A continuous-state offset-dynamics reinforcement learner. In Proceedings of the 24th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 53–61, 2008. Jeffrey B. Burl. Linear Optimal Control. Prentice Hall, 1998. ISBN 9780201808681. Pablo Castro and Doina Precup. Using linear programming for Bayesian exploration in Markov decision processes. In Proceedings of the 20th International Joint Conference on Artiﬁcial Intelligence (IJCAI), pages 2437–2442, 2007. Chee-Seng Chow and John N. Tsitsiklis. The complexity of dynamic programming. Journal of Complexity, 5(4):466–488, 1989. Chee-Seng Chow and John N. Tsitsiklis. An optimal one-way multigrid algorithm for discrete-time stochastic control. IEEE Transactions on Automatic Control, 36(8):898–914, 1991. Finale Doshi, Joelle Pineau, and Nicholas Roy. Reinforcement learning with limited reinforcement: Using Bayes risk for active learning in POMDPs. In Proceedings of the 25th International Conference on Machine Learning (ICML), pages 256–263, 2008. Alain Dutech, Timothy Edmunds, Jelle Kok, Michail Lagoudakis, Michael L. Littman, Martin Riedmiller, Bryan Russell, Bruno Scherrer, Richard Sutton, Stephan Timmer, Nikos Vlassis, Adam White, and Shimon Whiteson. Reinforcement learning benchmarks and bake-offs II. In Advances in Neural Information Processing Systems (NIPS) 17 Workshop, 2005. Jakob Eriksson, Hari Balakrishnan, and Samuel Madden. Cabernet: A WiFi-Based Vehicular Content Delivery Network. In Proceedings of the 14th Conference on Mobile Computing and Networking (MOBICOM), pages 199–210, 2008. 1986  P ROVABLY E FFICIENT L EARNING WITH T YPED PARAMETRIC M ODELS  Gene H. Golub and Charles F. Van Loan. Matrix Computations. The Johns Hopkins University Press, 3rd edition, 1996. ISBN 0-801-85414-8. Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 1986. ISBN 0-521-38632-2. Eric Horvitz, Johnson Apacible, Raman Sarin, and Lin Liao. Prediction, expectation, and surprise: Methods, designs, and study of a deployed trafﬁc forecasting service. In Proceedings of the 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 275–283, 2005. Nicholas K. Jong and P. Stone. Model-based exploration in continuous state spaces. In Proceedings of the 7th Symposium on Abstraction, Reformulation, and Approximation (SARA), pages 258– 272, 2007. Sham Kakade. On the sample complexity of reinforcement learning. PhD thesis, University College London, 2003. Michael J. Kearns and Satinder P. Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning, 49(2–3):209–232, 2002. Levente Kocsis and Csaba Szepesv´ ri. Bandit based Monte-Carlo planning. In Proceedings of the a 17th European Conference on Machine Learning (ECML), pages 282–293, 2006. Solomon Kullback. A lower bound for discrimination in terms of variation. IEEE Transactions on Information Theory, 13(1):126–127, January 1967. Branislav Kveton and Milos Hauskrecht. Solving factored MDPs with exponential-family transition models. In Proceedings of the 16th International Conference on Automated Planning and Scheduling (ICAPS), pages 114–120, 2006. M.G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4:1107–1149, 2003. Bethany R. Lefﬂer, Michael L. Littman, and Timothy Edmunds. Efﬁcient reinforcement learning with relocatable action models. In Proceedings of the 22nd Conference on Artiﬁcial Intelligence (AAAI), pages 572–577, 2007. Lihong Li. A Unifying Framework for Computational Reinforcement Learning Theory. PhD thesis, Rutgers University, New Brunswick, NJ, 2009. Lihong Li, Michael L. Littman, and Thomas J. Walsh. Knows what it knows: A framework for self-aware learning. In Proceedings of the 25th International Conference on Machine Learning (ICML), pages 568–575, 2008. Janusz Marecki and Milind Tambe. Towards faster planning with continuous resources in stochastic domains. In Proceedings of the 23rd Conference on Artiﬁcial Intelligence (AAAI), pages 1049– 1055, 2008. Andrew Ng, H.Jin Kim, Michael Jordan, and Shankar Sastry. Autonomous helicopter ﬂight via reinforcement learning. In Advances in Neural Information Processing Systems (NIPS) 16, pages 799–806, 2004. 1987  B RUNSKILL , L EFFLER , L I , L ITTMAN AND ROY  Pascal Poupart, Nikos Vlassis, Jesse Hoey, and Kevin Regan. An analytic solution to discrete Bayesian reinforcement learning. In Proceedings of the 23rd International Conference on Machine Learning (ICML), pages 697–704, 2006. Stephane Ross, Brahim Chaib-draa, and Joelle Pineau. Bayesian reinforcement learning in continuous POMDPs with application to robot navigation. In Proceedings of the International Conference on Robotics and Automation (ICRA), pages 2845–2851, 2008. Alexander L. Strehl and Michael L. Littman. Online linear regression and its application to modelbased reinforcement learning. In Advances in Neural Information Processing Systems (NIPS) 20, pages 1417–1424, 2008. Alexander L. Strehl, Lihong Li, and Michael L. Littman. Incremental model-based learners with formal learning-time guarantees. In Proceedings of the 22nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 485–492, 2006. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998. Gerald J. Tesauro. TD-Gammon, a self-teaching backgammon program, achieves master-level play. Neural Computation, 6(2):215–219, 1994. John von Neumann. Some matrix-inequalities and metrization of matrix-space. Tomsk University Review, 1:286–300, 1937. Christopher J.C.H. Watkins. Learning from delayed rewards. PhD thesis, King’s College, University of Cambridge, United Kingdom, 1989.  1988</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
