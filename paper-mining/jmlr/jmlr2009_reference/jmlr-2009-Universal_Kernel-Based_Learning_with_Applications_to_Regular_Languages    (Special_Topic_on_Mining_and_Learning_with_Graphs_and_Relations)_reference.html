<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-98" href="../jmlr2009/jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">jmlr2009-98</a> <a title="jmlr-2009-98-reference" href="#">jmlr2009-98-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</h1>
<br/><p>Source: <a title="jmlr-2009-98-pdf" href="http://jmlr.org/papers/volume10/kontorovich09a/kontorovich09a.pdf">pdf</a></p><p>Author: Leonid (Aryeh) Kontorovich, Boaz Nadler</p><p>Abstract: We propose a novel framework for supervised learning of discrete concepts. Since the 1970’s, the standard computational primitive has been to ﬁnd the most consistent hypothesis in a given complexity class. In contrast, in this paper we propose a new basic operation: for each pair of input instances, count how many concepts of bounded complexity contain both of them. Our approach maps instances to a Hilbert space, whose metric is induced by a universal kernel coinciding with our computational primitive, and identiﬁes concepts with half-spaces. We prove that all concepts are linearly separable under this mapping. Hence, given a labeled sample and an oracle for evaluating the universal kernel, we can efﬁciently compute a linear classiﬁer (via SVM, for example) and use margin bounds to control its generalization error. Even though exact evaluation of the universal kernel may be infeasible, in various natural situations it is efﬁciently approximable. Though our approach is general, our main application is to regular languages. Our approach presents a substantial departure from current learning paradigms and in particular yields a novel method for learning this fundamental concept class. Unlike existing techniques, we make no structural assumptions on the corresponding unknown automata, the string distribution or the completeness of the training set. Instead, given a labeled sample our algorithm outputs a classiﬁer with guaranteed distribution-free generalization bounds; to our knowledge, the proposed framework is the only one capable of achieving the latter. Along the way, we touch upon several fundamental questions in complexity, automata, and machine learning. Keywords: grammar induction, regular language, ﬁnite state automaton, maximum margin hyperplane, kernel approximation</p><br/>
<h2>reference text</h2><p>Dana Angluin. On the complexity of minimum inference of regular sets. Information and Control, 3(39):337–350, 1978. Dana Angluin. Inference of reversible languages. Journal of the ACM (JACM), 3(29):741–765, 1982. Dana Angluin. Learning regular sets from queries and counterexamples. Inf. Comput., 75(2):87– 106, 1987. ISSN 0890-5401. doi: http://dx.doi.org/10.1016/0890-5401(87)90052-6. Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, 1999. Maria-Florina Balcan, Avrim Blum, and Santosh Vempala. Kernels as Features: On Kernels, Margins, and Low-dimensional Mappings. Machine Learning, 65(1):79–94, 2006. Peter Bartlett, Yoav Freund, Wee Sun Lee, and Robert E. Schapire. Boosting the margin: a new explanation for the effectiveness of voting methods. Ann. Statist., 26(5):1651–1686, 1998. Jeremiah Blocki. Turing machine kernel is not computable, 2007. tion/unpublished note.  Private communica-  Miguel Bugalho and Arlindo L. Oliveira. Inference of regular languages using state merging algorithms with search. Pattern Recognition, 38:1457, 2005. Vaˇek Chv´ tal and Endre Szemer´ di. Many hard examples for resolution. J. ACM, 35(4):759–768, s a e 1988. ISSN 0004-5411. doi: http://doi.acm.org/10.1145/48014.48016. Alexander Clark and Franck Thollard. Pac-learnability of probabilistic deterministic ﬁnite state automata. Journal of Machine Learning Research (JMLR), 5:473–497, 2004. Corinna Cortes, Leonid Kontorovich, and Mehryar Mohri. Learning languages with rational kernels. Computational Learning Theory (COLT), 2007. 1126  U NIVERSAL R EGULAR K ERNEL  Colin de la Higuera. A bibligraphical study of grammatical inference. Pattern Recognition, 38: 1332, 2005. Abraham Flaxman. A spectral technique for random satisﬁable 3cnf formulas. In SODA ’03: Proceedings of the fourteenth annual ACM-SIAM symposium on Discrete algorithms, pages 357– 363, Philadelphia, PA, USA, 2003. Society for Industrial and Applied Mathematics. ISBN 089871-538-5. Yoav Freund and Robert E. Schapire. Predicting {0, 1}-Functions on Randomly Drawn Points. In Proceedings of the Ninth Annual Conference on Computational Learning Theory (COLT 1996), pages 325–332, 1996. Yoav Freund, Michael Kearns, Dana Ron, Ronitt Rubinfeld, Robert E. Schapire, and Linda Sellie. Efﬁcient learning of typical ﬁnite automata from random walks. In STOC ’93: Proceedings of the Twenty-ﬁfth Annual ACM Symposium on Theory of Computing, pages 315–324, New York, NY, USA, 1993. ACM Press. Ashutosh Garg, Sariel Har-Peled, and Dan Roth. On generalization bounds, projection proﬁle, and margin distribution. In ICML ’02: Proceedings of the Nineteenth International Conference on Machine Learning, pages 171–178, San Francisco, CA, USA, 2002. Morgan Kaufmann Publishers Inc. ISBN 1-55860-873-7. E. Mark Gold. Language identiﬁcation in the limit. Information and Control, 50(10):447–474, 1967. E. Mark Gold. Complexity of automaton identiﬁcation from given data. Information and Control, 3(37):302–420, 1978. Wassily Hoeffding. Probability inequalities for sums of bounded random variables. American Statistical Association Journal, 58:13–30, 1963. Yoshiyasu Ishigami and Sei’ichi Tani. Vc-dimensions of ﬁnite automata and commutative ﬁnite automata with k letters and n states. Discrete Applied Mathematics, 74(3):229–240, 1997. Michael J. Kearns and Leslie G. Valiant. Cryptographic limitations on learning boolean formulae and ﬁnite automata. Journal of the ACM (JACM), 41(1):67–95, 1994. Micheal Kearns and Umesh Vazirani. An Introduction to Computational Learning Theory. The MIT Press, 1997. George Kimeldorf and Grace Wahba. Some results on Tchebychefﬁan spline functions. J. Math. Anal. Appl., 33:82–95, 1971. ISSN 0022-247x. Leonid Kontorovich. A universal kernel for learning regular languages. In The 5th International Workshop on Mining and Learning with Graphs (MLG 2007), Florence, Italy, 2007. Leonid Kontorovich, Corinna Cortes, and Mehryar Mohri. Learning linearly separable languages. In Proceedings of The 17th International Conference on Algorithmic Learning Theory (ALT 2006), volume 4264 of Lecture Notes in Computer Science, pages 288–303, Barcelona, Spain, October 2006. Springer, Heidelberg, Germany. 1127  KONTOROVICH AND NADLER  Kevin J. Lang. Random dfa’s can be approximately learned from sparse uniform samples. In Fifth Conference on Computational Learning Theory, page 45. ACM, 1992. Harry R. Lewis and Christos H. Papadimitriou. Elements of the Theory of Computation. Prentice Hall, 1981. Arlindo L. Oliveira and Joao P.M. Silva. Efﬁcient algorithms for the inference of minimum size dfas. Machine Learning, 44:93, 2001. Jos´ Oncina and Pedro Garcia. Inferring regular languages in polynomial update time. In Pattern e Recognition and Image Analysis, page 49. World Scientiﬁc, 1992. Jos´ Oncina, Pedro Garc´a, and Enrique Vidal. Learning subsequential transducers for pattern e ı recognition interpretation tasks. IEEE Trans. Pattern Anal. Mach. Intell., 15(5):448–458, 1993. Leonard Pitt and Manfred Warmuth. Prediction-preserving reducibility. Journal of Computer and System Sciences, 41(3):430–467, 1990. Leonard Pitt and Manfred Warmuth. The minimum consistent DFA problem cannot be approximated within any polynomial. Journal of the Assocation for Computing Machinery, 40(1):95– 142, 1993. Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In NIPS 2007. MIT Press, 2007. Jorma Rissanen. Stochastic Complexity in Statistical Inquiry Theory. World Scientiﬁc Publishing Co., Inc., 1989. Ronald L. Rivest and Robert E. Schapire. Diversity-based inference of ﬁnite automata. In Proc. 28th Annu. IEEE Sympos. Found. Comput. Sci., pages 78–87. IEEE Computer Society Press, Los Alamitos, CA, 1987. Dana Ron, Yoram Singer, and Naftali Tishby. On the learnability and usage of acyclic probabilistic ﬁnite automata. Journal of Computer and System Sciences, 56(2):133–152, 1998. Bernhard Sch¨ lkopf and Alex Smola. Learning with Kernels. MIT Press, 2002. o Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal estimated sub-gradient solver for svm. In ICML ’07: Proceedings of the 24th International Conference on Machine learning, pages 807–814, New York, NY, USA, 2007. ACM. ISBN 9781595937933. doi: 10.1145/1273496.1273598. URL http://portal.acm.org/citation.cfm?id=1273598. John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. A framework for structural risk minimisation. In COLT, pages 68–76, 1996. John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. Structural risk minimization over data-dependent hierarchies. IEEE transactions on Information Theory, 44: 1926–1940, 1998. Michael Sipser. Introduction to the Theory of Computation. Course Technology, 2005. 1128  U NIVERSAL R EGULAR K ERNEL  Boris A. Trakhtenbrot and Janis M. Barzdin. Finite Automata: Behavior and Synthesis, volume 1 of Fundamental Studies in Computer Science. North-Holland, Amsterdam, 1973. Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984. Vladimir N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-Verlag, 1982. Vladimir N. Vapnik. Statistical Learning Theory. John Wiley & Sons, 1998.  1129</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
