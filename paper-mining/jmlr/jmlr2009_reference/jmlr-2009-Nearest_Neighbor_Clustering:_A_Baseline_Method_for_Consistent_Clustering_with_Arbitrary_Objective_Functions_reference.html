<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>59 jmlr-2009-Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-59" href="../jmlr2009/jmlr-2009-Nearest_Neighbor_Clustering%3A_A_Baseline_Method_for_Consistent_Clustering_with_Arbitrary_Objective_Functions.html">jmlr2009-59</a> <a title="jmlr-2009-59-reference" href="#">jmlr2009-59-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>59 jmlr-2009-Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions</h1>
<br/><p>Source: <a title="jmlr-2009-59-pdf" href="http://jmlr.org/papers/volume10/bubeck09a/bubeck09a.pdf">pdf</a></p><p>Author: Sébastien Bubeck, Ulrike von Luxburg</p><p>Abstract: Clustering is often formulated as a discrete optimization problem. The objective is to ﬁnd, among all partitions of the data set, the best one according to some quality measure. However, in the statistical setting where we assume that the ﬁnite data set has been sampled from some underlying space, the goal is not to ﬁnd the best partition of the given sample, but to approximate the true partition of the underlying space. We argue that the discrete optimization approach usually does not achieve this goal, and instead can lead to inconsistency. We construct examples which provably have this behavior. As in the case of supervised learning, the cure is to restrict the size of the function classes under consideration. For appropriate “small” function classes we can prove very general consistency theorems for clustering optimization schemes. As one particular algorithm for clustering with a restricted function space we introduce “nearest neighbor clustering”. Similar to the k-nearest neighbor classiﬁer in supervised learning, this algorithm can be seen as a general baseline algorithm to minimize arbitrary clustering objective functions. We prove that it is statistically consistent for all commonly used clustering objective functions. Keywords: clustering, minimizing objective functions, consistency</p><br/>
<h2>reference text</h2><p>S. Ben-David. A framework for statistical clustering with constant time approximation algorithms for k-median and k-means clustering. Machine Learning, 66:243 – 257, 2007. J. Buhmann. Empirical risk approximation: An induction principle for unsupervised learning. Technical report, University of Bonn, 1998. A. Czumaj and C. Sohler. Sublinear-time approximation algorithms for clustering via random sampling. Random Struct. Algorithms, 30(1-2):226–256, 2007. L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, o New York, 1996. M. Figueiredo and A. Jain. Unsupervised learning of ﬁnite mixture models. PAMI, 24(3):381–396, 2002. C. Fraley and A. Raftery. How many clusters? Which clustering method? Answers via model-based cluster analysis. Comput. J, 41(8):578–588, 1998. J. Fritz. Distribution-free exponential error bound for nearest neighbor pattern classiﬁcation. IEEE Trans. Inf. Th., 21(5):552 – 557, 1975. M. Garey, D. Johnson, and H. Witsenhausen. The complexity of the generalized Lloyd - max problem (corresp.). IEEE Trans. Inf. Theory, 28(2):255–256, 1982. P. Gr¨ nwald. The Minimum Description Length Principle. MIT Press, Cambridge, MA, 2007. u S. Guattery and G. Miller. On the quality of spectral separators. SIAM Journal of Matrix Anal. Appl., 19(3):701 – 719, 1998. J. Hartigan. Consistency of single linkage for high-density clusters. JASA, 76(374):388 – 394, 1981. J. Hartigan. Statistical theory in clustering. Journal of Classiﬁcation, 2:63 – 76, 1985. M. Inaba, N. Katoh, and H. Imai. Applications of weighted Voronoi diagrams and randomization to variance-based k-clustering. In Proceedings of the 10th Annual Symposium on Computational Geometry, pages 332–339. ACM Press, Stony Brook, USA, 1994. 696  C ONSISTENT C LUSTERING WITH A RBITRARY O BJECTIVE F UNCTIONS  P. Indyk. Sublinear time algorithms for metric space problems. In Proceedings of the Thirty-ﬁrst Annual ACM Symposium on Theory of Computing (STOC), pages 428–434. ACM Press, New York, 1999. S. Jegelka. Statistical learning theory approaches to clustering. Master’s thesis, University of T¨ bingen, 2007. Available at http://www.kyb.mpg.de/publication.html?user=jegelka. u R. Kannan, S. Vempala, and A. Vetta. On clusterings: Good, bad and spectral. Journal of the ACM, 51(3):497–515, 2004. C. McDiarmid. On the method of bounded differences. Surveys in Combinatorics, pages 148 – 188, 1989. Cambridge University Press. G. McLachlan and D. Peel. Finite Mixture Models. John Wiley, New York, 2004. N. Mishra, D. Oblinger, and L. Pitt. Sublinear time approximate clustering. In Proceedings of the Twelfth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA-01), pages 439–447. ACM Press, New York, 2001. M. Newman. Finding community structure in networks using the eigenvectors of matrices. Physical Review E, 74:036104, 2006. D. Pollard. Strong consistency of k-means clustering. Annals of Statistics, 9(1):135 – 140, 1981. A. Rakhlin and A. Caponnetto. Stability of k-means clustering. In B. Sch¨ lkopf, J. Platt, and o T. Hoffman, editors, Advances in Neural Information Processing Systems 19. MIT Press, Cambridge, MA, 2007. D. Spielman and S. Teng. Spectral partitioning works: planar graphs and ﬁnite element meshes. In 37th Annual Symposium on Foundations of Computer Science (Burlington, VT, 1996), pages 96 – 105. IEEE Comput. Soc. Press, Los Alamitos, CA, 1996. (See also extended technical report version.). A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes. Springer, New York, 1996. V. Vapnik. The Nature of Statistical Learning Theory. Springer Verlag, New York, 1995. U. von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395 – 416, 2007. U. von Luxburg and S. Ben-David. Towards a statistical theory of clustering. In PASCAL workshop on Statistics and Optimization of Clustering, London, 2005. U. von Luxburg, M. Belkin, and O. Bousquet. Consistency of spectral clustering. Annals of Statistics, 36(2):555 – 586, 2008. U. von Luxburg, S. Bubeck, S. Jegelka, and M. Kaufmann. Consistent minimization of clustering objective functions. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems (NIPS) 21. MIT Press, Cambridge, MA, 2008. 697  B UBECK AND VON L UXBURG  D. Wagner and F. Wagner. Between min cut and graph bisection. In Proceedings of the 18th International Symposium on Mathematical Foundations of Computer Science (MFCS), pages 744 – 750, London, 1993. Springer. M. Wong and T. Lane. A kth nearest neighbor clustering procedure. J.R. Statist.Soc B, 45(3): 362 – 368, 1983.  698</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
