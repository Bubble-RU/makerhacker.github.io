<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 jmlr-2009-Incorporating Functional Knowledge in Neural Networks</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-42" href="../jmlr2009/jmlr-2009-Incorporating_Functional_Knowledge_in_Neural_Networks.html">jmlr2009-42</a> <a title="jmlr-2009-42-reference" href="#">jmlr2009-42-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>42 jmlr-2009-Incorporating Functional Knowledge in Neural Networks</h1>
<br/><p>Source: <a title="jmlr-2009-42-pdf" href="http://jmlr.org/papers/volume10/dugas09a/dugas09a.pdf">pdf</a></p><p>Author: Charles Dugas, Yoshua Bengio, François Bélisle, Claude Nadeau, René Garcia</p><p>Abstract: Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in its two arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of Lipschitz1 functions with these and other properties. We apply this new class of functions to the task of modelling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints. Keywords: neural networks, universal approximation, monotonicity, convexity, call options</p><br/>
<h2>reference text</h2><p>A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory, 39(3):930–945, 1993. F. Black and M. Scholes. The pricing of options and corporate liabilities. Journal of Political Economy, 81(3):637–654, 1973. G. Cybenko. Continuous valued neural networks with two hidden layers are sufﬁcient. Technical report, Department of Computer Science, Tufts University, Medford, MA, 1988. G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems, 2:303–314, 1989. M.C. Delfour and J.-P. Zol´ sio. Shapes and Geometries: Analysis, Differential Calculus, and Optie mization. SIAM, 2001. C. Dugas, O. Bardou, and Y. Bengio. Analyses empiriques sur des transactions d’options. Technical Report 1176, D´ partment d’informatique et de Recherche Op´ rationnelle, Universit´ de e e e Montr´ al, Montr´ al, Qu´ bec, Canada, 2000. e e e R. Garcia and R. Gencay. Pricing and hedging derivative securities with neural networks and a ¸ homogeneity hint. Technical Report 98s-35, CIRANO, Montr´ al, Qu´ bec, Canada, 1998. e e K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators. Neural Networks, 2:359–366, 1989. 1261  ´ D UGAS , B ENGIO , B E LISLE , NADEAU AND G ARCIA  J.M. Hutchinson, A.W. Lo, and T. Poggio. A nonparametric approach to pricing and hedging derivative securities via learning networks. Journal of Finance, 49(3):851–889, 1994. M. Leshno, V. Lin, A. Pinkus, and S. Schocken. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural Networks, 6:861–867, 1993. J. Moody. Prediction Risk and Architecture Selection for Neural Networks. Springer, 1994.  1262</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
