<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 jmlr-2009-Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-28" href="../jmlr2009/jmlr-2009-Entropy_Inference_and_the_James-Stein_Estimator%2C_with_Application_to_Nonlinear_Gene_Association_Networks.html">jmlr2009-28</a> <a title="jmlr-2009-28-reference" href="#">jmlr2009-28-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>28 jmlr-2009-Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks</h1>
<br/><p>Source: <a title="jmlr-2009-28-pdf" href="http://jmlr.org/papers/volume10/hausser09a/hausser09a.pdf">pdf</a></p><p>Author: Jean Hausser, Korbinian Strimmer</p><p>Abstract: We present a procedure for effective estimation of entropy and mutual information from smallsample data, and apply it to the problem of inferring high-dimensional gene association networks. SpeciÄ?Ĺš cally, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efÄ?Ĺš cient statistically as well as computationally. Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling. We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data. A computer program is available that implements the proposed shrinkage estimator. Keywords: entropy, shrinkage estimation, James-Stein estimator, Ă˘&euro;&oelig;small n, large pĂ˘&euro;? setting, mutual information, gene association network</p><br/>
<h2>reference text</h2><p>A. Agresti and D. B. Hitchcock. Bayesian inference for categorical data analysis. Statist. Meth. Appl., 14:297Ă˘&euro;&ldquo;330, 2005. A. J. Butte, P. Tamayo, D. Slonim, T. R. Golub, and I. S. Kohane. Discovering functional relationships between RNA expression and chemotherapeutic susceptibility using relevance networks. Proc. Natl. Acad. Sci. USA, 97:12182Ă˘&euro;&ldquo;12186, 2000. A. Chao and T.-J. Shen. Nonparametric estimation of ShannonĂ˘&euro;&trade;s index of diversity when there are unseen species. Environ. Ecol. Stat., 10:429Ă˘&euro;&ldquo;443, 2003. A. Dobra, C. Hans, B. Jones, J. R. Nevins, G. Yao, and M. West. Sparse graphical models for exploring gene expression data. J. Multiv. Anal., 90:196Ă˘&euro;&ldquo;212, 2004. B. Efron and C. N. Morris. SteinĂ˘&euro;&trade;s estimation rule and its competitorsĂ˘&euro;&ldquo;an empirical Bayes approach. J. Amer. Statist. Assoc., 68:117Ă˘&euro;&ldquo;130, 1973. 1481  H AUSSER AND S TRIMMER  S. E. Fienberg and P. W. Holland. Simultaneous estimation of multinomial cell probabilities. J. Amer. Statist. Assoc., 68:683Ă˘&euro;&ldquo;691, 1973. D. Freedman and P. Diaconis. On the histogram as a density estimator: L2 theory. Z. Wahrscheinlichkeitstheorie verw. Gebiete, 57:453Ă˘&euro;&ldquo;476, 1981. N. Friedman. Inferring cellular networks using probabilistic graphical models. Science, 303:799Ă˘&euro;&ldquo; 805, 2004. S. Geisser. On prior distributions for binary trials. The American Statistician, 38:244Ă˘&euro;&ldquo;251, 1984. A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian Data Analysis. Chapman & Hall/CRC, Boca Raton, 2nd edition, 2004. I. J. Good. The population frequencies of species and the estimation of population parameters. Biometrika, 40:237Ă˘&euro;&ldquo;264, 1953. L. A. Goodman. A simple method for improving some estimators. Ann. Math. Statist., 24:114Ă˘&euro;&ldquo;117, 1953. M. H. J. Gruber. Improving EfÄ?Ĺš ciency By Shrinkage. Marcel Dekker, Inc., New York, 1998. D. Holste, I. GroÄ&sbquo;&Yuml;e, and H. Herzel. BayesĂ˘&euro;&trade; estimators of generalized entropies. J. Phys. A: Math. Gen., 31:2551Ă˘&euro;&ldquo;2566, 1998. D. G. Horvitz and D. J. Thompson. A generalization of sampling without replacement from a Ä?Ĺš nite universe. J. Amer. Statist. Assoc., 47:663Ă˘&euro;&ldquo;685, 1952. W. James and C. Stein. Estimation with quadratic loss. In Proc. Fourth Berkeley Symp. Math. Statist. Probab., volume 1, pages 361Ă˘&euro;&ldquo;379, Berkeley, 1961. Univ. California Press. H. Jeffreys. An invariant form for the prior probability in estimation problems. Proc. Roc. Soc. (Lond.) A, 186:453Ă˘&euro;&ldquo;461, 1946. M. Kalisch and P. BÄ&sbquo;Ĺşhlmann. Estimating high-dimensional directed acyclic graphs with the PCalgorithm. J. Machine Learn. Res., 8:613Ă˘&euro;&ldquo;636, 2007. R. E. Krichevsky and V. K. TroÄ?Ĺš mov. The performance of universal encoding. IEEE Trans. Inf. Theory, 27:199Ă˘&euro;&ldquo;207, 1981. H. LÄ&sbquo;Â¤hdesmÄ&sbquo;Â¤ki and I. Shmulevich. Learning the structure of dynamic Bayesian networks from time series and steady state measurements. Mach. Learn., 71:185Ă˘&euro;&ldquo;217, 2008. O. Ledoit and M. Wolf. Improved estimation of the covariance matrix of stock returns with an application to portfolio selection. J. Empir. Finance, 10:603Ă˘&euro;&ldquo;621, 2003. D. J. C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University Press, Cambridge, 2003. A.A. Margolin, I. Nemenman, K. Basso, C. Wiggins, G. Stolovitzky, R. Dalla Favera, and A. Califano. ARACNE: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context. BMC Bioinformatics, 7 (Suppl. 1):S7, 2006. 1482  E NTROPY I NFERENCE AND THE JAMES -S TEIN E STIMATOR  N. Meinshausen and P. BÄ&sbquo;Ĺşhlmann. High-dimensional graphs and variable selection with the Lasso. Ann. Statist., 34:1436Ă˘&euro;&ldquo;1462, 2006. P. E. Meyer, K. Kontos, F. LaÄ?Ĺš tte, and G. Bontempi. Information-theoretic inference of large transcriptional regulatory networks. EURASIP J. Bioinf. Sys. Biol., page doi:10.1155/2007/79879, 2007. G. A. Miller. Note on the bias of information estimates. In H. Quastler, editor, Information Theory in Psychology II-B, pages 95Ă˘&euro;&ldquo;100. Free Press, Glencoe, IL, 1955. I. Nemenman, F. Shafee, and W. Bialek. Entropy and inference, revisited. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 471Ă˘&euro;&ldquo;478, Cambridge, MA, 2002. MIT Press. R. Opgen-Rhein and K. Strimmer. Accurate ranking of differentially expressed genes by a distribution-free shrinkage approach. Statist. Appl. Genet. Mol. Biol., 6:9, 2007a. R. Opgen-Rhein and K. Strimmer. From correlation to causation networks: a simple approximate learning algorithm and its application to high-dimensional plant gene expression data. BMC Systems Biology, 1:37, 2007b. A. Orlitsky, N. P. Santhanam, and J. Zhang. Always Good Turing: asymptotically optimal probability estimation. Science, 302:427Ă˘&euro;&ldquo;431, 2003. W. Perks. Some observations on inverse probability including a new indifference rule. J. Inst. Actuaries, 73:285Ă˘&euro;&ldquo;334, 1947. R Development Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2008. URL http://www.R-project.org. ISBN 3-900051-07-0. C. Rangel, J. Angus, Z. Ghahramani, M. Lioumi, E. Sotheran, A. Gaiba, D. L. Wild, and F. Falciani. Modeling T-cell activation using gene expression proÄ?Ĺš ling and state space modeling. Bioinformatics, 20:1361Ă˘&euro;&ldquo;1372, 2004. J. SchÄ&sbquo;Â¤fer and K. Strimmer. An empirical Bayes approach to inferring large-scale gene association networks. Bioinformatics, 21:754Ă˘&euro;&ldquo;764, 2005a. J. SchÄ&sbquo;Â¤fer and K. Strimmer. A shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics. Statist. Appl. Genet. Mol. Biol., 4:32, 2005b. W. Schmidt-Heck, R. Guthke, S. Toepfer, H. Reischer, K. Duerrschmid, and K. Bayer. Reverse engineering of the stress response during expression of a recombinant protein. In Proceedings of the EUNITE symposium, 10-12 June 2004, Aachen, Germany, pages 407Ă˘&euro;&ldquo;412, 2004. Verlag Mainz. T. SchÄ&sbquo;Ĺşrmann and P. Grassberger. Entropy estimation of symbol sequences. Chaos, 6:414Ă˘&euro;&ldquo;427, 1996. S. M. Stigler. A Galtonian perspective on shrinkage estimators. Statistical Science, 5:147Ă˘&euro;&ldquo;155, 1990. 1483  H AUSSER AND S TRIMMER  D.R. Stinson. Cryptography: Theory and Practice. CRC Press, 2006. S. P. Strong, R. Koberle, R. de Ruyter van Steveninck, and W. Bialek. Entropy and information in neural spike trains. Phys. Rev. Letters, 80:197Ă˘&euro;&ldquo;200, 1998. J. R. Thompson. Some shrinkage techniques for estimating the mean. J. Amer. Statist. Assoc., 63: 113Ă˘&euro;&ldquo;122, 1968. S. Trybula. Some problems of simultaneous minimax estimation. Ann. Math. Statist., 29:245Ă˘&euro;&ldquo;253, 1958. F. Tuyl, R. Gerlach, and K. Mengersen. A comparison of Bayes-Laplace, Jeffreys, and other priors: the case of zero events. The American Statistician, 62:40Ă˘&euro;&ldquo;44, 2008. V. Q. Vu, B. Yu, and R. E. Kass. Coverage-adjusted entropy estimation. Stat. Med., 26:4039Ă˘&euro;&ldquo;4060, 2007. G. Yeo and C. B. Burge. Maximum entropy modeling of short sequence motifs with applications to RNA splicing signals. J. Comp. Biol., 11:377Ă˘&euro;&ldquo;394, 2004.  1484</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
