<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-24" href="../jmlr2009/jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">jmlr2009-24</a> <a title="jmlr-2009-24-reference" href="#">jmlr2009-24-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</h1>
<br/><p>Source: <a title="jmlr-2009-24-pdf" href="http://jmlr.org/papers/volume10/weinberger09a/weinberger09a.pdf">pdf</a></p><p>Author: Kilian Q. Weinberger, Lawrence K. Saul</p><p>Abstract: The accuracy of k-nearest neighbor (kNN) classiﬁcation depends signiﬁcantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classiﬁcation from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classiﬁcation using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modiﬁcation or extension for problems in multiway (as opposed to binary) classiﬁcation. In our framework, the Mahalanobis distance metric is obtained as the solution to a semideﬁnite program. On several data sets of varying size and difﬁculty, we ﬁnd that metrics trained in this way lead to signiﬁcant improvements in kNN classiﬁcation. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner. Keywords: convex optimization, semi-deﬁnite programming, Mahalanobis distance, metric learning, multi-class classiﬁcation, support vector machines</p><br/>
<h2>reference text</h2><p>A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. Learning a Mahalanobis metric from equivalence constraints. Journal of Machine Learning Research, 6(1):937–965, 2006. S. Belongie, J. Malik, and J. Puzicha. Shape matching and object recognition using shape contexts. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24 (4):509–522, 2002. A. Beygelzimer, S. Kakade, and J. Langford. Cover trees for nearest neighbor. In Proceedings of the Twenty Third International Conference on Machine Learning, pages 97–104, Pittsburgh, PA, 2006. M. Bilenko, S. Basu, and R.J. Mooney. Integrating constraints and metric learning in semisupervised clustering. In Proceedings of the Twenty First International Conference on Machine Learning (ICML-04), pages 839–846, Banff, Canada, 2004. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004. S. Chopra, R. Hadsell, and Y. LeCun. Learning a similiarty metric discriminatively, with application to face veriﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR-05), pages 349–356, San Diego, CA, 2005. T. Cover and P. Hart. Nearest neighbor pattern classiﬁcation. In IEEE Transactions in Information Theory, IT-13, pages 21–27, 1967. 241  W EINBERGER AND S AUL  K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning Research, 2:265–292, 2001. S. Dasgupta and A. Gupta. An elementary proof of the Johnson-Lindenstrauss lemma. Technical Report 99–006, International Computer Science Institute, UC Berkeley, 1999. T. De Bie, M. Momma, and N. Cristianini. Efﬁciently Learning the Metric with Side-Information. Lecture Notes in Computer Science, pages 175–189, 2003. T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting output codes. In Journal of Artiﬁcial Intelligence Research, volume 2, pages 263–286, 1995. R. A. Fisher. The use of multiple measures in taxonomic problems. Annals of Eugenics, 7:179–188, 1936. J. H. Friedman, J. L. Bentley, and R. A. Finkel. An algorithm for ﬁnding best matches in logarithmic expected time. ACM Transactions on Mathematical Software, 3(3):209–226, 1977. A. Globerson and S. T. Roweis. Metric learning by collapsing classes. In Advances in Neural Information Processing Systems 18, 2006. J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 513–520, Cambridge, MA, 2005. MIT Press. T. Hastie and R. Tibshirani. Discriminant adaptive nearest neighbor classiﬁcation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 18:607–616, 1996. P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604–613, 1998. I. T. Jolliffe. Principal Component Analysis. Springer-Verlag, New York, 1986. M. P. Kumar, P. H. S. Torr, and A. Zisserman. An invariant large margin nearest neighbour classiﬁer. In Proceedings of the Eleventh IEEE International Conference on Computer Vision (ICCV-07), pages 1–8, Rio de Janeiro, Brazil, 2007. J.T. Kwok and I.W. Tsang. Learning with idealized kernels. In Proceedings of the Twentieth International Conference on Machine Learning (ICML-03), pages 400–407, Washington, D.C., 2003. G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel matrix with semideﬁnite programming. Journal of Machine Learning Research, 5:27–72, 2004. Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes, J. Denker, H. Drucker, I. Guyon, U. Muller, E. Sackinger, P. Simard, and V. Vapnik. A comparison of learning algorithms for handwritten digit recognition. In F. Fogelman and P. Gallinari, editors, Proceedings of the 1995 International Conference on Artiﬁcial Neural Networks (ICANN-95), pages 53–60, Paris, 1995. 242  D ISTANCE M ETRIC L EARNING  T. Liu, A. W. Moore, A. Gray, and K. Yang. An investigation of practical approximate nearest neighbor algorithms. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 825–832. MIT Press, Cambridge, MA, 2005. A. Kachites McCallum. Bow: A toolkit for statistical language modeling, text retrieval, classiﬁcation and clustering. http://www.cs.cmu.edu/ mccallum/bow, 1996. K.-R. M¨ ller, S. Mika, G. R¨ sch, K. Tsuda, and B. Sch¨ kopf. An introduction to kernel-based u a o learning algorithms. IEEE Transactions on Neural Networks, 12(2):181–201, 2001. S. Omohundro. Efﬁcient algorithms with neural network behavior. Complex Systems, 1:273–347, 1987. B. Sch¨ lkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regularization, o Optimization, and Beyond. MIT Press, Cambridge, MA, 2002. B. Sch¨ lkopf, A. J. Smola, and K.-R. M¨ ller. Nonlinear component analysis as a kernel eigenvalue o u problem. Neural Computation, 10:1299–1319, 1998. S. Shalev-Shwartz, Y. Singer, and A. Y. Ng. Online and batch learning of pseudo-metrics. In Proceedings of the Twenty First International Conference on Machine Learning (ICML-04), pages 94–101, Banff, Canada, 2004. N. Shental, T. Hertz, D. Weinshall, and M. Pavel. Adjustment learning and relevant component analysis. In Proceedings of the Seventh European Conference on Computer Vision (ECCV-02), volume 4, pages 776–792, London, UK, 2002. Springer-Verlag. J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), pages 888–905, August 2000. P. Y. Simard, Y. LeCun, and J. Decker. Efﬁcient pattern recognition using a new transformation distance. In S. Hanson, J. Cowan, and L. Giles, editors, Advances in Neural Information Processing Systems 6, pages 50–58, San Mateo, CA, 1993. Morgan Kaufman. L. Torresani and K.C. Lee. Large margin component analysis. In B. Sch¨ lkopf, J. Platt, and T. Hofo mann, editors, Advances in Neural Information Processing Systems 19, pages 1385–1392. MIT Press, Cambridge, MA, 2007. I.W. Tsang, P.M. Cheung, and J.T. Kwok. Kernel relevant component analysis for distance metric learning. In Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN-05), volume 2, pages 954–959, Montreal, Canada, 2005. M. Turk and A. Pentland. Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1): 71–86, 1991. L. Vandenberghe and S. P. Boyd. Semideﬁnite programming. SIAM Review, 38(1):49–95, March 1996. M. Varma and D. Ray. Learning the discriminative power-invariance trade-off. In Proceedings of the Eleventh IEEE International Conference on Computer Vision (ICCV-07), pages 1–8, 2007. 243  W EINBERGER AND S AUL  K. Q. Weinberger and L. K. Saul. Fast solvers and efﬁcient implementations for distance metric learning. In Proceedings of the Twenty Fifth International Conference on Machine learning, pages 1160–1167, Helsinki, Finland, 2008. K. Q. Weinberger, J. Blitzer, and L. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation. In Y. Weiss, B. Sch¨ lkopf, and J. Platt, editors, Advances in Neural Information o Processing Systems 18, pages 1473–1480. MIT Press, Cambridge, MA, 2006. E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance metric learning, with application to clustering with side-information. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 521–528, Cambridge, MA, 2002. MIT Press.  244</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
