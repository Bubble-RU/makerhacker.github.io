<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 jmlr-2009-On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-65" href="../jmlr2009/jmlr-2009-On_Uniform_Deviations_of_General_Empirical_Risks_with_Unboundedness%2C_Dependence%2C_and_High_Dimensionality.html">jmlr2009-65</a> <a title="jmlr-2009-65-reference" href="#">jmlr2009-65-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>65 jmlr-2009-On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality</h1>
<br/><p>Source: <a title="jmlr-2009-65-pdf" href="http://jmlr.org/papers/volume10/jiang09a/jiang09a.pdf">pdf</a></p><p>Author: Wenxin Jiang</p><p>Abstract: The statistical learning theory of risk minimization depends heavily on probability bounds for uniform deviations of the empirical risks. Classical probability bounds using Hoeffding’s inequality cannot accommodate more general situations with unbounded loss and dependent data. The current paper introduces an inequality that extends Hoeffding’s inequality to handle these more general situations. We will apply this inequality to provide probability bounds for uniform deviations in a very general framework, which can involve discrete decision rules, unbounded loss, and a dependence structure that can be more general than either martingale or strong mixing. We will consider two examples with high dimensional predictors: autoregression (AR) with ℓ1 -loss, and ARX model with variable selection for sign classiﬁcation, which uses both lagged responses and exogenous predictors. Keywords: dependence, empirical risk, probability bound, unbounded loss, uniform deviation</p><br/>
<h2>reference text</h2><p>P. L. Bartlett and G. Lugosi. An inequality for uniform deviations of sample averages from their means. Statistics and Probability Letters, 44:55-62, 1999. 994  U NIFORM D EVIATION OF G ENERAL E MPIRICAL R ISKS  B. Bercu, E. Gassiat, and E. Rio. Concentration inequalities, large and moderate deviations for self-normalized empirical processes. Annals of Probability, 30:1576-1604, 2002. O. Bousquet, S. Boucheron, and G. Lugosi. Introduction to statistical learning theory. Advanced Lectures on Machine Learning 2003 (O. Bousquet, U. von Luxburg, G. R¨ tsch, eds.) Springer, a Berlin, 169-207, 2003. J. Davidson. Stochastic Limit Theory. Oxford University Press, Oxford, 1994. R. M. de Jong and T. M. Woutersen. Dynamic time series binary choice. Manuscript, Ohio State University, 2004. Downloadable at http://www.econ.ohio-state.edu/dejong/tiemen45. pdf. L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, New o York, 1996. C. Francq and J.-M. Zako¨an. Mixing properties of a general class of GARCH(1,1) models without ı moment assumptions on the observed process. Econometric Theory, 22:815-834, 2006. D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning applications. Information and Computation, 100:78-150, 1992. W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:13-30, 1963. J. L. Horowitz. A smoothed maximum score estimator for the binary response model. Econometrica, 60:505-531, 1992. W. Jiang and M. A. Tanner. Risk minimization for time series binary choice with variable selection. Technical Report 07-02, Department of Statistics, Northwestern University, 2007. Downloadable at http://newton.stats.northwestern.edu/˜jiang/tr/choice1.tr.pdf. W. Jiang and M. A. Tanner. Gibbs posterior for variable selection in high dimensional classiﬁcation and data mining. Annals of Statistics, 36:2207-2231. 2008. Downloadable at http://newton. stats.northwestern.edu/˜jiang/tr/gibbsone2.tr.pdf. A. C. Lozano, S. R. Kulkarni, and R. E. Schapire. Convergence and consistency of regularized boosting algorithms with stationary beta-mixing observations. In Advances in Neural Information Processing Systems 18, 2006. Downloadable at http://www.cs.princeton.edu/˜schapire/ boost.html. C. McDiarmid. Concentration. Probabilistic Methods for Algorithmic Discrete Mathematics (M. Habib, C. McDiarmid, J. Ramirez, B. Reed, eds.) 195–248, Springer, Berlin, 1998. R. Meir and T. Zhang. Generalization error bounds for Bayesian mixture algorithms. Journal of Machine Learning Research, 4:839-860, 2003. W. K. Newey. Uniform convergence in probability and stochastic equicontinuity. Econometrica, 59:1161-1167, 1991. 995  J IANG  D. Pollard. Uniform ratio limit theorems for empirical processes. Scandinavian Journal of Statistics, 22:271-278, 1995. S. Van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, Cambridge, 2000. M. Vidyasagar. Convergence of empirical means with alpha-mixing input sequences, and an application to PAC learning. Proceedings of the 44th IEEE Conference on Decision and Control, and the European Control Conference 2005, pages 560-565, 2005. B. Yu. Rates of convergence for empirical processes of stationary mixing sequences. Annals of Probability, 22:94-114, 1994. T. Zhang. Information theoretical upper and lower bounds for statistical estimation. IEEE Transaction on Information Theory, 52:1307- 1321, 2006. B. Zou and L. Li. The performance bounds of learning machines based on exponentially strong mixing sequences, Computers and Mathematics with Applications, 53:1050-1058, 2007.  996</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
