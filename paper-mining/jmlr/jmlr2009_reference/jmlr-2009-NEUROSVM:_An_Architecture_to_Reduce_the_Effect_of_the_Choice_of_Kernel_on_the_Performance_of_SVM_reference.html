<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-58" href="../jmlr2009/jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">jmlr2009-58</a> <a title="jmlr-2009-58-reference" href="#">jmlr2009-58-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</h1>
<br/><p>Source: <a title="jmlr-2009-58-pdf" href="http://jmlr.org/papers/volume10/ghanty09a/ghanty09a.pdf">pdf</a></p><p>Author: Pradip Ghanty, Samrat Paul, Nikhil R. Pal</p><p>Abstract: In this paper we propose a new multilayer classiﬁer architecture. The proposed hybrid architecture has two cascaded modules: feature extraction module and classiﬁcation module. In the feature extraction module we use the multilayered perceptron (MLP) neural networks, although other tools such as radial basis function (RBF) networks can be used. In the classiﬁcation module we use support vector machines (SVMs)—here also other tool such as MLP or RBF can be used. The feature extraction module has several sub-modules each of which is expected to extract features capturing the discriminating characteristics of different areas of the input space. The classiﬁcation module classiﬁes the data based on the extracted features. The resultant architecture with MLP in feature extraction module and SVM in classiﬁcation module is called NEUROSVM. The NEUROSVM is tested on twelve benchmark data sets and the performance of the NEUROSVM is found to be better than both MLP and SVM. We also compare the performance of proposed architecture with that of two ensemble methods: majority voting and averaging. Here also the NEUROSVM is found to perform better than these two ensemble methods. Further we explore the use of MLP and RBF in the classiﬁcation module of the proposed architecture. The most attractive feature of NEUROSVM is that it practically eliminates the severe dependency of SVM on the choice of kernel. This has been veriﬁed with respect to both linear and non-linear kernels. We have also demonstrated that for the feature extraction module, the full training of MLPs is not needed. Keywords: feature extraction, neural networks (NNs), support vector machines (SVMs), hybrid system, majority voting, averaging c 2009 Pradip Ghanty, Samrat Paul and Nikhil R. Pal. G HANTY, PAUL AND PAL</p><br/>
<h2>reference text</h2><p>M. M. Adankon and M. Cheriet. Optimizing resources in model selection for support vector machine. Pattern Recognition, 40(3):953–963, 2007. 619  G HANTY, PAUL AND PAL  T. Andersen and T. Martinezr. Cross validation and mlp architecture selection. In Proceedings of the International Joint Conference on Neural Networks (IJCNN ’99), volume 3, pages 1614–1619, 1999. C. L. Blake and C. J. Merz. UCI Repository of Machine Learning Databases: Univ. of California. Dept. of Inform. and Comput. Sci, 1998. B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classiﬁers. In Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory, pages 144–152, 1992. L. Bottou and P. Gallinari. A framework for the cooperation of learning algorithms. Advances in Neural Information Processing Systems, 3:781–788, 1991. G. Brown, J. L. Wyatt, and P. Tino. Managing diversity in regression ensembles. Journal of Machine Learning Research, 6:1621–1650, 2005. N. V. Chawla, L. O. Hall, K. W. Bowyer, and W. P. Kegelmeyer. Learning ensembles from bites: A scalable and accurate approach. Journal of Machine Learning Research, 5:421–451, 2004. C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20(3):273–297, 1995. T. G. Dietterich. Approximation statistical tests for comparing supervised classiﬁcation learning algorithms. Neural Computation, 10(7):1895–1923, 1998. N. Garcia-Pedrajas, C. Hervas-Martinez, and D. Ortiz-Boyer. Cooperative coevolution of artiﬁcial neural network ensembles for pattern classiﬁcation. IEEE Trans. on Evolutionary Computation, 9(3):271–302, 2005. N. Garcia-Pedrajas, C. Garcia-Osorio, and C. Fyfe. Nonlinear boosting projections for ensemble construction. Journal of Machine Learning Research, 8:1–33, 2007. K. S. Guimaraes, J. C. B. Melo, and G. D. C. Cavalcanti. Combining few neural networks for effective secondary structure prediction. In Proceedings of the IEEE Symposium on Bioinformatics and BioEngineering (BIBE’03), pages 415–420, 2003. J. V. Hansen. Combining predictors: comparison of ﬁve meta machine learning methods. Information Sciences, 119(1-2):91–105, 1999. B. Happel and J. Murre. Design and evolution of modular neural network architectures. Neural Networks, 7(6-7):985–1004, 1994. S. Haykin. Neural Networks: A Comprehensive Foundation. Englewood Cliffs, NJ: Prentice-Hall, 1999. F. J. Huang and Y. LeCun. Large-scale learning with svm and convolutional for generic object categorization. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR06), volume 1, pages 284–291, 2006. Md. M. Islam, X. Yao, and K. Murase. A constructive algorithm for training cooperative neural network ensembles. IEEE Trans. on Neural Networks, 14(4):820–834, 2003. 620  NEUROSVM: A N A RCHITECTURE TO R EDUCE THE E FFECT OF THE C HOICE OF SVM K ERNEL  R. E. Jenkins and B. P. Yuhas. A simpliﬁed neural network solution through problem decomposition: The case of the truck backer-upper. IEEE Trans. on Neural Networks, 4(4):718–720, 1993. T. Joachims. SVMlight : Support Vector Machine. http://svmlight.joachims.org/., 2002. K. I. Kim, K. Jung, S. H. Park, and H. J. Kim. Support vector machines for texture classiﬁcation. IEEE Trans. Pattern Anal. Machine Intell., 24(11):1542–1550, 2002. A. H. R. Ko, R. Sabourin, A. de Souza Britto Jr., and L. Oliveira. Pairwise fusion matrix for combining classiﬁers. Pattern Recognition, 40(8):2198–2210, 2007. L. I. Kuncheva and C. J. Whitaker. Measures of diversity in classiﬁer ensembles and their relationship with the ensemble accuracy. Machine Learning, 51(2):181–207, 2003. I. Maqsood, M. R. Khan, and A. Abraham. An ensemble of neural networks for weather forecasting. Neural Computing and Applications, 13(2):112–122, 2004. P. Melin, C. Felix, and O. Castillo. Face recognition using modular neural networks and the fuzzy sugeno integral for response integration. International Journal of Intelligent Systems, 20(2):275– 291, 2005. V. Mitra, C-J. Wang, and S. Banerjee. A neuro-svm model for text classiﬁcation using latent semantic indexing. In Proceedings of the International Joint Conference on Neural Networks (IJCNN ’05), volume 1, pages 564–569, 2005. V. Mitra, C-J. Wang, and S. Banerjee. Lidar detection of underwater objects using a neuro-svmbased architecture. IEEE Trans. on Neural Networks, 17(3):717–731, 2006. U. Naftaly, N. Intrator, and D. Horn. Optimal ensemble averaging of neural networks. Network: Computation in Neural Systems, 8(3):283–296, 1997. M. N. Nguyen and J. C. Rajapakse. Multi-class support vector machines for protein secondary structure prediction. Genome Informatics, 14:218–227, 2003. N. R. Pal, S. Pal, J. Das, and K. Majumder. Sofm-mlp: A hybrid neural network for atmospheric temperature prediction. IEEE Trans. Geoscience and Remote Sensing, 41(12):2783–2791, 2003. N. R. Pal, A. Sharma, S. K. Sanadhya, and Karmeshu. On identifying marker genes from gene expression data in a neural framework through online feature analysis. International Journal of Intelligent Systems, 21(4):453–467, 2006. M. Pontil and A. Verri. Support vector machines for 3-d object recognition. IEEE Trans. Pattern Anal. Machine Intell., 20(6):637–646, 1998. L. Prevost, C. Michel-Sendis, L. Oudot A. Moises, and M. Milgram. Combining model-based and discriminative classiﬁers: application to handwritten character recognition. In Proceedings of the Seventh International Conference on Document Analysis and Recognition (ICDAR’03), pages 31–35, 2003. 621  G HANTY, PAUL AND PAL  E. Ronco and P. Gawthrop. Modular Neural Networks: A State of the Art. Technical Report CSC-95026. Centre for System and Control. Faculty of Mechanical Engineering, University of Glasgow, UK, 1995. N. Stepenosky, D. Green, J. Kounios, C. M. Clark, and R. Polikar. Majority vote and decision template based ensemble classiﬁers trained on event related potentials for early diagnosis of alzheimers’s disease. In Proceedings of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, pages 901–904, 2006. E. K. Tang, P. N. Suganthan, and X. Yao. An analysis of diversity measures. Machine Learning, 65 (1):247–271, 2006. V. Vapnik. The Nature of Statistical Learning Theory. New York: Springer-Verlag, 1995. P. Vincent and Y. Bengio. A neural support vector network architecture with adaptive kernels. In Proceedings of the International Joint Conference on Neural Networks (IJCNN 2000), volume 5, pages 187–192, 2000. U. von Luxburg, O. Bousquet, and B. Scholkopf. A compression approach to support vector model selection. Journal of Machine Learning Research, 5:293–323, 2004. J. Weston and C. Watkins. Support vector machines for multi-class pattern recognition. In Proceedings of the Seventh European Symposium On Artiﬁcial Neural Networks, pages 219–224, 1999. T. Windeatt. Accuracy/diversity and ensemble mlp classiﬁer design. IEEE Trans. on Neural Networks, 17(5):1194–1211, 2006. J-X. Wu, Z-H. Zhou, and Z-Q. Chen. Ensemble of ga based selective neural network ensembles. In Proceedings of the 8th International Conference on Neural Information Processing, volume 3, pages 1477–1482, 2001. Z-H. Zhou, J. Wu, and W. Tang. Ensembling neural networks: Many could be better than all. Artiﬁcial Intelligence, 137(1-2):239–263, 2002.  622</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
