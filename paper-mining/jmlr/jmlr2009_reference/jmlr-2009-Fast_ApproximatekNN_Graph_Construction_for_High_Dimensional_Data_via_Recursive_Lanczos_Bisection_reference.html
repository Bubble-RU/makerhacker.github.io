<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 jmlr-2009-Fast ApproximatekNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-34" href="../jmlr2009/jmlr-2009-Fast_ApproximatekNN_Graph_Construction_for_High_Dimensional_Data_via_Recursive_Lanczos_Bisection.html">jmlr2009-34</a> <a title="jmlr-2009-34-reference" href="#">jmlr2009-34-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>34 jmlr-2009-Fast ApproximatekNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</h1>
<br/><p>Source: <a title="jmlr-2009-34-pdf" href="http://jmlr.org/papers/volume10/chen09b/chen09b.pdf">pdf</a></p><p>Author: Jie Chen, Haw-ren Fang, Yousef Saad</p><p>Abstract: Nearest neighbor graphs are widely used in data mining and machine learning. A brute-force method to compute the exact kNN graph takes Θ(dn2 ) time for n data points in the d dimensional Euclidean space. We propose two divide and conquer methods for computing an approximate kNN graph in Θ(dnt ) time for high dimensional data (large d). The exponent t ∈ (1, 2) is an increasing function of an internal parameter α which governs the size of the common region in the divide step. Experiments show that a high quality graph can usually be obtained with small overlaps, that is, for small values of t. A few of the practical details of the algorithms are as follows. First, the divide step uses an inexpensive Lanczos procedure to perform recursive spectral bisection. After each conquer step, an additional reﬁnement step is performed to improve the accuracy of the graph. Finally, a hash table is used to avoid repeating distance calculations during the divide and conquer process. The combination of these techniques is shown to yield quite effective algorithms for building kNN graphs. Keywords: nearest neighbors graph, high dimensional data, divide and conquer, Lanczos algorithm, spectral method</p><br/>
<h2>reference text</h2><p>M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computatioin, 16(6):1373–1396, 2003. J. Bentley. Multidimensional divide-and-conquer. Communications of the ACM, 23:214–229, 1980. J. Bentley, D. Stanat, and E. Williams. The complexity of ﬁnding ﬁxed-radius near neighbors. Information Processing Letters, 6:209–213, 1977. M. W. Berry. Large scale sparse singular value computations. International Journal of Supercomputer Applications, 6(1):13–49, 1992. D. L. Boley. Principal direction divisive partitioning. Data Mining and Knowledge Discovery, 2(4): 324–344, 1998. M. Brito, E. Chávez, A. Quiroz, and J. Yukich. Connectivity of the mutual k-nearest neighbor graph in clustering and outlier detection. Statistics & Probability Letters, 35:33–42, 1997. P. B. Callahan. Optimal parallel all-nearest-neighbors using the well-separated pair decomposition. In Proceedings of the 34th IEEE Symposium on Foundations of Computer Science, 1993. P. B. Callahan and S. Rao Kosaraju. A decomposition of multidimensional point sets with applications to k-nearest-neighbors and n-body potential ﬁelds. Journal of the ACM, 42(1):67–90, 1995. B. Chazelle. An improved algorithm for the ﬁxed-radius neighbor problem. Information Processing Letters, 16:193–198, 1983. H. Choset, K. M. Lynch, S. Hutchinson, G. Kantor, W. Burgard, L. E. Kavraki, and S. Thrun. Principles of Robot Motion: Theory, Algorithms, and Implementations. The MIT Press, 2005. K. L. Clarkson. Fast algorithms for the all nearest neighbors problem. In Proceedings of the 24th Annual IEEE Symposium on the Foundations of Computer Science, pages 226–232, 1983. 2010  FAST kNN G RAPH C ONSTRUCTION FOR H IGH D IMENSIONAL DATA  T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms. The MIT Press, 2nd edition, 2001. B. V. Dasarathy. Nearest-neighbor approaches. In Willi Klosgen, Jan M. Zytkow, and Jan Zyt, editors, Handbook of Data Mining and Knowledge Discovery, pages 88–298. Oxford University Press, 2002. P. Fränti, O. Virmajoki, and V. Hautamäki. Fast agglomerative clustering using a k-nearest neighbor graph. IEEE Transactions on Pattern Analysis & Machine Intelligence, 28(11):1875–1881, 2006. X. He and P. Niyogi. Locality preserving projections. In Advances in Neural Information Processing Systems 16 (NIPS 2004), 2004. P. Indyk. Nearest neighbors in high-dimensional spaces. In J. E. Goodman and J. O’Rourke, editors, Handbook of Discrete and Computational Geometry. CRC Press LLC, 2nd edition, 2004. P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, 1998. F. Juhász and K. Mályusz. Problems of cluster analysis from the viewpoint of numerical analysis, volume 22 of Colloquia Mathematica Societatis Janos Bolyai. North-Holland, Amsterdam, 1980. J. M. Kleinberg. Two algorithms for nearest-neighbor search in high dimensions. In Proceedings of the Twenty-Ninth Annual ACM Symposium on Theory of Computing, 1997. E. Kokiopoulou and Y. Saad. Orthogonal neighborhood preserving projections: A projection-based dimensionality reduction technique. IEEE Transactions on Pattern Analysis & Machine Intelligence, 29(12):2143–2156, 2007. E. Kushilevitza, R. Ostrovsky, and Y. Rabani. Efﬁcient search for approximate nearest neighbor in high dimensional spaces. SIAM Journal on Computing, 30(2):457–474, 2000. C. Lanczos. An iteration method for the solution of the eigenvalue problem of linear differential and integral operators. Journal of Research of the National Bureau of Standards, 45:255–282, 1950. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. K.C. Lee, J. Ho, and D. Kriegman. Acquiring linear subspaces for face recognition under variable lighting. IEEE Transactions on Pattern Analysis & Machine Intelligence, 27(5):684–698, 2005. T. Liu, A. W. Moore, A. Gray, and K. Yang. An investigation of practical approximate nearest neighbor algorithms. In Proceedings of Neural Information Processing Systems (NIPS 2004), 2004. R. Paredes, E. Chávez, K. Figueroa, and G. Navarro. Practical construction of k-nearest neighbor graphs in metric spaces. In Proceedings of the 5th Workshop on Efﬁcient and Experimental Algorithms (WEA’06), 2006. 2011  C HEN , FANG AND S AAD  E. Plaku and L. E. Kavraki. Distributed computation of the knn graph for large high-dimensional point sets. Journal of Parallel and Distributed Computing, 67(3):346–359, 2007. S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323–2326, 2000. Y. Saad. On the rates of convergence of the Lanczos and the block-Lanczos methods. SIAM Journal on Numerical Analysis, 17(5):687–706, 1980. J. Sankaranarayanan, H. Samet, and A. Varshney. A fast all nearest neighbor algorithm for applications involving large point-clouds. Computers and Graphics, 31(2):157–174, 2007. L. K. Saul and S. T. Roweis. Think globally, ﬁt locally: unsupervised learning of low dimensional manifolds. Journal of Machine Learning Research, 4:119–155, 2003. G. Shakhnarovich, T. Darrell, and P. Indyk, editors. Nearest-Neighbor Methods in Learning and Vision: Theory and Practice. The MIT Press, 2006. J. Shanbehzadeh and P. O. Ogunbona. On the computational complexity of the LBG and PNN algorithms. IEEE Transactions on Image Processing, 6(4):614–616, 1997. T. Sim, S. Baker, and M. Bsat. The CMU pose, illumination, and expression database. IEEE Transactions on Pattern Analysis & Machine Intelligence, 25(12):1615–1618, 2003. J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. D. Tritchler, S. Fallah, and J. Beyene. A spectral clustering method for microarray data. Computational Statistics & Data Analysis, 49:63–76, 2005. P. M. Vaidya. An O(n log n) algorithm for the all-nearest-neighbors problem. Discrete Computational Geometry, 4:101–115, 1989. O. Virmajoki and P. Fränti. Divide-and-conquer algorithm for creating neighborhood graph for clustering. In Proceedings of the 17th International Conference on Pattern Recognition (ICPR’04), 2004. J. H. Ward. Hierarchical grouping to optimize an objective function. Journal of the American Statistical Association, 58(301):236–244, 1963. Y. Zhao and G. Karypis. Empirical and theoretical comparisons of selected criterion functions for document clustering. Machine Learning, 55(3):311–331, 2004.  2012</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
