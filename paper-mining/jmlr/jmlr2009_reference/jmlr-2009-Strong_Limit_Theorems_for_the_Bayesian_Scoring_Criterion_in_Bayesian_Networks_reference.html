<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>89 jmlr-2009-Strong Limit Theorems for the Bayesian Scoring Criterion in Bayesian Networks</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-89" href="../jmlr2009/jmlr-2009-Strong_Limit_Theorems_for_the_Bayesian_Scoring_Criterion_in_Bayesian_Networks.html">jmlr2009-89</a> <a title="jmlr-2009-89-reference" href="#">jmlr2009-89-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>89 jmlr-2009-Strong Limit Theorems for the Bayesian Scoring Criterion in Bayesian Networks</h1>
<br/><p>Source: <a title="jmlr-2009-89-pdf" href="http://jmlr.org/papers/volume10/slobodianik09a/slobodianik09a.pdf">pdf</a></p><p>Author: Nikolai Slobodianik, Dmitry Zaporozhets, Neal Madras</p><p>Abstract: In the machine learning community, the Bayesian scoring criterion is widely used for model selection problems. One of the fundamental theoretical properties justifying the usage of the Bayesian scoring criterion is its consistency. In this paper we reﬁne this property for the case of binomial Bayesian network models. As a by-product of our derivations we establish strong consistency and obtain the law of iterated logarithm for the Bayesian scoring criterion. Keywords: Bayesian networks, consistency, scoring criterion, model selection, BIC</p><br/>
<h2>reference text</h2><p>David M. Chickering. Optimal structure identiﬁcation with greedy search. Journal of Machine Learning Research, 3:507–554, 2002. Yuan S. Chow and Henry Teicher. Probability Theory: Independence, Interchangeability, Martingales. Springer-Verlag, New York, 1978. Gregory F. Cooper and Edward Herskovits. A bayesian method for the induction of probabilistic networks from data. Machine Learning, 9:309–347, 1992. Dan Geiger, David Heckerman, Henry King, and Christopher Meek. Stratiﬁed exponential families: Graphical models and model selection. The Annals of Statistics, 29(2):505–529, 2001. Peter D. Gr¨ nwald. The Minimum Description Length Principle (Adaptive Computation and Mau chine Learning). The MIT Press, 2007. Dominique M. A. Haughton. On the choice of a model to ﬁt data from an exponential family. The Annals of Statistics, 16(1):342–355, 1988. Parhasarathi Lahiri. Model selection. Institute of Mathematical Statistics, Beachwood, Ohio, 2001. Richard E. Neapolitan. Learning Bayesian Networks. Prentice Hall, 2004. Guoqi Qian and Chris Field. Law of iterated logarithm and consistent model selection criterion in logistic regression. Statistics and Probability Letters, 56(1):101, 2002. Gideon Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6(2):461–464, 1978. 1525  S LOBODIANIK , Z APOROZHETS AND M ADRAS  Aad W. van der Vaart and Jon A. Wellner. Weak Convergence and Empirical Processes. SpringerVerlag, New York, 1996. Vladimir N. Vapnik. Statistical Learning Theory. John Wiley & Sons, New York, 1998.  1526</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
