<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-63" href="../jmlr2009/jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">jmlr2009-63</a> <a title="jmlr-2009-63-reference" href="#">jmlr2009-63-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</h1>
<br/><p>Source: <a title="jmlr-2009-63-pdf" href="http://jmlr.org/papers/volume10/wang09a/wang09a.pdf">pdf</a></p><p>Author: Junhui Wang, Xiaotong Shen, Wei Pan</p><p>Abstract: In classiﬁcation, semisupervised learning usually involves a large amount of unlabeled data with only a small number of labeled data. This imposes a great challenge in that it is difﬁcult to achieve good classiﬁcation performance through labeled data alone. To leverage unlabeled data for enhancing classiﬁcation, this article introduces a large margin semisupervised learning method within the framework of regularization, based on an efﬁcient margin loss for unlabeled data, which seeks efﬁcient extraction of the information from unlabeled data for estimating the Bayes decision boundary for classiﬁcation. For implementation, an iterative scheme is derived through conditional expectations. Finally, theoretical and numerical analyses are conducted, in addition to an application to gene function prediction. They suggest that the proposed method enables to recover the performance of its supervised counterpart based on complete data in rates of convergence, when possible. Keywords: difference convex programming, classiﬁcation, nonconvex minimization, regularization, support vectors</p><br/>
<h2>reference text</h2><p>S. Abney. Understanding the Yarowsky algorithm. Computat. Linguistics, 30: 365-395, 2004. R. A. Adams. Sobolev Spaces. Academic Press, New York, 1975. M. Amini, and P. Gallinari. Semi-supervised learning with an explicit label-error model for misclassiﬁed data. In IJCAI, 2003. L. An and P. Tao. Solving a class of linearly constrained indeﬁnite quadratic problems by D.C. algorithms. J. of Global Optimization, 11:253-285, 1997. R. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. J. Mach. Learn. Res., 6:1817–1853, 2005. M. Balcan, A. Blum, P. Choi, J. Lafferty, B. Pantano, M. Rwebangira and X. Zhu. Person identiﬁcation in webcam images: an application of semi-supervised learning. In ICML, 2005. C. L. Blake and C. J. Merz. UCI repository of machine learning databases. University of California, Irvine, Department of Information and Computer Science, 1998. A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In Proc. 11th Ann. Conf. on Computat. Learn. Theory, 1998. O. Chapelle, B. Sch¨ lkopf, and A. Zien. Semi-supervised Learning. MIT press, Cambridge, 2006. o O. Chapelle and A. Zien. Semi-supervised classiﬁcation by low density separation. In Proc. Int. Workshop on Artif. Intell. and Statist., pages 57-64, 2005. M. Collins and Y. Singer. Unsupervised models for named entity classiﬁcation. In Proc. Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 100-110, 1999. R. A. Fisher. A system of scoring linkage data, with special reference to the pied factors in mice. Amer. Nat., 80:568-578, 1946. C. Gu. Multidimension smoothing with splines. In, M. G. Shimek, (ed.), Smoothing and Regression: Approaches, Computation and Application, 2000. T. Hughes, M. Marton, A. Jones, C. Roberts, R. Stoughton, C. Armour, H. Bennett, E. Coffey, H. Dai, Y. He, M. Kidd, A. King, M. Meyer, D. Slade, P. Lum, S. Stepaniants, D. Shoemaker, D. Gachotte, K. Chakraburtty, J. Simon, M. Bard and S. Friend. Functional discovery via a compendium of expression proﬁles. Cell, 102:109-126, 2000. 740  E FFICIENT L ARGE M ARGIN S EMISUPERVISED L EARNING  D. Hunter and K. Lange. Quantile regression via an MM algorithm. J. Computat. & Graph. Statist., 9:60-77, 2000. T. Jaakkola, M. Diekhans and D. Haussler. Using the Fisher kernel method to detect remote protein homologies. In Proc. Int. Conf. on Intelligent Systems for Molecular Biology, pages 149-158, 1999. A. N. Kolmogorov and V. M. Tihomirov. ε-entropy and ε-capacity of sets in function spaces. Uspekhi Mat. Nauk., 14:3-86, 1959. [In Russian. English translation, Ameri. Math. Soc. Transl., 14:277-364, 1961. Y. Lin. Support vector machines and the Bayes rule in classiﬁcation. Data Mining and Knowledge Discovery, 6:259-275, 2002. S. Liu, X. Shen and W. Wong. Computational development of ψ-learning. In Proc. SIAM 2005 Int. Data Mining Conf., pages 1-12, 2005. Y. Liu and X. Shen. Multicategory ψ-learning. J. Amer. Statist. Assoc., 101:500-509, 2006. P. Mason, L. Baxter, J. Bartlett and M. Frean. Boosting algorithms as gradient descent. In Advances in Neural Information Processing Systems, 12:512-518. MIT Press, Cambridge, 2000. P. McCullagh and J. Nelder. Generalized Linear Models, 2nd edition. Chapman and Hall/CRC, 1983. H. Mewes, K. Albermann, K. Heumann, S. Liebl and F. Pfeiffer. MIPS: a database for protein sequences, homology data and yeast genome information. Nucleic Acids Res., 25:28-30, 2002. K. Nigam, A. McCallum, S. Thrun and T. Mitchell . Text classiﬁcation from labeled and unlabeled documents using EM. Mach. Learn., 39:103–134, 1998. J. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classiﬁers, pages 61-74, MIT press, Cambridge, 1999. P. Rigollet. Generalization Error Bounds in Semi-supervised Classiﬁcation Under the Cluster Assumption. J. Mach. Learn. Res., 8:1369-1392, 2007. B. Sch¨ lkopf, A. Smola, R. Williamson and P. Bartlett. New support vector algorithms. Neural o Computation, 12:1207-1245, 2000. X. Shen. On method of sieves and penalization. Ann. Statist., 25:2555-2591, 1997. X. Shen, G. C. Tseng, X. Zhang and W. Wong. On psi-learning. J. Amer. Statist. Assoc., 98:724-734, 2003. X. Shen and L. Wang. Generalization error for multi-class margin classiﬁcation. Electronic J. of Statist., 1:307-330, 2007. X. Shen and W. Wong. Convergence rate of sieve estimates. Ann. Statist., 22:580-615, 1994. A. Singh, R. Nowak and X. Zhu. Unlabeled data: Now it helps, now it doesn’t. In NIPS, 2008. 741  WANG , S HEN AND PAN  A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Ann. Statist., 32:135-166, 2004. V. Vapnik. Statistical Learning Theory. Wiley, New York, 1998. G. Wahba. Spline models for observational data. Series in Applied Mathematics, Vol. 59, SIAM, Philadelphia, 1990. G. Wahba. Support vector machines, reproducing kernel Hilbert spaces and the randomized GACV. In Advances in Kernel Methods: Support Vector Learning, edited by B. Schoelkopf, C. Burges and A. Smola, MIT Press, Cambridge, 1998. J. Wang and X. Shen. Large margin semi-supervised learning. J. Mach. Learn. Res., 8:1867-1891, 2007. J. Wang, X. Shen and Y. Liu. Probability estimation for large margin classiﬁers. Biometrika, 95:149167, 2008. J. Wang, X. Shen and W. Pan. On transductive support vector machine. Contemp. Math., 43:7-19, 2007. G. Xiao and W. Pan. Gene function prediction by a combined analysis of gene expression data and protein-protein interaction data. J. Bioinformatics and Computat. Biol., 3:1371-1390, 2005. D. Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189-196, 1995. T. Zhang and F. Oles. A probability analysis on the value of unlabeled data for classiﬁcation problems. In ICML, 2000. X. Zhou, M. Kao and W. Wong. Transitive functional annotation by shortest-path analysis of gene expression data. Proc. Nat. Acad. Sci., 99:12783-12788, 2000. J. Zhu and T. Hastie. Kernel logistic regression and the import vector machine. J. Computat. Graph. Statist., 14:185-205, 2005. X. Zhu, Z. Ghahramani and J. Lafferty. Semi-supervised learning using Gaussian ﬁelds and harmonic functions. In ICML, 2003. X. Zhu. Semi-supervised learning literature survey. Technical Report 1530, University of Wisconsin, Madison, 2005.  742</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
