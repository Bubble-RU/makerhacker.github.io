<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-66" href="../jmlr2009/jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">jmlr2009-66</a> <a title="jmlr-2009-66-reference" href="#">jmlr2009-66-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</h1>
<br/><p>Source: <a title="jmlr-2009-66-pdf" href="http://jmlr.org/papers/volume10/zhang09a/zhang09a.pdf">pdf</a></p><p>Author: Tong Zhang</p><p>Abstract: This paper studies the feature selection problem using a greedy least squares regression algorithm. We show that under a certain irrepresentable condition on the design matrix (but independent of the sparse target), the greedy algorithm can select features consistently when the sample size approaches inﬁnity. The condition is identical to a corresponding condition for Lasso. Moreover, under a sparse eigenvalue condition, the greedy algorithm can reliably identify features as long as each nonzero coefﬁcient is larger than a constant times the noise level. In compar√ ison, Lasso may require the coefﬁcients to be larger than O( s) times the noise level in the worst case, where s is the number of nonzero coefﬁcients. Keywords: greedy algorithm, feature selection, sparsity</p><br/>
<h2>reference text</h2><p>S. Mallat and Z. Zhang. Matching pursuits with time-frequency dictionaries. IEEE Transactions on Signal Processing, 41(12):3397–3415, 1993. Nicolai Meinshausen and Peter Buhlmann. High-dimensional graphs and variable selection with the Lasso. The Annals of Statistics, 34:1436–1462, 2006. Nicolai Meinshausen and Bin Yu. Lasso-type recovery of sparse representations for highdimensional data. Annals of Statistics, 2008. to appear. B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM J. Comput., 24(2):227–234, 1995. ISSN 0097-5397. Joel A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans. Info. Theory, 50(10):2231–2242, 2004. Martin Wainwright. Sharp thresholds for high-dimensional and noisy recovery of sparsity. Technical report, Department of Statistics, UC. Berkeley, 2006. Tong Zhang. Some sharp performance bounds for least squares regression with L1 regularization. The Annals of Statistics, 2009. to appear. Tong Zhang. Forward-backward greedy algorithm for learning sparse representations. Technical report, Rutgers Statistics Department, 2008. A short version is to appear in NIPS 08. Peng Zhao and Bin Yu. On model selection consistency of Lasso. Journal of Machine Learning Research, 7:2541–2567, 2006.  568</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
