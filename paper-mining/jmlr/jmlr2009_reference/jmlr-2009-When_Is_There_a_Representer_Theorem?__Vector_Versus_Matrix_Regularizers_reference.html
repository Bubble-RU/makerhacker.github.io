<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-100" href="../jmlr2009/jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">jmlr2009-100</a> <a title="jmlr-2009-100-reference" href="#">jmlr2009-100-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</h1>
<br/><p>Source: <a title="jmlr-2009-100-pdf" href="http://jmlr.org/papers/volume10/argyriou09a/argyriou09a.pdf">pdf</a></p><p>Author: Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the L2 norm, then the learned vector is a linear combination of the input data. This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, in the case of differentiable regularizers. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufﬁcient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions. Keywords: kernel methods, matrix learning, minimal norm interpolation, multi-task learning, regularization</p><br/>
<h2>reference text</h2><p>D. A. Aaker, V. Kumar, and G. S. Day. Marketing Research. John Wiley & Sons, 2004. 8th edition. J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. A new approach to collaborative ﬁltering: operator estimation with spectral regularization. Journal of Machine Learning Research, 10:803–826, 2009. Y. Amit, M. Fink, N. Srebro, and S. Ullman. Uncovering shared structures in multiclass classiﬁcation. In Proceedings of the Twenty-Fourth International Conference on Machine Learning, 2007. R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6:1817–1853, 2005. A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In B. Sch¨ lkopf, J. Platt, and o T. Hoffman, editors, Advances in Neural Information Processing Systems 19. MIT Press, 2006. A. Argyriou, C. A. Micchelli, M. Pontil, and Y. Ying. A spectral regularization framework for multi-task structure learning. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20. MIT Press, 2007. A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243–272, 2008a. Special Issue on Inductive Transfer Learning; Guest Editors: D. L. Silver and K. P. Bennett. A. Argyriou, A. Maurer, and M. Pontil. An algorithm for transfer learning in a heterogeneous environment. In Proceedings of the European Conference on Machine Learning, volume 5211 of Lecture Notes in Computer Science, pages 71–85. Springer, 2008b. N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society, 686:337–404, 1950. B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classiﬁers. In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, pages 144–152. ACM, 1992. E. J. Cand` s and B. Recht. Exact matrix completion via convex optimization. In Foundations of e Computational Mathematics, 2009. To appear. G. Cavallanti, N. Cesa-Bianchi, and C. Gentile. Linear algorithms for online multitask classiﬁcation. In Proceedings of the Twenty-First Annual Conference on Learning Theory, 2008. F. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin of the American Mathematical Society, 39(1):1–49, 2001. E. De Vito, L. Rosasco, A. Caponnetto, M. Piana, and A. Verri. Some properties of regularized kernel methods. Journal of Machine Learning Research, 5:1363–1390, 2004. F. Dinuzzo, M. Neve, G. De Nicolao, and U. P. Gianazza. On the representer theorem and equivalent degrees of freedom of SVR. Journal of Machine Learning Research, 8:2467–2495, 2007. 2527  A RGYRIOU , M ICCHELLI AND P ONTIL  T. Evgeniou, M. Pontil, and T. Poggio. Regularization networks and support vector machines. Advances in Computational Mathematics, 13(1):1–50, 2000. T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. Journal of Machine Learning Research, 6:615–637, 2005. M. Fazel, H. Hindi, and S. P. Boyd. A rank minimization heuristic with application to minimum order system approximation. In Proceedings, American Control Conference, volume 6, pages 4734–4739, 2001. F. Girosi, M. Jones, and T. Poggio. Regularization theory and neural networks architectures. Neural Computation, 7(2):219–269, 1995. A. E. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 42:80–86, 1970. R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 1985. R. A. Horn and C. R. Johnson. Topics in Matrix Analysis. Cambridge University Press, 1991. A. J. Izenman. Reduced-rank regression for the multivariate linear model. Journal of Multivariate Analysis, 5:248–264, 1975. G.S. Kimeldorf and G. Wahba. A correspondence between Bayesian estimation on stochastic processes and smoothing by splines. The Annals of Mathematical Statistics, 41(2):495–502, 1970. P. J. Lenk, W. S. DeSarbo, P. E. Green, and M. R. Young. Hierarchical Bayes conjoint analysis: recovery of partworth heterogeneity from reduced experimental designs. Marketing Science, 15 (2):173–191, 1996. A. Maurer. Bounds for linear multi-task learning. Journal of Machine Learning Research, 7:117– 139, 2006a. A. Maurer. The Rademacher complexity of linear transformation classes. In Proceedings of the Nineteenth Annual Conference on Learning Theory, volume 4005 of LNAI, pages 65–78. Springer, 2006b. C. A. Micchelli and A. Pinkus. Variational problems arising from balancing several error criteria. Rendiconti di Matematica, Serie VII, 14:37–86, 1994. C. A. Micchelli and M. Pontil. A function representation for learning in Banach spaces. In Proceedings of the Nineteenth Annual Conference on Learning Theory, volume 3120 of Lecture Notes in Computer Science, pages 255–269. Springer, 2004. C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine Learning Research, 6:1099–1125, 2005a. C. A. Micchelli and M. Pontil. On learning vector–valued functions. Neural Computation, 17: 177–204, 2005b. 2528  W HEN I S T HERE A R EPRESENTER T HEOREM ? V ECTOR VERSUS M ATRIX R EGULARIZERS  C. A. Micchelli and T. J. Rivlin. Lectures on optimal recovery. In P. R. Turner, editor, Lecture Notes in Mathematics, volume 1129. Springer Verlag, 1985. R. Raina, A. Y. Ng, and D. Koller. Constructing informative priors using transfer learning. In Proceedings of the Twenty-Third International Conference on Machine Learning, 2006. B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed minimum rank solutions to linear matrix equations via nuclear norm minimization. Preprint, 2008. B. Sch¨ lkopf and A. J. Smola. Learning with Kernels. The MIT Press, 2002. o B. Sch¨ lkopf, R. Herbrich, and A.J. Smola. A generalized representer theorem. In Proceedings of o the Fourteenth Annual Conference on Computational Learning Theory, 2001. J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004. N. Srebro, J. D. M. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. In Advances in Neural Information Processing Systems 17, pages 1329–1336. MIT Press, 2005. I. Steinwart. Sparseness of support vector machines. Journal of Machine Learning Research, 4: 1071–1105, 2003. A. N. Tikhonov and V. Y. Arsenin. Solutions of Ill Posed Problems. V. H. Winston and Sons (distributed by Wiley), 1977. F. John, Translation Editor. L. Vandenberghe and S. Boyd. Semideﬁnite programming. SIAM Review, 38(1):49–95, 1996. V. N. Vapnik. The Nature of Statistical Learning Theory. Springer, 2000. G. Wahba. Spline Models for Observational Data, volume 59 of Series in Applied Mathematics. SIAM, Philadelphia, 1990. G. Wahba. Multivariate function and operator estimation, based on smoothing splines and reproducing kernels. In Nonlinear Modeling and Forecasting, SFI Studies in the Sciences of Complexity, volume XII. Addison-Wesley, 1992. L. Wolf, H. Jhuang, and T. Hazan. Modeling appearances with low-rank SVM. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–6, 2007. Z. Xiang and K. P. Bennett. Inductive transfer using kernel multitask latent analysis. In Inductive Transfer : 10 Years Later, NIPS Workshop, 2005. T. Xiong, J. Bi, B. Rao, and V. Cherkassky. Probabilistic joint feature selection for multi-task learning. In Proceedings of SIAM International Conference on Data Mining, 2006. M. Yuan, A. Ekici, Z. Lu, and R. Monteiro. Dimension reduction and coefﬁcient estimation in multivariate linear regression. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(3):329–346, 2007.  2529</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
