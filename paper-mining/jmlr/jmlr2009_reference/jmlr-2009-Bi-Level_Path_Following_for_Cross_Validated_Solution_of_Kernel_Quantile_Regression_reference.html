<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-12" href="../jmlr2009/jmlr-2009-Bi-Level_Path_Following_for_Cross_Validated_Solution_of_Kernel_Quantile_Regression.html">jmlr2009-12</a> <a title="jmlr-2009-12-reference" href="#">jmlr2009-12-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>12 jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</h1>
<br/><p>Source: <a title="jmlr-2009-12-pdf" href="http://jmlr.org/papers/volume10/rosset09a/rosset09a.pdf">pdf</a></p><p>Author: Saharon Rosset</p><p>Abstract: We show how to follow the path of cross validated solutions to families of regularized optimization problems, deﬁned by a combination of a parameterized loss function and a regularization term. A primary example is kernel quantile regression, where the parameter of the loss function is the quantile being estimated. Even though the bi-level optimization problem we encounter for every quantile is non-convex, the manner in which the optimal cross-validated solution evolves with the parameter of the loss function allows tracking of this solution. We prove this property, construct the resulting algorithm, and demonstrate it on real and artiﬁcial data. This algorithm allows us to efﬁciently solve the whole family of bi-level problems. We show how it can be extended to cover other modeling problems, like support vector regression, and alternative in-sample model selection approaches.1</p><br/>
<h2>reference text</h2><p>M. Buchinsky. Changes in the u.s. wage structure 1963-1987: Application of quantile regression. Econometrica, 62(2):405–458, Mar. 1994. A. Christmann and I. Steinwart. Consistency of kernel-based quantile regression. Applied Stochastic Models in Business and Industry, 24(2):171–183, 2008. ISSN 1524-1904. B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32 (2):407–499, 2004. E. Eide and M.H. Showalter. The effect of school quality on student performance: A quantile regression approach. Economics Letters, 58(3):345–350, Mar. 1998. L. Gunther and J. Zhu. Efﬁcient computation and model selection for the support vector regression. Neural Computation, 19(6), 2005. T. Hastie, R. Tibshirani, and J. Friedman. Elements of Statistical Learning. Springer, 2001. T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. The entire regularization path of the support vector machine. JMLR, 5:1391–1415, Oct 2004. X. He, P. Ng, and S. Portnoy. Bivariate quantile smoothing splines. Journal of the Royal Statistical Society, Ser. B, 60:537–550, 1998. G. Kimeldorf and G. Wahba. Some results on chebychefﬁan spline functions. Journal of Mathematical Analysis and Applications, 33:82–95, 1971. R. Koenker. Quantile Regression. New York : Cambridge University Press, 2005. G. Kunapuli, K.P. Bennett, J. Hu, and J.-S. Pang. Bilevel model selection for support vector machines. In Pierre Hansen and Panos Pardalos, editors, Data Mining and Mathematical Programming [CRM Proceedings and Lecture Notes], volume 45. American Mathematical Society, 2008. Y. Li, Y. Liu, and J. Zhu. Quantile regression in reproducing kernel hilbert spaces. JASA, 102(477), 2007. D. Mease, A.J. Wyner, and A. Buja. Boosted classiﬁcation trees and class probability/quantile estimation. JMLR, 8:409–439, Oct 2007. N. Meinshausen. Quantile regression forests. JMLR, 7:983–999, Jun 2006. R. K. Pace and R. Barry. Sparse spatial autoregressions. Statistics and Probability Letters, 33: 291–297, 1997. C. Perlich, S. Rosset, R. Lawrence, and B. Zadrozny. High quantile modeling for customer wallet estimation with other applications. In Proceedings of the Twelfth International Conference on Data Mining, KDD-07, 2007. S. Rosset. Bi-level path following for cross validated solution of kernel quantile regression. In Proceedings of the 25th international conference on Machine Learning, 2008. 2504  B I -L EVEL PATH F OLLOWING  S. Rosset and J. Zhu. Piecewise linear regularized solution paths. Annals of Statistics, 35(3), 2007. B. Sch¨ lkopf and A. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002. o G. Schwarz. Estimating the dimension of a model. Annals of Statistics, 6:461–464, 1978. M. Sharir and P. K. Agarwal. Davenport-Schinzel Sequences and their Geometric Applications. Cambridge University Press, 1995. J. Sherman and W.J. Morrison. Adjustment of an inverse matrix corresponding to changes in the elements of a given column or a given row of the original matrix. Annals of Mathematical Statistics, 20:621, 1949. A.J. Smola and B. Sch¨ lkopf. A tutorial on support vector regression. Statistics and Computing, o 14:199–222, 2004. I. Steinwart and A. Christmann. How SVMs can estimate quantiles and the median. In Neural Information Processing Systems 20, pages 305–312, 2008. I. Takeuchi, Q.V. Le, T.D. Sears, and A.J. Smola. Nonparametric quantile estimation. JMLR, 7: 1231–1264, Jul 2006. I. Takeuchi, K. Nomura, and T. Kanamori. Nonparametric conditional density estimation using piecewise-linear solution path of kernel quantile regression. Neural Computation, 21(2):533– 559, 2009. G. Wang, D.-Y. Yeung, and F. H. Lochovsky. Two-dimensional solution path for support vector regression. In Proceedings of the 23rd international conference on Machine learning, pages 993–1000, 2006. J. Wang, X. Shen, and Y. Liu. Probability estimation for large margin classiﬁers. Biometrika, 95 (1):149–167, 2008. M. Yuan. GACV for quantile smoothing splines. Computational Statistics and Data Analysis, 5: 813–829, 2006. H. Zou, T. Hastie, and R. Tibshirani. On the degrees of freedom of the Lasso. Annals of Statistics, 35:2173–2192, 2007.  2505</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
