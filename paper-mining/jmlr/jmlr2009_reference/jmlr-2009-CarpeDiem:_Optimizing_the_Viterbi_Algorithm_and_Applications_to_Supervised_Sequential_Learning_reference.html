<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 jmlr-2009-CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-14" href="../jmlr2009/jmlr-2009-CarpeDiem%3A_Optimizing_the_Viterbi_Algorithm_and_Applications_to_Supervised_Sequential_Learning.html">jmlr2009-14</a> <a title="jmlr-2009-14-reference" href="#">jmlr2009-14-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>14 jmlr-2009-CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning</h1>
<br/><p>Source: <a title="jmlr-2009-14-pdf" href="http://jmlr.org/papers/volume10/esposito09a/esposito09a.pdf">pdf</a></p><p>Author: Roberto Esposito, Daniele P. Radicioni</p><p>Abstract: The growth of information available to learning systems and the increasing complexity of learning tasks determine the need for devising algorithms that scale well with respect to all learning parameters. In the context of supervised sequential learning, the Viterbi algorithm plays a fundamental role, by allowing the evaluation of the best (most probable) sequence of labels with a time complexity linear in the number of time events, and quadratic in the number of labels. In this paper we propose CarpeDiem, a novel algorithm allowing the evaluation of the best possible sequence of labels with a sub-quadratic time complexity.1 We provide theoretical grounding together with solid empirical results supporting two chief facts. CarpeDiem always ﬁnds the optimal solution requiring, in most cases, only a small fraction of the time taken by the Viterbi algorithm; meantime, CarpeDiem is never asymptotically worse than the Viterbi algorithm, thus conﬁrming it as a sound replacement. Keywords: Viterbi algorithm, sequence labeling, conditional models, classiﬁers optimization, exact inference</p><br/>
<h2>reference text</h2><p>Steve Austin, Pat Peterson, Paul Placeway, Richard Schwartz, and Jeff Vandergrift. Toward a realtime spoken language system using commercial hardware. In HLT ’90: Proceedings of the Workshop on Speech and Natural Language, pages 72–77, Hidden Valley, Pennsylvania, 1990. John S. Bridle, Michael D. Brown, and Richard M. Chamberlain. An algorithm for connected word recognition. In Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP 1982, pages 899–902, San Francisco, CA, USA, 1982. Michael Collins. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1–8, Philadelphia, PA, 2002. Association for Computational Linguistics. Michael Collins and Brian Roark. Incremental parsing with the perceptron algorithm. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 111–118, Barcelona, Spain, 2004. Association for Computational Linguistics. Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. Introduction to Algorithms. MIT Press, Cambridge, Massachussets, 1990. Thomas G. Dietterich. Machine learning for sequential data: A review. In T. Caelli, editor, Structural, Syntactic, and Statistical Pattern Recognition, volume 2396 of Lecture Notes in Computer Science, pages 15–30, Windsor, Ontario, Canada, 2002. Springer-Verlag. Thomas G. Dietterich, Pedro Domingos, Lise Getoor, Stephen Muggleton, and Prasad Tadepalli. Structured machine learning: The next ten years. Machine Learning, 73(1):3–23, 2008. Robert M. Fano. A heuristic discussion of probabilistic decoding. IEEE Transactions on Information Theory, 9:64–73, 1963. Pedro F. Felzenszwalb, Daniel P. Huttenlocher, and Jon M. Kleinberg. Fast algorithms for largestate-space HMMs with applications to web usage analysis. In Advances in Neural Information Processing Systems. MIT Press, 2003. Peter W. Frey and David J. Slate. Letter recognition using Holland-style adaptive classiﬁers. Machine Learning, 6(2):161–182, 1991. Peter E. Hart, Nils J. Nilsson, and Bertram Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions of Systems Science and Cybernetics, 4:100–107, 1968. John Lafferty, Andrew McCallum, and Fernando Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289, San Francisco, CA, 2001. Morgan Kaufmann. Bruce Lowerre and Raj Reddy. Trends in Speech Recognition, chapter The Harpy Speech Understanding System, pages 340–360. Prentice-Hall, Englewood Cliffs, New Jersey, 1980. 1879  E SPOSITO AND R ADICIONI  Andrew McCallum, Dayne Freitag, and Fernando Pereira. Maximum entropy Markov models for information extraction and segmentation. In Proceedings of the 17th International Conference on Machine Learning, pages 591–598, Stanford, California, USA, 2000. Morgan Kaufmann. Shay Mozes, Oren Weimann, and Michal Ziv-Ukelson. Speeding up HMM decoding and training by exploiting sequence repetitions. Combinatorial Pattern Matching, 4580:4–15, 2007. Hermann Ney, Dieter Mergel, Andreas Noll, and Annedore Paeseler. Data driven search organization for continuous speech recognition. IEEE Transactions on Signal Processing, 40:272–281, 1987. Hermann Ney, Reinhold Haeb-Umbach, Bach-Hiep Tran, and Martin Oerder. Improvements in beam search for 10000-word continuous speech recognition. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 9–12, San Francisco, California, USA, 1992. Bryan Pardo and William P. Birmingham. Algorithms for chordal analysis. Computer Music Journal, 26:27–49, 2002. Lawrence R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. In Proceedings of the IEEE, volume 77, pages 267–296, San Francisco, CA, USA, February 1989. Morgan Kaufmann. ISBN 1-55860-124-4. Daniele P. Radicioni and Roberto Esposito. Tonal harmony analysis: a supervised sequential learning approach. In M. T. Pazienza and R. Basili, editors, AI*IA 2007: Artiﬁcial Intelligence and Human-Oriented Computing, 10th Congress of the Italian Association for Artiﬁcial Intelligence. Springer-Verlag, 2007. Sajid M. Siddiqi and Andrew W. Moore. Fast inference and learning in large-state-space HMMs. In Proceedings of the 22nd International Conference on Machine Learning, pages 800–807, Bonn, Germany, 2005. ACM. James C. Spohrer, Peter F. Brown, Peter H. Hochschild, and James K. Baker. Partial traceback in continuous speech recognition. In Proceedings of the IEEE Interntational Conference on Cybernetics and Society, pages 36–42, Boston, Massachussets, USA, 1980. Charles Sutton, Andrew McCallum, and Khashayar Rohanimanesh. Dynamic conditional random ﬁelds: factorized probabilistic models for labeling and segmenting sequence data. Journal of Machine Learning Research, 8:693–723, 2007. David Temperley. The Cognition of Basic Musical Structures. MIT, Cambridge, MASS, 2001. Andrew J. Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. In IEEE Transaction on Information Theory, volume 13, pages 260–269, 1967. Yuehua Xu and Alan Fern. On learning linear ranking functions for beam search. In Z. Ghahramani, editor, Proceedings of the 24th International Conference on Machine Learning, pages 1047– 1054, Corvallis, Oregon, USA, 2007. ACM.  1880</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
