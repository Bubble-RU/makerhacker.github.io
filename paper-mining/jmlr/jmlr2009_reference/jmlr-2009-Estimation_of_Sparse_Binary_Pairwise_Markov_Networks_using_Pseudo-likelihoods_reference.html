<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 jmlr-2009-Estimation of Sparse Binary Pairwise Markov Networks using Pseudo-likelihoods</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-30" href="../jmlr2009/jmlr-2009-Estimation_of_Sparse_Binary_Pairwise_Markov_Networks_using_Pseudo-likelihoods.html">jmlr2009-30</a> <a title="jmlr-2009-30-reference" href="#">jmlr2009-30-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>30 jmlr-2009-Estimation of Sparse Binary Pairwise Markov Networks using Pseudo-likelihoods</h1>
<br/><p>Source: <a title="jmlr-2009-30-pdf" href="http://jmlr.org/papers/volume10/hoefling09a/hoefling09a.pdf">pdf</a></p><p>Author: Holger Höfling, Robert Tibshirani</p><p>Abstract: We consider the problems of estimating the parameters as well as the structure of binary-valued Markov networks. For maximizing the penalized log-likelihood, we implement an approximate procedure based on the pseudo-likelihood of Besag (1975) and generalize it to a fast exact algorithm. The exact algorithm starts with the pseudo-likelihood solution and then adjusts the pseudolikelihood criterion so that each additional iterations moves it closer to the exact solution. Our results show that this procedure is faster than the competing exact method proposed by Lee, Ganapathi, and Koller (2006a). However, we also ﬁnd that the approximate pseudo-likelihood as well as the approaches of Wainwright et al. (2006), when implemented using the coordinate descent procedure of Friedman, Hastie, and Tibshirani (2008b), are much faster than the exact methods, and only slightly less accurate. Keywords: Markov networks, logistic regression, L1 penalty, model selection, Binary variables</p><br/>
<h2>reference text</h2><p>O. Banerjee, L. El Ghaoui, and A. d’Aspremont. Model selection through sparse maximum likelihood estimation. Journal of Machine Learning Research, 9:485–516, 2008. J. Besag. Statistical analysis of non-lattice data. In Proceedings of the Twenty-First National Conference on Artiﬁcial Intelligence (AAAI-06), volume 24, pages 179–195, 1975. D. R. Cox and N. Reid. A note on pseudolikelihood constructed from marginal densities. Biometrika, 91:729–737, 2004. J. Dahl, L. Vandenberghe, and V. Roychowdhury. Covariance selection for non-chordal graphs via chordal embedding. Optimization Methods and Software, 2008. J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9:432–441, 2008a. J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via coordinate descent. Technical Report, Stanford University, 2008b. Submitted. G. E. Hinton. Learning multiple layers of representation. Trends in Cognitive Sciences, 11:428–434, 2007. S.-I. Lee, V. Ganapathi, and D. Koller. Efﬁcient structure learning of Markov networks using L1 regularization. In Advances in Neural Information Processing Systems (NIPS 2006), 2006a. S.-I. Lee, H. Lee, P. Abbeel, and A.Y. Ng. Efﬁcient L1 regularized logistic regression. In Proceedings of the Twenty-First National Conference on Artiﬁcial Intelligence (AAAI-06), 2006b. B. Lindsay. Composite likelihood methods. In Contemporary Mathemtics, volume 80, pages 221– 239, 1998. N. Meinshausen and P. B¨ hlmann. High dimensional graphs and variable selection with the lasso. u Annals of Statistics, 34:1436–1462, 2006. S. Perkins, K. Lacker, and j. Theiler. Grafting: Fast, incremental feature selection by gradient descent in function space. Journal of Machine Learning Research, 3:1333–1356, 2003. M. J. Wainwright, P. Ravikumar, and J. Lafferty. High-dimensional graphical model selection using L1 -regularized logistic regression. In Advances in Neural Information Processing Systems, Vancouver, 2006. M. J. Wainwright, P. Ravikumar, and J. Lafferty. High-dimensional graphical model selection using L1 -regularized logistic regression. Technical report, University of California, Berkeley, April 2008. M. Yuan and Y. Lin. Model selection and estimation in the gaussian graphical model. Biometrika, 94(1):19–35, 2007.  906</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
