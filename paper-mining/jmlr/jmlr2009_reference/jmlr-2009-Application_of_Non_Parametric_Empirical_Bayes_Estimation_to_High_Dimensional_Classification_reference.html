<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-10" href="../jmlr2009/jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">jmlr2009-10</a> <a title="jmlr-2009-10-reference" href="#">jmlr2009-10-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</h1>
<br/><p>Source: <a title="jmlr-2009-10-pdf" href="http://jmlr.org/papers/volume10/greenshtein09a/greenshtein09a.pdf">pdf</a></p><p>Author: Eitan Greenshtein, Junyong Park</p><p>Abstract: We consider the problem of classiﬁcation using high dimensional features’ space. In a paper by Bickel and Levina (2004), it is recommended to use naive-Bayes classiﬁers, that is, to treat the features as if they are statistically independent. Consider now a sparse setup, where only a few of the features are informative for classiﬁcation. Fan and Fan (2008), suggested a variable selection and classiﬁcation method, called FAIR. The FAIR method improves the design of naive-Bayes classiﬁers in sparse setups. The improvement is due to reducing the noise in estimating the features’ means. This reduction is since that only the means of a few selected variables should be estimated. We also consider the design of naive Bayes classiﬁers. We show that a good alternative to variable selection is estimation of the means through a certain non parametric empirical Bayes procedure. In sparse setups the empirical Bayes implicitly performs an efﬁcient variable selection. It also adapts very well to non sparse setups, and has the advantage of making use of the information from many “weakly informative” variables, which variable selection type of classiﬁcation procedures give up on using. We compare our method with FAIR and other classiﬁcation methods in simulation for sparse and non sparse setups, and in real data examples involving classiﬁcation of normal versus malignant tissues based on microarray data. Keywords: non parametric empirical Bayes, high dimension, classiﬁcation</p><br/>
<h2>reference text</h2><p>P.J. Bickel and E. Levina. Some theory for Fisher’s linear discriminant function, ”naive Bayes”, and some alternatives where there are many more variables than observations. Bernoulli, 10(6):9891010, 2004. L.D. Brown. Admissible estimators, recurrent diffusions, and insoluble boundary value problems. The Annals of Mathematical Statistics, 42(3):855-903, 1971. L.D. Brown. In-season prediction of batting averages: a ﬁeld-test of simple empirical Bayes and Bayes methodologies. Annals of Applied Statistics, 2(1):113-152, 2008. L.D. Brown and E. Greenshtein. Nonparametric empirical Bayes and compound decision approaches to estimation of a high-dimensional vector of means. Annals of Statistics, 37(4):16851704, 2009. J.B. Copas. Compound decisions and empirical Bayes. Journal of the Royal Statistical Society Series B(Methodological), 31(3):397-425, 1969. B. Efron. Empirical Bayes estimates for large-scale prediction problems. Journal of the American Statistical Association, forthcoming. J. Fan and Y. Fan. High dimensional classiﬁcation using features annealed independence rules. Annals of Statistics, 36(6):2605-2637, 2008. E. Greenshtein, J. Park, and G. Lebannon. Regularization through variable selection and conditional mle with application to classiﬁcation in high dimensions. Journal of Statistical Planning and Inference, 139(2):385-395, 2009. E. Greenshtein and Y. Ritov. Asymptotic efﬁciency of simple decisions for the compound decision problem. The Third Lehmann Symposium, IMS Lecture Notes Monograph Series, forthcoming. T.R. Golub, D.K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J.P. Mesirov, H. Coller, M.L. Loh, J.R. Downing, M.A. Caligiuri, C.D. Bloomﬁeld, and E.S. Lander. Molecular classiﬁcation of cancer: class discovery and class prediction by gene expression monitoring. Science, 286:531537, 1999. G.J. Gordon, R.V. Jensen, L.L. Hsiao, S.R. Gullans, J.E. Blumenstock, S. Ramaswamy, W.G. Richards, D.J. Sugarbaker, and R. Bueno. Translation of microarray data into clinically relevant cancer diagnostic tests using gene expression ratios in lung cancer and mesothelioma. Cancer Res, 62(17):4963-4967, 2002. W. Jiang and C.H. Zhang. General maximum likelihood empirical Bayes estimation of normal means. Annals of Statistics, 37(4):1647-1684, 2009. E.L. Lehmann. Testing Statistical Hypothesis. Wiley, 1986. H. Robbins. Asymptotically subminimax solutions of compound decision problems. In Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, pages 131-148, Berkeley, California, 1951. 1703  G REENSHTEIN AND PARK  D. Singh, P.G. Febbo, K. Ross, D.G. Jackson, J. Manola, C. Ladd, P. Tamayo, A.A. Renshaw, A.V. D’Amico, J.P. Richie, E.S. Lander, M. Loda, P.W. Kantoff, T.R. Golub, W.R. Sellers. Gene expression correlates of clinical prostate cancer behavior. Cancer Cell, 1(2):203-209, 2002. R. Tibshirani, T. Hastie, B. Narasimhan, and G. Chu. Diagnosis of multiple cancer types by shrunken centroids of gene expression. In Proceedings of National Academy of Sciences, 99(1):6567-6572, 2002. C.H. Zhang. Compound decision theory and empirical Bayes methods. Annals of Statistics, 31(2):379-390, 2003.  1704</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
