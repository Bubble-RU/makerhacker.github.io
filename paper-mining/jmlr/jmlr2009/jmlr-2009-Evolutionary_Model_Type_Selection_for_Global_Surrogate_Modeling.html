<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-31" href="#">jmlr2009-31</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</h1>
<br/><p>Source: <a title="jmlr-2009-31-pdf" href="http://jmlr.org/papers/volume10/gorissen09a/gorissen09a.pdf">pdf</a></p><p>Author: Dirk Gorissen, Tom Dhaene, Filip De Turck</p><p>Abstract: Due to the scale and computational complexity of currently used simulation codes, global surrogate (metamodels) models have become indispensable tools for exploring and understanding the design space. Due to their compact formulation they are cheap to evaluate and thus readily facilitate visualization, design space exploration, rapid prototyping, and sensitivity analysis. They can also be used as accurate building blocks in design packages or larger simulation environments. Consequently, there is great interest in techniques that facilitate the construction of such approximation models while minimizing the computational cost and maximizing model accuracy. Many surrogate model types exist (Support Vector Machines, Kriging, Neural Networks, etc.) but no type is optimal in all circumstances. Nor is there any hard theory available that can help make this choice. In this paper we present an automatic approach to the model type selection problem. We describe an adaptive global surrogate modeling environment with adaptive sampling, driven by speciated evolution. Different model types are evolved cooperatively using a Genetic Algorithm (heterogeneous evolution) and compete to approximate the iteratively selected data. In this way the optimal model type and complexity for a given data set or simulation code can be dynamically determined. Its utility and performance is demonstrated on a number of problems where it outperforms traditional sequential execution of each model type. Keywords: model type selection, genetic algorithms, global surrogate modeling, function approximation, active learning, adaptive sampling</p><p>Reference: <a title="jmlr-2009-31-reference" href="../jmlr2009_reference/jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Many surrogate model types exist (Support Vector Machines, Kriging, Neural Networks, etc. [sent-11, score-0.276]
</p><p>2 We describe an adaptive global surrogate modeling environment with adaptive sampling, driven by speciated evolution. [sent-15, score-0.364]
</p><p>3 Keywords: model type selection, genetic algorithms, global surrogate modeling, function approximation, active learning, adaptive sampling  1. [sent-19, score-0.588]
</p><p>4 G ORISSEN , D HAENE AND D E T URCK  centrates on the use of data-driven, global approximations using compact surrogate models (also known as emulators, metamodels or response surface models) in the context of computer experiments. [sent-33, score-0.302]
</p><p>5 Once constructed, the global surrogate model (also referred to as a replacement metamodel1 ) is reused in other stages of the computational science and engineering pipeline. [sent-35, score-0.268]
</p><p>6 The primary users of global surrogate modeling methods are domain experts, few of which will be experts in the intricacies of efﬁcient sampling and modeling strategies. [sent-37, score-0.395]
</p><p>7 An evolutionary algorithm is presented that combines automatic model type selection, automatic model parameter optimization, and sequential design exploration. [sent-43, score-0.462]
</p><p>8 In the next Section we describe the problem of global surrogate modeling followed by an in depth discussion of the motivation for this work in Section 3. [sent-44, score-0.29]
</p><p>9 Section 7 describes a number of surrogate modeling problems we shall use to demonstrate the proposed approach, followed by their discussion in Section 10 (the experimental setup is described in Section 9). [sent-46, score-0.259]
</p><p>10 This means that the global surrogate model generation problem (i. [sent-58, score-0.323]
</p><p>11 The terms surrogate model and metamodel are used interchangeably. [sent-62, score-0.275]
</p><p>12 Motivation While the mathematical formulation of global surrogate modeling is clear cut, its practical implementation raises an obvious question: How should the minimization over t ∈ T and θ ∈ Θ in Equation 1 be performed? [sent-104, score-0.29]
</p><p>13 Many model types exist: rational functions, Artiﬁcial Neural Networks (ANN), Support Vector Machines (SVM), Gaussian Process (GP) models, Multivariate Adaptive Regression Splines (MARS), Radial Basis Function (RBF) models, projection pursuit regression, rational functions, etc. [sent-108, score-0.424]
</p><p>14 For example, rational functions are popular for all kinds of Linear Time-Invariant systems since theory is available that can be used to prove that rational pole-residue models conserve certain physical quantities (Triverio et al. [sent-144, score-0.38]
</p><p>15 A main reason for turning towards global surrogate modeling methods is that little is known about the behavior of the response (Simpson et al. [sent-172, score-0.329]
</p><p>16 We describe the application of a single GA with speciation to both problems: the selection of the surrogate type and the optimization of the surrogate model parameters (= hyperparameter optimization). [sent-197, score-0.682]
</p><p>17 The idea is to maintain a heterogeneous population of surrogate model types and let them evolve cooperatively and dynamically with the changing data distribution. [sent-199, score-0.493]
</p><p>18 Related Work The evolutionary generation of regression models for given input-output data has been widely studied in the genetic programming community (Vladislavleva et al. [sent-207, score-0.406]
</p><p>19 However, such an approach will still require an a priori model type selection and does not allow for dynamic switching of the model type or the generation of hybrids. [sent-232, score-0.316]
</p><p>20 There has also been much research on the use of surrogate models in evolutionary optimization of expensive simulators (to approximate the ﬁtness function). [sent-233, score-0.46]
</p><p>21 (2007) compare the utility of different local surrogate modeling techniques (quadratic polynomials, GP, RBF, . [sent-244, score-0.259]
</p><p>22 (2007) are interested in the optimum and not the surrogate itself (they make only a “mild assumption on the accuracy of the metamodeling technique”). [sent-250, score-0.252]
</p><p>23 In contrast we place very strong emphasis on the surrogate model accuracy, the automatic setting of the hyperparameters, and the efﬁcient sampling of the complete design space. [sent-252, score-0.364]
</p><p>24 For example, instead of returning the single ﬁnal best model, an optimal ensemble member selection algorithm can be used to return a potentially much better model based on the ﬁnal population or Pareto front. [sent-258, score-0.409]
</p><p>25 The authors describe an interesting classiﬁcation algorithm COMB that combines online an ensemble of active learners so as to expedite the learning progress in pool-based active learning. [sent-262, score-0.279]
</p><p>26 A weighted ensemble of active learners is maintained and each learner is allowed to express interest in a pool of unlabeled training points. [sent-264, score-0.241]
</p><p>27 Heterogeneous Evolution of Surrogate Models This Section discusses how different surrogate models may be evolved cooperatively in order perform model type selection. [sent-301, score-0.347]
</p><p>28 Different sub-populations, called demes, exist (initialized differently) and sporadic migration can occur between islands allowing for the exchange of genetic material between species and inter-species competition for resources. [sent-311, score-0.329]
</p><p>29 The island model introduces ﬁve new parameters: the migration topology, the migration frequency, the number of individuals to migrate, a strategy to select the emigrants, and a replacement strategy to incorporate the immigrants. [sent-315, score-0.473]
</p><p>30 2 Global Surrogate Modeling Control Flow Before we can discuss the concrete implementation of the automatic model type selection algorithm it is important to revisit the general global surrogate modeling methodology described in Section 2. [sent-318, score-0.469]
</p><p>31 Based on this initial set, one or more surrogate models are constructed and their hyperparameters optimized according to a chosen hyperparameter optimization algorithm (e. [sent-324, score-0.326]
</p><p>32 3 Algorithm Algorithm 1 Automatic global surrogate modeling with heterogeneous evolution and active learning 01. [sent-343, score-0.502]
</p><p>33 The speciation model used is the island model since we found it the most natural way of evolving multiple model types while still allowing for hybrid solutions. [sent-380, score-0.261]
</p><p>34 Once every deme has gone through a generation, migration between individuals is allowed to occur at migration interval mi , with migration fraction m f and migration direction md (a ring topology is used). [sent-396, score-0.792]
</p><p>35 The migration strategy is as follows: the l = (|Mi | · m f ) ﬁttest individuals of Mi replace the l worst individuals in the next deme (deﬁned by md ). [sent-397, score-0.289]
</p><p>36 Once the GA has terminated, control passes back to the main global surrogate modeling algorithm of the SUMO Toolbox. [sent-403, score-0.29]
</p><p>37 EP works by monitoring the population and each generation recording the number of individuals of each model type. [sent-419, score-0.267]
</p><p>38 If two models of different types are selected to recombine, an ensemble is created with the models as ensemble members. [sent-438, score-0.513]
</p><p>39 Thus, as soon as migration occurs, model types start mixing, and ensemble models arise as a result. [sent-439, score-0.484]
</p><p>40 In addition we enforce a maximum ensemble size ESmax and require that ensemble members must differ ESdi f f percent in their response (their ‘behavior’). [sent-445, score-0.445]
</p><p>41 ensemble - ensemble recombination: a single-point crossover is made between the ensemble member lists of each model (note that the type of the ensemble members is irrelevant) 2. [sent-448, score-0.96]
</p><p>42 ensemble - model recombination: the model replaces a randomly selected ensemble member with probability pswap or gets absorbed into the ensemble with probability 1 − pswap (respecting ESmax and ESdi f f ). [sent-449, score-0.687]
</p><p>43 ensemble mutation: one ensemble member is randomly deleted Besides enabling hybrid solutions, using ensembles has the additional beneﬁt of allowing a model to lie ‘dormant’ in an ensemble with the possibility of re-emerging later (e. [sent-451, score-0.704]
</p><p>44 (2007) for example, the type of the ensemble members is not ﬁxed in any way but varies dynamically. [sent-455, score-0.279]
</p><p>45 We have not yet mentioned what type of ensemble will be used. [sent-456, score-0.279]
</p><p>46 Instead of performing two modeling runs (doing a separate hyperparameter optimization for each output) both outputs can be modeled simultaneously if models with multiple outputs are used in conjunction with a multiobjective optimization routine. [sent-475, score-0.359]
</p><p>47 In both cases such a multi-objective approach can be integrated with the automatic surrogate model type selection algorithm described here. [sent-476, score-0.377]
</p><p>48 This means that the best model type can vary per criteria or, more interestingly, that it enables automatic selection of the best model type for each output without having to resort to multiple runs. [sent-477, score-0.294]
</p><p>49 The obvious advantage is the ability to perform automatic selection of the model type and complexity for a given data source (no need to do multiple parallel runs or train a complex classiﬁer). [sent-484, score-0.244]
</p><p>50 If reasonable population sizes are used together with migration and the extinction prevention algorithm described in Section 5. [sent-508, score-0.398]
</p><p>51 The objective is to validate if the best model type can indeed be determined automatically, and in a way that is cheaper and better than the simple brute force method: doing multiple, single model type runs in parallel. [sent-520, score-0.295]
</p><p>52 Model Types For the tests the following model types are used: Artiﬁcial Neural Networks (ANN), rational functions, RBF models, Kriging models, LS-SVMs, and for the AF example: also smoothing splines. [sent-591, score-0.251]
</p><p>53 When run alone (without the HGA) this results in high quality models with a much faster run time than training the weights by evolution as well. [sent-601, score-0.245]
</p><p>54 LOLA’s strengths are that it scales well with the number of dimensions, makes no assumptions about the underlying problem or surrogate model type, and works in both the R and C domains. [sent-627, score-0.237]
</p><p>55 However, in many cases it may be desirable to also include information about the surrogate model itself when choosing potential sample locations. [sent-635, score-0.237]
</p><p>56 The migration interval mi is set to 7, the migration fraction m f to 0. [sent-641, score-0.371]
</p><p>57 1 and the migration direction is both (copies of the m f best individuals from island i replace the worst individuals in islands i − 1 and i + 1). [sent-642, score-0.302]
</p><p>58 4 Others Each problem was modeled twice with the heterogeneous evolution algorithm (once with EP = true, once with EP = f alse) and once with homogeneous evolution (a single model type run for each model type in the HGA). [sent-667, score-0.629]
</p><p>59 33e+01 00 ] 90  60  Ensemble Spline Kriging Rational RBF LS−SVM ANN  50 40 30  20  20  10  10  0  0  1 2 3 4 5 6 7 8 9 101112131415 Number of runs  1 2 3 4 5 6 7 8 9 101112131415 Number of runs  Figure 4: AF: Composition of the ﬁnal population (Left: EP=false, Right: EP=true) avg: [ 00 6. [sent-690, score-0.266]
</p><p>60 31e+00 ]  80 70 60 50 40 30 20 10 0  1 2 3 4 5 6 7 8 9 101112131415 Number of runs  1 2 3 4 5 6 7 8 9 101112131415 Number of runs  Figure 5: AF: Error histogram of the ﬁnal best model in each run (Left: EP=false, Right: EP=true) deviation over all runs. [sent-714, score-0.249]
</p><p>61 The population evolution for the run that produced the best model in both cases is shown in Figure 6. [sent-717, score-0.346]
</p><p>62 Due to randomness in the initial population and genetic operators a model type may be driven extinct, unable to return. [sent-803, score-0.408]
</p><p>63 88e+00 ]  80 70 60 50 40 30 20 10 0  1 2 3 4 5 6 7 8 9 101112131415 Number of runs  1 2 3 4 5 6 7 8 9 101112131415 Number of runs  Figure 9: KF: Error histogram of the ﬁnal best model in each run (Left: EP=false, Right: EP=true) Method  |X|  σ  ¯ V ERRSE  σ  ¯ T ERRSE  σ  ¯ time (min)  σ  ANN  1. [sent-838, score-0.249]
</p><p>64 533E+01  Table 2: KF: Comparison with homogeneous evolution  The natural question that remains, is how do these results compare with simply doing multiple homogeneous evolution (single model type, using the same GA settings) runs, one for each type separately? [sent-894, score-0.485]
</p><p>65 The population evolution and corresponding error evolution for the best run are shown in Figures 11 and 12. [sent-903, score-0.438]
</p><p>66 The fact that the rational functions succeed in doing this is thanks to a weighting scheme used in the genetic operators and described further in Hendrickx et al. [sent-924, score-0.33]
</p><p>67 3 EM Example The composition of the ﬁnal population for each run is shown in Figure 13, and the associated error histogram in Figure 14. [sent-935, score-0.263]
</p><p>68 The population evolution and corresponding error evolution for the best run are shown in Figures 15 and 16. [sent-936, score-0.438]
</p><p>69 The results are very clear cut, rational functions dominate in every run, easily reaching the accuracy requirements in about 200 data points (with the EP=true runs generally reaching higher accuracies). [sent-939, score-0.238]
</p><p>70 4 LGBB Example The composition of the ﬁnal population for each run is shown in Figure 18 and the associated error histogram in Figure 19. [sent-972, score-0.263]
</p><p>71 The population evolution of the best run is shown in Figure 20. [sent-973, score-0.307]
</p><p>72 079E+00  Table 3: EE: Comparison with homogeneous evolution  example consists of a 3 dimensional data set and unlike the AF and EE examples there are no clues as to which model type is most adequate. [sent-1028, score-0.3]
</p><p>73 58e+00 ] 80  Ensemble Kriging Rational RBF LS−SVM ANN  50 40 30  20  20  10  10  0  0  1 2 3 4 5 6 7 8 9 101112131415 Number of runs  1 2 3 4 5 6 7 8 9 101112131415 Number of runs  Figure 18: LE: Composition of the ﬁnal population (Left: EP=false, Right: EP=true)  100  e >= 1e0 1e0 > e >= 1e−1 1. [sent-1047, score-0.266]
</p><p>74 31e−01 ]  80 70 60 50 40 30  20  20  10  10  0  0  1 2 3 4 5 6 7 8 9 101112131415 Number of runs  1 2 3 4 5 6 7 8 9 101112131415 Number of runs  Figure 19: LE: Error histogram of the ﬁnal best model in each run (Left: EP=false, Right: EP=true) Method  |X|  ¯ V ERRSE  σ  ¯ time (min)  ANN  7. [sent-1067, score-0.249]
</p><p>75 The composition of the ﬁnal population for each run is shown in Figure 22 and the associated error histogram in Figure 23. [sent-1108, score-0.263]
</p><p>76 The population evolution of the best run is shown in Figure 24. [sent-1109, score-0.307]
</p><p>77 17e−01 ]  80 70 60 50 40 30 20 10 0  1 2 3 4 5 6 7 8 9 101112131415 Number of runs  1 2 3 4 5 6 7 8 9 101112131415 Number of runs  Figure 23: BH: Error histogram of the ﬁnal best model in each run (Left: EP=false, Right: EP=true)  Method  |X|  ¯ V ERRSE  σ  ¯ time (min)  ANN  5. [sent-1154, score-0.249]
</p><p>78 Striking, though, is that about half of the ﬁnal population consists of ensembles and that most of these ensembles turn out to be {ANN, RBF} pairs or multiple ANNs. [sent-1190, score-0.248]
</p><p>79 Also striking (and interesting) are the oscillations in the population evolution (see Figure 24, or Figure 26 for a more marked example). [sent-1195, score-0.302]
</p><p>80 In this respect the migration interval and migration topology parameters are particularly important since they determine how the different model types interact. [sent-1229, score-0.416]
</p><p>81 For example, if the migration interval is set too high, each deme will produce high quality models (most of the time is spent optimizing the parameters of a single model type) but there will have been very little competition between models. [sent-1230, score-0.288]
</p><p>82 Conclusion and Future Work Due to the computational complexity of current simulation codes, the use of global surrogate modeling techniques (adaptive sampling, adaptive modeling) has become standard practice among scientists and engineers alike. [sent-1234, score-0.371]
</p><p>83 However, a recurring problem is selecting the most adequate surrogate model type and associated complexity. [sent-1235, score-0.313]
</p><p>84 In this contribution we explored an approach based on the evolutionary migration model that can help tackle this problem in an automatic way if little information is known about the true response behavior and there are no a priori model type requirements. [sent-1236, score-0.587]
</p><p>85 In addition, we have illustrated the usefulness of extinction prevention and ensemble based recombination. [sent-1237, score-0.296]
</p><p>86 Future work will consist of investigating the oscillations in the BH example, exploring different GA parameter values (role of the migration frequency, migration topology, etc. [sent-1240, score-0.373]
</p><p>87 ), incorporating more model types, and more advanced ensemble methods (e. [sent-1241, score-0.242]
</p><p>88 Furthermore, we have been experimenting with sampling strategies that vary dynamically depending on the remaining sample budget and quality & type of surrogate currently used in the modeling process. [sent-1246, score-0.379]
</p><p>89 Expert application of a surrogate model type will invariably lead to problem speciﬁc bias, reducing the performance on other problems. [sent-1259, score-0.313]
</p><p>90 This allows for automatic model type switching during optimization (any model type that supports prediction variance can be used) and may be beneﬁcial for computational expensive codes. [sent-1264, score-0.299]
</p><p>91 A novel sequential design strategy for global surrogate modeling. [sent-1342, score-0.279]
</p><p>92 Single- and multiobjective evolutionary optimization assisted by gaussian random ﬁeld metamodels. [sent-1368, score-0.297]
</p><p>93 Comparison of three surrogate modeling techniques: Datascape, kriging, and second order regression. [sent-1418, score-0.259]
</p><p>94 Evolutionary algorithms with surrogate modeling for computationally expensive optimization problem. [sent-1435, score-0.295]
</p><p>95 A study on metamodeling techniques, ensembles, and multi-surrogates in evolutionary computation. [sent-1626, score-0.246]
</p><p>96 Exact schema theory for genetic programming and variable-length genetic algorithms with one-point crossover. [sent-1722, score-0.25]
</p><p>97 Theoretical analysis of evolutionary algorithms with an inﬁnite population size in continuous space. [sent-1732, score-0.328]
</p><p>98 Theoretical analysis of evolutionary algorithms with an inﬁnite population size in continuous space. [sent-1742, score-0.328]
</p><p>99 The island model genetic algorithm: On separability, population size and convergence. [sent-1952, score-0.359]
</p><p>100 A nonstationary covariance-based kriging method for metamodeling in engineering design. [sent-1959, score-0.384]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kriging', 0.33), ('ep', 0.242), ('ensemble', 0.203), ('surrogate', 0.198), ('evolutionary', 0.192), ('rational', 0.173), ('migration', 0.169), ('gorissen', 0.163), ('rbf', 0.16), ('haene', 0.153), ('orissen', 0.153), ('urck', 0.153), ('odeling', 0.146), ('urrogate', 0.146), ('volutionary', 0.146), ('ann', 0.141), ('population', 0.136), ('evolution', 0.131), ('genetic', 0.125), ('lobal', 0.124), ('ype', 0.124), ('percantage', 0.123), ('std', 0.117), ('avg', 0.117), ('hgaep', 0.107), ('doi', 0.1), ('lgbb', 0.1), ('hga', 0.098), ('odel', 0.095), ('ga', 0.092), ('ls', 0.084), ('type', 0.076), ('election', 0.076), ('bh', 0.076), ('lift', 0.072), ('medv', 0.069), ('multiobjective', 0.069), ('runs', 0.065), ('tness', 0.064), ('modeling', 0.061), ('island', 0.059), ('hyperparameter', 0.058), ('ensembles', 0.056), ('generation', 0.055), ('homogeneous', 0.054), ('errse', 0.054), ('extinction', 0.054), ('lola', 0.054), ('metamodeling', 0.054), ('recombination', 0.054), ('sumo', 0.054), ('design', 0.05), ('composition', 0.047), ('kf', 0.046), ('deme', 0.046), ('multidisciplinary', 0.046), ('speciation', 0.046), ('sampling', 0.044), ('simulation', 0.044), ('heterogeneous', 0.043), ('ee', 0.042), ('false', 0.042), ('run', 0.04), ('histogram', 0.04), ('response', 0.039), ('prevention', 0.039), ('alse', 0.039), ('model', 0.039), ('types', 0.039), ('issn', 0.038), ('couckuyt', 0.038), ('evolve', 0.038), ('kotanchek', 0.038), ('mach', 0.038), ('metamodel', 0.038), ('posts', 0.038), ('xnew', 0.038), ('active', 0.038), ('svm', 0.037), ('adaptive', 0.037), ('individuals', 0.037), ('optimization', 0.036), ('oscillations', 0.035), ('species', 0.035), ('af', 0.035), ('models', 0.034), ('toolbox', 0.034), ('automatic', 0.033), ('mi', 0.033), ('ackley', 0.033), ('comb', 0.033), ('crossover', 0.033), ('escalante', 0.033), ('gramacy', 0.033), ('simpson', 0.033), ('operators', 0.032), ('nal', 0.032), ('global', 0.031), ('jin', 0.031), ('selection', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999821 <a title="31-tfidf-1" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>Author: Dirk Gorissen, Tom Dhaene, Filip De Turck</p><p>Abstract: Due to the scale and computational complexity of currently used simulation codes, global surrogate (metamodels) models have become indispensable tools for exploring and understanding the design space. Due to their compact formulation they are cheap to evaluate and thus readily facilitate visualization, design space exploration, rapid prototyping, and sensitivity analysis. They can also be used as accurate building blocks in design packages or larger simulation environments. Consequently, there is great interest in techniques that facilitate the construction of such approximation models while minimizing the computational cost and maximizing model accuracy. Many surrogate model types exist (Support Vector Machines, Kriging, Neural Networks, etc.) but no type is optimal in all circumstances. Nor is there any hard theory available that can help make this choice. In this paper we present an automatic approach to the model type selection problem. We describe an adaptive global surrogate modeling environment with adaptive sampling, driven by speciated evolution. Different model types are evolved cooperatively using a Genetic Algorithm (heterogeneous evolution) and compete to approximate the iteratively selected data. In this way the optimal model type and complexity for a given data set or simulation code can be dynamically determined. Its utility and performance is demonstrated on a number of problems where it outperforms traditional sequential execution of each model type. Keywords: model type selection, genetic algorithms, global surrogate modeling, function approximation, active learning, adaptive sampling</p><p>2 0.089181729 <a title="31-tfidf-2" href="./jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">70 jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<p>Author: Hugo Jair Escalante, Manuel Montes, Luis Enrique Sucar</p><p>Abstract: This paper proposes the application of particle swarm optimization (PSO) to the problem of full model selection, FMS, for classiﬁcation tasks. FMS is deﬁned as follows: given a pool of preprocessing methods, feature selection and learning algorithms, to select the combination of these that obtains the lowest classiﬁcation error for a given data set; the task also includes the selection of hyperparameters for the considered methods. This problem generates a vast search space to be explored, well suited for stochastic optimization techniques. FMS can be applied to any classiﬁcation domain as it does not require domain knowledge. Different model types and a variety of algorithms can be considered under this formulation. Furthermore, competitive yet simple models can be obtained with FMS. We adopt PSO for the search because of its proven performance in different problems and because of its simplicity, since neither expensive computations nor complicated operations are needed. Interestingly, the way the search is guided allows PSO to avoid overﬁtting to some extend. Experimental results on benchmark data sets give evidence that the proposed approach is very effective, despite its simplicity. Furthermore, results obtained in the framework of a model selection challenge show the competitiveness of the models selected with PSO, compared to models selected with other techniques that focus on a single algorithm and that use domain knowledge. Keywords: full model selection, machine learning challenge, particle swarm optimization, experimentation, cross validation</p><p>3 0.088234149 <a title="31-tfidf-3" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>Author: Eugene Tuv, Alexander Borisov, George Runger, Kari Torkkola</p><p>Abstract: Predictive models beneﬁt from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for ﬁlters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach. Keywords: trees, resampling, importance, masking, residuals</p><p>4 0.076459818 <a title="31-tfidf-4" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>Author: Ulrich Paquet, Ole Winther, Manfred Opper</p><p>Abstract: Bayesian inference is intractable for many interesting models, making deterministic algorithms for approximate inference highly desirable. Unlike stochastic methods, which are exact in the limit, the accuracy of these approaches cannot be reasonably judged. In this paper we show how low order perturbation corrections to an expectation-consistent (EC) approximation can provide the necessary tools to ameliorate inference accuracy, and to give an indication of the quality of approximation without having to resort to Monte Carlo methods. Further comparisons are given with variational Bayes and parallel tempering (PT) combined with thermodynamic integration on a Gaussian mixture model. To obtain practical results we further generalize PT to temper from arbitrary distributions rather than a prior in Bayesian inference. Keywords: Bayesian inference, mixture models, expectation propagation, expectation consistent, perturbation correction, variational Bayes, parallel tempering, thermodynamic integration</p><p>5 0.071818061 <a title="31-tfidf-5" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>Author: Pradip Ghanty, Samrat Paul, Nikhil R. Pal</p><p>Abstract: In this paper we propose a new multilayer classiﬁer architecture. The proposed hybrid architecture has two cascaded modules: feature extraction module and classiﬁcation module. In the feature extraction module we use the multilayered perceptron (MLP) neural networks, although other tools such as radial basis function (RBF) networks can be used. In the classiﬁcation module we use support vector machines (SVMs)—here also other tool such as MLP or RBF can be used. The feature extraction module has several sub-modules each of which is expected to extract features capturing the discriminating characteristics of different areas of the input space. The classiﬁcation module classiﬁes the data based on the extracted features. The resultant architecture with MLP in feature extraction module and SVM in classiﬁcation module is called NEUROSVM. The NEUROSVM is tested on twelve benchmark data sets and the performance of the NEUROSVM is found to be better than both MLP and SVM. We also compare the performance of proposed architecture with that of two ensemble methods: majority voting and averaging. Here also the NEUROSVM is found to perform better than these two ensemble methods. Further we explore the use of MLP and RBF in the classiﬁcation module of the proposed architecture. The most attractive feature of NEUROSVM is that it practically eliminates the severe dependency of SVM on the choice of kernel. This has been veriﬁed with respect to both linear and non-linear kernels. We have also demonstrated that for the feature extraction module, the full training of MLPs is not needed. Keywords: feature extraction, neural networks (NNs), support vector machines (SVMs), hybrid system, majority voting, averaging c 2009 Pradip Ghanty, Samrat Paul and Nikhil R. Pal. G HANTY, PAUL AND PAL</p><p>6 0.048331238 <a title="31-tfidf-6" href="./jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems.html">8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</a></p>
<p>7 0.038287241 <a title="31-tfidf-7" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>8 0.036342587 <a title="31-tfidf-8" href="./jmlr-2009-Marginal_Likelihood_Integrals_for_Mixtures_of_Independence_Models.html">53 jmlr-2009-Marginal Likelihood Integrals for Mixtures of Independence Models</a></p>
<p>9 0.034717493 <a title="31-tfidf-9" href="./jmlr-2009-Robust_Process_Discovery_with_Artificial_Negative_Events%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">81 jmlr-2009-Robust Process Discovery with Artificial Negative Events    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>10 0.034065746 <a title="31-tfidf-10" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>11 0.033713792 <a title="31-tfidf-11" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>12 0.033299308 <a title="31-tfidf-12" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>13 0.032720163 <a title="31-tfidf-13" href="./jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">16 jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<p>14 0.03226874 <a title="31-tfidf-14" href="./jmlr-2009-Hybrid_MPI_OpenMP_Parallel_Linear_Support_Vector_Machine_Training.html">39 jmlr-2009-Hybrid MPI OpenMP Parallel Linear Support Vector Machine Training</a></p>
<p>15 0.031264793 <a title="31-tfidf-15" href="./jmlr-2009-Dlib-ml%3A_A_Machine_Learning_Toolkit%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">26 jmlr-2009-Dlib-ml: A Machine Learning Toolkit    (Machine Learning Open Source Software Paper)</a></p>
<p>16 0.029821197 <a title="31-tfidf-16" href="./jmlr-2009-Model_Monitor_%28M2%29%3A_Evaluating%2C_Comparing%2C_and_Monitoring_Models%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">56 jmlr-2009-Model Monitor (M2): Evaluating, Comparing, and Monitoring Models    (Machine Learning Open Source Software Paper)</a></p>
<p>17 0.028805897 <a title="31-tfidf-17" href="./jmlr-2009-Bi-Level_Path_Following_for_Cross_Validated_Solution_of_Kernel_Quantile_Regression.html">12 jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</a></p>
<p>18 0.028073754 <a title="31-tfidf-18" href="./jmlr-2009-Subgroup_Analysis_via_Recursive_Partitioning.html">91 jmlr-2009-Subgroup Analysis via Recursive Partitioning</a></p>
<p>19 0.026878007 <a title="31-tfidf-19" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>20 0.026464161 <a title="31-tfidf-20" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, -0.096), (2, 0.086), (3, 0.002), (4, 0.024), (5, -0.092), (6, 0.032), (7, 0.081), (8, 0.119), (9, 0.093), (10, 0.141), (11, 0.004), (12, 0.04), (13, -0.091), (14, -0.043), (15, 0.015), (16, -0.156), (17, 0.121), (18, -0.344), (19, 0.289), (20, 0.176), (21, -0.032), (22, -0.063), (23, -0.153), (24, -0.155), (25, 0.054), (26, -0.013), (27, -0.052), (28, 0.104), (29, -0.003), (30, -0.076), (31, -0.005), (32, -0.041), (33, -0.095), (34, 0.001), (35, 0.007), (36, -0.01), (37, 0.128), (38, -0.108), (39, 0.04), (40, -0.018), (41, -0.117), (42, -0.058), (43, 0.004), (44, -0.068), (45, -0.041), (46, 0.069), (47, -0.061), (48, -0.043), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9453283 <a title="31-lsi-1" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>Author: Dirk Gorissen, Tom Dhaene, Filip De Turck</p><p>Abstract: Due to the scale and computational complexity of currently used simulation codes, global surrogate (metamodels) models have become indispensable tools for exploring and understanding the design space. Due to their compact formulation they are cheap to evaluate and thus readily facilitate visualization, design space exploration, rapid prototyping, and sensitivity analysis. They can also be used as accurate building blocks in design packages or larger simulation environments. Consequently, there is great interest in techniques that facilitate the construction of such approximation models while minimizing the computational cost and maximizing model accuracy. Many surrogate model types exist (Support Vector Machines, Kriging, Neural Networks, etc.) but no type is optimal in all circumstances. Nor is there any hard theory available that can help make this choice. In this paper we present an automatic approach to the model type selection problem. We describe an adaptive global surrogate modeling environment with adaptive sampling, driven by speciated evolution. Different model types are evolved cooperatively using a Genetic Algorithm (heterogeneous evolution) and compete to approximate the iteratively selected data. In this way the optimal model type and complexity for a given data set or simulation code can be dynamically determined. Its utility and performance is demonstrated on a number of problems where it outperforms traditional sequential execution of each model type. Keywords: model type selection, genetic algorithms, global surrogate modeling, function approximation, active learning, adaptive sampling</p><p>2 0.63488853 <a title="31-lsi-2" href="./jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">70 jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<p>Author: Hugo Jair Escalante, Manuel Montes, Luis Enrique Sucar</p><p>Abstract: This paper proposes the application of particle swarm optimization (PSO) to the problem of full model selection, FMS, for classiﬁcation tasks. FMS is deﬁned as follows: given a pool of preprocessing methods, feature selection and learning algorithms, to select the combination of these that obtains the lowest classiﬁcation error for a given data set; the task also includes the selection of hyperparameters for the considered methods. This problem generates a vast search space to be explored, well suited for stochastic optimization techniques. FMS can be applied to any classiﬁcation domain as it does not require domain knowledge. Different model types and a variety of algorithms can be considered under this formulation. Furthermore, competitive yet simple models can be obtained with FMS. We adopt PSO for the search because of its proven performance in different problems and because of its simplicity, since neither expensive computations nor complicated operations are needed. Interestingly, the way the search is guided allows PSO to avoid overﬁtting to some extend. Experimental results on benchmark data sets give evidence that the proposed approach is very effective, despite its simplicity. Furthermore, results obtained in the framework of a model selection challenge show the competitiveness of the models selected with PSO, compared to models selected with other techniques that focus on a single algorithm and that use domain knowledge. Keywords: full model selection, machine learning challenge, particle swarm optimization, experimentation, cross validation</p><p>3 0.40573651 <a title="31-lsi-3" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>Author: Eugene Tuv, Alexander Borisov, George Runger, Kari Torkkola</p><p>Abstract: Predictive models beneﬁt from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for ﬁlters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach. Keywords: trees, resampling, importance, masking, residuals</p><p>4 0.39282158 <a title="31-lsi-4" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>Author: Pradip Ghanty, Samrat Paul, Nikhil R. Pal</p><p>Abstract: In this paper we propose a new multilayer classiﬁer architecture. The proposed hybrid architecture has two cascaded modules: feature extraction module and classiﬁcation module. In the feature extraction module we use the multilayered perceptron (MLP) neural networks, although other tools such as radial basis function (RBF) networks can be used. In the classiﬁcation module we use support vector machines (SVMs)—here also other tool such as MLP or RBF can be used. The feature extraction module has several sub-modules each of which is expected to extract features capturing the discriminating characteristics of different areas of the input space. The classiﬁcation module classiﬁes the data based on the extracted features. The resultant architecture with MLP in feature extraction module and SVM in classiﬁcation module is called NEUROSVM. The NEUROSVM is tested on twelve benchmark data sets and the performance of the NEUROSVM is found to be better than both MLP and SVM. We also compare the performance of proposed architecture with that of two ensemble methods: majority voting and averaging. Here also the NEUROSVM is found to perform better than these two ensemble methods. Further we explore the use of MLP and RBF in the classiﬁcation module of the proposed architecture. The most attractive feature of NEUROSVM is that it practically eliminates the severe dependency of SVM on the choice of kernel. This has been veriﬁed with respect to both linear and non-linear kernels. We have also demonstrated that for the feature extraction module, the full training of MLPs is not needed. Keywords: feature extraction, neural networks (NNs), support vector machines (SVMs), hybrid system, majority voting, averaging c 2009 Pradip Ghanty, Samrat Paul and Nikhil R. Pal. G HANTY, PAUL AND PAL</p><p>5 0.36894131 <a title="31-lsi-5" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>Author: Ulrich Paquet, Ole Winther, Manfred Opper</p><p>Abstract: Bayesian inference is intractable for many interesting models, making deterministic algorithms for approximate inference highly desirable. Unlike stochastic methods, which are exact in the limit, the accuracy of these approaches cannot be reasonably judged. In this paper we show how low order perturbation corrections to an expectation-consistent (EC) approximation can provide the necessary tools to ameliorate inference accuracy, and to give an indication of the quality of approximation without having to resort to Monte Carlo methods. Further comparisons are given with variational Bayes and parallel tempering (PT) combined with thermodynamic integration on a Gaussian mixture model. To obtain practical results we further generalize PT to temper from arbitrary distributions rather than a prior in Bayesian inference. Keywords: Bayesian inference, mixture models, expectation propagation, expectation consistent, perturbation correction, variational Bayes, parallel tempering, thermodynamic integration</p><p>6 0.33199111 <a title="31-lsi-6" href="./jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems.html">8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</a></p>
<p>7 0.23246925 <a title="31-lsi-7" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>8 0.22285654 <a title="31-lsi-8" href="./jmlr-2009-Robust_Process_Discovery_with_Artificial_Negative_Events%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">81 jmlr-2009-Robust Process Discovery with Artificial Negative Events    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>9 0.22040677 <a title="31-lsi-9" href="./jmlr-2009-Marginal_Likelihood_Integrals_for_Mixtures_of_Independence_Models.html">53 jmlr-2009-Marginal Likelihood Integrals for Mixtures of Independence Models</a></p>
<p>10 0.21916689 <a title="31-lsi-10" href="./jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">16 jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<p>11 0.20772463 <a title="31-lsi-11" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>12 0.20711689 <a title="31-lsi-12" href="./jmlr-2009-Bi-Level_Path_Following_for_Cross_Validated_Solution_of_Kernel_Quantile_Regression.html">12 jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</a></p>
<p>13 0.18706574 <a title="31-lsi-13" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>14 0.18330723 <a title="31-lsi-14" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>15 0.18315844 <a title="31-lsi-15" href="./jmlr-2009-Estimation_of_Sparse_Binary_Pairwise_Markov_Networks_using_Pseudo-likelihoods.html">30 jmlr-2009-Estimation of Sparse Binary Pairwise Markov Networks using Pseudo-likelihoods</a></p>
<p>16 0.16651684 <a title="31-lsi-16" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>17 0.15787266 <a title="31-lsi-17" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>18 0.15293103 <a title="31-lsi-18" href="./jmlr-2009-Python_Environment_for_Bayesian_Learning%3A_Inferring_the_Structure_of_Bayesian_Networks_from_Knowledge_and_Data%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">76 jmlr-2009-Python Environment for Bayesian Learning: Inferring the Structure of Bayesian Networks from Knowledge and Data    (Machine Learning Open Source Software Paper)</a></p>
<p>19 0.15006928 <a title="31-lsi-19" href="./jmlr-2009-Optimized_Cutting_Plane_Algorithm_for_Large-Scale_Risk_Minimization.html">69 jmlr-2009-Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization</a></p>
<p>20 0.14924429 <a title="31-lsi-20" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.025), (11, 0.025), (19, 0.02), (26, 0.038), (38, 0.034), (47, 0.015), (52, 0.035), (55, 0.027), (58, 0.037), (66, 0.074), (68, 0.022), (86, 0.479), (90, 0.053), (96, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73826575 <a title="31-lda-1" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>Author: Dirk Gorissen, Tom Dhaene, Filip De Turck</p><p>Abstract: Due to the scale and computational complexity of currently used simulation codes, global surrogate (metamodels) models have become indispensable tools for exploring and understanding the design space. Due to their compact formulation they are cheap to evaluate and thus readily facilitate visualization, design space exploration, rapid prototyping, and sensitivity analysis. They can also be used as accurate building blocks in design packages or larger simulation environments. Consequently, there is great interest in techniques that facilitate the construction of such approximation models while minimizing the computational cost and maximizing model accuracy. Many surrogate model types exist (Support Vector Machines, Kriging, Neural Networks, etc.) but no type is optimal in all circumstances. Nor is there any hard theory available that can help make this choice. In this paper we present an automatic approach to the model type selection problem. We describe an adaptive global surrogate modeling environment with adaptive sampling, driven by speciated evolution. Different model types are evolved cooperatively using a Genetic Algorithm (heterogeneous evolution) and compete to approximate the iteratively selected data. In this way the optimal model type and complexity for a given data set or simulation code can be dynamically determined. Its utility and performance is demonstrated on a number of problems where it outperforms traditional sequential execution of each model type. Keywords: model type selection, genetic algorithms, global surrogate modeling, function approximation, active learning, adaptive sampling</p><p>2 0.27134132 <a title="31-lda-2" href="./jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">70 jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<p>Author: Hugo Jair Escalante, Manuel Montes, Luis Enrique Sucar</p><p>Abstract: This paper proposes the application of particle swarm optimization (PSO) to the problem of full model selection, FMS, for classiﬁcation tasks. FMS is deﬁned as follows: given a pool of preprocessing methods, feature selection and learning algorithms, to select the combination of these that obtains the lowest classiﬁcation error for a given data set; the task also includes the selection of hyperparameters for the considered methods. This problem generates a vast search space to be explored, well suited for stochastic optimization techniques. FMS can be applied to any classiﬁcation domain as it does not require domain knowledge. Different model types and a variety of algorithms can be considered under this formulation. Furthermore, competitive yet simple models can be obtained with FMS. We adopt PSO for the search because of its proven performance in different problems and because of its simplicity, since neither expensive computations nor complicated operations are needed. Interestingly, the way the search is guided allows PSO to avoid overﬁtting to some extend. Experimental results on benchmark data sets give evidence that the proposed approach is very effective, despite its simplicity. Furthermore, results obtained in the framework of a model selection challenge show the competitiveness of the models selected with PSO, compared to models selected with other techniques that focus on a single algorithm and that use domain knowledge. Keywords: full model selection, machine learning challenge, particle swarm optimization, experimentation, cross validation</p><p>3 0.23442349 <a title="31-lda-3" href="./jmlr-2009-Settable_Systems%3A_An_Extension_of_Pearl%27s_Causal_Model_with_Optimization%2C_Equilibrium%2C_and_Learning.html">85 jmlr-2009-Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning</a></p>
<p>Author: Halbert White, Karim Chalak</p><p>Abstract: Judea Pearl’s Causal Model is a rich framework that provides deep insight into the nature of causal relations. As yet, however, the Pearl Causal Model (PCM) has had a lesser impact on economics or econometrics than on other disciplines. This may be due in part to the fact that the PCM is not as well suited to analyzing structures that exhibit features of central interest to economists and econometricians: optimization, equilibrium, and learning. We offer the settable systems framework as an extension of the PCM that permits causal discourse in systems embodying optimization, equilibrium, and learning. Because these are common features of physical, natural, or social systems, our framework may prove generally useful for machine learning. Important features distinguishing the settable system framework from the PCM are its countable dimensionality and the use of partitioning and partition-speciﬁc response functions to accommodate the behavior of optimizing and interacting agents and to eliminate the requirement of a unique ﬁxed point for the system. Reﬁnements of the PCM include the settable systems treatment of attributes, the causal role of exogenous variables, and the dual role of variables as causes and responses. A series of closely related machine learning examples and examples from game theory and machine learning with feedback demonstrates some limitations of the PCM and motivates the distinguishing features of settable systems. Keywords: equations causal models, game theory, machine learning, recursive estimation, simultaneous</p><p>4 0.234155 <a title="31-lda-4" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>Author: Lisa Hellerstein, Bernard Rosell, Eric Bach, Soumya Ray, David Page</p><p>Abstract: A Boolean function f is correlation immune if each input variable is independent of the output, under the uniform distribution on inputs. For example, the parity function is correlation immune. We consider the problem of identifying relevant variables of a correlation immune function, in the presence of irrelevant variables. We address this problem in two different contexts. First, we analyze Skewing, a heuristic method that was developed to improve the ability of greedy decision tree algorithms to identify relevant variables of correlation immune Boolean functions, given examples drawn from the uniform distribution (Page and Ray, 2003). We present theoretical results revealing both the capabilities and limitations of skewing. Second, we explore the problem of identifying relevant variables in the Product Distribution Choice (PDC) learning model, a model in which the learner can choose product distributions and obtain examples from them. We prove a lemma establishing a property of Boolean functions that may be of independent interest. Using this lemma, we give two new algorithms for ﬁnding relevant variables of correlation immune functions in the PDC model. Keywords: correlation immune functions, skewing, relevant variables, Boolean functions, product distributions c 2009 Lisa Hellerstein, Bernard Rosell, Eric Bach, Soumya Ray and David Page. H ELLERSTEIN , ROSELL , BACH , R AY AND PAGE</p><p>5 0.23251806 <a title="31-lda-5" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>Author: Emma Brunskill, Bethany R. Leffler, Lihong Li, Michael L. Littman, Nicholas Roy</p><p>Abstract: To quickly achieve good performance, reinforcement-learning algorithms for acting in large continuous-valued domains must use a representation that is both sufﬁciently powerful to capture important domain characteristics, and yet simultaneously allows generalization, or sharing, among experiences. Our algorithm balances this tradeoff by using a stochastic, switching, parametric dynamics representation. We argue that this model characterizes a number of signiﬁcant, real-world domains, such as robot navigation across varying terrain. We prove that this representational assumption allows our algorithm to be probably approximately correct with a sample complexity that scales polynomially with all problem-speciﬁc quantities including the state-space dimension. We also explicitly incorporate the error introduced by approximate planning in our sample complexity bounds, in contrast to prior Probably Approximately Correct (PAC) Markov Decision Processes (MDP) approaches, which typically assume the estimated MDP can be solved exactly. Our experimental results on constructing plans for driving to work using real car trajectory data, as well as a small robot experiment on navigating varying terrain, demonstrate that our dynamics representation enables us to capture real-world dynamics in a sufﬁcient manner to produce good performance. Keywords: reinforcement learning, provably efﬁcient learning</p><p>6 0.23245585 <a title="31-lda-6" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>7 0.23240362 <a title="31-lda-7" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>8 0.22960691 <a title="31-lda-8" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>9 0.22839732 <a title="31-lda-9" href="./jmlr-2009-Optimized_Cutting_Plane_Algorithm_for_Large-Scale_Risk_Minimization.html">69 jmlr-2009-Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization</a></p>
<p>10 0.22816935 <a title="31-lda-10" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>11 0.22780785 <a title="31-lda-11" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>12 0.2257909 <a title="31-lda-12" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>13 0.22479196 <a title="31-lda-13" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>14 0.22466999 <a title="31-lda-14" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>15 0.224519 <a title="31-lda-15" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>16 0.22440591 <a title="31-lda-16" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>17 0.22424504 <a title="31-lda-17" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>18 0.22332932 <a title="31-lda-18" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>19 0.22303195 <a title="31-lda-19" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>20 0.22294787 <a title="31-lda-20" href="./jmlr-2009-Learning_Linear_Ranking_Functions_for_Beam_Search_with_Application_to_Planning.html">47 jmlr-2009-Learning Linear Ranking Functions for Beam Search with Application to Planning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
