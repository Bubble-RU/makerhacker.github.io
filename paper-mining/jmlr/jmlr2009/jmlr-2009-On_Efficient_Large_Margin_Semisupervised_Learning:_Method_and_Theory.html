<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-63" href="#">jmlr2009-63</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</h1>
<br/><p>Source: <a title="jmlr-2009-63-pdf" href="http://jmlr.org/papers/volume10/wang09a/wang09a.pdf">pdf</a></p><p>Author: Junhui Wang, Xiaotong Shen, Wei Pan</p><p>Abstract: In classiﬁcation, semisupervised learning usually involves a large amount of unlabeled data with only a small number of labeled data. This imposes a great challenge in that it is difﬁcult to achieve good classiﬁcation performance through labeled data alone. To leverage unlabeled data for enhancing classiﬁcation, this article introduces a large margin semisupervised learning method within the framework of regularization, based on an efﬁcient margin loss for unlabeled data, which seeks efﬁcient extraction of the information from unlabeled data for estimating the Bayes decision boundary for classiﬁcation. For implementation, an iterative scheme is derived through conditional expectations. Finally, theoretical and numerical analyses are conducted, in addition to an application to gene function prediction. They suggest that the proposed method enables to recover the performance of its supervised counterpart based on complete data in rates of convergence, when possible. Keywords: difference convex programming, classiﬁcation, nonconvex minimization, regularization, support vectors</p><p>Reference: <a title="jmlr-2009-63-reference" href="../jmlr2009_reference/jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Division of Biostatistics University of Minnesota Minneapolis, MN 55455, USA  Editor: John Shawe-Taylor  Abstract In classiﬁcation, semisupervised learning usually involves a large amount of unlabeled data with only a small number of labeled data. [sent-7, score-0.536]
</p><p>2 They suggest that the proposed method enables to recover the performance of its supervised counterpart based on complete data in rates of convergence, when possible. [sent-12, score-0.12]
</p><p>3 Introduction Semisupervised learning occurs in classiﬁcation, where only a small number of labeled data is available with a large amount of unlabeled data, because of the difﬁculty of labeling. [sent-14, score-0.275]
</p><p>4 WANG , S HEN AND PAN  situations as such, the primary goal is to leverage unlabeled data to enhance predictive performance of classiﬁcation (Zhu, 2005). [sent-21, score-0.197]
</p><p>5 In semisupervised learning, labeled data {(xi , yi )nl } are sampled from an unknown distribution i=1 P(x, y), together with an independent unlabeled sample {x j }n l +1 from its marginal distribution j=n q(x). [sent-22, score-0.562]
</p><p>6 Here label yi ∈ {−1, 1}, xi = (xi1 , · · · , xid ) is an d-dimensional input, nl ≪ nu and n = nl + nu is the combined size of labeled and unlabeled samples. [sent-23, score-1.343]
</p><p>7 This article develops a large margin semisupervised learning method, which aims to extract the information from unlabeled data for estimating the Bayes decision boundary. [sent-32, score-0.632]
</p><p>8 This is achieved by constructing an efﬁcient loss for unlabeled data with regard to reconstruction of the Bayes decision boundary and by incorporating some knowledge from an estimate of p. [sent-33, score-0.281]
</p><p>9 This permits efﬁcient use of unlabeled data for accurate estimation of the Bayes decision boundary thus enhancing the classiﬁcation performance based on labeled data alone. [sent-34, score-0.323]
</p><p>10 The proposed method, using both the grouping (clustering) structure of unlabeled data and the smoothness structure of p, is designed to recover the classiﬁcation performance based on complete data without missing labels, when possible. [sent-35, score-0.308]
</p><p>11 That is, given a consistent initial classiﬁer, an iterative improvement can be obtained through the constructed loss function. [sent-37, score-0.157]
</p><p>12 Numerical analysis indicates that the proposed method performs well against several state-of-theart semisupervised methods, including TSVM and Wang and Shen (2007), where Wang and Shen (2007) compares favorably against several smooth and clustering based semisupervised methods. [sent-38, score-0.617]
</p><p>13 The theory reveals that the ψ-learning classiﬁer’s generalization performance based on complete data can be recovered by its semisupervised counterpart based on incomplete data in rates of convergence, when some regularity assumptions are satisﬁed. [sent-40, score-0.336]
</p><p>14 The theory also says that the least favorable situation for a semisupervised problem occurs at points near p(x) = 0 or 1 because little information can be provided by these points for reconstructing the classiﬁcation boundary as discussed in Section 2. [sent-41, score-0.391]
</p><p>15 In conclusion, this semisupervised method achieves the desired objective of delivering higher generalization performance. [sent-45, score-0.261]
</p><p>16 This article also examines one novel application in gene function prediction, which has been a primary focus of biomedical research. [sent-46, score-0.11]
</p><p>17 In gene function prediction, microarray gene expression proﬁles can be used to predict gene functions, because genes sharing the same function tend to co720  E FFICIENT L ARGE M ARGIN S EMISUPERVISED L EARNING  express, see Zhou, Kao and Wong (2002). [sent-47, score-0.292]
</p><p>18 Therefore, gene function prediction is an ideal application for semisupervised methods and also employed in this article as a real numerical example. [sent-51, score-0.41]
</p><p>19 Methodology In this section, we present our proposed efﬁcient large margin semisupervised learning method as well its connection to other existing popular methodologies. [sent-59, score-0.348]
</p><p>20 1 Large Margin Classiﬁcation Consider large margin classiﬁcation with labeled data (xi , yi )nl . [sent-61, score-0.165]
</p><p>21 In linear classiﬁcation, given a i=1 class of candidate decision functions F , a cost function nl  C ∑ L(yi f (xi )) + J( f )  (1)  i=1  is minimized over f ∈ F = { f (x) = wT x + w f ,0 ≡ (1, xT )w f } to yield the minimizer fˆ leading to ˜f ˆ). [sent-62, score-0.437]
</p><p>22 Here J( f ) is the reciprocal of the geometric margin of various form with the usual classiﬁer sign( f L2 margin J( f ) = w f 2 /2 to be discussed in further detail, and L(·) is a margin loss deﬁned by ˜ functional margin z = y f (x), and C > 0 is a regularization parameter. [sent-63, score-0.384]
</p><p>23 A margin loss L(z) is said to be large margin if L(z) is non-increasing in z, penalizing small margin values. [sent-70, score-0.297]
</p><p>24 5 being a global minimizer of the generalization error GE( f ) = EI(Y = sign( f (X))), which is usually estimated by labeled data through L(·) in (1). [sent-77, score-0.103]
</p><p>25 In absence of sufﬁcient labeled data, the focus is on how to improve (1) by using additional unlabeled data. [sent-78, score-0.275]
</p><p>26 For this, we construct a margin loss U to measure the performance of estimating f¯. [sent-79, score-0.123]
</p><p>27 Lemma 1 (Optimal loss) For any margin loss L(z), argmin E(L(Y f (X)) − T ( f (X)))2 = E(L(Y f (X))|X = x) = U( f (x)), T  where U( f (x)) = p(x)L( f (x)) + (1 − p(x))L(− f (x)) and p(x) = P(Y = 1|X = x). [sent-83, score-0.123]
</p><p>28 ˆ Based on Lemma 1, we deﬁne U( f ) to be p(x)L( f (x)) + (1 − p(x))L(− f (x)) by replacing p in ˆ ˆ ˆ f ) approximates the ideal loss U( f ) for reconstructing the Bayes decision U( f ) by p. [sent-85, score-0.118]
</p><p>29 Through (approximately) optimal loss U( f ), an iterative improvement of estimation accuracy is achieved by starting with a consistent estimate ˆ p of p, which, for instance, can be obtained through SVM or TSVM. [sent-89, score-0.112]
</p><p>30 The preceding discussion leads to our proposed cost function: nl  s( f ) = C n−1 ∑ L(yi f (xi )) + n−1 u l i=1  n  ∑  ˆ U( f (x j )) + J( f ). [sent-95, score-0.437]
</p><p>31 By comparison, U( f ) enables not only to identify the clustering boundary through the hat function as L(| f |) does but also to discriminate f (x) from − f (x) through an estimated p(x). [sent-105, score-0.168]
</p><p>32 5, and vice versa, That is, U( ˆ whereas L(| f |) is in-discriminative with regard to the sign of f (x). [sent-107, score-0.169]
</p><p>33 There U( f ) favors one clustering boundary for classiﬁcation if a consistent p is provided, whereas L(| f |) fails to disˆ criminate these two. [sent-110, score-0.143]
</p><p>34 ˆ In conclusion, U( f ) yields a more efﬁcient loss for a semisupervised problem as it uses the clustering information from the unlabeled data as L(| f |) does, in addition to guidance about labeling through p to gain a higher discriminative power. [sent-113, score-0.589]
</p><p>35 Computation In this section, we implement the proposed semisupervised method through an iterative scheme as well as a nonconvex optimization technique. [sent-115, score-0.357]
</p><p>36 1, respectively, to f ˆ which are a more general version of the clustering assumption and a smoothness assumption of p. [sent-121, score-0.253]
</p><p>37 In other words, the marginal information from unlabeled data has been effectively incorporated in each iteration of Algorithm 1 for improving estimation accuracy of fˆ and p. [sent-122, score-0.223]
</p><p>38 Train weighted margin classiﬁers fˆπt by solving min Cn−1 (1 − πt ) f ∈F  ∑ L(yi f (xi )) + πt ∑  yi =1  L(yi f (xi )) + J( f ),  yi =−1  with 1 − πt associated with positive instances and πt associated with negative instances. [sent-130, score-0.152]
</p><p>39 Algorithm 1 also differs from Yarowsky’s algorithm (Yarowsky, 1995; Abney, 2004) in that Yarowsky’s algorithm solely relies on the strength of the estimated p, ignoring the potential ˆ information from the clustering assumption. [sent-156, score-0.12]
</p><p>40 Most importantly, the smoothness and clustering assumptions have been used in estimating p, and thus semisupervised learning. [sent-160, score-0.438]
</p><p>41 2 Nonconvex Minimization This section develops a nonconvex minimization method based on DC programming (An and Tao, 1997) for (2) with the ψ-loss, which was previously employed in Liu, Shen and Wong (2005) for supervised ψ-learning. [sent-165, score-0.153]
</p><p>42 They are SVM (with labeled data alone), TSVM (TSVMDCA ; Wang, Shen and Pan, 2007) , and the methods of Wang and Shen (2007) with the hinge loss (SSVM) and with the ψloss (SPSI), where SSVM and SPSI compare favorably against their competitors. [sent-207, score-0.114]
</p><p>43 Corresponding to these methods, our method, with m = n1/2 and ε = 10−3 , yields four semisupervised classiﬁers denoted as ESVM, ETSVM, ESSVM and ESPSI. [sent-208, score-0.261]
</p><p>44 1 S IMULATED E XAMPLES Examples 1 and 2 are taken from Wang and Shen (2007), where 200 and 800 labeled instances are randomly selected for training and testing. [sent-211, score-0.116]
</p><p>45 Instances in WBC, PIMA, HEART, and MUSHROOM are randomly divided into two halves with 10 labeled and 190 unlabeled instances for training, and the remaining 400 for testing. [sent-222, score-0.313]
</p><p>46 Instances in SPAM are randomly divided into halves with 20 labeled and 380 unlabeled instances for training, and the remaining instances for testing. [sent-223, score-0.351]
</p><p>47 html, with 10 labeled and 390 unlabeled instances while no instance for testing. [sent-229, score-0.313]
</p><p>48 An averaged error rate over the unlabeled set is used in BCI example to approximate the test error. [sent-230, score-0.228]
</p><p>49 SVMc denotes the performance of SVM with complete labeled data. [sent-423, score-0.107]
</p><p>50 This is the case for ESVM with linear kernel in SPAM, where ESVM performs worse than SVM with nl = 10 labeled data alone. [sent-610, score-0.546]
</p><p>51 This suggests that a better initial estimate should be used together with unlabeled data. [sent-611, score-0.242]
</p><p>52 Moreover, ESPSI nearly recovers the classiﬁcation performance of its counterpart SVM with complete labeled data in the two simulated examples, WBC and HEART. [sent-613, score-0.179]
</p><p>53 2 Gene Function Prediction Through Expression Proﬁles This section applies the proposed method to predict gene functions through gene data in Hughes et al. [sent-615, score-0.154]
</p><p>54 The error rate is computed on the unlabeled data and averaged over twelve splits. [sent-618, score-0.228]
</p><p>55 In this case almost half of the genes have unknown functions although gene expression proﬁles are available for almost the entire yeast genome. [sent-622, score-0.168]
</p><p>56 The goal is to predict gene functional categories for genes annotated within these two categories by training our semisupervised classiﬁer on expression proﬁles of genes, where some genes are treated as if their functions are unknown to mimic the semisupervised scenario in complete dataset. [sent-626, score-0.75]
</p><p>57 The training set involves a random sample of nl = 20 labeled and nu = 380 unlabeled gene proﬁles, while the testing set contains 280 remaining proﬁles. [sent-629, score-0.886]
</p><p>58 5%  Table 3: Averaged test errors as well as estimated standard errors (in parenthesis) of ESVM, ETSVM, ESSVM, ESPSI, and their initial counterparts, over 100 pairs of training and testing samples, in gene function prediction. [sent-672, score-0.147]
</p><p>59 Here I stands for an initial classiﬁer, E stands for our proposed method with the initial method, and the amount of improvement is deﬁned in (6). [sent-673, score-0.124]
</p><p>60 Statistical Learning Theory In the literature, several theories have been developed to understand the problem of semisupervised learning, including Rigollet (2007) and Singh, Nowak and Zhu (2008). [sent-678, score-0.261]
</p><p>61 Both the theories rely on a different clustering assumption that homogeneous labels are assumed over local clusters. [sent-679, score-0.133]
</p><p>62 Based on the original clustering assumption, as well as a smoothness assumption on the conditional probability p(x), this section develops a novel statistical learning theory. [sent-680, score-0.269]
</p><p>63 5 ) are expressed in terms of complexity of the candidate class F , the (0) sample size n, tuning parameter λ = (nC)−1 , the error rate of the initial classiﬁer δn , and the maximum number of iteration K in Algorithm 1. [sent-688, score-0.108]
</p><p>64 The results imply that ESPSI, without knowing labels of the unlabeled data, enables to recover the classiﬁcation accuracy of ψ-learning based on complete data under regularity conditions. [sent-689, score-0.226]
</p><p>65 Deﬁne the margin loss Vπ ( f , Z) for unequal cost classiﬁcation to be Sπ (y)L(y f (x)), with cost 0 < π < 1 for the positive class and Sπ (y) = 1 − π if y = 1, and π otherwise. [sent-692, score-0.172]
</p><p>66 5 )≤δ}  sup { f ∈F :eVπ ( f , f¯π )≤δ}  sup { f ∈F :eVπ ( f , f¯π )≤δ}  e( f , f¯. [sent-700, score-0.118]
</p><p>67 (9)  Assumption B describes local smoothness of the Bayesian regret e( f , f¯. [sent-702, score-0.119]
</p><p>68 5 {βπ } as β and γ respectively, where β quantiﬁes the clustering assumption through the degree to which the positive and negative clusters are distinguishable, and γ measures the conversion rate between the classiﬁcation and probability estimation accuracies. [sent-709, score-0.164]
</p><p>69 Given any ε > 0, denote {( frl , fru )}R as an ε-bracketing function r=1 set of F if for any f ∈ F , there exists an r such that frl ≤ f ≤ fru and frl − fru 2 ≤ ε; r = 1, · · · , R. [sent-711, score-0.258]
</p><p>70 5 ), where the parameter B measures the level of difﬁculty of a semisupervised problem, with small value of B indicating more difﬁculty. [sent-736, score-0.261]
</p><p>71 In fact, α, β and γ quantify the local smoothness of the Bayesian regret e( f , f¯. [sent-738, score-0.119]
</p><p>72 Next, by letting nl , nu tending inﬁnity, we obtain the rates of convergence of ESPSI in terms of the error rate δ2α of its supervised counterpart ψ-learning based on complete data, and the initial n (0) error rate δn , B, and the maximum number K of iteration. [sent-740, score-0.761]
</p><p>73 Corollary 4 Under the assumptions of Theorem 3, as nu , nl → ∞, (0)  K  |e( fˆC , f¯. [sent-741, score-0.534]
</p><p>74 5 ) ≥ 2a11 ρn (δn )2 → 0, with any (0)  slow varying sequence ρn → ∞ and ρn δn → 0, and the tuning parameter λ is chosen such that ∗ ∗ n(λJπ )max(1,2−ζ) and nl (λJ. [sent-745, score-0.469]
</p><p>75 When B > 1, ESPSI achieves the convergence rate δ2α of its supervised counterpart ψ-learning based on complete data, c. [sent-748, score-0.151]
</p><p>76 Corollary 5 (Optimality) Under the assumptions of Corollary 4, as nu , nl → ∞, ˆ sup U( f ) −U( f ) f ∈F  = O p max(δβγ , (ρn δn )βγ max(1,B ) ) , n (0)  1  K  ˆ where U( f ) is estimated U( f ) loss with p estimated based on fˆC . [sent-757, score-0.679]
</p><p>77 ˆ To argue that the approximation error rate of U( f ) to U( f ) is sufﬁciently small, note that fˆC obtained from minimizing (2) recovers the classiﬁcation error rate of its supervised counterpart based on complete data, as suggested by Corollary 4. [sent-758, score-0.208]
</p><p>78 In conclusion, ESPSI, without knowing label values of unlabeled instances, enables to reconstruct the classiﬁcation and estimation performance of ψ-learning based on complete data in rates of convergence, when possible. [sent-760, score-0.226]
</p><p>79 In all cases, ESPSI (nearly) achieves the generalization error rates of ψ-learning for complete data when unlabeled data provides useful information, and yields no worse performance then its initial classiﬁer otherwise. [sent-763, score-0.271]
</p><p>80 It is evident that the clustering assumption (Assumption B) is met since the neighborhood of f. [sent-769, score-0.17]
</p><p>81 5 (x) has low density as showed in the left panel of Figure 3, and the smoothness assumption (Assumption D) and the boundedness assumption of p(x) (Assumption E) are met as well since p(x) is a hyperplane bounded by (0. [sent-770, score-0.195]
</p><p>82 (0) For ESPSI fˆC , we choose δn = n−1 log nl , the convergence rate of supervised linear ψ-learning, l ρn → ∞ to be an arbitrarily slow sequence and C = O((log n)−1 ). [sent-778, score-0.551]
</p><p>83 5 )| = O(max(n−1 log n, (n−1 (log nl )2 )max(1,2B ) )), with B = 2κ2(1+κ1 ) 2 ) . [sent-780, score-0.475]
</p><p>84 (0)  Similarly, we choose δn = n−1 (log nl )3 to be the convergence rate of supervised ψ-learning l with Gaussian kernel, ρn → ∞ to be an arbitrarily slow sequence and C = O((log n)−3 ). [sent-789, score-0.513]
</p><p>85 5 )| = O(max((n−1 (log nl )3 )max(1,2B ) , n−1 (log n)3 )) = O(n−1 (log n)3 ) when κ1 + 1 > l 2κ2 (1 + κ2 ) and K is sufﬁciently large, and O(n−1 (log nl )3 ) otherwise. [sent-791, score-0.874]
</p><p>86 Summary This article introduces a large margin semisupervised learning method through an iterative scheme based on an efﬁcient loss for unlabeled data. [sent-794, score-0.656]
</p><p>87 In contrast to most methods assuming a relationship between the conditional and the marginal distributions, the proposed method integrates labeled and unlabeled data through using the clustering structure of unlabeled data as well as the smoothness structure of the estimated p. [sent-795, score-0.7]
</p><p>88 The theoretical and numerical results suggest that the method compares favorably against top competitors, and achieves the desired goal of reconstructing the classiﬁcation performance of its supervised counterpart on complete labeled data. [sent-796, score-0.241]
</p><p>89 One critical issue is how to use unlabeled data to enhance the accuracy of estimating the generalization error so that adaptive tuning is possible. [sent-798, score-0.229]
</p><p>90 In this proof, we denote labeled and unlabeled samples by {(Xi ,Yi )}nl and {X j }n l +1 to indicate that they are all random variables. [sent-815, score-0.275]
</p><p>91 j=n i=1 Step 1: First we bound the probability of the percentage of wrongly labeled unlabeled instances by sign( fˆ(k) ) by the tail probability of eV. [sent-816, score-0.313]
</p><p>92 5 (X j )); nl + 1 ≤ j ≤ n} to be the set of unlabeled data that are wrongly labeled by sign( fˆ(k) ), n with n f = #{D f } being its cardinality. [sent-820, score-0.712]
</p><p>93 According to Markov’s inequality, the fact that E( nf ) = nu ˆ(k+1) ) − sign( f¯. [sent-821, score-0.194]
</p><p>94 Let Vπ ( f , Z) = Vπ ( f , Z) + λJ( f ), and Z j = (X j ,Y j ) with ˆ(k) (X j )); nl + 1 ≤ j ≤ n. [sent-832, score-0.437]
</p><p>95 n−1 ∑i∈D + ∑i∈D / f  f  (k) By the deﬁnition of fˆπ and (11), (k)  P eVπ ( fˆπ , f¯π ) ≥ δ2 k  nf (k) ≥ a1 (a11 ρ2 (δn )2 )β + n n nf 1 n ˜ ∗ (k) ˜ P∗ sup ∑ (Vπ ( fπ , Zi ) − Vπ ( f , Zi )) ≥ 0, ≤ a1 (a11 ρ2 (δn )2 )β n n Nk n i=1  ≤ P  (k) ≤ P eV. [sent-834, score-0.253]
</p><p>96 loss of generality, assume that 4sn < δk π π π π ∗ ) ≥ e ( f , f ∗ ) − n f E|V ( f , Z) − V ( f ∗ , Z) + V ( f , Z) − ¯π For the ﬁrst moment, note that ∇( f , fπ Vπ π π π π n n ∗ ∗ ¯ ∗ ¯ Vπ ( fπ , Z)| ≥ eVπ ( f , fπ ) − 4 nf with Vπ ( f , z) = Sπ (−y)L(−y f (x)). [sent-841, score-0.133]
</p><p>97 5 ( f (x)) = 1 (p(x)L( f (x)) + (1 − p(x))L(− f (x))) is 2 2 ˆ (k) ˆ the ideal loss for unlabeled data. [sent-872, score-0.272]
</p><p>98 A framework for learning predictive structures from multiple tasks and unlabeled data. [sent-1070, score-0.197]
</p><p>99 Text classiﬁcation from labeled and unlabeled documents using EM. [sent-1247, score-0.275]
</p><p>100 A probability analysis on the value of unlabeled data for classiﬁcation problems. [sent-1369, score-0.197]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nl', 0.437), ('espsi', 0.272), ('semisupervised', 0.261), ('shen', 0.26), ('unlabeled', 0.197), ('pan', 0.194), ('wang', 0.186), ('esvm', 0.172), ('sign', 0.169), ('ev', 0.16), ('emisupervised', 0.157), ('essvm', 0.114), ('etsvm', 0.114), ('arge', 0.11), ('fficient', 0.11), ('spsi', 0.1), ('hen', 0.099), ('nu', 0.097), ('nf', 0.097), ('argin', 0.096), ('clustering', 0.095), ('margin', 0.087), ('spam', 0.086), ('ssvm', 0.086), ('tsvm', 0.086), ('smoothness', 0.082), ('labeled', 0.078), ('gene', 0.077), ('pima', 0.076), ('mushroom', 0.073), ('wbc', 0.073), ('bci', 0.071), ('zi', 0.062), ('genes', 0.061), ('sup', 0.059), ('chapelle', 0.058), ('zhu', 0.058), ('var', 0.057), ('yarowsky', 0.057), ('max', 0.056), ('en', 0.055), ('hb', 0.054), ('nonconvex', 0.054), ('develops', 0.054), ('bayes', 0.054), ('classi', 0.053), ('nk', 0.049), ('unequal', 0.049), ('ge', 0.049), ('zien', 0.049), ('boundary', 0.048), ('earning', 0.047), ('counterpart', 0.046), ('initial', 0.045), ('supervised', 0.045), ('pro', 0.044), ('frl', 0.043), ('fru', 0.043), ('reconstructing', 0.043), ('svmc', 0.043), ('iterative', 0.042), ('heart', 0.041), ('exp', 0.041), ('corollary', 0.04), ('ideal', 0.039), ('dc', 0.039), ('favorable', 0.039), ('assumption', 0.038), ('instances', 0.038), ('log', 0.038), ('regret', 0.037), ('met', 0.037), ('parenthesis', 0.036), ('loss', 0.036), ('les', 0.035), ('improvement', 0.034), ('svm', 0.033), ('sn', 0.033), ('article', 0.033), ('tuning', 0.032), ('liu', 0.031), ('rate', 0.031), ('kernel', 0.031), ('mitchell', 0.03), ('yeast', 0.03), ('blum', 0.03), ('complete', 0.029), ('junhui', 0.029), ('kao', 0.029), ('mason', 0.029), ('mewes', 0.029), ('mips', 0.029), ('min', 0.027), ('el', 0.026), ('deferred', 0.026), ('distributional', 0.026), ('recovers', 0.026), ('marginal', 0.026), ('er', 0.025), ('estimated', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="63-tfidf-1" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>Author: Junhui Wang, Xiaotong Shen, Wei Pan</p><p>Abstract: In classiﬁcation, semisupervised learning usually involves a large amount of unlabeled data with only a small number of labeled data. This imposes a great challenge in that it is difﬁcult to achieve good classiﬁcation performance through labeled data alone. To leverage unlabeled data for enhancing classiﬁcation, this article introduces a large margin semisupervised learning method within the framework of regularization, based on an efﬁcient margin loss for unlabeled data, which seeks efﬁcient extraction of the information from unlabeled data for estimating the Bayes decision boundary for classiﬁcation. For implementation, an iterative scheme is derived through conditional expectations. Finally, theoretical and numerical analyses are conducted, in addition to an application to gene function prediction. They suggest that the proposed method enables to recover the performance of its supervised counterpart based on complete data in rates of convergence, when possible. Keywords: difference convex programming, classiﬁcation, nonconvex minimization, regularization, support vectors</p><p>2 0.075727664 <a title="63-tfidf-2" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>Author: Steffen Bickel, Michael Brückner, Tobias Scheffer</p><p>Abstract: We address classiﬁcation problems for which the training instances are governed by an input distribution that is allowed to differ arbitrarily from the test distribution—problems also referred to as classiﬁcation under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. The problem of learning under covariate shift can be written as an integrated optimization problem. Instantiating the general optimization problem leads to a kernel logistic regression and an exponential model classiﬁer for covariate shift. The optimization problem is convex under certain conditions; our ﬁndings also clarify the relationship to the known kernel mean matching procedure. We report on experiments on problems of spam ﬁltering, text classiﬁcation, and landmine detection. Keywords: covariate shift, discriminative learning, transfer learning</p><p>3 0.058630645 <a title="63-tfidf-3" href="./jmlr-2009-Estimating_Labels_from_Label_Proportions.html">29 jmlr-2009-Estimating Labels from Label Proportions</a></p>
<p>Author: Novi Quadrianto, Alex J. Smola, Tibério S. Caetano, Quoc V. Le</p><p>Abstract: Consider the following problem: given sets of unlabeled observations, each set with known label proportions, predict the labels of another set of observations, possibly with known label proportions. This problem occurs in areas like e-commerce, politics, spam ﬁltering and improper content detection. We present consistent estimators which can reconstruct the correct labels with high probability in a uniform convergence sense. Experiments show that our method works well in practice. Keywords: unsupervised learning, Gaussian processes, classiﬁcation and prediction, probabilistic models, missing variables</p><p>4 0.054050684 <a title="63-tfidf-4" href="./jmlr-2009-Entropy_Inference_and_the_James-Stein_Estimator%2C_with_Application_to_Nonlinear_Gene_Association_Networks.html">28 jmlr-2009-Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks</a></p>
<p>Author: Jean Hausser, Korbinian Strimmer</p><p>Abstract: We present a procedure for effective estimation of entropy and mutual information from smallsample data, and apply it to the problem of inferring high-dimensional gene association networks. SpeciÄ?Ĺš cally, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efÄ?Ĺš cient statistically as well as computationally. Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling. We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data. A computer program is available that implements the proposed shrinkage estimator. Keywords: entropy, shrinkage estimation, James-Stein estimator, Ă˘&euro;&oelig;small n, large pĂ˘&euro;? setting, mutual information, gene association network</p><p>5 0.050545104 <a title="63-tfidf-5" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>Author: Christian Rieger, Barbara Zwicknagl</p><p>Abstract: We introduce a new technique for the analysis of kernel-based regression problems. The basic tools are sampling inequalities which apply to all machine learning problems involving penalty terms induced by kernels related to Sobolev spaces. They lead to explicit deterministic results concerning the worst case behaviour of ε- and ν-SVRs. Using these, we show how to adjust regularization parameters to get best possible approximation orders for regression. The results are illustrated by some numerical examples. Keywords: sampling inequality, radial basis functions, approximation theory, reproducing kernel Hilbert space, Sobolev space</p><p>6 0.048974618 <a title="63-tfidf-6" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>7 0.048467975 <a title="63-tfidf-7" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>8 0.047482874 <a title="63-tfidf-8" href="./jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">16 jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<p>9 0.04492379 <a title="63-tfidf-9" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>10 0.044390637 <a title="63-tfidf-10" href="./jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>11 0.043769479 <a title="63-tfidf-11" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>12 0.042675771 <a title="63-tfidf-12" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>13 0.042388145 <a title="63-tfidf-13" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>14 0.042019814 <a title="63-tfidf-14" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>15 0.041845109 <a title="63-tfidf-15" href="./jmlr-2009-Using_Local_Dependencies_within_Batches_to_Improve_Large_Margin_Classifiers.html">99 jmlr-2009-Using Local Dependencies within Batches to Improve Large Margin Classifiers</a></p>
<p>16 0.04117012 <a title="63-tfidf-16" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>17 0.040875968 <a title="63-tfidf-17" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>18 0.039771322 <a title="63-tfidf-18" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>19 0.039622951 <a title="63-tfidf-19" href="./jmlr-2009-Nearest_Neighbor_Clustering%3A_A_Baseline_Method_for_Consistent_Clustering_with_Arbitrary_Objective_Functions.html">59 jmlr-2009-Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions</a></p>
<p>20 0.039433617 <a title="63-tfidf-20" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.203), (1, -0.039), (2, 0.102), (3, -0.025), (4, -0.023), (5, -0.003), (6, 0.011), (7, 0.038), (8, -0.014), (9, 0.119), (10, 0.116), (11, -0.03), (12, -0.072), (13, -0.055), (14, 0.123), (15, -0.15), (16, 0.102), (17, 0.023), (18, 0.113), (19, -0.143), (20, -0.13), (21, 0.052), (22, -0.036), (23, -0.021), (24, 0.0), (25, 0.052), (26, -0.042), (27, 0.028), (28, -0.016), (29, 0.021), (30, -0.086), (31, 0.035), (32, -0.184), (33, -0.208), (34, 0.113), (35, -0.102), (36, -0.13), (37, -0.048), (38, 0.041), (39, 0.183), (40, -0.117), (41, 0.008), (42, -0.159), (43, -0.041), (44, -0.144), (45, 0.211), (46, -0.087), (47, -0.209), (48, -0.15), (49, -0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9316048 <a title="63-lsi-1" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>Author: Junhui Wang, Xiaotong Shen, Wei Pan</p><p>Abstract: In classiﬁcation, semisupervised learning usually involves a large amount of unlabeled data with only a small number of labeled data. This imposes a great challenge in that it is difﬁcult to achieve good classiﬁcation performance through labeled data alone. To leverage unlabeled data for enhancing classiﬁcation, this article introduces a large margin semisupervised learning method within the framework of regularization, based on an efﬁcient margin loss for unlabeled data, which seeks efﬁcient extraction of the information from unlabeled data for estimating the Bayes decision boundary for classiﬁcation. For implementation, an iterative scheme is derived through conditional expectations. Finally, theoretical and numerical analyses are conducted, in addition to an application to gene function prediction. They suggest that the proposed method enables to recover the performance of its supervised counterpart based on complete data in rates of convergence, when possible. Keywords: difference convex programming, classiﬁcation, nonconvex minimization, regularization, support vectors</p><p>2 0.40230134 <a title="63-lsi-2" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>Author: Christian Rieger, Barbara Zwicknagl</p><p>Abstract: We introduce a new technique for the analysis of kernel-based regression problems. The basic tools are sampling inequalities which apply to all machine learning problems involving penalty terms induced by kernels related to Sobolev spaces. They lead to explicit deterministic results concerning the worst case behaviour of ε- and ν-SVRs. Using these, we show how to adjust regularization parameters to get best possible approximation orders for regression. The results are illustrated by some numerical examples. Keywords: sampling inequality, radial basis functions, approximation theory, reproducing kernel Hilbert space, Sobolev space</p><p>3 0.39291608 <a title="63-lsi-3" href="./jmlr-2009-Entropy_Inference_and_the_James-Stein_Estimator%2C_with_Application_to_Nonlinear_Gene_Association_Networks.html">28 jmlr-2009-Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks</a></p>
<p>Author: Jean Hausser, Korbinian Strimmer</p><p>Abstract: We present a procedure for effective estimation of entropy and mutual information from smallsample data, and apply it to the problem of inferring high-dimensional gene association networks. SpeciÄ?Ĺš cally, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efÄ?Ĺš cient statistically as well as computationally. Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling. We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data. A computer program is available that implements the proposed shrinkage estimator. Keywords: entropy, shrinkage estimation, James-Stein estimator, Ă˘&euro;&oelig;small n, large pĂ˘&euro;? setting, mutual information, gene association network</p><p>4 0.33075082 <a title="63-lsi-4" href="./jmlr-2009-Using_Local_Dependencies_within_Batches_to_Improve_Large_Margin_Classifiers.html">99 jmlr-2009-Using Local Dependencies within Batches to Improve Large Margin Classifiers</a></p>
<p>Author: Volkan Vural, Glenn Fung, Balaji Krishnapuram, Jennifer G. Dy, Bharat Rao</p><p>Abstract: Most classiﬁcation methods assume that the samples are drawn independently and identically from an unknown data generating distribution, yet this assumption is violated in several real life problems. In order to relax this assumption, we consider the case where batches or groups of samples may have internal correlations, whereas the samples from different batches may be considered to be uncorrelated. This paper introduces three algorithms for classifying all the samples in a batch jointly: one based on a probabilistic formulation, and two based on mathematical programming analysis. Experiments on three real-life computer aided diagnosis (CAD) problems demonstrate that the proposed algorithms are signiﬁcantly more accurate than a naive support vector machine which ignores the correlations among the samples. Keywords: batch-wise classiﬁcation, support vector machine, linear programming, machine learning, statistical methods, unconstrained optimization</p><p>5 0.3177121 <a title="63-lsi-5" href="./jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">16 jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<p>Author: Dao-Hong Xiang, Ding-Xuan Zhou</p><p>Abstract: This paper considers binary classiﬁcation algorithms generated from Tikhonov regularization schemes associated with general convex loss functions and varying Gaussian kernels. Our main goal is to provide fast convergence rates for the excess misclassiﬁcation error. Allowing varying Gaussian kernels in the algorithms improves learning rates measured by regularization error and sample error. Special structures of Gaussian kernels enable us to construct, by a nice approximation scheme with a Fourier analysis technique, uniformly bounded regularizing functions achieving polynomial decays of the regularization error under a Sobolev smoothness condition. The sample error is estimated by using a projection operator and a tight bound for the covering numbers of reproducing kernel Hilbert spaces generated by Gaussian kernels. The convexity of the general loss function plays a very important role in our analysis. Keywords: reproducing kernel Hilbert space, binary classiﬁcation, general convex loss, varying Gaussian kernels, covering number, approximation</p><p>6 0.31256649 <a title="63-lsi-6" href="./jmlr-2009-Estimating_Labels_from_Label_Proportions.html">29 jmlr-2009-Estimating Labels from Label Proportions</a></p>
<p>7 0.30340451 <a title="63-lsi-7" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>8 0.30203596 <a title="63-lsi-8" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>9 0.29925165 <a title="63-lsi-9" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>10 0.27679071 <a title="63-lsi-10" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>11 0.26456395 <a title="63-lsi-11" href="./jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>12 0.24758497 <a title="63-lsi-12" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>13 0.23923789 <a title="63-lsi-13" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>14 0.23499216 <a title="63-lsi-14" href="./jmlr-2009-Nearest_Neighbor_Clustering%3A_A_Baseline_Method_for_Consistent_Clustering_with_Arbitrary_Objective_Functions.html">59 jmlr-2009-Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions</a></p>
<p>15 0.23184809 <a title="63-lsi-15" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>16 0.23086913 <a title="63-lsi-16" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>17 0.23026724 <a title="63-lsi-17" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>18 0.2277666 <a title="63-lsi-18" href="./jmlr-2009-Bi-Level_Path_Following_for_Cross_Validated_Solution_of_Kernel_Quantile_Regression.html">12 jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</a></p>
<p>19 0.21836983 <a title="63-lsi-19" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>20 0.21560974 <a title="63-lsi-20" href="./jmlr-2009-A_Parameter-Free_Classification_Method_for_Large_Scale_Learning.html">3 jmlr-2009-A Parameter-Free Classification Method for Large Scale Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.017), (11, 0.022), (38, 0.029), (47, 0.012), (52, 0.052), (55, 0.579), (58, 0.03), (66, 0.093), (68, 0.013), (90, 0.041), (96, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95276833 <a title="63-lda-1" href="./jmlr-2009-On_Uniform_Deviations_of_General_Empirical_Risks_with_Unboundedness%2C_Dependence%2C_and_High_Dimensionality.html">65 jmlr-2009-On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality</a></p>
<p>Author: Wenxin Jiang</p><p>Abstract: The statistical learning theory of risk minimization depends heavily on probability bounds for uniform deviations of the empirical risks. Classical probability bounds using Hoeffding’s inequality cannot accommodate more general situations with unbounded loss and dependent data. The current paper introduces an inequality that extends Hoeffding’s inequality to handle these more general situations. We will apply this inequality to provide probability bounds for uniform deviations in a very general framework, which can involve discrete decision rules, unbounded loss, and a dependence structure that can be more general than either martingale or strong mixing. We will consider two examples with high dimensional predictors: autoregression (AR) with ℓ1 -loss, and ARX model with variable selection for sign classiﬁcation, which uses both lagged responses and exogenous predictors. Keywords: dependence, empirical risk, probability bound, unbounded loss, uniform deviation</p><p>2 0.94195849 <a title="63-lda-2" href="./jmlr-2009-Identification_of_Recurrent_Neural_Networks_by_Bayesian_Interrogation_Techniques.html">40 jmlr-2009-Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques</a></p>
<p>Author: Barnabás Póczos, András Lőrincz</p><p>Abstract: We introduce novel online Bayesian methods for the identiﬁcation of a family of noisy recurrent neural networks (RNNs). We present Bayesian active learning techniques for stimulus selection given past experiences. In particular, we consider the unknown parameters as stochastic variables and use A-optimality and D-optimality principles to choose optimal stimuli. We derive myopic cost functions in order to maximize the information gain concerning network parameters at each time step. We also derive the A-optimal and D-optimal estimations of the additive noise that perturbs the dynamical system of the RNN. Here we investigate myopic as well as non-myopic estimations, and study the problem of simultaneous estimation of both the system parameters and the noise. Employing conjugate priors our derivations remain approximation-free and give rise to simple update rules for the online learning of the parameters. The efﬁciency of our method is demonstrated for a number of selected cases, including the task of controlled independent component analysis. Keywords: active learning, system identiﬁcation, online Bayesian learning, A-optimality, Doptimality, infomax control, optimal design</p><p>same-paper 3 0.92639303 <a title="63-lda-3" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>Author: Junhui Wang, Xiaotong Shen, Wei Pan</p><p>Abstract: In classiﬁcation, semisupervised learning usually involves a large amount of unlabeled data with only a small number of labeled data. This imposes a great challenge in that it is difﬁcult to achieve good classiﬁcation performance through labeled data alone. To leverage unlabeled data for enhancing classiﬁcation, this article introduces a large margin semisupervised learning method within the framework of regularization, based on an efﬁcient margin loss for unlabeled data, which seeks efﬁcient extraction of the information from unlabeled data for estimating the Bayes decision boundary for classiﬁcation. For implementation, an iterative scheme is derived through conditional expectations. Finally, theoretical and numerical analyses are conducted, in addition to an application to gene function prediction. They suggest that the proposed method enables to recover the performance of its supervised counterpart based on complete data in rates of convergence, when possible. Keywords: difference convex programming, classiﬁcation, nonconvex minimization, regularization, support vectors</p><p>4 0.51866078 <a title="63-lda-4" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>Author: Sanjoy Dasgupta, Adam Tauman Kalai, Claire Monteleoni</p><p>Abstract: We start by showing that in an active learning setting, the Perceptron algorithm needs Ω( ε12 ) labels to learn linear separators within generalization error ε. We then present a simple active learning algorithm for this problem, which combines a modiﬁcation of the Perceptron update with an adaptive ﬁltering rule for deciding which points to query. For data distributed uniformly over the unit ˜ sphere, we show that our algorithm reaches generalization error ε after asking for just O(d log 1 ) ε labels. This exponential improvement over the usual sample complexity of supervised learning had previously been demonstrated only for the computationally more complex query-by-committee algorithm. Keywords: active learning, perceptron, label complexity bounds, online learning</p><p>5 0.49771515 <a title="63-lda-5" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>Author: Tong Zhang</p><p>Abstract: This paper studies the feature selection problem using a greedy least squares regression algorithm. We show that under a certain irrepresentable condition on the design matrix (but independent of the sparse target), the greedy algorithm can select features consistently when the sample size approaches inﬁnity. The condition is identical to a corresponding condition for Lasso. Moreover, under a sparse eigenvalue condition, the greedy algorithm can reliably identify features as long as each nonzero coefﬁcient is larger than a constant times the noise level. In compar√ ison, Lasso may require the coefﬁcients to be larger than O( s) times the noise level in the worst case, where s is the number of nonzero coefﬁcients. Keywords: greedy algorithm, feature selection, sparsity</p><p>6 0.47776544 <a title="63-lda-6" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>7 0.4659251 <a title="63-lda-7" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>8 0.45314217 <a title="63-lda-8" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>9 0.44667438 <a title="63-lda-9" href="./jmlr-2009-Adaptive_False_Discovery_Rate_Control_under_Independence_and_Dependence.html">5 jmlr-2009-Adaptive False Discovery Rate Control under Independence and Dependence</a></p>
<p>10 0.44583318 <a title="63-lda-10" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>11 0.4445661 <a title="63-lda-11" href="./jmlr-2009-Entropy_Inference_and_the_James-Stein_Estimator%2C_with_Application_to_Nonlinear_Gene_Association_Networks.html">28 jmlr-2009-Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks</a></p>
<p>12 0.44355103 <a title="63-lda-12" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>13 0.42671865 <a title="63-lda-13" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>14 0.42392188 <a title="63-lda-14" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>15 0.42355105 <a title="63-lda-15" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>16 0.42058963 <a title="63-lda-16" href="./jmlr-2009-A_Least-squares_Approach_to_Direct_Importance_Estimation.html">1 jmlr-2009-A Least-squares Approach to Direct Importance Estimation</a></p>
<p>17 0.41562781 <a title="63-lda-17" href="./jmlr-2009-Strong_Limit_Theorems_for_the_Bayesian_Scoring_Criterion_in_Bayesian_Networks.html">89 jmlr-2009-Strong Limit Theorems for the Bayesian Scoring Criterion in Bayesian Networks</a></p>
<p>18 0.41021627 <a title="63-lda-18" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>19 0.40859625 <a title="63-lda-19" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>20 0.40157643 <a title="63-lda-20" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
