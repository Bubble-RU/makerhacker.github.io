<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-66" href="#">jmlr2009-66</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</h1>
<br/><p>Source: <a title="jmlr-2009-66-pdf" href="http://jmlr.org/papers/volume10/zhang09a/zhang09a.pdf">pdf</a></p><p>Author: Tong Zhang</p><p>Abstract: This paper studies the feature selection problem using a greedy least squares regression algorithm. We show that under a certain irrepresentable condition on the design matrix (but independent of the sparse target), the greedy algorithm can select features consistently when the sample size approaches inﬁnity. The condition is identical to a corresponding condition for Lasso. Moreover, under a sparse eigenvalue condition, the greedy algorithm can reliably identify features as long as each nonzero coefﬁcient is larger than a constant times the noise level. In compar√ ison, Lasso may require the coefﬁcients to be larger than O( s) times the noise level in the worst case, where s is the number of nonzero coefﬁcients. Keywords: greedy algorithm, feature selection, sparsity</p><p>Reference: <a title="jmlr-2009-66-reference" href="../jmlr2009_reference/jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Statistics Department 110 Frelinghuysen Road Rutgers University, NJ 08854  Editor: Bin Yu  Abstract This paper studies the feature selection problem using a greedy least squares regression algorithm. [sent-3, score-0.532]
</p><p>2 We show that under a certain irrepresentable condition on the design matrix (but independent of the sparse target), the greedy algorithm can select features consistently when the sample size approaches inﬁnity. [sent-4, score-0.735]
</p><p>3 The condition is identical to a corresponding condition for Lasso. [sent-5, score-0.096]
</p><p>4 Moreover, under a sparse eigenvalue condition, the greedy algorithm can reliably identify features as long as each nonzero coefﬁcient is larger than a constant times the noise level. [sent-6, score-0.517]
</p><p>5 In compar√ ison, Lasso may require the coefﬁcients to be larger than O( s) times the noise level in the worst case, where s is the number of nonzero coefﬁcients. [sent-7, score-0.111]
</p><p>6 Introduction We are interested in the statistical feature selection problem for least squares regression. [sent-9, score-0.211]
</p><p>7 , yn ] ∈ Rn is generated from a sparse linear combination of the basis vectors {x j } plus a zero-mean stochastic noise vector z ∈ Rn : ¯ y = Xβ + z =  d  ¯ ∑ β j x j + z,  (1)  j=1  ¯ where most coefﬁcients β j equal zero. [sent-20, score-0.117]
</p><p>8 The goal of feature selection is to identify the set of non¯ j = 0}, where β = [β1 , . [sent-21, score-0.089]
</p><p>9 The purpose of this paper is study the performance of ¯ ¯ ¯ zeros { j : β greedy least squares regression for feature selection. [sent-25, score-0.504]
</p><p>10 Z HANG  ˆ ¯ ¯ That is, βX (F, x) is the least squares solution with coefﬁcients restricted to F. [sent-35, score-0.146]
</p><p>11 ¯ following quantity, appeared in Tropp (2004), is important in our analysis (also see Wainwright, 2006): T T ¯ µX (F) = max (XF XF )−1 XF x j 1 . [sent-41, score-0.081]
</p><p>12 1 T This quantity is the smallest eigenvalue of the restricted design matrix n XF XF , which has also ¯ ¯ appeared in previous work such as Wainwright (2006) and Zhao and Yu (2006). [sent-46, score-0.092]
</p><p>13 The requirement that ρX (F) is bounded away from zero for small |F| is often referred to as the sparse eigenvalue condition (or the restricted isometry condition). [sent-47, score-0.146]
</p><p>14 One of the frequently used method for feature selection is Lasso, which solves the following L1 regularization problem:   2 1 d ˆ β = arg min  (2) ∑ β j x j − y + λ β 1 , n j=1 β 2  where λ > 0 is an appropriately chosen regularization parameter. [sent-50, score-0.085]
</p><p>15 The effectiveness of feature selection using Lasso was established in Zhao and Yu (2006) (also see Meinshausen and Buhlmann, 2006) under irrepresentable conditions that depend on the signs of ¯ the true target sgn(β). [sent-51, score-0.488]
</p><p>16 Results established in this paper have cruder forms that depend only on the ¯ design matrix but not the sparse target β. [sent-52, score-0.135]
</p><p>17 In addition to Lasso, greedy algorithms have also been widely used for feature selection. [sent-54, score-0.316]
</p><p>18 Greedy algorithms for least squares regression are called matching pursuit in the signal processing community (Mallat and Zhang, 1993). [sent-55, score-0.249]
</p><p>19 The algorithm is often called forward greedy selection in the machine learning literature. [sent-57, score-0.351]
</p><p>20 This paper investigates the behavior of greedy least squares algorithm in Figure 1 for feature selection under the stochastic noise model (1). [sent-58, score-0.554]
</p><p>21 It was shown by Tropp (2004) that µX (F) < 1 is ¯ when the noise vector sufﬁcient for the greedy algorithm to identify the correct feature set supp(β) z = 0. [sent-60, score-0.385]
</p><p>22 In particular, we will establish conditions on min j∈supp(β) |β j | and ¯ ¯ the stopping criterion ε in Figure 1, so that the algorithm ﬁnds the correct feature set supp(β). [sent-62, score-0.161]
</p><p>23 The selection of stopping criterion ε in the greedy algorithm is equivalent to selecting an appropriate 556  F EATURE S ELECTION USING G REEDY L EAST S QUARES  Input: X = [x1 , . [sent-63, score-0.361]
</p><p>24 In fact, our result shows that the condition of min j∈supp(β) |β j | required for greedy ¯ algorithm is weaker than the corresponding condition for Lasso. [sent-74, score-0.395]
</p><p>25 The greedy algorithm analysis employed in this paper is a combination of an observation by Tropp (2004, see Lemma 11) and some technical lemmas for the behavior of greedy least squares regression by Zhang (2008), which are included in Appendix A for completeness. [sent-75, score-0.763]
</p><p>26 Note that Zhang (2008) only studied a forward-backward procedure, but not the more standard forward greedy algo¯ rithm considered here. [sent-76, score-0.338]
</p><p>27 ¯ As we shall see in this paper, the condition µX (F) ≤ 1 is necessary for the success of the forward greedy procedure. [sent-78, score-0.371]
</p><p>28 It is worth mentioning that Lasso is consistent in parameter estimation under a ¯ weaker sparse eigenvalue condition, even if the condition µX (F) ≤ 1 fails (which means Lasso may not estimate the true feature set correctly): for example, see Meinshausen and Yu (2008) and Zhang (2009). [sent-79, score-0.166]
</p><p>29 Although similar results may be obtained for greedy least squares regression, when the ¯ condition µX (F) ≤ 1 fails, it was shown by Zhang (2008) that the performance of greedy algorithm can be improved by incorporating backward steps. [sent-80, score-0.77]
</p><p>30 In contrast, results in this paper show that if ¯ the design matrix satisﬁes the additional condition µX (F) < 1, then the standard forward greedy algorithm will be successful without complicated backward steps. [sent-81, score-0.406]
</p><p>31 Feature Selection using Greedy Least Squares Regression We would like to establish conditions under which the forward greedy algorithm in Figure 1 never makes any mistake (with large probability), and thus suitable for feature selection. [sent-83, score-0.41]
</p><p>32 The following theorem gives conditions under which the forward greedy algorithm can identify the correct set of features. [sent-101, score-0.405]
</p><p>33 Theorem 1 Consider the greedy least squares algorithm in Figure 1, where Assumption 1 holds. [sent-102, score-0.425]
</p><p>34 5), with probability larger than 1 − 2η, if the stopping criterion satisﬁes ε>  √ ¯ ¯ min |β j | ≥ 3ερX (F)−1 / n,  1 ¯ σ 2 ln(2d/η), 1 − µX (F)  ¯ j∈F  ¯ then when the procedure stops, we have F (k−1) = F and ¯ β(k−1) − β  ∞  ≤σ  ¯ ¯ (2 ln(2|F|/η))/(nρX (F)). [sent-104, score-0.116]
</p><p>35 Theorem 2 Consider the greedy least squares algorithm in Figure 1, where Assumption 1 holds. [sent-106, score-0.425]
</p><p>36 5), with probability larger than 1 − 2η, if the stopping criterion satisﬁes ε>  1 ¯ σ 2 ln(2d/η), 1 − µX (F)  then when the procedure stops, the following claims are true: ¯ • F (k−1) ⊂ F. [sent-108, score-0.145]
</p><p>37 In such (k−1) selected by the greedy least squares algorithm is approximately correct. [sent-114, score-0.425]
</p><p>38 This implies that when µX (F) < 1, greedy ¯ least squares regression leads to a good estimation of the true parameter β. [sent-121, score-0.495]
</p><p>39 By choosing ε = (k−1) − β ¯ 2 = O( |F|/n + k(ε) ln d/n). [sent-122, score-0.154]
</p><p>40 Therefore, if k(ε) is small, then Lasso is inferior due to the extra ln d factor. [sent-124, score-0.154]
</p><p>41 In this paper, we are mainly interested in the situation k(ε) = 0, which implies that with the stopping criterion ε, greedy least squares regression can correctly identify all features with large ¯ probability. [sent-126, score-0.598]
</p><p>42 This means that under the assumption of Theorem 1, it is possible to identify all features correctly using the greedy least squares algorithm as long as the target ¯ ¯ coefﬁcients β j ( j ∈ F) are larger than the order σ ln(d/η)/n. [sent-129, score-0.573]
</p><p>43 ¯ In fact, since σ ln(d/η)/n is the noise level, if there exists a target coefﬁcient β j that is smaller than O(σ ln(d/η)/n) in absolute value, then we cannot distinguish such a small coefﬁcient from ¯ zero (or noise) with large probability. [sent-130, score-0.109]
</p><p>44 Therefore when the condition µX (F) < 1 holds, it is not possible to do much better than greedy least squares regression except for the constant hidden in ¯ ¯ O(·) and its dependency on ρ(F) and µX (F). [sent-131, score-0.515]
</p><p>45 ¯ ¯ In comparison, for Lasso, the condition required of min j∈F |β j | depends not only on ρ(F)−1 and ¯ −1 , but also on the quantity (X T X )−1 ¯ (see Wainwright, 2006), where (1 − µX (F)) ∞,∞ ¯ ¯ F F T (XF XF )−1 ¯ ¯  ∞,∞  = sup ¯ u∈R|F|  T (XF XF )−1 u ¯ ¯ u ∞  ∞  . [sent-132, score-0.115]
</p><p>46 This is a more restrictive condition ¯ than that of greedy least squares regression. [sent-143, score-0.473]
</p><p>47 For example, for the greedy algorithm, we can show βX (F, y)− β 2 = O(σ |F|/n) ˆ ¯ ¯ ¯ and βX (F, y) − β ∞ = O(σ ln |F|/n). [sent-146, score-0.433]
</p><p>48 Under the conditions of Theorem 1, we have β(k−1) = ˆ X (F, y), and thus β(k−1) − β 2 = O(σ |F|/n) and β(k−1) − β ∞ = O(σ ln |F|/n). [sent-147, score-0.189]
</p><p>49 Under ¯ ¯ ¯ ¯ β ¯ 559  Z HANG  ˆ ˆ ¯ ¯ the same conditions, for the Lasso estimator β of (2), we have β − β 2 = O(σ |F| ln d/n) and ˆ ¯ β − β ∞ = O(σ ln d/n). [sent-148, score-0.308]
</p><p>50 The ln d factor (bias) is inherent to Lasso, which can be removed with two-stage procedures (e. [sent-149, score-0.154]
</p><p>51 However, such procedures are less robust and more complicated than the simple greedy algorithm. [sent-152, score-0.279]
</p><p>52 Feature Selection Consistency As we have mentioned before, the effectiveness of feature selection using Lasso was established by Zhao and Yu (2006), under more reﬁned irrepresentable conditions that depend on the signs of the ¯ ¯ true target sgn(β). [sent-154, score-0.488]
</p><p>53 In comparison, the condition µX (F) < 1 in Theorem 2 depends only on the design ¯ That is, the condition is with respect to the worst case choice of matrix but not the sparse target β. [sent-155, score-0.234]
</p><p>54 Due to the complexity of greedy procedure, we cannot establish a simple target dependent condition that ensures feature selection consistency. [sent-157, score-0.471]
</p><p>55 This means for any speciﬁc target, the behavior of forward greedy algorithm and Lasso might be different, and one may be preferred over the other under different scenarios. [sent-158, score-0.323]
</p><p>56 In the following, we introduce the target independent irrepresentable conditions that are equiv¯ alent to the irrepresentable conditions of Zhao and Yu (2006) with the worst case choice of sgn(β) (also see Wainwright, 2006). [sent-160, score-0.74]
</p><p>57 ¯ ¯ ¯ be the feature set, where Ey We say that the sequence satisﬁes the strong target independent irrepresentable condition if there ¯ exists δ > 0 such that limn→∞ µX (n) (F (n) ) ≤ 1 − δ. [sent-163, score-0.451]
</p><p>58 We say that the sequence satisﬁes the weak target independent irrepresentable condition if ¯ µX (n) (F (n) ) ≤ 1 for all sufﬁciently large n. [sent-164, score-0.422]
</p><p>59 It was shown by Zhao and Yu (2006) that the strong (target independent) irrepresentable condition ¯ is sufﬁcient for Lasso to select features consistently for all possible sign combination of β(n) when n → ∞ (under appropriate assumptions). [sent-165, score-0.417]
</p><p>60 In addition, the weak (target independent) irrepresentable condition is necessary for Lasso to select features consistently when n → ∞. [sent-166, score-0.425]
</p><p>61 The target independent irrepresentable conditions are considered by Zhao and Yu (2006) and Wainwright (2006). [sent-167, score-0.385]
</p><p>62 Speciﬁcally, the following two theorems show that the strong target independent irrepresentable condition is sufﬁcient for Algorithm 1 to select features consistently, while the weak target independent irrepresentable condition is necessary. [sent-170, score-0.882]
</p><p>63 Theorem 4 Consider regression problems indexed by the sample size n, and use notations in Deﬁnition 3. [sent-171, score-0.084]
</p><p>64 For each problem of sample size n, denote by Fn the feature set from Algorithm 1 when it stops with ε = ns/2 for some s ∈ (0, 1]. [sent-174, score-0.107]
</p><p>65 Then for all sufﬁciently large n, we have ¯ P(Fn = F (n) ) ≤ exp(−ns / ln n) if the following conditions hold: 560  F EATURE S ELECTION USING G REEDY L EAST S QUARES  1. [sent-175, score-0.189]
</p><p>66 Theorem 5 Consider regression problems indexed by the sample size n, and use notations in Deﬁnition 3. [sent-181, score-0.084]
</p><p>67 Assume that the weak irrepresentable ¯ condition is violated at sample sizes n1 < n2 < · · · . [sent-183, score-0.358]
</p><p>68 ¯ T ¯ Proof By deﬁnition of µX (F), there exists v = (XF XF )u ∈ R|F| such that ¯ ¯ T T ¯ µX (F) = max (XF XF )−1 XF x j ¯ ¯ ¯ j∈F /¯  = max j∈F /¯  = max j∈F /¯  =  1  T T |vT (XF XF )−1 XF x j | ¯ ¯ ¯ v ∞ T |uT XF x j | ¯ T (XF XF )u ∞ ¯ ¯  max j∈F |xT XF u| / j ¯  maxi∈F |(xT XF )u| ¯ ¯ i  . [sent-186, score-0.26]
</p><p>69 At any sample size n = n j , since ¯ ¯ µX (n) (F (n) ) > 1, we can ﬁnd a sufﬁciently large target vector β(n) such that  ¯ ¯ max |xT X (n) β(n) | < max |xT X (n) β(n) | − 2δn . [sent-192, score-0.194]
</p><p>70 Therefore (3) implies that j ¯ ¯ ¯ ¯ max [|xT X (n) β(n) | + |xT (y − X (n) β(n) )|] < max [|xT X (n) β(n) | − |xT (y − X (n) β(n) )|]. [sent-198, score-0.158]
</p><p>71 i i j j  ¯ i∈F (n)  j∈F (n) /¯  Therefore max |xT y| < max |xT y|. [sent-199, score-0.13]
</p><p>72 Conclusion We have shown that weak and strong target independent irrepresentable conditions are necessary and sufﬁcient conditions for a greedy least squares regression algorithm to select features consistently. [sent-203, score-0.973]
</p><p>73 These conditions match the target independent versions of the necessary and sufﬁcient conditions for Lasso by Zhao and Yu (2006). [sent-204, score-0.134]
</p><p>74 ¯ Moreover, if the eigenvalue ρ(F) is bounded away from zero, then the greedy algorithm can reliably identify features as long as each nonzero coefﬁcient is larger than a constant times the noise level. [sent-205, score-0.496]
</p><p>75 In comparison, under the same condition, Lasso may require the coefﬁcients to be larger than √ O( s) times the noise level, where s is the number of nonzero coefﬁcients. [sent-206, score-0.092]
</p><p>76 This implies that under some conditions, greedy least squares regression may potentially select features more effectively than Lasso in the presence of stochastic noise. [sent-207, score-0.56]
</p><p>77 Although the target independent versions of the irrepresentable conditions for greedy least squares regression match those of Lasso, our result does not show which algorithm is better for any speciﬁc target. [sent-208, score-0.852]
</p><p>78 Acknowledgments This paper is the outcome of some private discussion with Joel Tropp, which drew the author’s attention to the similarities and differences between orthogonal matching pursuit and Lasso for estimating sparse signals. [sent-210, score-0.099]
</p><p>79 Auxiliary Lemmas The following technical lemmas from Zhang (2008) are needed to analyze the behavior of greedy least squares regression under stochastic noise. [sent-212, score-0.503]
</p><p>80 The following lemma relates the squared error reduction in one greedy step to correlation coefﬁcients. [sent-214, score-0.324]
</p><p>81 2  The following lemma provides a bound on the squared error reduction of one forward greedy step. [sent-217, score-0.368]
</p><p>82 , m, we have for all η ∈ (0, 1), with probability larger than 1 − η: 2 2  n  sup | ∑ gi, j (ξi − Eξi )| ≤ a 2 ln(2m/η), j  where  a2  =  i=1  sup j ∑n g2 j σ2 . [sent-246, score-0.081]
</p><p>83 2 t 2 /2  ,  Z HANG  This implies that n  P sup j  ∑ gi, j (ξi − Eξi ) ≥ ε ≤ m sup P j  i=1  n  ∑ gi, j (ξi − Eξi )  i=1  ≥ ε ≤ 2me−ε  2 /(2a2 )  . [sent-249, score-0.09]
</p><p>84 ¯ Since by deﬁnition, ρ(F)n is no larger than the smallest eigenvalue of GT G, the desired inequality follows from the estimate eT (GT G)−1 GT j  2 2  ¯ = eT (GT G)−1 e j ≤ 1/(nρ(F)). [sent-264, score-0.078]
</p><p>85 For all η ∈ (0, 1), with probability larger than 1 − η, we have ˆ ¯ max |(X βX (F, y) − y)T x j | ≤ σ 2n ln(2d/η). [sent-271, score-0.084]
</p><p>86 Lemma 8 implies that with probability larger than 1 − η: sup |(y − Ey)T (I − P)x j | ≤ σ 2n ln(2d/η),  j=1,. [sent-273, score-0.078]
</p><p>87 First, Lemma 10 imˆ ¯ plies that with large probability, max j∈F |(X βX (F, y) − y)T x j | is small. [sent-283, score-0.08]
</p><p>88 The claims of the theorem then follow by induction. [sent-285, score-0.072]
</p><p>89 We have ˆ ¯ ¯ max |(Xβ − y)T x j | ≤ max |(X βX (F, y) − y)T x j | + µX (F) max |(Xβ − y)T xi |. [sent-289, score-0.215]
</p><p>90 Therefore T max |(Xβ − y)T xi | = max |xT X(β − β′ )| = XF XF (β − β′ ) ¯ ¯ i ¯ i∈F  ¯ i∈F  ∞. [sent-292, score-0.15]
</p><p>91 T ¯ Let v = XF XF (β − β′ ), then the deﬁnition of µX (F) implies that ¯ ¯  ¯ µX (F) ≥ max j∈F /¯  T |xT XF (XF XF )−1 v| max j∈F |xT XF (β − β′ )| /¯ ¯ ¯ j ¯ j ¯ = . [sent-293, score-0.158]
</p><p>92 T X (β − β′ ) v ∞ XF F ∞ ¯ ¯  We obtain from the above T ¯ max |xT X(β − β′ )| ≤ µX (F) XF XF (β − β′ ) ¯ ¯ j j∈F /¯  ∞  ¯ = µX (F) max |(Xβ − y)T xi |. [sent-294, score-0.15]
</p><p>93 ¯ i∈F  Now, the lemma follows from the simple inequality max |(Xβ − y)T x j | ≤ max |xT X(β − β′ )| + max |xT (Xβ′ − y)| j j j∈F /¯  j∈F /¯  j∈F /¯  Now we are ready to prove the theorem. [sent-295, score-0.256]
</p><p>94 565  Z HANG  From Lemma 10, we obtain with probability larger than 1 − η, ˆ ¯ ˜ max |(X(βX (F, y)) − y)T x j | ≤ σ j∈F /¯  ¯ 2 ln(2d/η) < (1 − µX (F))ε. [sent-297, score-0.084]
</p><p>95 In this case, we have ˆ ¯ ¯ ˜ ˜ max |(Xβ(k−1) − y)T xi | > ε > max |(X βX (F, y) − y)T x j |/(1 − µX (F)). [sent-305, score-0.15]
</p><p>96 Now by combining this estimate with Lemma 11, we obtain ˜ ˜ max |(Xβ(k−1) − y)T x j | < max |(Xβ(k−1) − y)T xi |. [sent-307, score-0.15]
</p><p>97 i(k) ∈ F: By Lemma 11, we must have: / ¯ ˜ ˜ max |(Xβ − y)T xi | ≤ max |(Xβ − y)T x j | ¯ i∈F  j∈F /¯  ˆ ¯ ¯ ˜ ˜ ≤ max |(X βX (F, y) − y)T x j | + µX (F) max |(Xβ − y)T xi | ¯ j∈F  ¯ i∈F  ˆ ¯ ¯ ˜ ˜ ≤ max |(X βX (F, y) − y) x j | + µX (F) max |(Xβ − y)T x j |. [sent-311, score-0.43]
</p><p>98 T  ¯ j∈F  j∈F /¯  566  F EATURE S ELECTION USING G REEDY L EAST S QUARES  Therefore ˜ max |(Xβ − y)T x j | ≤ j∈F /¯  1 T ˆ ¯ ˜ ¯ max |(X βX (F, y) − y) x j | < ε. [sent-312, score-0.13]
</p><p>99 Therefore by induction, when the procedure stops, the following three claims hold: ¯ • F (k−1) ⊂ F. [sent-321, score-0.072]
</p><p>100 Some sharp performance bounds for least squares regression with L1 regularization. [sent-358, score-0.205]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xf', 0.714), ('irrepresentable', 0.286), ('greedy', 0.279), ('lasso', 0.243), ('ln', 0.154), ('supp', 0.143), ('gt', 0.143), ('squares', 0.122), ('zhao', 0.12), ('ey', 0.112), ('tropp', 0.109), ('reedy', 0.107), ('xt', 0.097), ('quares', 0.091), ('yu', 0.088), ('hang', 0.081), ('eature', 0.075), ('east', 0.075), ('stops', 0.07), ('wainwright', 0.069), ('max', 0.065), ('target', 0.064), ('election', 0.056), ('claims', 0.049), ('condition', 0.048), ('maxi', 0.047), ('rn', 0.045), ('noise', 0.045), ('lemma', 0.045), ('forward', 0.044), ('zhang', 0.044), ('gi', 0.044), ('eigenvalue', 0.043), ('regression', 0.042), ('meinshausen', 0.039), ('stopping', 0.038), ('sparse', 0.038), ('pursuit', 0.037), ('feature', 0.037), ('coef', 0.037), ('eyi', 0.036), ('mallat', 0.036), ('omp', 0.036), ('ns', 0.035), ('conditions', 0.035), ('tong', 0.034), ('sup', 0.031), ('rutgers', 0.031), ('joel', 0.03), ('selection', 0.028), ('bin', 0.028), ('implies', 0.028), ('nonzero', 0.028), ('nicolai', 0.027), ('inf', 0.027), ('rd', 0.027), ('features', 0.025), ('least', 0.024), ('matching', 0.024), ('identify', 0.024), ('weak', 0.024), ('procedure', 0.023), ('theorem', 0.023), ('sgn', 0.023), ('signs', 0.022), ('notations', 0.021), ('select', 0.021), ('indexed', 0.021), ('consistently', 0.021), ('xd', 0.021), ('xi', 0.02), ('min', 0.02), ('stochastic', 0.019), ('larger', 0.019), ('cients', 0.019), ('worst', 0.019), ('recovery', 0.019), ('backward', 0.018), ('consistency', 0.018), ('design', 0.017), ('sharp', 0.017), ('lemmas', 0.017), ('away', 0.017), ('reliably', 0.016), ('criterion', 0.016), ('strong', 0.016), ('proof', 0.016), ('quantity', 0.016), ('appeared', 0.016), ('established', 0.016), ('inequality', 0.016), ('assumption', 0.016), ('basis', 0.015), ('establish', 0.015), ('rithm', 0.015), ('limn', 0.015), ('pursuits', 0.015), ('plies', 0.015), ('alent', 0.015), ('eet', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="66-tfidf-1" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>Author: Tong Zhang</p><p>Abstract: This paper studies the feature selection problem using a greedy least squares regression algorithm. We show that under a certain irrepresentable condition on the design matrix (but independent of the sparse target), the greedy algorithm can select features consistently when the sample size approaches inﬁnity. The condition is identical to a corresponding condition for Lasso. Moreover, under a sparse eigenvalue condition, the greedy algorithm can reliably identify features as long as each nonzero coefﬁcient is larger than a constant times the noise level. In compar√ ison, Lasso may require the coefﬁcients to be larger than O( s) times the noise level in the worst case, where s is the number of nonzero coefﬁcients. Keywords: greedy algorithm, feature selection, sparsity</p><p>2 0.08818724 <a title="66-tfidf-2" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>Author: Jianqing Fan, Richard Samworth, Yichao Wu</p><p>Abstract: Variable selection in high-dimensional space characterizes many contemporary problems in scientiﬁc discovery and decision making. Many frequently-used techniques are based on independence screening; examples include correlation ranking (Fan & Lv, 2008) or feature selection using a twosample t-test in high-dimensional classiﬁcation (Tibshirani et al., 2003). Within the context of the linear model, Fan & Lv (2008) showed that this simple correlation ranking possesses a sure independence screening property under certain conditions and that its revision, called iteratively sure independent screening (ISIS), is needed when the features are marginally unrelated but jointly related to the response variable. In this paper, we extend ISIS, without explicit deﬁnition of residuals, to a general pseudo-likelihood framework, which includes generalized linear models as a special case. Even in the least-squares setting, the new method improves ISIS by allowing feature deletion in the iterative process. Our technique allows us to select important features in high-dimensional classiﬁcation where the popularly used two-sample t-method fails. A new technique is introduced to reduce the false selection rate in the feature screening stage. Several simulated and two real data examples are presented to illustrate the methodology. Keywords: classiﬁcation, feature screening, generalized linear models, robust regression, feature selection</p><p>3 0.079493798 <a title="66-tfidf-3" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>Author: John Langford, Lihong Li, Tong Zhang</p><p>Abstract: We propose a general method called truncated gradient to induce sparsity in the weights of onlinelearning algorithms with convex loss functions. This method has several essential properties: 1. The degree of sparsity is continuous—a parameter controls the rate of sparsiﬁcation from no sparsiﬁcation to total sparsiﬁcation. 2. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1 -regularization method in the batch setting. We prove that small rates of sparsiﬁcation result in only small additional regret with respect to typical online-learning guarantees. 3. The approach works well empirically. We apply the approach to several data sets and ﬁnd for data sets with large numbers of features, substantial sparsity is discoverable. Keywords: truncated gradient, stochastic gradient descent, online learning, sparsity, regularization, Lasso</p><p>4 0.077895492 <a title="66-tfidf-4" href="./jmlr-2009-Identification_of_Recurrent_Neural_Networks_by_Bayesian_Interrogation_Techniques.html">40 jmlr-2009-Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques</a></p>
<p>Author: Barnabás Póczos, András Lőrincz</p><p>Abstract: We introduce novel online Bayesian methods for the identiﬁcation of a family of noisy recurrent neural networks (RNNs). We present Bayesian active learning techniques for stimulus selection given past experiences. In particular, we consider the unknown parameters as stochastic variables and use A-optimality and D-optimality principles to choose optimal stimuli. We derive myopic cost functions in order to maximize the information gain concerning network parameters at each time step. We also derive the A-optimal and D-optimal estimations of the additive noise that perturbs the dynamical system of the RNN. Here we investigate myopic as well as non-myopic estimations, and study the problem of simultaneous estimation of both the system parameters and the noise. Employing conjugate priors our derivations remain approximation-free and give rise to simple update rules for the online learning of the parameters. The efﬁciency of our method is demonstrated for a number of selected cases, including the task of controlled independent component analysis. Keywords: active learning, system identiﬁcation, online Bayesian learning, A-optimality, Doptimality, infomax control, optimal design</p><p>5 0.063800864 <a title="66-tfidf-5" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>Author: Brijnesh J. Jain, Klaus Obermayer</p><p>Abstract: Finite structures such as point patterns, strings, trees, and graphs occur as ”natural” representations of structured data in different application areas of machine learning. We develop the theory of structure spaces and derive geometrical and analytical concepts such as the angle between structures and the derivative of functions on structures. In particular, we show that the gradient of a differentiable structural function is a well-deﬁned structure pointing in the direction of steepest ascent. Exploiting the properties of structure spaces, it will turn out that a number of problems in structural pattern recognition such as central clustering or learning in structured output spaces can be formulated as optimization problems with cost functions that are locally Lipschitz. Hence, methods from nonsmooth analysis are applicable to optimize those cost functions. Keywords: graphs, graph matching, learning in structured domains, nonsmooth optimization</p><p>6 0.062434167 <a title="66-tfidf-6" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>7 0.059848774 <a title="66-tfidf-7" href="./jmlr-2009-SGD-QN%3A_Careful_Quasi-Newton_Stochastic_Gradient_Descent.html">83 jmlr-2009-SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent</a></p>
<p>8 0.053099103 <a title="66-tfidf-8" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>9 0.048498519 <a title="66-tfidf-9" href="./jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">16 jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<p>10 0.044430822 <a title="66-tfidf-10" href="./jmlr-2009-On_Uniform_Deviations_of_General_Empirical_Risks_with_Unboundedness%2C_Dependence%2C_and_High_Dimensionality.html">65 jmlr-2009-On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality</a></p>
<p>11 0.040760007 <a title="66-tfidf-11" href="./jmlr-2009-Estimation_of_Sparse_Binary_Pairwise_Markov_Networks_using_Pseudo-likelihoods.html">30 jmlr-2009-Estimation of Sparse Binary Pairwise Markov Networks using Pseudo-likelihoods</a></p>
<p>12 0.039453749 <a title="66-tfidf-12" href="./jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">79 jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>13 0.038949568 <a title="66-tfidf-13" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>14 0.037297178 <a title="66-tfidf-14" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>15 0.035721164 <a title="66-tfidf-15" href="./jmlr-2009-Prediction_With_Expert_Advice_For_The_Brier_Game.html">73 jmlr-2009-Prediction With Expert Advice For The Brier Game</a></p>
<p>16 0.035393942 <a title="66-tfidf-16" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>17 0.034758486 <a title="66-tfidf-17" href="./jmlr-2009-Data-driven_Calibration_of_Penalties_for_Least-Squares_Regression.html">21 jmlr-2009-Data-driven Calibration of Penalties for Least-Squares Regression</a></p>
<p>18 0.034560248 <a title="66-tfidf-18" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>19 0.034340989 <a title="66-tfidf-19" href="./jmlr-2009-Exploring_Strategies_for_Training_Deep_Neural_Networks.html">33 jmlr-2009-Exploring Strategies for Training Deep Neural Networks</a></p>
<p>20 0.033767916 <a title="66-tfidf-20" href="./jmlr-2009-Learning_Permutations_with_Exponential_Weights.html">49 jmlr-2009-Learning Permutations with Exponential Weights</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.175), (1, 0.05), (2, -0.005), (3, -0.014), (4, 0.009), (5, 0.075), (6, -0.076), (7, 0.034), (8, -0.135), (9, -0.021), (10, 0.13), (11, 0.019), (12, -0.015), (13, 0.17), (14, 0.153), (15, -0.031), (16, -0.319), (17, -0.16), (18, -0.01), (19, 0.084), (20, -0.004), (21, 0.042), (22, 0.214), (23, -0.07), (24, 0.037), (25, 0.042), (26, 0.102), (27, 0.14), (28, -0.012), (29, -0.127), (30, 0.097), (31, -0.019), (32, 0.05), (33, -0.055), (34, 0.228), (35, 0.092), (36, 0.091), (37, 0.058), (38, 0.072), (39, 0.055), (40, -0.077), (41, -0.031), (42, 0.076), (43, -0.127), (44, 0.2), (45, 0.046), (46, -0.086), (47, 0.121), (48, -0.116), (49, -0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96094108 <a title="66-lsi-1" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>Author: Tong Zhang</p><p>Abstract: This paper studies the feature selection problem using a greedy least squares regression algorithm. We show that under a certain irrepresentable condition on the design matrix (but independent of the sparse target), the greedy algorithm can select features consistently when the sample size approaches inﬁnity. The condition is identical to a corresponding condition for Lasso. Moreover, under a sparse eigenvalue condition, the greedy algorithm can reliably identify features as long as each nonzero coefﬁcient is larger than a constant times the noise level. In compar√ ison, Lasso may require the coefﬁcients to be larger than O( s) times the noise level in the worst case, where s is the number of nonzero coefﬁcients. Keywords: greedy algorithm, feature selection, sparsity</p><p>2 0.46481243 <a title="66-lsi-2" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>Author: Jianqing Fan, Richard Samworth, Yichao Wu</p><p>Abstract: Variable selection in high-dimensional space characterizes many contemporary problems in scientiﬁc discovery and decision making. Many frequently-used techniques are based on independence screening; examples include correlation ranking (Fan & Lv, 2008) or feature selection using a twosample t-test in high-dimensional classiﬁcation (Tibshirani et al., 2003). Within the context of the linear model, Fan & Lv (2008) showed that this simple correlation ranking possesses a sure independence screening property under certain conditions and that its revision, called iteratively sure independent screening (ISIS), is needed when the features are marginally unrelated but jointly related to the response variable. In this paper, we extend ISIS, without explicit deﬁnition of residuals, to a general pseudo-likelihood framework, which includes generalized linear models as a special case. Even in the least-squares setting, the new method improves ISIS by allowing feature deletion in the iterative process. Our technique allows us to select important features in high-dimensional classiﬁcation where the popularly used two-sample t-method fails. A new technique is introduced to reduce the false selection rate in the feature screening stage. Several simulated and two real data examples are presented to illustrate the methodology. Keywords: classiﬁcation, feature screening, generalized linear models, robust regression, feature selection</p><p>3 0.37413645 <a title="66-lsi-3" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>Author: John Langford, Lihong Li, Tong Zhang</p><p>Abstract: We propose a general method called truncated gradient to induce sparsity in the weights of onlinelearning algorithms with convex loss functions. This method has several essential properties: 1. The degree of sparsity is continuous—a parameter controls the rate of sparsiﬁcation from no sparsiﬁcation to total sparsiﬁcation. 2. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1 -regularization method in the batch setting. We prove that small rates of sparsiﬁcation result in only small additional regret with respect to typical online-learning guarantees. 3. The approach works well empirically. We apply the approach to several data sets and ﬁnd for data sets with large numbers of features, substantial sparsity is discoverable. Keywords: truncated gradient, stochastic gradient descent, online learning, sparsity, regularization, Lasso</p><p>4 0.32950166 <a title="66-lsi-4" href="./jmlr-2009-Identification_of_Recurrent_Neural_Networks_by_Bayesian_Interrogation_Techniques.html">40 jmlr-2009-Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques</a></p>
<p>Author: Barnabás Póczos, András Lőrincz</p><p>Abstract: We introduce novel online Bayesian methods for the identiﬁcation of a family of noisy recurrent neural networks (RNNs). We present Bayesian active learning techniques for stimulus selection given past experiences. In particular, we consider the unknown parameters as stochastic variables and use A-optimality and D-optimality principles to choose optimal stimuli. We derive myopic cost functions in order to maximize the information gain concerning network parameters at each time step. We also derive the A-optimal and D-optimal estimations of the additive noise that perturbs the dynamical system of the RNN. Here we investigate myopic as well as non-myopic estimations, and study the problem of simultaneous estimation of both the system parameters and the noise. Employing conjugate priors our derivations remain approximation-free and give rise to simple update rules for the online learning of the parameters. The efﬁciency of our method is demonstrated for a number of selected cases, including the task of controlled independent component analysis. Keywords: active learning, system identiﬁcation, online Bayesian learning, A-optimality, Doptimality, infomax control, optimal design</p><p>5 0.3188518 <a title="66-lsi-5" href="./jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">16 jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<p>Author: Dao-Hong Xiang, Ding-Xuan Zhou</p><p>Abstract: This paper considers binary classiﬁcation algorithms generated from Tikhonov regularization schemes associated with general convex loss functions and varying Gaussian kernels. Our main goal is to provide fast convergence rates for the excess misclassiﬁcation error. Allowing varying Gaussian kernels in the algorithms improves learning rates measured by regularization error and sample error. Special structures of Gaussian kernels enable us to construct, by a nice approximation scheme with a Fourier analysis technique, uniformly bounded regularizing functions achieving polynomial decays of the regularization error under a Sobolev smoothness condition. The sample error is estimated by using a projection operator and a tight bound for the covering numbers of reproducing kernel Hilbert spaces generated by Gaussian kernels. The convexity of the general loss function plays a very important role in our analysis. Keywords: reproducing kernel Hilbert space, binary classiﬁcation, general convex loss, varying Gaussian kernels, covering number, approximation</p><p>6 0.28755602 <a title="66-lsi-6" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>7 0.26548797 <a title="66-lsi-7" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>8 0.2437185 <a title="66-lsi-8" href="./jmlr-2009-Prediction_With_Expert_Advice_For_The_Brier_Game.html">73 jmlr-2009-Prediction With Expert Advice For The Brier Game</a></p>
<p>9 0.2398957 <a title="66-lsi-9" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>10 0.23300995 <a title="66-lsi-10" href="./jmlr-2009-Exploring_Strategies_for_Training_Deep_Neural_Networks.html">33 jmlr-2009-Exploring Strategies for Training Deep Neural Networks</a></p>
<p>11 0.22058597 <a title="66-lsi-11" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>12 0.22058316 <a title="66-lsi-12" href="./jmlr-2009-Data-driven_Calibration_of_Penalties_for_Least-Squares_Regression.html">21 jmlr-2009-Data-driven Calibration of Penalties for Least-Squares Regression</a></p>
<p>13 0.19244045 <a title="66-lsi-13" href="./jmlr-2009-SGD-QN%3A_Careful_Quasi-Newton_Stochastic_Gradient_Descent.html">83 jmlr-2009-SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent</a></p>
<p>14 0.19170617 <a title="66-lsi-14" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>15 0.18707491 <a title="66-lsi-15" href="./jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">79 jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>16 0.18209258 <a title="66-lsi-16" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>17 0.17620577 <a title="66-lsi-17" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>18 0.1531994 <a title="66-lsi-18" href="./jmlr-2009-Learning_Halfspaces_with_Malicious_Noise.html">46 jmlr-2009-Learning Halfspaces with Malicious Noise</a></p>
<p>19 0.15280668 <a title="66-lsi-19" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>20 0.1507545 <a title="66-lsi-20" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.01), (11, 0.025), (19, 0.018), (26, 0.042), (38, 0.016), (41, 0.355), (52, 0.035), (55, 0.078), (58, 0.033), (66, 0.146), (68, 0.043), (90, 0.047), (96, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70233911 <a title="66-lda-1" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>Author: Tong Zhang</p><p>Abstract: This paper studies the feature selection problem using a greedy least squares regression algorithm. We show that under a certain irrepresentable condition on the design matrix (but independent of the sparse target), the greedy algorithm can select features consistently when the sample size approaches inﬁnity. The condition is identical to a corresponding condition for Lasso. Moreover, under a sparse eigenvalue condition, the greedy algorithm can reliably identify features as long as each nonzero coefﬁcient is larger than a constant times the noise level. In compar√ ison, Lasso may require the coefﬁcients to be larger than O( s) times the noise level in the worst case, where s is the number of nonzero coefﬁcients. Keywords: greedy algorithm, feature selection, sparsity</p><p>2 0.44632018 <a title="66-lda-2" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>Author: Sanjoy Dasgupta, Adam Tauman Kalai, Claire Monteleoni</p><p>Abstract: We start by showing that in an active learning setting, the Perceptron algorithm needs Ω( ε12 ) labels to learn linear separators within generalization error ε. We then present a simple active learning algorithm for this problem, which combines a modiﬁcation of the Perceptron update with an adaptive ﬁltering rule for deciding which points to query. For data distributed uniformly over the unit ˜ sphere, we show that our algorithm reaches generalization error ε after asking for just O(d log 1 ) ε labels. This exponential improvement over the usual sample complexity of supervised learning had previously been demonstrated only for the computationally more complex query-by-committee algorithm. Keywords: active learning, perceptron, label complexity bounds, online learning</p><p>3 0.44154489 <a title="66-lda-3" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>Author: John Duchi, Yoram Singer</p><p>Abstract: We describe, analyze, and experiment with a framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we ﬁrst perform an unconstrained gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the ﬁrst phase. This view yields a simple yet effective algorithm that can be used for batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as ℓ1 . We derive concrete and very simple algorithms for minimization of loss functions with ℓ1 , ℓ2 , ℓ2 , and ℓ∞ regularization. We also show how to construct ef2 ﬁcient algorithms for mixed-norm ℓ1 /ℓq regularization. We further extend the algorithms and give efﬁcient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in a series of experiments with synthetic and natural data sets. Keywords: subgradient methods, group sparsity, online learning, convex optimization</p><p>4 0.43738434 <a title="66-lda-4" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider regularized support vector machines (SVMs) and show that they are precisely equivalent to a new robust optimization formulation. We show that this equivalence of robust optimization and regularization has implications for both algorithms, and analysis. In terms of algorithms, the equivalence suggests more general SVM-like algorithms for classiﬁcation that explicitly build in protection to noise, and at the same time control overﬁtting. On the analysis front, the equivalence of robustness and regularization provides a robust optimization interpretation for the success of regularized SVMs. We use this new robustness interpretation of SVMs to give a new proof of consistency of (kernelized) SVMs, thus establishing robustness as the reason regularized SVMs generalize well. Keywords: robustness, regularization, generalization, kernel, support vector machine</p><p>5 0.43551719 <a title="66-lda-5" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>Author: Junning Li, Z. Jane Wang</p><p>Abstract: In real world applications, graphical statistical models are not only a tool for operations such as classiﬁcation or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-speciﬁed level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-speciﬁed level, and a heuristic modiﬁcation of the method is able to control the FDR more accurately around the user-speciﬁed level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models. Keywords: Bayesian networks, false discovery rate, PC algorithm, directed acyclic graph, skeleton</p><p>6 0.43015793 <a title="66-lda-6" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>7 0.42854214 <a title="66-lda-7" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>8 0.42804843 <a title="66-lda-8" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>9 0.42586085 <a title="66-lda-9" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>10 0.42540547 <a title="66-lda-10" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>11 0.42405403 <a title="66-lda-11" href="./jmlr-2009-CarpeDiem%3A_Optimizing_the_Viterbi_Algorithm_and_Applications_to_Supervised_Sequential_Learning.html">14 jmlr-2009-CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning</a></p>
<p>12 0.42353258 <a title="66-lda-12" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>13 0.42298877 <a title="66-lda-13" href="./jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">79 jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>14 0.41985196 <a title="66-lda-14" href="./jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">16 jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<p>15 0.41899967 <a title="66-lda-15" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>16 0.4167197 <a title="66-lda-16" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>17 0.41573012 <a title="66-lda-17" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>18 0.4156788 <a title="66-lda-18" href="./jmlr-2009-Adaptive_False_Discovery_Rate_Control_under_Independence_and_Dependence.html">5 jmlr-2009-Adaptive False Discovery Rate Control under Independence and Dependence</a></p>
<p>19 0.41407672 <a title="66-lda-19" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>20 0.41373086 <a title="66-lda-20" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
