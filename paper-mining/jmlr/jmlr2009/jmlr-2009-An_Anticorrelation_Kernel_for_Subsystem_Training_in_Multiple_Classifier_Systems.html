<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-8" href="#">jmlr2009-8</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</h1>
<br/><p>Source: <a title="jmlr-2009-8-pdf" href="http://jmlr.org/papers/volume10/ferrer09a/ferrer09a.pdf">pdf</a></p><p>Author: Luciana Ferrer, Kemal Sönmez, Elizabeth Shriberg</p><p>Abstract: We present a method for training support vector machine (SVM)-based classiﬁcation systems for combination with other classiﬁcation systems designed for the same task. Ideally, a new system should be designed such that, when combined with existing systems, the resulting performance is optimized. We present a simple model for this problem and use the understanding gained from this analysis to propose a method to achieve better combination performance when training SVM systems. We include a regularization term in the SVM objective function that aims to reduce the average class-conditional covariance between the resulting scores and the scores produced by the existing systems, introducing a trade-off between such covariance and the system’s individual performance. That is, the new system “takes one for the team”, falling somewhat short of its best possible performance in order to increase the diversity of the ensemble. We report results on the NIST 2005 and 2006 speaker recognition evaluations (SREs) for a variety of subsystems. We show a gain of 19% on the equal error rate (EER) of a combination of four systems when applying the proposed method with respect to the performance obtained when the four systems are trained independently of each other. Keywords: system combination, ensemble diversity, multiple classiﬁer systems, support vector machines, speaker recognition, kernel methods ∗. This author performed part of the work presented in this paper while at the Information Systems Laboratory, Department of Electrical Engineering, Stanford University. c 2009 Luciana Ferrer, Kemal S¨ nmez and Elizabeth Shriberg. o ¨ F ERRER , S ONMEZ AND S HRIBERG</p><p>Reference: <a title="jmlr-2009-8-reference" href="../jmlr2009_reference/jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We include a regularization term in the SVM objective function that aims to reduce the average class-conditional covariance between the resulting scores and the scores produced by the existing systems, introducing a trade-off between such covariance and the system’s individual performance. [sent-12, score-0.434]
</p><p>2 We report results on the NIST 2005 and 2006 speaker recognition evaluations (SREs) for a variety of subsystems. [sent-14, score-0.51]
</p><p>3 Keywords: system combination, ensemble diversity, multiple classiﬁer systems, support vector machines, speaker recognition, kernel methods  ∗. [sent-16, score-0.732]
</p><p>4 Introduction The work presented in this paper is motivated by our work on the task of speaker veriﬁcation. [sent-20, score-0.471]
</p><p>5 In the last decade, many successful speaker veriﬁcation systems have relied on the combination of various component systems to achieve superior performance. [sent-21, score-0.681]
</p><p>6 When training each individual system, all other systems available for combination are usually ignored while, in fact, the ultimate goal of the systems is to perform well in combination with all the other systems and not necessarily individually. [sent-33, score-0.456]
</p><p>7 It is easy to see that system combination at the score level is not guaranteed to give strictly better performance than those of the individual systems being combined. [sent-34, score-0.411]
</p><p>8 System diversity has been the subject of a large amount of research in recent years, with two main goals: deﬁning a measure of diversity that can predict the performance of the combination, and designing procedures for achieving diversity in an ensemble of systems. [sent-37, score-0.552]
</p><p>9 The term score is commonly used in the speaker veriﬁcation community to refer to the numerical output of a system, which may or may not be a probability measure. [sent-50, score-0.532]
</p><p>10 In Sections 5 and 6 we present results on simulated data and speaker veriﬁcation data, respectively. [sent-52, score-0.471]
</p><p>11 Classiﬁcation error is usually considered inappropriate for problems in which the prior probabilities of the classes are very different (as is the case in most speaker veriﬁcation applications). [sent-79, score-0.471]
</p><p>12 In binary problems like speaker veriﬁcation one can train classiﬁers that output a score instead of a probability. [sent-89, score-0.557]
</p><p>13 Nevertheless, in our speaker veriﬁcation experiments we have found that combining the scores directly leads to better results than combining the estimated posteriors when using a linear combiner. [sent-92, score-0.605]
</p><p>14 The best performance across all combinations including any number of systems is obtained when combining the scores of six systems and this combination is 12% better than the best combination of posterior probabilities. [sent-95, score-0.558]
</p><p>15 We then replace the additive error assumption with an assumption that the distribution of the scores for each class is Gaussian, which, as we will see, is a good approximation for most speaker veriﬁcation scores. [sent-97, score-0.605]
</p><p>16 Hence, the linear combiner is a good choice when combining speaker veriﬁcation scores. [sent-102, score-0.522]
</p><p>17 Our proposed anticorrelation method follows the spirit of the NC learning technique of explicitly creating diversity through the modiﬁcation of the learner’s objective function when the learners are SVMs instead of neural networks. [sent-125, score-0.493]
</p><p>18 The 2083  ¨ F ERRER , S ONMEZ AND S HRIBERG  speaker veriﬁcation system described in this paper can, in fact, be considered as a cascade of two diversity creation methods. [sent-133, score-0.764]
</p><p>19 The proposed method is shown to be equivalent to deﬁning a new kernel that we call the anticorrelation kernel. [sent-138, score-0.407]
</p><p>20 This particular scenario is not applicable to speaker recognition where the speaker models are usually trained before any test sample is available. [sent-156, score-1.009]
</p><p>21 , 2008a) to perform comparably to or better than other linear and nonlinear classiﬁers for the combination of speaker veriﬁcation scores, and it is one of the most commonly used methods for combining speaker veriﬁcation systems (Brummer et al. [sent-168, score-1.103]
</p><p>22 Other common combination procedures used in speaker veriﬁcation include neural networks (Reynolds et al. [sent-170, score-0.583]
</p><p>23 In speaker veriﬁcation, class a corresponds to impostor and b to true-speaker. [sent-177, score-0.558]
</p><p>24 In speaker veriﬁcation, a false acceptance (which we will call eb|a ) is an impostor trial accepted by the system as the target speaker, and a false rejection (ea|b ) is a true-speaker trial considered an impostor trial by the system. [sent-190, score-0.797]
</p><p>25 SNERF−SVM system scores  Figure 3: Distribution of scores for two individual systems compared to their Gaussian approximation. [sent-227, score-0.481]
</p><p>26 This is not a very restrictive assumption for speaker veriﬁcation since for this task we have repeatedly found that linear combination procedures perform as well (or better) than nonlinear ones (Ferrer et al. [sent-230, score-0.583]
</p><p>27 In fact, a set of initial experiments showed that LLR was signiﬁcantly better than LDA on our speaker veriﬁcation data. [sent-280, score-0.471]
</p><p>28 2  0  4  scores 2  ec ˆ  2  −2 −2  6  ρ=0  0 2 scores 1  4  −2  0 2 scores 1  4  Figure 4: Left: Curves of ec (the upper bound on the EER of the combination) for two systems as a ˆ function of ρ, for e1 ﬁxed and various values of e2 , where ei denotes the upper bound on ˆ ˆ ˆ the EER of system i. [sent-297, score-0.603]
</p><p>29 Our strategy will be to train system S using information about system B in order to improve the combination performance over the one obtained when system S is designed with no knowledge of system B. [sent-333, score-0.586]
</p><p>30 3, reducing the ρ value between system S and system B could lead to an improvement in combination performance as long as the performance of the individual systems does not degrade too greatly. [sent-362, score-0.481]
</p><p>31 1 + λK t K k l  (14)  We call this kernel the anticorrelation kernel. [sent-403, score-0.407]
</p><p>32 Kt K  (15)  In the case of speaker veriﬁcation, a separate K vector is computed for each target model being trained. [sent-412, score-0.471]
</p><p>33 5, when the original SVM problem uses a kernel other than the inner-product one, implementing the anticorrelation method as a kernel may be the only feasible option. [sent-416, score-0.464]
</p><p>34 The average class-conditional covariance between the scores from system B and the scores from system S is given by wt K. [sent-419, score-0.572]
</p><p>35 One way of implementing the anticorrelation method in this case is to simply transform the features using φ0 (x) and then treat the transformed features as the feature vectors x in Section 4. [sent-459, score-0.489]
</p><p>36 Luckily, there is a way of implementing the anticorrelation method without ever computing the transform but only the kernel function between pairs of features. [sent-462, score-0.407]
</p><p>37 = K0 (xk , xl ) − 1 + λ ∑ j ∑i c j c j K0 (xi , x j )  K(xk , xl ) = φ0 (xk )t φ0 (xl ) −  2095  ¨ F ERRER , S ONMEZ AND S HRIBERG  The anticorrelation kernel can then be computed exclusively as a function of the original kernel K0 . [sent-470, score-0.578]
</p><p>38 1 F EATURE - LEVEL C OMBINATION When system B is also an SVM system and the features corresponding to the samples used for training system S are also available for system B, an SVM using the features from both systems concatenated into a single vector can be trained. [sent-480, score-0.631]
</p><p>39 2 F EATURE +S CORE C OMBINATION Another method can be considered in which we present the scores generated by system B as input features to the SVM, along with all the features from system S. [sent-490, score-0.452]
</p><p>40 The anticorrelation kernel is implemented for varying values of λ. [sent-505, score-0.407]
</p><p>41 Nevertheless, as λ grows, the performance of the score-level combination using the anticorrelation kernel improves signiﬁcantly (from 1. [sent-530, score-0.544]
</p><p>42 Overall, we see a reduction in EER of around 50%, relative to the EER of the best combined system when the anticorrelation kernel is not used. [sent-533, score-0.547]
</p><p>43 2  0 −2 10  0  2  10  ρ  4  10  λ  System B EER System S EER Score−level combination EER Feature−level combination EER Score+Feature combination EER ρ  10  Figure 5: Error of individual systems and their combination, and value of the ρ coefﬁcient as a function of λ for an artiﬁcial problem. [sent-543, score-0.443]
</p><p>44 S scores for lambda = 10000  S scores for lambda = 0  6 4 2 0 −2 −4 −6 −8  3  2  1  0  −1  −2  −3 −6  −4  −2  0  2  4  6  −6  B scores  −4  −2  0  2  4  6  B scores  Figure 6: Scores from system B versus scores from system S for two values of λ. [sent-547, score-0.882]
</p><p>45 Plain score-level combination (without anticorrelation) performs comparably to featurelevel combination when the number of training samples is small and the number of features is large, in which case the feature-level combination suffers from the additional complexity. [sent-556, score-0.441]
</p><p>46 0, the anticorrelation method signiﬁcantly outperforms all other combination methods for both values of d and the three values of N. [sent-571, score-0.462]
</p><p>47 1 Databases and Error Measures Experiments were conducted using data from the NIST speaker recognition evaluations (SRE) from 2005 and 2006. [sent-589, score-0.51]
</p><p>48 Each speaker veriﬁcation trial consists of a test sample and a speaker model. [sent-590, score-0.942]
</p><p>49 We consider the 1-side training conditions in which we are given 1 conversation side to train the speaker model. [sent-593, score-0.523]
</p><p>50 2 Individual System Descriptions The systems chosen to run the experiments in this paper are representative of the systems being used in most state-of-the-art speaker recognition systems. [sent-610, score-0.608]
</p><p>51 1 UBM-GMM S YSTEM (G) This is probably the most widely used paradigm for speaker veriﬁcation. [sent-615, score-0.471]
</p><p>52 The target speaker models are trained by maximum a posteriori adaptation of the background model to the training data. [sent-617, score-0.526]
</p><p>53 , 2007, 2006) uses the speaker adaptation transforms used in the speech recognition system as features for speaker veriﬁcation. [sent-639, score-1.235]
</p><p>54 Initial experiments (not shown here) indicate that improvements from the anticorrelation method are still obtained when the more complex MLLR system is used, with similar relative gains as the ones shown here. [sent-647, score-0.456]
</p><p>55 3 Application of the Proposed Method to the Speaker Veriﬁcation Problem Most speaker veriﬁcation systems that use SVMs as models consider each train or test utterance as a single sample. [sent-659, score-0.545]
</p><p>56 In our experiments, since we are presenting results on the 1-side training condition from NIST evaluations, this implies that only one positive sample is available during training for each speaker model. [sent-665, score-0.525]
</p><p>57 This is indicated by the use of a subindex corresponding to the system with respect to which the anticorrelation is performed. [sent-676, score-0.456]
</p><p>58 For example, MG corresponds to a system that uses the MLLR features and anticorrelation kernel with respect to the GMM-UBM system (that is, with K given by the vector of average class-conditional covariances between the MLLR features and the scores from the GMM-UBM system). [sent-677, score-0.859]
</p><p>59 A list of subindices corresponds to performing anticorrelation with respect to more than one system as described in Section 4. [sent-678, score-0.456]
</p><p>60 157  Table 1: Results for individual systems (G: GMM-UBM, V: Supervector-SVM, M: MLLR-SVM, S: SNERF-SVM) with inner product kernel and anticorrelation kernel. [sent-739, score-0.514]
</p><p>61 When the anticorrelation kernel is used, a subindex indicates the name of the system or systems with respect to which the anticorrelation is performed. [sent-740, score-0.912]
</p><p>62 It can be seen that in most cases, using the anticorrelation kernel results in a degradation in performance in the system. [sent-741, score-0.432]
</p><p>63 A notable exception is the result for system VM (Supervector features using anticorrelation kernel with respect to the MLLR-SVM system). [sent-742, score-0.566]
</p><p>64 That is, applying the anticorrelation kernel to system Y always gives a gain in the combination performance, even though in most cases system YX has worse individual performance than system Y. [sent-759, score-0.92]
</p><p>65 The most notable gain from using the anticorrelation kernel is found for the combination M + VM . [sent-763, score-0.519]
</p><p>66 The three combinations shown that use the anticorrelation kernel on the V and S systems perform very similarly, resulting in a performance improvement of 18. [sent-778, score-0.527]
</p><p>67 We can see that using multiple anticorrelation on the S system with respect to the other two systems already in the combination (M and VM ) does not lead to further improvements (line M + VM + SM,VM ). [sent-780, score-0.617]
</p><p>68 This is, in fact, good news, since doing the multiple anticorrelation involves a signiﬁcant amount of extra computation to obtain the K vector of system S with respect to the scores from VM . [sent-781, score-0.59]
</p><p>69 This was expected since the two-way gain from using anticorrelation on that system was the largest among all two-way combinations. [sent-786, score-0.456]
</p><p>70 Nevertheless, the systems used here are representative of the kinds of systems used for speaker recognition on stateof-the-art systems, where very large feature vectors have been found to outperform smaller ones. [sent-913, score-0.641]
</p><p>71 Furthermore, the small amount of positive training examples is an inherent characteristic of the speaker recognition task. [sent-914, score-0.537]
</p><p>72 5 Interaction with Intersession Variability Compensation The variability found across different recordings of the same speaker is commonly called intersession variability (ISV). [sent-946, score-0.632]
</p><p>73 This effect can be caused by a mismatch in channel conditions, emotional state, phonetic content, and so on, and it is one of the biggest sources of errors in speaker veriﬁcation. [sent-947, score-0.502]
</p><p>74 This matrix is in turn estimated from held-out data for which several samples of each speaker are available. [sent-953, score-0.496]
</p><p>75 In the case of NAP these directions are the ones estimated to have information superﬂuous to the task of speaker veriﬁcation. [sent-957, score-0.471]
</p><p>76 In the anticorrelation procedure, a single direction is eliminated: the one that maximizes the average class-conditional covariance between the two systems being combined. [sent-958, score-0.453]
</p><p>77 That is, m = s + c, where s is a speaker supervector and c a channel supervector. [sent-962, score-0.553]
</p><p>78 In order to estimate the matrix u, a database with several samples for each speaker (as the one required for NAP) is needed. [sent-965, score-0.496]
</p><p>79 We can see that, for these systems, the anticorrelation kernel does not reduce the class-dependent correlations between pairs of systems enough to result in a gain in the combination performance. [sent-1008, score-0.568]
</p><p>80 The fact that applying the anticorrelation kernel does not result in a signiﬁcant reduction of the class-dependent correlations between the systems indicates that the K vectors computed from the held-out data are not a good estimation of the K vectors in the test data. [sent-1009, score-0.456]
</p><p>81 We believe that the observations presented in this section do not invalidate the usefulness of the method for the speaker veriﬁcation task. [sent-1028, score-0.471]
</p><p>82 (2007), for ISVC to work, a well-balanced database is required where samples from several different recording conditions for each speaker are available. [sent-1030, score-0.496]
</p><p>83 Conclusions While speaker veriﬁcation systems have seen large gains in performance from ad hoc combination of several component systems, a uniﬁed framework for joint development of a combined system that ensures system diversity has been lacking. [sent-1035, score-1.046]
</p><p>84 Based on this result we presented a technique for taking into account the characteristics of the scores from a set of ﬁxed existing systems during the development of a new SVM system in order to improve the combined system performance. [sent-1038, score-0.429]
</p><p>85 This is realized through a modiﬁcation of the SVM optimization problem via the introduction of a regularization term involving the covariance between the scores of the previously existing systems and the input features to the SVM, explicitly encouraging diversity of the resulting system ensemble. [sent-1039, score-0.539]
</p><p>86 We demonstrate a performance gain of around 19% for a four-way combination using the anticorrelation kernel with respect to the performance of the combination obtained without anticorrelation. [sent-1044, score-0.681]
</p><p>87 When the same four speaker veriﬁcation systems are compensated for intersession variability, the gains from the anticorrelation method disappear. [sent-1045, score-0.943]
</p><p>88 The anticorrelation method can then be seen as a replacement for intersession variability compensation methods when the right databases are not available for the estimation of the matrices needed for those methods. [sent-1048, score-0.492]
</p><p>89 We note that the anticorrelation technique is general in that it can be applied to any binary classiﬁcation task for which more than one system can be trained and at least one of them is an SVM. [sent-1054, score-0.484]
</p><p>90 The values tda and tdb are determined such that eb|a (tda ) = EERs and ea|b (tdb ) = EERs , respectively (with ea|b and eb|a deﬁned in (2) and (3)), and they are given by ∗ tdb = σb /σ(ts − µb ) + µb  ∗ tda = σa /σ(ts − µa ) + µa . [sent-1075, score-0.436]
</p><p>91 ∗ ∗ ∗ If tda ≤ tdb then there will be a threshold td between tda and tdb for which ea|b (td ) = eb|a (td ) = EERd ≤ EERs . [sent-1076, score-0.464]
</p><p>92 Fusion of heterogeneous speaker recognition systems in the STBU submission for the NIST speaker recognition evaluation 2006. [sent-1130, score-1.069]
</p><p>93 Support vector machines using GMM supervectors for speaker veriﬁcation. [sent-1153, score-0.471]
</p><p>94 Continuous prosodic features and formant modeling with joint factor analysis for speaker veriﬁcation. [sent-1159, score-0.582]
</p><p>95 Parameterization of prosodic feature distrio butions for SVM modeling in speaker recognition. [sent-1173, score-0.562]
</p><p>96 The o contribution of cepstral and stylistic features to SRI’s 2005 NIST speaker recognition evaluation system. [sent-1183, score-0.592]
</p><p>97 An anticorrelation kernel for improved system combinao tion in speaker veriﬁcation. [sent-1188, score-0.984]
</p><p>98 Experiments in speaker veriﬁcation using factor analysis likelihood ratios. [sent-1233, score-0.471]
</p><p>99 A straightforward and efﬁcient implementation of the factor analysis model for speaker veriﬁcation. [sent-1272, score-0.471]
</p><p>100 The SuperSID project: Exploiting high-level information for high-accuracy speaker recognition. [sent-1303, score-0.471]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('speaker', 0.471), ('eer', 0.446), ('anticorrelation', 0.35), ('diversity', 0.143), ('eers', 0.139), ('scores', 0.134), ('errer', 0.131), ('hriberg', 0.131), ('onmez', 0.131), ('nticorrelation', 0.124), ('ubsystem', 0.124), ('ferrer', 0.118), ('combination', 0.112), ('tda', 0.109), ('tdb', 0.109), ('system', 0.106), ('dcf', 0.102), ('ensemble', 0.098), ('isvc', 0.095), ('impostor', 0.087), ('raining', 0.076), ('sb', 0.074), ('intersession', 0.073), ('tumer', 0.073), ('speech', 0.067), ('kk', 0.067), ('svm', 0.066), ('mllr', 0.066), ('ernel', 0.065), ('vm', 0.064), ('veri', 0.063), ('score', 0.061), ('brummer', 0.058), ('nap', 0.058), ('prosodic', 0.058), ('individual', 0.058), ('xl', 0.057), ('kernel', 0.057), ('covariance', 0.054), ('eb', 0.053), ('features', 0.053), ('combiner', 0.051), ('supervector', 0.051), ('ghosh', 0.05), ('systems', 0.049), ('ea', 0.048), ('combinations', 0.046), ('nist', 0.046), ('creation', 0.044), ('correlation', 0.044), ('variability', 0.044), ('kenny', 0.044), ('preexisting', 0.044), ('shriberg', 0.044), ('stolcke', 0.044), ('sri', 0.043), ('recognition', 0.039), ('wt', 0.038), ('reynolds', 0.037), ('eerd', 0.036), ('fumera', 0.036), ('kajarekar', 0.036), ('mg', 0.036), ('odyssey', 0.036), ('combined', 0.034), ('mahalanobis', 0.033), ('feature', 0.033), ('posterior', 0.031), ('channel', 0.031), ('classi', 0.03), ('brown', 0.029), ('kt', 0.029), ('cepstral', 0.029), ('feat', 0.029), ('llr', 0.029), ('nmez', 0.029), ('trained', 0.028), ('transforms', 0.028), ('campbell', 0.028), ('icassp', 0.028), ('td', 0.028), ('training', 0.027), ('svms', 0.027), ('xk', 0.025), ('samples', 0.025), ('train', 0.025), ('compensation', 0.025), ('gmm', 0.025), ('speakers', 0.025), ('unequal', 0.025), ('performance', 0.025), ('ers', 0.024), ('fc', 0.024), ('upper', 0.023), ('false', 0.023), ('peak', 0.022), ('sv', 0.022), ('mv', 0.022), ('ta', 0.022), ('fusion', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="8-tfidf-1" href="./jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems.html">8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</a></p>
<p>Author: Luciana Ferrer, Kemal Sönmez, Elizabeth Shriberg</p><p>Abstract: We present a method for training support vector machine (SVM)-based classiﬁcation systems for combination with other classiﬁcation systems designed for the same task. Ideally, a new system should be designed such that, when combined with existing systems, the resulting performance is optimized. We present a simple model for this problem and use the understanding gained from this analysis to propose a method to achieve better combination performance when training SVM systems. We include a regularization term in the SVM objective function that aims to reduce the average class-conditional covariance between the resulting scores and the scores produced by the existing systems, introducing a trade-off between such covariance and the system’s individual performance. That is, the new system “takes one for the team”, falling somewhat short of its best possible performance in order to increase the diversity of the ensemble. We report results on the NIST 2005 and 2006 speaker recognition evaluations (SREs) for a variety of subsystems. We show a gain of 19% on the equal error rate (EER) of a combination of four systems when applying the proposed method with respect to the performance obtained when the four systems are trained independently of each other. Keywords: system combination, ensemble diversity, multiple classiﬁer systems, support vector machines, speaker recognition, kernel methods ∗. This author performed part of the work presented in this paper while at the Information Systems Laboratory, Department of Electrical Engineering, Stanford University. c 2009 Luciana Ferrer, Kemal S¨ nmez and Elizabeth Shriberg. o ¨ F ERRER , S ONMEZ AND S HRIBERG</p><p>2 0.058580946 <a title="8-tfidf-2" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>Author: Steffen Bickel, Michael Brückner, Tobias Scheffer</p><p>Abstract: We address classiﬁcation problems for which the training instances are governed by an input distribution that is allowed to differ arbitrarily from the test distribution—problems also referred to as classiﬁcation under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. The problem of learning under covariate shift can be written as an integrated optimization problem. Instantiating the general optimization problem leads to a kernel logistic regression and an exponential model classiﬁer for covariate shift. The optimization problem is convex under certain conditions; our ﬁndings also clarify the relationship to the known kernel mean matching procedure. We report on experiments on problems of spam ﬁltering, text classiﬁcation, and landmine detection. Keywords: covariate shift, discriminative learning, transfer learning</p><p>3 0.055664536 <a title="8-tfidf-3" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>Author: Pradip Ghanty, Samrat Paul, Nikhil R. Pal</p><p>Abstract: In this paper we propose a new multilayer classiﬁer architecture. The proposed hybrid architecture has two cascaded modules: feature extraction module and classiﬁcation module. In the feature extraction module we use the multilayered perceptron (MLP) neural networks, although other tools such as radial basis function (RBF) networks can be used. In the classiﬁcation module we use support vector machines (SVMs)—here also other tool such as MLP or RBF can be used. The feature extraction module has several sub-modules each of which is expected to extract features capturing the discriminating characteristics of different areas of the input space. The classiﬁcation module classiﬁes the data based on the extracted features. The resultant architecture with MLP in feature extraction module and SVM in classiﬁcation module is called NEUROSVM. The NEUROSVM is tested on twelve benchmark data sets and the performance of the NEUROSVM is found to be better than both MLP and SVM. We also compare the performance of proposed architecture with that of two ensemble methods: majority voting and averaging. Here also the NEUROSVM is found to perform better than these two ensemble methods. Further we explore the use of MLP and RBF in the classiﬁcation module of the proposed architecture. The most attractive feature of NEUROSVM is that it practically eliminates the severe dependency of SVM on the choice of kernel. This has been veriﬁed with respect to both linear and non-linear kernels. We have also demonstrated that for the feature extraction module, the full training of MLPs is not needed. Keywords: feature extraction, neural networks (NNs), support vector machines (SVMs), hybrid system, majority voting, averaging c 2009 Pradip Ghanty, Samrat Paul and Nikhil R. Pal. G HANTY, PAUL AND PAL</p><p>4 0.055078775 <a title="8-tfidf-4" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>Author: Eugene Tuv, Alexander Borisov, George Runger, Kari Torkkola</p><p>Abstract: Predictive models beneﬁt from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for ﬁlters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach. Keywords: trees, resampling, importance, masking, residuals</p><p>5 0.049495783 <a title="8-tfidf-5" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>Author: Omid Madani, Michael Connor, Wiley Greiner</p><p>Abstract: Many learning tasks, such as large-scale text categorization and word prediction, can beneﬁt from efﬁcient training and classiﬁcation when the number of classes, in addition to instances and features, is large, that is, in the thousands and beyond. We investigate the learning of sparse class indices to address this challenge. An index is a mapping from features to classes. We compare the index-learning methods against other techniques, including one-versus-rest and top-down classiﬁcation using perceptrons and support vector machines. We ﬁnd that index learning is highly advantageous for space and time efﬁciency, at both training and classiﬁcation times. Moreover, this approach yields similar and at times better accuracies. On problems with hundreds of thousands of instances and thousands of classes, the index is learned in minutes, while other methods can take hours or days. As we explain, the design of the learning update enables conveniently constraining each feature to connect to a small subset of the classes in the index. This constraint is crucial for scalability. Given an instance with l active (positive-valued) features, each feature on average connecting to d classes in the index (in the order of 10s in our experiments), update and classiﬁcation take O(dl log(dl)). Keywords: index learning, many-class learning, multiclass learning, online learning, text categorization</p><p>6 0.048331238 <a title="8-tfidf-6" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>7 0.047823407 <a title="8-tfidf-7" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>8 0.04750384 <a title="8-tfidf-8" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>9 0.045682959 <a title="8-tfidf-9" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>10 0.041884754 <a title="8-tfidf-10" href="./jmlr-2009-Hybrid_MPI_OpenMP_Parallel_Linear_Support_Vector_Machine_Training.html">39 jmlr-2009-Hybrid MPI OpenMP Parallel Linear Support Vector Machine Training</a></p>
<p>11 0.040930178 <a title="8-tfidf-11" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>12 0.039950978 <a title="8-tfidf-12" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>13 0.036042668 <a title="8-tfidf-13" href="./jmlr-2009-SGD-QN%3A_Careful_Quasi-Newton_Stochastic_Gradient_Descent.html">83 jmlr-2009-SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent</a></p>
<p>14 0.0350977 <a title="8-tfidf-14" href="./jmlr-2009-Identification_of_Recurrent_Neural_Networks_by_Bayesian_Interrogation_Techniques.html">40 jmlr-2009-Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques</a></p>
<p>15 0.035047948 <a title="8-tfidf-15" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>16 0.03391815 <a title="8-tfidf-16" href="./jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>17 0.033728488 <a title="8-tfidf-17" href="./jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>18 0.030165238 <a title="8-tfidf-18" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>19 0.029609809 <a title="8-tfidf-19" href="./jmlr-2009-Optimized_Cutting_Plane_Algorithm_for_Large-Scale_Risk_Minimization.html">69 jmlr-2009-Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization</a></p>
<p>20 0.0291503 <a title="8-tfidf-20" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.169), (1, -0.048), (2, 0.083), (3, -0.052), (4, 0.061), (5, -0.044), (6, 0.052), (7, 0.051), (8, 0.04), (9, 0.053), (10, 0.076), (11, 0.003), (12, 0.042), (13, -0.135), (14, 0.004), (15, -0.073), (16, -0.103), (17, -0.01), (18, -0.099), (19, 0.041), (20, 0.036), (21, 0.022), (22, -0.165), (23, 0.087), (24, -0.139), (25, 0.042), (26, -0.116), (27, 0.12), (28, -0.161), (29, -0.01), (30, 0.028), (31, 0.147), (32, 0.242), (33, 0.002), (34, -0.093), (35, -0.168), (36, 0.118), (37, -0.016), (38, -0.101), (39, 0.047), (40, 0.209), (41, -0.034), (42, 0.126), (43, 0.331), (44, -0.056), (45, -0.031), (46, -0.059), (47, -0.144), (48, -0.064), (49, 0.133)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91424805 <a title="8-lsi-1" href="./jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems.html">8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</a></p>
<p>Author: Luciana Ferrer, Kemal Sönmez, Elizabeth Shriberg</p><p>Abstract: We present a method for training support vector machine (SVM)-based classiﬁcation systems for combination with other classiﬁcation systems designed for the same task. Ideally, a new system should be designed such that, when combined with existing systems, the resulting performance is optimized. We present a simple model for this problem and use the understanding gained from this analysis to propose a method to achieve better combination performance when training SVM systems. We include a regularization term in the SVM objective function that aims to reduce the average class-conditional covariance between the resulting scores and the scores produced by the existing systems, introducing a trade-off between such covariance and the system’s individual performance. That is, the new system “takes one for the team”, falling somewhat short of its best possible performance in order to increase the diversity of the ensemble. We report results on the NIST 2005 and 2006 speaker recognition evaluations (SREs) for a variety of subsystems. We show a gain of 19% on the equal error rate (EER) of a combination of four systems when applying the proposed method with respect to the performance obtained when the four systems are trained independently of each other. Keywords: system combination, ensemble diversity, multiple classiﬁer systems, support vector machines, speaker recognition, kernel methods ∗. This author performed part of the work presented in this paper while at the Information Systems Laboratory, Department of Electrical Engineering, Stanford University. c 2009 Luciana Ferrer, Kemal S¨ nmez and Elizabeth Shriberg. o ¨ F ERRER , S ONMEZ AND S HRIBERG</p><p>2 0.42691746 <a title="8-lsi-2" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>Author: Pradip Ghanty, Samrat Paul, Nikhil R. Pal</p><p>Abstract: In this paper we propose a new multilayer classiﬁer architecture. The proposed hybrid architecture has two cascaded modules: feature extraction module and classiﬁcation module. In the feature extraction module we use the multilayered perceptron (MLP) neural networks, although other tools such as radial basis function (RBF) networks can be used. In the classiﬁcation module we use support vector machines (SVMs)—here also other tool such as MLP or RBF can be used. The feature extraction module has several sub-modules each of which is expected to extract features capturing the discriminating characteristics of different areas of the input space. The classiﬁcation module classiﬁes the data based on the extracted features. The resultant architecture with MLP in feature extraction module and SVM in classiﬁcation module is called NEUROSVM. The NEUROSVM is tested on twelve benchmark data sets and the performance of the NEUROSVM is found to be better than both MLP and SVM. We also compare the performance of proposed architecture with that of two ensemble methods: majority voting and averaging. Here also the NEUROSVM is found to perform better than these two ensemble methods. Further we explore the use of MLP and RBF in the classiﬁcation module of the proposed architecture. The most attractive feature of NEUROSVM is that it practically eliminates the severe dependency of SVM on the choice of kernel. This has been veriﬁed with respect to both linear and non-linear kernels. We have also demonstrated that for the feature extraction module, the full training of MLPs is not needed. Keywords: feature extraction, neural networks (NNs), support vector machines (SVMs), hybrid system, majority voting, averaging c 2009 Pradip Ghanty, Samrat Paul and Nikhil R. Pal. G HANTY, PAUL AND PAL</p><p>3 0.3938832 <a title="8-lsi-3" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>Author: Eugene Tuv, Alexander Borisov, George Runger, Kari Torkkola</p><p>Abstract: Predictive models beneﬁt from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for ﬁlters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach. Keywords: trees, resampling, importance, masking, residuals</p><p>4 0.37300742 <a title="8-lsi-4" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>Author: Juan José del Coz, Jorge Díez, Antonio Bahamonde</p><p>Abstract: Nondeterministic classiﬁers are deﬁned as those allowed to predict more than one class for some entries from an input space. Given that the true class should be included in predictions and the number of classes predicted should be as small as possible, these kind of classiﬁers can be considered as Information Retrieval (IR) procedures. In this paper, we propose a family of IR loss functions to measure the performance of nondeterministic learners. After discussing such measures, we derive an algorithm for learning optimal nondeterministic hypotheses. Given an entry from the input space, the algorithm requires the posterior probabilities to compute the subset of classes with the lowest expected loss. From a general point of view, nondeterministic classiﬁers provide an improvement in the proportion of predictions that include the true class compared to their deterministic counterparts; the price to be paid for this increase is usually a tiny proportion of predictions with more than one class. The paper includes an extensive experimental study using three deterministic learners to estimate posterior probabilities: a multiclass Support Vector Machine (SVM), a Logistic Regression, and a Na¨ve Bayes. The data sets considered comprise both UCI ı multi-class learning tasks and microarray expressions of different kinds of cancer. We successfully compare nondeterministic classiﬁers with other alternative approaches. Additionally, we shall see how the quality of posterior probabilities (measured by the Brier score) determines the goodness of nondeterministic predictions. Keywords: nondeterministic, multiclassiﬁcation, reject option, multi-label classiﬁcation, posterior probabilities</p><p>5 0.32159379 <a title="8-lsi-5" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>Author: Dirk Gorissen, Tom Dhaene, Filip De Turck</p><p>Abstract: Due to the scale and computational complexity of currently used simulation codes, global surrogate (metamodels) models have become indispensable tools for exploring and understanding the design space. Due to their compact formulation they are cheap to evaluate and thus readily facilitate visualization, design space exploration, rapid prototyping, and sensitivity analysis. They can also be used as accurate building blocks in design packages or larger simulation environments. Consequently, there is great interest in techniques that facilitate the construction of such approximation models while minimizing the computational cost and maximizing model accuracy. Many surrogate model types exist (Support Vector Machines, Kriging, Neural Networks, etc.) but no type is optimal in all circumstances. Nor is there any hard theory available that can help make this choice. In this paper we present an automatic approach to the model type selection problem. We describe an adaptive global surrogate modeling environment with adaptive sampling, driven by speciated evolution. Different model types are evolved cooperatively using a Genetic Algorithm (heterogeneous evolution) and compete to approximate the iteratively selected data. In this way the optimal model type and complexity for a given data set or simulation code can be dynamically determined. Its utility and performance is demonstrated on a number of problems where it outperforms traditional sequential execution of each model type. Keywords: model type selection, genetic algorithms, global surrogate modeling, function approximation, active learning, adaptive sampling</p><p>6 0.31058517 <a title="8-lsi-6" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>7 0.29885483 <a title="8-lsi-7" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>8 0.29376602 <a title="8-lsi-8" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>9 0.24256736 <a title="8-lsi-9" href="./jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>10 0.1982346 <a title="8-lsi-10" href="./jmlr-2009-Hybrid_MPI_OpenMP_Parallel_Linear_Support_Vector_Machine_Training.html">39 jmlr-2009-Hybrid MPI OpenMP Parallel Linear Support Vector Machine Training</a></p>
<p>11 0.19515818 <a title="8-lsi-11" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>12 0.19463168 <a title="8-lsi-12" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>13 0.1940116 <a title="8-lsi-13" href="./jmlr-2009-Optimized_Cutting_Plane_Algorithm_for_Large-Scale_Risk_Minimization.html">69 jmlr-2009-Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization</a></p>
<p>14 0.19352792 <a title="8-lsi-14" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>15 0.19148527 <a title="8-lsi-15" href="./jmlr-2009-Settable_Systems%3A_An_Extension_of_Pearl%27s_Causal_Model_with_Optimization%2C_Equilibrium%2C_and_Learning.html">85 jmlr-2009-Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning</a></p>
<p>16 0.18635525 <a title="8-lsi-16" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>17 0.18208121 <a title="8-lsi-17" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>18 0.17531702 <a title="8-lsi-18" href="./jmlr-2009-Online_Learning_with_Sample_Path_Constraints.html">67 jmlr-2009-Online Learning with Sample Path Constraints</a></p>
<p>19 0.17034501 <a title="8-lsi-19" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>20 0.16968684 <a title="8-lsi-20" href="./jmlr-2009-Identification_of_Recurrent_Neural_Networks_by_Bayesian_Interrogation_Techniques.html">40 jmlr-2009-Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.016), (11, 0.019), (19, 0.011), (26, 0.024), (38, 0.025), (52, 0.059), (55, 0.033), (58, 0.03), (66, 0.066), (68, 0.022), (90, 0.584), (96, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95778072 <a title="8-lda-1" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>Author: Brijnesh J. Jain, Klaus Obermayer</p><p>Abstract: Finite structures such as point patterns, strings, trees, and graphs occur as ”natural” representations of structured data in different application areas of machine learning. We develop the theory of structure spaces and derive geometrical and analytical concepts such as the angle between structures and the derivative of functions on structures. In particular, we show that the gradient of a differentiable structural function is a well-deﬁned structure pointing in the direction of steepest ascent. Exploiting the properties of structure spaces, it will turn out that a number of problems in structural pattern recognition such as central clustering or learning in structured output spaces can be formulated as optimization problems with cost functions that are locally Lipschitz. Hence, methods from nonsmooth analysis are applicable to optimize those cost functions. Keywords: graphs, graph matching, learning in structured domains, nonsmooth optimization</p><p>same-paper 2 0.94681108 <a title="8-lda-2" href="./jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems.html">8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</a></p>
<p>Author: Luciana Ferrer, Kemal Sönmez, Elizabeth Shriberg</p><p>Abstract: We present a method for training support vector machine (SVM)-based classiﬁcation systems for combination with other classiﬁcation systems designed for the same task. Ideally, a new system should be designed such that, when combined with existing systems, the resulting performance is optimized. We present a simple model for this problem and use the understanding gained from this analysis to propose a method to achieve better combination performance when training SVM systems. We include a regularization term in the SVM objective function that aims to reduce the average class-conditional covariance between the resulting scores and the scores produced by the existing systems, introducing a trade-off between such covariance and the system’s individual performance. That is, the new system “takes one for the team”, falling somewhat short of its best possible performance in order to increase the diversity of the ensemble. We report results on the NIST 2005 and 2006 speaker recognition evaluations (SREs) for a variety of subsystems. We show a gain of 19% on the equal error rate (EER) of a combination of four systems when applying the proposed method with respect to the performance obtained when the four systems are trained independently of each other. Keywords: system combination, ensemble diversity, multiple classiﬁer systems, support vector machines, speaker recognition, kernel methods ∗. This author performed part of the work presented in this paper while at the Information Systems Laboratory, Department of Electrical Engineering, Stanford University. c 2009 Luciana Ferrer, Kemal S¨ nmez and Elizabeth Shriberg. o ¨ F ERRER , S ONMEZ AND S HRIBERG</p><p>3 0.91712463 <a title="8-lda-3" href="./jmlr-2009-The_P-Norm_Push%3A_A_Simple_Convex_Ranking_Algorithm_that_Concentrates_at_the_Top_of_the_List.html">95 jmlr-2009-The P-Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List</a></p>
<p>Author: Cynthia Rudin</p><p>Abstract: We are interested in supervised ranking algorithms that perform especially well near the top of the ranked list, and are only required to perform sufﬁciently well on the rest of the list. In this work, we provide a general form of convex objective that gives high-scoring examples more importance. This “push” near the top of the list can be chosen arbitrarily large or small, based on the preference of the user. We choose ℓ p -norms to provide a speciﬁc type of push; if the user sets p larger, the objective concentrates harder on the top of the list. We derive a generalization bound based on the p-norm objective, working around the natural asymmetry of the problem. We then derive a boosting-style algorithm for the problem of ranking with a push at the top. The usefulness of the algorithm is illustrated through experiments on repository data. We prove that the minimizer of the algorithm’s objective is unique in a speciﬁc sense. Furthermore, we illustrate how our objective is related to quality measurements for information retrieval. Keywords: ranking, RankBoost, generalization bounds, ROC, information retrieval</p><p>4 0.67663735 <a title="8-lda-4" href="./jmlr-2009-Incorporating_Functional_Knowledge_in_Neural_Networks.html">42 jmlr-2009-Incorporating Functional Knowledge in Neural Networks</a></p>
<p>Author: Charles Dugas, Yoshua Bengio, François Bélisle, Claude Nadeau, René Garcia</p><p>Abstract: Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in its two arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of Lipschitz1 functions with these and other properties. We apply this new class of functions to the task of modelling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints. Keywords: neural networks, universal approximation, monotonicity, convexity, call options</p><p>5 0.63359302 <a title="8-lda-5" href="./jmlr-2009-Settable_Systems%3A_An_Extension_of_Pearl%27s_Causal_Model_with_Optimization%2C_Equilibrium%2C_and_Learning.html">85 jmlr-2009-Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning</a></p>
<p>Author: Halbert White, Karim Chalak</p><p>Abstract: Judea Pearl’s Causal Model is a rich framework that provides deep insight into the nature of causal relations. As yet, however, the Pearl Causal Model (PCM) has had a lesser impact on economics or econometrics than on other disciplines. This may be due in part to the fact that the PCM is not as well suited to analyzing structures that exhibit features of central interest to economists and econometricians: optimization, equilibrium, and learning. We offer the settable systems framework as an extension of the PCM that permits causal discourse in systems embodying optimization, equilibrium, and learning. Because these are common features of physical, natural, or social systems, our framework may prove generally useful for machine learning. Important features distinguishing the settable system framework from the PCM are its countable dimensionality and the use of partitioning and partition-speciﬁc response functions to accommodate the behavior of optimizing and interacting agents and to eliminate the requirement of a unique ﬁxed point for the system. Reﬁnements of the PCM include the settable systems treatment of attributes, the causal role of exogenous variables, and the dual role of variables as causes and responses. A series of closely related machine learning examples and examples from game theory and machine learning with feedback demonstrates some limitations of the PCM and motivates the distinguishing features of settable systems. Keywords: equations causal models, game theory, machine learning, recursive estimation, simultaneous</p><p>6 0.63214344 <a title="8-lda-6" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>7 0.62979543 <a title="8-lda-7" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>8 0.59947801 <a title="8-lda-8" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>9 0.59592098 <a title="8-lda-9" href="./jmlr-2009-Using_Local_Dependencies_within_Batches_to_Improve_Large_Margin_Classifiers.html">99 jmlr-2009-Using Local Dependencies within Batches to Improve Large Margin Classifiers</a></p>
<p>10 0.59253263 <a title="8-lda-10" href="./jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">70 jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<p>11 0.58362603 <a title="8-lda-11" href="./jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification.html">45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</a></p>
<p>12 0.58351028 <a title="8-lda-12" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>13 0.57532918 <a title="8-lda-13" href="./jmlr-2009-Nearest_Neighbor_Clustering%3A_A_Baseline_Method_for_Consistent_Clustering_with_Arbitrary_Objective_Functions.html">59 jmlr-2009-Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions</a></p>
<p>14 0.56696063 <a title="8-lda-14" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>15 0.56357586 <a title="8-lda-15" href="./jmlr-2009-Fast_ApproximatekNN_Graph_Construction_for_High_Dimensional_Data_via_Recursive_Lanczos_Bisection.html">34 jmlr-2009-Fast ApproximatekNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</a></p>
<p>16 0.55474466 <a title="8-lda-16" href="./jmlr-2009-Hybrid_MPI_OpenMP_Parallel_Linear_Support_Vector_Machine_Training.html">39 jmlr-2009-Hybrid MPI OpenMP Parallel Linear Support Vector Machine Training</a></p>
<p>17 0.55320376 <a title="8-lda-17" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>18 0.54898423 <a title="8-lda-18" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>19 0.54396749 <a title="8-lda-19" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>20 0.54080808 <a title="8-lda-20" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
