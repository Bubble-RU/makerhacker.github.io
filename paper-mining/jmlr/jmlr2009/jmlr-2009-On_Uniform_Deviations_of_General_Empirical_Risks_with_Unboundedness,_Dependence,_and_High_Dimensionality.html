<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 jmlr-2009-On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-65" href="#">jmlr2009-65</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>65 jmlr-2009-On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality</h1>
<br/><p>Source: <a title="jmlr-2009-65-pdf" href="http://jmlr.org/papers/volume10/jiang09a/jiang09a.pdf">pdf</a></p><p>Author: Wenxin Jiang</p><p>Abstract: The statistical learning theory of risk minimization depends heavily on probability bounds for uniform deviations of the empirical risks. Classical probability bounds using Hoeffding’s inequality cannot accommodate more general situations with unbounded loss and dependent data. The current paper introduces an inequality that extends Hoeffding’s inequality to handle these more general situations. We will apply this inequality to provide probability bounds for uniform deviations in a very general framework, which can involve discrete decision rules, unbounded loss, and a dependence structure that can be more general than either martingale or strong mixing. We will consider two examples with high dimensional predictors: autoregression (AR) with ℓ1 -loss, and ARX model with variable selection for sign classiﬁcation, which uses both lagged responses and exogenous predictors. Keywords: dependence, empirical risk, probability bound, unbounded loss, uniform deviation</p><p>Reference: <a title="jmlr-2009-65-reference" href="../jmlr2009_reference/jmlr-2009-On_Uniform_Deviations_of_General_Empirical_Risks_with_Unboundedness%2C_Dependence%2C_and_High_Dimensionality_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Department of Statistics Northwestern University Evanston, IL 60208, USA  Editor: G´ bor Lugosi a  Abstract The statistical learning theory of risk minimization depends heavily on probability bounds for uniform deviations of the empirical risks. [sent-2, score-0.081]
</p><p>2 Classical probability bounds using Hoeffding’s inequality cannot accommodate more general situations with unbounded loss and dependent data. [sent-3, score-0.107]
</p><p>3 The current paper introduces an inequality that extends Hoeffding’s inequality to handle these more general situations. [sent-4, score-0.102]
</p><p>4 We will apply this inequality to provide probability bounds for uniform deviations in a very general framework, which can involve discrete decision rules, unbounded loss, and a dependence structure that can be more general than either martingale or strong mixing. [sent-5, score-0.337]
</p><p>5 Keywords: dependence, empirical risk, probability bound, unbounded loss, uniform deviation  1. [sent-7, score-0.133]
</p><p>6 This again shows the dependence of the risk performance on the probability of uniform deviation. [sent-23, score-0.105]
</p><p>7 When ρ has sufﬁciently thin tail in the distribution and when ωt ’s have certain kind of decaying dependence over t, we derive bounds of the form n  n  t=1  t=1  P[sup |n−1 ∑ ρ(ωt , b) − n−1 ∑ Eρ(ωt , b)| > n−0. [sent-27, score-0.103]
</p><p>8 5 )) despite high dimensionality in b, dependence in ωt , and unbounded loss function ρ. [sent-30, score-0.116]
</p><p>9 This allows unbounded loss and a framework of dependence that is more general than strong mixing, which is therefore more general than previous works using strong mixing (e. [sent-32, score-0.346]
</p><p>10 An Inequality We ﬁrst introduce an inequality that is more general than Hoeffding’s inequality (Hoeffding, 1963). [sent-40, score-0.102]
</p><p>11 In addition to a term that is of an exponential form as in the Hoeffding’s inequality, it also includes a term to gauge the dependence, and a term to control the unboundedness of the random variables. [sent-42, score-0.194]
</p><p>12 The second term will be called the ‘dependence term’ since it is related to the dependence described in the framework of L1 -mixingale (see, e. [sent-59, score-0.095]
</p><p>13 , Chapter 16, Davidson, 1994), which is more general than either martingale or strong mixing. [sent-61, score-0.089]
</p><p>14 When {ρt } is a sequence of martingale differences, the dependence term vanishes. [sent-62, score-0.129]
</p><p>15 If {ρt } is strong mixing with coefﬁcients αm , and has bounded Lq norms (q > 1), then Theorem 14. [sent-63, score-0.202]
</p><p>16 2 of Davidson (1994) would imply that the 1−1/q dependence term decreases according to order O(αm ) as m increases. [sent-64, score-0.095]
</p><p>17 The mixingale formulation of the dependence term can also handle a process ρt that is not strong mixing. [sent-66, score-0.183]
</p><p>18 We will provide an example below when ρt is not strong mixing but is approximable to a strong mixing process, where we can still make the dependence term small for large m. [sent-67, score-0.445]
</p><p>19 The problem of strong mixing is that a function of a mixing sequence (even an independent sequence) that depends on an inﬁnite number of lags is not generally mixing. [sent-69, score-0.381]
</p><p>20 t=1  Then apply the triangular inequality and note that the dependence term is proportional to n  n  n  n−1 ∑ E|E(ρt |Ft−m ) − Eρt | ≤ n−1 ∑ E|E(ρt,k |Ft−m ) − Eρt,k | + 2n−1 ∑ E|ρt − ρt,k | t=1  t=1 ∞ 1−1/q 6c2 αm ({ρt,k }t=−∞ ) + 2c1 νk . [sent-78, score-0.186]
</p><p>21 When k indicates the number of lags involved as in the following example, one can choose k(m) ≈ m/2. [sent-81, score-0.086]
</p><p>22 ) Then ρt is not necessarily strong mixing even when Vt ’s are independent innovations, even when |θ j | decreases very rapidly, due to the inﬁnitely many lags involved (see, e. [sent-86, score-0.261]
</p><p>23 , when innovations are independent) with mixing coefﬁcient αm ({Vt }∞ ). [sent-93, score-0.154]
</p><p>24 Then the strong mixing coefﬁcient of ρt,k satisﬁes −∞ ∞ αm ({ρt,k }t=−∞ ) ≤ αm−k ({Vt }∞ ), −∞  since the ρt,k depends on lags Vt ,Vt−1 , . [sent-94, score-0.261]
</p><p>25 Note that ||ρt,k ||q ≤ ∑k |θ j | supt ||Vt−k ||q ≤ j=0 |θ|1 supt ||Vt ||q which can be taken as the constant c2 . [sent-98, score-0.136]
</p><p>26 Now note that the dependence term of interest is proportional to n  n−1 ∑ E|E(ρt |Ft−m ) − Eρt | t=1  ∞ ≤ 6c2 αm ({ρt,k }t=−∞ )1−1/q + 2c1 νk  ≤ 6c2 αm−k ({Vt }∞ )1−1/q + 2c1 νk . [sent-99, score-0.095]
</p><p>27 This shows that the current formulation of the inequality can handle dependence that is more general than strong mixing. [sent-101, score-0.166]
</p><p>28 Similarly is P[|U j,n | > nε/(2m)] ≤ 2e n P[|Vn | > nε/2] ≤ (2/ε)n−1 E|Vn | ≤ (2/ε)n−1 ∑t=1 E|E(Xt |Ft−m ) − EXt | using the Markov inequality and the triangular inequalities. [sent-145, score-0.091]
</p><p>29 The inequality appearing in the current lemma holds without assumption of a dependence structure. [sent-150, score-0.111]
</p><p>30 The third term is a probability of a deter1 ministic event, which is zero if the righthand side of the triplex inequality in Theorem 1 does not 2 2 2 n exceed one. [sent-157, score-0.142]
</p><p>31 The connection between the pointwise and uniform deviations can be obtained by covering B with many (say, Γ) smaller sets Bi ’s, so that B ⊂ ∪Γ Bi . [sent-167, score-0.117]
</p><p>32 Choose bi to be some parameter located i=1 in Bi for each i. [sent-168, score-0.305]
</p><p>33 The ﬁrst term involves pointwise deviations and can be bounded by the inequality derived before. [sent-172, score-0.185]
</p><p>34 For example, the classiﬁcation error can be written as ρ(ωt , b) = |yt − I[xt′ b > 0]|, which is discontinuous in b, when a linear boundary (in predictor xt ) is used to classify a {0, 1} valued label yt . [sent-178, score-0.659]
</p><p>35 Let ρ(ωt , b) be of the form ρ(ωt , b) = ft (b, At (b)) where ft (·, ·) is continuous in the ﬁrst argument, but the second argument At (b) = I[g(ωt , b) > 0] for some ﬁxed function g that determines a decision boundary. [sent-181, score-0.748]
</p><p>36 The function ft depends on t through observation ωt . [sent-182, score-0.374]
</p><p>37 ˆ Proposition 2 For each parameter b in a convex set Bi that contains bi , denote Tn (b) = R(b) − −1 n ρ(ω , b) and for each t, ω is measurable -F (from an increasing ˆ ˆ E R(b), where R(b) = n ∑t=1 t t t sequence of σ-ﬁelds). [sent-186, score-0.357]
</p><p>38 Assume that ρ has the form ρ(ωt , b) = ft (b, At (b)) where At (b) = I[g(ωt , b) > 0] for some ﬁxed function g that determines a decision boundary. [sent-187, score-0.374]
</p><p>39 Denote λ = supb,b∗ ∈Bi |b − b∗ |q and Mit = | ft (bi , 1) − ft (bi , 0)|. [sent-190, score-0.748]
</p><p>40 In the ‘continuous case’, ft (b, a) is constant in a ∈ {0, 1}. [sent-194, score-0.374]
</p><p>41 In the ‘discontinuous case’, ft (b, a) is constant in b and we can drop the ﬁrst term in the above bound, since the Lipshitz constant Nit can be taken as 0. [sent-197, score-0.409]
</p><p>42 The result above holds also for the more general ‘mixed case’ when ft (b, a) varies with both b and a. [sent-198, score-0.374]
</p><p>43 Assumption A2 can often be validated by bounding the partial derivative of ft (b, a) on the ﬁrst argument. [sent-200, score-0.374]
</p><p>44 In a later example with L1 loss we will use a triangular inequality to validate this assumption. [sent-201, score-0.091]
</p><p>45 The situation is clariﬁed when ωt = (yt , xt ), g(ωt , b) depends on ωt only through predictor xt , and xt = (wt , vt′ )′ has a dim(v)  2. [sent-205, score-0.609]
</p><p>46 For a vector v with component v j ’s, deﬁne the ℓq norm as |v|q = (∑ j=1 |v j |q )1/q for q ∈ (0, ∞), and |v|∞ = dim(v)  dim(v)  sup j=1 |v j |. [sent-206, score-0.121]
</p><p>47 983  J IANG  scalar component wt and other components vt , so that the decision boundary [g(ωt , b) = 0] ‘can be solved’ as [wt = w(vt , b)] for some ﬁxed function w. [sent-208, score-0.502]
</p><p>48 In this case the boundary set Si = {(wt , vt ) : wt = w(vt , b), b ∈ Bi } = [infb∈Bi w(vt , b) ≤ wt ≤ supb∈Bi w(vt , b)] if Bi is compact and w(vt , b) is continuous in b. [sent-209, score-0.627]
</p><p>49 Then in the Appendix we will show that P(ωt ∈ Si ) ≤ EVt {sup p(wt |Vt ) sup |∂b w(vt , b)|∞ }λd0 . [sent-212, score-0.121]
</p><p>50 wt  (4)  b∈Bi  Here ∂b w(vt , b) denotes a partial derivative of w, Vt is some σ-ﬁeld such that vt is measurable -Vt for each t, and p(wt |Vt ) denotes the conditional density. [sent-213, score-0.497]
</p><p>51 It is also noted that in this paper, we will consider ‘boundary sets’ of the ‘solvable’ form Si = [infb∈Bi w(vt , b) ≤ wt ≤ supb∈Bi w(vt , b)] which is assumed to be measurable. [sent-218, score-0.125]
</p><p>52 In our later examples we will focus on ‘linear solvable type’ described above, with decision boundary [wt = ∓vt′ bv ], and Bi being a closed ℓ∞ ball centered at bi and with radius h = λ/2 > 0. [sent-219, score-0.396]
</p><p>53 Then Si = [∓vt′ (bi )v −h|vt |1 ≤ wt ≤ ∓vt′ (bi )v +h|vt |1 ] which is indeed measurable. [sent-220, score-0.125]
</p><p>54 [More generally, when supb∈Bi w(vt , b) and infb∈Bi w(vt , b) are both continuous in vt , Si is measurable. [sent-221, score-0.32]
</p><p>55 The dependence term E|E(I(ωt ∈ Si )|Ft−m ) − EI(ωt ∈ Si )| will be small for large m when ωt can be approximated by strong mixing sequences in some sense. [sent-225, score-0.27]
</p><p>56 , br )), which represents predicting Yt by r of its own lags under an L1 loss. [sent-238, score-0.147]
</p><p>57 j=k This is a situation when Yt can have a dependence structure that is not strong mixing, since it involves the inﬁnite past of Zt− j ’s (see, e. [sent-250, score-0.115]
</p><p>58 The Lipshitz constant Nit can be obtained from the triangular inequality |ρ(ωt , b) − ρ(ωt , b∗ )| ≤ |b1 − b∗ ||Yt−1 | + . [sent-260, score-0.091]
</p><p>59 We will consider a high dimensional case where the number of lags can increase with sample size n: Condition (B3): r = O((ln n)M ) for some power M > 0. [sent-274, score-0.086]
</p><p>60 A Discontinuous Example Let ωt = (yt , xt ), where yt is a real-valued response at t and xt = (1, yt−1 , . [sent-287, score-0.739]
</p><p>61 , yt−r , zt′ )′ is a vector of predictors that can include r lags of y as well as a vector of exogenous variable zt . [sent-290, score-0.307]
</p><p>62 Suppose we are interested in predicting the sign of yt by using a discontinuous loss ρt = |I(yt > 0) − I(xt′ b > 0)|. [sent-291, score-0.399]
</p><p>63 Later we will allow v (maximal number of selected variables), r (number of lags allowed) and K ≡ dim(zt ) to increase with n in certain ways for high dimensional variable selection. [sent-293, score-0.086]
</p><p>64 The true model of yt is assumed to be an MA(∞) transform of a strong mixing process: yt = ∞ ∑ j=0 θ j f j (zt− j , εt− j ), where f j is some ﬁxed measurable function for each j so that supt, j || f j (zt , εt )||1 < ∞, and εt is a stochastic sequence independent of zt called the ‘innovation’. [sent-294, score-1.08]
</p><p>65 We assume that: Condition (C1): {zt , εt } is strong mixing with mixing coefﬁcient αm decreasing exponentially fast in m. [sent-295, score-0.295]
</p><p>66 Note that yt itself may no longer be strong mixing due to its dependence on the inﬁnite past. [sent-297, score-0.568]
</p><p>67 For example, in an ARX model yt = ϕyt−1 + ′ zt′ β + εt (|ϕ| < 1), an MA(∞) representation gives yt = ∑∞ ϕ j (zt− j β + εt− j ). [sent-299, score-0.666]
</p><p>68 Here, εt does not have j=0 to be iid; it can be an ‘exponential’ strong (or β-) mixing process such as a GARCH process (see, e. [sent-300, score-0.175]
</p><p>69 , Francq and Zako¨an, 2006) when yt follows an ARX-GARCH model. [sent-302, score-0.333]
</p><p>70 ı In the Appendix, we show that under some additional conditions (C3 and C4) on the underlying process {zt , εt }, we have, for any positive integer k < m − r, E|E(ρt |Ft−m ) − Eρt | ≤ 6αm−r−k + 8( 2My +  2Mx rCb ) sup || fl (zt , εt )||1 ∑ |θ j |. [sent-303, score-0.208]
</p><p>71 , Γ, with each Bi being a closed ℓ∞ ball centered at some bi , with radius h = λ/2 > 0, and with dimension at most v − 1. [sent-312, score-0.305]
</p><p>72 Assuming (C4), we have, for any positive integer k < m − r, E|E(I(ωt ∈ Si )|Ft−m ) − EI(ωt ∈ Si )| ≤ 6αm−r−k + 4 r ∑ |θ j | sup || fl (zt , εt )||1 (2Mx 2(1 +Cb + h)). [sent-322, score-0.208]
</p><p>73 Discussion This paper presents a very general inequality that generalizes Hoeffding’s inequality to dependent and unbounded summands. [sent-337, score-0.158]
</p><p>74 The inequality may not be very tight, but it involves few assumptions and can be very useful in deriving convergence rates of pointwise and uniform deviations in a number of situations that cannot be dealt with before. [sent-338, score-0.203]
</p><p>75 In the direction of unbounded loss, Meir and Zhang (2003) consider uniform deviations for iid data using a bound of the Rademaker complexity. [sent-350, score-0.178]
</p><p>76 Various ratios of empirical processes can also be used to handle unboundedness (see, e. [sent-351, score-0.089]
</p><p>77 In addition, we 988  U NIFORM D EVIATION OF G ENERAL E MPIRICAL R ISKS  note that the ‘bad set’ term bounding the probability of a large variance will often require applying a large deviation inequality again, while our triplex inequality is one in closed form. [sent-376, score-0.225]
</p><p>78 n Proof of Proposition 2 Note that for any b ∈ Bi , |Tn (b) − Tn (bi )| = |n−1 ∑t=1 { ft (b, At (b)) − −1 n E{ f (b, A (b)) − f (b , A (b ))}|. [sent-394, score-0.374]
</p><p>79 We therefore investigate the differences ft (bi , At (bi ))} − n ∑t=1 t t t i t i of the form ft (b, At (b)) − ft (bi , At (bi )) = { ft (b, At (b)) − ft (bi , At (b))} + { ft (bi , At (b))− ft (bi , At (bi ))}. [sent-395, score-2.618]
</p><p>80 989  J IANG  [The difference |At (b) − At (bi )| is {0, 1} valued and takes value 1 only when only one of g(ωt , b) and g(ωt , bi ) is positive. [sent-400, score-0.305]
</p><p>81 This would imply g(ωt , b∗ ) = 0 at some intermediate point b∗ on the line segment between b and bi , which must fall in Bi due to its convexity. [sent-401, score-0.305]
</p><p>82 ]  990  U NIFORM D EVIATION OF G ENERAL E MPIRICAL R ISKS  Now apply the pointwise inequality (1) on the last term and we obtain Proposition 2. [sent-407, score-0.122]
</p><p>83 − brYt−r,k |, which j=0 are strong mixing due to dependence on ﬁnite number of lags. [sent-418, score-0.235]
</p><p>84 This is achieved for some small enough u since ∞  ∞  j=0  j=0  Ee±uYs = exp{ ∑ ln Ee±uθ j Z1 } = exp{ ∑ K(±uθ j )} ∞  ≤ exp{ ∑ sup |K ′ (v)||uθ j |} ≤ exp{ sup |K ′ (v)|u|θ|1 }. [sent-437, score-0.269]
</p><p>85 |v|≤u|θ|1  j=0 |v|≤u|θ|1  991  J IANG  Then 2 Eeu|Ys | ≤ EeuYs + Ee−uYs ≤ 2 exp{ sup |K ′ (v)|u|θ|1 } ≡ Cu < ∞ |v|≤u|θ|1  for some small enough u, due to continuous differentiability of K(·) at 0, which is assumed in Condition (B2). [sent-438, score-0.121]
</p><p>86 Proof of Equation (9) We notice the process (yt , xt ) can be approximated by strong mixing processes. [sent-459, score-0.378]
</p><p>87 , yt−r,k , zt′ )′ j=0 then ||yt −yt,k ||1 ≤ ∑ j>k |θ j | supt,l || fl (zt , εt )||1 and ||xt −xt,k ||1 ≡ E|xt −xt,k |1 = ∑t−r ||ys −ys,k ||1 ≤ s=t−1 r ∑ j>k |θ j | supt,l || fl (zt , εt )||1 , both of which will decrease exponentially fast with k. [sent-463, score-0.174]
</p><p>88 , εt−r−k ) and is therefore strong mixing with mixing coefﬁcient αm−r−k for m > r + k. [sent-470, score-0.295]
</p><p>89 Each B± (γ) can be covered by at most (2Cb /λ + 1)card(γ) sets Bi ’s of the form Bi = {b ∈ ℜK+r+1 : br+2 = ±1, |b j − (bi ) j | ≤ λ/2, ∀ j ∈ γ; |b j − (bi ) j | = 0, ∀ j ∈ γ ∪ {r + 2}} for some bi ∈ B± (γ), where card(γ) ≤ v − 1. [sent-502, score-0.305]
</p><p>90 We can apply the arguments in Remark 8 by identifying wt = zt,1 , vt = (1, yt−1 , . [sent-508, score-0.445]
</p><p>91 −∞ Then A3 is satisﬁed if (C4) holds [the conditional density p(wt |Vt ) is bounded above by c (=Mx )] and if λ ≤ δ/(12vCc(1 +Cz + r|θ|1 sup || fl (zt , εt )||1 )), (14) t,l  which would then be ≤ δ/(12vCc(1 +Cz + r supt ||yt ||1 )) ≤ δ/(12d0Cc supt E|vt |∞ ). [sent-519, score-0.371]
</p><p>92 5+γ1 /2 for any γ1 > 0, and λ = n−1 (the ℓ∞ diameter of Bi ), and assume that the number of lags r and the maximal number of selected variables v follow condition (C6). [sent-521, score-0.086]
</p><p>93 , zt,K )′ , we here identify wt = zt,1 = (xt )r+2 , and vt = (1, yt−1 , . [sent-533, score-0.445]
</p><p>94 Note that according to Remark 9, Si = [w− (vt ) ≤ wt ≤ w+ (vt )] where w± (vt ) = vt′ (bi )v ± h|vt |1 . [sent-540, score-0.125]
</p><p>95 For examples, for vector xt , its component (xt )r+2 = wt is now placed behind other components (denoted as vt ). [sent-546, score-0.648]
</p><p>96 Now we can bound ED ≤ P[D = 1, |xt − xt,k |1 ≤ η] + P[|xt − xt,k |1 > η]  ≤ P[∪|wt − w± (vt )| ≤ (1 +Cb + h)η] + P[|xt − xt,k |1 > η]  (a)  ≤ 2c2(1 +Cb + h)η + E|xt − xt,k |1 /η  ≤ 2c2(1 +Cb + h)η + r ∑ |θ j | sup || fl (zt , εt )||1 /η. [sent-548, score-0.208]
</p><p>97 j>k  Now take η =  t,l  r ∑ j>k |θ j | supt,l || fl (zt , εt )||1 /(2c2(1 +Cb + h)) and obtain ED ≤ 2 r ∑ |θ j | sup || fl (zt , εt )||1 (2c2(1 +Cb + h)). [sent-549, score-0.295]
</p><p>98 An inequality for uniform deviations of sample averages from their means. [sent-560, score-0.132]
</p><p>99 Rates of convergence for empirical processes of stationary mixing sequences. [sent-691, score-0.155]
</p><p>100 The performance bounds of learning machines based on exponentially strong mixing sequences, Computers and Mathematics with Applications, 53:1050-1058, 2007. [sent-700, score-0.175]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ft', 0.374), ('yt', 0.333), ('vt', 0.32), ('bi', 0.305), ('cb', 0.214), ('nit', 0.208), ('xt', 0.203), ('zt', 0.187), ('enit', 0.178), ('si', 0.175), ('tn', 0.128), ('wt', 0.125), ('davidson', 0.123), ('sup', 0.121), ('mixing', 0.12), ('supb', 0.114), ('eviation', 0.1), ('isks', 0.1), ('iang', 0.095), ('unboundedness', 0.089), ('fl', 0.087), ('lags', 0.086), ('niform', 0.085), ('mpirical', 0.085), ('emit', 0.078), ('eneral', 0.076), ('ext', 0.076), ('ei', 0.071), ('supt', 0.068), ('discontinuous', 0.066), ('br', 0.061), ('dependence', 0.06), ('cu', 0.059), ('boundary', 0.057), ('unbounded', 0.056), ('eeu', 0.056), ('evt', 0.056), ('lipshitz', 0.056), ('triplex', 0.056), ('strong', 0.055), ('measurable', 0.052), ('ys', 0.051), ('inequality', 0.051), ('zs', 0.047), ('uniform', 0.045), ('bercu', 0.045), ('gassiat', 0.045), ('tail', 0.043), ('bw', 0.042), ('hoeffding', 0.041), ('iid', 0.041), ('triangular', 0.04), ('upperbound', 0.038), ('infb', 0.038), ('lugosi', 0.037), ('mx', 0.037), ('jiang', 0.037), ('deviations', 0.036), ('pointwise', 0.036), ('term', 0.035), ('proposition', 0.035), ('convergence', 0.035), ('downloadable', 0.034), ('exogenous', 0.034), ('bv', 0.034), ('arx', 0.034), ('innovations', 0.034), ('martingale', 0.034), ('cei', 0.033), ('garch', 0.033), ('jvt', 0.033), ('mixingale', 0.033), ('northwestern', 0.033), ('deviation', 0.032), ('vn', 0.029), ('gy', 0.029), ('boucheron', 0.028), ('geer', 0.028), ('innovation', 0.028), ('meir', 0.028), ('supa', 0.028), ('cz', 0.027), ('ee', 0.027), ('devroye', 0.027), ('mcdiarmid', 0.027), ('bounded', 0.027), ('ln', 0.027), ('dim', 0.027), ('fn', 0.027), ('tanner', 0.025), ('integers', 0.025), ('rio', 0.023), ('ex', 0.023), ('bryt', 0.022), ('francq', 0.022), ('kulkarni', 0.022), ('newey', 0.022), ('rademaker', 0.022), ('rcb', 0.022), ('uys', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="65-tfidf-1" href="./jmlr-2009-On_Uniform_Deviations_of_General_Empirical_Risks_with_Unboundedness%2C_Dependence%2C_and_High_Dimensionality.html">65 jmlr-2009-On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality</a></p>
<p>Author: Wenxin Jiang</p><p>Abstract: The statistical learning theory of risk minimization depends heavily on probability bounds for uniform deviations of the empirical risks. Classical probability bounds using Hoeffding’s inequality cannot accommodate more general situations with unbounded loss and dependent data. The current paper introduces an inequality that extends Hoeffding’s inequality to handle these more general situations. We will apply this inequality to provide probability bounds for uniform deviations in a very general framework, which can involve discrete decision rules, unbounded loss, and a dependence structure that can be more general than either martingale or strong mixing. We will consider two examples with high dimensional predictors: autoregression (AR) with ℓ1 -loss, and ARX model with variable selection for sign classiﬁcation, which uses both lagged responses and exogenous predictors. Keywords: dependence, empirical risk, probability bound, unbounded loss, uniform deviation</p><p>2 0.34983838 <a title="65-tfidf-2" href="./jmlr-2009-Online_Learning_with_Samples_Drawn_from_Non-identical_Distributions.html">68 jmlr-2009-Online Learning with Samples Drawn from Non-identical Distributions</a></p>
<p>Author: Ting Hu, Ding-Xuan Zhou</p><p>Abstract: Learning algorithms are based on samples which are often drawn independently from an identical distribution (i.i.d.). In this paper we consider a different setting with samples drawn according to a non-identical sequence of probability distributions. Each time a sample is drawn from a different distribution. In this setting we investigate a fully online learning algorithm associated with a general convex loss function and a reproducing kernel Hilbert space (RKHS). Error analysis is conducted under the assumption that the sequence of marginal distributions converges polynomially in the dual of a H¨ lder space. For regression with least square or insensitive loss, learning rates are given o in both the RKHS norm and the L2 norm. For classiﬁcation with hinge loss and support vector machine q-norm loss, rates are explicitly stated with respect to the excess misclassiﬁcation error. Keywords: sampling with non-identical distributions, online learning, classiﬁcation with a general convex loss, regression with insensitive loss and least square loss, reproducing kernel Hilbert space</p><p>3 0.33519277 <a title="65-tfidf-3" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>Author: Sanjoy Dasgupta, Adam Tauman Kalai, Claire Monteleoni</p><p>Abstract: We start by showing that in an active learning setting, the Perceptron algorithm needs Ω( ε12 ) labels to learn linear separators within generalization error ε. We then present a simple active learning algorithm for this problem, which combines a modiﬁcation of the Perceptron update with an adaptive ﬁltering rule for deciding which points to query. For data distributed uniformly over the unit ˜ sphere, we show that our algorithm reaches generalization error ε after asking for just O(d log 1 ) ε labels. This exponential improvement over the usual sample complexity of supervised learning had previously been demonstrated only for the computationally more complex query-by-committee algorithm. Keywords: active learning, perceptron, label complexity bounds, online learning</p><p>4 0.31904858 <a title="65-tfidf-4" href="./jmlr-2009-Bounded_Kernel-Based_Online_Learning.html">13 jmlr-2009-Bounded Kernel-Based Online Learning</a></p>
<p>Author: Francesco Orabona, Joseph Keshet, Barbara Caputo</p><p>Abstract: A common problem of kernel-based online algorithms, such as the kernel-based Perceptron algorithm, is the amount of memory required to store the online hypothesis, which may increase without bound as the algorithm progresses. Furthermore, the computational load of such algorithms grows linearly with the amount of memory used to store the hypothesis. To attack these problems, most previous work has focused on discarding some of the instances, in order to keep the memory bounded. In this paper we present a new algorithm, in which the instances are not discarded, but are instead projected onto the space spanned by the previous online hypothesis. We call this algorithm Projectron. While the memory size of the Projectron solution cannot be predicted before training, we prove that its solution is guaranteed to be bounded. We derive a relative mistake bound for the proposed algorithm, and deduce from it a slightly different algorithm which outperforms the Perceptron. We call this second algorithm Projectron++. We show that this algorithm can be extended to handle the multiclass and the structured output settings, resulting, as far as we know, in the ﬁrst online bounded algorithm that can learn complex classiﬁcation tasks. The method of bounding the hypothesis representation can be applied to any conservative online algorithm and to other online algorithms, as it is demonstrated for ALMA2 . Experimental results on various data sets show the empirical advantage of our technique compared to various bounded online algorithms, both in terms of memory and accuracy. Keywords: online learning, kernel methods, support vector machines, bounded support set</p><p>5 0.17232922 <a title="65-tfidf-5" href="./jmlr-2009-CarpeDiem%3A_Optimizing_the_Viterbi_Algorithm_and_Applications_to_Supervised_Sequential_Learning.html">14 jmlr-2009-CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning</a></p>
<p>Author: Roberto Esposito, Daniele P. Radicioni</p><p>Abstract: The growth of information available to learning systems and the increasing complexity of learning tasks determine the need for devising algorithms that scale well with respect to all learning parameters. In the context of supervised sequential learning, the Viterbi algorithm plays a fundamental role, by allowing the evaluation of the best (most probable) sequence of labels with a time complexity linear in the number of time events, and quadratic in the number of labels. In this paper we propose CarpeDiem, a novel algorithm allowing the evaluation of the best possible sequence of labels with a sub-quadratic time complexity.1 We provide theoretical grounding together with solid empirical results supporting two chief facts. CarpeDiem always ﬁnds the optimal solution requiring, in most cases, only a small fraction of the time taken by the Viterbi algorithm; meantime, CarpeDiem is never asymptotically worse than the Viterbi algorithm, thus conﬁrming it as a sound replacement. Keywords: Viterbi algorithm, sequence labeling, conditional models, classiﬁers optimization, exact inference</p><p>6 0.15611006 <a title="65-tfidf-6" href="./jmlr-2009-SGD-QN%3A_Careful_Quasi-Newton_Stochastic_Gradient_Descent.html">83 jmlr-2009-SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent</a></p>
<p>7 0.14906681 <a title="65-tfidf-7" href="./jmlr-2009-Identification_of_Recurrent_Neural_Networks_by_Bayesian_Interrogation_Techniques.html">40 jmlr-2009-Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques</a></p>
<p>8 0.13557895 <a title="65-tfidf-8" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>9 0.11356588 <a title="65-tfidf-9" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>10 0.1072127 <a title="65-tfidf-10" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>11 0.10651518 <a title="65-tfidf-11" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>12 0.071789421 <a title="65-tfidf-12" href="./jmlr-2009-Optimized_Cutting_Plane_Algorithm_for_Large-Scale_Risk_Minimization.html">69 jmlr-2009-Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization</a></p>
<p>13 0.068082862 <a title="65-tfidf-13" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>14 0.059114106 <a title="65-tfidf-14" href="./jmlr-2009-Online_Learning_with_Sample_Path_Constraints.html">67 jmlr-2009-Online Learning with Sample Path Constraints</a></p>
<p>15 0.057817146 <a title="65-tfidf-15" href="./jmlr-2009-Generalization_Bounds_for_Ranking_Algorithms_via_Algorithmic_Stability.html">37 jmlr-2009-Generalization Bounds for Ranking Algorithms via Algorithmic Stability</a></p>
<p>16 0.044430822 <a title="65-tfidf-16" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>17 0.04410512 <a title="65-tfidf-17" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>18 0.042290013 <a title="65-tfidf-18" href="./jmlr-2009-Exploring_Strategies_for_Training_Deep_Neural_Networks.html">33 jmlr-2009-Exploring Strategies for Training Deep Neural Networks</a></p>
<p>19 0.039519597 <a title="65-tfidf-19" href="./jmlr-2009-Prediction_With_Expert_Advice_For_The_Brier_Game.html">73 jmlr-2009-Prediction With Expert Advice For The Brier Game</a></p>
<p>20 0.035420839 <a title="65-tfidf-20" href="./jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">16 jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.371), (1, 0.502), (2, -0.305), (3, 0.136), (4, 0.0), (5, -0.132), (6, 0.034), (7, 0.038), (8, 0.098), (9, 0.005), (10, 0.007), (11, -0.021), (12, -0.056), (13, -0.003), (14, -0.047), (15, 0.006), (16, 0.161), (17, 0.032), (18, -0.044), (19, -0.042), (20, 0.079), (21, -0.058), (22, 0.012), (23, -0.002), (24, 0.101), (25, 0.051), (26, 0.006), (27, -0.035), (28, -0.025), (29, -0.003), (30, 0.049), (31, 0.081), (32, -0.0), (33, -0.023), (34, -0.074), (35, -0.003), (36, -0.036), (37, 0.073), (38, -0.034), (39, 0.04), (40, 0.023), (41, -0.019), (42, -0.016), (43, -0.02), (44, 0.006), (45, 0.068), (46, -0.065), (47, 0.041), (48, 0.01), (49, -0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98558545 <a title="65-lsi-1" href="./jmlr-2009-On_Uniform_Deviations_of_General_Empirical_Risks_with_Unboundedness%2C_Dependence%2C_and_High_Dimensionality.html">65 jmlr-2009-On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality</a></p>
<p>Author: Wenxin Jiang</p><p>Abstract: The statistical learning theory of risk minimization depends heavily on probability bounds for uniform deviations of the empirical risks. Classical probability bounds using Hoeffding’s inequality cannot accommodate more general situations with unbounded loss and dependent data. The current paper introduces an inequality that extends Hoeffding’s inequality to handle these more general situations. We will apply this inequality to provide probability bounds for uniform deviations in a very general framework, which can involve discrete decision rules, unbounded loss, and a dependence structure that can be more general than either martingale or strong mixing. We will consider two examples with high dimensional predictors: autoregression (AR) with ℓ1 -loss, and ARX model with variable selection for sign classiﬁcation, which uses both lagged responses and exogenous predictors. Keywords: dependence, empirical risk, probability bound, unbounded loss, uniform deviation</p><p>2 0.77260566 <a title="65-lsi-2" href="./jmlr-2009-Bounded_Kernel-Based_Online_Learning.html">13 jmlr-2009-Bounded Kernel-Based Online Learning</a></p>
<p>Author: Francesco Orabona, Joseph Keshet, Barbara Caputo</p><p>Abstract: A common problem of kernel-based online algorithms, such as the kernel-based Perceptron algorithm, is the amount of memory required to store the online hypothesis, which may increase without bound as the algorithm progresses. Furthermore, the computational load of such algorithms grows linearly with the amount of memory used to store the hypothesis. To attack these problems, most previous work has focused on discarding some of the instances, in order to keep the memory bounded. In this paper we present a new algorithm, in which the instances are not discarded, but are instead projected onto the space spanned by the previous online hypothesis. We call this algorithm Projectron. While the memory size of the Projectron solution cannot be predicted before training, we prove that its solution is guaranteed to be bounded. We derive a relative mistake bound for the proposed algorithm, and deduce from it a slightly different algorithm which outperforms the Perceptron. We call this second algorithm Projectron++. We show that this algorithm can be extended to handle the multiclass and the structured output settings, resulting, as far as we know, in the ﬁrst online bounded algorithm that can learn complex classiﬁcation tasks. The method of bounding the hypothesis representation can be applied to any conservative online algorithm and to other online algorithms, as it is demonstrated for ALMA2 . Experimental results on various data sets show the empirical advantage of our technique compared to various bounded online algorithms, both in terms of memory and accuracy. Keywords: online learning, kernel methods, support vector machines, bounded support set</p><p>3 0.70897752 <a title="65-lsi-3" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>Author: Sanjoy Dasgupta, Adam Tauman Kalai, Claire Monteleoni</p><p>Abstract: We start by showing that in an active learning setting, the Perceptron algorithm needs Ω( ε12 ) labels to learn linear separators within generalization error ε. We then present a simple active learning algorithm for this problem, which combines a modiﬁcation of the Perceptron update with an adaptive ﬁltering rule for deciding which points to query. For data distributed uniformly over the unit ˜ sphere, we show that our algorithm reaches generalization error ε after asking for just O(d log 1 ) ε labels. This exponential improvement over the usual sample complexity of supervised learning had previously been demonstrated only for the computationally more complex query-by-committee algorithm. Keywords: active learning, perceptron, label complexity bounds, online learning</p><p>4 0.6198526 <a title="65-lsi-4" href="./jmlr-2009-Online_Learning_with_Samples_Drawn_from_Non-identical_Distributions.html">68 jmlr-2009-Online Learning with Samples Drawn from Non-identical Distributions</a></p>
<p>Author: Ting Hu, Ding-Xuan Zhou</p><p>Abstract: Learning algorithms are based on samples which are often drawn independently from an identical distribution (i.i.d.). In this paper we consider a different setting with samples drawn according to a non-identical sequence of probability distributions. Each time a sample is drawn from a different distribution. In this setting we investigate a fully online learning algorithm associated with a general convex loss function and a reproducing kernel Hilbert space (RKHS). Error analysis is conducted under the assumption that the sequence of marginal distributions converges polynomially in the dual of a H¨ lder space. For regression with least square or insensitive loss, learning rates are given o in both the RKHS norm and the L2 norm. For classiﬁcation with hinge loss and support vector machine q-norm loss, rates are explicitly stated with respect to the excess misclassiﬁcation error. Keywords: sampling with non-identical distributions, online learning, classiﬁcation with a general convex loss, regression with insensitive loss and least square loss, reproducing kernel Hilbert space</p><p>5 0.47641861 <a title="65-lsi-5" href="./jmlr-2009-CarpeDiem%3A_Optimizing_the_Viterbi_Algorithm_and_Applications_to_Supervised_Sequential_Learning.html">14 jmlr-2009-CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning</a></p>
<p>Author: Roberto Esposito, Daniele P. Radicioni</p><p>Abstract: The growth of information available to learning systems and the increasing complexity of learning tasks determine the need for devising algorithms that scale well with respect to all learning parameters. In the context of supervised sequential learning, the Viterbi algorithm plays a fundamental role, by allowing the evaluation of the best (most probable) sequence of labels with a time complexity linear in the number of time events, and quadratic in the number of labels. In this paper we propose CarpeDiem, a novel algorithm allowing the evaluation of the best possible sequence of labels with a sub-quadratic time complexity.1 We provide theoretical grounding together with solid empirical results supporting two chief facts. CarpeDiem always ﬁnds the optimal solution requiring, in most cases, only a small fraction of the time taken by the Viterbi algorithm; meantime, CarpeDiem is never asymptotically worse than the Viterbi algorithm, thus conﬁrming it as a sound replacement. Keywords: Viterbi algorithm, sequence labeling, conditional models, classiﬁers optimization, exact inference</p><p>6 0.38360313 <a title="65-lsi-6" href="./jmlr-2009-Identification_of_Recurrent_Neural_Networks_by_Bayesian_Interrogation_Techniques.html">40 jmlr-2009-Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques</a></p>
<p>7 0.3557429 <a title="65-lsi-7" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>8 0.34969708 <a title="65-lsi-8" href="./jmlr-2009-SGD-QN%3A_Careful_Quasi-Newton_Stochastic_Gradient_Descent.html">83 jmlr-2009-SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent</a></p>
<p>9 0.34858486 <a title="65-lsi-9" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>10 0.26673219 <a title="65-lsi-10" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>11 0.25616333 <a title="65-lsi-11" href="./jmlr-2009-Optimized_Cutting_Plane_Algorithm_for_Large-Scale_Risk_Minimization.html">69 jmlr-2009-Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization</a></p>
<p>12 0.23611335 <a title="65-lsi-12" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>13 0.19497226 <a title="65-lsi-13" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>14 0.17966735 <a title="65-lsi-14" href="./jmlr-2009-Generalization_Bounds_for_Ranking_Algorithms_via_Algorithmic_Stability.html">37 jmlr-2009-Generalization Bounds for Ranking Algorithms via Algorithmic Stability</a></p>
<p>15 0.17542836 <a title="65-lsi-15" href="./jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">16 jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<p>16 0.16168563 <a title="65-lsi-16" href="./jmlr-2009-Online_Learning_with_Sample_Path_Constraints.html">67 jmlr-2009-Online Learning with Sample Path Constraints</a></p>
<p>17 0.15860839 <a title="65-lsi-17" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>18 0.15713981 <a title="65-lsi-18" href="./jmlr-2009-Exploring_Strategies_for_Training_Deep_Neural_Networks.html">33 jmlr-2009-Exploring Strategies for Training Deep Neural Networks</a></p>
<p>19 0.15417837 <a title="65-lsi-19" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>20 0.15066962 <a title="65-lsi-20" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.012), (11, 0.011), (26, 0.013), (38, 0.016), (52, 0.031), (55, 0.636), (58, 0.026), (66, 0.093), (68, 0.01), (90, 0.037), (96, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94473344 <a title="65-lda-1" href="./jmlr-2009-On_Uniform_Deviations_of_General_Empirical_Risks_with_Unboundedness%2C_Dependence%2C_and_High_Dimensionality.html">65 jmlr-2009-On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality</a></p>
<p>Author: Wenxin Jiang</p><p>Abstract: The statistical learning theory of risk minimization depends heavily on probability bounds for uniform deviations of the empirical risks. Classical probability bounds using Hoeffding’s inequality cannot accommodate more general situations with unbounded loss and dependent data. The current paper introduces an inequality that extends Hoeffding’s inequality to handle these more general situations. We will apply this inequality to provide probability bounds for uniform deviations in a very general framework, which can involve discrete decision rules, unbounded loss, and a dependence structure that can be more general than either martingale or strong mixing. We will consider two examples with high dimensional predictors: autoregression (AR) with ℓ1 -loss, and ARX model with variable selection for sign classiﬁcation, which uses both lagged responses and exogenous predictors. Keywords: dependence, empirical risk, probability bound, unbounded loss, uniform deviation</p><p>2 0.93041116 <a title="65-lda-2" href="./jmlr-2009-Identification_of_Recurrent_Neural_Networks_by_Bayesian_Interrogation_Techniques.html">40 jmlr-2009-Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques</a></p>
<p>Author: Barnabás Póczos, András Lőrincz</p><p>Abstract: We introduce novel online Bayesian methods for the identiﬁcation of a family of noisy recurrent neural networks (RNNs). We present Bayesian active learning techniques for stimulus selection given past experiences. In particular, we consider the unknown parameters as stochastic variables and use A-optimality and D-optimality principles to choose optimal stimuli. We derive myopic cost functions in order to maximize the information gain concerning network parameters at each time step. We also derive the A-optimal and D-optimal estimations of the additive noise that perturbs the dynamical system of the RNN. Here we investigate myopic as well as non-myopic estimations, and study the problem of simultaneous estimation of both the system parameters and the noise. Employing conjugate priors our derivations remain approximation-free and give rise to simple update rules for the online learning of the parameters. The efﬁciency of our method is demonstrated for a number of selected cases, including the task of controlled independent component analysis. Keywords: active learning, system identiﬁcation, online Bayesian learning, A-optimality, Doptimality, infomax control, optimal design</p><p>3 0.90944695 <a title="65-lda-3" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>Author: Junhui Wang, Xiaotong Shen, Wei Pan</p><p>Abstract: In classiﬁcation, semisupervised learning usually involves a large amount of unlabeled data with only a small number of labeled data. This imposes a great challenge in that it is difﬁcult to achieve good classiﬁcation performance through labeled data alone. To leverage unlabeled data for enhancing classiﬁcation, this article introduces a large margin semisupervised learning method within the framework of regularization, based on an efﬁcient margin loss for unlabeled data, which seeks efﬁcient extraction of the information from unlabeled data for estimating the Bayes decision boundary for classiﬁcation. For implementation, an iterative scheme is derived through conditional expectations. Finally, theoretical and numerical analyses are conducted, in addition to an application to gene function prediction. They suggest that the proposed method enables to recover the performance of its supervised counterpart based on complete data in rates of convergence, when possible. Keywords: difference convex programming, classiﬁcation, nonconvex minimization, regularization, support vectors</p><p>4 0.48988852 <a title="65-lda-4" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>Author: Sanjoy Dasgupta, Adam Tauman Kalai, Claire Monteleoni</p><p>Abstract: We start by showing that in an active learning setting, the Perceptron algorithm needs Ω( ε12 ) labels to learn linear separators within generalization error ε. We then present a simple active learning algorithm for this problem, which combines a modiﬁcation of the Perceptron update with an adaptive ﬁltering rule for deciding which points to query. For data distributed uniformly over the unit ˜ sphere, we show that our algorithm reaches generalization error ε after asking for just O(d log 1 ) ε labels. This exponential improvement over the usual sample complexity of supervised learning had previously been demonstrated only for the computationally more complex query-by-committee algorithm. Keywords: active learning, perceptron, label complexity bounds, online learning</p><p>5 0.47462091 <a title="65-lda-5" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>Author: Tong Zhang</p><p>Abstract: This paper studies the feature selection problem using a greedy least squares regression algorithm. We show that under a certain irrepresentable condition on the design matrix (but independent of the sparse target), the greedy algorithm can select features consistently when the sample size approaches inﬁnity. The condition is identical to a corresponding condition for Lasso. Moreover, under a sparse eigenvalue condition, the greedy algorithm can reliably identify features as long as each nonzero coefﬁcient is larger than a constant times the noise level. In compar√ ison, Lasso may require the coefﬁcients to be larger than O( s) times the noise level in the worst case, where s is the number of nonzero coefﬁcients. Keywords: greedy algorithm, feature selection, sparsity</p><p>6 0.44518784 <a title="65-lda-6" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>7 0.43198359 <a title="65-lda-7" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>8 0.42720726 <a title="65-lda-8" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>9 0.41920701 <a title="65-lda-9" href="./jmlr-2009-Adaptive_False_Discovery_Rate_Control_under_Independence_and_Dependence.html">5 jmlr-2009-Adaptive False Discovery Rate Control under Independence and Dependence</a></p>
<p>10 0.41615912 <a title="65-lda-10" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>11 0.41063356 <a title="65-lda-11" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>12 0.40565348 <a title="65-lda-12" href="./jmlr-2009-Entropy_Inference_and_the_James-Stein_Estimator%2C_with_Application_to_Nonlinear_Gene_Association_Networks.html">28 jmlr-2009-Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks</a></p>
<p>13 0.3934111 <a title="65-lda-13" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>14 0.38811928 <a title="65-lda-14" href="./jmlr-2009-Strong_Limit_Theorems_for_the_Bayesian_Scoring_Criterion_in_Bayesian_Networks.html">89 jmlr-2009-Strong Limit Theorems for the Bayesian Scoring Criterion in Bayesian Networks</a></p>
<p>15 0.38779387 <a title="65-lda-15" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>16 0.38684207 <a title="65-lda-16" href="./jmlr-2009-A_Least-squares_Approach_to_Direct_Importance_Estimation.html">1 jmlr-2009-A Least-squares Approach to Direct Importance Estimation</a></p>
<p>17 0.38559312 <a title="65-lda-17" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>18 0.38003626 <a title="65-lda-18" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>19 0.37912294 <a title="65-lda-19" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>20 0.36567402 <a title="65-lda-20" href="./jmlr-2009-SGD-QN%3A_Careful_Quasi-Newton_Stochastic_Gradient_Descent.html">83 jmlr-2009-SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
