<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 jmlr-2009-Fast ApproximatekNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-34" href="#">jmlr2009-34</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>34 jmlr-2009-Fast ApproximatekNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</h1>
<br/><p>Source: <a title="jmlr-2009-34-pdf" href="http://jmlr.org/papers/volume10/chen09b/chen09b.pdf">pdf</a></p><p>Author: Jie Chen, Haw-ren Fang, Yousef Saad</p><p>Abstract: Nearest neighbor graphs are widely used in data mining and machine learning. A brute-force method to compute the exact kNN graph takes Θ(dn2 ) time for n data points in the d dimensional Euclidean space. We propose two divide and conquer methods for computing an approximate kNN graph in Θ(dnt ) time for high dimensional data (large d). The exponent t ∈ (1, 2) is an increasing function of an internal parameter α which governs the size of the common region in the divide step. Experiments show that a high quality graph can usually be obtained with small overlaps, that is, for small values of t. A few of the practical details of the algorithms are as follows. First, the divide step uses an inexpensive Lanczos procedure to perform recursive spectral bisection. After each conquer step, an additional reﬁnement step is performed to improve the accuracy of the graph. Finally, a hash table is used to avoid repeating distance calculations during the divide and conquer process. The combination of these techniques is shown to yield quite effective algorithms for building kNN graphs. Keywords: nearest neighbors graph, high dimensional data, divide and conquer, Lanczos algorithm, spectral method</p><p>Reference: <a title="jmlr-2009-34-reference" href="../jmlr2009_reference/jmlr-2009-Fast_ApproximatekNN_Graph_Construction_for_High_Dimensional_Data_via_Recursive_Lanczos_Bisection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Department of Computer Science and Engineering University of Minnesota Minneapolis, MN 55455, USA  Editor: Sanjoy Dasgupta  Abstract Nearest neighbor graphs are widely used in data mining and machine learning. [sent-7, score-0.127]
</p><p>2 A brute-force method to compute the exact kNN graph takes Θ(dn2 ) time for n data points in the d dimensional Euclidean space. [sent-8, score-0.138]
</p><p>3 We propose two divide and conquer methods for computing an approximate kNN graph in Θ(dnt ) time for high dimensional data (large d). [sent-9, score-0.379]
</p><p>4 Experiments show that a high quality graph can usually be obtained with small overlaps, that is, for small values of t. [sent-11, score-0.116]
</p><p>5 First, the divide step uses an inexpensive Lanczos procedure to perform recursive spectral bisection. [sent-13, score-0.156]
</p><p>6 After each conquer step, an additional reﬁnement step is performed to improve the accuracy of the graph. [sent-14, score-0.183]
</p><p>7 Finally, a hash table is used to avoid repeating distance calculations during the divide and conquer process. [sent-15, score-0.398]
</p><p>8 Keywords: nearest neighbors graph, high dimensional data, divide and conquer, Lanczos algorithm, spectral method  1. [sent-17, score-0.343]
</p><p>9 Introduction Building nearest neighbor graphs is often a necessary step when dealing with problems arising from applications in such areas as data mining (Brito et al. [sent-18, score-0.242]
</p><p>10 , xn }, a nearest neighbor graph consists of the vertex set X and an edge set which is a subset of X × X. [sent-26, score-0.271]
</p><p>11 Two types of nearest neighbor graphs (Belkin and Niyogi, 2003; He and Niyogi, 2004) are often used: 1. [sent-29, score-0.242]
</p><p>12 When k = 1, only the nearest neighbor for each data point is considered. [sent-48, score-0.183]
</p><p>13 This particular case, the all nearest neighbors problem, has been extensively studied in the literature. [sent-49, score-0.187]
</p><p>14 Note that the problem of constructing a kNN graph is different from the problem of nearest neighbor(s) search (see, e. [sent-59, score-0.203]
</p><p>15 , 2006, and references therein), where given a set of data points, the task is to ﬁnd the k nearest points for any query point. [sent-62, score-0.141]
</p><p>16 Usually, the nearest neighbors search problem is handled by ﬁrst building a data structure (such as a search tree) for the given points in a preprocessing phase. [sent-63, score-0.187]
</p><p>17 Of course, the construction of a kNN graph can be viewed as a nearest neighbors search problem where each data point itself is a query. [sent-65, score-0.275]
</p><p>18 On the other hand, many (1 + ε) nearest neighbor search methods1 have been proposed with guaranteed complexity bounds (Indyk, 2004). [sent-69, score-0.183]
</p><p>19 These methods report a point within (1 + ε) times the distance from the query to the actual nearest neighbor. [sent-70, score-0.164]
</p><p>20 (2000) proposed a −2 O((dn)2 (n log(d log n/ε))O(ε ) poly(1/ε) poly log(dn/ε)) data structure that can answer a query in O(d 2 poly(1/ε) poly log(dn/ε)) time. [sent-78, score-0.116]
</p><p>21 Divide and Conquer kNN Our general framework for computing an approximate kNN graph is as follows: We divide the set of data points into subsets (possibly with overlaps), recursively compute the (approximate) kNN graphs for the subsets, then conquer the results into a ﬁnal kNN graph. [sent-87, score-0.456]
</p><p>22 This divide and conquer framework can clearly be separated in two distinct sub-problems: how to divide and how to conquer. [sent-88, score-0.374]
</p><p>23 The conquer step is simple: If a data point belongs to more than one subsets, then its k nearest neighbors are selected from its neighbors in each subset. [sent-89, score-0.419]
</p><p>24 However, the divide step can be implemented in many different ways, resulting in different qualities of graphs. [sent-90, score-0.192]
</p><p>25 (1)  Then, the hyperplane is deﬁned as u, x − c = 0, that is, it splits the set of data points into two subsets: X+ = {xi | uT xi ≥ 0} and X− = {xi | uT xi < 0}. [sent-105, score-0.121]
</p><p>26 ˆ ˆ 1991  C HEN , FANG AND S AAD  This hyperplane maximizes the sum of squared distances between the centered points xi to the ˆ hyperplane that passes through the centroid. [sent-106, score-0.206]
</p><p>27 By a standard property of the SVD (Equation 1), this bisection technique is equivalent to splitting the set by the following criterion: X+ = {xi | vi ≥ 0} and X− = {xi | vi < 0},  (2)  where vi is the i-th entry of the right singular vector v. [sent-108, score-0.29]
</p><p>28 If it is preferred that the sizes of the two subsets be balanced, an alternative is to replace the above criterion by X+ = {xi | vi ≥ m(v)} and X− = {xi | vi < m(v)},  (3)  where m(v) represents the median of the entries of v. [sent-109, score-0.152]
</p><p>29 Therefore, θ(s) is an approximation to the square of the largest singular value σ, while the vector ˆ vs ≡ Qs ys is an approximation of the right singular vector v of X. [sent-121, score-0.159]
</p><p>30 2 The Divide Step: Two Methods Based on the above general bisection technique, we propose two ways to perform the divide step for computing an approximate kNN graph. [sent-129, score-0.17]
</p><p>31 The ﬁrst, called the overlap method, divides the current set into two overlapping subsets. [sent-130, score-0.246]
</p><p>32 The second, called the glue method, divides the current set into two disjoint subsets, and uses a third set called the gluing set to help merge the two resulting disjoint kNN graphs in the conquer phase. [sent-131, score-0.638]
</p><p>33 hyperplane  X1  hyperplane  X1  X2  (a) The overlap method. [sent-135, score-0.34]
</p><p>34 1 T HE OVERLAP M ETHOD In this method, we divide the set X into two overlapping subsets X1 and X2 : X1 ∪ X2 = X, / X1 ∩ X2 = 0. [sent-141, score-0.149]
</p><p>35 The purpose of criterion (4) is to ensure that the overlap of the two subsets consists of (100α)% of all the data, that is, that2 |X1 ∩ X2 | = ⌈α|X|⌉ . [sent-147, score-0.288]
</p><p>36 Here, we assume that the distances between points and the hyperplane are all different. [sent-149, score-0.122]
</p><p>37 2 T HE G LUE M ETHOD In this method, we divide the set X into two disjoint subsets X1 and X2 with a gluing subset X3 :   X1 ∪ X2 = X,    X ∩ X = 0, / 1 2  X1 ∩ X3 = 0, /    / X2 ∩ X3 = 0. [sent-154, score-0.181]
</p><p>38 The criterion to build these subsets is as follows: X1 = {xi | vi ≥ 0},  X2 = {xi | vi < 0},  X3 = {xi | −hα (V ) ≤ vi < hα (V )}. [sent-155, score-0.207]
</p><p>39 Note that the gluing subset X3 in this method is exactly the intersection of the two subsets in the overlap method. [sent-156, score-0.32]
</p><p>40 3 Reﬁnement In order to improve the quality of the resulting graph, during each recursion after the conquer step, the graph can be reﬁned at a small cost. [sent-159, score-0.276]
</p><p>41 The idea is to update the k nearest neighbors for each point by selecting from a pool consisting of its neighbors and the neighbors of its neighbors. [sent-160, score-0.331]
</p><p>42 Formally, if N(x) is the set of current nearest neighbors of x before reﬁnement, then, for each point x, we re-select its k nearest neighbors from   N(x) ∪   [  z∈N(x)  N(z) . [sent-161, score-0.374]
</p><p>43 The difference is that Algorithm 1 calls D IVIDE -OVERLAP to divide the set into two subsets (Section 2. [sent-165, score-0.149]
</p><p>44 1), while Algorithm 2 calls D IVIDE -G LUE to divide the set into three subsets (Section 2. [sent-167, score-0.149]
</p><p>45 One advantage of the methods presented in this paper over brute-force methods is that the distance calculations can be signiﬁcantly reduced thanks to the divide and conquer approach. [sent-173, score-0.336]
</p><p>46 3  k smallest distances from at most 2k candidates for each point, and (3) the R EFINE procedure which selects the k smallest distances from at most k + k2 candidates for each point. [sent-180, score-0.15]
</p><p>47 Complexity Analysis A thorough analysis shows that the time complexities for the overlap method and the glue method are sub-quadratic (in n), and the glue method is always asymptotically faster than the overlap method. [sent-195, score-1.192]
</p><p>48 To this end we assume that in each divide step the subsets X1 and X2 (in both methods) are balanced. [sent-196, score-0.149]
</p><p>49 Hence, the time complexity To for the overlap method and Tg for the glue method satisfy the following recurrence relations: To (n) = 2To ((1 + α)n/2) + f (n),  (5)  Tg (n) = 2Tg (n/2) + Tg (αn) + f (n),  (6)  where f (n) is the combined time for the divide, conquer, and reﬁne steps. [sent-198, score-0.596]
</p><p>50 1 T HE T IME FOR THE D IVIDE S TEP ˆ This includes the time to compute the largest right singular vector v of the centered matrix X, and the time to divide points into subsets X1 and X2 (in the overlap method) or subsets X1 , X2 and X3 (in the glue method). [sent-203, score-0.849]
</p><p>51 2 T HE T IME FOR THE C ONQUER S TEP This step only involves the points in X1 ∩ X2 in the overlap method or X3 in the glue method. [sent-209, score-0.596]
</p><p>52 For each of the αn points in these subsets, k nearest neighbors are chosen from at most 2k candidates. [sent-210, score-0.187]
</p><p>53 3 T HE T IME FOR THE R EFINE S TEP For each point, k nearest neighbors are chosen from at most k + k2 candidates. [sent-214, score-0.187]
</p><p>54 In practice, by using a hash table, only a very small fraction of the k + k2 distances are actually computed in this step; see also Table 3. [sent-217, score-0.137]
</p><p>55 Theorem 1 The time complexity for the overlap method is To (n) = Θ(dnto ), where to = log2/(1+α) 2 =  1 . [sent-225, score-0.246]
</p><p>56 1 − log2 (1 + α)  Theorem 2 The time complexity for the glue method is Tg (n) = Θ(dntg /α), where tg is the solution to the equation 2 + αt = 1. [sent-226, score-0.529]
</p><p>57 Figure 2 plots the two curves of to and tg as functions of α, together with a table that lists some of their values. [sent-228, score-0.209]
</p><p>58 Figure 2 suggests that the glue method is asymptotically faster than the overlap method. [sent-229, score-0.596]
</p><p>59 4  Figure 2: The exponents to and tg as functions of α. [sent-262, score-0.179]
</p><p>60 Theorem 3 When 0 < α < 1, the exponents to in Theorem 1 and tg in Theorem 2 obey the following relation: 1 < tg < t o . [sent-263, score-0.358]
</p><p>61 In this situation the overlap method becomes asymptotically slower than the brute√ force method. [sent-267, score-0.287]
</p><p>62 The divide and conquer methods were implemented according to Algorithms 1 and 2, and the brute-force method was exactly as in the procedure kNN-B RUTE F ORCE given in Appendix A. [sent-277, score-0.267]
</p><p>63 22 + c5 n + c6 to ﬁt the running times of the overlap method and the glue method, respectively. [sent-289, score-0.619]
</p><p>64 The qualities of the resulting graphs versus the running times are plotted in Figure 4. [sent-319, score-0.167]
</p><p>65 The accuracy of an approximate kNN graph G′ (with regard to the exact graph G) is deﬁned as |E(G′ ) ∩ E(G)| , accuracy(G′ ) = |E(G)|  where E(·) means the set of directed edges in the graph. [sent-329, score-0.225]
</p><p>66 (By default, the rank of the nearest neighbor of u is 1. [sent-332, score-0.183]
</p><p>67 The exact kNN graph has the average rank (1 + k)/2. [sent-334, score-0.114]
</p><p>68 It can be seen from Figure 4 that the qualities of the resulting graphs exhibit similar trends by using both measures. [sent-335, score-0.144]
</p><p>69 In addition, the glue method is much faster than the overlap method for the same α, while the latter yields more accurate graphs than the former. [sent-339, score-0.655]
</p><p>70 However, we note that different approximate graphs can yield the same accuracy value or average rank value, hence the actual quality of the graph depends on real applications. [sent-343, score-0.198]
</p><p>71 13%  Table 3: Percentages of actual distance calculations with respect to the total number of needed distances in the reﬁne step, for different data sets, different methods, and different α’s. [sent-510, score-0.144]
</p><p>72 (2006) proposed, at each iteration, to maintain the kNN graph of the clusters, and merge two clusters that are the nearest neighbors in the graph. [sent-536, score-0.334]
</p><p>73 With a delicate implementation using a doubly linked list, they showed that the overall running time of the clustering process reduces to O(τn log n), where τ is the number of nearest neighbor updates at each iteration. [sent-539, score-0.262]
</p><p>74 Their method greatly speeds up the clustering process, while the clustering quality is not much degraded. [sent-540, score-0.14]
</p><p>75 However, the quadratic time to create the initial kNN graph eclipses the improvement in the clustering time. [sent-541, score-0.144]
</p><p>76 However, we bring three improvements over previous work: (1) Two methods to perform the divide step are proposed, (2) an efﬁcient way to compute the separating hyperplane is described, and (3) a detailed and rigorous analysis on the time complexity is provided. [sent-545, score-0.154]
</p><p>77 Figure 6: Agglomerative clustering using kNN graphs on the image data set PIE (68 human subjects). [sent-586, score-0.115]
</p><p>78 They are deﬁned as q  1 j max ni , j ni  ni Purity(i), i=1 n  where  Purity(i) =  ni Entropy(i), i=1 n  where  Entropy(i) = − ∑  Purity = ∑ and  q  q  Entropy = ∑  j  j  n ni logq i . [sent-587, score-0.205]
</p><p>79 ni j=1 ni j  Here, q is the number of classes/clusters, ni is the size of cluster i, and ni is the number of class j data that are assigned to the i-th cluster. [sent-588, score-0.164]
</p><p>80 As can be seen the qualities of the clusterings obtained from the approximate kNN graphs are very close to those resulting from the exact graph, with a few being even much better. [sent-592, score-0.197]
</p><p>81 It is interesting to note that the clustering results seem to have little correlation with the qualities of the graphs governed by the value α. [sent-593, score-0.2]
</p><p>82 The value r′ is in practice a few times of r, and in our situation, the matrix is the (normalized) graph Laplacian, hence nnz = O(kn). [sent-612, score-0.115]
</p><p>83 extract the exact kNN graph, since the approximate graphs are accurate enough for the purpose of dimensionality reduction, while the time costs are signiﬁcantly smaller. [sent-623, score-0.124]
</p><p>84 Figure 7(a) shows the result when using the exact kNN graph, while Figure 7(b) shows the result when using the approximate kNN graph by the overlap method with α = 0. [sent-626, score-0.36]
</p><p>85 Figures 7(c) and 7(d) give two plots when the glue method is used. [sent-629, score-0.38]
</p><p>86 Figure 8 shows the plots of the dimensionality reduction results of Laplacian eigenmaps applied to the data set MNIST, where we used k = 5. [sent-636, score-0.127]
</p><p>87 Figure 8(a) shows the original result by using the exact kNN graph, while 8(b), 8(c) and 8(d) show the results by using the overlap method with α = 0. [sent-637, score-0.272]
</p><p>88 Conclusions We have proposed two sub-quadratic time methods under the framework of divide and conquer for computing approximate kNN graphs for high dimensional data. [sent-643, score-0.35]
</p><p>89 The running times of the methods, as well as the qualities of the resulting graphs, depend on an internal parameter that controls the overlap size of the subsets in the divide step. [sent-644, score-0.503]
</p><p>90 Experiments show that in order to obtain a high quality graph, a small overlap size is usually sufﬁcient and this leads to a small exponent in the time complexity. [sent-645, score-0.274]
</p><p>91 An avenue of future research is to theoretically analyze the quality of the resulting graphs in relation to the overlap size. [sent-646, score-0.333]
</p><p>92 We have shown two such examples: one in agglomerative clustering and the other in dimensionality reduction. [sent-648, score-0.14]
</p><p>93 2009  C HEN , FANG AND S AAD  Proof of Theorem 3 From Theorem 2 we have tg = 1 − log2 (1 − αtg ) > 1. [sent-681, score-0.179]
</p><p>94 = 1 − log2 (1 + α)  to − tg =  Since 0 < α < 1, the denominator 1 − log2 (1 + α) is positive. [sent-683, score-0.179]
</p><p>95 Connectivity of the mutual k-nearest neighbor graph in clustering and outlier detection. [sent-716, score-0.212]
</p><p>96 Approximate nearest neighbors: towards removing the curse of dimensionality. [sent-789, score-0.115]
</p><p>97 Efﬁcient search for approximate nearest neighbor in high dimensional spaces. [sent-810, score-0.207]
</p><p>98 Practical construction of k-nearest neighbor graphs in metric spaces. [sent-843, score-0.127]
</p><p>99 Distributed computation of the knn graph for large high-dimensional point sets. [sent-849, score-0.741]
</p><p>100 A fast all nearest neighbor algorithm for applications involving large point-clouds. [sent-866, score-0.183]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('knn', 0.653), ('glue', 0.35), ('overlap', 0.246), ('tg', 0.179), ('conquer', 0.16), ('lanczos', 0.128), ('onstruction', 0.117), ('nearest', 0.115), ('fang', 0.114), ('divide', 0.107), ('igh', 0.1), ('aad', 0.098), ('efine', 0.096), ('imensional', 0.089), ('graph', 0.088), ('onquer', 0.085), ('qualities', 0.085), ('raph', 0.082), ('distances', 0.075), ('ivide', 0.075), ('orce', 0.075), ('purity', 0.075), ('rute', 0.075), ('brute', 0.075), ('hen', 0.074), ('lue', 0.073), ('pie', 0.073), ('neighbors', 0.072), ('neighbor', 0.068), ('dn', 0.064), ('extyaleb', 0.064), ('bisection', 0.063), ('hash', 0.062), ('mnist', 0.062), ('singular', 0.062), ('graphs', 0.059), ('clustering', 0.056), ('frey', 0.056), ('vi', 0.055), ('hyperplane', 0.047), ('laplacian', 0.047), ('calculations', 0.046), ('agglomerative', 0.045), ('seconds', 0.045), ('poly', 0.045), ('subsets', 0.042), ('indyk', 0.042), ('force', 0.041), ('ni', 0.041), ('belkin', 0.041), ('dimensionality', 0.039), ('merge', 0.037), ('hashing', 0.037), ('xi', 0.037), ('niyogi', 0.037), ('eigenmaps', 0.036), ('qs', 0.036), ('cpu', 0.036), ('ys', 0.035), ('saad', 0.033), ('saul', 0.032), ('callahan', 0.032), ('cosh', 0.032), ('gluing', 0.032), ('lle', 0.032), ('nti', 0.032), ('sdn', 0.032), ('tep', 0.032), ('virmajoki', 0.032), ('plots', 0.03), ('quality', 0.028), ('bentley', 0.027), ('nnz', 0.027), ('cet', 0.027), ('clusterings', 0.027), ('sse', 0.027), ('query', 0.026), ('exact', 0.026), ('nement', 0.026), ('spectral', 0.025), ('preserving', 0.025), ('inexpensive', 0.024), ('minnesota', 0.024), ('dimensional', 0.024), ('roweis', 0.024), ('neighborhood', 0.024), ('running', 0.023), ('accuracy', 0.023), ('distance', 0.023), ('ime', 0.022), ('reduction', 0.022), ('clusters', 0.022), ('argonne', 0.021), ('choset', 0.021), ('juh', 0.021), ('kavraki', 0.021), ('kokiopoulou', 0.021), ('kushilevitza', 0.021), ('paredes', 0.021), ('plaku', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="34-tfidf-1" href="./jmlr-2009-Fast_ApproximatekNN_Graph_Construction_for_High_Dimensional_Data_via_Recursive_Lanczos_Bisection.html">34 jmlr-2009-Fast ApproximatekNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</a></p>
<p>Author: Jie Chen, Haw-ren Fang, Yousef Saad</p><p>Abstract: Nearest neighbor graphs are widely used in data mining and machine learning. A brute-force method to compute the exact kNN graph takes Θ(dn2 ) time for n data points in the d dimensional Euclidean space. We propose two divide and conquer methods for computing an approximate kNN graph in Θ(dnt ) time for high dimensional data (large d). The exponent t ∈ (1, 2) is an increasing function of an internal parameter α which governs the size of the common region in the divide step. Experiments show that a high quality graph can usually be obtained with small overlaps, that is, for small values of t. A few of the practical details of the algorithms are as follows. First, the divide step uses an inexpensive Lanczos procedure to perform recursive spectral bisection. After each conquer step, an additional reﬁnement step is performed to improve the accuracy of the graph. Finally, a hash table is used to avoid repeating distance calculations during the divide and conquer process. The combination of these techniques is shown to yield quite effective algorithms for building kNN graphs. Keywords: nearest neighbors graph, high dimensional data, divide and conquer, Lanczos algorithm, spectral method</p><p>2 0.32380357 <a title="34-tfidf-2" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>Author: Kilian Q. Weinberger, Lawrence K. Saul</p><p>Abstract: The accuracy of k-nearest neighbor (kNN) classiﬁcation depends signiﬁcantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classiﬁcation from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classiﬁcation using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modiﬁcation or extension for problems in multiway (as opposed to binary) classiﬁcation. In our framework, the Mahalanobis distance metric is obtained as the solution to a semideﬁnite program. On several data sets of varying size and difﬁculty, we ﬁnd that metrics trained in this way lead to signiﬁcant improvements in kNN classiﬁcation. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner. Keywords: convex optimization, semi-deﬁnite programming, Mahalanobis distance, metric learning, multi-class classiﬁcation, support vector machines</p><p>3 0.13239813 <a title="34-tfidf-3" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>Author: Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, S.V.N. Vishwanathan</p><p>Abstract: We propose hashing to facilitate efﬁcient kernels. This generalizes previous work using sampling and we show a principled way to compute the kernel matrix for data streams and sparse feature spaces. Moreover, we give deviation bounds from the exact kernel matrix. This has applications to estimation on strings and graphs. Keywords: hashing, stream, string kernel, graphlet kernel, multiclass classiﬁcation</p><p>4 0.086717896 <a title="34-tfidf-4" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Thomas Abeel, Yves Van de Peer, Yvan Saeys</p><p>Abstract: Java-ML is a collection of machine learning and data mining algorithms, which aims to be a readily usable and easily extensible API for both software developers and research scientists. The interfaces for each type of algorithm are kept simple and algorithms strictly follow their respective interface. Comparing different classiﬁers or clustering algorithms is therefore straightforward, and implementing new algorithms is also easy. The implementations of the algorithms are clearly written, properly documented and can thus be used as a reference. The library is written in Java and is available from http://java-ml.sourceforge.net/ under the GNU GPL license. Keywords: open source, machine learning, data mining, java library, clustering, feature selection, classiﬁcation</p><p>5 0.056551103 <a title="34-tfidf-5" href="./jmlr-2009-Polynomial-Delay_Enumeration_of_Monotonic_Graph_Classes.html">72 jmlr-2009-Polynomial-Delay Enumeration of Monotonic Graph Classes</a></p>
<p>Author: Jan Ramon, Siegfried Nijssen</p><p>Abstract: Algorithms that list graphs such that no two listed graphs are isomorphic, are important building blocks of systems for mining and learning in graphs. Algorithms are already known that solve this problem efﬁciently for many classes of graphs of restricted topology, such as trees. In this article we introduce the concept of a dense augmentation schema, and introduce an algorithm that can be used to enumerate any class of graphs with polynomial delay, as long as the class of graphs can be described using a monotonic predicate operating on a dense augmentation schema. In practice this means that this is the ﬁrst enumeration algorithm that can be applied theoretically efﬁciently in any frequent subgraph mining algorithm, and that this algorithm generalizes to situations beyond the standard frequent subgraph mining setting. Keywords: graph mining, enumeration, monotonic graph classes</p><p>6 0.054035123 <a title="34-tfidf-6" href="./jmlr-2009-Strong_Limit_Theorems_for_the_Bayesian_Scoring_Criterion_in_Bayesian_Networks.html">89 jmlr-2009-Strong Limit Theorems for the Bayesian Scoring Criterion in Bayesian Networks</a></p>
<p>7 0.048599057 <a title="34-tfidf-7" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>8 0.041381083 <a title="34-tfidf-8" href="./jmlr-2009-Nearest_Neighbor_Clustering%3A_A_Baseline_Method_for_Consistent_Clustering_with_Arbitrary_Objective_Functions.html">59 jmlr-2009-Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions</a></p>
<p>9 0.038222998 <a title="34-tfidf-9" href="./jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification.html">45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</a></p>
<p>10 0.037830245 <a title="34-tfidf-10" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>11 0.036923632 <a title="34-tfidf-11" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>12 0.035538718 <a title="34-tfidf-12" href="./jmlr-2009-Cautious_Collective_Classification.html">15 jmlr-2009-Cautious Collective Classification</a></p>
<p>13 0.032174949 <a title="34-tfidf-13" href="./jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>14 0.03209798 <a title="34-tfidf-14" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>15 0.029806834 <a title="34-tfidf-15" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>16 0.02879164 <a title="34-tfidf-16" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>17 0.026126659 <a title="34-tfidf-17" href="./jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>18 0.025915211 <a title="34-tfidf-18" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>19 0.025272859 <a title="34-tfidf-19" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>20 0.023744656 <a title="34-tfidf-20" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.167), (1, -0.064), (2, 0.14), (3, -0.053), (4, -0.003), (5, -0.268), (6, 0.122), (7, -0.08), (8, -0.452), (9, 0.102), (10, -0.389), (11, -0.256), (12, -0.116), (13, -0.078), (14, 0.018), (15, 0.035), (16, -0.043), (17, 0.004), (18, 0.005), (19, 0.011), (20, 0.16), (21, -0.097), (22, -0.036), (23, 0.037), (24, 0.061), (25, 0.002), (26, 0.057), (27, -0.029), (28, 0.031), (29, 0.015), (30, -0.008), (31, 0.011), (32, -0.024), (33, 0.024), (34, 0.009), (35, -0.022), (36, -0.038), (37, -0.018), (38, -0.015), (39, -0.003), (40, -0.037), (41, -0.033), (42, -0.022), (43, -0.021), (44, -0.025), (45, 0.041), (46, 0.015), (47, -0.005), (48, -0.044), (49, -0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96308148 <a title="34-lsi-1" href="./jmlr-2009-Fast_ApproximatekNN_Graph_Construction_for_High_Dimensional_Data_via_Recursive_Lanczos_Bisection.html">34 jmlr-2009-Fast ApproximatekNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</a></p>
<p>Author: Jie Chen, Haw-ren Fang, Yousef Saad</p><p>Abstract: Nearest neighbor graphs are widely used in data mining and machine learning. A brute-force method to compute the exact kNN graph takes Θ(dn2 ) time for n data points in the d dimensional Euclidean space. We propose two divide and conquer methods for computing an approximate kNN graph in Θ(dnt ) time for high dimensional data (large d). The exponent t ∈ (1, 2) is an increasing function of an internal parameter α which governs the size of the common region in the divide step. Experiments show that a high quality graph can usually be obtained with small overlaps, that is, for small values of t. A few of the practical details of the algorithms are as follows. First, the divide step uses an inexpensive Lanczos procedure to perform recursive spectral bisection. After each conquer step, an additional reﬁnement step is performed to improve the accuracy of the graph. Finally, a hash table is used to avoid repeating distance calculations during the divide and conquer process. The combination of these techniques is shown to yield quite effective algorithms for building kNN graphs. Keywords: nearest neighbors graph, high dimensional data, divide and conquer, Lanczos algorithm, spectral method</p><p>2 0.8023265 <a title="34-lsi-2" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>Author: Kilian Q. Weinberger, Lawrence K. Saul</p><p>Abstract: The accuracy of k-nearest neighbor (kNN) classiﬁcation depends signiﬁcantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classiﬁcation from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classiﬁcation using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modiﬁcation or extension for problems in multiway (as opposed to binary) classiﬁcation. In our framework, the Mahalanobis distance metric is obtained as the solution to a semideﬁnite program. On several data sets of varying size and difﬁculty, we ﬁnd that metrics trained in this way lead to signiﬁcant improvements in kNN classiﬁcation. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner. Keywords: convex optimization, semi-deﬁnite programming, Mahalanobis distance, metric learning, multi-class classiﬁcation, support vector machines</p><p>3 0.33665544 <a title="34-lsi-3" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>Author: Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, S.V.N. Vishwanathan</p><p>Abstract: We propose hashing to facilitate efﬁcient kernels. This generalizes previous work using sampling and we show a principled way to compute the kernel matrix for data streams and sparse feature spaces. Moreover, we give deviation bounds from the exact kernel matrix. This has applications to estimation on strings and graphs. Keywords: hashing, stream, string kernel, graphlet kernel, multiclass classiﬁcation</p><p>4 0.23130985 <a title="34-lsi-4" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Thomas Abeel, Yves Van de Peer, Yvan Saeys</p><p>Abstract: Java-ML is a collection of machine learning and data mining algorithms, which aims to be a readily usable and easily extensible API for both software developers and research scientists. The interfaces for each type of algorithm are kept simple and algorithms strictly follow their respective interface. Comparing different classiﬁers or clustering algorithms is therefore straightforward, and implementing new algorithms is also easy. The implementations of the algorithms are clearly written, properly documented and can thus be used as a reference. The library is written in Java and is available from http://java-ml.sourceforge.net/ under the GNU GPL license. Keywords: open source, machine learning, data mining, java library, clustering, feature selection, classiﬁcation</p><p>5 0.1854891 <a title="34-lsi-5" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>Author: Brijnesh J. Jain, Klaus Obermayer</p><p>Abstract: Finite structures such as point patterns, strings, trees, and graphs occur as ”natural” representations of structured data in different application areas of machine learning. We develop the theory of structure spaces and derive geometrical and analytical concepts such as the angle between structures and the derivative of functions on structures. In particular, we show that the gradient of a differentiable structural function is a well-deﬁned structure pointing in the direction of steepest ascent. Exploiting the properties of structure spaces, it will turn out that a number of problems in structural pattern recognition such as central clustering or learning in structured output spaces can be formulated as optimization problems with cost functions that are locally Lipschitz. Hence, methods from nonsmooth analysis are applicable to optimize those cost functions. Keywords: graphs, graph matching, learning in structured domains, nonsmooth optimization</p><p>6 0.17430194 <a title="34-lsi-6" href="./jmlr-2009-Polynomial-Delay_Enumeration_of_Monotonic_Graph_Classes.html">72 jmlr-2009-Polynomial-Delay Enumeration of Monotonic Graph Classes</a></p>
<p>7 0.16598344 <a title="34-lsi-7" href="./jmlr-2009-Cautious_Collective_Classification.html">15 jmlr-2009-Cautious Collective Classification</a></p>
<p>8 0.16209555 <a title="34-lsi-8" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>9 0.15932947 <a title="34-lsi-9" href="./jmlr-2009-Strong_Limit_Theorems_for_the_Bayesian_Scoring_Criterion_in_Bayesian_Networks.html">89 jmlr-2009-Strong Limit Theorems for the Bayesian Scoring Criterion in Bayesian Networks</a></p>
<p>10 0.14595814 <a title="34-lsi-10" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>11 0.14467636 <a title="34-lsi-11" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>12 0.13410965 <a title="34-lsi-12" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>13 0.13171279 <a title="34-lsi-13" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>14 0.13150659 <a title="34-lsi-14" href="./jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification.html">45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</a></p>
<p>15 0.12763943 <a title="34-lsi-15" href="./jmlr-2009-Nearest_Neighbor_Clustering%3A_A_Baseline_Method_for_Consistent_Clustering_with_Arbitrary_Objective_Functions.html">59 jmlr-2009-Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions</a></p>
<p>16 0.11657378 <a title="34-lsi-16" href="./jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>17 0.1148818 <a title="34-lsi-17" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>18 0.11076804 <a title="34-lsi-18" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>19 0.11046913 <a title="34-lsi-19" href="./jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>20 0.10808197 <a title="34-lsi-20" href="./jmlr-2009-Bayesian_Network_Structure_Learning_by_Recursive_Autonomy_Identification.html">11 jmlr-2009-Bayesian Network Structure Learning by Recursive Autonomy Identification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.416), (9, 0.01), (11, 0.019), (19, 0.028), (21, 0.018), (26, 0.013), (38, 0.027), (47, 0.014), (52, 0.034), (55, 0.044), (58, 0.025), (66, 0.092), (68, 0.074), (90, 0.054), (96, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69045824 <a title="34-lda-1" href="./jmlr-2009-Fast_ApproximatekNN_Graph_Construction_for_High_Dimensional_Data_via_Recursive_Lanczos_Bisection.html">34 jmlr-2009-Fast ApproximatekNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</a></p>
<p>Author: Jie Chen, Haw-ren Fang, Yousef Saad</p><p>Abstract: Nearest neighbor graphs are widely used in data mining and machine learning. A brute-force method to compute the exact kNN graph takes Θ(dn2 ) time for n data points in the d dimensional Euclidean space. We propose two divide and conquer methods for computing an approximate kNN graph in Θ(dnt ) time for high dimensional data (large d). The exponent t ∈ (1, 2) is an increasing function of an internal parameter α which governs the size of the common region in the divide step. Experiments show that a high quality graph can usually be obtained with small overlaps, that is, for small values of t. A few of the practical details of the algorithms are as follows. First, the divide step uses an inexpensive Lanczos procedure to perform recursive spectral bisection. After each conquer step, an additional reﬁnement step is performed to improve the accuracy of the graph. Finally, a hash table is used to avoid repeating distance calculations during the divide and conquer process. The combination of these techniques is shown to yield quite effective algorithms for building kNN graphs. Keywords: nearest neighbors graph, high dimensional data, divide and conquer, Lanczos algorithm, spectral method</p><p>2 0.62382859 <a title="34-lda-2" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>Author: Alon Zakai, Ya'acov Ritov</p><p>Abstract: We show that all consistent learning methodsĂ˘&euro;&rdquo;that is, that asymptotically achieve the lowest possible expected loss for any distribution on (X,Y )Ă˘&euro;&rdquo;are necessarily localizable, by which we mean that they do not signiÄ?Ĺš cantly change their response at a particular point when we show them only the part of the training set that is close to that point. This is true in particular for methods that appear to be deÄ?Ĺš ned in a non-local manner, such as support vector machines in classiÄ?Ĺš cation and least-squares estimators in regression. Aside from showing that consistency implies a speciÄ?Ĺš c form of localizability, we also show that consistency is logically equivalent to the combination of two properties: (1) a form of localizability, and (2) that the methodĂ˘&euro;&trade;s global mean (over the entire X distribution) correctly estimates the true mean. Consistency can therefore be seen as comprised of two aspects, one local and one global. Keywords: consistency, local learning, regression, classiÄ?Ĺš cation</p><p>3 0.3371394 <a title="34-lda-3" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>Author: John Duchi, Yoram Singer</p><p>Abstract: We describe, analyze, and experiment with a framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we ﬁrst perform an unconstrained gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the ﬁrst phase. This view yields a simple yet effective algorithm that can be used for batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as ℓ1 . We derive concrete and very simple algorithms for minimization of loss functions with ℓ1 , ℓ2 , ℓ2 , and ℓ∞ regularization. We also show how to construct ef2 ﬁcient algorithms for mixed-norm ℓ1 /ℓq regularization. We further extend the algorithms and give efﬁcient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in a series of experiments with synthetic and natural data sets. Keywords: subgradient methods, group sparsity, online learning, convex optimization</p><p>4 0.32273495 <a title="34-lda-4" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>Author: Kilian Q. Weinberger, Lawrence K. Saul</p><p>Abstract: The accuracy of k-nearest neighbor (kNN) classiﬁcation depends signiﬁcantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classiﬁcation from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classiﬁcation using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modiﬁcation or extension for problems in multiway (as opposed to binary) classiﬁcation. In our framework, the Mahalanobis distance metric is obtained as the solution to a semideﬁnite program. On several data sets of varying size and difﬁculty, we ﬁnd that metrics trained in this way lead to signiﬁcant improvements in kNN classiﬁcation. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner. Keywords: convex optimization, semi-deﬁnite programming, Mahalanobis distance, metric learning, multi-class classiﬁcation, support vector machines</p><p>5 0.31864825 <a title="34-lda-5" href="./jmlr-2009-Estimation_of_Sparse_Binary_Pairwise_Markov_Networks_using_Pseudo-likelihoods.html">30 jmlr-2009-Estimation of Sparse Binary Pairwise Markov Networks using Pseudo-likelihoods</a></p>
<p>Author: Holger Höfling, Robert Tibshirani</p><p>Abstract: We consider the problems of estimating the parameters as well as the structure of binary-valued Markov networks. For maximizing the penalized log-likelihood, we implement an approximate procedure based on the pseudo-likelihood of Besag (1975) and generalize it to a fast exact algorithm. The exact algorithm starts with the pseudo-likelihood solution and then adjusts the pseudolikelihood criterion so that each additional iterations moves it closer to the exact solution. Our results show that this procedure is faster than the competing exact method proposed by Lee, Ganapathi, and Koller (2006a). However, we also ﬁnd that the approximate pseudo-likelihood as well as the approaches of Wainwright et al. (2006), when implemented using the coordinate descent procedure of Friedman, Hastie, and Tibshirani (2008b), are much faster than the exact methods, and only slightly less accurate. Keywords: Markov networks, logistic regression, L1 penalty, model selection, Binary variables</p><p>6 0.30349654 <a title="34-lda-6" href="./jmlr-2009-SGD-QN%3A_Careful_Quasi-Newton_Stochastic_Gradient_Descent.html">83 jmlr-2009-SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent</a></p>
<p>7 0.30206782 <a title="34-lda-7" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>8 0.29802939 <a title="34-lda-8" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>9 0.29440513 <a title="34-lda-9" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>10 0.29312429 <a title="34-lda-10" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>11 0.29272258 <a title="34-lda-11" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>12 0.29247779 <a title="34-lda-12" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>13 0.29197651 <a title="34-lda-13" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>14 0.29111898 <a title="34-lda-14" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>15 0.2886827 <a title="34-lda-15" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>16 0.28498235 <a title="34-lda-16" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>17 0.2838392 <a title="34-lda-17" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>18 0.28254032 <a title="34-lda-18" href="./jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">79 jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>19 0.28232941 <a title="34-lda-19" href="./jmlr-2009-Fourier_Theoretic_Probabilistic_Inference_over_Permutations.html">36 jmlr-2009-Fourier Theoretic Probabilistic Inference over Permutations</a></p>
<p>20 0.28230479 <a title="34-lda-20" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
