<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-22" href="#">jmlr2009-22</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</h1>
<br/><p>Source: <a title="jmlr-2009-22-pdf" href="http://jmlr.org/papers/volume10/rieger09a/rieger09a.pdf">pdf</a></p><p>Author: Christian Rieger, Barbara Zwicknagl</p><p>Abstract: We introduce a new technique for the analysis of kernel-based regression problems. The basic tools are sampling inequalities which apply to all machine learning problems involving penalty terms induced by kernels related to Sobolev spaces. They lead to explicit deterministic results concerning the worst case behaviour of ε- and ν-SVRs. Using these, we show how to adjust regularization parameters to get best possible approximation orders for regression. The results are illustrated by some numerical examples. Keywords: sampling inequality, radial basis functions, approximation theory, reproducing kernel Hilbert space, Sobolev space</p><p>Reference: <a title="jmlr-2009-22-reference" href="../jmlr2009_reference/jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The basic tools are sampling inequalities which apply to all machine learning problems involving penalty terms induced by kernels related to Sobolev spaces. [sent-8, score-0.099]
</p><p>2 They lead to explicit deterministic results concerning the worst case behaviour of ε- and ν-SVRs. [sent-9, score-0.078]
</p><p>3 Using these, we show how to adjust regularization parameters to get best possible approximation orders for regression. [sent-10, score-0.117]
</p><p>4 Keywords: sampling inequality, radial basis functions, approximation theory, reproducing kernel Hilbert space, Sobolev space  1. [sent-12, score-0.276]
</p><p>5 Therefore, SV regression and classiﬁcation algorithms are closely related to regularized problems from classical approximation theory as pointed out by Girosi (1998) and Evgeniou et al. [sent-16, score-0.082]
</p><p>6 This paper provides a theoretical framework to derive deterministic error bounds for some popular SV machines. [sent-18, score-0.11]
</p><p>7 We show how a sampling inequality by Wendland and Rieger (2005) can be used to bound the worst-case generalization error for the ν- and the ε-regression without making any statistical assumptions on the inaccuracy of the training data. [sent-19, score-0.106]
</p><p>8 In contrast to the literature, our error bounds explicitly depend on the pointwise noise in the data. [sent-20, score-0.104]
</p><p>9 In the next section we recall some basic facts about reproducing kernels in Hilbert spaces. [sent-23, score-0.134]
</p><p>10 Section 3 deals with regularized approximation problems in Hilbert spaces with reproducing kernels and outlines the connection to classical SV regression (SVR) alc 2009 Christian Rieger and Barbara Zwicknagl. [sent-24, score-0.261]
</p><p>11 We provide a deterministic error analysis for the ν- and the ε-SVR for both exact and inexact training data. [sent-26, score-0.208]
</p><p>12 Our analytical results showing optimal convergence orders in Sobolev spaces are illustrated by numerical experiments. [sent-27, score-0.245]
</p><p>13 Reproducing Kernels in Hilbert Spaces We suppose that K is a positive deﬁnite kernel on some domain Ω ⊂ Rd which should contain at least one point. [sent-29, score-0.096]
</p><p>14 To start with, we brieﬂy recall the well known deﬁnition of a reproducing kernel in a Hilbert space. [sent-30, score-0.147]
</p><p>15 A function K : Ω × Ω → R is called reproducing kernel of H (Ω), if • K(y, ·) ∈ H (Ω) for all y ∈ Ω and • f (y) = ( f , K(y, ·))H (Ω) for all f ∈ H (Ω) and all y ∈ Ω. [sent-36, score-0.147]
</p><p>16 For each positive deﬁnite kernel K : Ω × Ω → R there exists a unique Hilbert space NK (Ω) of functions f : Ω → R, such that K is the reproducing kernel of NK (Ω) (see Wendland, 2005, Theorems 10. [sent-37, score-0.216]
</p><p>17 This Hilbert space NK (Ω) is called the native space of K. [sent-40, score-0.246]
</p><p>18 Though this deﬁnition of a native space is rather abstract, it can be shown that in some cases the native spaces coincide with classical function spaces. [sent-41, score-0.564]
</p><p>19 From now on we shall only consider radial kernels K, that is, K(x, y) = K( x − y ) for all x, y ∈ Rd , where we use the same notation for the kernel K : Rd × Rd → R and for the function K : Rd → R. [sent-42, score-0.257]
</p><p>20 We shall mainly focus on continuous kernels K ∈ L1 (Ω), that is, Z K  L1 (Ω)  :=  Ω  |K (x)| dx < ∞ . [sent-44, score-0.131]
</p><p>21 For the case Ω = Rd there is the following characterization of native spaces of certain radial kernels K : Ω → Rd (Wendland, 2005, Theorem 10. [sent-46, score-0.404]
</p><p>22 Then the native space of K is given by fˆ f ∈ L2 (Rd ) ∩C(Rd ) : √ ∈ L2 (Rd ) ˆ K g ˆ fˆ , = (2π)−d/2 √ , √ ˆ ˆ K K L2 (Rd )  NK (Rd ) = ( f , g)NK (Rd )  where fˆ denotes the Fourier transform of f . [sent-49, score-0.276]
</p><p>23 (1)  Therefore for a radial kernel function K whose Fourier transform decays like ˆ c1 (1 + · 2 )s ≤ K ≤ c2 (1 + · 2 )s , s > d/2 2 2  (2)  for some constants c1 , c2 > 0, the associated native space NK (Rd ) is W2s (Rd ) with an equivalent norm. [sent-51, score-0.429]
</p><p>24 Such nice reproducing kernels are so far only available for certain choices of the space dimension d and the decay parameter s (see Wendland, 2005), but a recent result by Schaback (2009) covers almost all cases of practical interest. [sent-56, score-0.158]
</p><p>25 We shall explain some more properties of these kernels in the experimental part, see Section 10, and refer to the recent monograph by Wendland (2005) for details. [sent-57, score-0.131]
</p><p>26 In order to establish the equivalence of native spaces and Sobolev spaces on bounded domains one needs certain extension theorems for Sobolev functions on bounded domains (see Wendland, 2005). [sent-58, score-0.386]
</p><p>27 We deﬁne the Sobolev spaces of integer orders k ∈ N as W2k (Ω) = { f ∈ L2 (Ω) : f has weak derivatives Dα f ∈ L2 (Ω) of order |α| ≤ k}  with the norm 1/2  u  k W2 (Ω)  :=  ∑  α  D u  2 L2 (Ω)  . [sent-60, score-0.133]
</p><p>28 Regularized Problems in Native Hilbert Spaces In the native Hilbert spaces we consider the following learning or recovery problem. [sent-72, score-0.291]
</p><p>29 Theorem 5 (Representer theorem) If (sX,y , ε∗ ) is a solution of the optimization problem (3), then there exists a vector w ∈ RN such that N  sX,y (·) =  ∑ w jK  j=1  that is sX,y ∈ span K x(1) , · , . [sent-90, score-0.101]
</p><p>30 By the reproducing property of the kernel K in the native K space, the problem (3) can be rewritten as 1 s=s|| +s⊥ N min  ε∈R+  N  ∑ Vε  j=1  s|| , K x( j) , ·  −yj  +  1 1 2 s|| N (Ω) + s⊥ 2 K (Ω) . [sent-99, score-0.393]
</p><p>31 N K 2C 2C  Therefore a solution (sX,y , ε∗ ) of the optimization problem (3) satisﬁes (sX,y )⊥ = 0, which implies sX,y ∈ span K x(1) , · , . [sent-100, score-0.101]
</p><p>32 2118  D ETERMINISTIC E RROR A NALYSIS OF SV R EGRESSION  Corollary 6 If sX,y is a solution of the optimization problem 1 s∈NK (Ω) N min  N  ∑ Vε  j=1  s x( j) − y j  +  1 s 2 K (Ω) , N 2C  with ε ∈ R+ being a ﬁxed parameter, then sX,y ∈ span K x(1) , · , . [sent-106, score-0.101]
</p><p>33 N 2C  (5)  (ν)  Theorem 7 The optimization problem (5) possesses a solution sX,y , ε∗ . [sent-116, score-0.111]
</p><p>34 The Hilbert space H then has the reproducing 1 ˜ kernel K := K, 2Cν 1 where 1 denotes the constant function which maps everything to 1, that is ˜ K ((x, r) , (y, s)) = K (x, y) + 1/(2Cν) for all r, s ∈ R. [sent-121, score-0.147]
</p><p>35 Since Qy is continuous on RN+1 for all y ∈ RN , the problem (7) possesses a solution as shown by Micchelli and Pontil (2005). [sent-126, score-0.079]
</p><p>36 N 2C  (10)  Like the ν-SVR, this optimization problem possesses a solution (see Micchelli and Pontil, 2005, Lemma 1). [sent-140, score-0.111]
</p><p>37 A Sampling Inequality We shall employ a special case of a sampling inequality introduced by Wendland and Rieger (2005). [sent-142, score-0.142]
</p><p>38 Let Ω ⊂ Rd be a bounded 2120  D ETERMINISTIC E RROR A NALYSIS OF SV R EGRESSION  domain with Lipschitz boundary that satisﬁes an interior cone condition. [sent-144, score-0.168]
</p><p>39 A domain Ω is said to satisfy an interior cone condition with radius r > 0 and angle θ ∈ 0, π if for every x ∈ Ω there is 2 a unit vector ξ (x) such that the cone C (x, ξ(x) , θ, r) := x + λy : y ∈ Rd , y  2  = 1, yT ξ (x) ≥ cos(θ), λ ∈ [0, r]  is contained in Ω. [sent-145, score-0.18]
</p><p>40 In particular, a domain which satisﬁes an interior cone condition cannot have any outward cusps. [sent-146, score-0.111]
</p><p>41 We shall assume for the rest of this paper that Ω satisﬁes an interior cone condition with radius Rmax and angle θ. [sent-147, score-0.186]
</p><p>42 We shall derive estimates that are valid only if the training points are sufﬁciently dense in Ω. [sent-148, score-0.105]
</p><p>43 To make this condition precise, we will need a slightly unhandy constant which depends only on the geometry of Ω, namely (see Wendland, 2005) sin 2 arcsin CΩ :=  sin θ 4(1+sin θ)  8 1 + sin 2 arcsin  sin θ 4(1+sin θ)  sin θ (1 + sin θ)  Rmax . [sent-149, score-0.73]
</p><p>44 Suppose that K is a radial kernel function such that the native Hilbert space of K is norm-equivalent to a Sobolev space, that is NK (Ω) = W2τ (Ω). [sent-150, score-0.372]
</p><p>45 But there is a drawback, since the error bounds in terms of N depend crucially on the space dimension d, while error bounds in terms of the ﬁll distance h are dominated by the smoothness of the function to be learned. [sent-168, score-0.205]
</p><p>46 Theorem 8 Suppose Ω ⊂ Rd is a bounded domain with Lipschitz boundary that satisﬁes an interior 1 cone condition. [sent-171, score-0.168]
</p><p>47 Then there 2 exists a positive constant C > 0 such that for all discrete sets X ⊂ Ω with sufﬁciently small ﬁll distance h := hX,Ω ≤ CΩ τ−2 the inequality u holds for all u  ∈ W2τ (Ω),  ≤ C hτ−d( 2 − q )+ u 1  Lq (Ω)  1  τ W2 (Ω) +  u|X  ℓ∞ (X)  where we use the notation (t)+ := max {0,t}. [sent-173, score-0.114]
</p><p>48 We shall apply this theorem to the residual function f − sX,y of the function f ∈ W2τ (Ω) to be recovered and a solution sX,y ∈ W2τ (Ω) of the regression problem. [sent-174, score-0.105]
</p><p>49 In presence of noise the resulting error will explicitly be bounded in terms of the noise in the data. [sent-178, score-0.128]
</p><p>50 ν-SVR with Exact Data In order to derive error bounds for the ν-SVR optimization problem (5) we shall apply Theo(ν) (ν) rem 8 to the residual f − sX,y , where sX,y , ε∗ denotes a solution to the problem (5) for X := x(1) , . [sent-180, score-0.209]
</p><p>51 N 2C  Proof We denote the objective function of the optimization problem (5) by y HC,ν (s, ε) :=  1 N  N  ∑  j=1  s x( j) − y j + νε + ε  1 s 2 K (Ω) , N 2C  (12)  and the interpolant to f with respect to X and K with I f , that is I f |X = y and I f ∈ span K x(1) , · , . [sent-190, score-0.109]
</p><p>52 Theorem 10 Suppose Ω ⊂ Rd is a bounded domain with Lipschitz boundary that satisﬁes an inte1 rior cone condition. [sent-200, score-0.126]
</p><p>53 If this inequality is not satisﬁed, the problem (8) possesses only the trivial solution s ≡ 0 which is not interesting. [sent-214, score-0.103]
</p><p>54 Moreover, the parameter C appears in our error bound as a factor 2C which implies that we expect convergence only in the case C → ∞. [sent-224, score-0.08]
</p><p>55 2123  R IEGER AND Z WICKNAGL  We shall now make our bounds more explicit for the case of quasi-uniformly distributed points. [sent-226, score-0.108]
</p><p>56 Note that these bounds yield arbitrarily high convergence orders, provided that the functions are smooth enough, that is τ is large enough. [sent-230, score-0.106]
</p><p>57 In the following we shall only give our error estimates in terms of the ﬁll distance h rather than in terms of the number of points N. [sent-232, score-0.205]
</p><p>58 However it should be clear how the approximation rates translate into error estimates in terms of N in the case of quasiuniform data due to the inequality (13). [sent-234, score-0.149]
</p><p>59 Corollary 11 shows, that the solution of the ν-SVR leads to the same approximation orders with respect to the ﬁll distance h as classical kernel-based interpolation (see Wendland, 2005). [sent-236, score-0.269]
</p><p>60 Note that there are no assumptions concerning the error distribution. [sent-250, score-0.079]
</p><p>61 Using the sampling inequality as in the case of exact data leads to the following result on Lq -norms. [sent-263, score-0.105]
</p><p>62 The case C → ∞ and ε → 0 leads to exact interpolation, and the well known error bounds for kernel-based interpolation (see Wendland, 2005) are attained. [sent-289, score-0.144]
</p><p>63 Proceeding along the lines of this section, (sℓε) we ﬁnd for a solution sX,y of (16) for exact data the stability bound (sℓε)  sX,y  NK (Ω)  ≤ f NK (Ω)  and the consistency estimate (sℓε) sX,y |X  −y  ℓ∞ (X)  ≤  √  N f 2 K (Ω) + ε2 2 N 2C  1/2  √ √ N ≤√ f NK (Ω) + 2ε . [sent-292, score-0.095]
</p><p>64 C  Therefore, we obtain similar approximation results for the ε-squared loss as for the ε-SVR by inserting the estimates into the sampling inequalities. [sent-293, score-0.102]
</p><p>65 For the special case ε = 0, we obtain the usual least squares, which was analyzed by Wendland and Rieger (2005) in the case of exact data, and by Riplinger (2007) in the case of inexact data. [sent-295, score-0.131]
</p><p>66 N 2C i=1  These bounds shall now be plugged into the sampling inequality. [sent-302, score-0.151]
</p><p>67 If we again assume that the error level δ does not overrule the native space norm of the generating function, r ℓ∞ (X) ≤ δ ≤ f W τ (Ω) , 2  we get the following convergence orders, for our speciﬁc choices of the parameters. [sent-306, score-0.35]
</p><p>68 Numerical Results In this section we present some numerical examples to support our analytical results, in particular the rates of convergence in case of exact training data, and the detection of the error levels in case of noisy data. [sent-310, score-0.216]
</p><p>69 1 Exact Training Data Figure 1 illustrates the approximation orders in case of exact given data as considered in Sections 6 and 8. [sent-312, score-0.155]
</p><p>70 For that, we used regular data sets generated by the respective functions to be reconstructed and employed the ε- and the ν-SVR with the parameter choices provided in Corollaries 17 and 11, respectively. [sent-313, score-0.092]
</p><p>71 As kernel functions we used Wendland’s functions for two reasons: On the one hand side they yield rather sparse kernel matrices K due to their compact support, on the other hand side they are easy to implement since they are piecewise polynomials. [sent-315, score-0.138]
</p><p>72 (17)  By construction we have supp K (c) ⊂ B(0, c), such that small choices of the scaling parameter c imply rather sparse kernel matrices K(c) = K (c) x(i) − x( j) 2 i, j=1. [sent-319, score-0.129]
</p><p>73 This is a typical trade-off situation between good approximation properties and good condition numbers of the kernel matrices K(c) (Wendland, 2005). [sent-324, score-0.098]
</p><p>74 The double logarithmic plots in Figure 1 visualize the convergence orders in terms of the ﬁll distance. [sent-329, score-0.129]
</p><p>75 For that, the L∞ -approximation error f − sX,y L∞ is plotted versus the ﬁll distance h. [sent-330, score-0.1]
</p><p>76 This function f is sampled on regular grids in the unit interval I := [0, 1] with 30 to 96 points. [sent-336, score-0.138]
</p><p>77 We use two different kernel functions, namely (see Wendland, 2005) • K1 (x) = (1 − |x|)3 (3 |x| + 1) with native space W22 ([0, 1]), and + • K2 (x) = (1 − |x|)5 8 |x|2 + 5 |x| + 1 with native space W23 ([0, 1]) . [sent-338, score-0.561]
</p><p>78 The respective corollaries 2129  R IEGER AND Z WICKNAGL  predict convergence rates of 1. [sent-342, score-0.103]
</p><p>79 In subﬁgure 1(a) the plots for the ε- and ν-SVR (almost identical) both show orders 1. [sent-345, score-0.088]
</p><p>80 The data is generated by the smooth function f (x) = sin (x1 + x2 ) . [sent-349, score-0.141]
</p><p>81 This function f is sampled on regular grids in the unit interval I := [0, 1]2 with 16 to 144 points. [sent-350, score-0.138]
</p><p>82 We use three different kernel functions, namely (see Wendland, 2005) • K3 (x) = (1 − x )4 (4 x + 1) with native space W22. [sent-352, score-0.315]
</p><p>83 5 [0, 1]2 , + • K4 (x) = (1 − x )6 35 x +  2  + 18 x + 3 with native space W23. [sent-353, score-0.246]
</p><p>84 5 [0, 1]2 , and  • K5 (x) = (1 − x )8 32 x +  3  + 25 x  2  + 8 x + 1 with native space W24. [sent-354, score-0.246]
</p><p>85 The predicted convergence rates in the ﬁll distance h are 1. [sent-358, score-0.129]
</p><p>86 2  −4  3 (a) Data generated by f ∈ W2 (I) on regular grids in I. [sent-374, score-0.138]
</p><p>87 6  (b) Data generated by smooth function on regular grids in I 2 . [sent-386, score-0.17]
</p><p>88 Figure 1: Logarithm of the L∞ -approximation error plotted versus the logarithm of the ﬁll distance h for exact training data. [sent-392, score-0.138]
</p><p>89 The plots show the L∞ -approximation error f − sX,y L∞ versus the ﬁll distance h. [sent-395, score-0.1]
</p><p>90 In Subﬁgure 2(a) the function f (x) = sin(10x) is sampled on regular grids of 2 to 56 points in [0, 1]. [sent-398, score-0.138]
</p><p>91 The data is disturbed by an error r which is normally distributed with mean zero and standard deviation 0. [sent-399, score-0.096]
</p><p>92 In Subﬁgure 2(b) the function f (x) = sin(10x) is sampled on regular grids of 5 to 56 points in the unit interval I = [0, 1]. [sent-403, score-0.138]
</p><p>93 The plot shows that the L∞ approximation error converges to a constant of the order of magnitude of the error level for h → 0. [sent-408, score-0.107]
</p><p>94 16  (a) Data disturbed by random error with mean zero and standard deviation 0. [sent-431, score-0.096]
</p><p>95 Approximation error for h → 0 reaches the error level and remains bounded of the same order of magnitude as the error level. [sent-433, score-0.142]
</p><p>96 14  (b) Data disturbed by random sign deterministic error ±0. [sent-441, score-0.134]
</p><p>97 Figure 2: L∞ -approximation error versus ﬁll distance in case of inexact data. [sent-444, score-0.193]
</p><p>98 Summary and Outlook We proved deterministic worst-case error estimates for kernel-based regression algorithms. [sent-446, score-0.107]
</p><p>99 We provided a detailed analysis only for the ν- and the ε-SVR for both exact and inexact training data. [sent-448, score-0.131]
</p><p>100 So far, our error estimates depend explicitly on the pointwise noise in the data, and we do not make any assumptions on the noise distribution. [sent-451, score-0.133]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wendland', 0.531), ('nk', 0.292), ('native', 0.246), ('rd', 0.211), ('hx', 0.19), ('rieger', 0.19), ('ieger', 0.171), ('wicknagl', 0.171), ('ll', 0.162), ('sv', 0.162), ('eterministic', 0.152), ('rror', 0.129), ('sobolev', 0.123), ('egression', 0.116), ('sin', 0.109), ('lq', 0.106), ('grids', 0.097), ('hc', 0.097), ('inexact', 0.093), ('orders', 0.088), ('kw', 0.087), ('hilbert', 0.085), ('nalysis', 0.083), ('nu', 0.081), ('bonn', 0.08), ('reproducing', 0.078), ('eps', 0.076), ('qy', 0.076), ('zwicknagl', 0.076), ('shall', 0.075), ('rn', 0.072), ('cone', 0.069), ('kernel', 0.069), ('lipschitz', 0.066), ('lkopf', 0.061), ('distance', 0.061), ('disturbed', 0.057), ('nhc', 0.057), ('radial', 0.057), ('kernels', 0.056), ('fourier', 0.052), ('sch', 0.052), ('possesses', 0.049), ('micchelli', 0.046), ('spaces', 0.045), ('numerical', 0.044), ('sampling', 0.043), ('interior', 0.042), ('regular', 0.041), ('convergence', 0.041), ('sub', 0.04), ('concerning', 0.04), ('error', 0.039), ('span', 0.039), ('exact', 0.038), ('pontil', 0.038), ('arcsin', 0.038), ('interpolant', 0.038), ('deterministic', 0.038), ('ri', 0.038), ('scaling', 0.036), ('ix', 0.035), ('barbara', 0.035), ('corollaries', 0.035), ('corollary', 0.034), ('interpolation', 0.034), ('bounds', 0.033), ('corrupted', 0.033), ('svr', 0.032), ('schaback', 0.032), ('brenner', 0.032), ('yj', 0.032), ('smooth', 0.032), ('boundary', 0.032), ('optimization', 0.032), ('noise', 0.032), ('transform', 0.03), ('solution', 0.03), ('estimates', 0.03), ('approximation', 0.029), ('des', 0.029), ('und', 0.029), ('vd', 0.029), ('hausdorff', 0.029), ('scattered', 0.029), ('discrete', 0.029), ('evgeniou', 0.028), ('stability', 0.027), ('representer', 0.027), ('analytical', 0.027), ('rates', 0.027), ('classical', 0.027), ('suppose', 0.027), ('rmax', 0.027), ('decays', 0.027), ('reconstructed', 0.027), ('regularized', 0.026), ('bounded', 0.025), ('inequality', 0.024), ('choices', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="22-tfidf-1" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>Author: Christian Rieger, Barbara Zwicknagl</p><p>Abstract: We introduce a new technique for the analysis of kernel-based regression problems. The basic tools are sampling inequalities which apply to all machine learning problems involving penalty terms induced by kernels related to Sobolev spaces. They lead to explicit deterministic results concerning the worst case behaviour of ε- and ν-SVRs. Using these, we show how to adjust regularization parameters to get best possible approximation orders for regression. The results are illustrated by some numerical examples. Keywords: sampling inequality, radial basis functions, approximation theory, reproducing kernel Hilbert space, Sobolev space</p><p>2 0.14002332 <a title="22-tfidf-2" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: We continue our recent study on constructing a reﬁnement kernel for a given kernel so that the reproducing kernel Hilbert space associated with the reﬁnement kernel contains that with the original kernel as a subspace. To motivate this study, we ﬁrst develop a reﬁnement kernel method for learning, which gives an efﬁcient algorithm for updating a learning predictor. Several characterizations of reﬁnement kernels are then presented. It is shown that a nontrivial reﬁnement kernel for a given kernel always exists if the input space has an inﬁnite cardinal number. Reﬁnement kernels for translation invariant kernels and Hilbert-Schmidt kernels are investigated. Various concrete examples are provided. Keywords: reproducing kernels, reproducing kernel Hilbert spaces, learning with kernels, reﬁnement kernels, translation invariant kernels, Hilbert-Schmidt kernels</p><p>3 0.094564445 <a title="22-tfidf-3" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>Author: Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the L2 norm, then the learned vector is a linear combination of the input data. This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, in the case of differentiable regularizers. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufﬁcient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions. Keywords: kernel methods, matrix learning, minimal norm interpolation, multi-task learning, regularization</p><p>4 0.088348597 <a title="22-tfidf-4" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>Author: Haizhang Zhang, Yuesheng Xu, Jun Zhang</p><p>Abstract: We introduce the notion of reproducing kernel Banach spaces (RKBS) and study special semiinner-product RKBS by making use of semi-inner-products and the duality mapping. Properties of an RKBS and its reproducing kernel are investigated. As applications, we develop in the framework of RKBS standard learning schemes including minimal norm interpolation, regularization network, support vector machines, and kernel principal component analysis. In particular, existence, uniqueness and representer theorems are established. Keywords: reproducing kernel Banach spaces, reproducing kernels, learning theory, semi-innerproducts, representer theorems</p><p>5 0.06338881 <a title="22-tfidf-5" href="./jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">16 jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<p>Author: Dao-Hong Xiang, Ding-Xuan Zhou</p><p>Abstract: This paper considers binary classiﬁcation algorithms generated from Tikhonov regularization schemes associated with general convex loss functions and varying Gaussian kernels. Our main goal is to provide fast convergence rates for the excess misclassiﬁcation error. Allowing varying Gaussian kernels in the algorithms improves learning rates measured by regularization error and sample error. Special structures of Gaussian kernels enable us to construct, by a nice approximation scheme with a Fourier analysis technique, uniformly bounded regularizing functions achieving polynomial decays of the regularization error under a Sobolev smoothness condition. The sample error is estimated by using a projection operator and a tight bound for the covering numbers of reproducing kernel Hilbert spaces generated by Gaussian kernels. The convexity of the general loss function plays a very important role in our analysis. Keywords: reproducing kernel Hilbert space, binary classiﬁcation, general convex loss, varying Gaussian kernels, covering number, approximation</p><p>6 0.060139865 <a title="22-tfidf-6" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>7 0.059025157 <a title="22-tfidf-7" href="./jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>8 0.050545104 <a title="22-tfidf-8" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>9 0.043048248 <a title="22-tfidf-9" href="./jmlr-2009-Fourier_Theoretic_Probabilistic_Inference_over_Permutations.html">36 jmlr-2009-Fourier Theoretic Probabilistic Inference over Permutations</a></p>
<p>10 0.040549409 <a title="22-tfidf-10" href="./jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>11 0.039623663 <a title="22-tfidf-11" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>12 0.035708588 <a title="22-tfidf-12" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>13 0.033698343 <a title="22-tfidf-13" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>14 0.032522522 <a title="22-tfidf-14" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>15 0.032412566 <a title="22-tfidf-15" href="./jmlr-2009-Online_Learning_with_Samples_Drawn_from_Non-identical_Distributions.html">68 jmlr-2009-Online Learning with Samples Drawn from Non-identical Distributions</a></p>
<p>16 0.030638771 <a title="22-tfidf-16" href="./jmlr-2009-Identification_of_Recurrent_Neural_Networks_by_Bayesian_Interrogation_Techniques.html">40 jmlr-2009-Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques</a></p>
<p>17 0.03042894 <a title="22-tfidf-17" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>18 0.030340927 <a title="22-tfidf-18" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>19 0.029597348 <a title="22-tfidf-19" href="./jmlr-2009-Dlib-ml%3A_A_Machine_Learning_Toolkit%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">26 jmlr-2009-Dlib-ml: A Machine Learning Toolkit    (Machine Learning Open Source Software Paper)</a></p>
<p>20 0.029022193 <a title="22-tfidf-20" href="./jmlr-2009-Estimating_Labels_from_Label_Proportions.html">29 jmlr-2009-Estimating Labels from Label Proportions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.173), (1, 0.014), (2, 0.089), (3, -0.082), (4, -0.221), (5, 0.105), (6, 0.085), (7, -0.198), (8, -0.029), (9, 0.024), (10, 0.064), (11, 0.039), (12, 0.136), (13, 0.016), (14, 0.019), (15, -0.047), (16, -0.006), (17, 0.033), (18, 0.02), (19, 0.014), (20, 0.018), (21, -0.004), (22, 0.169), (23, 0.059), (24, -0.041), (25, 0.049), (26, -0.092), (27, -0.094), (28, -0.186), (29, -0.201), (30, -0.022), (31, -0.022), (32, -0.057), (33, -0.09), (34, 0.003), (35, 0.039), (36, -0.09), (37, 0.153), (38, -0.029), (39, 0.076), (40, -0.025), (41, -0.055), (42, -0.038), (43, 0.051), (44, -0.143), (45, 0.14), (46, -0.001), (47, -0.199), (48, -0.06), (49, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94051892 <a title="22-lsi-1" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>Author: Christian Rieger, Barbara Zwicknagl</p><p>Abstract: We introduce a new technique for the analysis of kernel-based regression problems. The basic tools are sampling inequalities which apply to all machine learning problems involving penalty terms induced by kernels related to Sobolev spaces. They lead to explicit deterministic results concerning the worst case behaviour of ε- and ν-SVRs. Using these, we show how to adjust regularization parameters to get best possible approximation orders for regression. The results are illustrated by some numerical examples. Keywords: sampling inequality, radial basis functions, approximation theory, reproducing kernel Hilbert space, Sobolev space</p><p>2 0.62176299 <a title="22-lsi-2" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: We continue our recent study on constructing a reﬁnement kernel for a given kernel so that the reproducing kernel Hilbert space associated with the reﬁnement kernel contains that with the original kernel as a subspace. To motivate this study, we ﬁrst develop a reﬁnement kernel method for learning, which gives an efﬁcient algorithm for updating a learning predictor. Several characterizations of reﬁnement kernels are then presented. It is shown that a nontrivial reﬁnement kernel for a given kernel always exists if the input space has an inﬁnite cardinal number. Reﬁnement kernels for translation invariant kernels and Hilbert-Schmidt kernels are investigated. Various concrete examples are provided. Keywords: reproducing kernels, reproducing kernel Hilbert spaces, learning with kernels, reﬁnement kernels, translation invariant kernels, Hilbert-Schmidt kernels</p><p>3 0.53751069 <a title="22-lsi-3" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>Author: Haizhang Zhang, Yuesheng Xu, Jun Zhang</p><p>Abstract: We introduce the notion of reproducing kernel Banach spaces (RKBS) and study special semiinner-product RKBS by making use of semi-inner-products and the duality mapping. Properties of an RKBS and its reproducing kernel are investigated. As applications, we develop in the framework of RKBS standard learning schemes including minimal norm interpolation, regularization network, support vector machines, and kernel principal component analysis. In particular, existence, uniqueness and representer theorems are established. Keywords: reproducing kernel Banach spaces, reproducing kernels, learning theory, semi-innerproducts, representer theorems</p><p>4 0.40995982 <a title="22-lsi-4" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>Author: Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the L2 norm, then the learned vector is a linear combination of the input data. This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, in the case of differentiable regularizers. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufﬁcient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions. Keywords: kernel methods, matrix learning, minimal norm interpolation, multi-task learning, regularization</p><p>5 0.39128605 <a title="22-lsi-5" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>Author: Junhui Wang, Xiaotong Shen, Wei Pan</p><p>Abstract: In classiﬁcation, semisupervised learning usually involves a large amount of unlabeled data with only a small number of labeled data. This imposes a great challenge in that it is difﬁcult to achieve good classiﬁcation performance through labeled data alone. To leverage unlabeled data for enhancing classiﬁcation, this article introduces a large margin semisupervised learning method within the framework of regularization, based on an efﬁcient margin loss for unlabeled data, which seeks efﬁcient extraction of the information from unlabeled data for estimating the Bayes decision boundary for classiﬁcation. For implementation, an iterative scheme is derived through conditional expectations. Finally, theoretical and numerical analyses are conducted, in addition to an application to gene function prediction. They suggest that the proposed method enables to recover the performance of its supervised counterpart based on complete data in rates of convergence, when possible. Keywords: difference convex programming, classiﬁcation, nonconvex minimization, regularization, support vectors</p><p>6 0.38581088 <a title="22-lsi-6" href="./jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">16 jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<p>7 0.26635024 <a title="22-lsi-7" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>8 0.26289383 <a title="22-lsi-8" href="./jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>9 0.2586132 <a title="22-lsi-9" href="./jmlr-2009-Distributed_Algorithms_for_Topic_Models.html">25 jmlr-2009-Distributed Algorithms for Topic Models</a></p>
<p>10 0.23616119 <a title="22-lsi-10" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>11 0.23256941 <a title="22-lsi-11" href="./jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>12 0.22511113 <a title="22-lsi-12" href="./jmlr-2009-Learning_Linear_Ranking_Functions_for_Beam_Search_with_Application_to_Planning.html">47 jmlr-2009-Learning Linear Ranking Functions for Beam Search with Application to Planning</a></p>
<p>13 0.2231883 <a title="22-lsi-13" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>14 0.20880894 <a title="22-lsi-14" href="./jmlr-2009-Dlib-ml%3A_A_Machine_Learning_Toolkit%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">26 jmlr-2009-Dlib-ml: A Machine Learning Toolkit    (Machine Learning Open Source Software Paper)</a></p>
<p>15 0.2084336 <a title="22-lsi-15" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>16 0.17963398 <a title="22-lsi-16" href="./jmlr-2009-Hybrid_MPI_OpenMP_Parallel_Linear_Support_Vector_Machine_Training.html">39 jmlr-2009-Hybrid MPI OpenMP Parallel Linear Support Vector Machine Training</a></p>
<p>17 0.17526989 <a title="22-lsi-17" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>18 0.17220989 <a title="22-lsi-18" href="./jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems.html">8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</a></p>
<p>19 0.16545616 <a title="22-lsi-19" href="./jmlr-2009-The_P-Norm_Push%3A_A_Simple_Convex_Ranking_Algorithm_that_Concentrates_at_the_Top_of_the_List.html">95 jmlr-2009-The P-Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List</a></p>
<p>20 0.16396844 <a title="22-lsi-20" href="./jmlr-2009-Fourier_Theoretic_Probabilistic_Inference_over_Permutations.html">36 jmlr-2009-Fourier Theoretic Probabilistic Inference over Permutations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.01), (11, 0.045), (19, 0.032), (21, 0.025), (26, 0.012), (38, 0.02), (48, 0.398), (52, 0.028), (55, 0.073), (58, 0.03), (66, 0.109), (68, 0.023), (90, 0.084), (96, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.66781235 <a title="22-lda-1" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>Author: Christian Rieger, Barbara Zwicknagl</p><p>Abstract: We introduce a new technique for the analysis of kernel-based regression problems. The basic tools are sampling inequalities which apply to all machine learning problems involving penalty terms induced by kernels related to Sobolev spaces. They lead to explicit deterministic results concerning the worst case behaviour of ε- and ν-SVRs. Using these, we show how to adjust regularization parameters to get best possible approximation orders for regression. The results are illustrated by some numerical examples. Keywords: sampling inequality, radial basis functions, approximation theory, reproducing kernel Hilbert space, Sobolev space</p><p>2 0.36642128 <a title="22-lda-2" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>Author: Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the L2 norm, then the learned vector is a linear combination of the input data. This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, in the case of differentiable regularizers. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufﬁcient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions. Keywords: kernel methods, matrix learning, minimal norm interpolation, multi-task learning, regularization</p><p>3 0.36432731 <a title="22-lda-3" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>Author: Sanjoy Dasgupta, Adam Tauman Kalai, Claire Monteleoni</p><p>Abstract: We start by showing that in an active learning setting, the Perceptron algorithm needs Ω( ε12 ) labels to learn linear separators within generalization error ε. We then present a simple active learning algorithm for this problem, which combines a modiﬁcation of the Perceptron update with an adaptive ﬁltering rule for deciding which points to query. For data distributed uniformly over the unit ˜ sphere, we show that our algorithm reaches generalization error ε after asking for just O(d log 1 ) ε labels. This exponential improvement over the usual sample complexity of supervised learning had previously been demonstrated only for the computationally more complex query-by-committee algorithm. Keywords: active learning, perceptron, label complexity bounds, online learning</p><p>4 0.36394 <a title="22-lda-4" href="./jmlr-2009-Bounded_Kernel-Based_Online_Learning.html">13 jmlr-2009-Bounded Kernel-Based Online Learning</a></p>
<p>Author: Francesco Orabona, Joseph Keshet, Barbara Caputo</p><p>Abstract: A common problem of kernel-based online algorithms, such as the kernel-based Perceptron algorithm, is the amount of memory required to store the online hypothesis, which may increase without bound as the algorithm progresses. Furthermore, the computational load of such algorithms grows linearly with the amount of memory used to store the hypothesis. To attack these problems, most previous work has focused on discarding some of the instances, in order to keep the memory bounded. In this paper we present a new algorithm, in which the instances are not discarded, but are instead projected onto the space spanned by the previous online hypothesis. We call this algorithm Projectron. While the memory size of the Projectron solution cannot be predicted before training, we prove that its solution is guaranteed to be bounded. We derive a relative mistake bound for the proposed algorithm, and deduce from it a slightly different algorithm which outperforms the Perceptron. We call this second algorithm Projectron++. We show that this algorithm can be extended to handle the multiclass and the structured output settings, resulting, as far as we know, in the ﬁrst online bounded algorithm that can learn complex classiﬁcation tasks. The method of bounding the hypothesis representation can be applied to any conservative online algorithm and to other online algorithms, as it is demonstrated for ALMA2 . Experimental results on various data sets show the empirical advantage of our technique compared to various bounded online algorithms, both in terms of memory and accuracy. Keywords: online learning, kernel methods, support vector machines, bounded support set</p><p>5 0.36295709 <a title="22-lda-5" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider regularized support vector machines (SVMs) and show that they are precisely equivalent to a new robust optimization formulation. We show that this equivalence of robust optimization and regularization has implications for both algorithms, and analysis. In terms of algorithms, the equivalence suggests more general SVM-like algorithms for classiﬁcation that explicitly build in protection to noise, and at the same time control overﬁtting. On the analysis front, the equivalence of robustness and regularization provides a robust optimization interpretation for the success of regularized SVMs. We use this new robustness interpretation of SVMs to give a new proof of consistency of (kernelized) SVMs, thus establishing robustness as the reason regularized SVMs generalize well. Keywords: robustness, regularization, generalization, kernel, support vector machine</p><p>6 0.36267224 <a title="22-lda-6" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>7 0.3616353 <a title="22-lda-7" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>8 0.3593466 <a title="22-lda-8" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>9 0.35905582 <a title="22-lda-9" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>10 0.3574802 <a title="22-lda-10" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>11 0.35552755 <a title="22-lda-11" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>12 0.35503489 <a title="22-lda-12" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>13 0.35472205 <a title="22-lda-13" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>14 0.35396528 <a title="22-lda-14" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>15 0.35251254 <a title="22-lda-15" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>16 0.35105219 <a title="22-lda-16" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>17 0.35083166 <a title="22-lda-17" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>18 0.35078213 <a title="22-lda-18" href="./jmlr-2009-Settable_Systems%3A_An_Extension_of_Pearl%27s_Causal_Model_with_Optimization%2C_Equilibrium%2C_and_Learning.html">85 jmlr-2009-Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning</a></p>
<p>19 0.34886128 <a title="22-lda-19" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>20 0.34797555 <a title="22-lda-20" href="./jmlr-2009-SGD-QN%3A_Careful_Quasi-Newton_Stochastic_Gradient_Descent.html">83 jmlr-2009-SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
