<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>59 jmlr-2009-Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-59" href="#">jmlr2009-59</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>59 jmlr-2009-Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions</h1>
<br/><p>Source: <a title="jmlr-2009-59-pdf" href="http://jmlr.org/papers/volume10/bubeck09a/bubeck09a.pdf">pdf</a></p><p>Author: Sébastien Bubeck, Ulrike von Luxburg</p><p>Abstract: Clustering is often formulated as a discrete optimization problem. The objective is to ﬁnd, among all partitions of the data set, the best one according to some quality measure. However, in the statistical setting where we assume that the ﬁnite data set has been sampled from some underlying space, the goal is not to ﬁnd the best partition of the given sample, but to approximate the true partition of the underlying space. We argue that the discrete optimization approach usually does not achieve this goal, and instead can lead to inconsistency. We construct examples which provably have this behavior. As in the case of supervised learning, the cure is to restrict the size of the function classes under consideration. For appropriate “small” function classes we can prove very general consistency theorems for clustering optimization schemes. As one particular algorithm for clustering with a restricted function space we introduce “nearest neighbor clustering”. Similar to the k-nearest neighbor classiﬁer in supervised learning, this algorithm can be seen as a general baseline algorithm to minimize arbitrary clustering objective functions. We prove that it is statistically consistent for all commonly used clustering objective functions. Keywords: clustering, minimizing objective functions, consistency</p><p>Reference: <a title="jmlr-2009-59-reference" href="../jmlr2009_reference/jmlr-2009-Nearest_Neighbor_Clustering%3A_A_Baseline_Method_for_Consistent_Clustering_with_Arbitrary_Objective_Functions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In practice, the most common approach to clustering is to deﬁne a clustering quality function Qn , and then construct an algorithm which is able to minimize (or maximize) Qn . [sent-19, score-0.565]
</p><p>2 Once a particular clustering quality function Qn has been selected, the objective of clustering is stated as a discrete optimization problem. [sent-21, score-0.613]
</p><p>3 , Xn } and a clustering quality function Qn , the ideal clustering algorithm should take into account all possible partitions of the data set and output the one that minimizes Qn . [sent-25, score-0.591]
</p><p>4 We choose a clustering quality function Q on the set of partitions of the entire data space X , and deﬁne the true clustering f ∗ to be the partition of X which minimizes Q. [sent-35, score-0.591]
</p><p>5 To this end, we deﬁne an empirical quality function Qn which can be evaluated based on the ﬁnite sample only, and construct the empirical clustering fn as the minimizer of Qn . [sent-37, score-0.673]
</p><p>6 In this setting, a very important property of a clustering algorithm is consistency: we require that Q( fn ) converges to Q( f ∗ ) when n → ∞. [sent-38, score-0.629]
</p><p>7 We prove that nearest neighbor clustering is consistent under minimal assumptions on the clustering quality functions Qn and Q. [sent-73, score-0.74]
</p><p>8 Then we will apply nearest neighbor clustering to a large variety of clustering objective functions, such as the K-means objective function, normalized cut and ratio cut, the modularity objective function, or functions based on within-between cluster similarity ratios. [sent-74, score-1.065]
</p><p>9 Let Q : H → Ê+ denote a clustering quality function: for each clustering, it tells us “how good” a given clustering is. [sent-88, score-0.565]
</p><p>10 Our goal is to use this sample to construct a clustering fn which “approximates” an optimal clustering f ∗ . [sent-98, score-0.881]
</p><p>11 We then consider the clustering fn ∈ argmin Qn ( f ). [sent-103, score-0.646]
</p><p>12 The empirical clustering fn , if restricted to the data points, reﬂects the correct clustering. [sent-147, score-0.615]
</p><p>13 It is just the “extension” of the empirical clustering to non-training points which leads to the inconsistency of fn . [sent-148, score-0.636]
</p><p>14 We call the resulting clustering the nearest neighbor clustering and denote it by NNC(Qn ). [sent-256, score-0.693]
</p><p>15 Output:  fn := argmin f ∈Fn Qn ( f )  Figure 1: Nearest neighbor clustering for a general clustering objective function Qn . [sent-269, score-1.03]
</p><p>16 Using these methods, the running time of nearest neighbor clustering using m = log n seeds is roughly comparable to the one of the other clustering algorithms. [sent-274, score-0.721]
</p><p>17 In the following, we will often use the notation fk for the indicator function of the k-th cluster: fk (x) := ½ f (x)=k . [sent-284, score-1.194]
</p><p>18 However, from the context it will always be clear whether we will refer to fn or fk , respectively, as we will not mix up the letters n (for the sample size) and k (a cluster index). [sent-286, score-0.989]
</p><p>19 In most cases, we will use functionals Φ which operate on the individual cluster indicator functions fk . [sent-307, score-0.64]
</p><p>20 For example, Φ( fk ) could measure the size of cluster k, or the smoothness of the cluster boundary. [sent-308, score-0.683]
</p><p>21 The function class F will then be deﬁned as F = { f ∈ H | Φ( fk ) > a for all k = 1, . [sent-309, score-0.597]
</p><p>22 Note that this setup also includes the general case of F = H , that is the case where we do not want to make any further restrictions on F , for example by setting Φ( fk ) ≡ 1, a ≡ 0. [sent-314, score-0.613]
</p><p>23 Φn ( fk ) is a consistent estimator of Φ( fk ) which converges sufﬁciently fast for all f ∈ Fn : ∀ε > 0, K m (2n)(d+1)m sup È(|Φn ( fk ) − Φ( fk )| > ε) → 0, 2  f ∈Fn  3. [sent-351, score-2.473]
</p><p>24 Φk ( f ) := Φ( fk ) is uniformly continuous with respect to the pseudo-distance d( f , g) between F and Fn , as deﬁned in Condition (3) of Theorem 1, 5. [sent-353, score-0.597]
</p><p>25 Then nearest neighbor clustering based on m seed points using quality function Qn is weakly consistent, that is for fn ∈ argmin f ∈Fn Qn ( f ) and f ∗ ∈ argmin f ∈F Q( f ) we have Q( fn ) → Q( f ∗ ) in probability. [sent-356, score-1.254]
</p><p>26 , K} we introduce the following quantities: WSSn ( f ) := ck,n :=  1 n K ∑ ∑ fk (Xi ) Xi − ck,n n i=1 k=1  2  1 1 n ∑ fk (Xi )Xi nk n i=1  and  K  WSS( f ) :=  ∑ fk (X)  k=1  where  X − ck  2  where  nk :=  1 k ∑ fk (Xi ) n i=1  ck :=  fk (X)X . [sent-368, score-3.217]
</p><p>27 fk (X)  Here, WSSn plays the role of Qn and WSS the role of Q. [sent-369, score-0.597]
</p><p>28 However, at least we have nk = fk (X) and 1 ∑n fk (Xi )Xi = fk (X)X. [sent-372, score-1.828]
</p><p>29 Secondly, our setup for proving the consistency of nearest neighbor clustering with the WSS objective function is considerably more complicated than proving the consistency of the global minimizer of the K-means algorithm (e. [sent-374, score-0.618]
</p><p>30 To achieve this, we use the functionals ΦWSS ( fk ) := fk (X),  ΦWSSn ( fk ) := nk ( f ). [sent-388, score-1.828]
</p><p>31 and will only consider clusterings where Φ( fk ) ≥ a > 0. [sent-389, score-0.647]
</p><p>32 , K} ΦWSSn ( fk ) > an } Moreover, for technical convenience we restrict our attention to probability measures which have a bounded support inside some large ball, that is which satisfy supp È ⊂ B(0, A) for some constant A > 0. [sent-406, score-0.616]
</p><p>33 n(a − an )2 Then for all probability measures on Êd with bounded support, nearest neighbor clustering with WSS is consistent, that is if n → ∞ then WSS( fn ) → WSS( f ∗ ) in probability. [sent-409, score-0.776]
</p><p>34 2 NNC Using Standard Graph-cut Based Objective Functions In this section we want to look into the consistency of nearest neighbor clustering for graph based objective functions as they are used in spectral clustering (see von Luxburg, 2007 for details). [sent-427, score-0.914]
</p><p>35 For a given cluster described by the cluster indicator function fk : Êd → {0, 1}, we set cut( fk ) := cut( fk , È) := fk (X1 )(1 − fk (X2 ))s(X1 , X2 ), vol( fk ) := vol( fk , È) := fk (X1 )s(X1 , X2 ). [sent-430, score-4.862]
</p><p>36 668  C ONSISTENT C LUSTERING WITH A RBITRARY O BJECTIVE F UNCTIONS  For f ∈ H we can then deﬁne the normalized cut and the ratio cut by Ncut( f ) := Ncut( f , È) :=  K  cut( fk )  ∑ vol( fk ) ,  k=1  RatioCut( f ) := RatioCut( f , È) :=  K  cut( fk ) . [sent-431, score-2.149]
</p><p>37 We will use Φcut ( fk ) := vol( fk ),  ΦNcut ( fk ) := vol( fk ),  ΦRatioCut ( fk ) := fk (X). [sent-434, score-3.582]
</p><p>38 n(a − an )2 Then nearest neighbor clustering with cut, Ncut and RatioCut is universally weakly consistent, that is for all probability measures, if n → ∞ we have cut( fn ) → cut( f ∗ ), Ncut( fn ) → Ncut( f ∗ ) and RatioCut( fn ) → RatioCut( f ∗ ) in probability. [sent-437, score-1.485]
</p><p>39 Using our own notation we deﬁne: Modn ( f ) = n  1  ∑ n(n − 1) ∑ fk (Xi ) fk (X j )  k=1  i= j  1 ∑ s(Xi , Xl ) ∑ s(X j , Xl ) − s(Xi, X j ) , (n − 1)2 l,l=i l,l= j  Mod( f ) = n  ∑  k=1  Z Z  fk (X) fk (Y )  Z  s(X, Z)d È(Z)  Z  s(Y, Z)d È(Z) − s(X,Y ) d(È × È)(X,Y ). [sent-449, score-2.388]
</p><p>40 n Then nearest neighbor clustering with Mod is universally weakly consistent: for all probability measures, if n → ∞ then Mod( fn ) → Mod( f ∗ ) in probability. [sent-461, score-0.787]
</p><p>41 Thus the ratio of between- and within-cluster similarity is given as K  BWR( f ) :=  cut( fk )  ∑ WS( fk ) . [sent-466, score-1.211]
</p><p>42 k=1  670  C ONSISTENT C LUSTERING WITH A RBITRARY O BJECTIVE F UNCTIONS  Again we use their empirical estimations: WSn ( fk ) :=  n 1 ∑ fk (Xi ) fk (X j )s(Xi, X j ), n(n − 1) i, j=1 K  BWRn ( f ) :=  cutn ( fk )  ∑ WSn ( fk ) . [sent-467, score-3.098]
</p><p>43 k=1  To measure the size of the cluster we use ΦBWR ( fk ) := WS( fk ) and its natural empirical counterpart. [sent-468, score-1.237]
</p><p>44 n(a − an )2 Then nearest neighbor clustering with BWR is universally weakly consistent, that is for all probability measure if n → ∞ then BWR( fn ) → BWR( f ∗ ) in probability. [sent-485, score-0.787]
</p><p>45 While it is known that spectral clustering converges to “something”, for the solutions computed by nearest neighbor clustering we know that they converge to the global minimizer of Ncut (or RatioCut, respectively). [sent-541, score-0.803]
</p><p>46 The authors do not aim for consistent clustering solutions (that is, solutions which are close to the “true clustering solution” of the underlying space ), but they want to ﬁnd algorithms to approximate the optimal clustering on a given ﬁnite sample in sublinear time. [sent-571, score-0.86]
</p><p>47 Intuitively, a clustering problem can be described by a cluster description scheme of size l ∈ Æ if each clustering can be described using l points from the space (and perhaps some additional parameter). [sent-589, score-0.575]
</p><p>48 Thus, as opposed to other clustering algorithms such as the K-means algorithm or spectral clustering, nearest neighbor clustering is guaranteed to converge to a minimizer of the true global optimum on the underlying space. [sent-624, score-0.779]
</p><p>49 (2008) we have shown how nearest neighbor clustering can be implemented efﬁciently using branch and bound, and that in terms of quality, its results can compete with algorithms of spectral clustering (for the Ncut objective function) or K-means (for the WSS objective function). [sent-639, score-0.808]
</p><p>50 We have seen that for many commonly used objective functions, statistical guarantees for nearest neighbor clustering can be obtained, and we expect the same to be true for many more clustering objective functions. [sent-641, score-0.765]
</p><p>51 The applications of clustering are just too diverse, and 50 years of clustering literature show that people will not agree on a unique deﬁnition of what a good clustering algorithm is. [sent-651, score-0.798]
</p><p>52 2 Proof of Theorem 1 Additionally to the functions fn and f ∗ , we will deﬁne ∗ fn ∈ argmin Q( f ), f ∈Fn  f ∈ argmin d( f , f ∗ ). [sent-721, score-0.76]
</p><p>53 We can study each ”side” of this convergence independently:  È(|Q( fn ) − Q( f ∗ )| ≥ ε) = È(Q( fn ) − Q( f ∗ ) ≤ −ε) + È(Q( fn ) − Q( f ∗ ) ≥ ε). [sent-723, score-1.057]
</p><p>54 To treat the “ﬁrst side” observe that if fn ∈ F then Q( fn ) − Q( f ∗ ) > 0 by the deﬁnition of f ∗ . [sent-724, score-0.698]
</p><p>55 This leads to  È(Q( fn ) − Q( f ∗ ) ≤ −ε) ≤ È( fn ∈ F ). [sent-725, score-0.698]
</p><p>56 To this end we split Q( fn ) − Q( f ∗ ) in two terms, the estimation error and the approximation error: ∗ ∗ Q( fn ) − Q( f ∗ ) = Q( fn ) − Q( fn ) + Q( fn ) − Q( f ∗ ). [sent-728, score-1.745]
</p><p>57 For a ﬁxed ε > 0 we have  ∗ ∗ È(Q( fn ) − Q( f ∗ ) ≥ ε) ≤ È(Q( fn ) − Q( fn ) ≥ ε/2) + È(Q( fn ) − Q( f ∗ ) ≥ ε/2). [sent-729, score-1.396]
</p><p>58 1 E STIMATION E RROR The ﬁrst step is to see that ∗ Q( fn ) − Q( fn ) ≤ 2 sup |Qn ( f ) − Q( f )|. [sent-733, score-0.743]
</p><p>59 f ∈Fn  ∗ Indeed, since Qn ( fn ) ≤ Qn ( fn ) by the deﬁnition of fn we have ∗ ∗ ∗ ∗ Q( fn ) − Q( fn ) = Q( fn ) − Qn ( fn ) + Qn ( fn ) − Qn ( fn ) + Qn ( fn ) − Q( fn ) ∗ ∗ ≤ Q( fn ) − Qn ( fn ) + Qn ( fn ) − Q( fn )  ≤ 2 sup |Qn ( f ) − Q( f )|. [sent-734, score-5.28]
</p><p>60 f ∈Fn  Using Lemma 9 we obtain  ∗ È(Q( fn ) − Q( fn ) ≥ ε/2) ≤ 2s(Fn , 2n)  sup È(|Qn ( f ) − Q( f )| ≥ ε/16)  f ∈Fn  inf È(|Qn ( f ) − Q( f )| ≤ ε/8)  . [sent-735, score-0.758]
</p><p>61 3 Approximation Error ∗ By deﬁnition of fn it is clear that ∗ Q( fn ) − Q( f ∗ ) ≤ Q( f ∗ ) − Q( f ∗ ). [sent-738, score-0.698]
</p><p>62 To deal with the ﬁrst term on the right hand side, observe that  È( f ∈ Fn ) ≤ K sup È(Φn ( fk ) ≤ an ). [sent-832, score-0.642]
</p><p>63 / k  Because of Condition (4), for all ε > 0, f ∈ F and g ∈ Fn there exists δ(ε) > 0 such that d( f , g) ≤ δ(ε) ⇒ Φ( fk ) − Φ(gk ) ≤ ε. [sent-833, score-0.597]
</p><p>64 Indeed, δ(an /2) and b(δ(an /2)) tend to positive constants f since f ∈ F and thus an → infk Φ( fk ) − a > 0. [sent-838, score-0.622]
</p><p>65 Before we look at the “combined” objective functions such as Ncut, RatioCut, WSS, we will prove some technical conditions about their “ingredients” cut, vol, E fk (X) and WS. [sent-868, score-0.633]
</p><p>66 fk (X), and WS) Assume that  Lemma 13 (Conditions (2), (4), and (5) for cut, vol,  m2 log n →0 n(a − an )2 then vol, cut,  fk (X) and WS satisfy Conditions (2), (4) and (5) of Theorem 2. [sent-869, score-1.21]
</p><p>67 Observe that if one replaces one variable Xi by a new one Xi′ , then voln changes by at most 2C/n, cutn changes by at most 2C/n, WS( fk ) changes by at most 2C/n, and nk ( f ) changes by at most 1/n. [sent-871, score-0.835]
</p><p>68 Moreover, since n(a − an ) n 684  C ONSISTENT C LUSTERING WITH A RBITRARY O BJECTIVE F UNCTIONS  To prove (4) for each of the objective functions, let f , g ∈ H and fk and gk be the corresponding cluster indicator functions for cluster k. [sent-876, score-0.909]
</p><p>69 To this end we want to show that for any f ∈ Fn a {| cutn ( fk ) − cut( fk )| ≤ a ε} {| voln ( fk ) − vol( fk )| ≤ 2 ε} 2  T  cut cut( ⊂ {| voln ( ffk ) − vol( ffk ) | ≤ ε}. [sent-882, score-2.906]
</p><p>70 Assume that | cutn ( fk ) − cut( fk )| ≤ ε and | voln ( fk ) − vol( fk )| ≤ ε. [sent-884, score-2.589]
</p><p>71 On the other hand, if vol( fk ) = 0 then we have cut( fk ) = 0, which implies cutn ( fk ) ≤ ε by the assumption above. [sent-886, score-1.904]
</p><p>72 Together all this leads to  È(| Ncut( f ) − Ncutn ( f )| > ε) ≤ KsupÈ(| k  cutn ( fk ) cut( fk ) − | > ε/K) voln ( fk ) vol( fk )  ≤ Ksup È(| cutn ( fk ) − cut( fk )| > k  −  ≤ 4Ke  na2 ε2 8C2 K 2  a a ε) + È(| voln ( fk ) − vol( fk )| > ε) 2K 2K  . [sent-891, score-5.178]
</p><p>73 In the proof of Lemma 13 we have already seen that | cut( fk ) − cut(gk )| ≤ 2Cd( f , g), | vol( fk ) − vol(gk )| ≤ 2Cd( f , g). [sent-896, score-1.207]
</p><p>74 On the other hand if vol(gk ) = 0 then we have | cut( fk )| ≤ | vol( fk )| ≤ 2Cd( f , g), in which case the following holds true: cut( fk ) cut(gk ) cut( fk ) 2Cd( f , g) 4C − ≤ ≤ d( f , g). [sent-898, score-2.388]
</p><p>75 = vol( fk ) vol( fk ) vol( fk ) a a So all in all we have  4CK d( f , g). [sent-899, score-1.791]
</p><p>76 Proof Using exactly the same proof as for Lemma 14 (just changing voln ( fk ) to nk and vol( fk ) to fk (X) and using the fact that cut( fk ) ≤ C fk (X)) we get  È(| RatioCutn ( f ) − RatioCut( f )| > ε)  ≤ Ksup È(| cutn ( fk ) − cut( fk )| > k  a a ε) + È(|nk ( f ) − fk (X)| > ε) . [sent-903, score-5.027]
</p><p>77 Proof This follows by the same proof as Lemma 14, just changing voln ( fk ) to nk , vol( fk ) to fk (X) and using the fact that cut( fk ) ≤ C fk (X). [sent-907, score-3.123]
</p><p>78 If | WSn ( fk ) − WS( fk )| ≤ ε and | cutn ( fk ) − cut( fk )| ≤ ε then WS( fk ) ≥ a/2 > 0 (because WSn ( fk ) > an > a since f ∈ Fn ). [sent-911, score-3.695]
</p><p>79 This implies cutn ( fk ) WSn ( fk )  cut( − WS(ffkk))  WS( fk )(cut( fk )+ε)−(WS( fk )−ε) cut( fk ) WSn ( fk ) WS( fk )  =  WS( fk )+cut( fk ) ε WSn ( fk ) WS( fk )  ≤ cut( fk ) WS( fk )  WS( fk ) cutn ( fk )−WSn ( fk ) cut( fk ) WSn ( fk ) WS( fk )  ≤  The analogous statement holds for  =  2Cε . [sent-912, score-12.166]
</p><p>80 Thus, if ε ≤ C/a then  {| WSn ( fk ) − WS( fk )| ≤ a2 ε/(2C)} ∩ {| cutn ( fk ) − cut( fk )| ≤ a2 ε/(2C)} ⊂{  cutn ( fk ) cut( fk ) ≤ ε}. [sent-914, score-3.808]
</p><p>81 − WSn ( fk ) WS( fk )  As a consequence, if ε ≤ CK/a we have cutn ( fk ) cut( fk ) | > ε/K − WSn ( fk ) WS( fk )  È(| BWRn ( f ) − BWR( f )| > ε) ≤ K sup È k  ≤ K sup È(| WSn ( fk ) − WS( fk )| > a2 ε/(2CK)) + È(| cutn ( fk ) − cut( fk )| > a2 ε/(2CK)) . [sent-915, score-6.286]
</p><p>82 We have already proved the two following inequalities (in the proofs of Lemmas 13 and 15): | cut( fk ) − cut(gk )| ≤ 2Cd( f , g), | WS( fk ) − WS(gk )| ≤ 2Cd( f , g). [sent-922, score-1.204]
</p><p>83 If 2Cd( f , g) ≤ a/2, then using that WS( fk ) > a we get WS(gk ) ≥ a/2 > 0. [sent-923, score-0.597]
</p><p>84 Thus if one changes one variable 1 Xi then the term n ∑n ∑K fk (Xi ) Xi − ck 2 will change by at most A2 /(4n). [sent-935, score-0.676]
</p><p>85 This leads to i=1 k=1  È  1 n K ∑ ∑ fk (Xi ) Xi − ck n i=1 k=1  2  K  −  ∑ fk (X)  k=1  X − ck  2  ≥ε  ≤ 2e−  2nε2 A4  . [sent-936, score-1.352]
</p><p>86 Now we have to take care of the ﬁrst term, which can be written as 1 n K ∑ ∑ fk (Xi ) n i=1 k=1  Xi − ck,n  2  2  − Xi − ck  . [sent-937, score-0.676]
</p><p>87 So at this point we have 1 n K ∑ ∑ fk (Xi ) Xi − ck,n n i=1 k=1  2  − Xi − ck  2  ≤ 6A sup ck,n − ck . [sent-939, score-0.8]
</p><p>88 Using this notation we have ck,n − ck  2  d  =  ∑  j=1  fk (X)X j 1 1 n − ∑ fk (Xi )Xij fk (X) nk n i=1  2  . [sent-942, score-1.907]
</p><p>89 Before we can do this, we want to show that n  {|nk − fk (X)| ≤  aε A+1 }  ∩ {| 1 ∑ fk (Xi )Xi − fk (X)X j | ≤ n j  i=1  n  aε j j } ⊂ {|ck − ck,n | ≤ ε}. [sent-944, score-1.807]
</p><p>90 A+1  1 To this end, assume that |nk − fk (X)| ≤ ε and | n ∑ fk (Xi )Xi − fk (X)X j | ≤ ε. [sent-945, score-1.791]
</p><p>91 On the other hand, in case fk (X) = 0 we also have fk (X)X j = 0 (as fk is a non-negative j 1 function and |X| is bounded by A). [sent-947, score-1.791]
</p><p>92 Together with the assumption this means that n ∑n fk (Xi )Xi ≤ ε. [sent-948, score-0.597]
</p><p>93 i=1 This implies ε (A + 1)ε 1 1 n j j |ck − ck,n | = ∑ fk (Xi )Xij ≤ a ≤ a nk n i=1 which shows the inclusion stated above. [sent-949, score-0.634]
</p><p>94 The McDiarmid inequality now yields the two statements  È(|nk − fk (X)| > ε) ≤ 2e−2nε , 2  È  1 n ∑ fk (Xi )Xij − fk (X)X j > ε n i=1  ≤ 2e−  2nε2 A2  . [sent-950, score-1.791]
</p><p>95 First of all, observe that | fk (X) − gk (X)| ≤ d( f , g) and fk (X)X − gk (X)X ≤ Ad( f , g). [sent-962, score-1.574]
</p><p>96 In case gk (X) = 0 we have ck ( f ) − ck (g) = ≤ ≤ ≤ ≤  gk (X) fk (X)X − fk (X) gk (X)X fk (X) gk (X) gk (X) ( fk (X)X − gk (X)X) + ( gk (X) − fk (X)) gk (X)X fk (X) gk (X) gk (X) fk (X)X − gk (X)X + A gk (X)| gk (X) − fk (X)| fk (X) gk (X) 2A d( f , g) fk (x) 2A d( f , g). [sent-963, score-8.788]
</p><p>97 a  On the other hand, in case gk (X) = 0 we also have gk (X)X = 0 (as gk is a non-negative function and |X| is bounded by A). [sent-964, score-0.57]
</p><p>98 This leads to ck ( f ) − ck (g) =  fk (X)X gk (X)X − = fk (X) gk (X)  fk (X)X A 2A ≤ d( f , g) ≤ d( f , g). [sent-965, score-2.329]
</p><p>99 fk (X) a a  Combining all results leads to | WSS( f ) − WSS(g)| ≤ 4A2 (1 + 3/a)d( f , g) which proves the lemma. [sent-966, score-0.597]
</p><p>100 Using McDiarmid inequality one can prove K 2 1 − nε fk (Xi ) fk (X j )s(Xi , X j ) − ∑ fk (X) fk (Y )s(X,Y )| ≥ ε) ≤ 2e 2C2 K2 . [sent-969, score-2.388]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fk', 0.597), ('fn', 0.349), ('qn', 0.303), ('clustering', 0.266), ('vol', 0.238), ('gk', 0.19), ('cut', 0.179), ('wss', 0.16), ('ncut', 0.142), ('bwr', 0.113), ('cutn', 0.113), ('ubeck', 0.088), ('uxburg', 0.088), ('voln', 0.088), ('bjective', 0.084), ('onsistent', 0.084), ('ratiocut', 0.084), ('rbitrary', 0.084), ('neighbor', 0.082), ('ck', 0.079), ('nearest', 0.079), ('ws', 0.072), ('nnc', 0.071), ('von', 0.07), ('voronoi', 0.067), ('lustering', 0.064), ('unctions', 0.064), ('nnm', 0.063), ('wsn', 0.063), ('wssn', 0.059), ('consistency', 0.056), ('clusterings', 0.05), ('dp', 0.046), ('sup', 0.045), ('xn', 0.045), ('spectral', 0.043), ('cluster', 0.043), ('mcdiarmid', 0.038), ('nk', 0.037), ('objective', 0.036), ('luxburg', 0.035), ('quality', 0.033), ('argmin', 0.031), ('cells', 0.031), ('bwrn', 0.029), ('ncutn', 0.029), ('shattering', 0.029), ('mod', 0.029), ('xi', 0.029), ('pollard', 0.026), ('partitions', 0.026), ('infk', 0.025), ('modularity', 0.025), ('minimizer', 0.025), ('centers', 0.024), ('seed', 0.023), ('theorem', 0.022), ('inconsistency', 0.021), ('xl', 0.021), ('lemma', 0.021), ('ratiocutn', 0.021), ('clusters', 0.02), ('subsampling', 0.019), ('mb', 0.019), ('supp', 0.019), ('global', 0.018), ('similarity', 0.017), ('bubeck', 0.017), ('ffk', 0.017), ('rakhlin', 0.017), ('want', 0.016), ('log', 0.016), ('inf', 0.015), ('condition', 0.015), ('na', 0.014), ('converges', 0.014), ('consistent', 0.014), ('devroye', 0.013), ('proof', 0.013), ('ckj', 0.013), ('ksup', 0.013), ('modn', 0.013), ('ulrike', 0.013), ('xij', 0.013), ('measurable', 0.012), ('estimator', 0.012), ('optimization', 0.012), ('seeds', 0.012), ('subsample', 0.012), ('sublinear', 0.012), ('symmetrization', 0.012), ('relaxation', 0.011), ('tends', 0.011), ('weakly', 0.011), ('linkage', 0.011), ('ke', 0.011), ('jegelka', 0.011), ('convergence', 0.01), ('proofs', 0.01), ('solutions', 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="59-tfidf-1" href="./jmlr-2009-Nearest_Neighbor_Clustering%3A_A_Baseline_Method_for_Consistent_Clustering_with_Arbitrary_Objective_Functions.html">59 jmlr-2009-Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions</a></p>
<p>Author: Sébastien Bubeck, Ulrike von Luxburg</p><p>Abstract: Clustering is often formulated as a discrete optimization problem. The objective is to ﬁnd, among all partitions of the data set, the best one according to some quality measure. However, in the statistical setting where we assume that the ﬁnite data set has been sampled from some underlying space, the goal is not to ﬁnd the best partition of the given sample, but to approximate the true partition of the underlying space. We argue that the discrete optimization approach usually does not achieve this goal, and instead can lead to inconsistency. We construct examples which provably have this behavior. As in the case of supervised learning, the cure is to restrict the size of the function classes under consideration. For appropriate “small” function classes we can prove very general consistency theorems for clustering optimization schemes. As one particular algorithm for clustering with a restricted function space we introduce “nearest neighbor clustering”. Similar to the k-nearest neighbor classiﬁer in supervised learning, this algorithm can be seen as a general baseline algorithm to minimize arbitrary clustering objective functions. We prove that it is statistically consistent for all commonly used clustering objective functions. Keywords: clustering, minimizing objective functions, consistency</p><p>2 0.11711399 <a title="59-tfidf-2" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>Author: Ulrich Paquet, Ole Winther, Manfred Opper</p><p>Abstract: Bayesian inference is intractable for many interesting models, making deterministic algorithms for approximate inference highly desirable. Unlike stochastic methods, which are exact in the limit, the accuracy of these approaches cannot be reasonably judged. In this paper we show how low order perturbation corrections to an expectation-consistent (EC) approximation can provide the necessary tools to ameliorate inference accuracy, and to give an indication of the quality of approximation without having to resort to Monte Carlo methods. Further comparisons are given with variational Bayes and parallel tempering (PT) combined with thermodynamic integration on a Gaussian mixture model. To obtain practical results we further generalize PT to temper from arbitrary distributions rather than a prior in Bayesian inference. Keywords: Bayesian inference, mixture models, expectation propagation, expectation consistent, perturbation correction, variational Bayes, parallel tempering, thermodynamic integration</p><p>3 0.10947721 <a title="59-tfidf-3" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>Author: Han Liu, John Lafferty, Larry Wasserman</p><p>Abstract: Recent methods for estimating sparse undirected graphs for real-valued data in high dimensional problems rely heavily on the assumption of normality. We show how to use a semiparametric Gaussian copula—or “nonparanormal”—for high dimensional inference. Just as additive models extend linear models by replacing linear functions with a set of one-dimensional smooth functions, the nonparanormal extends the normal by transforming the variables by smooth functions. We derive a method for estimating the nonparanormal, study the method’s theoretical properties, and show that it works well in many examples. Keywords: graphical models, Gaussian copula, high dimensional inference, sparsity, ℓ1 regularization, graphical lasso, paranormal, occult</p><p>4 0.096136995 <a title="59-tfidf-4" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Thomas Abeel, Yves Van de Peer, Yvan Saeys</p><p>Abstract: Java-ML is a collection of machine learning and data mining algorithms, which aims to be a readily usable and easily extensible API for both software developers and research scientists. The interfaces for each type of algorithm are kept simple and algorithms strictly follow their respective interface. Comparing different classiﬁers or clustering algorithms is therefore straightforward, and implementing new algorithms is also easy. The implementations of the algorithms are clearly written, properly documented and can thus be used as a reference. The library is written in Java and is available from http://java-ml.sourceforge.net/ under the GNU GPL license. Keywords: open source, machine learning, data mining, java library, clustering, feature selection, classiﬁcation</p><p>5 0.055885591 <a title="59-tfidf-5" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>Author: Haizhang Zhang, Yuesheng Xu, Jun Zhang</p><p>Abstract: We introduce the notion of reproducing kernel Banach spaces (RKBS) and study special semiinner-product RKBS by making use of semi-inner-products and the duality mapping. Properties of an RKBS and its reproducing kernel are investigated. As applications, we develop in the framework of RKBS standard learning schemes including minimal norm interpolation, regularization network, support vector machines, and kernel principal component analysis. In particular, existence, uniqueness and representer theorems are established. Keywords: reproducing kernel Banach spaces, reproducing kernels, learning theory, semi-innerproducts, representer theorems</p><p>6 0.048792496 <a title="59-tfidf-6" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>7 0.041381083 <a title="59-tfidf-7" href="./jmlr-2009-Fast_ApproximatekNN_Graph_Construction_for_High_Dimensional_Data_via_Recursive_Lanczos_Bisection.html">34 jmlr-2009-Fast ApproximatekNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</a></p>
<p>8 0.039622951 <a title="59-tfidf-8" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>9 0.037767477 <a title="59-tfidf-9" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>10 0.034754686 <a title="59-tfidf-10" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>11 0.03291681 <a title="59-tfidf-11" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>12 0.030778227 <a title="59-tfidf-12" href="./jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>13 0.030231699 <a title="59-tfidf-13" href="./jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification.html">45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</a></p>
<p>14 0.028812161 <a title="59-tfidf-14" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>15 0.028606268 <a title="59-tfidf-15" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>16 0.026662726 <a title="59-tfidf-16" href="./jmlr-2009-On_Uniform_Deviations_of_General_Empirical_Risks_with_Unboundedness%2C_Dependence%2C_and_High_Dimensionality.html">65 jmlr-2009-On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality</a></p>
<p>17 0.023788067 <a title="59-tfidf-17" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>18 0.023748726 <a title="59-tfidf-18" href="./jmlr-2009-Prediction_With_Expert_Advice_For_The_Brier_Game.html">73 jmlr-2009-Prediction With Expert Advice For The Brier Game</a></p>
<p>19 0.021429148 <a title="59-tfidf-19" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>20 0.020678084 <a title="59-tfidf-20" href="./jmlr-2009-Polynomial-Delay_Enumeration_of_Monotonic_Graph_Classes.html">72 jmlr-2009-Polynomial-Delay Enumeration of Monotonic Graph Classes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.122), (1, -0.036), (2, 0.086), (3, -0.017), (4, -0.109), (5, -0.082), (6, 0.021), (7, -0.061), (8, -0.104), (9, 0.111), (10, 0.051), (11, 0.095), (12, -0.309), (13, 0.163), (14, 0.182), (15, 0.056), (16, 0.078), (17, 0.25), (18, -0.3), (19, -0.085), (20, -0.235), (21, 0.142), (22, -0.074), (23, -0.077), (24, 0.142), (25, -0.078), (26, -0.155), (27, -0.031), (28, -0.145), (29, 0.103), (30, 0.088), (31, 0.041), (32, -0.011), (33, 0.062), (34, 0.108), (35, 0.018), (36, -0.022), (37, -0.042), (38, -0.053), (39, 0.031), (40, 0.008), (41, -0.019), (42, 0.014), (43, -0.003), (44, 0.045), (45, -0.064), (46, -0.051), (47, 0.05), (48, -0.001), (49, -0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98266602 <a title="59-lsi-1" href="./jmlr-2009-Nearest_Neighbor_Clustering%3A_A_Baseline_Method_for_Consistent_Clustering_with_Arbitrary_Objective_Functions.html">59 jmlr-2009-Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions</a></p>
<p>Author: Sébastien Bubeck, Ulrike von Luxburg</p><p>Abstract: Clustering is often formulated as a discrete optimization problem. The objective is to ﬁnd, among all partitions of the data set, the best one according to some quality measure. However, in the statistical setting where we assume that the ﬁnite data set has been sampled from some underlying space, the goal is not to ﬁnd the best partition of the given sample, but to approximate the true partition of the underlying space. We argue that the discrete optimization approach usually does not achieve this goal, and instead can lead to inconsistency. We construct examples which provably have this behavior. As in the case of supervised learning, the cure is to restrict the size of the function classes under consideration. For appropriate “small” function classes we can prove very general consistency theorems for clustering optimization schemes. As one particular algorithm for clustering with a restricted function space we introduce “nearest neighbor clustering”. Similar to the k-nearest neighbor classiﬁer in supervised learning, this algorithm can be seen as a general baseline algorithm to minimize arbitrary clustering objective functions. We prove that it is statistically consistent for all commonly used clustering objective functions. Keywords: clustering, minimizing objective functions, consistency</p><p>2 0.65849864 <a title="59-lsi-2" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>Author: Han Liu, John Lafferty, Larry Wasserman</p><p>Abstract: Recent methods for estimating sparse undirected graphs for real-valued data in high dimensional problems rely heavily on the assumption of normality. We show how to use a semiparametric Gaussian copula—or “nonparanormal”—for high dimensional inference. Just as additive models extend linear models by replacing linear functions with a set of one-dimensional smooth functions, the nonparanormal extends the normal by transforming the variables by smooth functions. We derive a method for estimating the nonparanormal, study the method’s theoretical properties, and show that it works well in many examples. Keywords: graphical models, Gaussian copula, high dimensional inference, sparsity, ℓ1 regularization, graphical lasso, paranormal, occult</p><p>3 0.43807325 <a title="59-lsi-3" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>Author: Ulrich Paquet, Ole Winther, Manfred Opper</p><p>Abstract: Bayesian inference is intractable for many interesting models, making deterministic algorithms for approximate inference highly desirable. Unlike stochastic methods, which are exact in the limit, the accuracy of these approaches cannot be reasonably judged. In this paper we show how low order perturbation corrections to an expectation-consistent (EC) approximation can provide the necessary tools to ameliorate inference accuracy, and to give an indication of the quality of approximation without having to resort to Monte Carlo methods. Further comparisons are given with variational Bayes and parallel tempering (PT) combined with thermodynamic integration on a Gaussian mixture model. To obtain practical results we further generalize PT to temper from arbitrary distributions rather than a prior in Bayesian inference. Keywords: Bayesian inference, mixture models, expectation propagation, expectation consistent, perturbation correction, variational Bayes, parallel tempering, thermodynamic integration</p><p>4 0.35212687 <a title="59-lsi-4" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Thomas Abeel, Yves Van de Peer, Yvan Saeys</p><p>Abstract: Java-ML is a collection of machine learning and data mining algorithms, which aims to be a readily usable and easily extensible API for both software developers and research scientists. The interfaces for each type of algorithm are kept simple and algorithms strictly follow their respective interface. Comparing different classiﬁers or clustering algorithms is therefore straightforward, and implementing new algorithms is also easy. The implementations of the algorithms are clearly written, properly documented and can thus be used as a reference. The library is written in Java and is available from http://java-ml.sourceforge.net/ under the GNU GPL license. Keywords: open source, machine learning, data mining, java library, clustering, feature selection, classiﬁcation</p><p>5 0.20665117 <a title="59-lsi-5" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>Author: Haizhang Zhang, Yuesheng Xu, Jun Zhang</p><p>Abstract: We introduce the notion of reproducing kernel Banach spaces (RKBS) and study special semiinner-product RKBS by making use of semi-inner-products and the duality mapping. Properties of an RKBS and its reproducing kernel are investigated. As applications, we develop in the framework of RKBS standard learning schemes including minimal norm interpolation, regularization network, support vector machines, and kernel principal component analysis. In particular, existence, uniqueness and representer theorems are established. Keywords: reproducing kernel Banach spaces, reproducing kernels, learning theory, semi-innerproducts, representer theorems</p><p>6 0.1927781 <a title="59-lsi-6" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>7 0.18439445 <a title="59-lsi-7" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>8 0.16634399 <a title="59-lsi-8" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>9 0.14303617 <a title="59-lsi-9" href="./jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification.html">45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</a></p>
<p>10 0.13580592 <a title="59-lsi-10" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>11 0.13422297 <a title="59-lsi-11" href="./jmlr-2009-Fast_ApproximatekNN_Graph_Construction_for_High_Dimensional_Data_via_Recursive_Lanczos_Bisection.html">34 jmlr-2009-Fast ApproximatekNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</a></p>
<p>12 0.10437965 <a title="59-lsi-12" href="./jmlr-2009-On_The_Power_of_Membership_Queries_in_Agnostic_Learning.html">64 jmlr-2009-On The Power of Membership Queries in Agnostic Learning</a></p>
<p>13 0.095531426 <a title="59-lsi-13" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>14 0.094495885 <a title="59-lsi-14" href="./jmlr-2009-Marginal_Likelihood_Integrals_for_Mixtures_of_Independence_Models.html">53 jmlr-2009-Marginal Likelihood Integrals for Mixtures of Independence Models</a></p>
<p>15 0.094041489 <a title="59-lsi-15" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>16 0.093619257 <a title="59-lsi-16" href="./jmlr-2009-The_P-Norm_Push%3A_A_Simple_Convex_Ranking_Algorithm_that_Concentrates_at_the_Top_of_the_List.html">95 jmlr-2009-The P-Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List</a></p>
<p>17 0.085867755 <a title="59-lsi-17" href="./jmlr-2009-Generalization_Bounds_for_Ranking_Algorithms_via_Algorithmic_Stability.html">37 jmlr-2009-Generalization Bounds for Ranking Algorithms via Algorithmic Stability</a></p>
<p>18 0.084413737 <a title="59-lsi-18" href="./jmlr-2009-Dlib-ml%3A_A_Machine_Learning_Toolkit%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">26 jmlr-2009-Dlib-ml: A Machine Learning Toolkit    (Machine Learning Open Source Software Paper)</a></p>
<p>19 0.080413155 <a title="59-lsi-19" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>20 0.077009797 <a title="59-lsi-20" href="./jmlr-2009-Prediction_With_Expert_Advice_For_The_Brier_Game.html">73 jmlr-2009-Prediction With Expert Advice For The Brier Game</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.014), (11, 0.012), (19, 0.018), (21, 0.011), (38, 0.021), (47, 0.012), (52, 0.029), (55, 0.05), (58, 0.032), (66, 0.083), (68, 0.033), (90, 0.07), (94, 0.492), (96, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70650715 <a title="59-lda-1" href="./jmlr-2009-Nearest_Neighbor_Clustering%3A_A_Baseline_Method_for_Consistent_Clustering_with_Arbitrary_Objective_Functions.html">59 jmlr-2009-Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions</a></p>
<p>Author: Sébastien Bubeck, Ulrike von Luxburg</p><p>Abstract: Clustering is often formulated as a discrete optimization problem. The objective is to ﬁnd, among all partitions of the data set, the best one according to some quality measure. However, in the statistical setting where we assume that the ﬁnite data set has been sampled from some underlying space, the goal is not to ﬁnd the best partition of the given sample, but to approximate the true partition of the underlying space. We argue that the discrete optimization approach usually does not achieve this goal, and instead can lead to inconsistency. We construct examples which provably have this behavior. As in the case of supervised learning, the cure is to restrict the size of the function classes under consideration. For appropriate “small” function classes we can prove very general consistency theorems for clustering optimization schemes. As one particular algorithm for clustering with a restricted function space we introduce “nearest neighbor clustering”. Similar to the k-nearest neighbor classiﬁer in supervised learning, this algorithm can be seen as a general baseline algorithm to minimize arbitrary clustering objective functions. We prove that it is statistically consistent for all commonly used clustering objective functions. Keywords: clustering, minimizing objective functions, consistency</p><p>2 0.64885151 <a title="59-lda-2" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>Author: Eitan Greenshtein, Junyong Park</p><p>Abstract: We consider the problem of classiﬁcation using high dimensional features’ space. In a paper by Bickel and Levina (2004), it is recommended to use naive-Bayes classiﬁers, that is, to treat the features as if they are statistically independent. Consider now a sparse setup, where only a few of the features are informative for classiﬁcation. Fan and Fan (2008), suggested a variable selection and classiﬁcation method, called FAIR. The FAIR method improves the design of naive-Bayes classiﬁers in sparse setups. The improvement is due to reducing the noise in estimating the features’ means. This reduction is since that only the means of a few selected variables should be estimated. We also consider the design of naive Bayes classiﬁers. We show that a good alternative to variable selection is estimation of the means through a certain non parametric empirical Bayes procedure. In sparse setups the empirical Bayes implicitly performs an efﬁcient variable selection. It also adapts very well to non sparse setups, and has the advantage of making use of the information from many “weakly informative” variables, which variable selection type of classiﬁcation procedures give up on using. We compare our method with FAIR and other classiﬁcation methods in simulation for sparse and non sparse setups, and in real data examples involving classiﬁcation of normal versus malignant tissues based on microarray data. Keywords: non parametric empirical Bayes, high dimension, classiﬁcation</p><p>3 0.24971905 <a title="59-lda-3" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>Author: John Duchi, Yoram Singer</p><p>Abstract: We describe, analyze, and experiment with a framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we ﬁrst perform an unconstrained gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the ﬁrst phase. This view yields a simple yet effective algorithm that can be used for batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as ℓ1 . We derive concrete and very simple algorithms for minimization of loss functions with ℓ1 , ℓ2 , ℓ2 , and ℓ∞ regularization. We also show how to construct ef2 ﬁcient algorithms for mixed-norm ℓ1 /ℓq regularization. We further extend the algorithms and give efﬁcient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in a series of experiments with synthetic and natural data sets. Keywords: subgradient methods, group sparsity, online learning, convex optimization</p><p>4 0.24334148 <a title="59-lda-4" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider regularized support vector machines (SVMs) and show that they are precisely equivalent to a new robust optimization formulation. We show that this equivalence of robust optimization and regularization has implications for both algorithms, and analysis. In terms of algorithms, the equivalence suggests more general SVM-like algorithms for classiﬁcation that explicitly build in protection to noise, and at the same time control overﬁtting. On the analysis front, the equivalence of robustness and regularization provides a robust optimization interpretation for the success of regularized SVMs. We use this new robustness interpretation of SVMs to give a new proof of consistency of (kernelized) SVMs, thus establishing robustness as the reason regularized SVMs generalize well. Keywords: robustness, regularization, generalization, kernel, support vector machine</p><p>5 0.24171537 <a title="59-lda-5" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>Author: Christian Rieger, Barbara Zwicknagl</p><p>Abstract: We introduce a new technique for the analysis of kernel-based regression problems. The basic tools are sampling inequalities which apply to all machine learning problems involving penalty terms induced by kernels related to Sobolev spaces. They lead to explicit deterministic results concerning the worst case behaviour of ε- and ν-SVRs. Using these, we show how to adjust regularization parameters to get best possible approximation orders for regression. The results are illustrated by some numerical examples. Keywords: sampling inequality, radial basis functions, approximation theory, reproducing kernel Hilbert space, Sobolev space</p><p>6 0.24143279 <a title="59-lda-6" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>7 0.24062701 <a title="59-lda-7" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>8 0.23703423 <a title="59-lda-8" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>9 0.23671748 <a title="59-lda-9" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>10 0.23459481 <a title="59-lda-10" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>11 0.2343681 <a title="59-lda-11" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>12 0.23421061 <a title="59-lda-12" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>13 0.23420951 <a title="59-lda-13" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>14 0.23336795 <a title="59-lda-14" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>15 0.2333353 <a title="59-lda-15" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>16 0.23233555 <a title="59-lda-16" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>17 0.23198444 <a title="59-lda-17" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>18 0.23115028 <a title="59-lda-18" href="./jmlr-2009-Fast_ApproximatekNN_Graph_Construction_for_High_Dimensional_Data_via_Recursive_Lanczos_Bisection.html">34 jmlr-2009-Fast ApproximatekNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</a></p>
<p>19 0.23062907 <a title="59-lda-19" href="./jmlr-2009-Settable_Systems%3A_An_Extension_of_Pearl%27s_Causal_Model_with_Optimization%2C_Equilibrium%2C_and_Learning.html">85 jmlr-2009-Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning</a></p>
<p>20 0.23061815 <a title="59-lda-20" href="./jmlr-2009-Fourier_Theoretic_Probabilistic_Inference_over_Permutations.html">36 jmlr-2009-Fourier Theoretic Probabilistic Inference over Permutations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
