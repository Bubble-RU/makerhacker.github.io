<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 jmlr-2009-CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-14" href="#">jmlr2009-14</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 jmlr-2009-CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning</h1>
<br/><p>Source: <a title="jmlr-2009-14-pdf" href="http://jmlr.org/papers/volume10/esposito09a/esposito09a.pdf">pdf</a></p><p>Author: Roberto Esposito, Daniele P. Radicioni</p><p>Abstract: The growth of information available to learning systems and the increasing complexity of learning tasks determine the need for devising algorithms that scale well with respect to all learning parameters. In the context of supervised sequential learning, the Viterbi algorithm plays a fundamental role, by allowing the evaluation of the best (most probable) sequence of labels with a time complexity linear in the number of time events, and quadratic in the number of labels. In this paper we propose CarpeDiem, a novel algorithm allowing the evaluation of the best possible sequence of labels with a sub-quadratic time complexity.1 We provide theoretical grounding together with solid empirical results supporting two chief facts. CarpeDiem always ﬁnds the optimal solution requiring, in most cases, only a small fraction of the time taken by the Viterbi algorithm; meantime, CarpeDiem is never asymptotically worse than the Viterbi algorithm, thus conﬁrming it as a sound replacement. Keywords: Viterbi algorithm, sequence labeling, conditional models, classiﬁers optimization, exact inference</p><p>Reference: <a title="jmlr-2009-14-reference" href="../jmlr2009_reference/jmlr-2009-CarpeDiem%3A_Optimizing_the_Viterbi_Algorithm_and_Applications_to_Supervised_Sequential_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A layered graph is a connected graph where vertices are partitioned into a set of “layers” such that: i) edges connect only vertices in adjacent layers; ii) any vertex in a given layer is connected to all vertices of the successive layer. [sent-63, score-0.602]
</p><p>2 We adopt the convention of indicating the layer to which a vertex belongs as a subscript to the 0 vertex name, so that yt denotes a vertex in layer t. [sent-64, score-1.258]
</p><p>3 We associate to each vertex yt a weight Syt , and 1 to each edge (yt−1 , yt ) a weight Syt ,yt−1 (Figure 1). [sent-65, score-1.216]
</p><p>4 For instance, we will use the expressions “vertical weight” of 0 yt and “vertical information” to refer to Syt and to the information provided by evidence related to vertices, respectively. [sent-67, score-0.504]
</p><p>5 The distinction between vertical and horizontal information is important in the present work, the key idea in CarpeDiem is to exploit vertical information to avoid considering the horizontal one. [sent-70, score-0.362]
</p><p>6 Given a layered and weighted graph with T layers and K vertices per layer, a path is a sequence of vertices y1 , y2 , . [sent-71, score-0.311]
</p><p>7 The reward for a path is the sum of the vertical and horizontal weights associated to the path: t−1  reward(y1 , y2 , . [sent-75, score-0.332]
</p><p>8 u=1  We deﬁne γ(yt ) as the maximal reward associated to any path from any node in layer 1 to yt : γ(yt ) =  max  y1 ,y2 ,. [sent-79, score-0.84]
</p><p>9 We consider the problem of picking the maximal path from the leftmost layer to the rightmost layer. [sent-86, score-0.245]
</p><p>10 The main idea stems from noticing that the reward of the best path to node yt can be recursively computed as: i) the reward of the best path to the predecessor π(yt ) on the optimal 1 path to yt ; ii) plus the reward for transition Syt ,π(yt ) ; iii) plus the weight of node yt . [sent-89, score-2.09]
</p><p>11 (1)  We will also make use of the equivalent formulation obtained by noticing that π(yt ) is the best predecessor for yt . [sent-91, score-0.533]
</p><p>12 That is, π(yt ) is the vertex yt−1 (in layer t − 1) that maximizes the quantity 1 γ(yt−1 ) + Syt ,yt−1 . [sent-92, score-0.304]
</p><p>13 The symbol Syt conveys information about label yt provided by observing the data at time t. [sent-105, score-0.519]
</p><p>14 0 In hidden Markov models terminology, Syt corresponds to probability byt (xt ) of observing symbol 0 xt in state yt (Rabiner, 1989, deﬁnition of b j (k) pag. [sent-106, score-0.504]
</p><p>15 More generally, Syt is a quantity that depends on both the label yt predicted for time (layer) t and the observations at and around time 1 t. [sent-109, score-0.534]
</p><p>16 1854  C A R P E D I E M : O PTIMIZING THE V ITERBI A LGORITHM  begin forall y1 do 0 G(y1 ) ← Sy1 ;  1  end for t = 2 to T do forall yt do 0 1 G(yt ) ← maxyt−1 G(yt−1 ) + Syt ,yt−1 + Syt ; end end y∗ ← arg maxyT G(yT ); T return y∗ ; T end Algorithm 1: The Viterbi algorithm. [sent-115, score-0.526]
</p><p>17 Let us assume that the reward for the maximal weight for any transition is 60. [sent-192, score-0.206]
</p><p>18 For layer 1 we have no incoming paths, the best endpoint is simply the vertex with the maximal vertical weight: in the example, i1 . [sent-194, score-0.456]
</p><p>19 In our approach, we consider vertices with highest vertical weight ﬁrst. [sent-195, score-0.222]
</p><p>20 Hence we start by calculating the reward of the optimal path to node j2 : in the example, the path i1 , j2 (with score Si01 + S12 ,i1 + S02 = 100 + 20 + 100 = 220). [sent-196, score-0.215]
</p><p>21 j j We notice that the reward attainable by reaching i2 cannot be higher than 165, computed as the sum of the reward for the best path to layer 1 (i. [sent-197, score-0.36]
</p><p>22 Therefore the endpoint of the best path to layer 2 must be j2 and it is not necessary to calculate the reward for reaching i2 . [sent-204, score-0.319]
</p><p>23 This is why CarpeDiem makes use of two procedures: one that ﬁnds the best vertex in 1857  E SPOSITO AND R ADICIONI  each layer (Algorithm 3), and one that ﬁnds the reward for reaching a given node by traversing the graph from right to left (Algorithm 4). [sent-208, score-0.395]
</p><p>24 We say that a vertex is open if the reward of its best incoming path has been computed; otherwise the vertex is said to be closed. [sent-209, score-0.444]
</p><p>25 CarpeDiem ﬁnds the best vertex for each layer by calling Algorithm 3 (also referred to as forward search strategy), which leaves closed as many vertices as possible. [sent-210, score-0.438]
</p><p>26 The backward search strategy is called to open vertices whenever necessary. [sent-211, score-0.181]
</p><p>27 (5)  Also, we say that vertex yt is more promising than vertex yt′ iff yt ⊒t yt′ . [sent-216, score-1.317]
</p><p>28 2 Algorithm 3 – Forward search strategy The forward strategy searches for the best vertex for layer t stopping as soon as this vertex can be determined unambiguously. [sent-231, score-0.525]
</p><p>29 At the beginning of the analysis of each layer all vertices in the layer are closed. [sent-232, score-0.409]
</p><p>30 As mentioned at the beginning of Section 4, in the general 1858  C A R P E D I E M : O PTIMIZING THE V ITERBI A LGORITHM  2  begin foreach y1 { Initialization Step } do 0 G(y1 ) ← Sy1 ; { Opens vertex y1 } end  3  y∗ ← arg maxy1 (G(y1 )); 1 B2 ← G(y∗ ) + S1∗ ; 1 foreach layer t ∈ 2 . [sent-234, score-0.326]
</p><p>31 T do yt∗ ← result of Algorithm 3 on layer t; end return path to y∗ ; T end Algorithm 2: CarpeDiem. [sent-237, score-0.22]
</p><p>32 The search is stopped when the difference in vertical weight of yt∗ and yt′ is big enough to counterbalance any advantage that can be possibly derived from exploiting a better transition and/or a better ancestor. [sent-240, score-0.248]
</p><p>33 yt  ≥  ∗ γt−1 − γ(π(yt∗ )  +  1 S1∗ − Syt∗ ,π(yt∗ )  . [sent-247, score-0.504]
</p><p>34 If necessary, the “maximal” vertex yt∗ (the vertex that, so far, has associated maximal reward) is updated. [sent-253, score-0.317]
</p><p>35 3 Algorithm 4 – Backward search strategy The backward search strategy opens a vertex yt by ﬁnding its best ancestor and setting G(yt ) accordingly. [sent-256, score-0.777]
</p><p>36 The ﬁrst loop ﬁnds the best predecessor among the open vertices of layer t − 1. [sent-259, score-0.325]
</p><p>37 Namely, S1t ,y∗ takes into account the transition from the current best vertex (yt−1 ) y t−1  1860  C A R P E D I E M : O PTIMIZING THE V ITERBI A LGORITHM  ′ to the target vertex yt and S1∗ accounts for the maximal reward that a transition from yt−1 to yt can possibly obtain. [sent-264, score-1.568]
</p><p>38 ′ ′ After opening (through a recursive call) yt−1 , the current best vertex is set to the best of yt−1 and ∗ yt−1 . [sent-266, score-0.187]
</p><p>39 The weight shown on the edge between labels yt−1 and yt corre1 sponds to Syt ,yt−1 . [sent-271, score-0.561]
</p><p>40 Within rounded boxes, we report: • G(yt ), if yt is open; 0 • Bt + Syt , if yt is closed and Bt has already been computed;  • 0, otherwise. [sent-274, score-1.008]
</p><p>41 The initialization steps in Algorithm 2 open all vertices in layer 1. [sent-277, score-0.287]
</p><p>42 Clearly, there is no reward for arriving at vertices in layer 0 and no incoming transitions to be taken into account. [sent-278, score-0.321]
</p><p>43 The best vertex in layer 1 is thus the vertex having the maximum vertical weight. [sent-279, score-0.548]
</p><p>44 step (b) The analysis of layer 2 starts by opening the most promising vertex in that layer (vertex j). [sent-280, score-0.52]
</p><p>45 Since all vertices at layer 1 are open, the backward strategy already has complete information at disposal, and it does not need to enter the second loop to open j2 . [sent-281, score-0.34]
</p><p>46 step (c) To open i3 , the backward strategy goes back to layer 2 and searches for the best path to that vertex. [sent-284, score-0.284]
</p><p>47 Again, vertex i2 can be left closed, since there is no chance that the best path to i3 traverses it. [sent-285, score-0.208]
</p><p>48 Unfortunately, this does not allow to make a deﬁnitive decision about whether this is the best vertex of layer 3, since B3 + S03 is (220 + 60) + 70 = 350. [sent-288, score-0.304]
</p><p>49 Next step will thereby settle the j question by opening vertex j3 . [sent-289, score-0.187]
</p><p>50 step (e) By opening i2 , the algorithm sets G(i2 ) to 125 (the best path being i1 → i2 ), thus ruling it out as a candidate for being on the optimal path to j3 . [sent-293, score-0.179]
</p><p>51 step (f) In returning to consider layer 3, we are back to the path i1 → j2 → i3 . [sent-294, score-0.22]
</p><p>52 1862  C A R P E D I E M : O PTIMIZING THE V ITERBI A LGORITHM  step (g) The ﬁrst vertex to be opened in its layer is j4 . [sent-296, score-0.341]
</p><p>53 Interestingly, the best path to j4 is not through the best vertex in layer 3. [sent-297, score-0.366]
</p><p>54 In fact, while the highest reward for a three steps walk is on vertex i3 , it is more convenient to go through vertex j3 to reach vertex j4 . [sent-298, score-0.508]
</p><p>55 Then, a single vertex per layer is opened and CarpeDiem has order of O(T + T K log(K)) = O(T K log(K)) time complexity. [sent-313, score-0.356]
</p><p>56 Then, the optimality of CarpeDiem can be proved by showing that the vertex returned by the forward strategy at the end of the algorithm is the end-point of the optimal path through the graph. [sent-318, score-0.256]
</p><p>57 When Algorithm 3 terminates on layer t, the returned vertex yt∗ is the endpoint of the optimal path to layer t. [sent-324, score-0.553]
</p><p>58 More formally, given a time point t, a boolean feature is a 1/0-valued function of the whole sequence of feature vectors x, and of a restricted neighborhood of yt . [sent-356, score-0.554]
</p><p>59 The function is meant to return 1 if the characteristics of the sequence x around time step t support the classiﬁcations given at and around yt . [sent-357, score-0.537]
</p><p>60 Under a ﬁrst order Markov assumption, each φ depends only on yt and yt−1 . [sent-358, score-0.504]
</p><p>61 The classiﬁer learnt by the voted perceptron algorithm has the form T  H(x) = arg max ∑ ∑ wφ · φ(x, yt , yt−1 ,t) y  t=1 φ  and is suitable to be evaluated using the Viterbi algorithm. [sent-360, score-0.654]
</p><p>62 In practice, not all boolean features depend on both yt and yt−1 . [sent-361, score-0.545]
</p><p>63 Let us distinguish features depending on both yt and yt−1 from those depending only on yt . [sent-362, score-1.032]
</p><p>64 We denote with Φ0 the set of features that depends only on yt and thus models per vertex (vertical) information. [sent-363, score-0.674]
</p><p>65 Analogously, we denote with Φ1 the set of features that depend on both yt and yt−1 modeling, thus, per edge (horizontal) information. [sent-364, score-0.528]
</p><p>66 The vertical and horizontal weights can be then calculated as: 0 Syt =  ∑  wφ φ(x, yt ,t)  φ∈Φ0  1864  C A R P E D I E M : O PTIMIZING THE V ITERBI A LGORITHM  and 1 Syt ,yt−1 =  ∑  wφ φ(x, yt , yt−1 ,t). [sent-365, score-1.208]
</p><p>67 φ∈Φ1  In general, the bound on the maximal transition weight S1∗ can be set to the sum of all positive horizontal weights: S1∗ = ∑ J(wφ ) φ∈Φ1  where J(x) is x if x > 0, and 0 otherwise. [sent-366, score-0.219]
</p><p>68 , all horizontal features are mutually exclusive), the maximal horizontal weight can be used. [sent-373, score-0.246]
</p><p>69 The running time of an execution of CarpeDiem depends on how the weights of vertical and horizontal features compare: the more discriminative are vertical features with respect to horizontal features, the larger is the edge CarpeDiem has over the Viterbi algorithm. [sent-379, score-0.471]
</p><p>70 They have been engineered so that they take into account the prescriptions from music harmony theory, a ﬁeld where vertical and horizontal features naturally arise. [sent-393, score-0.29]
</p><p>71 , S1∗ is not the maximal of positive horizontal weights) and where horizontal weights may change over time. [sent-398, score-0.21]
</p><p>72 Such attributes capture highly heterogeneous facets of the scanned raw image such as: horizontal and vertical position, the width and height, the mean number of edges per pixel row. [sent-423, score-0.181]
</p><p>73 To explain the observed patterns, let us consider an informative vertical feature φ• and examine the ﬁrst iteration of the voted perceptron on a sequence of length T = 100. [sent-478, score-0.207]
</p><p>74 The behavior described clearly emerges in Figure 7, where the individual weights of vertical features (for the music analysis problem) are plotted as the updates occur. [sent-503, score-0.194]
</p><p>75 Each line corresponds to an individual vertical feature; vertical lines correspond to the beginning of new iterations. [sent-505, score-0.196]
</p><p>76 This fact explains why the pattern observed in Figures 4 and 5 is not observed in Figure 6: since vertical features remain predictive with respect to horizontal ones throughout the learning process the performances of CarpeDiem do not change over time. [sent-515, score-0.222]
</p><p>77 In order to investigate this similarity let us consider the following heuristic, based on the same ideas underlying CarpeDiem: h(yt ) =  ∑  t ′ >t  0 0 max Syt ′ + S1∗ = (T − t)S1∗ + ∑ max Syt ′ yt ′  t ′ >t  yt ′  . [sent-526, score-1.008]
</p><p>78 The heuristic estimates the distance to the goal by summing the best vertical weight in each layer and the best possible transition between any two layers. [sent-527, score-0.381]
</p><p>79 Given that the time spent by a Viterbi beam search algorithm is in the order of b2 T , the time saved by a VBS algorithm w. [sent-553, score-0.215]
</p><p>80 Proof Let us consider the optimal path to yt and denote with π(yt ) the predecessor of yt . [sent-609, score-1.099]
</p><p>81 When Algorithm 3 terminates on layer t, the returned vertex yt∗ is the endpoint of the optimal path to layer t. [sent-617, score-0.553]
</p><p>82 Also, for any yt ⊑t yt′ we have (by Deﬁnition 2–Equation 5): t 0 0 0 Bt + Syt ≤ Bt + Sy′ . [sent-626, score-0.504]
</p><p>83 Using Lemma 1 we have t  yt ⊑t yt′ ⇒ G(yt∗ ) ≥ γ(yt ). [sent-628, score-0.504]
</p><p>84 1875  E SPOSITO AND R ADICIONI  Moreover, since the algorithm scans the vertices in the order given by ⊒t , all vertices yt , yt ⊒t yt′ have been considered by the main loop. [sent-633, score-1.208]
</p><p>85 Putting together the two statements, we conclude that yt ⊒t yt′ ⇒ G(yt∗ ) ≥ γ(yt ). [sent-636, score-0.504]
</p><p>86 ˆ Theorem 2 Let us assume ∀t < t : Btˆ = βtˆ, then after opening vertex yt , G(yt ) = γ(yt ). [sent-639, score-0.691]
</p><p>87 CarpeDiem starts by opening the most promising vertex in layer 2, this is the ﬁrst time Algorithm 4 is called and hence the base case of the induction. [sent-645, score-0.377]
</p><p>88 Since all vertices in layer 1 have been opened by the initialization step, the ﬁrst loop in Algorithm 4 iterates on all of them and the second loop is never entered. [sent-647, score-0.354]
</p><p>89 Let us now assume by induction that after opening a vertex yt−1 in layer t − 1 (t > 2) it holds G(yt−1 ) = γ(yt−1 ). [sent-650, score-0.345]
</p><p>90 We focus on the execution of Algorithm 4 on a vertex yt in layer t. [sent-651, score-0.835]
</p><p>91 Let us denote with Ot−1 the set of vertices presently open in layer t − 1, and with Ct−1 the set of closed ones. [sent-652, score-0.271]
</p><p>92 1876  C A R P E D I E M : O PTIMIZING THE V ITERBI A LGORITHM  Also, since all vertices for which we have taken the arg max are in layer t − 1 and open, we apply the inductive hypothesis and conclude that: ∗ yt−1 = arg max  yt−1 ∈Ot−1  1 γ(yt−1 ) + Syt ,yt−1 . [sent-654, score-0.295]
</p><p>93 t−1  (16)  ′ Since vertices are considered in ⊒t−1 order and since yt−1 is the ﬁrst vertex that has not been opened, ′ it follows that all closed vertices follow yt−1 in the ⊒t−1 order. [sent-659, score-0.332]
</p><p>94 Using these 1∗ ≥ S1 facts, along with ∀yt , yt−1 : S yt ,yt−1 (Deﬁnition 2–Equation 3) we obtain: ∗ 1 1 ∀yt−1 ∈ Ct−1 : γ(yt−1 ) + Syt ,yt−1 ≥ γ(yt−1 ) + Syt ,yt−1 ∗  Which yields: ∗ 1 γ(yt−1 ) + Syt ,yt−1 ≥ max ∗  yt−1 ∈Ct−1  1 γ(yt−1 ) + Syt ,yt−1 . [sent-663, score-0.504]
</p><p>95 Proof Let us consider the ﬁnal step of an execution of CarpeDiem, and assume that for each layer t, exactly kt vertices have been opened. [sent-668, score-0.346]
</p><p>96 In our proof we separately consider the time spent to process each layer of the graph. [sent-669, score-0.217]
</p><p>97 We deﬁne the quantity T (t) to represent the overall time spent by Algorithms 3 and 4 to process layer t. [sent-670, score-0.217]
</p><p>98 Let us deﬁne: a(yt ): the number of steps needed by Algorithm 3 to process vertex yt ; 1877  E SPOSITO AND R ADICIONI  b(yt ): the number of steps needed by Algorithm 4 to ﬁnd the best parent for node yt . [sent-671, score-1.175]
</p><p>99 In fact, since only kt vertices have been opened at the end of the algorithm, it follows that the steps needed to analyse a vertex yt by (the loop in) Algorithm 3 is at most kt . [sent-678, score-0.941]
</p><p>100 From the above discussion it follows that:  T (t) =  ∑ a(yt ) + b(yt ) yt  =  ∑  O(kt ) + O(kt−1 )  yt in open vertices  = kt · (O(kt ) + O(kt−1 )) = O(kt2 + kt kt−1 ). [sent-685, score-1.257]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('carpediem', 0.533), ('yt', 0.504), ('syt', 0.409), ('viterbi', 0.302), ('layer', 0.158), ('vertex', 0.146), ('vertical', 0.098), ('vertices', 0.093), ('vbs', 0.092), ('horizontal', 0.083), ('adicioni', 0.081), ('iterbi', 0.081), ('sposito', 0.081), ('transition', 0.08), ('bt', 0.071), ('reward', 0.07), ('kt', 0.068), ('beam', 0.064), ('path', 0.062), ('ptimizing', 0.057), ('voted', 0.055), ('faqs', 0.054), ('music', 0.053), ('sy', 0.049), ('spent', 0.044), ('lgorithm', 0.044), ('opening', 0.041), ('opened', 0.037), ('perceptron', 0.036), ('ssl', 0.032), ('harmony', 0.032), ('hmms', 0.032), ('sequential', 0.032), ('segmentation', 0.031), ('weight', 0.031), ('endpoint', 0.029), ('predecessor', 0.029), ('esposito', 0.027), ('backward', 0.027), ('felzenszwalb', 0.027), ('radicioni', 0.027), ('siddiqi', 0.027), ('execution', 0.027), ('layers', 0.026), ('mccallum', 0.026), ('labels', 0.026), ('loop', 0.025), ('saved', 0.025), ('maximal', 0.025), ('features', 0.024), ('letter', 0.024), ('search', 0.024), ('exit', 0.023), ('learnt', 0.023), ('soundness', 0.023), ('calls', 0.023), ('arg', 0.022), ('savings', 0.022), ('musical', 0.022), ('ney', 0.022), ('node', 0.021), ('speech', 0.021), ('open', 0.02), ('weights', 0.019), ('recognition', 0.019), ('layered', 0.019), ('asserted', 0.018), ('opens', 0.018), ('events', 0.018), ('sequence', 0.018), ('performances', 0.017), ('strategy', 0.017), ('promising', 0.017), ('forward', 0.017), ('boolean', 0.017), ('ot', 0.016), ('classifiers', 0.016), ('daniele', 0.016), ('grounding', 0.016), ('maxyt', 0.016), ('mozes', 0.016), ('slate', 0.016), ('tonal', 0.016), ('initialization', 0.016), ('worst', 0.015), ('exploiting', 0.015), ('time', 0.015), ('formula', 0.015), ('experimentation', 0.015), ('classi', 0.015), ('algorithm', 0.014), ('labeling', 0.014), ('frey', 0.014), ('rabiner', 0.014), ('heuristic', 0.014), ('markov', 0.014), ('roberto', 0.014), ('accounts', 0.013), ('ers', 0.013), ('collins', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="14-tfidf-1" href="./jmlr-2009-CarpeDiem%3A_Optimizing_the_Viterbi_Algorithm_and_Applications_to_Supervised_Sequential_Learning.html">14 jmlr-2009-CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning</a></p>
<p>Author: Roberto Esposito, Daniele P. Radicioni</p><p>Abstract: The growth of information available to learning systems and the increasing complexity of learning tasks determine the need for devising algorithms that scale well with respect to all learning parameters. In the context of supervised sequential learning, the Viterbi algorithm plays a fundamental role, by allowing the evaluation of the best (most probable) sequence of labels with a time complexity linear in the number of time events, and quadratic in the number of labels. In this paper we propose CarpeDiem, a novel algorithm allowing the evaluation of the best possible sequence of labels with a sub-quadratic time complexity.1 We provide theoretical grounding together with solid empirical results supporting two chief facts. CarpeDiem always ﬁnds the optimal solution requiring, in most cases, only a small fraction of the time taken by the Viterbi algorithm; meantime, CarpeDiem is never asymptotically worse than the Viterbi algorithm, thus conﬁrming it as a sound replacement. Keywords: Viterbi algorithm, sequence labeling, conditional models, classiﬁers optimization, exact inference</p><p>2 0.17232922 <a title="14-tfidf-2" href="./jmlr-2009-On_Uniform_Deviations_of_General_Empirical_Risks_with_Unboundedness%2C_Dependence%2C_and_High_Dimensionality.html">65 jmlr-2009-On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality</a></p>
<p>Author: Wenxin Jiang</p><p>Abstract: The statistical learning theory of risk minimization depends heavily on probability bounds for uniform deviations of the empirical risks. Classical probability bounds using Hoeffding’s inequality cannot accommodate more general situations with unbounded loss and dependent data. The current paper introduces an inequality that extends Hoeffding’s inequality to handle these more general situations. We will apply this inequality to provide probability bounds for uniform deviations in a very general framework, which can involve discrete decision rules, unbounded loss, and a dependence structure that can be more general than either martingale or strong mixing. We will consider two examples with high dimensional predictors: autoregression (AR) with ℓ1 -loss, and ARX model with variable selection for sign classiﬁcation, which uses both lagged responses and exogenous predictors. Keywords: dependence, empirical risk, probability bound, unbounded loss, uniform deviation</p><p>3 0.13727626 <a title="14-tfidf-3" href="./jmlr-2009-Bounded_Kernel-Based_Online_Learning.html">13 jmlr-2009-Bounded Kernel-Based Online Learning</a></p>
<p>Author: Francesco Orabona, Joseph Keshet, Barbara Caputo</p><p>Abstract: A common problem of kernel-based online algorithms, such as the kernel-based Perceptron algorithm, is the amount of memory required to store the online hypothesis, which may increase without bound as the algorithm progresses. Furthermore, the computational load of such algorithms grows linearly with the amount of memory used to store the hypothesis. To attack these problems, most previous work has focused on discarding some of the instances, in order to keep the memory bounded. In this paper we present a new algorithm, in which the instances are not discarded, but are instead projected onto the space spanned by the previous online hypothesis. We call this algorithm Projectron. While the memory size of the Projectron solution cannot be predicted before training, we prove that its solution is guaranteed to be bounded. We derive a relative mistake bound for the proposed algorithm, and deduce from it a slightly different algorithm which outperforms the Perceptron. We call this second algorithm Projectron++. We show that this algorithm can be extended to handle the multiclass and the structured output settings, resulting, as far as we know, in the ﬁrst online bounded algorithm that can learn complex classiﬁcation tasks. The method of bounding the hypothesis representation can be applied to any conservative online algorithm and to other online algorithms, as it is demonstrated for ALMA2 . Experimental results on various data sets show the empirical advantage of our technique compared to various bounded online algorithms, both in terms of memory and accuracy. Keywords: online learning, kernel methods, support vector machines, bounded support set</p><p>4 0.10629082 <a title="14-tfidf-4" href="./jmlr-2009-Identification_of_Recurrent_Neural_Networks_by_Bayesian_Interrogation_Techniques.html">40 jmlr-2009-Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques</a></p>
<p>Author: Barnabás Póczos, András Lőrincz</p><p>Abstract: We introduce novel online Bayesian methods for the identiﬁcation of a family of noisy recurrent neural networks (RNNs). We present Bayesian active learning techniques for stimulus selection given past experiences. In particular, we consider the unknown parameters as stochastic variables and use A-optimality and D-optimality principles to choose optimal stimuli. We derive myopic cost functions in order to maximize the information gain concerning network parameters at each time step. We also derive the A-optimal and D-optimal estimations of the additive noise that perturbs the dynamical system of the RNN. Here we investigate myopic as well as non-myopic estimations, and study the problem of simultaneous estimation of both the system parameters and the noise. Employing conjugate priors our derivations remain approximation-free and give rise to simple update rules for the online learning of the parameters. The efﬁciency of our method is demonstrated for a number of selected cases, including the task of controlled independent component analysis. Keywords: active learning, system identiﬁcation, online Bayesian learning, A-optimality, Doptimality, infomax control, optimal design</p><p>5 0.078026474 <a title="14-tfidf-5" href="./jmlr-2009-Exploring_Strategies_for_Training_Deep_Neural_Networks.html">33 jmlr-2009-Exploring Strategies for Training Deep Neural Networks</a></p>
<p>Author: Hugo Larochelle, Yoshua Bengio, Jérôme Louradour, Pascal Lamblin</p><p>Abstract: Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments conﬁrm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms. Keywords: artiﬁcial neural networks, deep belief networks, restricted Boltzmann machines, autoassociators, unsupervised learning</p><p>6 0.071172357 <a title="14-tfidf-6" href="./jmlr-2009-Learning_Linear_Ranking_Functions_for_Beam_Search_with_Application_to_Planning.html">47 jmlr-2009-Learning Linear Ranking Functions for Beam Search with Application to Planning</a></p>
<p>7 0.05807589 <a title="14-tfidf-7" href="./jmlr-2009-SGD-QN%3A_Careful_Quasi-Newton_Stochastic_Gradient_Descent.html">83 jmlr-2009-SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent</a></p>
<p>8 0.049613655 <a title="14-tfidf-8" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>9 0.046391938 <a title="14-tfidf-9" href="./jmlr-2009-Online_Learning_with_Samples_Drawn_from_Non-identical_Distributions.html">68 jmlr-2009-Online Learning with Samples Drawn from Non-identical Distributions</a></p>
<p>10 0.046061046 <a title="14-tfidf-10" href="./jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">79 jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>11 0.04578355 <a title="14-tfidf-11" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>12 0.041522857 <a title="14-tfidf-12" href="./jmlr-2009-Hybrid_MPI_OpenMP_Parallel_Linear_Support_Vector_Machine_Training.html">39 jmlr-2009-Hybrid MPI OpenMP Parallel Linear Support Vector Machine Training</a></p>
<p>13 0.041483905 <a title="14-tfidf-13" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>14 0.040430579 <a title="14-tfidf-14" href="./jmlr-2009-Online_Learning_with_Sample_Path_Constraints.html">67 jmlr-2009-Online Learning with Sample Path Constraints</a></p>
<p>15 0.037576649 <a title="14-tfidf-15" href="./jmlr-2009-Markov_Properties_for_Linear_Causal_Models_with_Correlated_Errors%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">54 jmlr-2009-Markov Properties for Linear Causal Models with Correlated Errors    (Special Topic on Causality)</a></p>
<p>16 0.030877762 <a title="14-tfidf-16" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>17 0.029905466 <a title="14-tfidf-17" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>18 0.029120207 <a title="14-tfidf-18" href="./jmlr-2009-Transfer_Learning_for_Reinforcement_Learning_Domains%3A_A_Survey.html">96 jmlr-2009-Transfer Learning for Reinforcement Learning Domains: A Survey</a></p>
<p>19 0.02813895 <a title="14-tfidf-19" href="./jmlr-2009-Computing_Maximum_Likelihood_Estimates_in_Recursive_Linear_Models_with_Correlated_Errors.html">17 jmlr-2009-Computing Maximum Likelihood Estimates in Recursive Linear Models with Correlated Errors</a></p>
<p>20 0.025711125 <a title="14-tfidf-20" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.184), (1, 0.127), (2, -0.158), (3, 0.129), (4, 0.025), (5, -0.152), (6, -0.026), (7, 0.025), (8, 0.091), (9, -0.028), (10, -0.086), (11, 0.043), (12, 0.073), (13, -0.078), (14, -0.169), (15, 0.041), (16, -0.152), (17, 0.106), (18, -0.019), (19, -0.156), (20, -0.185), (21, -0.007), (22, -0.149), (23, 0.183), (24, 0.125), (25, 0.113), (26, 0.173), (27, -0.144), (28, -0.101), (29, -0.15), (30, 0.026), (31, 0.013), (32, 0.096), (33, -0.024), (34, 0.101), (35, 0.13), (36, 0.031), (37, 0.031), (38, 0.052), (39, 0.16), (40, 0.01), (41, -0.025), (42, -0.201), (43, -0.019), (44, 0.073), (45, -0.083), (46, 0.013), (47, -0.047), (48, -0.027), (49, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97143835 <a title="14-lsi-1" href="./jmlr-2009-CarpeDiem%3A_Optimizing_the_Viterbi_Algorithm_and_Applications_to_Supervised_Sequential_Learning.html">14 jmlr-2009-CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning</a></p>
<p>Author: Roberto Esposito, Daniele P. Radicioni</p><p>Abstract: The growth of information available to learning systems and the increasing complexity of learning tasks determine the need for devising algorithms that scale well with respect to all learning parameters. In the context of supervised sequential learning, the Viterbi algorithm plays a fundamental role, by allowing the evaluation of the best (most probable) sequence of labels with a time complexity linear in the number of time events, and quadratic in the number of labels. In this paper we propose CarpeDiem, a novel algorithm allowing the evaluation of the best possible sequence of labels with a sub-quadratic time complexity.1 We provide theoretical grounding together with solid empirical results supporting two chief facts. CarpeDiem always ﬁnds the optimal solution requiring, in most cases, only a small fraction of the time taken by the Viterbi algorithm; meantime, CarpeDiem is never asymptotically worse than the Viterbi algorithm, thus conﬁrming it as a sound replacement. Keywords: Viterbi algorithm, sequence labeling, conditional models, classiﬁers optimization, exact inference</p><p>2 0.52810681 <a title="14-lsi-2" href="./jmlr-2009-Learning_Linear_Ranking_Functions_for_Beam_Search_with_Application_to_Planning.html">47 jmlr-2009-Learning Linear Ranking Functions for Beam Search with Application to Planning</a></p>
<p>Author: Yuehua Xu, Alan Fern, Sungwook Yoon</p><p>Abstract: Beam search is commonly used to help maintain tractability in large search spaces at the expense of completeness and optimality. Here we study supervised learning of linear ranking functions for controlling beam search. The goal is to learn ranking functions that allow for beam search to perform nearly as well as unconstrained search, and hence gain computational efﬁciency without seriously sacriﬁcing optimality. In this paper, we develop theoretical aspects of this learning problem and investigate the application of this framework to learning in the context of automated planning. We ﬁrst study the computational complexity of the learning problem, showing that even for exponentially large search spaces the general consistency problem is in NP. We also identify tractable and intractable subclasses of the learning problem, giving insight into the problem structure. Next, we analyze the convergence of recently proposed and modiﬁed online learning algorithms, where we introduce several notions of problem margin that imply convergence for the various algorithms. Finally, we present empirical results in automated planning, where ranking functions are learned to guide beam search in a number of benchmark planning domains. The results show that our approach is often able to outperform an existing state-of-the-art planning heuristic as well as a recent approach to learning such heuristics. Keywords: beam search, speedup learning, automated planning, structured classiﬁcation</p><p>3 0.40756059 <a title="14-lsi-3" href="./jmlr-2009-Bounded_Kernel-Based_Online_Learning.html">13 jmlr-2009-Bounded Kernel-Based Online Learning</a></p>
<p>Author: Francesco Orabona, Joseph Keshet, Barbara Caputo</p><p>Abstract: A common problem of kernel-based online algorithms, such as the kernel-based Perceptron algorithm, is the amount of memory required to store the online hypothesis, which may increase without bound as the algorithm progresses. Furthermore, the computational load of such algorithms grows linearly with the amount of memory used to store the hypothesis. To attack these problems, most previous work has focused on discarding some of the instances, in order to keep the memory bounded. In this paper we present a new algorithm, in which the instances are not discarded, but are instead projected onto the space spanned by the previous online hypothesis. We call this algorithm Projectron. While the memory size of the Projectron solution cannot be predicted before training, we prove that its solution is guaranteed to be bounded. We derive a relative mistake bound for the proposed algorithm, and deduce from it a slightly different algorithm which outperforms the Perceptron. We call this second algorithm Projectron++. We show that this algorithm can be extended to handle the multiclass and the structured output settings, resulting, as far as we know, in the ﬁrst online bounded algorithm that can learn complex classiﬁcation tasks. The method of bounding the hypothesis representation can be applied to any conservative online algorithm and to other online algorithms, as it is demonstrated for ALMA2 . Experimental results on various data sets show the empirical advantage of our technique compared to various bounded online algorithms, both in terms of memory and accuracy. Keywords: online learning, kernel methods, support vector machines, bounded support set</p><p>4 0.39899608 <a title="14-lsi-4" href="./jmlr-2009-On_Uniform_Deviations_of_General_Empirical_Risks_with_Unboundedness%2C_Dependence%2C_and_High_Dimensionality.html">65 jmlr-2009-On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality</a></p>
<p>Author: Wenxin Jiang</p><p>Abstract: The statistical learning theory of risk minimization depends heavily on probability bounds for uniform deviations of the empirical risks. Classical probability bounds using Hoeffding’s inequality cannot accommodate more general situations with unbounded loss and dependent data. The current paper introduces an inequality that extends Hoeffding’s inequality to handle these more general situations. We will apply this inequality to provide probability bounds for uniform deviations in a very general framework, which can involve discrete decision rules, unbounded loss, and a dependence structure that can be more general than either martingale or strong mixing. We will consider two examples with high dimensional predictors: autoregression (AR) with ℓ1 -loss, and ARX model with variable selection for sign classiﬁcation, which uses both lagged responses and exogenous predictors. Keywords: dependence, empirical risk, probability bound, unbounded loss, uniform deviation</p><p>5 0.37941185 <a title="14-lsi-5" href="./jmlr-2009-Exploring_Strategies_for_Training_Deep_Neural_Networks.html">33 jmlr-2009-Exploring Strategies for Training Deep Neural Networks</a></p>
<p>Author: Hugo Larochelle, Yoshua Bengio, Jérôme Louradour, Pascal Lamblin</p><p>Abstract: Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments conﬁrm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms. Keywords: artiﬁcial neural networks, deep belief networks, restricted Boltzmann machines, autoassociators, unsupervised learning</p><p>6 0.36816221 <a title="14-lsi-6" href="./jmlr-2009-Identification_of_Recurrent_Neural_Networks_by_Bayesian_Interrogation_Techniques.html">40 jmlr-2009-Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques</a></p>
<p>7 0.20620556 <a title="14-lsi-7" href="./jmlr-2009-Markov_Properties_for_Linear_Causal_Models_with_Correlated_Errors%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">54 jmlr-2009-Markov Properties for Linear Causal Models with Correlated Errors    (Special Topic on Causality)</a></p>
<p>8 0.19744344 <a title="14-lsi-8" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>9 0.17907019 <a title="14-lsi-9" href="./jmlr-2009-Computing_Maximum_Likelihood_Estimates_in_Recursive_Linear_Models_with_Correlated_Errors.html">17 jmlr-2009-Computing Maximum Likelihood Estimates in Recursive Linear Models with Correlated Errors</a></p>
<p>10 0.1612204 <a title="14-lsi-10" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>11 0.15642986 <a title="14-lsi-11" href="./jmlr-2009-SGD-QN%3A_Careful_Quasi-Newton_Stochastic_Gradient_Descent.html">83 jmlr-2009-SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent</a></p>
<p>12 0.15387511 <a title="14-lsi-12" href="./jmlr-2009-Hybrid_MPI_OpenMP_Parallel_Linear_Support_Vector_Machine_Training.html">39 jmlr-2009-Hybrid MPI OpenMP Parallel Linear Support Vector Machine Training</a></p>
<p>13 0.15095881 <a title="14-lsi-13" href="./jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">79 jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>14 0.14179726 <a title="14-lsi-14" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>15 0.11990599 <a title="14-lsi-15" href="./jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">70 jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<p>16 0.1169157 <a title="14-lsi-16" href="./jmlr-2009-Online_Learning_with_Samples_Drawn_from_Non-identical_Distributions.html">68 jmlr-2009-Online Learning with Samples Drawn from Non-identical Distributions</a></p>
<p>17 0.11375086 <a title="14-lsi-17" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>18 0.11074362 <a title="14-lsi-18" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>19 0.1058059 <a title="14-lsi-19" href="./jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification.html">45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</a></p>
<p>20 0.10509627 <a title="14-lsi-20" href="./jmlr-2009-An_Algorithm_for_Reading_Dependencies_from_the_Minimal_Undirected_Independence_Map_of_a_Graphoid_that_Satisfies_Weak_Transitivity.html">6 jmlr-2009-An Algorithm for Reading Dependencies from the Minimal Undirected Independence Map of a Graphoid that Satisfies Weak Transitivity</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.016), (11, 0.033), (19, 0.011), (26, 0.018), (38, 0.038), (47, 0.016), (52, 0.043), (55, 0.028), (58, 0.019), (66, 0.574), (68, 0.015), (90, 0.056), (96, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99835116 <a title="14-lda-1" href="./jmlr-2009-Polynomial-Delay_Enumeration_of_Monotonic_Graph_Classes.html">72 jmlr-2009-Polynomial-Delay Enumeration of Monotonic Graph Classes</a></p>
<p>Author: Jan Ramon, Siegfried Nijssen</p><p>Abstract: Algorithms that list graphs such that no two listed graphs are isomorphic, are important building blocks of systems for mining and learning in graphs. Algorithms are already known that solve this problem efﬁciently for many classes of graphs of restricted topology, such as trees. In this article we introduce the concept of a dense augmentation schema, and introduce an algorithm that can be used to enumerate any class of graphs with polynomial delay, as long as the class of graphs can be described using a monotonic predicate operating on a dense augmentation schema. In practice this means that this is the ﬁrst enumeration algorithm that can be applied theoretically efﬁciently in any frequent subgraph mining algorithm, and that this algorithm generalizes to situations beyond the standard frequent subgraph mining setting. Keywords: graph mining, enumeration, monotonic graph classes</p><p>2 0.99754816 <a title="14-lda-2" href="./jmlr-2009-Learning_Permutations_with_Exponential_Weights.html">49 jmlr-2009-Learning Permutations with Exponential Weights</a></p>
<p>Author: David P. Helmbold, Manfred K. Warmuth</p><p>Abstract: We give an algorithm for the on-line learning of permutations. The algorithm maintains its uncertainty about the target permutation as a doubly stochastic weight matrix, and makes predictions using an efﬁcient method for decomposing the weight matrix into a convex combination of permutations. The weight matrix is updated by multiplying the current matrix entries by exponential factors, and an iterative procedure is needed to restore double stochasticity. Even though the result of this procedure does not have a closed form, a new analysis approach allows us to prove an optimal (up to small constant factors) bound on the regret of our algorithm. This regret bound is signiﬁcantly better than that of either Kalai and Vempala’s more efﬁcient Follow the Perturbed Leader algorithm or the computationally expensive method of explicitly representing each permutation as an expert. Keywords: permutation, ranking, on-line learning, Hedge algorithm, doubly stochastic matrix, relative entropy projection, Sinkhorn balancing</p><p>3 0.99070996 <a title="14-lda-3" href="./jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">16 jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<p>Author: Dao-Hong Xiang, Ding-Xuan Zhou</p><p>Abstract: This paper considers binary classiﬁcation algorithms generated from Tikhonov regularization schemes associated with general convex loss functions and varying Gaussian kernels. Our main goal is to provide fast convergence rates for the excess misclassiﬁcation error. Allowing varying Gaussian kernels in the algorithms improves learning rates measured by regularization error and sample error. Special structures of Gaussian kernels enable us to construct, by a nice approximation scheme with a Fourier analysis technique, uniformly bounded regularizing functions achieving polynomial decays of the regularization error under a Sobolev smoothness condition. The sample error is estimated by using a projection operator and a tight bound for the covering numbers of reproducing kernel Hilbert spaces generated by Gaussian kernels. The convexity of the general loss function plays a very important role in our analysis. Keywords: reproducing kernel Hilbert space, binary classiﬁcation, general convex loss, varying Gaussian kernels, covering number, approximation</p><p>same-paper 4 0.98756498 <a title="14-lda-4" href="./jmlr-2009-CarpeDiem%3A_Optimizing_the_Viterbi_Algorithm_and_Applications_to_Supervised_Sequential_Learning.html">14 jmlr-2009-CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning</a></p>
<p>Author: Roberto Esposito, Daniele P. Radicioni</p><p>Abstract: The growth of information available to learning systems and the increasing complexity of learning tasks determine the need for devising algorithms that scale well with respect to all learning parameters. In the context of supervised sequential learning, the Viterbi algorithm plays a fundamental role, by allowing the evaluation of the best (most probable) sequence of labels with a time complexity linear in the number of time events, and quadratic in the number of labels. In this paper we propose CarpeDiem, a novel algorithm allowing the evaluation of the best possible sequence of labels with a sub-quadratic time complexity.1 We provide theoretical grounding together with solid empirical results supporting two chief facts. CarpeDiem always ﬁnds the optimal solution requiring, in most cases, only a small fraction of the time taken by the Viterbi algorithm; meantime, CarpeDiem is never asymptotically worse than the Viterbi algorithm, thus conﬁrming it as a sound replacement. Keywords: Viterbi algorithm, sequence labeling, conditional models, classiﬁers optimization, exact inference</p><p>5 0.98493183 <a title="14-lda-5" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>Author: Jun Zhu, Eric P. Xing</p><p>Abstract: The standard maximum margin approach for structured prediction lacks a straightforward probabilistic interpretation of the learning scheme and the prediction rule. Therefore its unique advantages such as dual sparseness and kernel tricks cannot be easily conjoined with the merits of a probabilistic model such as Bayesian regularization, model averaging, and ability to model hidden variables. In this paper, we present a new general framework called maximum entropy discrimination Markov networks (MaxEnDNet, or simply, MEDN), which integrates these two approaches and combines and extends their merits. Major innovations of this approach include: 1) It extends the conventional max-entropy discrimination learning of classiﬁcation rules to a new structural maxentropy discrimination paradigm of learning a distribution of Markov networks. 2) It generalizes the extant Markov network structured-prediction rule based on a point estimator of model coefﬁcients to an averaging model akin to a Bayesian predictor that integrates over a learned posterior distribution of model coefﬁcients. 3) It admits ﬂexible entropic regularization of the model during learning. By plugging in different prior distributions of the model coefﬁcients, it subsumes the wellknown maximum margin Markov networks (M3 N) as a special case, and leads to a model similar to an L1 -regularized M3 N that is simultaneously primal and dual sparse, or other new types of Markov networks. 4) It applies a modular learning algorithm that combines existing variational inference techniques and convex-optimization based M3 N solvers as subroutines. Essentially, MEDN can be understood as a jointly maximum likelihood and maximum margin estimate of Markov network. It represents the ﬁrst successful attempt to combine maximum entropy learning (a dual form of maximum likelihood learning) with maximum margin learning of Markov network for structured input/output problems; and the basic principle can be generalized to learning arbi</p><p>6 0.86542189 <a title="14-lda-6" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>7 0.85551304 <a title="14-lda-7" href="./jmlr-2009-Prediction_With_Expert_Advice_For_The_Brier_Game.html">73 jmlr-2009-Prediction With Expert Advice For The Brier Game</a></p>
<p>8 0.85042584 <a title="14-lda-8" href="./jmlr-2009-Online_Learning_with_Samples_Drawn_from_Non-identical_Distributions.html">68 jmlr-2009-Online Learning with Samples Drawn from Non-identical Distributions</a></p>
<p>9 0.84854132 <a title="14-lda-9" href="./jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">79 jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>10 0.84812963 <a title="14-lda-10" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>11 0.84224617 <a title="14-lda-11" href="./jmlr-2009-Generalization_Bounds_for_Ranking_Algorithms_via_Algorithmic_Stability.html">37 jmlr-2009-Generalization Bounds for Ranking Algorithms via Algorithmic Stability</a></p>
<p>12 0.83863056 <a title="14-lda-12" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>13 0.83196414 <a title="14-lda-13" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>14 0.83104843 <a title="14-lda-14" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>15 0.82801282 <a title="14-lda-15" href="./jmlr-2009-Bi-Level_Path_Following_for_Cross_Validated_Solution_of_Kernel_Quantile_Regression.html">12 jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</a></p>
<p>16 0.82133496 <a title="14-lda-16" href="./jmlr-2009-Learning_Halfspaces_with_Malicious_Noise.html">46 jmlr-2009-Learning Halfspaces with Malicious Noise</a></p>
<p>17 0.81849313 <a title="14-lda-17" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>18 0.81675404 <a title="14-lda-18" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>19 0.81191778 <a title="14-lda-19" href="./jmlr-2009-Fourier_Theoretic_Probabilistic_Inference_over_Permutations.html">36 jmlr-2009-Fourier Theoretic Probabilistic Inference over Permutations</a></p>
<p>20 0.81071448 <a title="14-lda-20" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
