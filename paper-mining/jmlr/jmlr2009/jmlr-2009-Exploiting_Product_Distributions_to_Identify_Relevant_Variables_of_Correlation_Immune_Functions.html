<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-32" href="#">jmlr2009-32</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</h1>
<br/><p>Source: <a title="jmlr-2009-32-pdf" href="http://jmlr.org/papers/volume10/hellerstein09a/hellerstein09a.pdf">pdf</a></p><p>Author: Lisa Hellerstein, Bernard Rosell, Eric Bach, Soumya Ray, David Page</p><p>Abstract: A Boolean function f is correlation immune if each input variable is independent of the output, under the uniform distribution on inputs. For example, the parity function is correlation immune. We consider the problem of identifying relevant variables of a correlation immune function, in the presence of irrelevant variables. We address this problem in two different contexts. First, we analyze Skewing, a heuristic method that was developed to improve the ability of greedy decision tree algorithms to identify relevant variables of correlation immune Boolean functions, given examples drawn from the uniform distribution (Page and Ray, 2003). We present theoretical results revealing both the capabilities and limitations of skewing. Second, we explore the problem of identifying relevant variables in the Product Distribution Choice (PDC) learning model, a model in which the learner can choose product distributions and obtain examples from them. We prove a lemma establishing a property of Boolean functions that may be of independent interest. Using this lemma, we give two new algorithms for ﬁnding relevant variables of correlation immune functions in the PDC model. Keywords: correlation immune functions, skewing, relevant variables, Boolean functions, product distributions c 2009 Lisa Hellerstein, Bernard Rosell, Eric Bach, Soumya Ray and David Page. H ELLERSTEIN , ROSELL , BACH , R AY AND PAGE</p><p>Reference: <a title="jmlr-2009-32-reference" href="../jmlr2009_reference/jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We consider the problem of identifying relevant variables of a correlation immune function, in the presence of irrelevant variables. [sent-13, score-0.708]
</p><p>2 First, we analyze Skewing, a heuristic method that was developed to improve the ability of greedy decision tree algorithms to identify relevant variables of correlation immune Boolean functions, given examples drawn from the uniform distribution (Page and Ray, 2003). [sent-15, score-0.773]
</p><p>3 Using this lemma, we give two new algorithms for ﬁnding relevant variables of correlation immune functions in the PDC model. [sent-19, score-0.648]
</p><p>4 Keywords: correlation immune functions, skewing, relevant variables, Boolean functions, product distributions c 2009 Lisa Hellerstein, Bernard Rosell, Eric Bach, Soumya Ray and David Page. [sent-20, score-0.678]
</p><p>5 Introduction A Boolean function f : {0, 1}n → {0, 1} is correlation immune if for every input variable xi , the values of xi and f (x1 , . [sent-22, score-0.771]
</p><p>6 Examples of correlation immune functions include parity of k ≥ 2 variables, the constant functions f ≡ 1 and f ≡ 0, and the function f (x) = 1 iff all bits of x are equal. [sent-27, score-0.698]
</p><p>7 We consider the problem of identifying relevant variables of a correlation immune function, in the presence of irrelevant variables. [sent-30, score-0.708]
</p><p>8 1 Greedy decision tree learning algorithms perform poorly on correlation immune functions because they rely on measures such as Information Gain (Quinlan, 1997) and Gini gain (Breiman et al. [sent-36, score-0.632]
</p><p>9 The correlation immune functions are precisely those in which every attribute has zero gain under all standard gain measures, when the gain is computed on the complete data set (i. [sent-38, score-0.874]
</p><p>10 Thus when examples of a correlation immune function are drawn uniformly at random from the complete data set, the learning algorithms have no basis for distinguishing between relevant and irrelevant attributes. [sent-41, score-0.686]
</p><p>11 Experiments have shown skewing to be successful in learning many correlation immune functions (Page and Ray, 2003). [sent-42, score-1.143]
</p><p>12 One of the original motivations behind skewing was the observation that obtaining examples from non-uniform product distributions can be helpful in learning particular correlation immune functions such as parity. [sent-43, score-1.247]
</p><p>13 Our PDC model algorithms could be presented independently of any discussion of the skewing heuristic. [sent-53, score-0.656]
</p><p>14 Summary of Results We begin by showing that, given a complete data set, skewing will succeed. [sent-66, score-0.656]
</p><p>15 That is, given the complete truth table of a target Boolean function as the training set, skewing will ﬁnd a relevant variable of that function. [sent-67, score-0.902]
</p><p>16 (More particularly, under any random choice of skewing parameters, a single round of the skewing procedure will ﬁnd a relevant variable with probability 1. [sent-68, score-1.506]
</p><p>17 ) This result establishes that the approach taken by skewing is fundamentally sound. [sent-69, score-0.656]
</p><p>18 However, it says nothing about the effectiveness of skewing when, as is typically the case, the training set contains only a small fraction of the examples in the truth table. [sent-70, score-0.726]
</p><p>19 In particular, this result does not address the question of whether skewing would be effective given only a polynomial-size sample and polynomial time. [sent-71, score-0.721]
</p><p>20 We also analyze a variant of skewing called sequential skewing (Ray and Page, 2004), in the case that the full truth table is given as input. [sent-72, score-1.39]
</p><p>21 Experiments indicate that sequential skewing scales better to higher dimensional problems than standard skewing. [sent-73, score-0.68]
</p><p>22 We show here, however, that even when the entire truth table is available as the training set, sequential skewing is ineffective for a subset of the correlation immune functions known as the 2-correlation immune functions. [sent-74, score-1.615]
</p><p>23 A Boolean function f : {0, 1}n → {0, 1} is 2-correlation immune if, for every pair of distinct input variables xi and x j , the variables xi , x j , and f (x1 , . [sent-75, score-0.72]
</p><p>24 Thus, any practical advantage sequential skewing has over standard skewing comes at the cost of not working on this subset of functions. [sent-79, score-1.336]
</p><p>25 Since p-biased distributions are skewed distributions, our algorithms can be viewed as skewing algorithms for a setting in which it is possible to sample directly from skewed distributions, rather than to just simulate those distributions. [sent-93, score-0.843]
</p><p>26 We also examine skewing in the context for which it was originally designed: learning from a random sample drawn from the uniform distribution. [sent-94, score-0.728]
</p><p>27 Technically, we prove the bound for a variant of skewing, called skewing with independent samples, that is more amenable to analysis than standard skewing. [sent-96, score-0.68]
</p><p>28 The bound implies that skewing with independent samples requires a sample of size at least nΩ(log n) to ﬁnd (with constant probability of failure) a relevant variable of an n-variable Boolean function computing the parity of log n of its variables. [sent-98, score-1.092]
</p><p>29 Our analysis of skewing given a complete data set, and our two new algorithms in the PDC model, are both based on a lemma that we prove which shows that Boolean functions have a certain useful property. [sent-103, score-0.714]
</p><p>30 Speciﬁcally, we show that every non-constant Boolean function f on {0, 1}n has a variable xi such that induced functions fxi ←0 and fxi ←1 on {0, 1}n−1 (produced by hardwiring xi to 0 and 1) do not have the same number of positive examples of Hamming weight k, for some k. [sent-104, score-0.619]
</p><p>31 Organization of the Paper We ﬁrst give some background on skewing in Section 4. [sent-107, score-0.656]
</p><p>32 We begin our analysis of skewing in Section 9 with results for the setting in which the entire truth table is given as the training set. [sent-113, score-0.71]
</p><p>33 Finally, Section 11 contains our sample complexity lower bounds on learning parity functions using skewing with independent samples. [sent-116, score-0.852]
</p><p>34 The approach taken by skewing is to reweight the training data, to simulate receiving examples from another distribution. [sent-130, score-0.698]
</p><p>35 More particularly, the skewing algorithm works by choosing a “preferred setting” (either 0 or 1) for every variable xi in the examples, 1 and a weighting factor p where 2 < p < 1. [sent-131, score-0.835]
</p><p>36 To simulate receiving examples from this product distribution, the skewing algorithm begins by initializing the weight of every example in the training set to 1. [sent-133, score-0.771]
</p><p>37 The exact method used varies in different skewing implementations. [sent-139, score-0.656]
</p><p>38 In the context of decision tree learning, skewing is applied at every node of the decision tree, in place of standard gain calculations. [sent-141, score-0.801]
</p><p>39 After running skewing on the training set at that node, the variable chosen by the skewing procedure is used as the split variable at that node. [sent-142, score-1.428]
</p><p>40 Although skewing with independent samples is not an SQ-based algorithm, we prove a bound that is similar to the weaker of these two bounds, using a similar technique. [sent-193, score-0.68]
</p><p>41 ) The proof of Jackson’s stronger bound relies on properties of SQ-based algorithms that are not shared by skewing with independent samples, and it is an open question whether a similar bound is achievable for skewing with independent samples. [sent-195, score-1.36]
</p><p>42 While the gain estimates performed in skewing seem to correspond to honest statistical queries, the correspondence is not direct. [sent-197, score-0.813]
</p><p>43 Correlation immune functions and k-correlation immune functions have applications to secure communication, and have been widely studied in that ﬁeld (see Roy, 2002, for a survey). [sent-245, score-0.82]
</p><p>44 Gini gain was also used in the decision tree learners employed in experimental work on skewing (Page and Ray, 2003; Ray and Page, 2004). [sent-400, score-0.801]
</p><p>45 The Gini gain of xi with respect to f , under the uniform distribution on {0, 1}n , is equal to the Gini gain of xi with respect to the training set T consisting of all entries in the truth table of f . [sent-422, score-0.656]
</p><p>46 Given a skew (σ, p) and a function f , the Gini gain of a variable xi with respect to f under distribution D(σ,p) is equivalent to the gain that is calculated, using the procedure described in Section 4, by applying skew (σ, p) to the training set T consisting of the entire truth table for f. [sent-423, score-0.652]
</p><p>47 Then GD ( f , xi ), the Gini gain of variable xi with respect to f , under distribution D, is equal to 2pi (1 − pi )(PrD [ f = 1|xi = 1] − PrD [ f = 1|xi = 0])2 where pi = PrD [xi = 1]. [sent-426, score-0.624]
</p><p>48 A greedy decision tree learner would have difﬁculty learning k-correlation immune functions using only k-lookahead; to ﬁnd relevant variables in the presence of irrelevant ones for such functions, it would need to use k + 1-lookahead. [sent-494, score-0.67]
</p><p>49 Equivalently, a Boolean function f is correlation immune if all variables of f have zero gain for f , with respect to the uniform distribution on {0, 1}n . [sent-496, score-0.726]
</p><p>50 Skewing Given the Entire Truth Table In this section, we analyze skewing in an idealized setting, where the available data consists of the full truth table of a Boolean function. [sent-707, score-0.71]
</p><p>51 We then do an analysis of sequential skewing in the same setting. [sent-708, score-0.68]
</p><p>52 We are interested in the following question: When skewing is applied to a correlation immune function, will it cause a relevant variable to have non-zero gain under the skewed distribution? [sent-714, score-1.484]
</p><p>53 When we use a skew (σ, p) to reweight a data set that consists of an entire truth table, the weight assigned to each assignment a in the truth table by the skewing procedure is PD(σ,p) (a), where D(σ, p) is the skewed distribution deﬁned by (σ, p). [sent-718, score-0.943]
</p><p>54 1 suggests that skewing is an effective method for ﬁnding relevant variables of a nonconstant Boolean f , because for nearly all skews, there will be at least one variable with non-zero 2396  E XPLOITING P RODUCT D ISTRIBUTIONS  gain. [sent-864, score-0.893]
</p><p>55 3 Analysis of Sequential Skewing Sequential skewing is a variant of skewing. [sent-871, score-0.656]
</p><p>56 Speciﬁcally, there are 1058 2-correlation immune functions of 5 variables, but only 128 parity 2397  H ELLERSTEIN , ROSELL , BACH , R AY AND PAGE  functions and complements of these (with no constraint on the relevant variables). [sent-901, score-0.704]
</p><p>57 ) Denisov (1992) gave an asymptotic formula for the number of 2-correlation immune functions, and from this work it follows that for large n, only a small fraction of the 2-correlation immune functions will be parity functions. [sent-904, score-0.963]
</p><p>58 The following theorem shows that, in our idealized setting, sequential skewing can identify a relevant variable of a function, unless that function is 2-correlation immune. [sent-905, score-0.857]
</p><p>59 It follows that sequential skewing will be ineffective in ﬁnding relevant variables of a parity function, even with unlimited sample sizes. [sent-906, score-1.021]
</p><p>60 In contrast, standard skewing can identify relevant variables of a parity function if the sample size is large enough. [sent-907, score-0.997]
</p><p>61 In particular, skewing can magnify idiosyncracies in the sample in a way that does not occur when sampling from true alternative distributions. [sent-957, score-0.677]
</p><p>62 On the Limitations of Skewing One of the motivating problems for skewing was that of learning the parity of r of n variables. [sent-1153, score-0.815]
</p><p>63 The results of Section 9 imply that skewing is effective for learning parity functions if the entire truth table is available as the training set. [sent-1154, score-0.885]
</p><p>64 We now consider the following sample complexity question: how large a random sample is needed so that skewing can be used to identify a relevant variable of the parity function, with “high” probability? [sent-1159, score-1.034]
</p><p>65 It is difﬁcult to analyze the behavior of skewing because the same sample is used and re-used for many gain calculations. [sent-1163, score-0.806]
</p><p>66 Here we consider a modiﬁcation of the standard skewing procedure, in which we pick a new, independent random sample each time we estimate the gain of a variable with respect to a skew (σ, p). [sent-1165, score-0.936]
</p><p>67 ” Intuitively, since the motivation behind 2403  H ELLERSTEIN , ROSELL , BACH , R AY AND PAGE  skewing is based on estimating statistical quantities, choosing a new sample to make each estimate should not hurt accuracy. [sent-1167, score-0.677]
</p><p>68 In experiments, skewing with independent samples was more effective in ﬁnding relevant variables than standard skewing (Ray et al. [sent-1168, score-1.473]
</p><p>69 For simplicity, assume that the variable output by the skewing algorithm is one that exceeds a ﬁxed threshold the maximum number of times. [sent-1170, score-0.714]
</p><p>70 We prove a sample complexity lower bound for skewing with independent samples, when applied to a target function that is the parity of r of n variables. [sent-1172, score-0.875]
</p><p>71 The proof is based on the fact that the skewing algorithm does not use all the information in the examples. [sent-1173, score-0.656]
</p><p>72 Given a skew (σ, p), and an example (x, f (x)), the skewing algorithm weights this example according to d = ∆(x, σ), the Hamming distance between x and σ. [sent-1174, score-0.71]
</p><p>73 The skewing algorithm uses only these summaries; it does not use any other information about the examples. [sent-1177, score-0.656]
</p><p>74 We deﬁne S1 (b, c, d) = Pr[NEQ(σi , xi ) = b, f (x) = f ,σ c, and ∆(x, σ) = d] when xi is a relevant variable of f , and S2 (b, c, d) = Pr[NEQ(σi , xi ) = b, f (x) = c, and ∆(x, σ) = d] when xi is an irrelevant variable of f . [sent-1196, score-0.779]
</p><p>75 ) We now prove a sample complexity lower bound for learning parity functions, using skewing with independent samples. [sent-1250, score-0.86]
</p><p>76 1 Suppose we use skewing with independent samples to identify a relevant variable of f , where f ∈ Parityr,n . [sent-1252, score-0.833]
</p><p>77 Consider running skewing with independent samples with a target function f ∈ Parityr,n . [sent-1255, score-0.671]
</p><p>78 To estimate the gain of a variable xi with respect to a skew (σ, p), the skewing algorithm uses a sample drawn from the uniform distribution. [sent-1256, score-1.108]
</p><p>79 We may therefore assume that the skewing algorithm is, in fact, given only the summary tuples, rather than the raw examples. [sent-1259, score-0.678]
</p><p>80 r−1 r Let m be the total number of examples used to estimate the gain of all variables xi under all skews (σ, p) used by the skewing algorithm. [sent-1269, score-0.964]
</p><p>81 By the symmetry of the parity function, if the target function f is randomly chosen from Parityr,n , then with probability at least (1 − q)m , the ﬁnal variable output by the skewing algorithm when run on this f is equally likely to be any of the n input variables of f . [sent-1271, score-0.965]
</p><p>82 Thus the probability that the skewing algorithm outputs an irrelevant variable is at least (1−q)m ( n−r ), and the probability that n r r r it outputs a relevant variable is at most 1 − (1 − q)m ( n−r ) < 1 − (1 − qm)(1 − n ) < n + qm(1 − n ) < n r m n + qm. [sent-1272, score-1.003]
</p><p>83 It follows that if skewing with independent samples outputs a relevant variable of f (for any f ∈ Parityr,n ) with probability at least µ, then the total number of examples used must be at least 1/2 1/2 r −1/2 −1/2 min{(n−1) ,(n−1) } µ− n r−1 r . [sent-1275, score-0.902]
</p><p>84 n It follows from the theorem that for skewing to output a relevant variable with success “noticeably” 1 greater than random guessing, that is, with probability at least logn + p(n) , for some polynomial p, it n would need to use more than a superpolynomial number of examples. [sent-1279, score-0.912]
</p><p>85 The above proof relies crucially on the fact that skewing uses only the information in the summary tuples. [sent-1280, score-0.678]
</p><p>86 Thus the lower bound applies not only to the implementation of skewing that we assumed (in which the chosen variable is the one whose gain exceeds the ﬁxed threshold the maximum number of times). [sent-1282, score-0.867]
</p><p>87 Assuming independent samples, the lower bound would also apply to other skewing implementations, including, for example, an implementation in which the variable with highest gain over all skews was chosen as the output variable. [sent-1283, score-0.867]
</p><p>88 On the other hand, one can also imagine variants of skewing to which the proof would not apply. [sent-1284, score-0.656]
</p><p>89 For example, suppose that we replaced the single parameter p used in skewing by a vector of parameters [p1 , . [sent-1285, score-0.656]
</p><p>90 To put it another way, the proof exploits the fact that the distributions used by skewing are simple ones, deﬁned by a pair (σ, p). [sent-1290, score-0.694]
</p><p>91 The negative result above depends on the fact that for f a parity function with r relevant variables, the distribution of the summary tuples for a relevant variable xi is very close to the distribution of the summary tuples for an irrelevant variable xi . [sent-1292, score-0.983]
</p><p>92 For other correlation immune functions, the distributions are further apart, making those functions easier for skewing to handle. [sent-1293, score-1.181]
</p><p>93 We provided a theoretical study of skewing, an approach to learning correlation immune functions (through ﬁnding relevant variables) that has been shown empirically to be quite successful. [sent-1311, score-0.606]
</p><p>94 On the positive side, we showed that when the skewing algorithm has access to the complete truth table of a target Boolean function—a case in which standard greedy gain-based learners fail—skewing will succeed in ﬁnding a relevant variable of that function. [sent-1312, score-0.925]
</p><p>95 More particularly, under any random choice of skewing parameters, a single round of the skewing procedure will ﬁnd a relevant variable with probability 1. [sent-1313, score-1.506]
</p><p>96 In some sense the correlation immune functions are the hardest Boolean functions to learn, and parity functions are among the hardest of these to learn, since a parity function of k + 1 variables is k-correlation immune. [sent-1314, score-0.879]
</p><p>97 In contrast to the positive result above, we showed (using methods from statistical query learning) that skewing needs a sample size that is superpolynomial in n to learn parity of log n relevant variables, given examples from the uniform distribution. [sent-1315, score-1.059]
</p><p>98 We leave as an open question the characterization of the functions of log n variables that skewing can learn using a sample of size polynomial in n, given examples from the uniform distribution. [sent-1316, score-0.846]
</p><p>99 Generalized skewing for functions with continuous and nominal variables. [sent-1573, score-0.672]
</p><p>100 Why skewing works: Learning difﬁcult functions with greedy tree learners. [sent-1588, score-0.711]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('skewing', 0.656), ('immune', 0.394), ('pdc', 0.178), ('boolean', 0.171), ('prd', 0.165), ('parity', 0.159), ('fourier', 0.143), ('gini', 0.132), ('gain', 0.129), ('rosell', 0.128), ('xi', 0.121), ('relevant', 0.119), ('fxi', 0.118), ('ellerstein', 0.1), ('xploiting', 0.1), ('ay', 0.085), ('gd', 0.083), ('pr', 0.08), ('correlation', 0.077), ('istributions', 0.076), ('roduct', 0.076), ('bach', 0.074), ('ray', 0.074), ('pi', 0.071), ('arpe', 0.067), ('hamming', 0.066), ('ln', 0.061), ('hellerstein', 0.061), ('mossel', 0.061), ('irrelevant', 0.06), ('variable', 0.058), ('neq', 0.056), ('skew', 0.054), ('truth', 0.054), ('skewed', 0.051), ('jackson', 0.05), ('product', 0.05), ('page', 0.049), ('bshouty', 0.047), ('coef', 0.045), ('polynomial', 0.044), ('variables', 0.042), ('lemma', 0.042), ('membership', 0.041), ('furst', 0.038), ('distributions', 0.038), ('pd', 0.037), ('query', 0.037), ('iff', 0.036), ('identically', 0.036), ('queries', 0.035), ('distribution', 0.035), ('feldman', 0.034), ('fxl', 0.033), ('blum', 0.032), ('hi', 0.032), ('uniform', 0.031), ('orientation', 0.031), ('honest', 0.028), ('pn', 0.028), ('hardwiring', 0.028), ('reischuk', 0.028), ('cients', 0.028), ('tuples', 0.027), ('reweighting', 0.027), ('wk', 0.026), ('simulate', 0.026), ('hd', 0.025), ('xn', 0.025), ('sequential', 0.024), ('bound', 0.024), ('assignments', 0.024), ('weight', 0.023), ('greedy', 0.023), ('guijarro', 0.022), ('immunity', 0.022), ('summary', 0.022), ('sample', 0.021), ('martingale', 0.021), ('prime', 0.02), ('drawn', 0.02), ('log', 0.02), ('preferred', 0.019), ('let', 0.019), ('least', 0.018), ('respect', 0.018), ('wisconsin', 0.018), ('ai', 0.018), ('probability', 0.017), ('madison', 0.017), ('lisa', 0.017), ('damaschke', 0.017), ('lantz', 0.017), ('identifying', 0.016), ('tree', 0.016), ('assignment', 0.016), ('examples', 0.016), ('functions', 0.016), ('degree', 0.015), ('target', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="32-tfidf-1" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>Author: Lisa Hellerstein, Bernard Rosell, Eric Bach, Soumya Ray, David Page</p><p>Abstract: A Boolean function f is correlation immune if each input variable is independent of the output, under the uniform distribution on inputs. For example, the parity function is correlation immune. We consider the problem of identifying relevant variables of a correlation immune function, in the presence of irrelevant variables. We address this problem in two different contexts. First, we analyze Skewing, a heuristic method that was developed to improve the ability of greedy decision tree algorithms to identify relevant variables of correlation immune Boolean functions, given examples drawn from the uniform distribution (Page and Ray, 2003). We present theoretical results revealing both the capabilities and limitations of skewing. Second, we explore the problem of identifying relevant variables in the Product Distribution Choice (PDC) learning model, a model in which the learner can choose product distributions and obtain examples from them. We prove a lemma establishing a property of Boolean functions that may be of independent interest. Using this lemma, we give two new algorithms for ﬁnding relevant variables of correlation immune functions in the PDC model. Keywords: correlation immune functions, skewing, relevant variables, Boolean functions, product distributions c 2009 Lisa Hellerstein, Bernard Rosell, Eric Bach, Soumya Ray and David Page. H ELLERSTEIN , ROSELL , BACH , R AY AND PAGE</p><p>2 0.090730563 <a title="32-tfidf-2" href="./jmlr-2009-On_The_Power_of_Membership_Queries_in_Agnostic_Learning.html">64 jmlr-2009-On The Power of Membership Queries in Agnostic Learning</a></p>
<p>Author: Vitaly Feldman</p><p>Abstract: We study the properties of the agnostic learning framework of Haussler (1992) and Kearns, Schapire, and Sellie (1994). In particular, we address the question: is there any situation in which membership queries are useful in agnostic learning? Our results show that the answer is negative for distribution-independent agnostic learning and positive for agnostic learning with respect to a speciﬁc marginal distribution. Namely, we give a simple proof that any concept class learnable agnostically by a distribution-independent algorithm with access to membership queries is also learnable agnostically without membership queries. This resolves an open problem posed by Kearns et al. (1994). For agnostic learning with respect to the uniform distribution over {0, 1}n we show a concept class that is learnable with membership queries but computationally hard to learn from random examples alone (assuming that one-way functions exist). Keywords: agnostic learning, membership query, separation, PAC learning</p><p>3 0.090514623 <a title="32-tfidf-3" href="./jmlr-2009-Fourier_Theoretic_Probabilistic_Inference_over_Permutations.html">36 jmlr-2009-Fourier Theoretic Probabilistic Inference over Permutations</a></p>
<p>Author: Jonathan Huang, Carlos Guestrin, Leonidas Guibas</p><p>Abstract: Permutations are ubiquitous in many real-world problems, such as voting, ranking, and data association. Representing uncertainty over permutations is challenging, since there are n! possibilities, and typical compact and factorized probability distribution representations, such as graphical models, cannot capture the mutual exclusivity constraints associated with permutations. In this paper, we use the “low-frequency” terms of a Fourier decomposition to represent distributions over permutations compactly. We present Kronecker conditioning, a novel approach for maintaining and updating these distributions directly in the Fourier domain, allowing for polynomial time bandlimited approximations. Low order Fourier-based approximations, however, may lead to functions that do not correspond to valid distributions. To address this problem, we present a quadratic program deﬁned directly in the Fourier domain for projecting the approximation onto a relaxation of the polytope of legal marginal distributions. We demonstrate the effectiveness of our approach on a real camera-based multi-person tracking scenario. Keywords: identity management, permutations, approximate inference, group theoretical methods, sensor networks</p><p>4 0.039623979 <a title="32-tfidf-4" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>Author: Eugene Tuv, Alexander Borisov, George Runger, Kari Torkkola</p><p>Abstract: Predictive models beneﬁt from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for ﬁlters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach. Keywords: trees, resampling, importance, masking, residuals</p><p>5 0.038654152 <a title="32-tfidf-5" href="./jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>Author: Leonid (Aryeh) Kontorovich, Boaz Nadler</p><p>Abstract: We propose a novel framework for supervised learning of discrete concepts. Since the 1970’s, the standard computational primitive has been to ﬁnd the most consistent hypothesis in a given complexity class. In contrast, in this paper we propose a new basic operation: for each pair of input instances, count how many concepts of bounded complexity contain both of them. Our approach maps instances to a Hilbert space, whose metric is induced by a universal kernel coinciding with our computational primitive, and identiﬁes concepts with half-spaces. We prove that all concepts are linearly separable under this mapping. Hence, given a labeled sample and an oracle for evaluating the universal kernel, we can efﬁciently compute a linear classiﬁer (via SVM, for example) and use margin bounds to control its generalization error. Even though exact evaluation of the universal kernel may be infeasible, in various natural situations it is efﬁciently approximable. Though our approach is general, our main application is to regular languages. Our approach presents a substantial departure from current learning paradigms and in particular yields a novel method for learning this fundamental concept class. Unlike existing techniques, we make no structural assumptions on the corresponding unknown automata, the string distribution or the completeness of the training set. Instead, given a labeled sample our algorithm outputs a classiﬁer with guaranteed distribution-free generalization bounds; to our knowledge, the proposed framework is the only one capable of achieving the latter. Along the way, we touch upon several fundamental questions in complexity, automata, and machine learning. Keywords: grammar induction, regular language, ﬁnite state automaton, maximum margin hyperplane, kernel approximation</p><p>6 0.031458788 <a title="32-tfidf-6" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>7 0.030194685 <a title="32-tfidf-7" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>8 0.030156719 <a title="32-tfidf-8" href="./jmlr-2009-Learning_Halfspaces_with_Malicious_Noise.html">46 jmlr-2009-Learning Halfspaces with Malicious Noise</a></p>
<p>9 0.029719448 <a title="32-tfidf-9" href="./jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>10 0.02952043 <a title="32-tfidf-10" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>11 0.027871326 <a title="32-tfidf-11" href="./jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">79 jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>12 0.026471343 <a title="32-tfidf-12" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>13 0.024596246 <a title="32-tfidf-13" href="./jmlr-2009-Learning_Permutations_with_Exponential_Weights.html">49 jmlr-2009-Learning Permutations with Exponential Weights</a></p>
<p>14 0.023694448 <a title="32-tfidf-14" href="./jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification.html">45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</a></p>
<p>15 0.023047645 <a title="32-tfidf-15" href="./jmlr-2009-Learning_Acyclic_Probabilistic_Circuits_Using_Test_Paths.html">44 jmlr-2009-Learning Acyclic Probabilistic Circuits Using Test Paths</a></p>
<p>16 0.02229186 <a title="32-tfidf-16" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>17 0.022258166 <a title="32-tfidf-17" href="./jmlr-2009-Data-driven_Calibration_of_Penalties_for_Least-Squares_Regression.html">21 jmlr-2009-Data-driven Calibration of Penalties for Least-Squares Regression</a></p>
<p>18 0.022174397 <a title="32-tfidf-18" href="./jmlr-2009-The_P-Norm_Push%3A_A_Simple_Convex_Ranking_Algorithm_that_Concentrates_at_the_Top_of_the_List.html">95 jmlr-2009-The P-Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List</a></p>
<p>19 0.02173353 <a title="32-tfidf-19" href="./jmlr-2009-Subgroup_Analysis_via_Recursive_Partitioning.html">91 jmlr-2009-Subgroup Analysis via Recursive Partitioning</a></p>
<p>20 0.021347325 <a title="32-tfidf-20" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.126), (1, -0.048), (2, 0.048), (3, -0.003), (4, -0.071), (5, 0.025), (6, -0.074), (7, 0.054), (8, -0.033), (9, 0.032), (10, 0.114), (11, -0.097), (12, -0.048), (13, 0.142), (14, -0.311), (15, -0.108), (16, -0.029), (17, -0.129), (18, -0.024), (19, 0.109), (20, -0.057), (21, -0.111), (22, -0.074), (23, 0.202), (24, 0.185), (25, -0.239), (26, -0.137), (27, 0.069), (28, -0.074), (29, -0.044), (30, 0.033), (31, -0.072), (32, 0.037), (33, 0.109), (34, -0.085), (35, 0.08), (36, 0.003), (37, 0.069), (38, -0.155), (39, -0.097), (40, -0.209), (41, 0.047), (42, 0.109), (43, -0.092), (44, 0.158), (45, 0.19), (46, -0.12), (47, 0.034), (48, -0.028), (49, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94074434 <a title="32-lsi-1" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>Author: Lisa Hellerstein, Bernard Rosell, Eric Bach, Soumya Ray, David Page</p><p>Abstract: A Boolean function f is correlation immune if each input variable is independent of the output, under the uniform distribution on inputs. For example, the parity function is correlation immune. We consider the problem of identifying relevant variables of a correlation immune function, in the presence of irrelevant variables. We address this problem in two different contexts. First, we analyze Skewing, a heuristic method that was developed to improve the ability of greedy decision tree algorithms to identify relevant variables of correlation immune Boolean functions, given examples drawn from the uniform distribution (Page and Ray, 2003). We present theoretical results revealing both the capabilities and limitations of skewing. Second, we explore the problem of identifying relevant variables in the Product Distribution Choice (PDC) learning model, a model in which the learner can choose product distributions and obtain examples from them. We prove a lemma establishing a property of Boolean functions that may be of independent interest. Using this lemma, we give two new algorithms for ﬁnding relevant variables of correlation immune functions in the PDC model. Keywords: correlation immune functions, skewing, relevant variables, Boolean functions, product distributions c 2009 Lisa Hellerstein, Bernard Rosell, Eric Bach, Soumya Ray and David Page. H ELLERSTEIN , ROSELL , BACH , R AY AND PAGE</p><p>2 0.59487516 <a title="32-lsi-2" href="./jmlr-2009-On_The_Power_of_Membership_Queries_in_Agnostic_Learning.html">64 jmlr-2009-On The Power of Membership Queries in Agnostic Learning</a></p>
<p>Author: Vitaly Feldman</p><p>Abstract: We study the properties of the agnostic learning framework of Haussler (1992) and Kearns, Schapire, and Sellie (1994). In particular, we address the question: is there any situation in which membership queries are useful in agnostic learning? Our results show that the answer is negative for distribution-independent agnostic learning and positive for agnostic learning with respect to a speciﬁc marginal distribution. Namely, we give a simple proof that any concept class learnable agnostically by a distribution-independent algorithm with access to membership queries is also learnable agnostically without membership queries. This resolves an open problem posed by Kearns et al. (1994). For agnostic learning with respect to the uniform distribution over {0, 1}n we show a concept class that is learnable with membership queries but computationally hard to learn from random examples alone (assuming that one-way functions exist). Keywords: agnostic learning, membership query, separation, PAC learning</p><p>3 0.49010399 <a title="32-lsi-3" href="./jmlr-2009-Fourier_Theoretic_Probabilistic_Inference_over_Permutations.html">36 jmlr-2009-Fourier Theoretic Probabilistic Inference over Permutations</a></p>
<p>Author: Jonathan Huang, Carlos Guestrin, Leonidas Guibas</p><p>Abstract: Permutations are ubiquitous in many real-world problems, such as voting, ranking, and data association. Representing uncertainty over permutations is challenging, since there are n! possibilities, and typical compact and factorized probability distribution representations, such as graphical models, cannot capture the mutual exclusivity constraints associated with permutations. In this paper, we use the “low-frequency” terms of a Fourier decomposition to represent distributions over permutations compactly. We present Kronecker conditioning, a novel approach for maintaining and updating these distributions directly in the Fourier domain, allowing for polynomial time bandlimited approximations. Low order Fourier-based approximations, however, may lead to functions that do not correspond to valid distributions. To address this problem, we present a quadratic program deﬁned directly in the Fourier domain for projecting the approximation onto a relaxation of the polytope of legal marginal distributions. We demonstrate the effectiveness of our approach on a real camera-based multi-person tracking scenario. Keywords: identity management, permutations, approximate inference, group theoretical methods, sensor networks</p><p>4 0.31895661 <a title="32-lsi-4" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>Author: Eugene Tuv, Alexander Borisov, George Runger, Kari Torkkola</p><p>Abstract: Predictive models beneﬁt from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for ﬁlters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach. Keywords: trees, resampling, importance, masking, residuals</p><p>5 0.22931457 <a title="32-lsi-5" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>Author: Tong Zhang</p><p>Abstract: This paper studies the feature selection problem using a greedy least squares regression algorithm. We show that under a certain irrepresentable condition on the design matrix (but independent of the sparse target), the greedy algorithm can select features consistently when the sample size approaches inﬁnity. The condition is identical to a corresponding condition for Lasso. Moreover, under a sparse eigenvalue condition, the greedy algorithm can reliably identify features as long as each nonzero coefﬁcient is larger than a constant times the noise level. In compar√ ison, Lasso may require the coefﬁcients to be larger than O( s) times the noise level in the worst case, where s is the number of nonzero coefﬁcients. Keywords: greedy algorithm, feature selection, sparsity</p><p>6 0.19698551 <a title="32-lsi-6" href="./jmlr-2009-Generalization_Bounds_for_Ranking_Algorithms_via_Algorithmic_Stability.html">37 jmlr-2009-Generalization Bounds for Ranking Algorithms via Algorithmic Stability</a></p>
<p>7 0.19670503 <a title="32-lsi-7" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>8 0.16753796 <a title="32-lsi-8" href="./jmlr-2009-An_Algorithm_for_Reading_Dependencies_from_the_Minimal_Undirected_Independence_Map_of_a_Graphoid_that_Satisfies_Weak_Transitivity.html">6 jmlr-2009-An Algorithm for Reading Dependencies from the Minimal Undirected Independence Map of a Graphoid that Satisfies Weak Transitivity</a></p>
<p>9 0.16272412 <a title="32-lsi-9" href="./jmlr-2009-Incorporating_Functional_Knowledge_in_Neural_Networks.html">42 jmlr-2009-Incorporating Functional Knowledge in Neural Networks</a></p>
<p>10 0.15391409 <a title="32-lsi-10" href="./jmlr-2009-Data-driven_Calibration_of_Penalties_for_Least-Squares_Regression.html">21 jmlr-2009-Data-driven Calibration of Penalties for Least-Squares Regression</a></p>
<p>11 0.1515726 <a title="32-lsi-11" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>12 0.14826882 <a title="32-lsi-12" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>13 0.1462339 <a title="32-lsi-13" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>14 0.14442725 <a title="32-lsi-14" href="./jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">79 jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>15 0.14339134 <a title="32-lsi-15" href="./jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification.html">45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</a></p>
<p>16 0.14286338 <a title="32-lsi-16" href="./jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>17 0.14262357 <a title="32-lsi-17" href="./jmlr-2009-Learning_Acyclic_Probabilistic_Circuits_Using_Test_Paths.html">44 jmlr-2009-Learning Acyclic Probabilistic Circuits Using Test Paths</a></p>
<p>18 0.13634203 <a title="32-lsi-18" href="./jmlr-2009-The_P-Norm_Push%3A_A_Simple_Convex_Ranking_Algorithm_that_Concentrates_at_the_Top_of_the_List.html">95 jmlr-2009-The P-Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List</a></p>
<p>19 0.13149746 <a title="32-lsi-19" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>20 0.12694182 <a title="32-lsi-20" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.017), (11, 0.024), (19, 0.03), (26, 0.032), (38, 0.03), (39, 0.415), (47, 0.018), (52, 0.046), (55, 0.044), (58, 0.029), (66, 0.101), (90, 0.066), (93, 0.023), (96, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.71179181 <a title="32-lda-1" href="./jmlr-2009-Learning_Halfspaces_with_Malicious_Noise.html">46 jmlr-2009-Learning Halfspaces with Malicious Noise</a></p>
<p>Author: Adam R. Klivans, Philip M. Long, Rocco A. Servedio</p><p>Abstract: We give new algorithms for learning halfspaces in the challenging malicious noise model, where an adversary may corrupt both the labels and the underlying distribution of examples. Our algorithms can tolerate malicious noise rates exponentially larger than previous work in terms of the dependence on the dimension n, and succeed for the fairly broad class of all isotropic log-concave distributions. We give poly(n, 1/ε)-time algorithms for solving the following problems to accuracy ε: • Learning origin-centered halfspaces in Rn with respect to the uniform distribution on the unit ball with malicious noise rate η = Ω(ε2 / log(n/ε)). (The best previous result was Ω(ε/(n log(n/ε))1/4 ).) • Learning origin-centered halfspaces with respect to any isotropic logconcave distribution on Rn with malicious noise rate η = Ω(ε3 / log2 (n/ε)). This is the ﬁrst efﬁcient algorithm for learning under isotropic log-concave distributions in the presence of malicious noise. We also give a poly(n, 1/ε)-time algorithm for learning origin-centered halfspaces under any isotropic log-concave distribution on Rn in the presence of adversarial label noise at rate η = Ω(ε3 / log(1/ε)). In the adversarial label noise setting (or agnostic model), labels can be noisy, but not example points themselves. Previous results could handle η = Ω(ε) but had running time exponential in an unspeciﬁed function of 1/ε. Our analysis crucially exploits both concentration and anti-concentration properties of isotropic log-concave distributions. Our algorithms combine an iterative outlier removal procedure using Principal Component Analysis together with “smooth” boosting. Keywords: PAC learning, noise tolerance, malicious noise, agnostic learning, label noise, halfspace learning, linear classiﬁers c 2009 Adam R. Klivans, Philip M. Long and Rocco A. Servedio. K LIVANS , L ONG AND S ERVEDIO</p><p>same-paper 2 0.61452633 <a title="32-lda-2" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>Author: Lisa Hellerstein, Bernard Rosell, Eric Bach, Soumya Ray, David Page</p><p>Abstract: A Boolean function f is correlation immune if each input variable is independent of the output, under the uniform distribution on inputs. For example, the parity function is correlation immune. We consider the problem of identifying relevant variables of a correlation immune function, in the presence of irrelevant variables. We address this problem in two different contexts. First, we analyze Skewing, a heuristic method that was developed to improve the ability of greedy decision tree algorithms to identify relevant variables of correlation immune Boolean functions, given examples drawn from the uniform distribution (Page and Ray, 2003). We present theoretical results revealing both the capabilities and limitations of skewing. Second, we explore the problem of identifying relevant variables in the Product Distribution Choice (PDC) learning model, a model in which the learner can choose product distributions and obtain examples from them. We prove a lemma establishing a property of Boolean functions that may be of independent interest. Using this lemma, we give two new algorithms for ﬁnding relevant variables of correlation immune functions in the PDC model. Keywords: correlation immune functions, skewing, relevant variables, Boolean functions, product distributions c 2009 Lisa Hellerstein, Bernard Rosell, Eric Bach, Soumya Ray and David Page. H ELLERSTEIN , ROSELL , BACH , R AY AND PAGE</p><p>3 0.35513055 <a title="32-lda-3" href="./jmlr-2009-On_The_Power_of_Membership_Queries_in_Agnostic_Learning.html">64 jmlr-2009-On The Power of Membership Queries in Agnostic Learning</a></p>
<p>Author: Vitaly Feldman</p><p>Abstract: We study the properties of the agnostic learning framework of Haussler (1992) and Kearns, Schapire, and Sellie (1994). In particular, we address the question: is there any situation in which membership queries are useful in agnostic learning? Our results show that the answer is negative for distribution-independent agnostic learning and positive for agnostic learning with respect to a speciﬁc marginal distribution. Namely, we give a simple proof that any concept class learnable agnostically by a distribution-independent algorithm with access to membership queries is also learnable agnostically without membership queries. This resolves an open problem posed by Kearns et al. (1994). For agnostic learning with respect to the uniform distribution over {0, 1}n we show a concept class that is learnable with membership queries but computationally hard to learn from random examples alone (assuming that one-way functions exist). Keywords: agnostic learning, membership query, separation, PAC learning</p><p>4 0.32145485 <a title="32-lda-4" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider regularized support vector machines (SVMs) and show that they are precisely equivalent to a new robust optimization formulation. We show that this equivalence of robust optimization and regularization has implications for both algorithms, and analysis. In terms of algorithms, the equivalence suggests more general SVM-like algorithms for classiﬁcation that explicitly build in protection to noise, and at the same time control overﬁtting. On the analysis front, the equivalence of robustness and regularization provides a robust optimization interpretation for the success of regularized SVMs. We use this new robustness interpretation of SVMs to give a new proof of consistency of (kernelized) SVMs, thus establishing robustness as the reason regularized SVMs generalize well. Keywords: robustness, regularization, generalization, kernel, support vector machine</p><p>5 0.31873426 <a title="32-lda-5" href="./jmlr-2009-A_Least-squares_Approach_to_Direct_Importance_Estimation.html">1 jmlr-2009-A Least-squares Approach to Direct Importance Estimation</a></p>
<p>Author: Takafumi Kanamori, Shohei Hido, Masashi Sugiyama</p><p>Abstract: We address the problem of estimating the ratio of two probability density functions, which is often referred to as the importance. The importance values can be used for various succeeding tasks such as covariate shift adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally highly efﬁcient and simple to implement. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bounds. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efﬁcient than competing approaches. Keywords: importance sampling, covariate shift adaptation, novelty detection, regularization path, leave-one-out cross validation</p><p>6 0.31528884 <a title="32-lda-6" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>7 0.31515148 <a title="32-lda-7" href="./jmlr-2009-Learning_Acyclic_Probabilistic_Circuits_Using_Test_Paths.html">44 jmlr-2009-Learning Acyclic Probabilistic Circuits Using Test Paths</a></p>
<p>8 0.31373808 <a title="32-lda-8" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>9 0.3133837 <a title="32-lda-9" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>10 0.31310925 <a title="32-lda-10" href="./jmlr-2009-Settable_Systems%3A_An_Extension_of_Pearl%27s_Causal_Model_with_Optimization%2C_Equilibrium%2C_and_Learning.html">85 jmlr-2009-Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning</a></p>
<p>11 0.31281784 <a title="32-lda-11" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>12 0.3114616 <a title="32-lda-12" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>13 0.31136152 <a title="32-lda-13" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>14 0.31081182 <a title="32-lda-14" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>15 0.30971602 <a title="32-lda-15" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>16 0.30947384 <a title="32-lda-16" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>17 0.30914849 <a title="32-lda-17" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>18 0.30906236 <a title="32-lda-18" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>19 0.30890897 <a title="32-lda-19" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>20 0.30766252 <a title="32-lda-20" href="./jmlr-2009-Generalization_Bounds_for_Ranking_Algorithms_via_Algorithmic_Stability.html">37 jmlr-2009-Generalization Bounds for Ranking Algorithms via Algorithmic Stability</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
