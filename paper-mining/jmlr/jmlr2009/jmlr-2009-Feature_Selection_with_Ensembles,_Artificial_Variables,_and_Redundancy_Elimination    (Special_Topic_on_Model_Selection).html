<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-35" href="#">jmlr2009-35</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</h1>
<br/><p>Source: <a title="jmlr-2009-35-pdf" href="http://jmlr.org/papers/volume10/tuv09a/tuv09a.pdf">pdf</a></p><p>Author: Eugene Tuv, Alexander Borisov, George Runger, Kari Torkkola</p><p>Abstract: Predictive models beneﬁt from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for ﬁlters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach. Keywords: trees, resampling, importance, masking, residuals</p><p>Reference: <a title="jmlr-2009-35-reference" href="../jmlr2009_reference/jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. [sent-14, score-0.883]
</p><p>2 2 Subset Feature Selection Fundamentally, the goal of feature selection is to model a target response (or output) variable y, with a subset of the (important) predictor variables (inputs). [sent-37, score-0.423]
</p><p>3 An important concept here is the masking relationships among the predictor variables. [sent-43, score-0.46]
</p><p>4 3 Contributions of this Paper Existing tree ensembles such as random forest (Breiman, 2001) or gradient boosting trees (Friedman, 1999) were developed primarily for predictive modeling. [sent-47, score-0.445]
</p><p>5 A variable masking measure 1342  F EATURE S ELECTION WITH E NSEMBLES  is then introduced that incorporates surrogate variable scores from ensembles of trees. [sent-52, score-0.944]
</p><p>6 Section 3 describes variable importance measures deﬁned through tree ensembles and explains how they could be used to remove irrelevant features using random, artiﬁcial features. [sent-58, score-0.662]
</p><p>7 Next, we introduce a masking measure and use it for redundancy elimination. [sent-59, score-0.673]
</p><p>8 Usually feature redundancy is deﬁned in terms of feature correlation (Hall, 2000). [sent-74, score-0.375]
</p><p>9 In reality, it is not so straightforward to determine feature redundancy if a feature is partially correlated to a set of features. [sent-76, score-0.375]
</p><p>10 In practice, most algorithms just try to remove irrelevant features and then apply some heuristics that remove “possibly” redundant variables. [sent-81, score-0.281]
</p><p>11 Filter methods are also useless for the minimal subset selection 1343  T UV, B ORISOV, RUNGER AND T ORKKOLA  problem, as they do not deal with the notion of redundancy and most of them are inherently univariate. [sent-89, score-0.297]
</p><p>12 However, there are ﬁlters that use a“local” feature importance measure (like RELIEF) that can be considered multivariate (Kira and Rendell, 1992), but still they do not deal with redundancy giving just a ranked list of features instead of a selected minimal subset. [sent-90, score-0.567]
</p><p>13 Also, selection of important model parameters (kernel width and type, feature relevance thresholds, etc) is non-obvious, and the results of feature selection depend heavily on them. [sent-100, score-0.34]
</p><p>14 Our method approximates the optimal Markov blanket redundancy elimination procedure, but without most of the drawbacks of previous methods. [sent-118, score-0.396]
</p><p>15 It is well known that trees and especially ensembles of trees can provide robust and accurate models in “real-life” data settings. [sent-122, score-0.367]
</p><p>16 It is accomplished by comparing the relevance of the original variables with the relevance of random, artiﬁcial features (appended to the original data) constructed from the same distribution, but independently from the response. [sent-127, score-0.283]
</p><p>17 We measure feature relevance as variable importance in random forests with a modiﬁed robust splitting criteria. [sent-129, score-0.377]
</p><p>18 , 2003), its application to tree ensembles is novel and promising. [sent-133, score-0.274]
</p><p>19 If surrogate variables (ones that partition the node in same way as the primary variable) are present, these surrogate variables are considered as “masked”. [sent-138, score-0.398]
</p><p>20 Masking scores between all pairs of important variables are computed and evaluated using a statistical test, and variables masked by more important variables (“approximately redundant”) are removed iteratively. [sent-139, score-0.403]
</p><p>21 Because redundancy elimination is approximate in nature this iterative approach is another advantage of our method. [sent-141, score-0.325]
</p><p>22 It allows one to recover variables with small importance and to reduce the chance to lose important variables during redundancy elimination. [sent-142, score-0.509]
</p><p>23 Tree Ensembles for Feature Selection For our embedded method, we focus on ensembles of decision trees for the following reasons. [sent-144, score-0.308]
</p><p>24 A simple decision tree also provides an 1345  T UV, B ORISOV, RUNGER AND T ORKKOLA  embedded measure of variable importance that can be obtained from the number and the quality of splits that are generated from a predictor variable. [sent-149, score-0.36]
</p><p>25 Therefore, the tree constructing process itself can be considered as a type of variable selection (a kind of forward selection, embedded algorithm), and the impurity reduction due to a split on a speciﬁc variable indicates the relative importance of that variable to the tree model (Breiman et al. [sent-189, score-0.778]
</p><p>26 Note, that this relative importance automatically incorporates variable interaction effects thus being very different from the relevance measured by a univariate ﬁlter method. [sent-192, score-0.296]
</p><p>27 For a single decision tree the measure of variable importance is V I(Xi , T ) =  ∑ ∆I(Xi ,t),  (1)  t∈T  where ∆I(Xi ,t) is the decrease in impurity due to an actual (or potential) split on variable Xi at a node t of the optimally pruned tree T (Breiman et al. [sent-193, score-0.662]
</p><p>28 For an ensemble of M trees this importance measure is easily generalized. [sent-201, score-0.385]
</p><p>29 This provides a more accurate and unbiased estimate of variable importance in each tree and improves the ﬁltering of noise variables. [sent-208, score-0.323]
</p><p>30 The following two subsections discuss how to amend the ranking so that irrelevant variables can be reliably detected, and how the redundancies among the remaining relevant variables can then be handled. [sent-223, score-0.282]
</p><p>31 2 Removing Irrelevant Features by Artiﬁcial Contrasts Although an ensemble can be used to calculate a relative feature ranking from the variable importance score in (2) the metric does not separate relevant features from irrelevant. [sent-225, score-0.619]
</p><p>32 The variable importance score in (2) is based on the relevance of an input variable to the target. [sent-230, score-0.415]
</p><p>33 Consequently, any stable feature ranking method should favor a relevant input Xi over an artiﬁcially generated variable with the same distribution as Xi but generated to be irrelevant to the target. [sent-231, score-0.291]
</p><p>34 That is, a higher variable importance score is expected from a true relevant variable than from an artiﬁcially generated contrast variable. [sent-232, score-0.373]
</p><p>35 With sufﬁcient replicates in an analysis one can select important variables from those that have statistically signiﬁcantly higher variable importance scores than the contrast variables (Tuv et al. [sent-233, score-0.54]
</p><p>36 Also, artiﬁcial contrasts can be applied to masking discussed in the next subsection. [sent-237, score-0.491]
</p><p>37 Given a selected subset of relevant variables, one computes the masking scores of all variables by elements of this subset, and the masking of contrast variables by this subset. [sent-238, score-1.236]
</p><p>38 3 Masking Measures An important issue for variable importance in tree-based models is how to evaluate or rank variables that were masked by others with slightly higher splitting scores, but could provide as accurate a model if used instead. [sent-242, score-0.403]
</p><p>39 The variable importance sum in Equation (1) is taken over all internal tree nodes where Xi is a primary splitter or a surrogate variable (λ(X ∗ |Xi ) > 0 for a primary splitter X ∗ ). [sent-250, score-0.658]
</p><p>40 Often a variable that does not appear as a primary splitter in a tree is still ranked high on the variable importance list constructed using surrogate variables. [sent-251, score-0.601]
</p><p>41 We extend the surrogate concept to deﬁne a masking score as follows. [sent-252, score-0.58]
</p><p>42 Variable i is said to mask variable j in a tree, if there is a split in variable i in a tree with a surrogate on variable j. [sent-253, score-0.461]
</p><p>43 For an ensemble the masking measure is simply averaged over the trees. [sent-256, score-0.603]
</p><p>44 One variable may mask several others, but for a single selected masked variable the reverse may not be true. [sent-258, score-0.291]
</p><p>45 Then the signiﬁcance from a paired t-test over the replicates is used to identify important variables and masked variables. [sent-264, score-0.303]
</p><p>46 Identify Important Variables: Artiﬁcially generated noise variables are used to determine a threshold to test for statistically signiﬁcant variable importance scores. [sent-270, score-0.299]
</p><p>47 Then in each replicate a small RF is trained and variable importance scores are computed for real and artiﬁcial variables. [sent-281, score-0.348]
</p><p>48 For each real variable X j a paired t-test compares importance scores for X j (obtained from the jth column of V) to the vector of scores v. [sent-285, score-0.404]
</p><p>49 Each replicate uses a RF with L = 20-50 trees to score the importance of the original and artiﬁcial noise variables. [sent-291, score-0.333]
</p><p>50 Also, the split weight calculation for variable importance in (2) only uses OOB samples as described previously. [sent-292, score-0.273]
</p><p>51 Calculate Masking Scores: A masking matrix is computed from independent replicates in order to evaluate the statistical signiﬁcance of masking results. [sent-294, score-1.012]
</p><p>52 For similar reasons as in the previous step, replicates and noise variables are used to detect masking among the relevant variables. [sent-296, score-0.657]
</p><p>53 Note that all variables are tested in each node in each tree in a serial ensemble. [sent-299, score-0.276]
</p><p>54 Therefore, richer, more effective masking information is obtained from a serial ensemble than from a random subspace method like RF. [sent-300, score-0.653]
</p><p>55 Let Mi, j denote the masking score for variables Xi and X j from the ensemble r in replicate r, for r = 1, 2, . [sent-302, score-0.769]
</p><p>56 Also, let Mi,α denote the (1 − α)-percentile of the masking score in replicate r from the distribution of scores between variable Xi and the noise variables. [sent-306, score-0.703]
</p><p>57 Similar to the check for variable importance, a paired t-test compares the masking score between variables (Xi , X j ) r with masking score Mi,α computed from the noise variables. [sent-311, score-1.187]
</p><p>58 There is a signiﬁcant masking between variables (Xi , X j ) if the paired t-test is signiﬁcant. [sent-312, score-0.567]
</p><p>59 Given a list of important variables upon entry to this step, the variables are sorted by the importance score calculated in step 2. [sent-316, score-0.369]
</p><p>60 To remove the information from this variable the remaining predictors and the target are othogonalized with respect to the selected variable. [sent-335, score-0.291]
</p><p>61 In the feature selection method here we do not require orthogonal predictors, but we adjust the target for the variables already selected through residuals. [sent-337, score-0.28]
</p><p>62 The algorithm returns to step 1 and continues until no variables with statistically signiﬁcant importance scores remain. [sent-344, score-0.295]
</p><p>63 This can occur after the effect of a masking variable is completely removed, and the partial masking is eliminated. [sent-348, score-0.998]
</p><p>64 A separate algorithm 3 describes the variable masking calculations. [sent-350, score-0.538]
</p><p>65 All features that have correlation with the selected feature higher than it’s correlation with response are considered redundant and removed. [sent-439, score-0.324]
</p><p>66 Calculate masking matrix M r = M(Gr ) (2m × 2m matrix). [sent-469, score-0.46]
</p><p>67 We use a surrogate masking measure instead of correlation. [sent-491, score-0.539]
</p><p>68 Furthermore, it ﬁlters features by relevance before computing redundancy between the features, and reports a ﬁnal minimum feature subset. [sent-511, score-0.43]
</p><p>69 current working set of variables set of important variables variable importance matrix (R × 2M) rth row of variable importance matrix V, r = 1 . [sent-516, score-0.598]
</p><p>70 Masking ﬂags matrix Table 1: Notation in Algorithms 1-3  However, the major difference is that our redundancy measure approximates KL-distance taking the response into account and uses local information. [sent-520, score-0.278]
</p><p>71 Any classiﬁer/regressor function can be used, from which the variable importance from all variable interactions can be derived. [sent-526, score-0.302]
</p><p>72 To our knowledge, only ensembles of trees can provide this conveniently. [sent-527, score-0.271]
</p><p>73 The algorithm is very fast with approximately a minute for one feature selection iteration on the challenge NOVA data set with 16K variables with 20 replicates with 70 trees on a Windows XP-based four-processor Xeon (2 x HT) 3. [sent-535, score-0.427]
</p><p>74 The effects of feature selection are mixed in with how well the learner is able to handle redundant or irrelevant features. [sent-544, score-0.321]
</p><p>75 1355  T UV, B ORISOV, RUNGER AND T ORKKOLA  Linear data 100 90 80 70 60 50 40 30 20 10 0  ace  cfs  cfs-gen  rfe4  relief4  Figure 1: Artiﬁcial data with linear relationships. [sent-575, score-0.53]
</p><p>76 The ﬁrst is the percentage of relevant variables detected (out of four), the second is the percentage of redundant variables detected (out of 100), and the third is the percentage of irrelevant variables detected (out of 100). [sent-578, score-0.545]
</p><p>77 The ﬁrst is the percentage of relevant variables detected (out of 10), the second is the percentage of redundant variables detected (out of 20), and the third is the percentage of irrelevant variables detected (out of 40). [sent-609, score-0.545]
</p><p>78 The following parameters of GBT were optimized: number of trees, tree depth, shrinkage, number of selected features per tree node, and the importance adjustment rate for embedded feature selection, stratiﬁed sampling for 0/1 class proportions, and priors. [sent-627, score-0.557]
</p><p>79 Redundancy elimination was applied on ADA, HIVA, SYLVA, and feature selection without redundancy elimination was used on NOVA and GINA. [sent-636, score-0.571]
</p><p>80 The list of statistically signiﬁcant variables reproduces all the variables in any of the Markov boundaries listed above, with false alarms from variables X14 , X15 , and X29 . [sent-688, score-0.296]
</p><p>81 Although ACE recovered the variables in the Markov boundaries, there are limitations with the masking methods for a multi-class target. [sent-689, score-0.566]
</p><p>82 The GBT ensembles model each class (versus the rest) with a binary logistic function and averages variable masking scores over the binary models. [sent-690, score-0.787]
</p><p>83 Redundancy elimination did not effectively eliminate masking in the TIED data. [sent-692, score-0.572]
</p><p>84 The results from ACE without redundancy elimination for the binary target are shown in Table 4. [sent-695, score-0.365]
</p><p>85 As the importance scores are arranged in decreasing order in Table 4, groups of variables with similar scores become noticeable and these groups correspond to the subsets (equivalence classes) in the cross-product that deﬁnes the Markov boundaries. [sent-713, score-0.369]
</p><p>86 The analysis with redundancy elimination generated the list of signiﬁcantly signiﬁcant variables in Table 5. [sent-716, score-0.432]
</p><p>87 The predictive performance of a tree ensemble on the recovered variables is nearly identical to a model on a true Markov boundary. [sent-719, score-0.383]
</p><p>88 Although these comprise false alarms, the magnitudes of the importance scores indicate that the last three variables are much less important than the others. [sent-721, score-0.295]
</p><p>89 In this case the ACE analysis without redundancy elimination recovered these four variables without false alarms. [sent-725, score-0.431]
</p><p>90 The analysis with redundancy elimination correctly recovered a single variable from this set. [sent-726, score-0.434]
</p><p>91 Similarly for class 3, without redundancy elimination all variables in the Markov boundaries {X12 , X13 , X14 } were recovered, and only one variable from this set was recovered with redundancy elimination. [sent-727, score-0.761]
</p><p>92 3%  Table 5: Variable importance for TIED data modiﬁed for a binary target (class 1 versus the rest) with redundancy elimination. [sent-735, score-0.399]
</p><p>93 Without redundancy elimination, 20 variables were identiﬁed as related to the target. [sent-744, score-0.288]
</p><p>94 However, after masking scores were used to remove redundant predictors the ﬁnal subset model consisted of only ﬁve predictors. [sent-745, score-0.79]
</p><p>95 190  Table 6: Manufacturing data with a binary target with redundancy elimination excludes many variables. [sent-786, score-0.365]
</p><p>96 We conﬁrmed the strong masking between variables 12 and 17 (and vice versa) from our masking matrix. [sent-791, score-0.995]
</p><p>97 Conclusions We have presented an efﬁcient method for feature subset selection that builds upon the known strengths of the tree ensembles and is designed explicitly to discover a non-redundant, effective subset of features in large, dirty, and complex data sets. [sent-796, score-0.534]
</p><p>98 Our method attempts to eliminate irrelevant variables using statistical comparisons with artiﬁcial contrasts to obtain a threshold for importance estimated from the parallel ensembles of trees capable of scoring very large number of variables. [sent-797, score-0.589]
</p><p>99 It uses serial ensembles to discover signiﬁcant masking effects for redundancy elimination. [sent-798, score-0.898]
</p><p>100 Furthermore we have showed that the redundancy elimination based on feature masking approximates the Markov blanket redundancy ﬁltering. [sent-799, score-1.15]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('masking', 0.46), ('ace', 0.348), ('fcbs', 0.225), ('redundancy', 0.213), ('cfs', 0.182), ('ensembles', 0.175), ('gbt', 0.174), ('runger', 0.174), ('importance', 0.146), ('ensemble', 0.143), ('orisov', 0.133), ('orkkola', 0.133), ('nsembles', 0.123), ('oob', 0.112), ('elimination', 0.112), ('predictors', 0.108), ('masked', 0.104), ('tree', 0.099), ('trees', 0.096), ('replicates', 0.092), ('eature', 0.086), ('redundant', 0.083), ('feature', 0.081), ('surrogate', 0.079), ('variable', 0.078), ('uv', 0.077), ('residuals', 0.075), ('variables', 0.075), ('scores', 0.074), ('relevance', 0.072), ('mbbe', 0.072), ('blanket', 0.071), ('rf', 0.071), ('breiman', 0.071), ('irrelevant', 0.066), ('response', 0.065), ('features', 0.064), ('election', 0.064), ('relief', 0.062), ('borisov', 0.061), ('impurity', 0.061), ('tuv', 0.061), ('tied', 0.056), ('numeric', 0.053), ('selection', 0.053), ('node', 0.052), ('endfor', 0.051), ('splitter', 0.051), ('markov', 0.051), ('mi', 0.05), ('replicate', 0.05), ('serial', 0.05), ('split', 0.049), ('detected', 0.047), ('guyon', 0.047), ('manufacturing', 0.047), ('gini', 0.043), ('rfe', 0.043), ('sahami', 0.043), ('hepatitis', 0.041), ('valentini', 0.041), ('score', 0.041), ('forest', 0.04), ('zm', 0.04), ('target', 0.04), ('boundaries', 0.039), ('agnostic', 0.039), ('nova', 0.039), ('primary', 0.038), ('mixed', 0.038), ('embedded', 0.037), ('ranking', 0.036), ('categorical', 0.036), ('learners', 0.035), ('predictive', 0.035), ('pti', 0.035), ('remove', 0.034), ('list', 0.032), ('paired', 0.032), ('bagging', 0.031), ('cially', 0.031), ('friedman', 0.031), ('subset', 0.031), ('contrasts', 0.031), ('niter', 0.031), ('removemasked', 0.031), ('stoppiglia', 0.031), ('selected', 0.031), ('lter', 0.031), ('recovered', 0.031), ('inputs', 0.03), ('challenge', 0.03), ('relevant', 0.03), ('xi', 0.03), ('gi', 0.029), ('yu', 0.029), ('koller', 0.029), ('classification', 0.029), ('permute', 0.029), ('base', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="35-tfidf-1" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>Author: Eugene Tuv, Alexander Borisov, George Runger, Kari Torkkola</p><p>Abstract: Predictive models beneﬁt from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for ﬁlters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach. Keywords: trees, resampling, importance, masking, residuals</p><p>2 0.097719274 <a title="35-tfidf-2" href="./jmlr-2009-Subgroup_Analysis_via_Recursive_Partitioning.html">91 jmlr-2009-Subgroup Analysis via Recursive Partitioning</a></p>
<p>Author: Xiaogang Su, Chih-Ling Tsai, Hansheng Wang, David M. Nickerson, Bogong Li</p><p>Abstract: Subgroup analysis is an integral part of comparative analysis where assessing the treatment effect on a response is of central interest. Its goal is to determine the heterogeneity of the treatment effect across subpopulations. In this paper, we adapt the idea of recursive partitioning and introduce an interaction tree (IT) procedure to conduct subgroup analysis. The IT procedure automatically facilitates a number of objectively deﬁned subgroups, in some of which the treatment effect is found prominent while in others the treatment has a negligible or even negative effect. The standard CART (Breiman et al., 1984) methodology is inherited to construct the tree structure. Also, in order to extract factors that contribute to the heterogeneity of the treatment effect, variable importance measure is made available via random forests of the interaction trees. Both simulated experiments and analysis of census wage data are presented for illustration. Keywords: CART, interaction, subgroup analysis, random forests</p><p>3 0.088234149 <a title="35-tfidf-3" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>Author: Dirk Gorissen, Tom Dhaene, Filip De Turck</p><p>Abstract: Due to the scale and computational complexity of currently used simulation codes, global surrogate (metamodels) models have become indispensable tools for exploring and understanding the design space. Due to their compact formulation they are cheap to evaluate and thus readily facilitate visualization, design space exploration, rapid prototyping, and sensitivity analysis. They can also be used as accurate building blocks in design packages or larger simulation environments. Consequently, there is great interest in techniques that facilitate the construction of such approximation models while minimizing the computational cost and maximizing model accuracy. Many surrogate model types exist (Support Vector Machines, Kriging, Neural Networks, etc.) but no type is optimal in all circumstances. Nor is there any hard theory available that can help make this choice. In this paper we present an automatic approach to the model type selection problem. We describe an adaptive global surrogate modeling environment with adaptive sampling, driven by speciated evolution. Different model types are evolved cooperatively using a Genetic Algorithm (heterogeneous evolution) and compete to approximate the iteratively selected data. In this way the optimal model type and complexity for a given data set or simulation code can be dynamically determined. Its utility and performance is demonstrated on a number of problems where it outperforms traditional sequential execution of each model type. Keywords: model type selection, genetic algorithms, global surrogate modeling, function approximation, active learning, adaptive sampling</p><p>4 0.055381808 <a title="35-tfidf-4" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>Author: Omid Madani, Michael Connor, Wiley Greiner</p><p>Abstract: Many learning tasks, such as large-scale text categorization and word prediction, can beneﬁt from efﬁcient training and classiﬁcation when the number of classes, in addition to instances and features, is large, that is, in the thousands and beyond. We investigate the learning of sparse class indices to address this challenge. An index is a mapping from features to classes. We compare the index-learning methods against other techniques, including one-versus-rest and top-down classiﬁcation using perceptrons and support vector machines. We ﬁnd that index learning is highly advantageous for space and time efﬁciency, at both training and classiﬁcation times. Moreover, this approach yields similar and at times better accuracies. On problems with hundreds of thousands of instances and thousands of classes, the index is learned in minutes, while other methods can take hours or days. As we explain, the design of the learning update enables conveniently constraining each feature to connect to a small subset of the classes in the index. This constraint is crucial for scalability. Given an instance with l active (positive-valued) features, each feature on average connecting to d classes in the index (in the order of 10s in our experiments), update and classiﬁcation take O(dl log(dl)). Keywords: index learning, many-class learning, multiclass learning, online learning, text categorization</p><p>5 0.055078775 <a title="35-tfidf-5" href="./jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems.html">8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</a></p>
<p>Author: Luciana Ferrer, Kemal Sönmez, Elizabeth Shriberg</p><p>Abstract: We present a method for training support vector machine (SVM)-based classiﬁcation systems for combination with other classiﬁcation systems designed for the same task. Ideally, a new system should be designed such that, when combined with existing systems, the resulting performance is optimized. We present a simple model for this problem and use the understanding gained from this analysis to propose a method to achieve better combination performance when training SVM systems. We include a regularization term in the SVM objective function that aims to reduce the average class-conditional covariance between the resulting scores and the scores produced by the existing systems, introducing a trade-off between such covariance and the system’s individual performance. That is, the new system “takes one for the team”, falling somewhat short of its best possible performance in order to increase the diversity of the ensemble. We report results on the NIST 2005 and 2006 speaker recognition evaluations (SREs) for a variety of subsystems. We show a gain of 19% on the equal error rate (EER) of a combination of four systems when applying the proposed method with respect to the performance obtained when the four systems are trained independently of each other. Keywords: system combination, ensemble diversity, multiple classiﬁer systems, support vector machines, speaker recognition, kernel methods ∗. This author performed part of the work presented in this paper while at the Information Systems Laboratory, Department of Electrical Engineering, Stanford University. c 2009 Luciana Ferrer, Kemal S¨ nmez and Elizabeth Shriberg. o ¨ F ERRER , S ONMEZ AND S HRIBERG</p><p>6 0.052069753 <a title="35-tfidf-6" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>7 0.04904696 <a title="35-tfidf-7" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>8 0.043079223 <a title="35-tfidf-8" href="./jmlr-2009-A_Parameter-Free_Classification_Method_for_Large_Scale_Learning.html">3 jmlr-2009-A Parameter-Free Classification Method for Large Scale Learning</a></p>
<p>9 0.04207911 <a title="35-tfidf-9" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>10 0.039623979 <a title="35-tfidf-10" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>11 0.03946317 <a title="35-tfidf-11" href="./jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">70 jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<p>12 0.038756132 <a title="35-tfidf-12" href="./jmlr-2009-Generalization_Bounds_for_Ranking_Algorithms_via_Algorithmic_Stability.html">37 jmlr-2009-Generalization Bounds for Ranking Algorithms via Algorithmic Stability</a></p>
<p>13 0.037715219 <a title="35-tfidf-13" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>14 0.037297178 <a title="35-tfidf-14" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>15 0.035144545 <a title="35-tfidf-15" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>16 0.034970392 <a title="35-tfidf-16" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>17 0.032485582 <a title="35-tfidf-17" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>18 0.031605922 <a title="35-tfidf-18" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>19 0.031450327 <a title="35-tfidf-19" href="./jmlr-2009-Hybrid_MPI_OpenMP_Parallel_Linear_Support_Vector_Machine_Training.html">39 jmlr-2009-Hybrid MPI OpenMP Parallel Linear Support Vector Machine Training</a></p>
<p>20 0.031448718 <a title="35-tfidf-20" href="./jmlr-2009-On_The_Power_of_Membership_Queries_in_Agnostic_Learning.html">64 jmlr-2009-On The Power of Membership Queries in Agnostic Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.163), (1, -0.097), (2, 0.106), (3, -0.003), (4, 0.045), (5, -0.094), (6, -0.031), (7, 0.158), (8, 0.084), (9, 0.063), (10, 0.11), (11, -0.107), (12, 0.15), (13, 0.026), (14, 0.013), (15, -0.026), (16, -0.139), (17, -0.003), (18, -0.206), (19, 0.272), (20, 0.124), (21, 0.018), (22, -0.076), (23, -0.014), (24, 0.102), (25, -0.059), (26, 0.08), (27, 0.071), (28, -0.085), (29, 0.031), (30, 0.022), (31, 0.083), (32, 0.142), (33, -0.113), (34, 0.094), (35, -0.008), (36, 0.039), (37, -0.114), (38, -0.026), (39, 0.047), (40, 0.008), (41, 0.066), (42, 0.111), (43, 0.117), (44, 0.045), (45, 0.272), (46, -0.07), (47, 0.059), (48, 0.137), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9386856 <a title="35-lsi-1" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>Author: Eugene Tuv, Alexander Borisov, George Runger, Kari Torkkola</p><p>Abstract: Predictive models beneﬁt from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for ﬁlters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach. Keywords: trees, resampling, importance, masking, residuals</p><p>2 0.46869585 <a title="35-lsi-2" href="./jmlr-2009-Subgroup_Analysis_via_Recursive_Partitioning.html">91 jmlr-2009-Subgroup Analysis via Recursive Partitioning</a></p>
<p>Author: Xiaogang Su, Chih-Ling Tsai, Hansheng Wang, David M. Nickerson, Bogong Li</p><p>Abstract: Subgroup analysis is an integral part of comparative analysis where assessing the treatment effect on a response is of central interest. Its goal is to determine the heterogeneity of the treatment effect across subpopulations. In this paper, we adapt the idea of recursive partitioning and introduce an interaction tree (IT) procedure to conduct subgroup analysis. The IT procedure automatically facilitates a number of objectively deﬁned subgroups, in some of which the treatment effect is found prominent while in others the treatment has a negligible or even negative effect. The standard CART (Breiman et al., 1984) methodology is inherited to construct the tree structure. Also, in order to extract factors that contribute to the heterogeneity of the treatment effect, variable importance measure is made available via random forests of the interaction trees. Both simulated experiments and analysis of census wage data are presented for illustration. Keywords: CART, interaction, subgroup analysis, random forests</p><p>3 0.44900993 <a title="35-lsi-3" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>Author: Dirk Gorissen, Tom Dhaene, Filip De Turck</p><p>Abstract: Due to the scale and computational complexity of currently used simulation codes, global surrogate (metamodels) models have become indispensable tools for exploring and understanding the design space. Due to their compact formulation they are cheap to evaluate and thus readily facilitate visualization, design space exploration, rapid prototyping, and sensitivity analysis. They can also be used as accurate building blocks in design packages or larger simulation environments. Consequently, there is great interest in techniques that facilitate the construction of such approximation models while minimizing the computational cost and maximizing model accuracy. Many surrogate model types exist (Support Vector Machines, Kriging, Neural Networks, etc.) but no type is optimal in all circumstances. Nor is there any hard theory available that can help make this choice. In this paper we present an automatic approach to the model type selection problem. We describe an adaptive global surrogate modeling environment with adaptive sampling, driven by speciated evolution. Different model types are evolved cooperatively using a Genetic Algorithm (heterogeneous evolution) and compete to approximate the iteratively selected data. In this way the optimal model type and complexity for a given data set or simulation code can be dynamically determined. Its utility and performance is demonstrated on a number of problems where it outperforms traditional sequential execution of each model type. Keywords: model type selection, genetic algorithms, global surrogate modeling, function approximation, active learning, adaptive sampling</p><p>4 0.41618234 <a title="35-lsi-4" href="./jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems.html">8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</a></p>
<p>Author: Luciana Ferrer, Kemal Sönmez, Elizabeth Shriberg</p><p>Abstract: We present a method for training support vector machine (SVM)-based classiﬁcation systems for combination with other classiﬁcation systems designed for the same task. Ideally, a new system should be designed such that, when combined with existing systems, the resulting performance is optimized. We present a simple model for this problem and use the understanding gained from this analysis to propose a method to achieve better combination performance when training SVM systems. We include a regularization term in the SVM objective function that aims to reduce the average class-conditional covariance between the resulting scores and the scores produced by the existing systems, introducing a trade-off between such covariance and the system’s individual performance. That is, the new system “takes one for the team”, falling somewhat short of its best possible performance in order to increase the diversity of the ensemble. We report results on the NIST 2005 and 2006 speaker recognition evaluations (SREs) for a variety of subsystems. We show a gain of 19% on the equal error rate (EER) of a combination of four systems when applying the proposed method with respect to the performance obtained when the four systems are trained independently of each other. Keywords: system combination, ensemble diversity, multiple classiﬁer systems, support vector machines, speaker recognition, kernel methods ∗. This author performed part of the work presented in this paper while at the Information Systems Laboratory, Department of Electrical Engineering, Stanford University. c 2009 Luciana Ferrer, Kemal S¨ nmez and Elizabeth Shriberg. o ¨ F ERRER , S ONMEZ AND S HRIBERG</p><p>5 0.36779898 <a title="35-lsi-5" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>Author: Pradip Ghanty, Samrat Paul, Nikhil R. Pal</p><p>Abstract: In this paper we propose a new multilayer classiﬁer architecture. The proposed hybrid architecture has two cascaded modules: feature extraction module and classiﬁcation module. In the feature extraction module we use the multilayered perceptron (MLP) neural networks, although other tools such as radial basis function (RBF) networks can be used. In the classiﬁcation module we use support vector machines (SVMs)—here also other tool such as MLP or RBF can be used. The feature extraction module has several sub-modules each of which is expected to extract features capturing the discriminating characteristics of different areas of the input space. The classiﬁcation module classiﬁes the data based on the extracted features. The resultant architecture with MLP in feature extraction module and SVM in classiﬁcation module is called NEUROSVM. The NEUROSVM is tested on twelve benchmark data sets and the performance of the NEUROSVM is found to be better than both MLP and SVM. We also compare the performance of proposed architecture with that of two ensemble methods: majority voting and averaging. Here also the NEUROSVM is found to perform better than these two ensemble methods. Further we explore the use of MLP and RBF in the classiﬁcation module of the proposed architecture. The most attractive feature of NEUROSVM is that it practically eliminates the severe dependency of SVM on the choice of kernel. This has been veriﬁed with respect to both linear and non-linear kernels. We have also demonstrated that for the feature extraction module, the full training of MLPs is not needed. Keywords: feature extraction, neural networks (NNs), support vector machines (SVMs), hybrid system, majority voting, averaging c 2009 Pradip Ghanty, Samrat Paul and Nikhil R. Pal. G HANTY, PAUL AND PAL</p><p>6 0.3570655 <a title="35-lsi-6" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>7 0.33267084 <a title="35-lsi-7" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>8 0.32114112 <a title="35-lsi-8" href="./jmlr-2009-A_Parameter-Free_Classification_Method_for_Large_Scale_Learning.html">3 jmlr-2009-A Parameter-Free Classification Method for Large Scale Learning</a></p>
<p>9 0.31376562 <a title="35-lsi-9" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>10 0.3105574 <a title="35-lsi-10" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>11 0.26376623 <a title="35-lsi-11" href="./jmlr-2009-Generalization_Bounds_for_Ranking_Algorithms_via_Algorithmic_Stability.html">37 jmlr-2009-Generalization Bounds for Ranking Algorithms via Algorithmic Stability</a></p>
<p>12 0.22209141 <a title="35-lsi-12" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>13 0.21715961 <a title="35-lsi-13" href="./jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">70 jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<p>14 0.20591743 <a title="35-lsi-14" href="./jmlr-2009-Markov_Properties_for_Linear_Causal_Models_with_Correlated_Errors%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">54 jmlr-2009-Markov Properties for Linear Causal Models with Correlated Errors    (Special Topic on Causality)</a></p>
<p>15 0.1934188 <a title="35-lsi-15" href="./jmlr-2009-A_Least-squares_Approach_to_Direct_Importance_Estimation.html">1 jmlr-2009-A Least-squares Approach to Direct Importance Estimation</a></p>
<p>16 0.18989262 <a title="35-lsi-16" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>17 0.17837363 <a title="35-lsi-17" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>18 0.17614995 <a title="35-lsi-18" href="./jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification.html">45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</a></p>
<p>19 0.17547397 <a title="35-lsi-19" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>20 0.17133021 <a title="35-lsi-20" href="./jmlr-2009-An_Algorithm_for_Reading_Dependencies_from_the_Minimal_Undirected_Independence_Map_of_a_Graphoid_that_Satisfies_Weak_Transitivity.html">6 jmlr-2009-An Algorithm for Reading Dependencies from the Minimal Undirected Independence Map of a Graphoid that Satisfies Weak Transitivity</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.022), (11, 0.023), (19, 0.01), (26, 0.54), (38, 0.03), (47, 0.017), (52, 0.053), (55, 0.022), (58, 0.032), (65, 0.012), (66, 0.065), (68, 0.017), (90, 0.042), (96, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96554345 <a title="35-lda-1" href="./jmlr-2009-DL-Learner%3A_Learning_Concepts_in_Description_Logics.html">20 jmlr-2009-DL-Learner: Learning Concepts in Description Logics</a></p>
<p>Author: Jens Lehmann</p><p>Abstract: In this paper, we introduce DL-Learner, a framework for learning in description logics and OWL. OWL is the ofÄ?Ĺš cial W3C standard ontology language for the Semantic Web. Concepts in this language can be learned for constructing and maintaining OWL ontologies or for solving problems similar to those in Inductive Logic Programming. DL-Learner includes several learning algorithms, support for different OWL formats, reasoner interfaces, and learning problems. It is a cross-platform framework implemented in Java. The framework allows easy programmatic access and provides a command line interface, a graphical interface as well as a WSDL-based web service. Keywords: concept learning, description logics, OWL, classiÄ?Ĺš cation, open-source</p><p>same-paper 2 0.90305996 <a title="35-lda-2" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>Author: Eugene Tuv, Alexander Borisov, George Runger, Kari Torkkola</p><p>Abstract: Predictive models beneﬁt from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for ﬁlters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach. Keywords: trees, resampling, importance, masking, residuals</p><p>3 0.39777416 <a title="35-lda-3" href="./jmlr-2009-A_Parameter-Free_Classification_Method_for_Large_Scale_Learning.html">3 jmlr-2009-A Parameter-Free Classification Method for Large Scale Learning</a></p>
<p>Author: Marc Boullé</p><p>Abstract: With the rapid growth of computer storage capacities, available data and demand for scoring models both follow an increasing trend, sharper than that of the processing power. However, the main limitation to a wide spread of data mining solutions is the non-increasing availability of skilled data analysts, which play a key role in data preparation and model selection. In this paper, we present a parameter-free scalable classiﬁcation method, which is a step towards fully automatic data mining. The method is based on Bayes optimal univariate conditional density estimators, naive Bayes classiﬁcation enhanced with a Bayesian variable selection scheme, and averaging of models using a logarithmic smoothing of the posterior distribution. We focus on the complexity of the algorithms and show how they can cope with data sets that are far larger than the available central memory. We ﬁnally report results on the Large Scale Learning challenge, where our method obtains state of the art performance within practicable computation time. Keywords: large scale learning, naive Bayes, Bayesianism, model selection, model averaging</p><p>4 0.39519191 <a title="35-lda-4" href="./jmlr-2009-Subgroup_Analysis_via_Recursive_Partitioning.html">91 jmlr-2009-Subgroup Analysis via Recursive Partitioning</a></p>
<p>Author: Xiaogang Su, Chih-Ling Tsai, Hansheng Wang, David M. Nickerson, Bogong Li</p><p>Abstract: Subgroup analysis is an integral part of comparative analysis where assessing the treatment effect on a response is of central interest. Its goal is to determine the heterogeneity of the treatment effect across subpopulations. In this paper, we adapt the idea of recursive partitioning and introduce an interaction tree (IT) procedure to conduct subgroup analysis. The IT procedure automatically facilitates a number of objectively deﬁned subgroups, in some of which the treatment effect is found prominent while in others the treatment has a negligible or even negative effect. The standard CART (Breiman et al., 1984) methodology is inherited to construct the tree structure. Also, in order to extract factors that contribute to the heterogeneity of the treatment effect, variable importance measure is made available via random forests of the interaction trees. Both simulated experiments and analysis of census wage data are presented for illustration. Keywords: CART, interaction, subgroup analysis, random forests</p><p>5 0.37095886 <a title="35-lda-5" href="./jmlr-2009-Settable_Systems%3A_An_Extension_of_Pearl%27s_Causal_Model_with_Optimization%2C_Equilibrium%2C_and_Learning.html">85 jmlr-2009-Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning</a></p>
<p>Author: Halbert White, Karim Chalak</p><p>Abstract: Judea Pearl’s Causal Model is a rich framework that provides deep insight into the nature of causal relations. As yet, however, the Pearl Causal Model (PCM) has had a lesser impact on economics or econometrics than on other disciplines. This may be due in part to the fact that the PCM is not as well suited to analyzing structures that exhibit features of central interest to economists and econometricians: optimization, equilibrium, and learning. We offer the settable systems framework as an extension of the PCM that permits causal discourse in systems embodying optimization, equilibrium, and learning. Because these are common features of physical, natural, or social systems, our framework may prove generally useful for machine learning. Important features distinguishing the settable system framework from the PCM are its countable dimensionality and the use of partitioning and partition-speciﬁc response functions to accommodate the behavior of optimizing and interacting agents and to eliminate the requirement of a unique ﬁxed point for the system. Reﬁnements of the PCM include the settable systems treatment of attributes, the causal role of exogenous variables, and the dual role of variables as causes and responses. A series of closely related machine learning examples and examples from game theory and machine learning with feedback demonstrates some limitations of the PCM and motivates the distinguishing features of settable systems. Keywords: equations causal models, game theory, machine learning, recursive estimation, simultaneous</p><p>6 0.3699933 <a title="35-lda-6" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>7 0.36784694 <a title="35-lda-7" href="./jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">70 jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<p>8 0.36175287 <a title="35-lda-8" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>9 0.35004827 <a title="35-lda-9" href="./jmlr-2009-Python_Environment_for_Bayesian_Learning%3A_Inferring_the_Structure_of_Bayesian_Networks_from_Knowledge_and_Data%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">76 jmlr-2009-Python Environment for Bayesian Learning: Inferring the Structure of Bayesian Networks from Knowledge and Data    (Machine Learning Open Source Software Paper)</a></p>
<p>10 0.33594444 <a title="35-lda-10" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>11 0.31341279 <a title="35-lda-11" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>12 0.30955613 <a title="35-lda-12" href="./jmlr-2009-Robust_Process_Discovery_with_Artificial_Negative_Events%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">81 jmlr-2009-Robust Process Discovery with Artificial Negative Events    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>13 0.29107073 <a title="35-lda-13" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>14 0.27857822 <a title="35-lda-14" href="./jmlr-2009-Dlib-ml%3A_A_Machine_Learning_Toolkit%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">26 jmlr-2009-Dlib-ml: A Machine Learning Toolkit    (Machine Learning Open Source Software Paper)</a></p>
<p>15 0.27502215 <a title="35-lda-15" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>16 0.2744194 <a title="35-lda-16" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>17 0.2699039 <a title="35-lda-17" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>18 0.26533702 <a title="35-lda-18" href="./jmlr-2009-Improving_the_Reliability_of_Causal_Discovery_from_Small_Data_Sets_Using_Argumentation%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">41 jmlr-2009-Improving the Reliability of Causal Discovery from Small Data Sets Using Argumentation    (Special Topic on Causality)</a></p>
<p>19 0.26296923 <a title="35-lda-19" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>20 0.26202744 <a title="35-lda-20" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
