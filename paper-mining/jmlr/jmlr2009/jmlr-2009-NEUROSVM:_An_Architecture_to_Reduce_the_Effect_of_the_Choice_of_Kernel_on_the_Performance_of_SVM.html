<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-58" href="#">jmlr2009-58</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</h1>
<br/><p>Source: <a title="jmlr-2009-58-pdf" href="http://jmlr.org/papers/volume10/ghanty09a/ghanty09a.pdf">pdf</a></p><p>Author: Pradip Ghanty, Samrat Paul, Nikhil R. Pal</p><p>Abstract: In this paper we propose a new multilayer classiﬁer architecture. The proposed hybrid architecture has two cascaded modules: feature extraction module and classiﬁcation module. In the feature extraction module we use the multilayered perceptron (MLP) neural networks, although other tools such as radial basis function (RBF) networks can be used. In the classiﬁcation module we use support vector machines (SVMs)—here also other tool such as MLP or RBF can be used. The feature extraction module has several sub-modules each of which is expected to extract features capturing the discriminating characteristics of different areas of the input space. The classiﬁcation module classiﬁes the data based on the extracted features. The resultant architecture with MLP in feature extraction module and SVM in classiﬁcation module is called NEUROSVM. The NEUROSVM is tested on twelve benchmark data sets and the performance of the NEUROSVM is found to be better than both MLP and SVM. We also compare the performance of proposed architecture with that of two ensemble methods: majority voting and averaging. Here also the NEUROSVM is found to perform better than these two ensemble methods. Further we explore the use of MLP and RBF in the classiﬁcation module of the proposed architecture. The most attractive feature of NEUROSVM is that it practically eliminates the severe dependency of SVM on the choice of kernel. This has been veriﬁed with respect to both linear and non-linear kernels. We have also demonstrated that for the feature extraction module, the full training of MLPs is not needed. Keywords: feature extraction, neural networks (NNs), support vector machines (SVMs), hybrid system, majority voting, averaging c 2009 Pradip Ghanty, Samrat Paul and Nikhil R. Pal. G HANTY, PAUL AND PAL</p><p>Reference: <a title="jmlr-2009-58-reference" href="../jmlr2009_reference/jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The proposed hybrid architecture has two cascaded modules: feature extraction module and classiﬁcation module. [sent-15, score-0.469]
</p><p>2 In the feature extraction module we use the multilayered perceptron (MLP) neural networks, although other tools such as radial basis function (RBF) networks can be used. [sent-16, score-0.43]
</p><p>3 In the classiﬁcation module we use support vector machines (SVMs)—here also other tool such as MLP or RBF can be used. [sent-17, score-0.259]
</p><p>4 The feature extraction module has several sub-modules each of which is expected to extract features capturing the discriminating characteristics of different areas of the input space. [sent-18, score-0.364]
</p><p>5 The classiﬁcation module classiﬁes the data based on the extracted features. [sent-19, score-0.259]
</p><p>6 The resultant architecture with MLP in feature extraction module and SVM in classiﬁcation module is called NEUROSVM. [sent-20, score-0.702]
</p><p>7 We also compare the performance of proposed architecture with that of two ensemble methods: majority voting and averaging. [sent-22, score-0.341]
</p><p>8 Here also the NEUROSVM is found to perform better than these two ensemble methods. [sent-23, score-0.176]
</p><p>9 Further we explore the use of MLP and RBF in the classiﬁcation module of the proposed architecture. [sent-24, score-0.259]
</p><p>10 Keywords: feature extraction, neural networks (NNs), support vector machines (SVMs), hybrid system, majority voting, averaging c 2009 Pradip Ghanty, Samrat Paul and Nikhil R. [sent-28, score-0.192]
</p><p>11 Use of an ensemble of classiﬁers is a popular approach to improve the classiﬁcation performance. [sent-56, score-0.176]
</p><p>12 Many ensemble methods are used by researchers to report the improvement in performance over single classiﬁer (Hansen, 1999; Maqsood et al. [sent-57, score-0.176]
</p><p>13 An ensemble of classiﬁers can be constructed using both homogeneous and heterogeneous classiﬁers (Hansen, 1999; Prevost et al. [sent-60, score-0.176]
</p><p>14 An ensemble of neural networks is often used for pattern classiﬁcation problems (Garcia-Pedrajas et al. [sent-63, score-0.242]
</p><p>15 Different approaches for constructing ensemble of neural networks have been suggested in the literatures (Wu et al. [sent-69, score-0.242]
</p><p>16 In this paper for the purpose of comparison we have considered two ensemble methods for neural networks, one uses the average output of the ensemble of networks while the other one makes the ensemble vote on a classiﬁcation task. [sent-72, score-0.62]
</p><p>17 This is an ensemble method where a large number of classiﬁers are trained and then their outcomes are aggregated using the majority voting rule. [sent-75, score-0.295]
</p><p>18 The second baseline classiﬁer uses data projected through the hidden layer of a projection network (MLP here). [sent-78, score-0.261]
</p><p>19 The projection network is an MLP network with number of hidden nodes equal to the number of inputs in the original training data and it is trained using only that subset of the training data which are not classiﬁed correctly by the ﬁrst baseline classiﬁer. [sent-79, score-0.36]
</p><p>20 The projection network (again an MLP with number of hidden nodes equal to the number of inputs in the original data) for the third baseline classiﬁer is trained using the original data points whose projected versions are wrongly classiﬁed by the second baseline classiﬁer. [sent-80, score-0.318]
</p><p>21 Huang and LeCun (2006) presented a hybrid system for object recognition that uses the outputs of the last hidden layer of a convolution network to train a SVM with Gaussian kernel. [sent-96, score-0.301]
</p><p>22 A convolution network has several hidden layers alternately consisting of convolution layer and sub-sampling layer. [sent-98, score-0.293]
</p><p>23 The basic philosophy of developing a modular network is to divide the task into a number of, preferably, meaningful subtasks, and then design one module for each subtask. [sent-101, score-0.317]
</p><p>24 A given input is routed to the appropriate MLP module using the SOM. [sent-106, score-0.259]
</p><p>25 We view the ﬁrst module as a 593  G HANTY, PAUL AND PAL  feature extraction module (FM), because outputs of this module can be used as inputs to any other classiﬁer. [sent-113, score-0.882]
</p><p>26 This new set of features is used in the next module, termed as the classiﬁcation module (CM). [sent-114, score-0.259]
</p><p>27 In the classiﬁcation module we have used SVM with different kernel functions. [sent-115, score-0.281]
</p><p>28 To further demonstrate the effectiveness of NEUROSVM we compare it with two other ensemble methods: majority voting and averaging. [sent-118, score-0.262]
</p><p>29 The classiﬁer uses features extracted from the hidden nodes of several trained networks where typically the number of hidden nodes in a network is smaller than input dimension. [sent-122, score-0.436]
</p><p>30 Each network used for feature extraction is trained using the same data and each network sees the entire input space as represented through the training data. [sent-123, score-0.232]
</p><p>31 Thus typically to get improved performance we need fewer feature extraction networks than that would be needed by the ensemble type methods. [sent-124, score-0.324]
</p><p>32 Unlike MLP, the activation functions of the hidden nodes are not of sigmoidal type, rather each hidden node represents a radial basis function. [sent-135, score-0.256]
</p><p>33 The transformation from the input space to the hidden space is nonlinear but each node in the output layer computes just the weighted sum of the outputs of the previous layer, that is, each output layer node makes a linear transformation. [sent-136, score-0.273]
</p><p>34 Then a supervised learning method, such as gradient descent or least square error estimate, is applied to tune the network weights between the hidden layer and the output layer. [sent-139, score-0.217]
</p><p>35 5 Proposed NEUROSVM Classiﬁer The proposed multilayer architecture can be thought of as a combination of two types of modules: feature extraction module (FM) and classiﬁcation module (CM). [sent-231, score-0.722]
</p><p>36 Similarly, the classiﬁcation module can use any principle like neurocomputing, support vector machines and so on. [sent-264, score-0.259]
</p><p>37 In this investigation, the sub-modules SFMi s are derived from multilayer perceptron networks, while the classiﬁcation module consists of support vector machines. [sent-265, score-0.279]
</p><p>38 In order to constitute the ith sub-module SFMi , we consider an MLP with just one hidden layer, with architecture (p, ni , c), where p is the input dimension, ni is the number of nodes in the hidden layer and c is the number of classes. [sent-267, score-0.425]
</p><p>39 Note that, although the number of input and output nodes in each MLP remains the same, the number of nodes in the hidden layer could be different for different MLPs. [sent-268, score-0.323]
</p><p>40 Once each network is trained, the output of the hidden layer can be taken as the extracted features. [sent-276, score-0.217]
</p><p>41 After the training, we remove the output layer and its associated connections from each of the MLPs and then the truncated two-layer sub-networks are taken as feature extraction sub-modules. [sent-283, score-0.195]
</p><p>42 The ﬁrst layer, as shown in Figure 1, is the SVM kernel layer where each node is associated to a mapped training sample zi (it is the output from an FM that represents a support vector) and it computes the kernel output K(z, zi ) on a mapped input z, while the other layer is the output layer. [sent-307, score-0.25]
</p><p>43 Hence when we combine the output of the hidden layer of different networks to generate new features, the learning task becomes simpler to the CM. [sent-314, score-0.226]
</p><p>44 The extracted features result in simpler classiﬁcation boundaries because a single layer network can classify the new data (consider a two layer network consisting of the hidden and output layers of an MLP). [sent-317, score-0.365]
</p><p>45 For high dimensional data, typically the number of nodes in the hidden layer is much smaller than the number of the input nodes and one does not need many feature extraction submodules (SFMs). [sent-320, score-0.428]
</p><p>46 This is not an ensemble method but it makes fusion of salient characteristics of the input space as extracted/learnt by different feature extraction networks. [sent-324, score-0.306]
</p><p>47 The hyper parameters of SVMs in the classiﬁcation module of NEUROSVM are also estimated through ten-fold cross-validation experiments. [sent-380, score-0.338]
</p><p>48 The svm learn and svm classify modules for binary SVMs training and classiﬁcation are used from SV M light (Joachims, 2002) software. [sent-418, score-0.271]
</p><p>49 The performance comparison of NEUROSVM with the baseline classiﬁers and standard ensemble methods is presented. [sent-424, score-0.22]
</p><p>50 Finally, we present the performance of other variants of NEUROSVM and compare it with the baseline classiﬁers as well as ensemble methods. [sent-425, score-0.22]
</p><p>51 For each of the outer level cross validation loop, ﬁnding the optimal number of hidden nodes and computation of the test error are explained in the procedure RunMLP in Appendix B. [sent-430, score-0.254]
</p><p>52 For each data set, a set of choices on the number of hidden nodes is used to train the MLPs. [sent-436, score-0.216]
</p><p>53 In Table 3, number of iterations and number of hidden nodes that are used to train the MLPs for the twelve data sets are listed. [sent-437, score-0.286]
</p><p>54 We have decided to use m = 5 neural networks for feature extraction modules and hence for each fold, we have to select a set of ﬁve hidden nodes to train ﬁve MLPs. [sent-438, score-0.401]
</p><p>55 First we display the variation of the average validation error of cross-validation experiments as a function of the number of hidden nodes for both Group A and Group B data sets. [sent-439, score-0.198]
</p><p>56 Then for each of the outer level fold, we shall have M different hidden nodes each associated with an average validation error. [sent-446, score-0.225]
</p><p>57 Now we order these M hidden nodes in ascending order of the associated validation error. [sent-447, score-0.198]
</p><p>58 These ﬁve different choices of hidden nodes will be used to train ﬁve MLPs for feature extraction for that particular fold. [sent-449, score-0.321]
</p><p>59 For each data set, in Table 4, we depict the list of selected hidden nodes for each fold (outer level). [sent-450, score-0.271]
</p><p>60 As an example, for the IRIS data for the ﬁrst fold (outer level), the selected hidden nodes are (7, 2, 5, 6, 8). [sent-451, score-0.271]
</p><p>61 This means that for the ﬁrst fold (outer level) we got the least validation error with 7 hidden nodes; the next smaller validation error is obtained with 2 hidden nodes and so on. [sent-452, score-0.415]
</p><p>62 Since the ﬁrst element of this set of ﬁve resulted in the smallest validation error, we use this choice of hidden nodes to train MLPs when we report the performance of the MLP networks as classiﬁers. [sent-453, score-0.273]
</p><p>63 For each data set in Group B, since the training and test partitions are ﬁxed, we have only one outer loop and hence only one set with ﬁve choices of hidden nodes as shown in Table 4. [sent-454, score-0.266]
</p><p>64 We follow the same protocol as that of Group A data sets to choose the number of hidden nodes for computing the performance of MLP networks. [sent-455, score-0.184]
</p><p>65 In a manner similar to the way the optimal number of hidden node is chosen for each fold (outer level), the optimal (C, γ) is chosen using ten-fold cross-validation experiments. [sent-494, score-0.182]
</p><p>66 For each of the twelve data sets, 603  G HANTY, PAUL AND PAL  Figure 2: For each of the ten-folds the variation of cross-validation error with different choices of number of hidden nodes for MLPs on the Vehicle data set. [sent-496, score-0.254]
</p><p>67 604  NEUROSVM: A N A RCHITECTURE TO R EDUCE THE E FFECT OF THE C HOICE OF SVM K ERNEL  Figure 3: Variation of cross-validation error with different choices of number of hidden nodes for MLPs on four data sets in Group B: (a) Pendigits (b) Img. [sent-498, score-0.205]
</p><p>68 3 S ELECTION  OF  SFM S  AND  H YPER PARAMETERS FOR NEUROSVM  We have already explained, for each fold how to choose the number of hidden nodes for the ﬁve MLPs that will be required in the feature extraction module of NEUROSVM. [sent-512, score-0.616]
</p><p>69 The selected combinations of SFMs along with the hyper parameters for the twelve data sets are listed in Table 6. [sent-693, score-0.221]
</p><p>70 As an illustration, for fold 1 of Iris, a set of 7 features is used in the classiﬁcation module, which is generated by two selected SFMs each with 2 and 5 hidden nodes. [sent-695, score-0.201]
</p><p>71 The (C, γ) pair within parenthesis followed by the combination shows the regularization parameter 607  G HANTY, PAUL AND PAL  (C) and spread (γ) of the RBF kernel for SVM classiﬁers in the classiﬁcation module that are selected by the cross-validation. [sent-696, score-0.3]
</p><p>72 It is probably because the hidden layers of neural networks are more suited for linear classiﬁcation than the original inputs, so a higher γ (less non-linearity) is more appropriate. [sent-701, score-0.183]
</p><p>73 For these four data sets dimensionality is reduced in the classiﬁcation module of NEUROSVM. [sent-703, score-0.28]
</p><p>74 The dimensions of the four data sets, WDBC, Sonar, Ionosphere and Optdigits, in the classiﬁcation module of NEUROSVM are reduced by 16. [sent-704, score-0.259]
</p><p>75 4 P ERFORMANCE COMPARISON OF NEUROSVM AND S TANDARD E NSEMBLE M ETHODS  WITH THE  BASELINE C LASSIFIERS  We compare the performance of NEUROSVM with MLP, SVM as well as two existing neural ensemble methods. [sent-718, score-0.199]
</p><p>76 The majority voting and averaging are simple yet effective ensemble methods. [sent-719, score-0.305]
</p><p>77 In Table 7, majority voting ensemble method is denoted by MVOTING while the average ensemble method is denoted by AVERAGING. [sent-721, score-0.438]
</p><p>78 The results in Table 7 show that based on the paired t-test for Group A data sets and McNemar test for Group B data sets NEUROSVM is signiﬁcantly better than the baseline classiﬁers for 11 data sets when compared with MLP and for 6 data sets when compared with SVM. [sent-722, score-0.183]
</p><p>79 From Table 7 we see that NEUROSVM performs signiﬁcantly better than the standard ensemble methods for 11 data sets when compared with majority voting and for 10 data sets when compared with averaging. [sent-724, score-0.304]
</p><p>80 No data set is found where two baseline classiﬁers (MLP and SVM) or two ensemble methods perform better than NEUROSVM. [sent-733, score-0.22]
</p><p>81 Based on the paired t-test for Group A data sets and McNemar test for Group B data sets, Table 9 reveals that NEUROSVM is signiﬁcantly better than 609  G HANTY, PAUL AND PAL  Baseline classiﬁers Standard ensemble methods Data set NEUROSVM MLP SVM MVOTING AVERAGING Iris 0. [sent-912, score-0.252]
</p><p>82 Table 7: Performance comparison of NEUROSVM with baseline classiﬁers and standard ensemble methods  the standard ensemble methods for 8 data sets when compared with majority voting and for 7 data sets when compared with averaging. [sent-1019, score-0.524]
</p><p>83 5 P ERFORMANCE C OMPARISON OF OTHER VARIANTS OF NEUROSVM C LASSIFIERS AND S TANDARD E NSEMBLE M ETHODS  WITH  BASELINE  As stated earlier, for the proposed architecture, in the classiﬁcation module we can use other tools also. [sent-1024, score-0.259]
</p><p>84 Here we demonstrate the effect of using MLP and RBF neural networks in the classiﬁcation module instead of SVM. [sent-1025, score-0.325]
</p><p>85 The combination of SFMs and the number of hidden nodes for MLP and RBF networks in the classiﬁcation module are selected using double ten-fold cross-validation for Group A data sets and using ten-fold cross-validation for Group B data sets. [sent-1027, score-0.505]
</p><p>86 Now we compare the performance of baseline classiﬁers and two ensemble methods with the two new variants of NEUROSVM, that is, NMLP and NRBF, by statistical test. [sent-1034, score-0.22]
</p><p>87 When compared with the standard ensemble methods the NRBF is found to perform signiﬁcantly better for majority of the data sets. [sent-1046, score-0.211]
</p><p>88 We ﬁnd that the NMLP is signiﬁcantly better than the standard ensemble methods for 3 data sets (Pima, Pendigits and Sat. [sent-1049, score-0.197]
</p><p>89 So, the proposed method consistently works better than baseline classiﬁers and standard ensemble methods. [sent-1185, score-0.22]
</p><p>90 Table 11: Performance comparison of NMLP with baseline classiﬁers and standard ensemble methods Baseline classiﬁers Standard ensemble methods Data set NRBF MLP SVM MVOTING AVERAGING Iris 0. [sent-1293, score-0.396]
</p><p>91 Table 12: Performance comparison of NRBF with baseline classiﬁers and standard ensemble methods  3. [sent-1400, score-0.22]
</p><p>92 We choose these kernels also for SVMs in the classiﬁcation module of NEUROSVM. [sent-1415, score-0.279]
</p><p>93 The ﬁrst module is the feature extraction module (FM), while the second module is the classiﬁcation module (CM). [sent-1469, score-1.141]
</p><p>94 The performance of NEUROSVM is also better than the ensemble methods based on majority voting and averaging. [sent-1503, score-0.262]
</p><p>95 • Typically the number of nodes in the hidden layer of MLPs is much smaller than the number of the input nodes, and one does not need many feature extraction sub-modules. [sent-1506, score-0.358]
</p><p>96 Hence, the dimensionality of the input for the SVMs (or MLP/RBF) in the classiﬁcation module can be reduced compared to the original dimension of the input. [sent-1507, score-0.259]
</p><p>97 end for /* end for j */ The average validation error for an architecture a related to XTi of the original fold (XTi , XTei ) is ea = ¯i  1 10  10  ∑ ea . [sent-1562, score-0.247]
</p><p>98 Find projected data of (XTi , XTei ) from hidden layer of above 5 MLPs and hence we get 5 SFMs. [sent-1621, score-0.183]
</p><p>99 /* To select the best combination among 31 combinations and (C, γ) pair of RBF kernel of SVM in the classiﬁcation module we perform ten-fold cross-validation experiment */ 15. [sent-1628, score-0.313]
</p><p>100 Majority vote and decision template based ensemble classiﬁers trained on event related potentials for early diagnosis of alzheimers’s disease. [sent-1889, score-0.235]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('neurosvm', 0.597), ('mlp', 0.365), ('module', 0.259), ('ensemble', 0.176), ('mlps', 0.158), ('pal', 0.146), ('sfms', 0.146), ('mvoting', 0.119), ('xti', 0.116), ('hanty', 0.106), ('svm', 0.105), ('optdigits', 0.101), ('pendigits', 0.101), ('wdbc', 0.101), ('educe', 0.1), ('pima', 0.096), ('sonar', 0.096), ('rbf', 0.094), ('nmlp', 0.093), ('nrbf', 0.093), ('hidden', 0.093), ('layer', 0.09), ('lymph', 0.09), ('fold', 0.089), ('rchitecture', 0.085), ('glass', 0.085), ('hoice', 0.085), ('group', 0.084), ('extraction', 0.083), ('architecture', 0.079), ('hyper', 0.079), ('ffect', 0.076), ('ionosphere', 0.074), ('nodes', 0.07), ('twelve', 0.07), ('vehicle', 0.069), ('iris', 0.065), ('xtei', 0.06), ('paul', 0.058), ('fm', 0.057), ('mcnemar', 0.053), ('mitra', 0.053), ('ernel', 0.052), ('voting', 0.051), ('classi', 0.05), ('sfmi', 0.046), ('cm', 0.045), ('baseline', 0.044), ('networks', 0.043), ('averaging', 0.043), ('svms', 0.042), ('ers', 0.04), ('datapreparation', 0.04), ('runmlp', 0.04), ('runsvm', 0.04), ('architectures', 0.036), ('modules', 0.035), ('validation', 0.035), ('majority', 0.035), ('network', 0.034), ('trained', 0.033), ('old', 0.033), ('train', 0.032), ('combinations', 0.032), ('zv', 0.03), ('test', 0.029), ('outer', 0.027), ('maqsood', 0.027), ('hybrid', 0.026), ('paired', 0.026), ('table', 0.026), ('training', 0.026), ('vote', 0.026), ('convolution', 0.026), ('fusion', 0.025), ('layers', 0.024), ('modular', 0.024), ('neural', 0.023), ('kernel', 0.022), ('ea', 0.022), ('feature', 0.022), ('sets', 0.021), ('choices', 0.021), ('er', 0.021), ('subtasks', 0.02), ('multilayer', 0.02), ('adankon', 0.02), ('calcutta', 0.02), ('ghanty', 0.02), ('guimaraes', 0.02), ('haykin', 0.02), ('islam', 0.02), ('lidar', 0.02), ('nns', 0.02), ('sfm', 0.02), ('yper', 0.02), ('kernels', 0.02), ('selected', 0.019), ('zt', 0.019), ('india', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="58-tfidf-1" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>Author: Pradip Ghanty, Samrat Paul, Nikhil R. Pal</p><p>Abstract: In this paper we propose a new multilayer classiﬁer architecture. The proposed hybrid architecture has two cascaded modules: feature extraction module and classiﬁcation module. In the feature extraction module we use the multilayered perceptron (MLP) neural networks, although other tools such as radial basis function (RBF) networks can be used. In the classiﬁcation module we use support vector machines (SVMs)—here also other tool such as MLP or RBF can be used. The feature extraction module has several sub-modules each of which is expected to extract features capturing the discriminating characteristics of different areas of the input space. The classiﬁcation module classiﬁes the data based on the extracted features. The resultant architecture with MLP in feature extraction module and SVM in classiﬁcation module is called NEUROSVM. The NEUROSVM is tested on twelve benchmark data sets and the performance of the NEUROSVM is found to be better than both MLP and SVM. We also compare the performance of proposed architecture with that of two ensemble methods: majority voting and averaging. Here also the NEUROSVM is found to perform better than these two ensemble methods. Further we explore the use of MLP and RBF in the classiﬁcation module of the proposed architecture. The most attractive feature of NEUROSVM is that it practically eliminates the severe dependency of SVM on the choice of kernel. This has been veriﬁed with respect to both linear and non-linear kernels. We have also demonstrated that for the feature extraction module, the full training of MLPs is not needed. Keywords: feature extraction, neural networks (NNs), support vector machines (SVMs), hybrid system, majority voting, averaging c 2009 Pradip Ghanty, Samrat Paul and Nikhil R. Pal. G HANTY, PAUL AND PAL</p><p>2 0.084732331 <a title="58-tfidf-2" href="./jmlr-2009-Exploring_Strategies_for_Training_Deep_Neural_Networks.html">33 jmlr-2009-Exploring Strategies for Training Deep Neural Networks</a></p>
<p>Author: Hugo Larochelle, Yoshua Bengio, Jérôme Louradour, Pascal Lamblin</p><p>Abstract: Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments conﬁrm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms. Keywords: artiﬁcial neural networks, deep belief networks, restricted Boltzmann machines, autoassociators, unsupervised learning</p><p>3 0.071818061 <a title="58-tfidf-3" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>Author: Dirk Gorissen, Tom Dhaene, Filip De Turck</p><p>Abstract: Due to the scale and computational complexity of currently used simulation codes, global surrogate (metamodels) models have become indispensable tools for exploring and understanding the design space. Due to their compact formulation they are cheap to evaluate and thus readily facilitate visualization, design space exploration, rapid prototyping, and sensitivity analysis. They can also be used as accurate building blocks in design packages or larger simulation environments. Consequently, there is great interest in techniques that facilitate the construction of such approximation models while minimizing the computational cost and maximizing model accuracy. Many surrogate model types exist (Support Vector Machines, Kriging, Neural Networks, etc.) but no type is optimal in all circumstances. Nor is there any hard theory available that can help make this choice. In this paper we present an automatic approach to the model type selection problem. We describe an adaptive global surrogate modeling environment with adaptive sampling, driven by speciated evolution. Different model types are evolved cooperatively using a Genetic Algorithm (heterogeneous evolution) and compete to approximate the iteratively selected data. In this way the optimal model type and complexity for a given data set or simulation code can be dynamically determined. Its utility and performance is demonstrated on a number of problems where it outperforms traditional sequential execution of each model type. Keywords: model type selection, genetic algorithms, global surrogate modeling, function approximation, active learning, adaptive sampling</p><p>4 0.055664536 <a title="58-tfidf-4" href="./jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems.html">8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</a></p>
<p>Author: Luciana Ferrer, Kemal Sönmez, Elizabeth Shriberg</p><p>Abstract: We present a method for training support vector machine (SVM)-based classiﬁcation systems for combination with other classiﬁcation systems designed for the same task. Ideally, a new system should be designed such that, when combined with existing systems, the resulting performance is optimized. We present a simple model for this problem and use the understanding gained from this analysis to propose a method to achieve better combination performance when training SVM systems. We include a regularization term in the SVM objective function that aims to reduce the average class-conditional covariance between the resulting scores and the scores produced by the existing systems, introducing a trade-off between such covariance and the system’s individual performance. That is, the new system “takes one for the team”, falling somewhat short of its best possible performance in order to increase the diversity of the ensemble. We report results on the NIST 2005 and 2006 speaker recognition evaluations (SREs) for a variety of subsystems. We show a gain of 19% on the equal error rate (EER) of a combination of four systems when applying the proposed method with respect to the performance obtained when the four systems are trained independently of each other. Keywords: system combination, ensemble diversity, multiple classiﬁer systems, support vector machines, speaker recognition, kernel methods ∗. This author performed part of the work presented in this paper while at the Information Systems Laboratory, Department of Electrical Engineering, Stanford University. c 2009 Luciana Ferrer, Kemal S¨ nmez and Elizabeth Shriberg. o ¨ F ERRER , S ONMEZ AND S HRIBERG</p><p>5 0.050053194 <a title="58-tfidf-5" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>Author: Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi, Luca Cazzanti</p><p>Abstract: This paper reviews and extends the ﬁeld of similarity-based classiﬁcation, presenting new analyses, algorithms, data sets, and a comprehensive set of experimental results for a rich collection of classiﬁcation problems. Speciﬁcally, the generalizability of using similarities as features is analyzed, design goals and methods for weighting nearest-neighbors for similarity-based learning are proposed, and different methods for consistently converting similarities into kernels are compared. Experiments on eight real data sets compare eight approaches and their variants to similarity-based learning. Keywords: similarity, dissimilarity, similarity-based learning, indeﬁnite kernels</p><p>6 0.04904696 <a title="58-tfidf-6" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>7 0.048994105 <a title="58-tfidf-7" href="./jmlr-2009-Hybrid_MPI_OpenMP_Parallel_Linear_Support_Vector_Machine_Training.html">39 jmlr-2009-Hybrid MPI OpenMP Parallel Linear Support Vector Machine Training</a></p>
<p>8 0.034133248 <a title="58-tfidf-8" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>9 0.030722851 <a title="58-tfidf-9" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>10 0.030239351 <a title="58-tfidf-10" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>11 0.030215178 <a title="58-tfidf-11" href="./jmlr-2009-Incorporating_Functional_Knowledge_in_Neural_Networks.html">42 jmlr-2009-Incorporating Functional Knowledge in Neural Networks</a></p>
<p>12 0.029264826 <a title="58-tfidf-12" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>13 0.028217837 <a title="58-tfidf-13" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>14 0.027603922 <a title="58-tfidf-14" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>15 0.026857221 <a title="58-tfidf-15" href="./jmlr-2009-Nieme%3A_Large-Scale_Energy-Based_Models%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">60 jmlr-2009-Nieme: Large-Scale Energy-Based Models    (Machine Learning Open Source Software Paper)</a></p>
<p>16 0.026345011 <a title="58-tfidf-16" href="./jmlr-2009-Model_Monitor_%28M2%29%3A_Evaluating%2C_Comparing%2C_and_Monitoring_Models%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">56 jmlr-2009-Model Monitor (M2): Evaluating, Comparing, and Monitoring Models    (Machine Learning Open Source Software Paper)</a></p>
<p>17 0.025161803 <a title="58-tfidf-17" href="./jmlr-2009-Estimating_Labels_from_Label_Proportions.html">29 jmlr-2009-Estimating Labels from Label Proportions</a></p>
<p>18 0.024055423 <a title="58-tfidf-18" href="./jmlr-2009-Optimized_Cutting_Plane_Algorithm_for_Large-Scale_Risk_Minimization.html">69 jmlr-2009-Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization</a></p>
<p>19 0.023975147 <a title="58-tfidf-19" href="./jmlr-2009-Dlib-ml%3A_A_Machine_Learning_Toolkit%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">26 jmlr-2009-Dlib-ml: A Machine Learning Toolkit    (Machine Learning Open Source Software Paper)</a></p>
<p>20 0.023601284 <a title="58-tfidf-20" href="./jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.125), (1, -0.045), (2, 0.088), (3, -0.039), (4, 0.055), (5, -0.07), (6, 0.056), (7, 0.069), (8, 0.089), (9, 0.084), (10, 0.076), (11, 0.059), (12, 0.139), (13, -0.175), (14, -0.109), (15, -0.069), (16, -0.235), (17, 0.178), (18, -0.115), (19, -0.127), (20, 0.076), (21, -0.074), (22, -0.113), (23, -0.076), (24, -0.182), (25, -0.053), (26, -0.008), (27, 0.171), (28, -0.056), (29, 0.048), (30, 0.054), (31, 0.122), (32, 0.144), (33, 0.048), (34, -0.003), (35, -0.113), (36, -0.032), (37, 0.062), (38, -0.002), (39, 0.081), (40, -0.022), (41, -0.013), (42, -0.121), (43, 0.028), (44, -0.015), (45, 0.141), (46, -0.03), (47, 0.147), (48, -0.074), (49, -0.234)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91559017 <a title="58-lsi-1" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>Author: Pradip Ghanty, Samrat Paul, Nikhil R. Pal</p><p>Abstract: In this paper we propose a new multilayer classiﬁer architecture. The proposed hybrid architecture has two cascaded modules: feature extraction module and classiﬁcation module. In the feature extraction module we use the multilayered perceptron (MLP) neural networks, although other tools such as radial basis function (RBF) networks can be used. In the classiﬁcation module we use support vector machines (SVMs)—here also other tool such as MLP or RBF can be used. The feature extraction module has several sub-modules each of which is expected to extract features capturing the discriminating characteristics of different areas of the input space. The classiﬁcation module classiﬁes the data based on the extracted features. The resultant architecture with MLP in feature extraction module and SVM in classiﬁcation module is called NEUROSVM. The NEUROSVM is tested on twelve benchmark data sets and the performance of the NEUROSVM is found to be better than both MLP and SVM. We also compare the performance of proposed architecture with that of two ensemble methods: majority voting and averaging. Here also the NEUROSVM is found to perform better than these two ensemble methods. Further we explore the use of MLP and RBF in the classiﬁcation module of the proposed architecture. The most attractive feature of NEUROSVM is that it practically eliminates the severe dependency of SVM on the choice of kernel. This has been veriﬁed with respect to both linear and non-linear kernels. We have also demonstrated that for the feature extraction module, the full training of MLPs is not needed. Keywords: feature extraction, neural networks (NNs), support vector machines (SVMs), hybrid system, majority voting, averaging c 2009 Pradip Ghanty, Samrat Paul and Nikhil R. Pal. G HANTY, PAUL AND PAL</p><p>2 0.54812884 <a title="58-lsi-2" href="./jmlr-2009-Exploring_Strategies_for_Training_Deep_Neural_Networks.html">33 jmlr-2009-Exploring Strategies for Training Deep Neural Networks</a></p>
<p>Author: Hugo Larochelle, Yoshua Bengio, Jérôme Louradour, Pascal Lamblin</p><p>Abstract: Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments conﬁrm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms. Keywords: artiﬁcial neural networks, deep belief networks, restricted Boltzmann machines, autoassociators, unsupervised learning</p><p>3 0.42501324 <a title="58-lsi-3" href="./jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems.html">8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</a></p>
<p>Author: Luciana Ferrer, Kemal Sönmez, Elizabeth Shriberg</p><p>Abstract: We present a method for training support vector machine (SVM)-based classiﬁcation systems for combination with other classiﬁcation systems designed for the same task. Ideally, a new system should be designed such that, when combined with existing systems, the resulting performance is optimized. We present a simple model for this problem and use the understanding gained from this analysis to propose a method to achieve better combination performance when training SVM systems. We include a regularization term in the SVM objective function that aims to reduce the average class-conditional covariance between the resulting scores and the scores produced by the existing systems, introducing a trade-off between such covariance and the system’s individual performance. That is, the new system “takes one for the team”, falling somewhat short of its best possible performance in order to increase the diversity of the ensemble. We report results on the NIST 2005 and 2006 speaker recognition evaluations (SREs) for a variety of subsystems. We show a gain of 19% on the equal error rate (EER) of a combination of four systems when applying the proposed method with respect to the performance obtained when the four systems are trained independently of each other. Keywords: system combination, ensemble diversity, multiple classiﬁer systems, support vector machines, speaker recognition, kernel methods ∗. This author performed part of the work presented in this paper while at the Information Systems Laboratory, Department of Electrical Engineering, Stanford University. c 2009 Luciana Ferrer, Kemal S¨ nmez and Elizabeth Shriberg. o ¨ F ERRER , S ONMEZ AND S HRIBERG</p><p>4 0.38492185 <a title="58-lsi-4" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>Author: Dirk Gorissen, Tom Dhaene, Filip De Turck</p><p>Abstract: Due to the scale and computational complexity of currently used simulation codes, global surrogate (metamodels) models have become indispensable tools for exploring and understanding the design space. Due to their compact formulation they are cheap to evaluate and thus readily facilitate visualization, design space exploration, rapid prototyping, and sensitivity analysis. They can also be used as accurate building blocks in design packages or larger simulation environments. Consequently, there is great interest in techniques that facilitate the construction of such approximation models while minimizing the computational cost and maximizing model accuracy. Many surrogate model types exist (Support Vector Machines, Kriging, Neural Networks, etc.) but no type is optimal in all circumstances. Nor is there any hard theory available that can help make this choice. In this paper we present an automatic approach to the model type selection problem. We describe an adaptive global surrogate modeling environment with adaptive sampling, driven by speciated evolution. Different model types are evolved cooperatively using a Genetic Algorithm (heterogeneous evolution) and compete to approximate the iteratively selected data. In this way the optimal model type and complexity for a given data set or simulation code can be dynamically determined. Its utility and performance is demonstrated on a number of problems where it outperforms traditional sequential execution of each model type. Keywords: model type selection, genetic algorithms, global surrogate modeling, function approximation, active learning, adaptive sampling</p><p>5 0.36524719 <a title="58-lsi-5" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>Author: Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi, Luca Cazzanti</p><p>Abstract: This paper reviews and extends the ﬁeld of similarity-based classiﬁcation, presenting new analyses, algorithms, data sets, and a comprehensive set of experimental results for a rich collection of classiﬁcation problems. Speciﬁcally, the generalizability of using similarities as features is analyzed, design goals and methods for weighting nearest-neighbors for similarity-based learning are proposed, and different methods for consistently converting similarities into kernels are compared. Experiments on eight real data sets compare eight approaches and their variants to similarity-based learning. Keywords: similarity, dissimilarity, similarity-based learning, indeﬁnite kernels</p><p>6 0.34624422 <a title="58-lsi-6" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>7 0.28983676 <a title="58-lsi-7" href="./jmlr-2009-Bayesian_Network_Structure_Learning_by_Recursive_Autonomy_Identification.html">11 jmlr-2009-Bayesian Network Structure Learning by Recursive Autonomy Identification</a></p>
<p>8 0.26922497 <a title="58-lsi-8" href="./jmlr-2009-Hybrid_MPI_OpenMP_Parallel_Linear_Support_Vector_Machine_Training.html">39 jmlr-2009-Hybrid MPI OpenMP Parallel Linear Support Vector Machine Training</a></p>
<p>9 0.24221796 <a title="58-lsi-9" href="./jmlr-2009-Incorporating_Functional_Knowledge_in_Neural_Networks.html">42 jmlr-2009-Incorporating Functional Knowledge in Neural Networks</a></p>
<p>10 0.23045024 <a title="58-lsi-10" href="./jmlr-2009-Optimized_Cutting_Plane_Algorithm_for_Large-Scale_Risk_Minimization.html">69 jmlr-2009-Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization</a></p>
<p>11 0.2135382 <a title="58-lsi-11" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>12 0.20804571 <a title="58-lsi-12" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>13 0.19624411 <a title="58-lsi-13" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>14 0.18866004 <a title="58-lsi-14" href="./jmlr-2009-Using_Local_Dependencies_within_Batches_to_Improve_Large_Margin_Classifiers.html">99 jmlr-2009-Using Local Dependencies within Batches to Improve Large Margin Classifiers</a></p>
<p>15 0.17621538 <a title="58-lsi-15" href="./jmlr-2009-Cautious_Collective_Classification.html">15 jmlr-2009-Cautious Collective Classification</a></p>
<p>16 0.17219225 <a title="58-lsi-16" href="./jmlr-2009-CarpeDiem%3A_Optimizing_the_Viterbi_Algorithm_and_Applications_to_Supervised_Sequential_Learning.html">14 jmlr-2009-CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning</a></p>
<p>17 0.16068408 <a title="58-lsi-17" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>18 0.15428613 <a title="58-lsi-18" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>19 0.15296949 <a title="58-lsi-19" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>20 0.14984596 <a title="58-lsi-20" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.017), (11, 0.041), (19, 0.011), (21, 0.013), (26, 0.025), (38, 0.032), (52, 0.054), (55, 0.027), (58, 0.053), (66, 0.091), (68, 0.02), (78, 0.436), (87, 0.015), (90, 0.053), (96, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.66679233 <a title="58-lda-1" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>Author: Pradip Ghanty, Samrat Paul, Nikhil R. Pal</p><p>Abstract: In this paper we propose a new multilayer classiﬁer architecture. The proposed hybrid architecture has two cascaded modules: feature extraction module and classiﬁcation module. In the feature extraction module we use the multilayered perceptron (MLP) neural networks, although other tools such as radial basis function (RBF) networks can be used. In the classiﬁcation module we use support vector machines (SVMs)—here also other tool such as MLP or RBF can be used. The feature extraction module has several sub-modules each of which is expected to extract features capturing the discriminating characteristics of different areas of the input space. The classiﬁcation module classiﬁes the data based on the extracted features. The resultant architecture with MLP in feature extraction module and SVM in classiﬁcation module is called NEUROSVM. The NEUROSVM is tested on twelve benchmark data sets and the performance of the NEUROSVM is found to be better than both MLP and SVM. We also compare the performance of proposed architecture with that of two ensemble methods: majority voting and averaging. Here also the NEUROSVM is found to perform better than these two ensemble methods. Further we explore the use of MLP and RBF in the classiﬁcation module of the proposed architecture. The most attractive feature of NEUROSVM is that it practically eliminates the severe dependency of SVM on the choice of kernel. This has been veriﬁed with respect to both linear and non-linear kernels. We have also demonstrated that for the feature extraction module, the full training of MLPs is not needed. Keywords: feature extraction, neural networks (NNs), support vector machines (SVMs), hybrid system, majority voting, averaging c 2009 Pradip Ghanty, Samrat Paul and Nikhil R. Pal. G HANTY, PAUL AND PAL</p><p>2 0.29718193 <a title="58-lda-2" href="./jmlr-2009-Optimized_Cutting_Plane_Algorithm_for_Large-Scale_Risk_Minimization.html">69 jmlr-2009-Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization</a></p>
<p>Author: Vojtěch Franc, Sören Sonnenburg</p><p>Abstract: We have developed an optimized cutting plane algorithm (OCA) for solving large-scale risk minimization problems. We prove that the number of iterations OCA requires to converge to a ε precise solution is approximately linear in the sample size. We also derive OCAS, an OCA-based linear binary Support Vector Machine (SVM) solver, and OCAM, a linear multi-class SVM solver. In an extensive empirical evaluation we show that OCAS outperforms current state-of-the-art SVM solvers like SVMlight , SVMperf and BMRM, achieving speedup factor more than 1,200 over SVMlight on some data sets and speedup factor of 29 over SVMperf , while obtaining the same precise support vector solution. OCAS, even in the early optimization steps, often shows faster convergence than the currently prevailing approximative methods in this domain, SGD and Pegasos. In addition, our proposed linear multi-class SVM solver, OCAM, achieves speedups of factor of up to 10 compared to SVMmulti−class . Finally, we use OCAS and OCAM in two real-world applications, the problem of human acceptor splice site detection and malware detection. Effectively parallelizing OCAS, we achieve state-of-the-art results on an acceptor splice site recognition problem only by being able to learn from all the available 50 million examples in a 12-million-dimensional feature space. Source code, data sets and scripts to reproduce the experiments are available at http://cmp.felk.cvut.cz/˜xfrancv/ocas/html/. Keywords: risk minimization, linear support vector machine, multi-class classiﬁcation, binary classiﬁcation, large-scale learning, parallelization</p><p>3 0.29434153 <a title="58-lda-3" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>Author: Junning Li, Z. Jane Wang</p><p>Abstract: In real world applications, graphical statistical models are not only a tool for operations such as classiﬁcation or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-speciﬁed level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-speciﬁed level, and a heuristic modiﬁcation of the method is able to control the FDR more accurately around the user-speciﬁed level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models. Keywords: Bayesian networks, false discovery rate, PC algorithm, directed acyclic graph, skeleton</p><p>4 0.28948283 <a title="58-lda-4" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>Author: Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi, Luca Cazzanti</p><p>Abstract: This paper reviews and extends the ﬁeld of similarity-based classiﬁcation, presenting new analyses, algorithms, data sets, and a comprehensive set of experimental results for a rich collection of classiﬁcation problems. Speciﬁcally, the generalizability of using similarities as features is analyzed, design goals and methods for weighting nearest-neighbors for similarity-based learning are proposed, and different methods for consistently converting similarities into kernels are compared. Experiments on eight real data sets compare eight approaches and their variants to similarity-based learning. Keywords: similarity, dissimilarity, similarity-based learning, indeﬁnite kernels</p><p>5 0.28901029 <a title="58-lda-5" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider regularized support vector machines (SVMs) and show that they are precisely equivalent to a new robust optimization formulation. We show that this equivalence of robust optimization and regularization has implications for both algorithms, and analysis. In terms of algorithms, the equivalence suggests more general SVM-like algorithms for classiﬁcation that explicitly build in protection to noise, and at the same time control overﬁtting. On the analysis front, the equivalence of robustness and regularization provides a robust optimization interpretation for the success of regularized SVMs. We use this new robustness interpretation of SVMs to give a new proof of consistency of (kernelized) SVMs, thus establishing robustness as the reason regularized SVMs generalize well. Keywords: robustness, regularization, generalization, kernel, support vector machine</p><p>6 0.28896558 <a title="58-lda-6" href="./jmlr-2009-Bounded_Kernel-Based_Online_Learning.html">13 jmlr-2009-Bounded Kernel-Based Online Learning</a></p>
<p>7 0.28659508 <a title="58-lda-7" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>8 0.28617117 <a title="58-lda-8" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>9 0.28543004 <a title="58-lda-9" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>10 0.28478009 <a title="58-lda-10" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>11 0.28473791 <a title="58-lda-11" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>12 0.28445593 <a title="58-lda-12" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>13 0.28328803 <a title="58-lda-13" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>14 0.28326529 <a title="58-lda-14" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>15 0.28267813 <a title="58-lda-15" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>16 0.28257301 <a title="58-lda-16" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>17 0.28215328 <a title="58-lda-17" href="./jmlr-2009-Settable_Systems%3A_An_Extension_of_Pearl%27s_Causal_Model_with_Optimization%2C_Equilibrium%2C_and_Learning.html">85 jmlr-2009-Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning</a></p>
<p>18 0.28175306 <a title="58-lda-18" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>19 0.28169298 <a title="58-lda-19" href="./jmlr-2009-Improving_the_Reliability_of_Causal_Discovery_from_Small_Data_Sets_Using_Argumentation%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">41 jmlr-2009-Improving the Reliability of Causal Discovery from Small Data Sets Using Argumentation    (Special Topic on Causality)</a></p>
<p>20 0.28124887 <a title="58-lda-20" href="./jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">70 jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
