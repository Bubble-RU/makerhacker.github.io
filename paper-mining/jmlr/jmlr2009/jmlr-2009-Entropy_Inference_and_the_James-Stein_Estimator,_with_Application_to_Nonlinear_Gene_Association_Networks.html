<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 jmlr-2009-Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-28" href="#">jmlr2009-28</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>28 jmlr-2009-Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks</h1>
<br/><p>Source: <a title="jmlr-2009-28-pdf" href="http://jmlr.org/papers/volume10/hausser09a/hausser09a.pdf">pdf</a></p><p>Author: Jean Hausser, Korbinian Strimmer</p><p>Abstract: We present a procedure for effective estimation of entropy and mutual information from smallsample data, and apply it to the problem of inferring high-dimensional gene association networks. SpeciÄ?Ĺš cally, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efÄ?Ĺš cient statistically as well as computationally. Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling. We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data. A computer program is available that implements the proposed shrinkage estimator. Keywords: entropy, shrinkage estimation, James-Stein estimator, Ă˘&euro;&oelig;small n, large pĂ˘&euro;? setting, mutual information, gene association network</p><p>Reference: <a title="jmlr-2009-28-reference" href="../jmlr2009_reference/jmlr-2009-Entropy_Inference_and_the_James-Stein_Estimator%2C_with_Application_to_Nonlinear_Gene_Association_Networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 16Ă˘&euro;&ldquo;18 D-04107 Leipzig, Germany  Editor: Xiaotong Shen  Abstract We present a procedure for effective estimation of entropy and mutual information from smallsample data, and apply it to the problem of inferring high-dimensional gene association networks. [sent-6, score-0.723]
</p><p>2 Ĺš cally, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efÄ? [sent-8, score-0.365]
</p><p>3 Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling. [sent-10, score-0.334]
</p><p>4 coli gene expression data and computing an entropy-based gene-association network from gene expression data. [sent-12, score-0.383]
</p><p>5 A computer program is available that implements the proposed shrinkage estimator. [sent-13, score-0.365]
</p><p>6 Keywords: entropy, shrinkage estimation, James-Stein estimator, Ă˘&euro;&oelig;small n, large pĂ˘&euro;? [sent-14, score-0.365]
</p><p>7 Here we focus on estimating entropy from small-sample data, with applications in genomics and gene network inference in mind (Margolin et al. [sent-19, score-0.467]
</p><p>8 In this setting, the Shannon entropy in natural units is given by1 p  H = Ă˘&circ;&rsquo; Ă˘&circ;&lsquo; Ă&#x17D;Â¸k log(Ă&#x17D;Â¸k ). [sent-30, score-0.267]
</p><p>9 (1)  k=1  In practice, the underlying probability mass function are unknown, hence H and Ă&#x17D;Â¸k need to be estimated from observed cell counts yk Ă˘&permil;Ä˝ 0. [sent-31, score-0.336]
</p><p>10 In situations with n Ă˘&permil;Ĺ¤ p, that is, when the dimension is low and when there are many observation, it is easy to infer entropy reliably, and it is well-known that in this case the ML estimator is optimal. [sent-33, score-0.582]
</p><p>11 regime the ML estimator performs very poorly and severely underestimates the true entropy. [sent-37, score-0.315]
</p><p>12 While entropy estimation has a long history tracing back to more than 50 years ago, it is only recently that the speciÄ? [sent-38, score-0.301]
</p><p>13 , 2002) and the Chao-Shen estimator (Chao and Shen, 2003), both of which are now widely considered as benchmarks for the small-sample entropy estimation problem (Vu et al. [sent-41, score-0.616]
</p><p>14 Ĺš cient small-sample entropy estimator based on JamesStein shrinkage (Gruber, 1998). [sent-44, score-0.947]
</p><p>15 Moreover, our procedure simultaneously provides estimates of the entropy and of the cell frequencies suitable for plugging into the Shannon entropy formula (Equation 1). [sent-46, score-0.805]
</p><p>16 Thus, in comparison the estimator we propose is simpler, very efÄ? [sent-47, score-0.315]
</p><p>17 Ĺš cient, and at the same time more versatile than currently available entropy estimators. [sent-48, score-0.267]
</p><p>18 Conventional Methods for Estimating Entropy Entropy estimators can be divided into two groups: i) methods, that rely on estimates of cell frequencies, and ii) estimators, that directly infer entropy without estimating a compatible set of Ă&#x17D;Â¸k . [sent-50, score-0.656]
</p><p>19 1 Maximum Likelihood Estimate The connection between observed counts yk and frequencies Ă&#x17D;Â¸k is given by the multinomial distribution p n! [sent-57, score-0.287]
</p><p>20 The ML estimator of Ă&#x17D;Â¸k maximizes the right hand side of Equation 3 for Ă&lsaquo;&dagger; Ă&lsaquo;&dagger; Ä? [sent-70, score-0.315]
</p><p>21 2 Miller-Madow Estimator Ă&lsaquo;&dagger; Ă&lsaquo;&dagger; While Ă&#x17D;Â¸ML is unbiased, the corresponding plugin entropy estimator H ML is not. [sent-73, score-0.618]
</p><p>22 This is known as the Miller-Madow estimator (Miller, 1955). [sent-75, score-0.315]
</p><p>23 3 Bayesian Estimators Bayesian regularization of cell counts may lead to vast improvements over the ML estimator (Agresti and Hitchcock, 2005). [sent-77, score-0.515]
</p><p>24 , a p as prior, the resulting posterior distribution is also Dirichlet with mean Ă&lsaquo;&dagger; Bayes = yk + ak , Ă&#x17D;Â¸k n+A p where A = Ă˘&circ;&lsquo;k=1 ak . [sent-81, score-0.217]
</p><p>25 Some common choices for ak are listed in Table 1, along with references to the corresponding plugin entropy estimators, p  Ă&lsaquo;&dagger; Bayes log(Ă&#x17D;Â¸Bayes ). [sent-84, score-0.365]
</p><p>26 (1998) SchÄ&sbquo;Ĺşrmann and Grassberger (1996)  Table 1: Common choices for the parameters of the Dirichlet prior in the Bayesian estimators of cell frequencies, and corresponding entropy estimators. [sent-87, score-0.654]
</p><p>27 But, as shown later in this article, choosing inappropriate ak can easily cause the resulting estimator to perform worse than the ML estimator, thereby defeating the originally intended purpose. [sent-91, score-0.377]
</p><p>28 , 2002) avoids overrelying on a particular choice of ak in the Bayes estimator by using a more reÄ? [sent-94, score-0.377]
</p><p>29 Ĺš nite number of components is employed, constructed such that the resulting prior over the entropy is uniform. [sent-98, score-0.297]
</p><p>30 While the NSB estimator is one of the best entropy estimators available at present in terms of statistical properties, using the Dirichlet mixture prior is computationally expensive and somewhat slow for practical applications. [sent-99, score-0.813]
</p><p>31 5 Chao-Shen Estimator Another recently proposed estimator is due to Chao and Shen (2003). [sent-101, score-0.315]
</p><p>32 This approach applies the Horvitz-Thompson estimator (Horvitz and Thompson, 1952) in combination with the Good-Turing correction (Good, 1953; Orlitsky et al. [sent-102, score-0.315]
</p><p>33 , 2003) of the empirical cell probabilities to the problem of entropy estimation. [sent-103, score-0.423]
</p><p>34 A James-Stein Shrinkage Estimator The contribution of this paper is to introduce an entropy estimator that employs James-Stein-type shrinkage at the level of cell frequencies. [sent-108, score-1.103]
</p><p>35 As we will show below, this leads to an entropy estimator that is highly effective, both in terms of statistical accuracy and computational complexity. [sent-109, score-0.582]
</p><p>36 James-Stein-type shrinkage is a simple analytic device to perform regularized high-dimensional inference. [sent-110, score-0.365]
</p><p>37 It is ideally suited for small-sample settings - the original estimator (James and Stein, 1961) considered sample size n = 1. [sent-111, score-0.315]
</p><p>38 A general recipe for constructing shrinkage estimators is given in Appendix A. [sent-112, score-0.61]
</p><p>39 James-Stein shrinkage is based on averaging two very different models: a high-dimensional model with low bias and high variance, and a lower dimensional model with larger bias but smaller variance. [sent-115, score-0.473]
</p><p>40 Considering that Bias(Ă&#x17D;Â¸ML ) = 0 and using the k p Ă&lsaquo;&dagger; unbiased estimator Var(Ă&#x17D;Â¸ML ) = k  Ă&lsaquo;&dagger; Ă&lsaquo;&dagger; Ă&#x17D;Â¸ML (1Ă˘&circ;&rsquo;Ă&#x17D;Â¸ML ) k k nĂ˘&circ;&rsquo;1  we obtain (cf. [sent-120, score-0.347]
</p><p>41 Appendix A) for the shrinkage intensity  p p Ă&lsaquo;&dagger; ML Ă&lsaquo;&dagger; 1 Ă˘&circ;&rsquo; Ă˘&circ;&lsquo;k=1 (Ă&#x17D;Â¸ML )2 k Ă&lsaquo;&dagger; Ă˘&lsaquo;&dagger; = Ă˘&circ;&lsquo;k=1 Var(Ă&#x17D;Â¸k ) = Ă&#x17D;ĹĽ . [sent-121, score-0.438]
</p><p>42 The resulting plugin shrinkage entropy estimate is p Ă&lsaquo;&dagger;k Ă&lsaquo;&dagger;k Ă&lsaquo;&dagger; H Shrink = Ă˘&circ;&rsquo; Ă˘&circ;&lsquo; Ă&#x17D;Â¸Shrink log(Ă&#x17D;Â¸Shrink ). [sent-123, score-0.668]
</p><p>43 (6) k=1  Remark 1 There is a one to one correspondence between the shrinkage and the Bayes estimator. [sent-124, score-0.365]
</p><p>44 This implies that the shrinkage estimator is an k k A empirical Bayes estimator with a data-driven choice of the Ä? [sent-126, score-0.995]
</p><p>45 For every choice of A there exists an equivalent shrinkage intensity Ă&#x17D;ĹĽ. [sent-128, score-0.438]
</p><p>46 In addition, the shrinkage estimator appears to be robust against assuming a larger p than necessary (see scenario 3 in the simulations). [sent-138, score-0.714]
</p><p>47 Remark 4 The shrinkage approach can easily be modiÄ? [sent-139, score-0.365]
</p><p>48 Ĺš ed to allow multiple targets with different shrinkage intensities. [sent-140, score-0.365]
</p><p>49 For instance, using the Good-Turing estimator (Good, 1953; Orlitsky et al. [sent-141, score-0.315]
</p><p>50 Comparative Evaluation of Statistical Properties In order to elucidate the relative strengths and weaknesses of the entropy estimators reviewed in the previous section, we set to benchmark them in a simulation study covering different data generation processes and sampling regimes. [sent-144, score-0.468]
</p><p>51 In each run, we generated a new set of true cell frequencies and subsequently sampled observed counts yk from the corresponding multinomial distribution. [sent-161, score-0.443]
</p><p>52 The resulting counts yk were then supplied to the various Ă&lsaquo;&dagger; entropy and cell frequencies estimators and the squared error Ă˘&circ;&lsquo;1000 (Ă&#x17D;Â¸k Ă˘&circ;&rsquo; Ă&#x17D;Â¸k )2 was computed. [sent-162, score-0.876]
</p><p>53 From i=k the 1000 repetitions we estimated the mean squared error (MSE) of the cell frequencies by averaging over the individual squared errors (except for the NSB, Miller-Madow, and Chao-Shen estimators). [sent-163, score-0.314]
</p><p>54 Ă˘&euro;Ë&tilde; The maximum likelihood and Miller-Madow estimators perform worst, except for scenario 1. [sent-167, score-0.264]
</p><p>55 Furthermore, the bias correction of the Miller-Madow estimator is not particularly effective. [sent-169, score-0.369]
</p><p>56 Ă˘&euro;Ë&tilde; The minimax and 1/p Bayesian estimators tend to perform slightly better than maximum likelihood, but not by much. [sent-170, score-0.243]
</p><p>57 Ă˘&euro;Ë&tilde; The Bayesian estimators with pseudocounts 1/2 and 1 perform very well even for small sample sizes in the scenarios 2 and 3. [sent-171, score-0.234]
</p><p>58 Ă˘&euro;Ë&tilde; Hence, the Bayesian estimators can perform better or worse than the ML estimator, depending on the choice of the prior and on the sampling scenario. [sent-174, score-0.231]
</p><p>59 Ă˘&euro;Ë&tilde; The NSB, the Chao-Shen and the shrinkage estimator all are statistically very efÄ? [sent-175, score-0.68]
</p><p>60 Ă˘&euro;Ë&tilde; The NSB and Chao-Shen estimators are nearly unbiased in scenario 3. [sent-177, score-0.267]
</p><p>61 The three top-performing estimators are the NSB, the Chao-Shen and the prosed shrinkage estimator. [sent-178, score-0.566]
</p><p>62 When it comes to estimating the entropy, these estimators can be considered identical for practical purposes. [sent-179, score-0.233]
</p><p>63 However, the shrinkage estimator is the only one that simultaneously estimates cell frequencies suitable for use with the Shannon entropy formula (Equation 1), and it does so with high accuracy even for small samples. [sent-180, score-1.218]
</p><p>64 In comparison, the NSB estimator is by far the slowest method: in our simulations, the shrinkage estimator was faster by a factor of 1000. [sent-181, score-0.995]
</p><p>65 47  Bias entropy  0 10  Dirichlet a=1, p=1000  0  probability  ML 1/2 1 1/p minimax Shrink  0. [sent-203, score-0.309]
</p><p>66 The estimators are compared in terms of MSE of the underlying cell frequencies (except for Miller-Madow, NSB, Chao-Shen) and according to MSE and Bias of the estimated entropies. [sent-214, score-0.515]
</p><p>67 Application to Statistical Learning of Nonlinear Gene Association Networks In this section we illustrate how the shrinkage entropy estimator can be applied to the problem of inferring regulatory interactions between genes through estimating the nonlinear association network. [sent-218, score-1.195]
</p><p>68 The starting point of these two methods is to compute the mutual information MI(X,Y ) for all pairs of genes X and Y , where X and Y represent the expression levels of the two genes for instance. [sent-235, score-0.4]
</p><p>69 2 Therefore, mutual information is a natural measure of the association between genes, regardless whether linear or nonlinear in nature. [sent-239, score-0.248]
</p><p>70 2 Estimation of Mutual Information To construct an entropy network, we Ä? [sent-241, score-0.267]
</p><p>71 The entropy representation MI(X,Y ) = H(X) + H(Y ) Ă˘&circ;&rsquo; H(X,Y ), 1476  (8)  E NTROPY I NFERENCE AND THE JAMES -S TEIN E STIMATOR  shows that MI can be computed from the joint and marginal entropies of the two genes X and Y . [sent-243, score-0.363]
</p><p>72 For gene expression data the estimation of MI and the underlying entropies is challenging due to the small sample size, which requires the use of a regularized entropy estimator such as the shrinkage approach we propose here. [sent-247, score-1.117]
</p><p>73 Ă˘&euro;Ë&tilde; Next, we estimate the p = K 2 cell frequencies of the K Ä&sbquo;&mdash; K contingency table for each pair X and Y using the shrinkage approach (Eqs. [sent-251, score-0.636]
</p><p>74 Ă˘&euro;Ë&tilde; Finally, from the estimated cell frequencies we calculate H(X), H(Y ), H(X,Y ) and the desired MI(X,Y ). [sent-254, score-0.314]
</p><p>75 Coli Stress Response Data  ARACNEĂ˘&circ;&rsquo;processed MIs  3000 0  0  1000  2000  Frequency  400 200  Frequency  600  4000  800  5000  MI shrinkage estimates  0. [sent-257, score-0.365]
</p><p>76 5  mutual information  Figure 2: Left: Distribution of estimated mutual information values for all 5151 gene pairs of the E. [sent-268, score-0.595]
</p><p>77 For each pair, the mutual information was based on an estimated 16 Ä&sbquo;&mdash; 16 contingency table, hence p = 256. [sent-281, score-0.251]
</p><p>78 As the number of time points is n = 9, this is a strongly undersampled situation which requires the use of a regularized estimate of entropy and mutual information. [sent-282, score-0.51]
</p><p>79 The distribution of the shrinkage estimates of mutual information for all 5151 gene pairs is shown in the left side of Figure 2. [sent-283, score-0.709]
</p><p>80 The right hand side depicts the distribution of mutual information values after applying the ARACNE procedure, which yields 112 gene pairs with nonzero MIs. [sent-284, score-0.344]
</p><p>81 Discussion We proposed a James-Stein-type shrinkage estimator for inferring entropy and mutual information from small samples. [sent-298, score-1.193]
</p><p>82 In terms of versatility, our estimator has two distinct advantages over the NSB and Chao-Shen estimators. [sent-301, score-0.315]
</p><p>83 Hence, our estimator suggests itself for applications in large scale estimation problems. [sent-305, score-0.349]
</p><p>84 In short, we believe the proposed small-sample entropy estimator will be a valuable contribution to the growing toolbox of machine learning and statistics procedures for high-dimensional data analysis. [sent-309, score-0.582]
</p><p>85 coli data inferred by the ARACNE algorithm based on shrinkage estimates of entropy and mutual information. [sent-315, score-0.919]
</p><p>86 Recipe For Constructing James-Stein-type Shrinkage Estimators The original James-Stein estimator (James and Stein, 1961) was proposed to estimate the mean of a multivariate normal distribution from a single (n = 1! [sent-321, score-0.315]
</p><p>87 Ĺš cally, if x is a sample from Np (Ă&sbquo;Äž, I) then James-Stein estimator is given by Ă&sbquo;ÄžJS = (1 Ă˘&circ;&rsquo; Ă&lsaquo;&dagger;k  pĂ˘&circ;&rsquo;2 )x . [sent-324, score-0.315]
</p><p>88 p 2 k Ă˘&circ;&lsquo;k=1 xk  Intriguingly, this estimator outperforms the maximum likelihood estimator Ă&sbquo;ÄžML = xk in terms of Ă&lsaquo;&dagger;k mean squared error if the dimension is p Ă˘&permil;Ä˝ 3. [sent-325, score-0.659]
</p><p>89 Hence, the James-Stein estimator dominates the maximum likelihood estimator. [sent-326, score-0.344]
</p><p>90 p Ă&sbquo;Ĺť Ă˘&circ;&lsquo;k=1 (xk Ă˘&circ;&rsquo; x)2  The James-Stein shrinkage principle is very general and can be put to to use in many other high-dimensional settings. [sent-328, score-0.365]
</p><p>91 In the following we summarize a simple recipe for constructing JamesStein-type shrinkage estimators along the lines of SchÄ&sbquo;Â¤fer and Strimmer (2005b) and Opgen-Rhein and Strimmer (2007a). [sent-329, score-0.61]
</p><p>92 A general form of a James-Stein-type shrinkage estimator is given by Ă&lsaquo;&dagger; Shrink = Ă&#x17D;ĹĽĂ&#x17D;Â¸Target + (1 Ă˘&circ;&rsquo; Ă&#x17D;ĹĽ)Ă&#x17D;Â¸. [sent-331, score-0.68]
</p><p>93 A very early version (univariate with zero target) even predates the estimator of James and Stein, see Goodman (1953). [sent-341, score-0.315]
</p><p>94 For the multinormal setting of James and Stein (1961), Equation 9 and Equation 10 reduce to the shrinkage estimator described in Stigler (1990). [sent-342, score-0.68]
</p><p>95 James-Stein shrinkage has an empirical Bayes interpretation (Efron and Morris, 1973). [sent-343, score-0.365]
</p><p>96 Computer Implementation The proposed shrinkage estimators of entropy and mutual information, as well as all other investigated entropy estimators, have been implemented in R (R Development Core Team, 2008). [sent-350, score-1.308]
</p><p>97 ARACNE: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context. [sent-557, score-0.23]
</p><p>98 Accurate ranking of differentially expressed genes by a distribution-free shrinkage approach. [sent-602, score-0.491]
</p><p>99 A shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics. [sent-655, score-0.399]
</p><p>100 Maximum entropy modeling of short sequence motifs with applications to RNA splicing signals. [sent-733, score-0.267]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('shrinkage', 0.365), ('estimator', 0.315), ('ml', 0.278), ('nsb', 0.278), ('entropy', 0.267), ('mutual', 0.208), ('estimators', 0.201), ('aracne', 0.174), ('cell', 0.156), ('mse', 0.149), ('ausser', 0.139), ('trimmer', 0.139), ('gene', 0.136), ('strimmer', 0.132), ('stimator', 0.122), ('tein', 0.122), ('frequencies', 0.115), ('genes', 0.096), ('yk', 0.093), ('nference', 0.093), ('ntropy', 0.093), ('dirichlet', 0.092), ('shrink', 0.09), ('james', 0.083), ('tk', 0.08), ('coli', 0.079), ('fer', 0.073), ('intensity', 0.073), ('chao', 0.069), ('nemenman', 0.069), ('shannon', 0.068), ('mi', 0.063), ('ak', 0.062), ('stein', 0.061), ('mis', 0.059), ('bias', 0.054), ('meyer', 0.053), ('cellular', 0.052), ('freedman', 0.052), ('hausser', 0.052), ('margolin', 0.052), ('bayes', 0.047), ('shen', 0.045), ('counts', 0.044), ('jeffreys', 0.044), ('orlitsky', 0.044), ('recipe', 0.044), ('estimated', 0.043), ('regulatory', 0.042), ('minimax', 0.042), ('target', 0.04), ('association', 0.04), ('jean', 0.04), ('vu', 0.04), ('leipzig', 0.04), ('inferring', 0.038), ('plugin', 0.036), ('var', 0.035), ('gt', 0.035), ('attening', 0.035), ('butte', 0.035), ('fienberg', 0.035), ('hdesm', 0.035), ('holste', 0.035), ('hupb', 0.035), ('krichevsky', 0.035), ('ledoit', 0.035), ('morris', 0.035), ('mrnet', 0.035), ('perks', 0.035), ('rangel', 0.035), ('recombinant', 0.035), ('rmann', 0.035), ('suca', 0.035), ('thompson', 0.035), ('tro', 0.035), ('tuyl', 0.035), ('undersampled', 0.035), ('yeo', 0.035), ('multinomial', 0.035), ('estimation', 0.034), ('scenario', 0.034), ('scenarios', 0.033), ('unbiased', 0.032), ('network', 0.032), ('stress', 0.032), ('cells', 0.032), ('hlmann', 0.032), ('estimating', 0.032), ('sch', 0.032), ('bayesian', 0.031), ('bioinformatics', 0.031), ('prior', 0.03), ('differentially', 0.03), ('kalisch', 0.03), ('basel', 0.03), ('gelman', 0.03), ('rna', 0.03), ('statistician', 0.03), ('likelihood', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="28-tfidf-1" href="./jmlr-2009-Entropy_Inference_and_the_James-Stein_Estimator%2C_with_Application_to_Nonlinear_Gene_Association_Networks.html">28 jmlr-2009-Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks</a></p>
<p>Author: Jean Hausser, Korbinian Strimmer</p><p>Abstract: We present a procedure for effective estimation of entropy and mutual information from smallsample data, and apply it to the problem of inferring high-dimensional gene association networks. SpeciÄ?Ĺš cally, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efÄ?Ĺš cient statistically as well as computationally. Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling. We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data. A computer program is available that implements the proposed shrinkage estimator. Keywords: entropy, shrinkage estimation, James-Stein estimator, Ă˘&euro;&oelig;small n, large pĂ˘&euro;? setting, mutual information, gene association network</p><p>2 0.065297581 <a title="28-tfidf-2" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>Author: Jun Zhu, Eric P. Xing</p><p>Abstract: The standard maximum margin approach for structured prediction lacks a straightforward probabilistic interpretation of the learning scheme and the prediction rule. Therefore its unique advantages such as dual sparseness and kernel tricks cannot be easily conjoined with the merits of a probabilistic model such as Bayesian regularization, model averaging, and ability to model hidden variables. In this paper, we present a new general framework called maximum entropy discrimination Markov networks (MaxEnDNet, or simply, MEDN), which integrates these two approaches and combines and extends their merits. Major innovations of this approach include: 1) It extends the conventional max-entropy discrimination learning of classiﬁcation rules to a new structural maxentropy discrimination paradigm of learning a distribution of Markov networks. 2) It generalizes the extant Markov network structured-prediction rule based on a point estimator of model coefﬁcients to an averaging model akin to a Bayesian predictor that integrates over a learned posterior distribution of model coefﬁcients. 3) It admits ﬂexible entropic regularization of the model during learning. By plugging in different prior distributions of the model coefﬁcients, it subsumes the wellknown maximum margin Markov networks (M3 N) as a special case, and leads to a model similar to an L1 -regularized M3 N that is simultaneously primal and dual sparse, or other new types of Markov networks. 4) It applies a modular learning algorithm that combines existing variational inference techniques and convex-optimization based M3 N solvers as subroutines. Essentially, MEDN can be understood as a jointly maximum likelihood and maximum margin estimate of Markov network. It represents the ﬁrst successful attempt to combine maximum entropy learning (a dual form of maximum likelihood learning) with maximum margin learning of Markov network for structured input/output problems; and the basic principle can be generalized to learning arbi</p><p>3 0.064806864 <a title="28-tfidf-3" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>Author: Eitan Greenshtein, Junyong Park</p><p>Abstract: We consider the problem of classiﬁcation using high dimensional features’ space. In a paper by Bickel and Levina (2004), it is recommended to use naive-Bayes classiﬁers, that is, to treat the features as if they are statistically independent. Consider now a sparse setup, where only a few of the features are informative for classiﬁcation. Fan and Fan (2008), suggested a variable selection and classiﬁcation method, called FAIR. The FAIR method improves the design of naive-Bayes classiﬁers in sparse setups. The improvement is due to reducing the noise in estimating the features’ means. This reduction is since that only the means of a few selected variables should be estimated. We also consider the design of naive Bayes classiﬁers. We show that a good alternative to variable selection is estimation of the means through a certain non parametric empirical Bayes procedure. In sparse setups the empirical Bayes implicitly performs an efﬁcient variable selection. It also adapts very well to non sparse setups, and has the advantage of making use of the information from many “weakly informative” variables, which variable selection type of classiﬁcation procedures give up on using. We compare our method with FAIR and other classiﬁcation methods in simulation for sparse and non sparse setups, and in real data examples involving classiﬁcation of normal versus malignant tissues based on microarray data. Keywords: non parametric empirical Bayes, high dimension, classiﬁcation</p><p>4 0.055187769 <a title="28-tfidf-4" href="./jmlr-2009-Multi-task_Reinforcement_Learning_in_Partially_Observable_Stochastic_Environments.html">57 jmlr-2009-Multi-task Reinforcement Learning in Partially Observable Stochastic Environments</a></p>
<p>Author: Hui Li, Xuejun Liao, Lawrence Carin</p><p>Abstract: We consider the problem of multi-task reinforcement learning (MTRL) in multiple partially observable stochastic environments. We introduce the regionalized policy representation (RPR) to characterize the agent’s behavior in each environment. The RPR is a parametric model of the conditional distribution over current actions given the history of past actions and observations; the agent’s choice of actions is directly based on this conditional distribution, without an intervening model to characterize the environment itself. We propose off-policy batch algorithms to learn the parameters of the RPRs, using episodic data collected when following a behavior policy, and show their linkage to policy iteration. We employ the Dirichlet process as a nonparametric prior over the RPRs across multiple environments. The intrinsic clustering property of the Dirichlet process imposes sharing of episodes among similar environments, which effectively reduces the number of episodes required for learning a good policy in each environment, when data sharing is appropriate. The number of distinct RPRs and the associated clusters (the sharing patterns) are automatically discovered by exploiting the episodic data as well as the nonparametric nature of the Dirichlet process. We demonstrate the effectiveness of the proposed RPR as well as the RPR-based MTRL framework on various problems, including grid-world navigation and multi-aspect target classiﬁcation. The experimental results show that the RPR is a competitive reinforcement learning algorithm in partially observable domains, and the MTRL consistently achieves better performance than single task reinforcement learning. Keywords: reinforcement learning, partially observable Markov decision processes, multi-task learning, Dirichlet processes, regionalized policy representation</p><p>5 0.054050684 <a title="28-tfidf-5" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>Author: Junhui Wang, Xiaotong Shen, Wei Pan</p><p>Abstract: In classiﬁcation, semisupervised learning usually involves a large amount of unlabeled data with only a small number of labeled data. This imposes a great challenge in that it is difﬁcult to achieve good classiﬁcation performance through labeled data alone. To leverage unlabeled data for enhancing classiﬁcation, this article introduces a large margin semisupervised learning method within the framework of regularization, based on an efﬁcient margin loss for unlabeled data, which seeks efﬁcient extraction of the information from unlabeled data for estimating the Bayes decision boundary for classiﬁcation. For implementation, an iterative scheme is derived through conditional expectations. Finally, theoretical and numerical analyses are conducted, in addition to an application to gene function prediction. They suggest that the proposed method enables to recover the performance of its supervised counterpart based on complete data in rates of convergence, when possible. Keywords: difference convex programming, classiﬁcation, nonconvex minimization, regularization, support vectors</p><p>6 0.048035242 <a title="28-tfidf-6" href="./jmlr-2009-Identification_of_Recurrent_Neural_Networks_by_Bayesian_Interrogation_Techniques.html">40 jmlr-2009-Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques</a></p>
<p>7 0.047709692 <a title="28-tfidf-7" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>8 0.046677068 <a title="28-tfidf-8" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>9 0.043094344 <a title="28-tfidf-9" href="./jmlr-2009-Nonextensive_Information_Theoretic_Kernels_on_Measures.html">61 jmlr-2009-Nonextensive Information Theoretic Kernels on Measures</a></p>
<p>10 0.041449547 <a title="28-tfidf-10" href="./jmlr-2009-Estimating_Labels_from_Label_Proportions.html">29 jmlr-2009-Estimating Labels from Label Proportions</a></p>
<p>11 0.038750488 <a title="28-tfidf-11" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>12 0.036573272 <a title="28-tfidf-12" href="./jmlr-2009-Computing_Maximum_Likelihood_Estimates_in_Recursive_Linear_Models_with_Correlated_Errors.html">17 jmlr-2009-Computing Maximum Likelihood Estimates in Recursive Linear Models with Correlated Errors</a></p>
<p>13 0.035576422 <a title="28-tfidf-13" href="./jmlr-2009-An_Algorithm_for_Reading_Dependencies_from_the_Minimal_Undirected_Independence_Map_of_a_Graphoid_that_Satisfies_Weak_Transitivity.html">6 jmlr-2009-An Algorithm for Reading Dependencies from the Minimal Undirected Independence Map of a Graphoid that Satisfies Weak Transitivity</a></p>
<p>14 0.034220397 <a title="28-tfidf-14" href="./jmlr-2009-Marginal_Likelihood_Integrals_for_Mixtures_of_Independence_Models.html">53 jmlr-2009-Marginal Likelihood Integrals for Mixtures of Independence Models</a></p>
<p>15 0.03343071 <a title="28-tfidf-15" href="./jmlr-2009-Adaptive_False_Discovery_Rate_Control_under_Independence_and_Dependence.html">5 jmlr-2009-Adaptive False Discovery Rate Control under Independence and Dependence</a></p>
<p>16 0.033063829 <a title="28-tfidf-16" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>17 0.032779202 <a title="28-tfidf-17" href="./jmlr-2009-Fourier_Theoretic_Probabilistic_Inference_over_Permutations.html">36 jmlr-2009-Fourier Theoretic Probabilistic Inference over Permutations</a></p>
<p>18 0.030746995 <a title="28-tfidf-18" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>19 0.029546095 <a title="28-tfidf-19" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>20 0.029300632 <a title="28-tfidf-20" href="./jmlr-2009-Dlib-ml%3A_A_Machine_Learning_Toolkit%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">26 jmlr-2009-Dlib-ml: A Machine Learning Toolkit    (Machine Learning Open Source Software Paper)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.137), (1, -0.08), (2, 0.07), (3, 0.047), (4, 0.011), (5, -0.025), (6, -0.084), (7, -0.0), (8, 0.011), (9, 0.067), (10, 0.146), (11, 0.066), (12, -0.143), (13, -0.079), (14, 0.149), (15, 0.062), (16, 0.019), (17, -0.093), (18, 0.162), (19, 0.016), (20, -0.174), (21, -0.322), (22, -0.191), (23, 0.104), (24, -0.153), (25, -0.047), (26, 0.204), (27, 0.022), (28, 0.042), (29, -0.085), (30, -0.037), (31, -0.003), (32, -0.218), (33, -0.219), (34, -0.01), (35, -0.005), (36, -0.092), (37, 0.047), (38, -0.042), (39, -0.054), (40, 0.008), (41, -0.025), (42, -0.005), (43, 0.143), (44, -0.052), (45, 0.095), (46, -0.071), (47, -0.002), (48, 0.098), (49, -0.096)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98467851 <a title="28-lsi-1" href="./jmlr-2009-Entropy_Inference_and_the_James-Stein_Estimator%2C_with_Application_to_Nonlinear_Gene_Association_Networks.html">28 jmlr-2009-Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks</a></p>
<p>Author: Jean Hausser, Korbinian Strimmer</p><p>Abstract: We present a procedure for effective estimation of entropy and mutual information from smallsample data, and apply it to the problem of inferring high-dimensional gene association networks. SpeciÄ?Ĺš cally, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efÄ?Ĺš cient statistically as well as computationally. Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling. We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data. A computer program is available that implements the proposed shrinkage estimator. Keywords: entropy, shrinkage estimation, James-Stein estimator, Ă˘&euro;&oelig;small n, large pĂ˘&euro;? setting, mutual information, gene association network</p><p>2 0.44985521 <a title="28-lsi-2" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>Author: Jun Zhu, Eric P. Xing</p><p>Abstract: The standard maximum margin approach for structured prediction lacks a straightforward probabilistic interpretation of the learning scheme and the prediction rule. Therefore its unique advantages such as dual sparseness and kernel tricks cannot be easily conjoined with the merits of a probabilistic model such as Bayesian regularization, model averaging, and ability to model hidden variables. In this paper, we present a new general framework called maximum entropy discrimination Markov networks (MaxEnDNet, or simply, MEDN), which integrates these two approaches and combines and extends their merits. Major innovations of this approach include: 1) It extends the conventional max-entropy discrimination learning of classiﬁcation rules to a new structural maxentropy discrimination paradigm of learning a distribution of Markov networks. 2) It generalizes the extant Markov network structured-prediction rule based on a point estimator of model coefﬁcients to an averaging model akin to a Bayesian predictor that integrates over a learned posterior distribution of model coefﬁcients. 3) It admits ﬂexible entropic regularization of the model during learning. By plugging in different prior distributions of the model coefﬁcients, it subsumes the wellknown maximum margin Markov networks (M3 N) as a special case, and leads to a model similar to an L1 -regularized M3 N that is simultaneously primal and dual sparse, or other new types of Markov networks. 4) It applies a modular learning algorithm that combines existing variational inference techniques and convex-optimization based M3 N solvers as subroutines. Essentially, MEDN can be understood as a jointly maximum likelihood and maximum margin estimate of Markov network. It represents the ﬁrst successful attempt to combine maximum entropy learning (a dual form of maximum likelihood learning) with maximum margin learning of Markov network for structured input/output problems; and the basic principle can be generalized to learning arbi</p><p>3 0.35224676 <a title="28-lsi-3" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>Author: Eitan Greenshtein, Junyong Park</p><p>Abstract: We consider the problem of classiﬁcation using high dimensional features’ space. In a paper by Bickel and Levina (2004), it is recommended to use naive-Bayes classiﬁers, that is, to treat the features as if they are statistically independent. Consider now a sparse setup, where only a few of the features are informative for classiﬁcation. Fan and Fan (2008), suggested a variable selection and classiﬁcation method, called FAIR. The FAIR method improves the design of naive-Bayes classiﬁers in sparse setups. The improvement is due to reducing the noise in estimating the features’ means. This reduction is since that only the means of a few selected variables should be estimated. We also consider the design of naive Bayes classiﬁers. We show that a good alternative to variable selection is estimation of the means through a certain non parametric empirical Bayes procedure. In sparse setups the empirical Bayes implicitly performs an efﬁcient variable selection. It also adapts very well to non sparse setups, and has the advantage of making use of the information from many “weakly informative” variables, which variable selection type of classiﬁcation procedures give up on using. We compare our method with FAIR and other classiﬁcation methods in simulation for sparse and non sparse setups, and in real data examples involving classiﬁcation of normal versus malignant tissues based on microarray data. Keywords: non parametric empirical Bayes, high dimension, classiﬁcation</p><p>4 0.35135847 <a title="28-lsi-4" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>Author: Junhui Wang, Xiaotong Shen, Wei Pan</p><p>Abstract: In classiﬁcation, semisupervised learning usually involves a large amount of unlabeled data with only a small number of labeled data. This imposes a great challenge in that it is difﬁcult to achieve good classiﬁcation performance through labeled data alone. To leverage unlabeled data for enhancing classiﬁcation, this article introduces a large margin semisupervised learning method within the framework of regularization, based on an efﬁcient margin loss for unlabeled data, which seeks efﬁcient extraction of the information from unlabeled data for estimating the Bayes decision boundary for classiﬁcation. For implementation, an iterative scheme is derived through conditional expectations. Finally, theoretical and numerical analyses are conducted, in addition to an application to gene function prediction. They suggest that the proposed method enables to recover the performance of its supervised counterpart based on complete data in rates of convergence, when possible. Keywords: difference convex programming, classiﬁcation, nonconvex minimization, regularization, support vectors</p><p>5 0.3460137 <a title="28-lsi-5" href="./jmlr-2009-Multi-task_Reinforcement_Learning_in_Partially_Observable_Stochastic_Environments.html">57 jmlr-2009-Multi-task Reinforcement Learning in Partially Observable Stochastic Environments</a></p>
<p>Author: Hui Li, Xuejun Liao, Lawrence Carin</p><p>Abstract: We consider the problem of multi-task reinforcement learning (MTRL) in multiple partially observable stochastic environments. We introduce the regionalized policy representation (RPR) to characterize the agent’s behavior in each environment. The RPR is a parametric model of the conditional distribution over current actions given the history of past actions and observations; the agent’s choice of actions is directly based on this conditional distribution, without an intervening model to characterize the environment itself. We propose off-policy batch algorithms to learn the parameters of the RPRs, using episodic data collected when following a behavior policy, and show their linkage to policy iteration. We employ the Dirichlet process as a nonparametric prior over the RPRs across multiple environments. The intrinsic clustering property of the Dirichlet process imposes sharing of episodes among similar environments, which effectively reduces the number of episodes required for learning a good policy in each environment, when data sharing is appropriate. The number of distinct RPRs and the associated clusters (the sharing patterns) are automatically discovered by exploiting the episodic data as well as the nonparametric nature of the Dirichlet process. We demonstrate the effectiveness of the proposed RPR as well as the RPR-based MTRL framework on various problems, including grid-world navigation and multi-aspect target classiﬁcation. The experimental results show that the RPR is a competitive reinforcement learning algorithm in partially observable domains, and the MTRL consistently achieves better performance than single task reinforcement learning. Keywords: reinforcement learning, partially observable Markov decision processes, multi-task learning, Dirichlet processes, regionalized policy representation</p><p>6 0.27380422 <a title="28-lsi-6" href="./jmlr-2009-Nonextensive_Information_Theoretic_Kernels_on_Measures.html">61 jmlr-2009-Nonextensive Information Theoretic Kernels on Measures</a></p>
<p>7 0.24354541 <a title="28-lsi-7" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>8 0.21621919 <a title="28-lsi-8" href="./jmlr-2009-Identification_of_Recurrent_Neural_Networks_by_Bayesian_Interrogation_Techniques.html">40 jmlr-2009-Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques</a></p>
<p>9 0.1901352 <a title="28-lsi-9" href="./jmlr-2009-Estimating_Labels_from_Label_Proportions.html">29 jmlr-2009-Estimating Labels from Label Proportions</a></p>
<p>10 0.18141261 <a title="28-lsi-10" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>11 0.18029308 <a title="28-lsi-11" href="./jmlr-2009-An_Algorithm_for_Reading_Dependencies_from_the_Minimal_Undirected_Independence_Map_of_a_Graphoid_that_Satisfies_Weak_Transitivity.html">6 jmlr-2009-An Algorithm for Reading Dependencies from the Minimal Undirected Independence Map of a Graphoid that Satisfies Weak Transitivity</a></p>
<p>12 0.16938441 <a title="28-lsi-12" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>13 0.15871206 <a title="28-lsi-13" href="./jmlr-2009-Python_Environment_for_Bayesian_Learning%3A_Inferring_the_Structure_of_Bayesian_Networks_from_Knowledge_and_Data%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">76 jmlr-2009-Python Environment for Bayesian Learning: Inferring the Structure of Bayesian Networks from Knowledge and Data    (Machine Learning Open Source Software Paper)</a></p>
<p>14 0.15496761 <a title="28-lsi-14" href="./jmlr-2009-Data-driven_Calibration_of_Penalties_for_Least-Squares_Regression.html">21 jmlr-2009-Data-driven Calibration of Penalties for Least-Squares Regression</a></p>
<p>15 0.14691898 <a title="28-lsi-15" href="./jmlr-2009-Computing_Maximum_Likelihood_Estimates_in_Recursive_Linear_Models_with_Correlated_Errors.html">17 jmlr-2009-Computing Maximum Likelihood Estimates in Recursive Linear Models with Correlated Errors</a></p>
<p>16 0.14660266 <a title="28-lsi-16" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>17 0.14626831 <a title="28-lsi-17" href="./jmlr-2009-Marginal_Likelihood_Integrals_for_Mixtures_of_Independence_Models.html">53 jmlr-2009-Marginal Likelihood Integrals for Mixtures of Independence Models</a></p>
<p>18 0.14520086 <a title="28-lsi-18" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>19 0.14209928 <a title="28-lsi-19" href="./jmlr-2009-DL-Learner%3A_Learning_Concepts_in_Description_Logics.html">20 jmlr-2009-DL-Learner: Learning Concepts in Description Logics</a></p>
<p>20 0.13974954 <a title="28-lsi-20" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.02), (11, 0.027), (19, 0.015), (26, 0.024), (38, 0.041), (47, 0.473), (52, 0.033), (55, 0.069), (58, 0.039), (66, 0.078), (68, 0.019), (75, 0.01), (90, 0.034), (96, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86491185 <a title="28-lda-1" href="./jmlr-2009-Entropy_Inference_and_the_James-Stein_Estimator%2C_with_Application_to_Nonlinear_Gene_Association_Networks.html">28 jmlr-2009-Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks</a></p>
<p>Author: Jean Hausser, Korbinian Strimmer</p><p>Abstract: We present a procedure for effective estimation of entropy and mutual information from smallsample data, and apply it to the problem of inferring high-dimensional gene association networks. SpeciÄ?Ĺš cally, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efÄ?Ĺš cient statistically as well as computationally. Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling. We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data. A computer program is available that implements the proposed shrinkage estimator. Keywords: entropy, shrinkage estimation, James-Stein estimator, Ă˘&euro;&oelig;small n, large pĂ˘&euro;? setting, mutual information, gene association network</p><p>2 0.81902432 <a title="28-lda-2" href="./jmlr-2009-Online_Learning_with_Sample_Path_Constraints.html">67 jmlr-2009-Online Learning with Sample Path Constraints</a></p>
<p>Author: Shie Mannor, John N. Tsitsiklis, Jia Yuan Yu</p><p>Abstract: We study online learning where a decision maker interacts with Nature with the objective of maximizing her long-term average reward subject to some sample path average constraints. We deﬁne the reward-in-hindsight as the highest reward the decision maker could have achieved, while satisfying the constraints, had she known Nature’s choices in advance. We show that in general the reward-in-hindsight is not attainable. The convex hull of the reward-in-hindsight function is, however, attainable. For the important case of a single constraint, the convex hull turns out to be the highest attainable function. Using a calibrated forecasting rule, we provide an explicit strategy that attains this convex hull. We also measure the performance of heuristic methods based on non-calibrated forecasters in experiments involving a CPU power management problem. Keywords: online learning, calibration, regret minimization, approachability</p><p>3 0.77453929 <a title="28-lda-3" href="./jmlr-2009-Scalable_Collaborative_Filtering_Approaches_for_Large_Recommender_Systems%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">84 jmlr-2009-Scalable Collaborative Filtering Approaches for Large Recommender Systems    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>Author: Gábor Takács, István Pilászy, Bottyán Németh, Domonkos Tikk</p><p>Abstract: The collaborative ﬁltering (CF) using known user ratings of items has proved to be effective for predicting user preferences in item selection. This thriving subﬁeld of machine learning became popular in the late 1990s with the spread of online services that use recommender systems, such as Amazon, Yahoo! Music, and Netﬂix. CF approaches are usually designed to work on very large data sets. Therefore the scalability of the methods is crucial. In this work, we propose various scalable solutions that are validated against the Netﬂix Prize data set, currently the largest publicly available collection. First, we propose various matrix factorization (MF) based techniques. Second, a neighbor correction method for MF is outlined, which alloys the global perspective of MF and the localized property of neighbor based approaches efﬁciently. In the experimentation section, we ﬁrst report on some implementation issues, and we suggest on how parameter optimization can be performed efﬁciently for MFs. We then show that the proposed scalable approaches compare favorably with existing ones in terms of prediction accuracy and/or required training time. Finally, we report on some experiments performed on MovieLens and Jester data sets. Keywords: collaborative ﬁltering, recommender systems, matrix factorization, neighbor based correction, Netﬂix prize ∗. All authors also afﬁliated with Gravity R&D; Ltd., 1092 Budapest, Kinizsi u. 11., Hungary; info@gravityrd.com. c 2009 G´ bor Tak´ cs, Istv´ n Pil´ szy, Botty´ n N´ meth and Domonkos Tikk. a a a a a e ´ ´ ´ TAK ACS , P IL ASZY, N E METH AND T IKK</p><p>4 0.34828424 <a title="28-lda-4" href="./jmlr-2009-Settable_Systems%3A_An_Extension_of_Pearl%27s_Causal_Model_with_Optimization%2C_Equilibrium%2C_and_Learning.html">85 jmlr-2009-Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning</a></p>
<p>Author: Halbert White, Karim Chalak</p><p>Abstract: Judea Pearl’s Causal Model is a rich framework that provides deep insight into the nature of causal relations. As yet, however, the Pearl Causal Model (PCM) has had a lesser impact on economics or econometrics than on other disciplines. This may be due in part to the fact that the PCM is not as well suited to analyzing structures that exhibit features of central interest to economists and econometricians: optimization, equilibrium, and learning. We offer the settable systems framework as an extension of the PCM that permits causal discourse in systems embodying optimization, equilibrium, and learning. Because these are common features of physical, natural, or social systems, our framework may prove generally useful for machine learning. Important features distinguishing the settable system framework from the PCM are its countable dimensionality and the use of partitioning and partition-speciﬁc response functions to accommodate the behavior of optimizing and interacting agents and to eliminate the requirement of a unique ﬁxed point for the system. Reﬁnements of the PCM include the settable systems treatment of attributes, the causal role of exogenous variables, and the dual role of variables as causes and responses. A series of closely related machine learning examples and examples from game theory and machine learning with feedback demonstrates some limitations of the PCM and motivates the distinguishing features of settable systems. Keywords: equations causal models, game theory, machine learning, recursive estimation, simultaneous</p><p>5 0.31003806 <a title="28-lda-5" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider regularized support vector machines (SVMs) and show that they are precisely equivalent to a new robust optimization formulation. We show that this equivalence of robust optimization and regularization has implications for both algorithms, and analysis. In terms of algorithms, the equivalence suggests more general SVM-like algorithms for classiﬁcation that explicitly build in protection to noise, and at the same time control overﬁtting. On the analysis front, the equivalence of robustness and regularization provides a robust optimization interpretation for the success of regularized SVMs. We use this new robustness interpretation of SVMs to give a new proof of consistency of (kernelized) SVMs, thus establishing robustness as the reason regularized SVMs generalize well. Keywords: robustness, regularization, generalization, kernel, support vector machine</p><p>6 0.29386207 <a title="28-lda-6" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>7 0.28738955 <a title="28-lda-7" href="./jmlr-2009-Multi-task_Reinforcement_Learning_in_Partially_Observable_Stochastic_Environments.html">57 jmlr-2009-Multi-task Reinforcement Learning in Partially Observable Stochastic Environments</a></p>
<p>8 0.28700283 <a title="28-lda-8" href="./jmlr-2009-Prediction_With_Expert_Advice_For_The_Brier_Game.html">73 jmlr-2009-Prediction With Expert Advice For The Brier Game</a></p>
<p>9 0.28543371 <a title="28-lda-9" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>10 0.28362352 <a title="28-lda-10" href="./jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">79 jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>11 0.28264958 <a title="28-lda-11" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>12 0.27935195 <a title="28-lda-12" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>13 0.27821586 <a title="28-lda-13" href="./jmlr-2009-Adaptive_False_Discovery_Rate_Control_under_Independence_and_Dependence.html">5 jmlr-2009-Adaptive False Discovery Rate Control under Independence and Dependence</a></p>
<p>14 0.27699181 <a title="28-lda-14" href="./jmlr-2009-Marginal_Likelihood_Integrals_for_Mixtures_of_Independence_Models.html">53 jmlr-2009-Marginal Likelihood Integrals for Mixtures of Independence Models</a></p>
<p>15 0.27696377 <a title="28-lda-15" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>16 0.27649334 <a title="28-lda-16" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>17 0.2750228 <a title="28-lda-17" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>18 0.27123654 <a title="28-lda-18" href="./jmlr-2009-Nonextensive_Information_Theoretic_Kernels_on_Measures.html">61 jmlr-2009-Nonextensive Information Theoretic Kernels on Measures</a></p>
<p>19 0.27106699 <a title="28-lda-19" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>20 0.26962274 <a title="28-lda-20" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
