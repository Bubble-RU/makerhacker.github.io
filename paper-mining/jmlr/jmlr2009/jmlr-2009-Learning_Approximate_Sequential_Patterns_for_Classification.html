<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-45" href="#">jmlr2009-45</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</h1>
<br/><p>Source: <a title="jmlr-2009-45-pdf" href="http://jmlr.org/papers/volume10/syed09a/syed09a.pdf">pdf</a></p><p>Author: Zeeshan Syed, Piotr Indyk, John Guttag</p><p>Abstract: In this paper, we present an automated approach to discover patterns that can distinguish between sequences belonging to different labeled groups. Our method searches for approximately conserved motifs that occur with varying statistical properties in positive and negative training examples. We propose a two-step process to discover such patterns. Using locality sensitive hashing (LSH), we ﬁrst estimate the frequency of all subsequences and their approximate matches within a given Hamming radius in labeled examples. The discriminative ability of each pattern is then assessed from the estimated frequencies by concordance and rank sum testing. The use of LSH to identify approximate matches for each candidate pattern helps reduce the runtime of our method. Space requirements are reduced by decomposing the search problem into an iterative method that uses a single LSH table in memory. We propose two further optimizations to the search for discriminative patterns. Clustering with redundancy based on a 2-approximate solution of the k-center problem decreases the number of overlapping approximate groups while providing exhaustive coverage of the search space. Sequential statistical methods allow the search process to use data from only as many training examples as are needed to assess signiﬁcance. We evaluated our algorithm on data sets from different applications to discover sequential patterns for classiﬁcation. On nucleotide sequences from the Drosophila genome compared with random background sequences, our method was able to discover approximate binding sites that were preserved upstream of genes. We observed a similar result in experiments on ChIP-on-chip data. For cardiovascular data from patients admitted with acute coronary syndromes, our pattern discovery approach identiﬁed approximately conserved sequences of morphology variations that were predictive of future death in a test population. Our data showed that the use of LSH, clustering, and sequential statistic</p><p>Reference: <a title="jmlr-2009-45-reference" href="../jmlr2009_reference/jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lsh', 0.619), ('lshcs', 0.278), ('pattern', 0.205), ('ham', 0.167), ('hash', 0.15), ('npc', 0.139), ('nucleotid', 0.139), ('discovery', 0.131), ('drosophil', 0.128), ('exhsearch', 0.128), ('ndyk', 0.128), ('yed', 0.128), ('subsequ', 0.123), ('bind', 0.122), ('ncc', 0.118), ('noclust', 0.117), ('clust', 0.112), ('morpholog', 0.109), ('tomp', 0.107), ('genom', 0.104)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="45-tfidf-1" href="./jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification.html">45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</a></p>
<p>Author: Zeeshan Syed, Piotr Indyk, John Guttag</p><p>Abstract: In this paper, we present an automated approach to discover patterns that can distinguish between sequences belonging to different labeled groups. Our method searches for approximately conserved motifs that occur with varying statistical properties in positive and negative training examples. We propose a two-step process to discover such patterns. Using locality sensitive hashing (LSH), we ﬁrst estimate the frequency of all subsequences and their approximate matches within a given Hamming radius in labeled examples. The discriminative ability of each pattern is then assessed from the estimated frequencies by concordance and rank sum testing. The use of LSH to identify approximate matches for each candidate pattern helps reduce the runtime of our method. Space requirements are reduced by decomposing the search problem into an iterative method that uses a single LSH table in memory. We propose two further optimizations to the search for discriminative patterns. Clustering with redundancy based on a 2-approximate solution of the k-center problem decreases the number of overlapping approximate groups while providing exhaustive coverage of the search space. Sequential statistical methods allow the search process to use data from only as many training examples as are needed to assess signiﬁcance. We evaluated our algorithm on data sets from different applications to discover sequential patterns for classiﬁcation. On nucleotide sequences from the Drosophila genome compared with random background sequences, our method was able to discover approximate binding sites that were preserved upstream of genes. We observed a similar result in experiments on ChIP-on-chip data. For cardiovascular data from patients admitted with acute coronary syndromes, our pattern discovery approach identiﬁed approximately conserved sequences of morphology variations that were predictive of future death in a test population. Our data showed that the use of LSH, clustering, and sequential statistic</p><p>2 0.1356449 <a title="45-tfidf-2" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>Author: Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, S.V.N. Vishwanathan</p><p>Abstract: We propose hashing to facilitate efﬁcient kernels. This generalizes previous work using sampling and we show a principled way to compute the kernel matrix for data streams and sparse feature spaces. Moreover, we give deviation bounds from the exact kernel matrix. This has applications to estimation on strings and graphs. Keywords: hashing, stream, string kernel, graphlet kernel, multiclass classiﬁcation</p><p>3 0.072818756 <a title="45-tfidf-3" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>Author: Brijnesh J. Jain, Klaus Obermayer</p><p>Abstract: Finite structures such as point patterns, strings, trees, and graphs occur as ”natural” representations of structured data in different application areas of machine learning. We develop the theory of structure spaces and derive geometrical and analytical concepts such as the angle between structures and the derivative of functions on structures. In particular, we show that the gradient of a differentiable structural function is a well-deﬁned structure pointing in the direction of steepest ascent. Exploiting the properties of structure spaces, it will turn out that a number of problems in structural pattern recognition such as central clustering or learning in structured output spaces can be formulated as optimization problems with cost functions that are locally Lipschitz. Hence, methods from nonsmooth analysis are applicable to optimize those cost functions. Keywords: graphs, graph matching, learning in structured domains, nonsmooth optimization</p><p>4 0.061317187 <a title="45-tfidf-4" href="./jmlr-2009-Fast_ApproximatekNN_Graph_Construction_for_High_Dimensional_Data_via_Recursive_Lanczos_Bisection.html">34 jmlr-2009-Fast ApproximatekNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</a></p>
<p>Author: Jie Chen, Haw-ren Fang, Yousef Saad</p><p>Abstract: Nearest neighbor graphs are widely used in data mining and machine learning. A brute-force method to compute the exact kNN graph takes Θ(dn2 ) time for n data points in the d dimensional Euclidean space. We propose two divide and conquer methods for computing an approximate kNN graph in Θ(dnt ) time for high dimensional data (large d). The exponent t ∈ (1, 2) is an increasing function of an internal parameter α which governs the size of the common region in the divide step. Experiments show that a high quality graph can usually be obtained with small overlaps, that is, for small values of t. A few of the practical details of the algorithms are as follows. First, the divide step uses an inexpensive Lanczos procedure to perform recursive spectral bisection. After each conquer step, an additional reﬁnement step is performed to improve the accuracy of the graph. Finally, a hash table is used to avoid repeating distance calculations during the divide and conquer process. The combination of these techniques is shown to yield quite effective algorithms for building kNN graphs. Keywords: nearest neighbors graph, high dimensional data, divide and conquer, Lanczos algorithm, spectral method</p><p>5 0.056753658 <a title="45-tfidf-5" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Thomas Abeel, Yves Van de Peer, Yvan Saeys</p><p>Abstract: Java-ML is a collection of machine learning and data mining algorithms, which aims to be a readily usable and easily extensible API for both software developers and research scientists. The interfaces for each type of algorithm are kept simple and algorithms strictly follow their respective interface. Comparing different classiﬁers or clustering algorithms is therefore straightforward, and implementing new algorithms is also easy. The implementations of the algorithms are clearly written, properly documented and can thus be used as a reference. The library is written in Java and is available from http://java-ml.sourceforge.net/ under the GNU GPL license. Keywords: open source, machine learning, data mining, java library, clustering, feature selection, classiﬁcation</p><p>6 0.056197334 <a title="45-tfidf-6" href="./jmlr-2009-Supervised_Descriptive_Rule_Discovery%3A_A_Unifying_Survey_of_Contrast_Set%2C_Emerging_Pattern_and_Subgroup_Mining.html">92 jmlr-2009-Supervised Descriptive Rule Discovery: A Unifying Survey of Contrast Set, Emerging Pattern and Subgroup Mining</a></p>
<p>7 0.041855425 <a title="45-tfidf-7" href="./jmlr-2009-Robust_Process_Discovery_with_Artificial_Negative_Events%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">81 jmlr-2009-Robust Process Discovery with Artificial Negative Events    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>8 0.038392466 <a title="45-tfidf-8" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>9 0.038264561 <a title="45-tfidf-9" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>10 0.037527688 <a title="45-tfidf-10" href="./jmlr-2009-Nearest_Neighbor_Clustering%3A_A_Baseline_Method_for_Consistent_Clustering_with_Arbitrary_Objective_Functions.html">59 jmlr-2009-Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions</a></p>
<p>11 0.036977336 <a title="45-tfidf-11" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>12 0.036531236 <a title="45-tfidf-12" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>13 0.035477385 <a title="45-tfidf-13" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>14 0.032527827 <a title="45-tfidf-14" href="./jmlr-2009-Improving_the_Reliability_of_Causal_Discovery_from_Small_Data_Sets_Using_Argumentation%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">41 jmlr-2009-Improving the Reliability of Causal Discovery from Small Data Sets Using Argumentation    (Special Topic on Causality)</a></p>
<p>15 0.032517388 <a title="45-tfidf-15" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>16 0.030065391 <a title="45-tfidf-16" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>17 0.029475994 <a title="45-tfidf-17" href="./jmlr-2009-Using_Local_Dependencies_within_Batches_to_Improve_Large_Margin_Classifiers.html">99 jmlr-2009-Using Local Dependencies within Batches to Improve Large Margin Classifiers</a></p>
<p>18 0.029250389 <a title="45-tfidf-18" href="./jmlr-2009-Polynomial-Delay_Enumeration_of_Monotonic_Graph_Classes.html">72 jmlr-2009-Polynomial-Delay Enumeration of Monotonic Graph Classes</a></p>
<p>19 0.028193049 <a title="45-tfidf-19" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>20 0.028094307 <a title="45-tfidf-20" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.148), (1, 0.09), (2, -0.097), (3, 0.005), (4, -0.006), (5, 0.163), (6, 0.008), (7, -0.023), (8, 0.059), (9, -0.085), (10, 0.139), (11, 0.026), (12, -0.041), (13, 0.08), (14, -0.057), (15, 0.048), (16, 0.02), (17, -0.054), (18, 0.356), (19, 0.256), (20, -0.054), (21, -0.013), (22, 0.055), (23, -0.074), (24, -0.173), (25, -0.077), (26, 0.04), (27, 0.142), (28, -0.133), (29, 0.07), (30, -0.067), (31, 0.162), (32, 0.027), (33, -0.063), (34, -0.29), (35, 0.14), (36, -0.032), (37, -0.154), (38, 0.017), (39, -0.14), (40, 0.054), (41, 0.018), (42, -0.062), (43, 0.074), (44, -0.014), (45, -0.056), (46, -0.118), (47, 0.014), (48, 0.092), (49, -0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93759662 <a title="45-lsi-1" href="./jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification.html">45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</a></p>
<p>Author: Zeeshan Syed, Piotr Indyk, John Guttag</p><p>Abstract: In this paper, we present an automated approach to discover patterns that can distinguish between sequences belonging to different labeled groups. Our method searches for approximately conserved motifs that occur with varying statistical properties in positive and negative training examples. We propose a two-step process to discover such patterns. Using locality sensitive hashing (LSH), we ﬁrst estimate the frequency of all subsequences and their approximate matches within a given Hamming radius in labeled examples. The discriminative ability of each pattern is then assessed from the estimated frequencies by concordance and rank sum testing. The use of LSH to identify approximate matches for each candidate pattern helps reduce the runtime of our method. Space requirements are reduced by decomposing the search problem into an iterative method that uses a single LSH table in memory. We propose two further optimizations to the search for discriminative patterns. Clustering with redundancy based on a 2-approximate solution of the k-center problem decreases the number of overlapping approximate groups while providing exhaustive coverage of the search space. Sequential statistical methods allow the search process to use data from only as many training examples as are needed to assess signiﬁcance. We evaluated our algorithm on data sets from different applications to discover sequential patterns for classiﬁcation. On nucleotide sequences from the Drosophila genome compared with random background sequences, our method was able to discover approximate binding sites that were preserved upstream of genes. We observed a similar result in experiments on ChIP-on-chip data. For cardiovascular data from patients admitted with acute coronary syndromes, our pattern discovery approach identiﬁed approximately conserved sequences of morphology variations that were predictive of future death in a test population. Our data showed that the use of LSH, clustering, and sequential statistic</p><p>2 0.55722433 <a title="45-lsi-2" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>Author: Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, S.V.N. Vishwanathan</p><p>Abstract: We propose hashing to facilitate efﬁcient kernels. This generalizes previous work using sampling and we show a principled way to compute the kernel matrix for data streams and sparse feature spaces. Moreover, we give deviation bounds from the exact kernel matrix. This has applications to estimation on strings and graphs. Keywords: hashing, stream, string kernel, graphlet kernel, multiclass classiﬁcation</p><p>3 0.29024082 <a title="45-lsi-3" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>Author: Brijnesh J. Jain, Klaus Obermayer</p><p>Abstract: Finite structures such as point patterns, strings, trees, and graphs occur as ”natural” representations of structured data in different application areas of machine learning. We develop the theory of structure spaces and derive geometrical and analytical concepts such as the angle between structures and the derivative of functions on structures. In particular, we show that the gradient of a differentiable structural function is a well-deﬁned structure pointing in the direction of steepest ascent. Exploiting the properties of structure spaces, it will turn out that a number of problems in structural pattern recognition such as central clustering or learning in structured output spaces can be formulated as optimization problems with cost functions that are locally Lipschitz. Hence, methods from nonsmooth analysis are applicable to optimize those cost functions. Keywords: graphs, graph matching, learning in structured domains, nonsmooth optimization</p><p>4 0.28534406 <a title="45-lsi-4" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>Author: Juan José del Coz, Jorge Díez, Antonio Bahamonde</p><p>Abstract: Nondeterministic classiﬁers are deﬁned as those allowed to predict more than one class for some entries from an input space. Given that the true class should be included in predictions and the number of classes predicted should be as small as possible, these kind of classiﬁers can be considered as Information Retrieval (IR) procedures. In this paper, we propose a family of IR loss functions to measure the performance of nondeterministic learners. After discussing such measures, we derive an algorithm for learning optimal nondeterministic hypotheses. Given an entry from the input space, the algorithm requires the posterior probabilities to compute the subset of classes with the lowest expected loss. From a general point of view, nondeterministic classiﬁers provide an improvement in the proportion of predictions that include the true class compared to their deterministic counterparts; the price to be paid for this increase is usually a tiny proportion of predictions with more than one class. The paper includes an extensive experimental study using three deterministic learners to estimate posterior probabilities: a multiclass Support Vector Machine (SVM), a Logistic Regression, and a Na¨ve Bayes. The data sets considered comprise both UCI ı multi-class learning tasks and microarray expressions of different kinds of cancer. We successfully compare nondeterministic classiﬁers with other alternative approaches. Additionally, we shall see how the quality of posterior probabilities (measured by the Brier score) determines the goodness of nondeterministic predictions. Keywords: nondeterministic, multiclassiﬁcation, reject option, multi-label classiﬁcation, posterior probabilities</p><p>5 0.26805398 <a title="45-lsi-5" href="./jmlr-2009-Robust_Process_Discovery_with_Artificial_Negative_Events%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">81 jmlr-2009-Robust Process Discovery with Artificial Negative Events    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>Author: Stijn Goedertier, David Martens, Jan Vanthienen, Bart Baesens</p><p>Abstract: Process discovery is the automated construction of structured process models from information system event logs. Such event logs often contain positive examples only. Without negative examples, it is a challenge to strike the right balance between recall and speciﬁcity, and to deal with problems such as expressiveness, noise, incomplete event logs, or the inclusion of prior knowledge. In this paper, we present a conﬁgurable technique that deals with these challenges by representing process discovery as a multi-relational classiﬁcation problem on event logs supplemented with Artiﬁcially Generated Negative Events (AGNEs). This problem formulation allows using learning algorithms and evaluation techniques that are well-know in the machine learning community. Moreover, it allows users to have a declarative control over the inductive bias and language bias. Keywords: graph pattern discovery, inductive logic programming, Petri net, process discovery, positive data only</p><p>6 0.26486775 <a title="45-lsi-6" href="./jmlr-2009-Supervised_Descriptive_Rule_Discovery%3A_A_Unifying_Survey_of_Contrast_Set%2C_Emerging_Pattern_and_Subgroup_Mining.html">92 jmlr-2009-Supervised Descriptive Rule Discovery: A Unifying Survey of Contrast Set, Emerging Pattern and Subgroup Mining</a></p>
<p>7 0.21587701 <a title="45-lsi-7" href="./jmlr-2009-Fast_ApproximatekNN_Graph_Construction_for_High_Dimensional_Data_via_Recursive_Lanczos_Bisection.html">34 jmlr-2009-Fast ApproximatekNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</a></p>
<p>8 0.20316437 <a title="45-lsi-8" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>9 0.19532432 <a title="45-lsi-9" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>10 0.19274357 <a title="45-lsi-10" href="./jmlr-2009-Improving_the_Reliability_of_Causal_Discovery_from_Small_Data_Sets_Using_Argumentation%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">41 jmlr-2009-Improving the Reliability of Causal Discovery from Small Data Sets Using Argumentation    (Special Topic on Causality)</a></p>
<p>11 0.17715557 <a title="45-lsi-11" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>12 0.17653218 <a title="45-lsi-12" href="./jmlr-2009-Using_Local_Dependencies_within_Batches_to_Improve_Large_Margin_Classifiers.html">99 jmlr-2009-Using Local Dependencies within Batches to Improve Large Margin Classifiers</a></p>
<p>13 0.17471781 <a title="45-lsi-13" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>14 0.17335479 <a title="45-lsi-14" href="./jmlr-2009-Generalization_Bounds_for_Ranking_Algorithms_via_Algorithmic_Stability.html">37 jmlr-2009-Generalization Bounds for Ranking Algorithms via Algorithmic Stability</a></p>
<p>15 0.16468394 <a title="45-lsi-15" href="./jmlr-2009-Nearest_Neighbor_Clustering%3A_A_Baseline_Method_for_Consistent_Clustering_with_Arbitrary_Objective_Functions.html">59 jmlr-2009-Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions</a></p>
<p>16 0.15770513 <a title="45-lsi-16" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>17 0.15712519 <a title="45-lsi-17" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>18 0.14891642 <a title="45-lsi-18" href="./jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">70 jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<p>19 0.14785558 <a title="45-lsi-19" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>20 0.14777288 <a title="45-lsi-20" href="./jmlr-2009-Optimized_Cutting_Plane_Algorithm_for_Large-Scale_Risk_Minimization.html">69 jmlr-2009-Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.039), (20, 0.022), (31, 0.622), (40, 0.024), (42, 0.042), (43, 0.061), (57, 0.016), (62, 0.012), (85, 0.022), (87, 0.012), (97, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89558822 <a title="45-lda-1" href="./jmlr-2009-Robust_Process_Discovery_with_Artificial_Negative_Events%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">81 jmlr-2009-Robust Process Discovery with Artificial Negative Events    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>Author: Stijn Goedertier, David Martens, Jan Vanthienen, Bart Baesens</p><p>Abstract: Process discovery is the automated construction of structured process models from information system event logs. Such event logs often contain positive examples only. Without negative examples, it is a challenge to strike the right balance between recall and speciﬁcity, and to deal with problems such as expressiveness, noise, incomplete event logs, or the inclusion of prior knowledge. In this paper, we present a conﬁgurable technique that deals with these challenges by representing process discovery as a multi-relational classiﬁcation problem on event logs supplemented with Artiﬁcially Generated Negative Events (AGNEs). This problem formulation allows using learning algorithms and evaluation techniques that are well-know in the machine learning community. Moreover, it allows users to have a declarative control over the inductive bias and language bias. Keywords: graph pattern discovery, inductive logic programming, Petri net, process discovery, positive data only</p><p>same-paper 2 0.85769296 <a title="45-lda-2" href="./jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification.html">45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</a></p>
<p>Author: Zeeshan Syed, Piotr Indyk, John Guttag</p><p>Abstract: In this paper, we present an automated approach to discover patterns that can distinguish between sequences belonging to different labeled groups. Our method searches for approximately conserved motifs that occur with varying statistical properties in positive and negative training examples. We propose a two-step process to discover such patterns. Using locality sensitive hashing (LSH), we ﬁrst estimate the frequency of all subsequences and their approximate matches within a given Hamming radius in labeled examples. The discriminative ability of each pattern is then assessed from the estimated frequencies by concordance and rank sum testing. The use of LSH to identify approximate matches for each candidate pattern helps reduce the runtime of our method. Space requirements are reduced by decomposing the search problem into an iterative method that uses a single LSH table in memory. We propose two further optimizations to the search for discriminative patterns. Clustering with redundancy based on a 2-approximate solution of the k-center problem decreases the number of overlapping approximate groups while providing exhaustive coverage of the search space. Sequential statistical methods allow the search process to use data from only as many training examples as are needed to assess signiﬁcance. We evaluated our algorithm on data sets from different applications to discover sequential patterns for classiﬁcation. On nucleotide sequences from the Drosophila genome compared with random background sequences, our method was able to discover approximate binding sites that were preserved upstream of genes. We observed a similar result in experiments on ChIP-on-chip data. For cardiovascular data from patients admitted with acute coronary syndromes, our pattern discovery approach identiﬁed approximately conserved sequences of morphology variations that were predictive of future death in a test population. Our data showed that the use of LSH, clustering, and sequential statistic</p><p>3 0.42175862 <a title="45-lda-3" href="./jmlr-2009-Supervised_Descriptive_Rule_Discovery%3A_A_Unifying_Survey_of_Contrast_Set%2C_Emerging_Pattern_and_Subgroup_Mining.html">92 jmlr-2009-Supervised Descriptive Rule Discovery: A Unifying Survey of Contrast Set, Emerging Pattern and Subgroup Mining</a></p>
<p>Author: Petra Kralj Novak, Nada Lavrač, Geoffrey I. Webb</p><p>Abstract: This paper gives a survey of contrast set mining (CSM), emerging pattern mining (EPM), and subgroup discovery (SD) in a unifying framework named supervised descriptive rule discovery. While all these research areas aim at discovering patterns in the form of rules induced from labeled data, they use different terminology and task deﬁnitions, claim to have different goals, claim to use different rule learning heuristics, and use different means for selecting subsets of induced patterns. This paper contributes a novel understanding of these subareas of data mining by presenting a uniﬁed terminology, by explaining the apparent differences between the learning tasks as variants of a unique supervised descriptive rule discovery task and by exploring the apparent differences between the approaches. It also shows that various rule learning heuristics used in CSM, EPM and SD algorithms all aim at optimizing a trade off between rule coverage and precision. The commonalities (and differences) between the approaches are showcased on a selection of best known variants of CSM, EPM and SD algorithms. The paper also provides a critical survey of existing supervised descriptive rule discovery visualization methods. Keywords: descriptive rules, rule learning, contrast set mining, emerging patterns, subgroup discovery</p><p>4 0.35644698 <a title="45-lda-4" href="./jmlr-2009-Learning_Linear_Ranking_Functions_for_Beam_Search_with_Application_to_Planning.html">47 jmlr-2009-Learning Linear Ranking Functions for Beam Search with Application to Planning</a></p>
<p>Author: Yuehua Xu, Alan Fern, Sungwook Yoon</p><p>Abstract: Beam search is commonly used to help maintain tractability in large search spaces at the expense of completeness and optimality. Here we study supervised learning of linear ranking functions for controlling beam search. The goal is to learn ranking functions that allow for beam search to perform nearly as well as unconstrained search, and hence gain computational efﬁciency without seriously sacriﬁcing optimality. In this paper, we develop theoretical aspects of this learning problem and investigate the application of this framework to learning in the context of automated planning. We ﬁrst study the computational complexity of the learning problem, showing that even for exponentially large search spaces the general consistency problem is in NP. We also identify tractable and intractable subclasses of the learning problem, giving insight into the problem structure. Next, we analyze the convergence of recently proposed and modiﬁed online learning algorithms, where we introduce several notions of problem margin that imply convergence for the various algorithms. Finally, we present empirical results in automated planning, where ranking functions are learned to guide beam search in a number of benchmark planning domains. The results show that our approach is often able to outperform an existing state-of-the-art planning heuristic as well as a recent approach to learning such heuristics. Keywords: beam search, speedup learning, automated planning, structured classiﬁcation</p><p>5 0.31567091 <a title="45-lda-5" href="./jmlr-2009-CarpeDiem%3A_Optimizing_the_Viterbi_Algorithm_and_Applications_to_Supervised_Sequential_Learning.html">14 jmlr-2009-CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning</a></p>
<p>Author: Roberto Esposito, Daniele P. Radicioni</p><p>Abstract: The growth of information available to learning systems and the increasing complexity of learning tasks determine the need for devising algorithms that scale well with respect to all learning parameters. In the context of supervised sequential learning, the Viterbi algorithm plays a fundamental role, by allowing the evaluation of the best (most probable) sequence of labels with a time complexity linear in the number of time events, and quadratic in the number of labels. In this paper we propose CarpeDiem, a novel algorithm allowing the evaluation of the best possible sequence of labels with a sub-quadratic time complexity.1 We provide theoretical grounding together with solid empirical results supporting two chief facts. CarpeDiem always ﬁnds the optimal solution requiring, in most cases, only a small fraction of the time taken by the Viterbi algorithm; meantime, CarpeDiem is never asymptotically worse than the Viterbi algorithm, thus conﬁrming it as a sound replacement. Keywords: Viterbi algorithm, sequence labeling, conditional models, classiﬁers optimization, exact inference</p><p>6 0.30905852 <a title="45-lda-6" href="./jmlr-2009-Distributed_Algorithms_for_Topic_Models.html">25 jmlr-2009-Distributed Algorithms for Topic Models</a></p>
<p>7 0.30840304 <a title="45-lda-7" href="./jmlr-2009-Transfer_Learning_for_Reinforcement_Learning_Domains%3A_A_Survey.html">96 jmlr-2009-Transfer Learning for Reinforcement Learning Domains: A Survey</a></p>
<p>8 0.29978025 <a title="45-lda-8" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>9 0.29547396 <a title="45-lda-9" href="./jmlr-2009-Polynomial-Delay_Enumeration_of_Monotonic_Graph_Classes.html">72 jmlr-2009-Polynomial-Delay Enumeration of Monotonic Graph Classes</a></p>
<p>10 0.2946665 <a title="45-lda-10" href="./jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">70 jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<p>11 0.28558201 <a title="45-lda-11" href="./jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">79 jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>12 0.26801857 <a title="45-lda-12" href="./jmlr-2009-A_Parameter-Free_Classification_Method_for_Large_Scale_Learning.html">3 jmlr-2009-A Parameter-Free Classification Method for Large Scale Learning</a></p>
<p>13 0.26402178 <a title="45-lda-13" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>14 0.26165432 <a title="45-lda-14" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>15 0.25874585 <a title="45-lda-15" href="./jmlr-2009-Using_Local_Dependencies_within_Batches_to_Improve_Large_Margin_Classifiers.html">99 jmlr-2009-Using Local Dependencies within Batches to Improve Large Margin Classifiers</a></p>
<p>16 0.25602612 <a title="45-lda-16" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>17 0.24905097 <a title="45-lda-17" href="./jmlr-2009-Bayesian_Network_Structure_Learning_by_Recursive_Autonomy_Identification.html">11 jmlr-2009-Bayesian Network Structure Learning by Recursive Autonomy Identification</a></p>
<p>18 0.24792477 <a title="45-lda-18" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>19 0.24785993 <a title="45-lda-19" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>20 0.24368182 <a title="45-lda-20" href="./jmlr-2009-Bi-Level_Path_Following_for_Cross_Validated_Solution_of_Kernel_Quantile_Regression.html">12 jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
