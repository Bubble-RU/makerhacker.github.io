<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-100" href="#">jmlr2009-100</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</h1>
<br/><p>Source: <a title="jmlr-2009-100-pdf" href="http://jmlr.org/papers/volume10/argyriou09a/argyriou09a.pdf">pdf</a></p><p>Author: Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the L2 norm, then the learned vector is a linear combination of the input data. This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, in the case of differentiable regularizers. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufﬁcient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions. Keywords: kernel methods, matrix learning, minimal norm interpolation, multi-task learning, regularization</p><p>Reference: <a title="jmlr-2009-100-reference" href="../jmlr2009_reference/jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It is well known that if the regularizer is a nondecreasing function of the L2 norm, then the learned vector is a linear combination of the input data. [sent-14, score-0.401]
</p><p>2 This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. [sent-15, score-0.418]
</p><p>3 In this context, we study a more general representer theorem, which holds for a larger class of regularizers. [sent-18, score-0.418]
</p><p>4 We provide a necessary and sufﬁcient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. [sent-19, score-0.286]
</p><p>5 Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions. [sent-20, score-0.349]
</p><p>6 A RGYRIOU , M ICCHELLI AND P ONTIL  property, called the representer theorem, which states that there exists a solution of the regularization problem that is a linear combination of the data. [sent-29, score-0.547]
</p><p>7 o The topic of interest in this paper will be to determine the conditions under which representer theorems hold. [sent-31, score-0.496]
</p><p>8 In the ﬁrst half of the paper, we describe a property which a regularizer should satisfy in order to give rise to a representer theorem. [sent-32, score-0.678]
</p><p>9 It turns out that this property has a simple geometric interpretation and that the regularizer can be equivalently expressed as a nondecreasing function of the Hilbert space norm. [sent-33, score-0.505]
</p><p>10 Thus, we show that this condition, which has already been known to be sufﬁcient for representer theorems, is also necessary. [sent-34, score-0.418]
</p><p>11 For such problems, which have recently appeared in several machine learning applications, we show a modiﬁed version of the representer theorem that holds for a class of regularizers signiﬁcantly larger than in the former context. [sent-36, score-0.675]
</p><p>12 As we shall see, these matrix regularizers are important in the context of multi-task learning: the matrix columns are the parameters of different regression tasks and the regularizer encourages certain dependences across the tasks. [sent-37, score-0.692]
</p><p>13 In some Hilbert spaces such as Sobolev spaces the regularizer is a measure of smoothness: the smaller the norm the smoother the function. [sent-56, score-0.268]
</p><p>14 Specifically, when the regularizer is the square of the Hilbert space norm, the representer theorem holds: there exists a solution w of (1) which is a linear combination of the input vectors, ˆ m  w = ∑ ci xi , ˆ  (2)  i=1  where ci are some real coefﬁcients. [sent-60, score-0.753]
</p><p>15 It is also known that it extends to any regularizer that is a nondecreasing function of the norm (Sch¨ lkopf et al. [sent-62, score-0.481]
</p><p>16 Moreover, the representer theorem has been important in machine learning, particularly within the context of learning in reproducing kernel Hilbert spaces (Aronszajn, 1950)—see Sch¨ lkopf and Smola (2002) and Shawe-Taylor and Cristianini (2004) and references o therein. [sent-70, score-0.505]
</p><p>17 Our ﬁrst objective in this paper is to derive necessary and sufﬁcient conditions for representer theorems to hold. [sent-71, score-0.496]
</p><p>18 Even though one is mainly interested in regularization problems, it is more convenient to study interpolation problems, that is, problems of the form min Ω(w) : w ∈ H , w, xi = yi , ∀i = 1, . [sent-72, score-0.431]
</p><p>19 (3)  Thus, we begin this paper (Section 2) by showing how representer theorems for interpolation and regularization relate. [sent-76, score-0.815]
</p><p>20 On one side, a representer theorem for interpolation easily implies such a theorem for regularization with the same regularizer and any error function. [sent-77, score-1.067]
</p><p>21 Therefore, all representer theorems obtained in this paper apply equally to interpolation and regularization. [sent-78, score-0.716]
</p><p>22 Having addressed this issue, we concentrate in Section 3 on proving that an interpolation problem (3) admits solutions representable in the form (2) if and only if the regularizer is a nondecreasing function of the Hilbert space norm. [sent-80, score-0.679]
</p><p>23 That is, we provide a complete characterization of regularizers that give rise to representer theorems, which had been an open question. [sent-81, score-0.62]
</p><p>24 Furthermore, we discuss how our proof is motivated by a geometric understanding of the representer theorem, which is equivalently expressed as a monotonicity property of the regularizer. [sent-82, score-0.567]
</p><p>25 Our second objective is to formulate and study the novel question of representer theorems for matrix problems. [sent-85, score-0.58]
</p><p>26 Consequently, the representer theorem applies if the matrix regularizer is a nondecreasing function of the Frobenius norm. [sent-113, score-0.958]
</p><p>27 This observation leads us to formulate a modiﬁed representer theorem, which is appropriate for matrix problems, namely, n ms  wt = ˆ  ∑ ∑ csi xsi (t)  ∀t = 1, . [sent-119, score-0.749]
</p><p>28 As a result, this deﬁnition greatly expands the class of regularizers that give rise to representer theorems. [sent-130, score-0.62]
</p><p>29 For instance, some of these approaches use regularizers which involve the trace norm2 of matrix W . [sent-141, score-0.386]
</p><p>30 In the case of the trace norm, the representer theorem (5) is known to hold—see Abernethy et al. [sent-145, score-0.573]
</p><p>31 That is, under which conditions on the regularizer a representer theorem holds. [sent-151, score-0.693]
</p><p>32 2, we provide an answer by proving a necessary and sufﬁcient condition for representer theorems (5) to hold for problem (4), expressed as a simple monotonicity property. [sent-153, score-0.541]
</p><p>33 We also give a functional description equivalent to this property, that is, we show that the regularizers of interest are the matrix nondecreasing functions of the quantity W ⊤W . [sent-155, score-0.502]
</p><p>34 That is, our main concern will be to obtain necessary and sufﬁcient conditions for representer theorems that hold for interpolation problems. [sent-178, score-0.716]
</p><p>35 However, in practical applications one encounters regularization problems more frequently than interpolation problems. [sent-179, score-0.361]
</p><p>36 Indeed, an interpolation problem can be simply obtained in the limit as the regularization parameter goes to zero (Micchelli and Pinkus, 1994). [sent-181, score-0.319]
</p><p>37 More importantly, regularization enables one to trade off interpolation of the data against smoothness or simplicity of the model, whereas interpolation frequently suffers from overﬁtting. [sent-182, score-0.539]
</p><p>38 One may also consider the associated interpolation problem, which is min Ω(w) : w ∈ H , w, xi = yi , ∀i ∈ Nm . [sent-193, score-0.29]
</p><p>39 The main objective of this paper is to obtain necessary and sufﬁcient conditions on Ω so that the solution of problem (6) satisﬁes a linear representer theorem. [sent-199, score-0.448]
</p><p>40 Deﬁnition 2 We say that a class of optimization problems such as (6) or (7) satisﬁes the linear representer theorem if, for any choice of data {xi , yi : i ∈ Nm } such that the problem has a solution, there exists a solution that belongs to span{xi : i ∈ Nm }. [sent-200, score-0.585]
</p><p>41 2511  A RGYRIOU , M ICCHELLI AND P ONTIL  In this section, we show that the existence of representer theorems for regularization problems is equivalent to the existence of representer theorems for interpolation problems, under a quite general condition that has a simple geometric interpretation. [sent-201, score-1.417]
</p><p>42 2) which states that (linear or not) representer theorems for interpolation lead to representer theorems for regularization, under no conditions on the error function. [sent-203, score-1.212]
</p><p>43 Then if the class of interpolation problems (7) satisﬁes the linear representer theorem, so does the class of regularization problems (6). [sent-205, score-0.821]
</p><p>44 Then if the class of regularization problems (6) satisﬁes the linear representer theorem, so does the class of interpolation problems (7). [sent-222, score-0.821]
</p><p>45 Corollary 5 If E : Rm × Y m → R is the square loss, the hinge loss (for m ≥ 2) or the logistic loss (for m ≥ 2) and Ω : H → R is lower semicontinuous with bounded sublevel sets, then the class of problems (6) satisﬁes the linear representer theorem if and only if the class of problems (7) does. [sent-258, score-0.662]
</p><p>46 Then the class of regularization problems (6) in which the inputs xi , i ∈ Nm , are linearly independent, satisﬁes the linear representer theorem. [sent-264, score-0.589]
</p><p>47 Representer Theorems for Interpolation Problems The results of the previous section allow us to focus on linear representer theorems for interpolation problems of the type (7). [sent-266, score-0.758]
</p><p>48 In this section, we consider the interpolation problem min{Ω(w) : w ∈ H , w, xi = yi , i ∈ Nm }. [sent-269, score-0.29]
</p><p>49 Deﬁnition 7 We say that the function Ω : H → R is admissible if, for every m ∈ N and any data set {(xi , yi ) : i ∈ Nm } ⊆ H × Y such that the interpolation constraints are satisﬁable, problem (10) 3. [sent-271, score-0.353]
</p><p>50 The following theorem provides a necessary and sufﬁcient condition for a regularizer to be admissible. [sent-278, score-0.275]
</p><p>51 It is well known that the above functional form is sufﬁcient for a representer theorem to hold, see, for example, Sch¨ lkopf et al. [sent-281, score-0.54]
</p><p>52 o The route we follow to prove the above theorem is based on a geometric interpretation of representer theorems. [sent-284, score-0.537]
</p><p>53 Also w satisﬁes the interpolation ˆ ¯ ¯ constraints and hence we conclude that w is a solution to problem (10). [sent-291, score-0.282]
</p><p>54 It remains to establish the equivalence of the geometric property (13) to condition (12), which says that Ω is a nondecreasing function of the L2 norm. [sent-295, score-0.285]
</p><p>55 Theorem 8 can be used to verify whether the linear representer theorem can be obtained when using a regularizer Ω. [sent-328, score-0.693]
</p><p>56 Matrix Learning Problems In this section, we investigate how representer theorems and results like Theorem 8 can be extended in the context of optimization problems which involve matrices. [sent-332, score-0.538]
</p><p>57 As in Section 3, we could consider interpolation problems of the form min {Ω(W ) : W ∈ Md,n , W, Xi = yi , ∀i ∈ Nm }  (15)  where Xi ∈ Md,n are prescribed input matrices and yi ∈ Y are scalar outputs, for i ∈ Nm . [sent-336, score-0.342]
</p><p>58 Thus, in many recent applications, some of which we shall brieﬂy discuss below, it is natural to consider problems like (18) min {Ω(W ) : W ∈ Md,n , wt⊤ xti = yti , ∀i ∈ Nmt ,t ∈ Nn } . [sent-340, score-0.283]
</p><p>59 Here, wt ∈ Rd denote the columns of matrix W , for t ∈ Nn , and xti ∈ Rd , yti ∈ Y are prescribed inputs and outputs, for i ∈ Nmt ,t ∈ Nn . [sent-341, score-0.509]
</p><p>60 In this case, one may encounter representer theorems of the form wt = ˆ  ∑ ∑  (t)  ∀t ∈ Nn ,  csi xsi  (19)  s∈Nn i∈Nms (t)  where csi are scalar coefﬁcients for s,t ∈ Nn , i ∈ Nms . [sent-343, score-0.818]
</p><p>61 The learning algorithm used is min E wt⊤ xti , yti : i ∈ Nmt ,t ∈ Nn + γ Ω(W ) : W ∈ Md,n ,  (20)  where E : RM × Y M → R, M = ∑t∈Nn mt . [sent-355, score-0.274]
</p><p>62 One common choice is the trace norm, which is deﬁned to be the sum of the singular values of a matrix or, equivalently, Ω(W ) = W  1  1  := tr(W ⊤W ) 2 . [sent-359, score-0.296]
</p><p>63 Regularization with the trace norm learns the tasks as one joint optimization problem and can be seen as a convex relaxation of regularization with the rank (Fazel et al. [sent-360, score-0.286]
</p><p>64 In fact, these problems can be seen as instances of problems of the form (15), because the quantity wt⊤ xti can be written as the inner product between W and a matrix having all its columns equal to zero except for the t-th column being equal to xti . [sent-366, score-0.517]
</p><p>65 Despite this fact, by focusing on the class (18) we concentrate on problems of more practical interest and we can obtain representer theorems for a richer class of regularizers, which includes 2518  W HEN I S T HERE A R EPRESENTER T HEOREM ? [sent-368, score-0.538]
</p><p>66 A representer theorem of the form (19) for regularization with the trace norm has been shown in Argyriou et al. [sent-372, score-0.72]
</p><p>67 ˆ Theorem 12 If Ω is the trace norm then problem (18) (or problem (20)) admits a solution W of the (t) form (19), for some csi ∈ R, i ∈ Nms , s,t ∈ Nn . [sent-379, score-0.311]
</p><p>68 We can decompose the ˆ ˆ ¯ ¯ columns of W as wt = wt + pt , ∀t ∈ Nn , where wt ∈ L and pt ∈ L ⊥ . [sent-381, score-0.426]
</p><p>69 Hence W = W + P, where W is ˆ ¯ ¯ ⊤ ¯ the matrix with columns wt and P is the matrix with columns pt . [sent-382, score-0.415]
</p><p>70 We also have that wt⊤ xti = wt⊤ xti , ˆ ¯ preserves the interpolation constraints (or the value of the error for every i ∈ Nmt ,t ∈ Nn . [sent-385, score-0.504]
</p><p>71 2 Characterization of Matrix Regularizers Our objective in this section will be to state and prove a general representer theorem for problems of the form (18) or (21) using a functional form analogous to (12). [sent-399, score-0.55]
</p><p>72 However, it turns out that with regard to representer theorems of the form (19) there is no distinction between these two classes of problems. [sent-409, score-0.496]
</p><p>73 In other words, the representer theorem holds for the same regularizers Ω, independently of whether each task has its own speciﬁc inputs or the inputs are the same across the tasks. [sent-410, score-0.675]
</p><p>74 More importantly, we can connect the existence of representer theorems to a geometric property of the regularizer, in a way analogous to property (13) in Section 3. [sent-411, score-0.64]
</p><p>75 Proposition 13 The following statements are equivalent: (a) Problem (21) admits a solution of the form (19), for every data set {(xi , yti ) : i ∈ Nm ,t ∈ Nn } ⊆ Rd × Y and every m ∈ N, such that the interpolation constraints are satisﬁable. [sent-413, score-0.455]
</p><p>76 (b) Problem (18) admits a solution of the form (19), for every data set {(xti , yti ) : i ∈ Nmt ,t ∈ Nn } ⊆ Rd × Y and every {mt : t ∈ Nn } ⊆ N, such that the interpolation constraints are satisﬁable. [sent-414, score-0.455]
</p><p>77 ˆ [(c) =⇒ (b)] Consider arbitrary xti ∈ Rd , yti ∈ Y , ∀i ∈ Nmt ,t ∈ Nn , and let W be a solution to ˆ problem (18). [sent-424, score-0.271]
</p><p>78 We can decompose the columns of W as wt = wt + pt , where wt ∈ L := span{xsi , i ∈ ˆ ¯ ¯ ⊥ , ∀t ∈ N . [sent-425, score-0.426]
</p><p>79 We remark in passing that, by a similar proof, property (22) is also equivalent to representer theorems (19) for the class of problems (15). [sent-429, score-0.578]
</p><p>80 The above proposition provides us with a criterion for characterizing all regularizers satisfying representer theorems of the form (19), in the context of problems (15), (18) or (21). [sent-430, score-0.74]
</p><p>81 V ECTOR VERSUS M ATRIX R EGULARIZERS  Similar to the Hilbert space setting (12), where we required h to be a nondecreasing real function, the functional description of the regularizer now involves the notion of a matrix nondecreasing function. [sent-435, score-0.701]
</p><p>82 The differentiable function Ω : Md,n → R satisﬁes property (22) if and only if there exists a matrix nondecreasing function h : Sn → R such that + Ω(W ) = h(W ⊤W )  ∀ W ∈ Md,n . [sent-438, score-0.353]
</p><p>83 Let r = rank(W ) and let us write W in a singular value decomposition as follows W = ∑ σi ui v⊤ , i i∈Nr  where σ1 ≥ σ2 ≥ · · · ≥ σr > 0 are the singular values and ui ∈ Rd , vi ∈ Rn , ∀i ∈ Nr , are sets of singular vectors, so that u⊤ u j = v⊤ v j = δi j , ∀i, j ∈ Nr . [sent-445, score-0.46]
</p><p>84 In other words, we ﬁx the singular values, the right singular vectors and the r − 1 left singular vectors u2 , . [sent-469, score-0.402]
</p><p>85 i=2  In other words, if we ﬁx the singular values of W , the right singular vectors and all the left singular vectors but one, Ω does not depend on the remaining left singular vector (because the choice of z is independent of u1 ). [sent-477, score-0.514]
</p><p>86 Consider the matrix Y (W ⊤W ) 2 , which can be written using the same singular values and right singular vectors as W . [sent-480, score-0.341]
</p><p>87 Remark 16 Let the function Ω : Md,n → R be of the form Ω(W ) = h(W ⊤W )  ∀ W ∈ Md,n ,  for some matrix nondecreasing function h : Sn → R. [sent-525, score-0.265]
</p><p>88 The following properties are equiva+ lent: 2523  A RGYRIOU , M ICCHELLI AND P ONTIL  (a) h is matrix nondecreasing (b) the matrix ∇h(A) :=  ∂h ∂ai j  : i, j ∈ Nn is positive semideﬁnite, for every A ∈ Sn . [sent-529, score-0.349]
</p><p>89 3 Examples Using Proposition 13, Theorem 15 and Remark 16, one may easily verify that the representer theorem holds for a variety of regularizers. [sent-537, score-0.473]
</p><p>90 1 that the representer theorem holds when the regularizer is the trace norm (the L1 norm of the spectrum). [sent-565, score-0.889]
</p><p>91 V ECTOR VERSUS M ATRIX R EGULARIZERS  Rank minimization is an NP-hard optimization problem (Vandenberghe and Boyd, 1996), but the representer theorem has some interesting implications. [sent-568, score-0.473]
</p><p>92 This regularizer is an extension of the trace norm (K = 1) and can be used for multi-task learning via dimensionality reduction on multiple subspaces (Argyriou et al. [sent-584, score-0.368]
</p><p>93 This regularizer immediately veriﬁes property (22), and so by Theorem 15 it is a matrix nondecreasing function of W ⊤W . [sent-590, score-0.525]
</p><p>94 Finally, it is worth noting that the representer theorem does not apply to a family of “mixed” matrix norms of the form p Ω(W ) = ∑ wi 2 , i∈Nd  where wi denotes the i-th row of W and p > 1, p = 2. [sent-592, score-0.557]
</p><p>95 Conclusion We have characterized the classes of vector and matrix regularizers which lead to certain forms of the solution of the associated interpolation problems. [sent-594, score-0.536]
</p><p>96 In the vector case, we have proved the necessity of a well-known sufﬁcient condition for the “standard representer theorem”, which is encountered in many learning and statistical estimation problems. [sent-595, score-0.418]
</p><p>97 In the matrix case, we have described 2525  A RGYRIOU , M ICCHELLI AND P ONTIL  a novel class of regularizers which lead to a modiﬁed representer theorem. [sent-596, score-0.704]
</p><p>98 In the future, it would be valuable to study in more detail special cases of the matrix regularizers which we have encountered, such as those based on orthogonally invariant functions. [sent-599, score-0.286]
</p><p>99 It would also be interesting to investigate how the presence of additional constraints affects the representer theorem. [sent-600, score-0.45]
</p><p>100 On the representer theorem and equivalent degrees of freedom of SVR. [sent-749, score-0.473]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('representer', 0.418), ('regularizer', 0.22), ('interpolation', 0.22), ('regularizers', 0.202), ('nondecreasing', 0.181), ('egularizers', 0.166), ('epresenter', 0.166), ('icchelli', 0.166), ('ontil', 0.166), ('rgyriou', 0.166), ('nm', 0.154), ('nn', 0.147), ('atrix', 0.141), ('ector', 0.141), ('argyriou', 0.137), ('micchelli', 0.129), ('heorem', 0.126), ('xti', 0.126), ('wt', 0.121), ('yti', 0.115), ('singular', 0.112), ('rd', 0.109), ('nmt', 0.105), ('trace', 0.1), ('regularization', 0.099), ('nr', 0.098), ('sn', 0.097), ('hen', 0.096), ('horn', 0.089), ('matrix', 0.084), ('hilbert', 0.082), ('ur', 0.08), ('theorems', 0.078), ('abernethy', 0.077), ('csi', 0.075), ('nms', 0.075), ('span', 0.074), ('geometric', 0.064), ('columns', 0.063), ('admissible', 0.061), ('evgeniou', 0.06), ('cand', 0.06), ('od', 0.06), ('sublevel', 0.06), ('admits', 0.058), ('srebro', 0.057), ('recht', 0.057), ('theorem', 0.055), ('pontil', 0.053), ('xsi', 0.051), ('differentiable', 0.048), ('norm', 0.048), ('ui', 0.046), ('frobenius', 0.046), ('johnson', 0.046), ('monotonicity', 0.045), ('cavallanti', 0.045), ('maurer', 0.045), ('semicontinuous', 0.045), ('tr', 0.043), ('problems', 0.042), ('property', 0.04), ('semide', 0.04), ('yi', 0.04), ('tasks', 0.039), ('versus', 0.039), ('fazel', 0.038), ('xiong', 0.038), ('weyl', 0.038), ('dim', 0.036), ('functional', 0.035), ('spectral', 0.035), ('transfer', 0.035), ('zr', 0.034), ('amit', 0.034), ('rr', 0.034), ('tikhonov', 0.034), ('vito', 0.034), ('inner', 0.034), ('rm', 0.034), ('det', 0.033), ('mt', 0.033), ('vectors', 0.033), ('zi', 0.033), ('lkopf', 0.032), ('vi', 0.032), ('constraints', 0.032), ('xi', 0.03), ('wahba', 0.03), ('aaker', 0.03), ('albany', 0.03), ('dinuzzo', 0.03), ('gower', 0.03), ('izenman', 0.03), ('lenk', 0.03), ('prenorms', 0.03), ('schatten', 0.03), ('spheres', 0.03), ('solution', 0.03), ('assertion', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="100-tfidf-1" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>Author: Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the L2 norm, then the learned vector is a linear combination of the input data. This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, in the case of differentiable regularizers. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufﬁcient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions. Keywords: kernel methods, matrix learning, minimal norm interpolation, multi-task learning, regularization</p><p>2 0.19643654 <a title="100-tfidf-2" href="./jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>Author: Jacob Abernethy, Francis Bach, Theodoros Evgeniou, Jean-Philippe Vert</p><p>Abstract: We present a general approach for collaborative ﬁltering (CF) using spectral regularization to learn linear operators mapping a set of “users” to a set of possibly desired “objects”. In particular, several recent low-rank type matrix-completion methods for CF are shown to be special cases of our proposed framework. Unlike existing regularization-based CF, our approach can be used to incorporate additional information such as attributes of the users/objects—a feature currently lacking in existing regularization-based CF approaches—using popular and well-known kernel methods. We provide novel representer theorems that we use to develop new estimation methods. We then provide learning algorithms based on low-rank decompositions and test them on a standard CF data set. The experiments indicate the advantages of generalizing the existing regularization-based CF methods to incorporate related information about users and objects. Finally, we show that certain multi-task learning methods can be also seen as special cases of our proposed approach. Keywords: collaborative ﬁltering, matrix completion, kernel methods, spectral regularization</p><p>3 0.1734544 <a title="100-tfidf-3" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>Author: Haizhang Zhang, Yuesheng Xu, Jun Zhang</p><p>Abstract: We introduce the notion of reproducing kernel Banach spaces (RKBS) and study special semiinner-product RKBS by making use of semi-inner-products and the duality mapping. Properties of an RKBS and its reproducing kernel are investigated. As applications, we develop in the framework of RKBS standard learning schemes including minimal norm interpolation, regularization network, support vector machines, and kernel principal component analysis. In particular, existence, uniqueness and representer theorems are established. Keywords: reproducing kernel Banach spaces, reproducing kernels, learning theory, semi-innerproducts, representer theorems</p><p>4 0.12643465 <a title="100-tfidf-4" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: We continue our recent study on constructing a reﬁnement kernel for a given kernel so that the reproducing kernel Hilbert space associated with the reﬁnement kernel contains that with the original kernel as a subspace. To motivate this study, we ﬁrst develop a reﬁnement kernel method for learning, which gives an efﬁcient algorithm for updating a learning predictor. Several characterizations of reﬁnement kernels are then presented. It is shown that a nontrivial reﬁnement kernel for a given kernel always exists if the input space has an inﬁnite cardinal number. Reﬁnement kernels for translation invariant kernels and Hilbert-Schmidt kernels are investigated. Various concrete examples are provided. Keywords: reproducing kernels, reproducing kernel Hilbert spaces, learning with kernels, reﬁnement kernels, translation invariant kernels, Hilbert-Schmidt kernels</p><p>5 0.10077687 <a title="100-tfidf-5" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>Author: John Duchi, Yoram Singer</p><p>Abstract: We describe, analyze, and experiment with a framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we ﬁrst perform an unconstrained gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the ﬁrst phase. This view yields a simple yet effective algorithm that can be used for batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as ℓ1 . We derive concrete and very simple algorithms for minimization of loss functions with ℓ1 , ℓ2 , ℓ2 , and ℓ∞ regularization. We also show how to construct ef2 ﬁcient algorithms for mixed-norm ℓ1 /ℓq regularization. We further extend the algorithms and give efﬁcient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in a series of experiments with synthetic and natural data sets. Keywords: subgradient methods, group sparsity, online learning, convex optimization</p><p>6 0.094564445 <a title="100-tfidf-6" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>7 0.090252928 <a title="100-tfidf-7" href="./jmlr-2009-SGD-QN%3A_Careful_Quasi-Newton_Stochastic_Gradient_Descent.html">83 jmlr-2009-SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent</a></p>
<p>8 0.06666182 <a title="100-tfidf-8" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>9 0.066404395 <a title="100-tfidf-9" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>10 0.063980907 <a title="100-tfidf-10" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>11 0.060787987 <a title="100-tfidf-11" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>12 0.052784152 <a title="100-tfidf-12" href="./jmlr-2009-Optimized_Cutting_Plane_Algorithm_for_Large-Scale_Risk_Minimization.html">69 jmlr-2009-Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization</a></p>
<p>13 0.049773313 <a title="100-tfidf-13" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>14 0.047581233 <a title="100-tfidf-14" href="./jmlr-2009-Marginal_Likelihood_Integrals_for_Mixtures_of_Independence_Models.html">53 jmlr-2009-Marginal Likelihood Integrals for Mixtures of Independence Models</a></p>
<p>15 0.047257666 <a title="100-tfidf-15" href="./jmlr-2009-Fourier_Theoretic_Probabilistic_Inference_over_Permutations.html">36 jmlr-2009-Fourier Theoretic Probabilistic Inference over Permutations</a></p>
<p>16 0.04518434 <a title="100-tfidf-16" href="./jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">16 jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<p>17 0.042475004 <a title="100-tfidf-17" href="./jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>18 0.04146241 <a title="100-tfidf-18" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>19 0.04117012 <a title="100-tfidf-19" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>20 0.041091222 <a title="100-tfidf-20" href="./jmlr-2009-Nieme%3A_Large-Scale_Energy-Based_Models%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">60 jmlr-2009-Nieme: Large-Scale Energy-Based Models    (Machine Learning Open Source Software Paper)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.254), (1, 0.021), (2, 0.127), (3, -0.176), (4, -0.143), (5, 0.257), (6, 0.085), (7, -0.32), (8, 0.027), (9, -0.089), (10, 0.011), (11, 0.001), (12, 0.101), (13, 0.014), (14, 0.018), (15, 0.017), (16, -0.056), (17, 0.091), (18, 0.004), (19, -0.065), (20, 0.199), (21, 0.052), (22, -0.136), (23, 0.113), (24, 0.076), (25, -0.016), (26, 0.065), (27, -0.132), (28, -0.027), (29, 0.069), (30, -0.09), (31, 0.074), (32, -0.083), (33, 0.026), (34, -0.026), (35, 0.002), (36, 0.095), (37, -0.023), (38, 0.083), (39, -0.025), (40, -0.005), (41, 0.064), (42, -0.019), (43, 0.062), (44, -0.028), (45, -0.007), (46, -0.082), (47, 0.047), (48, 0.08), (49, -0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95389783 <a title="100-lsi-1" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>Author: Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the L2 norm, then the learned vector is a linear combination of the input data. This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, in the case of differentiable regularizers. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufﬁcient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions. Keywords: kernel methods, matrix learning, minimal norm interpolation, multi-task learning, regularization</p><p>2 0.71556711 <a title="100-lsi-2" href="./jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>Author: Jacob Abernethy, Francis Bach, Theodoros Evgeniou, Jean-Philippe Vert</p><p>Abstract: We present a general approach for collaborative ﬁltering (CF) using spectral regularization to learn linear operators mapping a set of “users” to a set of possibly desired “objects”. In particular, several recent low-rank type matrix-completion methods for CF are shown to be special cases of our proposed framework. Unlike existing regularization-based CF, our approach can be used to incorporate additional information such as attributes of the users/objects—a feature currently lacking in existing regularization-based CF approaches—using popular and well-known kernel methods. We provide novel representer theorems that we use to develop new estimation methods. We then provide learning algorithms based on low-rank decompositions and test them on a standard CF data set. The experiments indicate the advantages of generalizing the existing regularization-based CF methods to incorporate related information about users and objects. Finally, we show that certain multi-task learning methods can be also seen as special cases of our proposed approach. Keywords: collaborative ﬁltering, matrix completion, kernel methods, spectral regularization</p><p>3 0.69467109 <a title="100-lsi-3" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>Author: Haizhang Zhang, Yuesheng Xu, Jun Zhang</p><p>Abstract: We introduce the notion of reproducing kernel Banach spaces (RKBS) and study special semiinner-product RKBS by making use of semi-inner-products and the duality mapping. Properties of an RKBS and its reproducing kernel are investigated. As applications, we develop in the framework of RKBS standard learning schemes including minimal norm interpolation, regularization network, support vector machines, and kernel principal component analysis. In particular, existence, uniqueness and representer theorems are established. Keywords: reproducing kernel Banach spaces, reproducing kernels, learning theory, semi-innerproducts, representer theorems</p><p>4 0.52478904 <a title="100-lsi-4" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: We continue our recent study on constructing a reﬁnement kernel for a given kernel so that the reproducing kernel Hilbert space associated with the reﬁnement kernel contains that with the original kernel as a subspace. To motivate this study, we ﬁrst develop a reﬁnement kernel method for learning, which gives an efﬁcient algorithm for updating a learning predictor. Several characterizations of reﬁnement kernels are then presented. It is shown that a nontrivial reﬁnement kernel for a given kernel always exists if the input space has an inﬁnite cardinal number. Reﬁnement kernels for translation invariant kernels and Hilbert-Schmidt kernels are investigated. Various concrete examples are provided. Keywords: reproducing kernels, reproducing kernel Hilbert spaces, learning with kernels, reﬁnement kernels, translation invariant kernels, Hilbert-Schmidt kernels</p><p>5 0.47100258 <a title="100-lsi-5" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>Author: Christian Rieger, Barbara Zwicknagl</p><p>Abstract: We introduce a new technique for the analysis of kernel-based regression problems. The basic tools are sampling inequalities which apply to all machine learning problems involving penalty terms induced by kernels related to Sobolev spaces. They lead to explicit deterministic results concerning the worst case behaviour of ε- and ν-SVRs. Using these, we show how to adjust regularization parameters to get best possible approximation orders for regression. The results are illustrated by some numerical examples. Keywords: sampling inequality, radial basis functions, approximation theory, reproducing kernel Hilbert space, Sobolev space</p><p>6 0.35009497 <a title="100-lsi-6" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>7 0.32117283 <a title="100-lsi-7" href="./jmlr-2009-SGD-QN%3A_Careful_Quasi-Newton_Stochastic_Gradient_Descent.html">83 jmlr-2009-SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent</a></p>
<p>8 0.32099879 <a title="100-lsi-8" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>9 0.3047744 <a title="100-lsi-9" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>10 0.30048415 <a title="100-lsi-10" href="./jmlr-2009-Optimized_Cutting_Plane_Algorithm_for_Large-Scale_Risk_Minimization.html">69 jmlr-2009-Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization</a></p>
<p>11 0.28541398 <a title="100-lsi-11" href="./jmlr-2009-Nieme%3A_Large-Scale_Energy-Based_Models%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">60 jmlr-2009-Nieme: Large-Scale Energy-Based Models    (Machine Learning Open Source Software Paper)</a></p>
<p>12 0.27951917 <a title="100-lsi-12" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>13 0.26080224 <a title="100-lsi-13" href="./jmlr-2009-Stable_and_Efficient_Gaussian_Process_Calculations.html">88 jmlr-2009-Stable and Efficient Gaussian Process Calculations</a></p>
<p>14 0.24169043 <a title="100-lsi-14" href="./jmlr-2009-Fourier_Theoretic_Probabilistic_Inference_over_Permutations.html">36 jmlr-2009-Fourier Theoretic Probabilistic Inference over Permutations</a></p>
<p>15 0.23776805 <a title="100-lsi-15" href="./jmlr-2009-Marginal_Likelihood_Integrals_for_Mixtures_of_Independence_Models.html">53 jmlr-2009-Marginal Likelihood Integrals for Mixtures of Independence Models</a></p>
<p>16 0.23298864 <a title="100-lsi-16" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>17 0.22533521 <a title="100-lsi-17" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>18 0.2179914 <a title="100-lsi-18" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>19 0.21648003 <a title="100-lsi-19" href="./jmlr-2009-Computing_Maximum_Likelihood_Estimates_in_Recursive_Linear_Models_with_Correlated_Errors.html">17 jmlr-2009-Computing Maximum Likelihood Estimates in Recursive Linear Models with Correlated Errors</a></p>
<p>20 0.21111368 <a title="100-lsi-20" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.018), (11, 0.429), (19, 0.017), (21, 0.059), (26, 0.016), (38, 0.018), (47, 0.015), (52, 0.023), (55, 0.056), (58, 0.033), (66, 0.114), (68, 0.029), (90, 0.059), (96, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95028663 <a title="100-lda-1" href="./jmlr-2009-Computing_Maximum_Likelihood_Estimates_in_Recursive_Linear_Models_with_Correlated_Errors.html">17 jmlr-2009-Computing Maximum Likelihood Estimates in Recursive Linear Models with Correlated Errors</a></p>
<p>Author: Mathias Drton, Michael Eichler, Thomas S. Richardson</p><p>Abstract: In recursive linear models, the multivariate normal joint distribution of all variables exhibits a dependence structure induced by a recursive (or acyclic) system of linear structural equations. These linear models have a long tradition and appear in seemingly unrelated regressions, structural equation modelling, and approaches to causal inference. They are also related to Gaussian graphical models via a classical representation known as a path diagram. Despite the models’ long history, a number of problems remain open. In this paper, we address the problem of computing maximum likelihood estimates in the subclass of ‘bow-free’ recursive linear models. The term ‘bow-free’ refers to the condition that the errors for variables i and j be uncorrelated if variable i occurs in the structural equation for variable j. We introduce a new algorithm, termed Residual Iterative Conditional Fitting (RICF), that can be implemented using only least squares computations. In contrast to existing algorithms, RICF has clear convergence properties and yields exact maximum likelihood estimates after the ﬁrst iteration whenever the MLE is available in closed form. Keywords: linear regression, maximum likelihood estimation, path diagram, structural equation model, recursive semi-Markov model, residual iterative conditional ﬁtting</p><p>2 0.91624123 <a title="100-lda-2" href="./jmlr-2009-Bayesian_Network_Structure_Learning_by_Recursive_Autonomy_Identification.html">11 jmlr-2009-Bayesian Network Structure Learning by Recursive Autonomy Identification</a></p>
<p>Author: Raanan Yehezkel, Boaz Lerner</p><p>Abstract: We propose the recursive autonomy identiﬁcation (RAI) algorithm for constraint-based (CB) Bayesian network structure learning. The RAI algorithm learns the structure by sequential application of conditional independence (CI) tests, edge direction and structure decomposition into autonomous sub-structures. The sequence of operations is performed recursively for each autonomous substructure while simultaneously increasing the order of the CI test. While other CB algorithms d-separate structures and then direct the resulted undirected graph, the RAI algorithm combines the two processes from the outset and along the procedure. By this means and due to structure decomposition, learning a structure using RAI requires a smaller number of CI tests of high orders. This reduces the complexity and run-time of the algorithm and increases the accuracy by diminishing the curse-of-dimensionality. When the RAI algorithm learned structures from databases representing synthetic problems, known networks and natural problems, it demonstrated superiority with respect to computational complexity, run-time, structural correctness and classiﬁcation accuracy over the PC, Three Phase Dependency Analysis, Optimal Reinsertion, greedy search, Greedy Equivalence Search, Sparse Candidate, and Max-Min Hill-Climbing algorithms. Keywords: Bayesian networks, constraint-based structure learning</p><p>same-paper 3 0.79789376 <a title="100-lda-3" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>Author: Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the L2 norm, then the learned vector is a linear combination of the input data. This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, in the case of differentiable regularizers. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufﬁcient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions. Keywords: kernel methods, matrix learning, minimal norm interpolation, multi-task learning, regularization</p><p>4 0.76048702 <a title="100-lda-4" href="./jmlr-2009-Bounded_Kernel-Based_Online_Learning.html">13 jmlr-2009-Bounded Kernel-Based Online Learning</a></p>
<p>Author: Francesco Orabona, Joseph Keshet, Barbara Caputo</p><p>Abstract: A common problem of kernel-based online algorithms, such as the kernel-based Perceptron algorithm, is the amount of memory required to store the online hypothesis, which may increase without bound as the algorithm progresses. Furthermore, the computational load of such algorithms grows linearly with the amount of memory used to store the hypothesis. To attack these problems, most previous work has focused on discarding some of the instances, in order to keep the memory bounded. In this paper we present a new algorithm, in which the instances are not discarded, but are instead projected onto the space spanned by the previous online hypothesis. We call this algorithm Projectron. While the memory size of the Projectron solution cannot be predicted before training, we prove that its solution is guaranteed to be bounded. We derive a relative mistake bound for the proposed algorithm, and deduce from it a slightly different algorithm which outperforms the Perceptron. We call this second algorithm Projectron++. We show that this algorithm can be extended to handle the multiclass and the structured output settings, resulting, as far as we know, in the ﬁrst online bounded algorithm that can learn complex classiﬁcation tasks. The method of bounding the hypothesis representation can be applied to any conservative online algorithm and to other online algorithms, as it is demonstrated for ALMA2 . Experimental results on various data sets show the empirical advantage of our technique compared to various bounded online algorithms, both in terms of memory and accuracy. Keywords: online learning, kernel methods, support vector machines, bounded support set</p><p>5 0.64836788 <a title="100-lda-5" href="./jmlr-2009-Markov_Properties_for_Linear_Causal_Models_with_Correlated_Errors%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">54 jmlr-2009-Markov Properties for Linear Causal Models with Correlated Errors    (Special Topic on Causality)</a></p>
<p>Author: Changsung Kang, Jin Tian</p><p>Abstract: A linear causal model with correlated errors, represented by a DAG with bi-directed edges, can be tested by the set of conditional independence relations implied by the model. A global Markov property speciﬁes, by the d-separation criterion, the set of all conditional independence relations holding in any model associated with a graph. A local Markov property speciﬁes a much smaller set of conditional independence relations which will imply all other conditional independence relations which hold under the global Markov property. For DAGs with bi-directed edges associated with arbitrary probability distributions, a local Markov property is given in Richardson (2003) which may invoke an exponential number of conditional independencies. In this paper, we show that for a class of linear structural equation models with correlated errors, there is a local Markov property which will invoke only a linear number of conditional independence relations. For general linear models, we provide a local Markov property that often invokes far fewer conditional independencies than that in Richardson (2003). The results have applications in testing linear structural equation models with correlated errors. Keywords: Markov properties, linear causal models, linear structural equation models, graphical models</p><p>6 0.58101308 <a title="100-lda-6" href="./jmlr-2009-Properties_of_Monotonic_Effects_on_Directed_Acyclic_Graphs.html">74 jmlr-2009-Properties of Monotonic Effects on Directed Acyclic Graphs</a></p>
<p>7 0.57297134 <a title="100-lda-7" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>8 0.54700488 <a title="100-lda-8" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>9 0.47607109 <a title="100-lda-9" href="./jmlr-2009-Settable_Systems%3A_An_Extension_of_Pearl%27s_Causal_Model_with_Optimization%2C_Equilibrium%2C_and_Learning.html">85 jmlr-2009-Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning</a></p>
<p>10 0.47281766 <a title="100-lda-10" href="./jmlr-2009-Strong_Limit_Theorems_for_the_Bayesian_Scoring_Criterion_in_Bayesian_Networks.html">89 jmlr-2009-Strong Limit Theorems for the Bayesian Scoring Criterion in Bayesian Networks</a></p>
<p>11 0.46181932 <a title="100-lda-11" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>12 0.45677453 <a title="100-lda-12" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>13 0.455654 <a title="100-lda-13" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>14 0.45177105 <a title="100-lda-14" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>15 0.44491282 <a title="100-lda-15" href="./jmlr-2009-Learning_Linear_Ranking_Functions_for_Beam_Search_with_Application_to_Planning.html">47 jmlr-2009-Learning Linear Ranking Functions for Beam Search with Application to Planning</a></p>
<p>16 0.43037656 <a title="100-lda-16" href="./jmlr-2009-Marginal_Likelihood_Integrals_for_Mixtures_of_Independence_Models.html">53 jmlr-2009-Marginal Likelihood Integrals for Mixtures of Independence Models</a></p>
<p>17 0.42991549 <a title="100-lda-17" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>18 0.42761645 <a title="100-lda-18" href="./jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>19 0.42693278 <a title="100-lda-19" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>20 0.42504686 <a title="100-lda-20" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
